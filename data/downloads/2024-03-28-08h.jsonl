{"created":"2024-03-27 17:59:56","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark","abstract":"We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques. Demos and datasets are available on our project page: https://facebookresearch.github.io/real-acoustic-fields/","sentences":["We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities.","The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms.","We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data.","In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data.","We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models.","Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach.","RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques.","Demos and datasets are available on our project page: https://facebookresearch.github.io/real-acoustic-fields/"],"url":"http://arxiv.org/abs/2403.18821v1","category":"cs.SD"}
{"created":"2024-03-27 17:59:54","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering","abstract":"Faithful human performance capture and free-view rendering from sparse RGB observations is a long-standing problem in Vision and Graphics. The main challenges are the lack of observations and the inherent ambiguities of the setting, e.g. occlusions and depth ambiguity. As a result, radiance fields, which have shown great promise in capturing high-frequency appearance and geometry details in dense setups, perform poorly when na\\\"ively supervising them on sparse camera views, as the field simply overfits to the sparse-view inputs. To address this, we propose MetaCap, a method for efficient and high-quality geometry recovery and novel view synthesis given very sparse or even a single view of the human. Our key idea is to meta-learn the radiance field weights solely from potentially sparse multi-view videos, which can serve as a prior when fine-tuning them on sparse imagery depicting the human. This prior provides a good network weight initialization, thereby effectively addressing ambiguities in sparse-view capture. Due to the articulated structure of the human body and motion-induced surface deformations, learning such a prior is non-trivial. Therefore, we propose to meta-learn the field weights in a pose-canonicalized space, which reduces the spatial feature range and makes feature learning more effective. Consequently, one can fine-tune our field parameters to quickly generalize to unseen poses, novel illumination conditions as well as novel and sparse (even monocular) camera views. For evaluating our method under different scenarios, we collect a new dataset, WildDynaCap, which contains subjects captured in, both, a dense camera dome and in-the-wild sparse camera rigs, and demonstrate superior results compared to recent state-of-the-art methods on both public and WildDynaCap dataset.","sentences":["Faithful human performance capture and free-view rendering from sparse RGB observations is a long-standing problem in Vision and Graphics.","The main challenges are the lack of observations and the inherent ambiguities of the setting, e.g. occlusions and depth ambiguity.","As a result, radiance fields, which have shown great promise in capturing high-frequency appearance and geometry details in dense setups, perform poorly when na\\\"ively supervising them on sparse camera views, as the field simply overfits to the sparse-view inputs.","To address this, we propose MetaCap, a method for efficient and high-quality geometry recovery and novel view synthesis given very sparse or even a single view of the human.","Our key idea is to meta-learn the radiance field weights solely from potentially sparse multi-view videos, which can serve as a prior when fine-tuning them on sparse imagery depicting the human.","This prior provides a good network weight initialization, thereby effectively addressing ambiguities in sparse-view capture.","Due to the articulated structure of the human body and motion-induced surface deformations, learning such a prior is non-trivial.","Therefore, we propose to meta-learn the field weights in a pose-canonicalized space, which reduces the spatial feature range and makes feature learning more effective.","Consequently, one can fine-tune our field parameters to quickly generalize to unseen poses, novel illumination conditions as well as novel and sparse (even monocular) camera views.","For evaluating our method under different scenarios, we collect a new dataset, WildDynaCap, which contains subjects captured in, both, a dense camera dome and in-the-wild sparse camera rigs, and demonstrate superior results compared to recent state-of-the-art methods on both public and WildDynaCap dataset."],"url":"http://arxiv.org/abs/2403.18820v1","category":"cs.CV"}
{"created":"2024-03-27 17:59:52","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion","abstract":"Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \\q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene.","sentences":["Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections.","By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \\q{counterfactual} dataset.","Our method involves capturing a scene before and after removing a single object, while minimizing other changes.","By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene.","However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset.","To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably.","Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene."],"url":"http://arxiv.org/abs/2403.18818v1","category":"cs.CV"}
{"created":"2024-03-27 17:59:35","title":"Signatures of electronic ordering in transport in graphene flat bands","abstract":"Recently, a wide family of electronic orders was unveiled in graphene flat bands, such as spin- and valley-polarized phases as well as nematic momentum-polarized phases, stabilized by exchange interactions via a generalized Stoner mechanism. Momentum polarization involves orbital degrees of freedom and is therefore expected to impact resistivity in a way which is uniquely sensitive to the ordering type. Under pocket polarization, carrier distribution shifts in k space and samples the band mass in regions defined by the displaced momentum distribution. This makes transport coefficients sensitive to pocket polarization, resulting in the ohmic resistivity decreasing with temperature. In addition, it leads to current switching and hysteresis under strong E field. This behavior remains robust in the presence of electron-phonon scattering and is therefore expected to be generic.","sentences":["Recently, a wide family of electronic orders was unveiled in graphene flat bands, such as spin- and valley-polarized phases as well as nematic momentum-polarized phases, stabilized by exchange interactions via a generalized Stoner mechanism.","Momentum polarization involves orbital degrees of freedom and is therefore expected to impact resistivity in a way which is uniquely sensitive to the ordering type.","Under pocket polarization, carrier distribution shifts in k space and samples the band mass in regions defined by the displaced momentum distribution.","This makes transport coefficients sensitive to pocket polarization, resulting in the ohmic resistivity decreasing with temperature.","In addition, it leads to current switching and hysteresis under strong E field.","This behavior remains robust in the presence of electron-phonon scattering and is therefore expected to be generic."],"url":"http://arxiv.org/abs/2403.18817v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 17:59:33","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","abstract":"We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention. One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset. We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments.","sentences":["We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance.","Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts.","The generated assets can be directly draped and simulated on human bodies.","First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries.","However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target.","Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated.","Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets.","With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention.","One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset.","We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments."],"url":"http://arxiv.org/abs/2403.18816v1","category":"cs.CV"}
{"created":"2024-03-27 17:59:04","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models","abstract":"In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.","sentences":["In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs).","Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini.","We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation.","To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count.","We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs.","In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously.","Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models.","Code and models are available at https://github.com/dvlab-research/MiniGemini."],"url":"http://arxiv.org/abs/2403.18814v1","category":"cs.CV"}
{"created":"2024-03-27 17:57:02","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment","abstract":"We introduce a novel task within the field of 3D dance generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the \"follower\", synchronized with the lead dancer's movements and the underlying musical rhythm. Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position. To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers' performances. To address the challenges inherent in this task, we propose a GPT-based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader's and the follower's movements. To further enhance the GPT's capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards. Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics.","sentences":["We introduce a novel task within the field of 3D dance generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the \"follower\", synchronized with the lead dancer's movements and the underlying musical rhythm.","Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position.","To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers' performances.","To address the challenges inherent in this task, we propose a GPT-based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader's and the follower's movements.","To further enhance the GPT's capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards.","Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics."],"url":"http://arxiv.org/abs/2403.18811v1","category":"cs.CV"}
{"created":"2024-03-27 17:55:32","title":"$L^\\infty$-error bounds for approximations of the Koopman operator by kernel extended dynamic mode decomposition","abstract":"Extended dynamic mode decomposition (EDMD) is a well-established method to generate a data-driven approximation of the Koopman operator for analysis and prediction of nonlinear dynamical systems. Recently, kernel EDMD (kEDMD) has gained popularity due to its ability to resolve the challenging task of choosing a suitable dictionary by defining data-based observables. In this paper, we provide the first pointwise bounds on the approximation error of kEDMD. The main idea consists of two steps. First, we show that the reproducing kernel Hilbert spaces of Wendland functions are invariant under the Koopman operator. Second, exploiting that the learning problem given by regression in the native norm can be recast as an interpolation problem, we prove our novel error bounds by using interpolation estimates. Finally, we validate our findings with numerical experiments.","sentences":["Extended dynamic mode decomposition (EDMD) is a well-established method to generate a data-driven approximation of the Koopman operator for analysis and prediction of nonlinear dynamical systems.","Recently, kernel EDMD (kEDMD) has gained popularity due to its ability to resolve the challenging task of choosing a suitable dictionary by defining data-based observables.","In this paper, we provide the first pointwise bounds on the approximation error of kEDMD.","The main idea consists of two steps.","First, we show that the reproducing kernel Hilbert spaces of Wendland functions are invariant under the Koopman operator.","Second, exploiting that the learning problem given by regression in the native norm can be recast as an interpolation problem, we prove our novel error bounds by using interpolation estimates.","Finally, we validate our findings with numerical experiments."],"url":"http://arxiv.org/abs/2403.18809v1","category":"math.DS"}
{"created":"2024-03-27 17:53:35","title":"Solid lines in axial algebras of Jordan type half and Jordan algebras","abstract":"Primitive axial algebras of Jordan type $\\eta$ were introduced in 2015 by Hall,   Rehren and Shpectorov. These algebras are not characterized by polynomial identities. Rather, they are required to be generated by primitive idempotents whose multiplication is diagonalizable with eigenvalues $0,1,\\eta$, and its eigenvectors multiply following specific fusion rules.   Based on the concept of solid subalgebras, introduced very recently by Gorshkov, Shpectorov and Staroletov, we provide a criterion for when a primitive axial algebra of Jordan type with parameter $\\eta = \\frac{1}{2}$ is a Jordan algebra. We also prove that this is equivalent to associators being derivations of the algebra.","sentences":["Primitive axial algebras of Jordan type $\\eta$ were introduced in 2015 by Hall,   Rehren and Shpectorov.","These algebras are not characterized by polynomial identities.","Rather, they are required to be generated by primitive idempotents whose multiplication is diagonalizable with eigenvalues $0,1,\\eta$, and its eigenvectors multiply following specific fusion rules.   ","Based on the concept of solid subalgebras, introduced very recently by Gorshkov, Shpectorov and Staroletov, we provide a criterion for when a primitive axial algebra of Jordan type with parameter $\\eta = \\frac{1}{2}$ is a Jordan algebra.","We also prove that this is equivalent to associators being derivations of the algebra."],"url":"http://arxiv.org/abs/2403.18808v1","category":"math.RA"}
{"created":"2024-03-27 17:53:30","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation","abstract":"In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth. The code is available at https://github.com/Aradhye2002/EcoDepth.","sentences":["In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image.","While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture.","It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications.","Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information.","We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings.","Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings.","Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD).","And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth).","For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth.","The code is available at https://github.com/Aradhye2002/EcoDepth."],"url":"http://arxiv.org/abs/2403.18807v1","category":"cs.CV"}
{"created":"2024-03-27 17:52:56","title":"The socle of subshift algebras, with applications to subshift conjugacy","abstract":"We introduce the concept of \"irrational paths\" for a given subshift and use it to characterize all minimal left ideals in the associated unital subshift algebra. Consequently, we characterize the socle as the sum of the ideals generated by irrational paths. Proceeding, we construct a graph such that the Leavitt path algebra of this graph is graded isomorphic to the socle. This realization allows us to show that the graded structure of the socle serves as an invariant for the conjugacy of Ott-Tomforde-Willis subshifts and for the isometric conjugacy of subshifts constructed with the product topology. Additionally, we establish that the socle of the unital subshift algebra is contained in the socle of the corresponding unital subshift C*-algebra.","sentences":["We introduce the concept of \"irrational paths\" for a given subshift and use it to characterize all minimal left ideals in the associated unital subshift algebra.","Consequently, we characterize the socle as the sum of the ideals generated by irrational paths.","Proceeding, we construct a graph such that the Leavitt path algebra of this graph is graded isomorphic to the socle.","This realization allows us to show that the graded structure of the socle serves as an invariant for the conjugacy of Ott-Tomforde-Willis subshifts and for the isometric conjugacy of subshifts constructed with the product topology.","Additionally, we establish that the socle of the unital subshift algebra is contained in the socle of the corresponding unital subshift C*-algebra."],"url":"http://arxiv.org/abs/2403.18806v1","category":"math.RA"}
{"created":"2024-03-27 17:48:55","title":"Long-form factuality in large language models","abstract":"Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).   Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.","sentences":["Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.","To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics.","We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE).","SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results.","Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality.","To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).   ","Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.","At the same time, SAFE is more than 20 times cheaper than human annotators.","We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality.","LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality."],"url":"http://arxiv.org/abs/2403.18802v1","category":"cs.CL"}
{"created":"2024-03-27 17:46:11","title":"On two algebras of token graphs","abstract":"The $k$-token graph $F_k(G)$ of a graph $G$ is the graph whose vertices are the $k$-subsets of vertices from $G$, two of which being adjacent whenever their symmetric difference is a pair of adjacent vertices in $G$.   In this article, we describe some properties of the Laplacian matrix $\\L_k$ of $F_k(G)$ and the Laplacian matrix $\\overline{\\L}_k$ of the $k$-token graph $F_k(\\overline{G})$ of its complement $\\overline{G}$.   In this context, a result about the commutativity of the matrices $\\L_k$ and $\\overline{\\L}_k$ was given in [C. Dalf\\'o, F. Duque, R. Fabila-Monroy, M. A. Fiol, C. Huemer, A. L. Trujillo-Negrete, and F. J. Zaragoza Mart\\'{\\i}nez,   On the Laplacian spectra of token graphs,   {\\em Linear Algebra Appl.} {\\bf 625} (2021) 322--348], but the proof was incomplete, and there were some typos. Here, we give the correct proof.   Based on this result, and fixed the pair $(n,k)$ and the graph $G$, we first introduce a `local' algebra ${\\cal L}(G)$, generated by the pair $(\\L_k, \\overline{\\L}_k)$, showing its closed relationship with the Bose-Mesner algebra of the Johnson graphs $J(n,k)$.   Finally, fixed only $(n,k)$, we present a `global' algebra ${\\cal A}(n,k)$ that contains ${\\cal L}(G)$ together with the Laplacian and adjacency matrices of the $k$-token graph of any graph $G$ on $n$ vertices.","sentences":["The $k$-token graph $F_k(G)$ of a graph $G$ is the graph whose vertices are the $k$-subsets of vertices from $G$, two of which being adjacent whenever their symmetric difference is a pair of adjacent vertices in $G$.   In this article, we describe some properties of the Laplacian matrix $\\L_k$ of $F_k(G)$ and the Laplacian matrix $\\overline{\\L}_k$ of the $k$-token graph $F_k(\\overline{G})$ of its complement $\\overline{G}$.   In this context, a result about the commutativity of the matrices $\\L_k$ and $\\overline{\\L}_k$ was given in [C. Dalf\\'o, F. Duque, R. Fabila-Monroy, M. A. Fiol, C. Huemer, A. L. Trujillo-Negrete, and F. J. Zaragoza Mart\\'{\\i}nez,   On the Laplacian spectra of token graphs,   {\\em Linear Algebra Appl.}","{\\bf 625} (2021) 322--348], but the proof was incomplete, and there were some typos.","Here, we give the correct proof.   ","Based on this result, and fixed the pair $(n,k)$ and the graph $G$, we first introduce a `local' algebra ${\\cal L}(G)$, generated by the pair $(\\L_k, \\overline{\\L}_k)$, showing its closed relationship with the Bose-Mesner algebra of the Johnson graphs $J(n,k)$.   Finally, fixed only $(n,k)$, we present a `global' algebra ${\\cal A}(n,k)$ that contains ${\\cal L}(G)$ together with the Laplacian and adjacency matrices of the $k$-token graph of any graph $G$ on $n$ vertices."],"url":"http://arxiv.org/abs/2403.18800v1","category":"math.CO"}
{"created":"2024-03-27 17:40:14","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction","abstract":"We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies. We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset. Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU.","sentences":["We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines.","Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF).","Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage.","In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians.","Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies.","We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset.","Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU."],"url":"http://arxiv.org/abs/2403.18795v1","category":"cs.CV"}
{"created":"2024-03-27 17:37:24","title":"Evaluation of transition rates from nonequilibrium instantons","abstract":"Equilibrium rate theories play a crucial role in understanding rare, reactive events. However, they are inapplicable to a range of irreversible processes in systems driven far from thermodynamic equilibrium like active and biological matter. Here, we develop a general, computationally efficient nonequilibrium rate theory in the weak-noise limit based on an instanton approximation to the stochastic path integral and illustrate its wide range of application in the study of rare nonequilibrium events. We demonstrate excellent agreement of the instanton rates with numerically exact results for a particle under a non-conservative force. We also study phase transitions in an active field theory. We elucidate how activity alters the stability of the two phases and their rates of interconversion in a manner that can be well-described by modifying classical nucleation theory","sentences":["Equilibrium rate theories play a crucial role in understanding rare, reactive events.","However, they are inapplicable to a range of irreversible processes in systems driven far from thermodynamic equilibrium like active and biological matter.","Here, we develop a general, computationally efficient nonequilibrium rate theory in the weak-noise limit based on an instanton approximation to the stochastic path integral and illustrate its wide range of application in the study of rare nonequilibrium events.","We demonstrate excellent agreement of the instanton rates with numerically exact results for a particle under a non-conservative force.","We also study phase transitions in an active field theory.","We elucidate how activity alters the stability of the two phases and their rates of interconversion in a manner that can be well-described by modifying classical nucleation theory"],"url":"http://arxiv.org/abs/2403.18794v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-27 17:36:35","title":"Theory of quantum error mitigation for non-Clifford gates","abstract":"Quantum error mitigation techniques mimic noiseless quantum circuits by running several related noisy circuits and combining their outputs in particular ways. How well such techniques work is thought to depend strongly on how noisy the underlying gates are. Weakly-entangling gates, like $R_{ZZ}(\\theta)$ for small angles $\\theta$, can be much less noisy than entangling Clifford gates, like CNOT and CZ, and they arise naturally in circuits used to simulate quantum dynamics. However, such weakly-entangling gates are non-Clifford, and are therefore incompatible with two of the most prominent error mitigation techniques to date: probabilistic error cancellation (PEC) and the related form of zero-noise extrapolation (ZNE). This paper generalizes these techniques to non-Clifford gates, and comprises two complementary parts. The first part shows how to effectively transform any given quantum channel into (almost) any desired channel, at the cost of a sampling overhead, by adding random Pauli gates and processing the measurement outcomes. This enables us to cancel or properly amplify noise in non-Clifford gates, provided we can first characterize such gates in detail. The second part therefore introduces techniques to do so for noisy $R_{ZZ}(\\theta)$ gates. These techniques are robust to state preparation and measurement (SPAM) errors, and exhibit concentration and sensitivity--crucial features in many experiments. They are related to randomized benchmarking, and may also be of interest beyond the context of error mitigation. We find that while non-Clifford gates can be less noisy than related Cliffords, their noise is fundamentally more complex, which can lead to surprising and sometimes unwanted effects in error mitigation. Whether this trade-off can be broadly advantageous remains to be seen.","sentences":["Quantum error mitigation techniques mimic noiseless quantum circuits by running several related noisy circuits and combining their outputs in particular ways.","How well such techniques work is thought to depend strongly on how noisy the underlying gates are.","Weakly-entangling gates, like $R_{ZZ}(\\theta)$ for small angles $\\theta$, can be much less noisy than entangling Clifford gates, like CNOT and CZ, and they arise naturally in circuits used to simulate quantum dynamics.","However, such weakly-entangling gates are non-Clifford, and are therefore incompatible with two of the most prominent error mitigation techniques to date: probabilistic error cancellation (PEC) and the related form of zero-noise extrapolation (ZNE).","This paper generalizes these techniques to non-Clifford gates, and comprises two complementary parts.","The first part shows how to effectively transform any given quantum channel into (almost) any desired channel, at the cost of a sampling overhead, by adding random Pauli gates and processing the measurement outcomes.","This enables us to cancel or properly amplify noise in non-Clifford gates, provided we can first characterize such gates in detail.","The second part therefore introduces techniques to do so for noisy $R_{ZZ}(\\theta)$ gates.","These techniques are robust to state preparation and measurement (SPAM) errors, and exhibit concentration and sensitivity--crucial features in many experiments.","They are related to randomized benchmarking, and may also be of interest beyond the context of error mitigation.","We find that while non-Clifford gates can be less noisy than related Cliffords, their noise is fundamentally more complex, which can lead to surprising and sometimes unwanted effects in error mitigation.","Whether this trade-off can be broadly advantageous remains to be seen."],"url":"http://arxiv.org/abs/2403.18793v1","category":"quant-ph"}
{"created":"2024-03-27 17:34:50","title":"Squeezing below the ground state of motion of a continuously monitored levitating nanoparticle","abstract":"Squeezing is a crucial resource for quantum information processing and quantum sensing. In levitated nanomechanics, squeezed states of motion can be generated via temporal control of the trapping frequency of a massive particle. However, the amount of achievable squeezing typically suffers from detrimental environmental effects. We analyze the performance of a scheme that, by embedding careful time-control of trapping potentials and fully accounting for the most relevant sources of noise -- including measurement backaction -- achieves significant levels of mechanical squeezing. The feasibility of our proposal, which is close to experimental state-of-the-art, makes it a valuable tool for quantum state engineering.","sentences":["Squeezing is a crucial resource for quantum information processing and quantum sensing.","In levitated nanomechanics, squeezed states of motion can be generated via temporal control of the trapping frequency of a massive particle.","However, the amount of achievable squeezing typically suffers from detrimental environmental effects.","We analyze the performance of a scheme that, by embedding careful time-control of trapping potentials and fully accounting for the most relevant sources of noise -- including measurement backaction -- achieves significant levels of mechanical squeezing.","The feasibility of our proposal, which is close to experimental state-of-the-art, makes it a valuable tool for quantum state engineering."],"url":"http://arxiv.org/abs/2403.18790v1","category":"quant-ph"}
{"created":"2024-03-27 17:32:22","title":"Measuring the Lense-Thirring precession and the neutron star moment of inertia with pulsars","abstract":"Neutron stars (NSs) are compact objects that host the densest forms of matter in the observable universe, providing unique opportunities to study the behaviour of matter at extreme densities. While precision measurements of NS masses through pulsar timing have imposed effective constraints on the equation of state (EoS) of dense matter, accurately determining the radius or moment of inertia (MoI) of a NS remains a major challenge. This article presents a detailed review on measuring the Lense-Thirring (LT) precession effect in the orbit of binary pulsars, which would give access to the MoI of NSs and offer further constraints on the EoS. We discuss the suitability of certain classes of binary pulsars for measuring the LT precession from the perspective of binary star evolution, and highlight five pulsars that exhibit properties promising to realise these goals in the near future. Finally, discoveries of compact binaries with shorter orbital periods hold the potential to greatly enhance measurements of the MoI of NSs. The MoI measurements of binary pulsars are pivotal to advancing our understanding of matter at supranuclear densities as well as improving the precision of gravity tests, such as the orbital decay due to gravitational wave emission and of tests of alternative gravity theories.","sentences":["Neutron stars (NSs) are compact objects that host the densest forms of matter in the observable universe, providing unique opportunities to study the behaviour of matter at extreme densities.","While precision measurements of NS masses through pulsar timing have imposed effective constraints on the equation of state (EoS) of dense matter, accurately determining the radius or moment of inertia (MoI) of a NS remains a major challenge.","This article presents a detailed review on measuring the Lense-Thirring (LT) precession effect in the orbit of binary pulsars, which would give access to the MoI of NSs and offer further constraints on the EoS. We discuss the suitability of certain classes of binary pulsars for measuring the LT precession from the perspective of binary star evolution, and highlight five pulsars that exhibit properties promising to realise these goals in the near future.","Finally, discoveries of compact binaries with shorter orbital periods hold the potential to greatly enhance measurements of the MoI of NSs.","The MoI measurements of binary pulsars are pivotal to advancing our understanding of matter at supranuclear densities as well as improving the precision of gravity tests, such as the orbital decay due to gravitational wave emission and of tests of alternative gravity theories."],"url":"http://arxiv.org/abs/2403.18785v1","category":"gr-qc"}
{"created":"2024-03-27 17:32:04","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface","abstract":"We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.","sentences":["We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry.","Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions.","We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images.","We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process.","A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance.","The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality.","Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision."],"url":"http://arxiv.org/abs/2403.18784v1","category":"cs.CV"}
{"created":"2024-03-27 17:31:39","title":"Towards a World-English Language Model for On-Device Virtual Assistants","abstract":"Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them. Combining NNLMs for one or more of the categories is one way to improve scalability. In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs. In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}. We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks. Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models.","sentences":["Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them.","Combining NNLMs for one or more of the categories is one way to improve scalability.","In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs.","In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}.","We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks.","Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models."],"url":"http://arxiv.org/abs/2403.18783v1","category":"cs.CL"}
{"created":"2024-03-27 17:23:39","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object","abstract":"We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d.","sentences":["We establish rigorous benchmarks for visual perception robustness.","Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality.","In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness.","Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%.","Our work suggests that diffusion models can be an effective source to test vision models.","The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d."],"url":"http://arxiv.org/abs/2403.18775v1","category":"cs.CV"}
{"created":"2024-03-27 17:22:07","title":"On the equivalence of all notions of generalized derivations whose domain is a C$^{\\ast}$-algebra","abstract":"Let $\\mathcal{M}$ be a Banach bimodule over an associative Banach algebra $\\mathcal{A}$, and let $F: \\mathcal{A}\\to \\mathcal{M}$ be a linear mapping. Three main uses of the term \\emph{generalized derivation} are identified in the available literature, namely,   ($\\checkmark$) $F$ is a generalized derivation of the first type if there exists a derivation $ d : \\mathcal{A}\\to \\mathcal{M}$ satisfying $F(a b ) = F(a) b + a d(b)$ for all $a,b\\in \\mathcal{A}$.   ($\\checkmark$) $F$ is a generalized derivation of the second type if there exists an element $\\xi\\in \\mathcal{M}^{**}$ satisfying $F(a b ) = F(a) b + a F(b) - a \\xi b$ for all $a,b\\in \\mathcal{A}$.   ($\\checkmark$) $F$ is a generalized derivation of the third type if there exist two (non-necessarily linear) mappings $G,H : \\mathcal{A}\\to \\mathcal{M}$ satisfying $F(a b ) = G(a) b + a H(b)$ for all $a,b\\in \\mathcal{A}$.   There are examples showing that these three definitions are not, in general, equivalent. Despite that the first two notions are well studied when $\\mathcal{A}$ is a C$^*$-algebra, it is not known if the three notions are equivalent under these special assumptions. In this note we prove that every generalized derivation of the third type whose domain is a C$^*$-algebra is automatically continuous. We also prove that every (continuous) generalized derivation of the third type from a C$^*$-algebra $\\mathcal{A}$ into a general Banach $\\mathcal{A}$-bimodule is a generalized derivation of the first and second type. In particular, the three notions coincide in this case. We also explore the possible notions of generalized Jordan derivations on a C$^*$-algebra and establish some continuity properties for them.","sentences":["Let $\\mathcal{M}$ be a Banach bimodule over an associative Banach algebra $\\mathcal{A}$, and let $F: \\mathcal{A}\\to \\mathcal{M}$ be a linear mapping.","Three main uses of the term \\emph{generalized derivation} are identified in the available literature, namely,   ($\\checkmark$) $F$ is a generalized derivation of the first type if there exists a derivation $ d :","\\mathcal{A}\\to \\mathcal{M}$ satisfying $F(a b ) = F(a) b + a d(b)$ for all $a,b\\in \\mathcal{A}$.   ($\\checkmark$) $F$ is a generalized derivation of the second type if there exists an element $\\xi\\in \\mathcal{M}^{**}$ satisfying $F(a b ) = F(a) b + a F(b) - a \\xi b$ for all $a,b\\in \\mathcal{A}$.   ($\\checkmark$) $F$ is a generalized derivation of the third type if there exist two (non-necessarily linear) mappings $G,H :","\\mathcal{A}\\to \\mathcal{M}$ satisfying $F(a b ) = G(a) b + a H(b)$ for all $a,b\\in \\mathcal{A}$.   There are examples showing that these three definitions are not, in general, equivalent.","Despite that the first two notions are well studied when $\\mathcal{A}$ is a C$^*$-algebra, it is not known if the three notions are equivalent under these special assumptions.","In this note we prove that every generalized derivation of the third type whose domain is a C$^*$-algebra is automatically continuous.","We also prove that every (continuous) generalized derivation of the third type from a C$^*$-algebra $\\mathcal{A}$ into a general Banach $\\mathcal{A}$-bimodule is a generalized derivation of the first and second type.","In particular, the three notions coincide in this case.","We also explore the possible notions of generalized Jordan derivations on a C$^*$-algebra and establish some continuity properties for them."],"url":"http://arxiv.org/abs/2403.18773v1","category":"math.OA"}
{"created":"2024-03-27 17:21:46","title":"Non-local quantum field theory from doubly special relativity","abstract":"Non-local quantum field theories could be a solution to the inconsistencies arising when quantizing gravity. Doubly special relativity is regarded as a low-energy limit of a quantum gravity theory with testable predictions. We present a new formulation of quantum field theories in doubly special relativity with non-local behavior. Our construction restricts the models to those showing linear Lorentz invariance. The deformed Klein--Gordon, Dirac, and electromagnetic Lagrangians are derived. The deformed Maxwell equations and the electric potential of a point charge are discussed.","sentences":["Non-local quantum field theories could be a solution to the inconsistencies arising when quantizing gravity.","Doubly special relativity is regarded as a low-energy limit of a quantum gravity theory with testable predictions.","We present a new formulation of quantum field theories in doubly special relativity with non-local behavior.","Our construction restricts the models to those showing linear Lorentz invariance.","The deformed Klein--Gordon, Dirac, and electromagnetic Lagrangians are derived.","The deformed Maxwell equations and the electric potential of a point charge are discussed."],"url":"http://arxiv.org/abs/2403.18772v1","category":"hep-th"}
{"created":"2024-03-27 17:06:00","title":"Efficient Generation of Multi-partite Entanglement between Non-local Superconducting Qubits using Classical Feedback","abstract":"Quantum entanglement is one of the primary features which distinguishes quantum computers from classical computers. In gate-based quantum computing, the creation of entangled states or the distribution of entanglement across a quantum processor often requires circuit depths which grow with the number of entangled qubits. However, in teleportation-based quantum computing, one can deterministically generate entangled states with a circuit depth that is constant in the number of qubits, provided that one has access to an entangled resource state, the ability to perform mid-circuit measurements, and can rapidly transmit classical information. In this work, aided by fast classical FPGA-based control hardware with a feedback latency of only 150 ns, we explore the utility of teleportation-based protocols for generating non-local, multi-partite entanglement between superconducting qubits. First, we demonstrate well-known protocols for generating Greenberger-Horne-Zeilinger (GHZ) states and non-local CNOT gates in constant depth. Next, we utilize both protocols for implementing an unbounded fan-out (i.e., controlled-NOT-NOT) gate in constant depth between three non-local qubits. Finally, we demonstrate deterministic state teleportation and entanglement swapping between qubits on opposite side of our quantum processor.","sentences":["Quantum entanglement is one of the primary features which distinguishes quantum computers from classical computers.","In gate-based quantum computing, the creation of entangled states or the distribution of entanglement across a quantum processor often requires circuit depths which grow with the number of entangled qubits.","However, in teleportation-based quantum computing, one can deterministically generate entangled states with a circuit depth that is constant in the number of qubits, provided that one has access to an entangled resource state, the ability to perform mid-circuit measurements, and can rapidly transmit classical information.","In this work, aided by fast classical FPGA-based control hardware with a feedback latency of only 150 ns, we explore the utility of teleportation-based protocols for generating non-local, multi-partite entanglement between superconducting qubits.","First, we demonstrate well-known protocols for generating Greenberger-Horne-Zeilinger (GHZ) states and non-local CNOT gates in constant depth.","Next, we utilize both protocols for implementing an unbounded fan-out (i.e., controlled-NOT-NOT) gate in constant depth between three non-local qubits.","Finally, we demonstrate deterministic state teleportation and entanglement swapping between qubits on opposite side of our quantum processor."],"url":"http://arxiv.org/abs/2403.18768v1","category":"quant-ph"}
{"created":"2024-03-27 17:05:06","title":"The best approximation pair problem relative to two subsets in a normed space","abstract":"In the classical best approximation pair (BAP) problem, one is given two nonempty, closed, convex and disjoint subsets in a finite- or an infinite-dimensional Hilbert space, and the goal is to find a pair of points, each from each subset, which realizes the distance between the subsets. This problem, which has a long history, has found applications in science and technology. We discuss the problem in more general normed spaces and with possibly non-convex subsets, and focus our attention on the issues of uniqueness and existence of the solution to the problem. To the best of our knowledge these fundamental issues have not received much attention. In particular, we present several sufficient geometric conditions for the (at most) uniqueness of a BAP relative to these subsets. These conditions are related to the structure of the boundaries of the subsets, their relative orientation, and the structure of the unit sphere of the space. In addition, we present many sufficient conditions for the existence of a BAP, possibly without convexity . Our results allow us to significantly extend the horizon of the recent alternating simultaneous Halpern-Lions-Wittmann-Bauschke (A-S-HLWB) algorithm [Censor, Mansour and Reem, The alternating simultaneous Halpern-Lions-Wittmann-Bauschke algorithm for finding the best approximation pair for two disjoint intersections of convex sets, arXiv:2304.09600 (2023)] for solving the BAP problem.","sentences":["In the classical best approximation pair (BAP) problem, one is given two nonempty, closed, convex and disjoint subsets in a finite- or an infinite-dimensional Hilbert space, and the goal is to find a pair of points, each from each subset, which realizes the distance between the subsets.","This problem, which has a long history, has found applications in science and technology.","We discuss the problem in more general normed spaces and with possibly non-convex subsets, and focus our attention on the issues of uniqueness and existence of the solution to the problem.","To the best of our knowledge these fundamental issues have not received much attention.","In particular, we present several sufficient geometric conditions for the (at most) uniqueness of a BAP relative to these subsets.","These conditions are related to the structure of the boundaries of the subsets, their relative orientation, and the structure of the unit sphere of the space.","In addition, we present many sufficient conditions for the existence of a BAP, possibly without convexity .","Our results allow us to significantly extend the horizon of the recent alternating simultaneous Halpern-Lions-Wittmann-Bauschke (A-S-HLWB)","algorithm","[Censor, Mansour and Reem, The alternating simultaneous Halpern-Lions-Wittmann-Bauschke algorithm for finding the best approximation pair for two disjoint intersections of convex sets, arXiv:2304.09600 (2023)] for solving the BAP problem."],"url":"http://arxiv.org/abs/2403.18767v1","category":"math.OC"}
{"created":"2024-03-27 17:05:03","title":"Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means","abstract":"This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.","sentences":["This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology.","The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications.","It addresses scalability and computation time challenges typically faced with traditional techniques.","The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance.","Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration.","By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated.","In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting."],"url":"http://arxiv.org/abs/2403.18766v1","category":"cs.LG"}
{"created":"2024-03-27 17:01:32","title":"Duality for Hodge-Witt cohomology with modulus","abstract":"Given an effective Cartier divisor D with simple normal crossing support on a smooth and proper scheme X over a perfect field of positive characteristic p, there is a natural notion of de Rham-Witt sheaves on X with zeros along D. We show that these sheaves correspond under Grothendieck duality for coherent sheaves to de Rham-Witt sheaves on X with modulus (X,D), as defined in the theory of cube invariant modulus sheaves with transfers developed by Kahn-Miyazaki-Saito-Yamazaki. From this we deduce refined versions of Ekedahl - and Poincar\\'e duality for crystalline cohomology generalizing results of Mokrane and Nakkajima for reduced D, and a modulus version of Milne-Kato duality for \\'etale motivic cohomology with p-primary torsion coefficients, which refines a result of Jannsen-Saito-Zhao. We furthermore get new integral models for rigid cohomology with compact supports on the complement of D and a modulus version of Milne's perfect Brauer group pairing for smooth projective surfaces over finite fields.","sentences":["Given an effective Cartier divisor D with simple normal crossing support on a smooth and proper scheme X over a perfect field of positive characteristic p, there is a natural notion of de Rham-Witt sheaves on X with zeros along D. We show that these sheaves correspond under Grothendieck duality for coherent sheaves to de Rham-Witt sheaves on X with modulus (X,D), as defined in the theory of cube invariant modulus sheaves with transfers developed by Kahn-Miyazaki-Saito-Yamazaki.","From this we deduce refined versions of Ekedahl - and Poincar\\'e duality for crystalline cohomology generalizing results of Mokrane and Nakkajima for reduced D, and a modulus version of Milne-Kato duality for \\'etale motivic cohomology with p-primary torsion coefficients, which refines a result of Jannsen-Saito-Zhao.","We furthermore get new integral models for rigid cohomology with compact supports on the complement of D and a modulus version of Milne's perfect Brauer group pairing for smooth projective surfaces over finite fields."],"url":"http://arxiv.org/abs/2403.18763v1","category":"math.AG"}
{"created":"2024-03-27 17:01:10","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition","abstract":"Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images. This encoder yields more distinctive global descriptors for retrieval. Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time. Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities. We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git.","sentences":["Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps.","While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem.","Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision.","In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors.","We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images.","This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance.","We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images.","This encoder yields more distinctive global descriptors for retrieval.","Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time.","Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities.","We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git."],"url":"http://arxiv.org/abs/2403.18762v1","category":"cs.CV"}
{"created":"2024-03-27 16:59:21","title":"MATTopo: Topology-preserving Medial Axis Transform with Restricted Power Diagram","abstract":"We present a novel volumetric RPD (restricted power diagram) based framework for approximating the medial axes of 3D CAD shapes adaptively, while preserving topological equivalence, medial features, and geometric convergence. To solve the topology preservation problem, we propose a volumetric RPD based strategy, which discretizes the input volume into sub-regions given a set of medial spheres. With this intermediate structure, we convert the homotopy equivalence between the generated medial mesh and the input 3D shape into a localized problem between each primitive of the medial mesh (vertex, edge, face) and its dual restricted elements (power cell, power face, power edge), by checking their connected components and Euler characteristics. We further proposed a fractional Euler characteristic strategy for efficient GPU-based computation of Euler characteristic for each restricted element on the fly while computing the volumetric RPD. Compared with existing voxel-based or sampling-based methods, our method is the first that can adaptively and directly revise the medial mesh without modifying the dependent structure globally, such as voxel size or sampling density. Compared with the feature preservation method MATFP, our method offers geometrically comparable results with fewer number of spheres, while more robustly captures the topology of the input shape.","sentences":["We present a novel volumetric RPD (restricted power diagram) based framework for approximating the medial axes of 3D CAD shapes adaptively, while preserving topological equivalence, medial features, and geometric convergence.","To solve the topology preservation problem, we propose a volumetric RPD based strategy, which discretizes the input volume into sub-regions given a set of medial spheres.","With this intermediate structure, we convert the homotopy equivalence between the generated medial mesh and the input 3D shape into a localized problem between each primitive of the medial mesh (vertex, edge, face) and its dual restricted elements (power cell, power face, power edge), by checking their connected components and Euler characteristics.","We further proposed a fractional Euler characteristic strategy for efficient GPU-based computation of Euler characteristic for each restricted element on the fly while computing the volumetric RPD.","Compared with existing voxel-based or sampling-based methods, our method is the first that can adaptively and directly revise the medial mesh without modifying the dependent structure globally, such as voxel size or sampling density.","Compared with the feature preservation method MATFP, our method offers geometrically comparable results with fewer number of spheres, while more robustly captures the topology of the input shape."],"url":"http://arxiv.org/abs/2403.18761v1","category":"cs.GR"}
{"created":"2024-03-27 16:58:20","title":"MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model","abstract":"In the realm of data-driven AI technology, the application of open-source large language models (LLMs) in robotic task planning represents a significant milestone. Recent robotic task planning methods based on open-source LLMs typically leverage vast task planning datasets to enhance models' planning abilities. While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences. This paper addresses this limitation by proposing MLDT, theMulti-Level Decomposition Task planning method. This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks. In order to enhance open-source LLMs' planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct instruction tuning on the generated corpus. Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks. We evaluate our method using various LLMs on four datasets in VirtualHome. Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT's effectiveness in overcoming the limitations of existing methods based on open-source LLMs as well as its practicality in complex, real-world scenarios.","sentences":["In the realm of data-driven AI technology, the application of open-source large language models (LLMs) in robotic task planning represents a significant milestone.","Recent robotic task planning methods based on open-source LLMs typically leverage vast task planning datasets to enhance models' planning abilities.","While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences.","This paper addresses this limitation by proposing MLDT, theMulti-Level Decomposition Task planning method.","This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks.","In order to enhance open-source LLMs' planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct instruction tuning on the generated corpus.","Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks.","We evaluate our method using various LLMs on four datasets in VirtualHome.","Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT's effectiveness in overcoming the limitations of existing methods based on open-source LLMs as well as its practicality in complex, real-world scenarios."],"url":"http://arxiv.org/abs/2403.18760v1","category":"cs.RO"}
{"created":"2024-03-27 16:58:09","title":"Symmetries of the Large Scale Structures of the Universe as a Phenomenology of a Fractal Turbulence: The Role of the Plasma Component","abstract":"We present a new perspective on the symmetries that govern the formation of large-scale structures across the Universe, particularly focusing on the transition from the seeds of galaxy clusters to the seeds of galaxies themselves. We address two main features of cosmological fluid dynamics pertaining to both the linear and non-linear regimes. The linear dynamics of cosmological perturbations within the Hubble horizon is characterized by the Jeans length, which separates stable configurations from unstable fluctuations due to the gravitational effect on sufficiently large (and therefore, massive enough) overdensities. On the other hand, the non-linear dynamics of the cosmological fluid is associated with a turbulent behavior once the Reynolds numbers reach a sufficiently high level. This turbulent regime leads to energy dissipation across smaller and smaller scales, resulting in a fractal distribution of eddies throughout physical space. The proposed scenario suggests that the spatial scale of eddy formation is associated with the Jeans length of various levels of fragmentation from an original large-scale structure. By focusing on the fragmentation of galaxy cluster seeds versus galaxy seeds, we arrived at a phenomenological law that links the ratio of the two structure densities to the number of galaxies in each cluster and to the Hausdorff number of the Universe matter distribution. Finally, we introduced a primordial magnetic field and studied its influence on the Jeans length dynamics. The resulting anisotropic behavior of the density contrast led us to infer that the main features of the turbulence could be reduced to a 2D Euler equation. Numerical simulations showed that the two lowest wavenumbers contained the major energy contribution of the spectrum.","sentences":["We present a new perspective on the symmetries that govern the formation of large-scale structures across the Universe, particularly focusing on the transition from the seeds of galaxy clusters to the seeds of galaxies themselves.","We address two main features of cosmological fluid dynamics pertaining to both the linear and non-linear regimes.","The linear dynamics of cosmological perturbations within the Hubble horizon is characterized by the Jeans length, which separates stable configurations from unstable fluctuations due to the gravitational effect on sufficiently large (and therefore, massive enough) overdensities.","On the other hand, the non-linear dynamics of the cosmological fluid is associated with a turbulent behavior once the Reynolds numbers reach a sufficiently high level.","This turbulent regime leads to energy dissipation across smaller and smaller scales, resulting in a fractal distribution of eddies throughout physical space.","The proposed scenario suggests that the spatial scale of eddy formation is associated with the Jeans length of various levels of fragmentation from an original large-scale structure.","By focusing on the fragmentation of galaxy cluster seeds versus galaxy seeds, we arrived at a phenomenological law that links the ratio of the two structure densities to the number of galaxies in each cluster and to the Hausdorff number of the Universe matter distribution.","Finally, we introduced a primordial magnetic field and studied its influence on the Jeans length dynamics.","The resulting anisotropic behavior of the density contrast led us to infer that the main features of the turbulence could be reduced to a 2D Euler equation.","Numerical simulations showed that the two lowest wavenumbers contained the major energy contribution of the spectrum."],"url":"http://arxiv.org/abs/2403.18759v1","category":"astro-ph.CO"}
{"created":"2024-03-27 16:57:48","title":"On the cohomological dimension of kernels of maps to $\\mathbb Z$","abstract":"We prove that if $G$ is a finitely generated RFRS group of cohomological dimension $2$, then $G$ is virtually free-by-cyclic if and only if $b_2^{(2)}(G) = 0$. This answers a question of Wise and generalises and gives a new proof of a recent theorem of Kielak and Linton, where the same result is obtained under the additional hypotheses that $G$ is virtually compact special and hyperbolic. More generally, we show that if $G$ is a RFRS group of cohomological dimension $n$ and of type $\\mathrm{FP}_{n-1}$, then $G$ admits a virtual map to $\\mathbb Z$ with kernel of rational cohomological dimension $n-1$ if and only if $b_n^{(2)}(G) = 0$.","sentences":["We prove that if $G$ is a finitely generated RFRS group of cohomological dimension $2$, then $G$ is virtually free-by-cyclic if and only if $b_2^{(2)}(G) = 0$.","This answers a question of Wise and generalises and gives a new proof of a recent theorem of Kielak and Linton, where the same result is obtained under the additional hypotheses that $G$ is virtually compact special and hyperbolic.","More generally, we show that if $G$ is a RFRS group of cohomological dimension $n$ and of type $\\mathrm{FP}_{n-1}$, then $G$ admits a virtual map to $\\mathbb Z$ with kernel of rational cohomological dimension $n-1$ if and only if $b_n^{(2)}(G) = 0$."],"url":"http://arxiv.org/abs/2403.18758v1","category":"math.GR"}
{"created":"2024-03-27 16:56:14","title":"Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray","abstract":"Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation cohort and 0.77 in the external validation cohort. Sensitivity was consistently above 92% in both cohorts. In the overall cohort (n=540), among patients with AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events (13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to accurately detect subclinical atherosclerosis on chest x-ray with elevated sensitivity, and to predict ASCVD events with elevated negative predictive value. Adoption of the AI-CAC model to refine CV risk stratification or as an opportunistic screening tool requires prospective evaluation.","sentences":["Aims.","To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray.","Methods and Results.","A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63","[51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months.","The CAC score calculated on chest CT was used as ground truth.","The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation).","The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome.","Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.","AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation cohort and 0.77 in the external validation cohort.","Sensitivity was consistently above 92% in both cohorts.","In the overall cohort (n=540), among patients with AI-CAC=0, a single ASCVD event occurred, after 4.3 years.","Patients with AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events (13.5% vs. 3.4%, log-rank=0.013).","Conclusion.","The AI-CAC model seems to accurately detect subclinical atherosclerosis on chest x-ray with elevated sensitivity, and to predict ASCVD events with elevated negative predictive value.","Adoption of the AI-CAC model to refine CV risk stratification or as an opportunistic screening tool requires prospective evaluation."],"url":"http://arxiv.org/abs/2403.18756v1","category":"cs.CV"}
{"created":"2024-03-27 16:54:45","title":"Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time","abstract":"The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness. However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization. We compare MOEIM in two experimental settings, including a total of nine graph datasets, two heuristic methods, a related MOEA, and a state-of-the-art Deep Learning approach. The experiments show that MOEIM overall outperforms the competitors in most of the tested many-objective settings. To conclude, we also investigate the correlation between the objectives, leading to novel insights into the topic. The codebase is available at https://github.com/eliacunegatti/MOEIM.","sentences":["The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most.","This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness.","However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time.","In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size.","To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization.","We compare MOEIM in two experimental settings, including a total of nine graph datasets, two heuristic methods, a related MOEA, and a state-of-the-art Deep Learning approach.","The experiments show that MOEIM overall outperforms the competitors in most of the tested many-objective settings.","To conclude, we also investigate the correlation between the objectives, leading to novel insights into the topic.","The codebase is available at https://github.com/eliacunegatti/MOEIM."],"url":"http://arxiv.org/abs/2403.18755v1","category":"cs.NE"}
{"created":"2024-03-27 16:53:29","title":"General quantum resources provide advantages in work extraction tasks","abstract":"We provide a thermodynamic task to certify the general quantum resources of both states and channels via work extraction, showing that general quantum resources provide advantages in work extraction. Such work extraction tasks can be further applied to certify quantum entanglement in a one-sided device-independent way. As an application, we report a novel type of anomalous energy flow -- a type of locally extractable energy that is attributed to the globally distributed entanglement. Finally, we show that the existence of this novel anomalous energy flow is equivalent to measurement incompatibility.","sentences":["We provide a thermodynamic task to certify the general quantum resources of both states and channels via work extraction, showing that general quantum resources provide advantages in work extraction.","Such work extraction tasks can be further applied to certify quantum entanglement in a one-sided device-independent way.","As an application, we report a novel type of anomalous energy flow -- a type of locally extractable energy that is attributed to the globally distributed entanglement.","Finally, we show that the existence of this novel anomalous energy flow is equivalent to measurement incompatibility."],"url":"http://arxiv.org/abs/2403.18753v1","category":"quant-ph"}
{"created":"2024-03-27 16:52:53","title":"Neutron Stars Mass-Radius relations analysis in the Quintessence scenario","abstract":"In this paper, we explore the effects of General Relativity modification on the Mass-Radius relations of Neutron Stars induced by the presence of the Quintessence field. We consider, in particular, the Kiselev model, according to which the Quintessence field, being present in the entire Universe, might also be present around massive objects. Considering the Equation of State (EoS) for Baryonic matter BSk22 derived by A. Y. Potekhin et al., we infer the upper limit for NS masses in the presence of Quintessence. The presence of Quintessence generates a peculiar effect for which the Mass-Radius relation is unvaried and therefore the presence of Quintessence is indistinguishable from ordinary matter, at least for the Kiselev model studied in this paper.","sentences":["In this paper, we explore the effects of General Relativity modification on the Mass-Radius relations of Neutron Stars induced by the presence of the Quintessence field.","We consider, in particular, the Kiselev model, according to which the Quintessence field, being present in the entire Universe, might also be present around massive objects.","Considering the Equation of State (EoS) for Baryonic matter BSk22 derived by A. Y. Potekhin et al., we infer the upper limit for NS masses in the presence of Quintessence.","The presence of Quintessence generates a peculiar effect for which the Mass-Radius relation is unvaried and therefore the presence of Quintessence is indistinguishable from ordinary matter, at least for the Kiselev model studied in this paper."],"url":"http://arxiv.org/abs/2403.18752v1","category":"gr-qc"}
{"created":"2024-03-27 16:45:02","title":"CYCLE: Learning to Self-Refine the Code Generation","abstract":"Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs. We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5%, across benchmarks and varied model sizes. We also notice that CYCLE outperforms code LMs that have 3$\\times$ more parameters in self-refinement.","sentences":["Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers.","However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction.","For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves.","Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   ","In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites.","We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS.","The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs.","We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5%, across benchmarks and varied model sizes.","We also notice that CYCLE outperforms code LMs that have 3$\\times$ more parameters in self-refinement."],"url":"http://arxiv.org/abs/2403.18746v1","category":"cs.SE"}
{"created":"2024-03-27 16:43:08","title":"A nonsmooth Frank-Wolfe algorithm through a dual cutting-plane approach","abstract":"An extension of the Frank-Wolfe Algorithm (FWA), also known as Conditional Gradient algorithm, is proposed. In its standard form, the FWA allows to solve constrained optimization problems involving $\\beta$-smooth cost functions, calling at each iteration a Linear Minimization Oracle. More specifically, the oracle solves a problem obtained by linearization of the original cost function. The algorithm designed and investigated in this article, named Dualized Level-Set (DLS) algorithm, extends the FWA and allows to address a class of nonsmooth costs, involving in particular support functions. The key idea behind the construction of the DLS method is a general interpretation of the FWA as a cutting-plane algorithm, from the dual point of view. The DLS algorithm essentially results from a dualization of a specific cutting-plane algorithm, based on projections on some level sets. The DLS algorithm generates a sequence of primal-dual candidates, and we prove that the corresponding primal-dual gap converges with a rate of $O(1/\\sqrt{t})$.","sentences":["An extension of the Frank-Wolfe Algorithm (FWA), also known as Conditional Gradient algorithm, is proposed.","In its standard form, the FWA allows to solve constrained optimization problems involving $\\beta$-smooth cost functions, calling at each iteration a Linear Minimization Oracle.","More specifically, the oracle solves a problem obtained by linearization of the original cost function.","The algorithm designed and investigated in this article, named Dualized Level-Set (DLS) algorithm, extends the FWA and allows to address a class of nonsmooth costs, involving in particular support functions.","The key idea behind the construction of the DLS method is a general interpretation of the FWA as a cutting-plane algorithm, from the dual point of view.","The DLS algorithm essentially results from a dualization of a specific cutting-plane algorithm, based on projections on some level sets.","The DLS algorithm generates a sequence of primal-dual candidates, and we prove that the corresponding primal-dual gap converges with a rate of $O(1/\\sqrt{t})$."],"url":"http://arxiv.org/abs/2403.18744v1","category":"math.OC"}
{"created":"2024-03-27 16:39:28","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","abstract":"Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.","sentences":["Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems.","While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question.","Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment.","We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy.","Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability.","We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches.","Disclaimer:","This paper contains potentially offensive text; reader discretion is advised."],"url":"http://arxiv.org/abs/2403.18742v1","category":"cs.LG"}
{"created":"2024-03-27 16:32:34","title":"Effects of Dark Matter on $f$-mode oscillations of Neutron Stars","abstract":"The aim of this study is to investigate the effect of dark matter (DM) on $f$-mode oscillations in DM admixed neutron stars (NSs). We consider hadronic matter modeled by the relativistic mean field model and the DM model based on the neutron decay anomaly. We study the non-radial $f$-mode oscillations for such DM admixed NS in a full general relativistic framework. We investigate the impact of DM, DM self-interaction, and DM fraction on the $f$-mode characteristics. We derive relations encoding the effect of DM on $f$-mode parameters. We then perform a systematic study by varying all the model parameters within their known uncertainty range and obtain a universal relation for the DM fraction based on the total mass of the star and DM self-interaction strength. We also perform a correlation study among model parameters and NS observables, in particular, $f$-mode parameters. Finally, we check the $f$-mode universal relations (URs) for the case of DM admixed NSs and demonstrate the existence of a degeneracy between purely hadronic NSs and DM admixed NSs.","sentences":["The aim of this study is to investigate the effect of dark matter (DM) on $f$-mode oscillations in DM admixed neutron stars (NSs).","We consider hadronic matter modeled by the relativistic mean field model and the DM model based on the neutron decay anomaly.","We study the non-radial $f$-mode oscillations for such DM admixed NS in a full general relativistic framework.","We investigate the impact of DM, DM self-interaction, and DM fraction on the $f$-mode characteristics.","We derive relations encoding the effect of DM on $f$-mode parameters.","We then perform a systematic study by varying all the model parameters within their known uncertainty range and obtain a universal relation for the DM fraction based on the total mass of the star and DM self-interaction strength.","We also perform a correlation study among model parameters and NS observables, in particular, $f$-mode parameters.","Finally, we check the $f$-mode universal relations (URs) for the case of DM admixed NSs and demonstrate the existence of a degeneracy between purely hadronic NSs and DM admixed NSs."],"url":"http://arxiv.org/abs/2403.18740v1","category":"gr-qc"}
{"created":"2024-03-27 16:25:35","title":"Characterization of genuine ramification using formal orbifolds","abstract":"We give a characterization of genuinely ramified maps of formal orbifolds in the Tannakian framework. In particular we show that a morphism is genuinely ramified if and only if the pullback of every stable bundle remains stable in the orbifold category. We also give some other characterizations of genuine ramification. This generalizes the results of [BKP1] and [BP1]. In fact, it is a positive characteristic analogue of results in [BKP2].","sentences":["We give a characterization of genuinely ramified maps of formal orbifolds in the Tannakian framework.","In particular we show that a morphism is genuinely ramified if and only if the pullback of every stable bundle remains stable in the orbifold category.","We also give some other characterizations of genuine ramification.","This generalizes the results of [BKP1] and [BP1].","In fact, it is a positive characteristic analogue of results in [BKP2]."],"url":"http://arxiv.org/abs/2403.18736v1","category":"math.AG"}
{"created":"2024-03-27 16:21:24","title":"Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods","abstract":"This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.","sentences":["This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case.","The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated.","This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models.","This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm."],"url":"http://arxiv.org/abs/2403.18731v1","category":"cs.AI"}
{"created":"2024-03-27 16:20:08","title":"ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses","abstract":"The uninterpretability of DNNs hinders their deployment to safety-critical applications. Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent. The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers.   General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical. The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses. However, they consist of only a few hard-coded abstract domains and abstract transformers (or transfer functions) and do not allow the user to define new analyses. Further, these libraries can handle only specific DNN architectures.   To address these issues, we develop a declarative DSL -- ConstraintFlow -- that can be used to specify Abstract Interpretation-based DNN certifiers. In ConstraintFlow, programmers can easily define various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries. We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures. Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes. We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow.","sentences":["The uninterpretability of DNNs hinders their deployment to safety-critical applications.","Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent.","The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers.   ","General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical.","The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses.","However, they consist of only a few hard-coded abstract domains and abstract transformers (or transfer functions) and do not allow the user to define new analyses.","Further, these libraries can handle only specific DNN architectures.   ","To address these issues, we develop a declarative DSL -- ConstraintFlow -- that can be used to specify Abstract Interpretation-based DNN certifiers.","In ConstraintFlow, programmers can easily define various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries.","We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures.","Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes.","We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow."],"url":"http://arxiv.org/abs/2403.18729v1","category":"cs.PL"}
{"created":"2024-03-27 16:16:53","title":"Modular representations of the Yangian $Y_2$","abstract":"Let $Y_2$ be the Yangian associated to the general linear Lie algebra $\\mathfrak{gl}_2$, defined over an algebraically closed field $\\mathbbm{k}$ of characteristic $p > 0$. In this paper, we study the representation theory of the restricted Yangian $Y^{[p]}_2$. This gives a description of the representations of $\\mathfrak{gl}_{2n}$, whose $p$-character is a nilpotent whose Jordan type is the two-row partition $( n, n)$.","sentences":["Let $Y_2$ be the Yangian associated to the general linear Lie algebra $\\mathfrak{gl}_2$, defined over an algebraically closed field $\\mathbbm{k}$ of characteristic $p > 0$.","In this paper, we study the representation theory of the restricted Yangian $Y^{[p]}_2$. This gives a description of the representations of $\\mathfrak{gl}_{2n}$, whose $p$-character is a nilpotent whose Jordan type is the two-row partition $( n, n)$."],"url":"http://arxiv.org/abs/2403.18727v1","category":"math.RT"}
{"created":"2024-03-27 16:15:21","title":"Probabilistic Model Checking of Stochastic Reinforcement Learning Policies","abstract":"We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.","sentences":["We introduce a method to verify stochastic reinforcement learning (RL) policies.","This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property.","In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions.","Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm.","We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking.","Our results show that our method is suited to verify stochastic RL policies."],"url":"http://arxiv.org/abs/2403.18725v1","category":"cs.AI"}
{"created":"2024-03-27 16:11:23","title":"Testing Resource Isolation for System-on-Chip Architectures","abstract":"Ensuring resource isolation at the hardware level is a crucial step towards more security inside the Internet of Things. Even though there is still no generally accepted technique to generate appropriate tests, it became clear that tests should be generated at the system level. In this paper, we illustrate the modeling aspects in test generation for resource isolation, namely modeling the behavior and expressing the intended test scenario. We present both aspects using the industrial standard PSS and an academic approach based on conformance testing.","sentences":["Ensuring resource isolation at the hardware level is a crucial step towards more security inside the Internet of Things.","Even though there is still no generally accepted technique to generate appropriate tests, it became clear that tests should be generated at the system level.","In this paper, we illustrate the modeling aspects in test generation for resource isolation, namely modeling the behavior and expressing the intended test scenario.","We present both aspects using the industrial standard PSS and an academic approach based on conformance testing."],"url":"http://arxiv.org/abs/2403.18720v1","category":"cs.AR"}
{"created":"2024-03-27 16:08:12","title":"Constructive proofs of existence and stability of solitary waves in the capillary-gravity Whitham equation","abstract":"In this manuscript, we present a method to prove constructively the existence and spectral stability of solitary waves (solitons) in the capillary-gravity Whitham equation. By employing Fourier series analysis and computer-aided techniques, we successfully approximate the Fourier multiplier operator in this equation, allowing the construction of an approximate inverse for the linearization around an approximate solution $u_0$. Then, using a Newton-Kantorovich approach, we provide a sufficient condition under which the existence of a unique solitary wave $\\tilde{u}$ in a ball centered at $u_0$ is obtained. The verification of such a condition is established combining analytic techniques and rigorous numerical computations. Moreover, we derive a methodology to control the spectrum of the linearization around $\\tilde{u}$, enabling the study of spectral stability of the solution. As an illustration, we provide a (constructive) computer-assisted proof of existence of a stable soliton in both the case with capillary effects ($T>0$) and without capillary effects ($T=0$). The methodology presented in this paper can be generalized and provides a new approach for addressing the existence and spectral stability of solitary waves in nonlocal nonlinear equations. All computer-assisted proofs, including the requisite codes, are accessible on GitHub at \\cite{julia_cadiot}.","sentences":["In this manuscript, we present a method to prove constructively the existence and spectral stability of solitary waves (solitons) in the capillary-gravity Whitham equation.","By employing Fourier series analysis and computer-aided techniques, we successfully approximate the Fourier multiplier operator in this equation, allowing the construction of an approximate inverse for the linearization around an approximate solution $u_0$. Then, using a Newton-Kantorovich approach, we provide a sufficient condition under which the existence of a unique solitary wave $\\tilde{u}$ in a ball centered at $u_0$ is obtained.","The verification of such a condition is established combining analytic techniques and rigorous numerical computations.","Moreover, we derive a methodology to control the spectrum of the linearization around $\\tilde{u}$, enabling the study of spectral stability of the solution.","As an illustration, we provide a (constructive) computer-assisted proof of existence of a stable soliton in both the case with capillary effects ($T>0$) and without capillary effects ($T=0$).","The methodology presented in this paper can be generalized and provides a new approach for addressing the existence and spectral stability of solitary waves in nonlocal nonlinear equations.","All computer-assisted proofs, including the requisite codes, are accessible on GitHub at \\cite{julia_cadiot}."],"url":"http://arxiv.org/abs/2403.18718v1","category":"math.AP"}
{"created":"2024-03-27 16:06:37","title":"Semi-Supervised Learning for Deep Causal Generative Models","abstract":"Developing models that can answer questions of the form \"How would $x$ change if $y$ had been $z$?\" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels.","sentences":["Developing models that can answer questions of the form \"How would $x$ change if $y$ had been $z$?\" is fundamental for advancing medical image analysis.","Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data.","However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this.","We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data.","We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample.","We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels."],"url":"http://arxiv.org/abs/2403.18717v1","category":"cs.LG"}
{"created":"2024-03-27 16:05:02","title":"Statistical testing of random number generators and their improvement using randomness extraction","abstract":"Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting. Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method. In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites. The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies. With it, we benchmark the statistical properties of several RNGs, comparing them against each other. We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG's output quality under different sets of assumptions and analyse their impact through numerical testing with the STE.","sentences":["Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting.","Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method.","In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites.","The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies.","With it, we benchmark the statistical properties of several RNGs, comparing them against each other.","We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG's output quality under different sets of assumptions and analyse their impact through numerical testing with the STE."],"url":"http://arxiv.org/abs/2403.18716v1","category":"cs.CR"}
{"created":"2024-03-27 16:04:47","title":"Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding","abstract":"Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.","sentences":["Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs.","However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents.","To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference.","Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules.","ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution.","Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations.","Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs."],"url":"http://arxiv.org/abs/2403.18715v1","category":"cs.CV"}
{"created":"2024-03-27 16:02:00","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","abstract":"AI-Generated Images (AGIs) have inherent multimodal nature. Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual prompt into consideration. This is coupled in the ground truth score, which confuses the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via corresponding image and prompt incorporation. Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual prompts. An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied. Both are plug-and-play and beneficial for the cooperation of image and its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.","sentences":["AI-Generated Images (AGIs) have inherent multimodal nature.","Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual prompt into consideration.","This is coupled in the ground truth score, which confuses the unimodal IQA methods.","To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via corresponding image and prompt incorporation.","Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual prompts.","An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied.","Both are plug-and-play and beneficial for the cooperation of image and its corresponding prompt.","Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets.","Code will be available."],"url":"http://arxiv.org/abs/2403.18714v1","category":"cs.CV"}
{"created":"2024-03-27 16:00:48","title":"Characterization of Spatial-Temporal Channel Statistics from Indoor Measurement Data at D Band","abstract":"Millimeter-wave (mmWave) and D Band (110--170~GHz) frequencies are poised to play a pivotal role in the advancement of sixth-generation (6G) systems and beyond, owing to their ability to enhance performance metrics such as capacity, ultra-low latency, and spectral efficiency. This paper concentrates on deriving statistical insights into power, delay, and the number of paths based on measurements conducted across four distinct locations at a center frequency of 143.1 GHz. The findings underscore the suitability of various distributions in characterizing power behavior in line-of-sight (LOS) scenarios, including lognormal, Nakagami, gamma, and beta distributions, whereas the loglogistic distribution gives the optimal fit for power distribution in non-line-of-sight (NLOS) scenarios. Moreover, the exponential distribution shows to be the most appropriate model for the delay distribution in both LOS and NLOS scenarios. In terms of the number of paths, observations indicate a tendency for the highest concentration within the 10 m to 30 m distance range between the transmitter (Tx) and receiver (Rx). These insights shed light on the statistical nature of D band propagation characteristics, which are vital for informing the design and optimization of future 6G communication systems","sentences":["Millimeter-wave (mmWave) and D Band (110--170~GHz) frequencies are poised to play a pivotal role in the advancement of sixth-generation (6G) systems and beyond, owing to their ability to enhance performance metrics such as capacity, ultra-low latency, and spectral efficiency.","This paper concentrates on deriving statistical insights into power, delay, and the number of paths based on measurements conducted across four distinct locations at a center frequency of 143.1 GHz.","The findings underscore the suitability of various distributions in characterizing power behavior in line-of-sight (LOS) scenarios, including lognormal, Nakagami, gamma, and beta distributions, whereas the loglogistic distribution gives the optimal fit for power distribution in non-line-of-sight (NLOS) scenarios.","Moreover, the exponential distribution shows to be the most appropriate model for the delay distribution in both LOS and NLOS scenarios.","In terms of the number of paths, observations indicate a tendency for the highest concentration within the 10 m to 30 m distance range between the transmitter (Tx) and receiver (Rx).","These insights shed light on the statistical nature of D band propagation characteristics, which are vital for informing the design and optimization of future 6G communication systems"],"url":"http://arxiv.org/abs/2403.18713v1","category":"cs.IT"}
{"created":"2024-03-27 15:58:25","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable Transient-Free 3D reconstruction from Satellite Imagery","abstract":"Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images. However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions. Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation). To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery. However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines. Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning. Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstruction.","sentences":["Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images.","However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions.","Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation).","To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery.","However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines.","Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning.","Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstruction."],"url":"http://arxiv.org/abs/2403.18711v1","category":"cs.CV"}
{"created":"2024-03-27 15:57:42","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture","abstract":"Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate training and test data. Another major contribution of this paper is the insight that training data for a large traffic system can actually be sampled from the simulations of a much smaller traffic system. This is achieved through observing that the normalized energy distribution of the statistical mechanics model is scale invariant, which significantly eases the burden of data generation for large scale traffic systems. The resulting simulations indicate good agreement between the predicted and the true traffic flow dynamics.","sentences":["Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results.","These approaches face two key challenges.","First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems.","Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states.","Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base.","In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate training and test data.","Another major contribution of this paper is the insight that training data for a large traffic system can actually be sampled from the simulations of a much smaller traffic system.","This is achieved through observing that the normalized energy distribution of the statistical mechanics model is scale invariant, which significantly eases the burden of data generation for large scale traffic systems.","The resulting simulations indicate good agreement between the predicted and the true traffic flow dynamics."],"url":"http://arxiv.org/abs/2403.18710v1","category":"cs.LG"}
{"created":"2024-03-27 15:57:30","title":"Revisiting Stochastic Gravitational-wave Background in the Strong Signal Case","abstract":"Weak-signal limit is often used in estimating stochastic gravitational-wave background (SGWB) intensities. This approximation fails and the signal-to-noise ratio (SNR) can be much weaker when background signals are loud compared to the detector noise. In this work, we highlight this limitation for the SGWB detection using space-borne detector networks. For the TianQin + LISA network, the SNR estimated under the weak-signal limit might be off by as large as an order of magnitude. Contour plots of SNR over the parameter spaces are also presented to indicate regions susceptible to this discrepancy. Our results suggest that DA and DB type extragalactic double white dwarfs may yield an SGWB with SNR surpassing 100 after 1 year of operation in the weak-signal-limit scenario, with a redshift-independent merger rate of about $500\\,\\,{\\rm Mpc^{-3}\\,Myr^{-1}}$. In fact, this value falls significantly below the necessary threshold. Similar influences arise for first-order phase transitions, yet pinning down parameter regions remains formidable due to model uncertainties.","sentences":["Weak-signal limit is often used in estimating stochastic gravitational-wave background (SGWB) intensities.","This approximation fails and the signal-to-noise ratio (SNR) can be much weaker when background signals are loud compared to the detector noise.","In this work, we highlight this limitation for the SGWB detection using space-borne detector networks.","For the TianQin + LISA network, the SNR estimated under the weak-signal limit might be off by as large as an order of magnitude.","Contour plots of SNR over the parameter spaces are also presented to indicate regions susceptible to this discrepancy.","Our results suggest that DA and DB type extragalactic double white dwarfs may yield an SGWB with SNR surpassing 100 after 1 year of operation in the weak-signal-limit scenario, with a redshift-independent merger rate of about $500\\,\\,{\\rm Mpc^{-3}\\,Myr^{-1}}$.","In fact, this value falls significantly below the necessary threshold.","Similar influences arise for first-order phase transitions, yet pinning down parameter regions remains formidable due to model uncertainties."],"url":"http://arxiv.org/abs/2403.18709v1","category":"gr-qc"}
{"created":"2024-03-27 15:56:35","title":"Connections between Reachability and Time Optimality","abstract":"This paper presents the concept of an equivalence relation between the set of optimal control problems. By leveraging this concept, we show that the boundary of the reachability set can be constructed by the solutions of time optimal problems. Alongside, a more generalized equivalence theorem is presented together. The findings facilitate the use of solution structures from a certain class of optimal control problems to address problems in corresponding equivalent classes. As a byproduct, we state and prove the construction methods of the reachability sets of three-dimensional curves with prescribed curvature bound. The findings are twofold: Firstly, we prove that any boundary point of the reachability set, with the terminal direction taken into account, can be accessed via curves of H, CSC, CCC, or their respective subsegments, where H denotes a helicoidal arc, C a circular arc with maximum curvature, and S a straight segment. Secondly, we show that any boundary point of the reachability set, without considering the terminal direction, can be accessed by curves of CC, CS, or their respective subsegments. These findings extend the developments presented in literature regarding planar curves, or Dubins car dynamics, into spatial curves in $\\mathbb{R}^3$. For higher dimensions, we confirm that the problem of identifying the reachability set of curvature bounded paths subsumes the well-known Markov-Dubins problem. These advancements in understanding the reachability of curvature bounded paths in $\\mathbb{R}^3$ hold significant practical implications, particularly in the contexts of mission planning problems and time optimal guidance.","sentences":["This paper presents the concept of an equivalence relation between the set of optimal control problems.","By leveraging this concept, we show that the boundary of the reachability set can be constructed by the solutions of time optimal problems.","Alongside, a more generalized equivalence theorem is presented together.","The findings facilitate the use of solution structures from a certain class of optimal control problems to address problems in corresponding equivalent classes.","As a byproduct, we state and prove the construction methods of the reachability sets of three-dimensional curves with prescribed curvature bound.","The findings are twofold: Firstly, we prove that any boundary point of the reachability set, with the terminal direction taken into account, can be accessed via curves of H, CSC, CCC, or their respective subsegments, where H denotes a helicoidal arc, C a circular arc with maximum curvature, and S a straight segment.","Secondly, we show that any boundary point of the reachability set, without considering the terminal direction, can be accessed by curves of CC, CS, or their respective subsegments.","These findings extend the developments presented in literature regarding planar curves, or Dubins car dynamics, into spatial curves in $\\mathbb{R}^3$. For higher dimensions, we confirm that the problem of identifying the reachability set of curvature bounded paths subsumes the well-known Markov-Dubins problem.","These advancements in understanding the reachability of curvature bounded paths in $\\mathbb{R}^3$ hold significant practical implications, particularly in the contexts of mission planning problems and time optimal guidance."],"url":"http://arxiv.org/abs/2403.18707v1","category":"math.OC"}
{"created":"2024-03-27 15:54:55","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching","abstract":"In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.","sentences":["In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation.","While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance.","In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors.","Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way.","We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs.","Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance.","Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation."],"url":"http://arxiv.org/abs/2403.18705v1","category":"cs.LG"}
{"created":"2024-03-27 15:48:16","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","abstract":"This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This \"over-fusion\" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding while simultaneously ensuring their aggregation into dense, well-defined clusters. Our method demonstrates remarkable improvements with just a fraction of the conventional label requirements, as evidenced by our results on CIFAR10 and CIFAR100 datasets.","sentences":["This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives.","We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point.","This \"over-fusion\" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks.","Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE.","In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase.","The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding while simultaneously ensuring their aggregation into dense, well-defined clusters.","Our method demonstrates remarkable improvements with just a fraction of the conventional label requirements, as evidenced by our results on CIFAR10 and CIFAR100 datasets."],"url":"http://arxiv.org/abs/2403.18699v1","category":"cs.LG"}
{"created":"2024-03-27 15:47:32","title":"Submanifold projections and hyperbolicity in ${\\rm Out}(F_n)$","abstract":"The free splitting graph of a free group $F_n$ with $n\\geq 2$ generators is a hyperbolic ${\\rm Out}(F_n)$-graph which has a geometric realization as a sphere graph in the connected sum of $n$ copies of $S^1\\times S^2$. We use this realization to construct submanifold projections of the free splitting graph into the free splitting graphs of proper free factors. This is used to construct for $n\\geq 3$ a new hyperbolic ${\\rm Out}(F_n)$-graph. If $n=3$, then every exponentially growing element acts on this graph with positive translation length.","sentences":["The free splitting graph of a free group $F_n$ with $n\\geq 2$ generators is a hyperbolic ${\\rm Out}(F_n)$-graph which has a geometric realization as a sphere graph in the connected sum of $n$ copies of $S^1\\times S^2$.","We use this realization to construct submanifold projections of the free splitting graph into the free splitting graphs of proper free factors.","This is used to construct for $n\\geq 3$ a new hyperbolic ${\\rm Out}(F_n)$-graph.","If $n=3$, then every exponentially growing element acts on this graph with positive translation length."],"url":"http://arxiv.org/abs/2403.18698v1","category":"math.GT"}
{"created":"2024-03-27 15:46:01","title":"Revisit the heavy quarkonium double-gluon hybrid mesons with $J^{PC}=1^{-+}$ and $2^{+-}$","abstract":"We revisit the masses of heavy quarkonium double-gluon hybrid mesons with $J^{PC}=1^{-+}$ and $2^{+-}$ in the framework of the QCD sum rules. Considering the $\\bar QGGQ$ operators in the $(\\mathbf{8}_{[\\bar{Q}Q]} \\otimes \\mathbf{8}_{[GG]})$ color structure, we have constructed two independent interpolating currents with $J^{PC}=1^{-+}$ and five independent currents with $J^{PC}=2^{+-}$, in which the glueball operators can be symmetric $d^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$ or antisymmetric $f^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$. For the interpolating currents with antisymmetric glueball operator $f^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$, there exist non-local divergences in one kind of additional Feynman diagrams of tri-gluon condensate, which will give important contributions to the sum rule stabilities and mass predictions. We use the diagrammatic operator renormalization to cancel out such divergences. At the leading order of $\\alpha_s$, the two-point correlation functions and spectral densities can be expressed in the analytic form of the generalized hypergeometric functions and Meijer's G-functions. After performing the numerical analysis, we predict the masses of the $1^{-+}$ and $2^{+-}$ charmonium double-gluon hybrid mesons to be around $6.1-7.2$ GeV and $6.3-6.4$ GeV, respectively. For the bottomonium systems, their masses are predicted to be $13.7-14.3$ GeV and $12.6-13.3$ GeV for the $1^{-+}$ and $2^{+-}$ channels, respectively. Besides, it is possible to hunt for these charmonium $\\bar{c}GGc$ hybrids in the radiative decays of $\\Upsilon (nS)$ and $\\chi_{bJ}$ mesons in BelleII experiment. Further investigations on these hybrid states in various theoretical and phenomenological methods are also anticipated in the future.","sentences":["We revisit the masses of heavy quarkonium double-gluon hybrid mesons with $J^{PC}=1^{-+}$ and $2^{+-}$ in the framework of the QCD sum rules.","Considering the $\\bar QGGQ$ operators in the $(\\mathbf{8}_{[\\bar{Q}Q]} \\otimes \\mathbf{8}_{[GG]})$ color structure, we have constructed two independent interpolating currents with $J^{PC}=1^{-+}$ and five independent currents with $J^{PC}=2^{+-}$, in which the glueball operators can be symmetric $d^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$ or antisymmetric $f^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$. For the interpolating currents with antisymmetric glueball operator $f^{rst}G^{s}_{\\mu\\nu}G^{t}_{\\alpha\\beta}$, there exist non-local divergences in one kind of additional Feynman diagrams of tri-gluon condensate, which will give important contributions to the sum rule stabilities and mass predictions.","We use the diagrammatic operator renormalization to cancel out such divergences.","At the leading order of $\\alpha_s$, the two-point correlation functions and spectral densities can be expressed in the analytic form of the generalized hypergeometric functions and Meijer's G-functions.","After performing the numerical analysis, we predict the masses of the $1^{-+}$ and $2^{+-}$ charmonium double-gluon hybrid mesons to be around $6.1-7.2$ GeV and $6.3-6.4$ GeV, respectively.","For the bottomonium systems, their masses are predicted to be $13.7-14.3$ GeV and $12.6-13.3$ GeV for the $1^{-+}$ and $2^{+-}$ channels, respectively.","Besides, it is possible to hunt for these charmonium $\\bar{c}GGc$ hybrids in the radiative decays of $\\Upsilon (nS)$ and $\\chi_{bJ}$ mesons in BelleII experiment.","Further investigations on these hybrid states in various theoretical and phenomenological methods are also anticipated in the future."],"url":"http://arxiv.org/abs/2403.18696v1","category":"hep-ph"}
{"created":"2024-03-27 15:44:25","title":"An Efficient Risk-aware Branch MPC for Automated Driving that is Robust to Uncertain Vehicle Behaviors","abstract":"One of the critical challenges in automated driving is ensuring safety of automated vehicles despite the unknown behavior of the other vehicles. Although motion prediction modules are able to generate a probability distribution associated with various behavior modes, their probabilistic estimates are often inaccurate, thus leading to a possibly unsafe trajectory. To overcome this challenge, we propose a risk-aware motion planning framework that appropriately accounts for the ambiguity in the estimated probability distribution. We formulate the risk-aware motion planning problem as a min-max optimization problem and develop an efficient iterative method by incorporating a regularization term in the probability update step. Via extensive numerical studies, we validate the convergence of our method and demonstrate its advantages compared to the state-of-the-art approaches.","sentences":["One of the critical challenges in automated driving is ensuring safety of automated vehicles despite the unknown behavior of the other vehicles.","Although motion prediction modules are able to generate a probability distribution associated with various behavior modes, their probabilistic estimates are often inaccurate, thus leading to a possibly unsafe trajectory.","To overcome this challenge, we propose a risk-aware motion planning framework that appropriately accounts for the ambiguity in the estimated probability distribution.","We formulate the risk-aware motion planning problem as a min-max optimization problem and develop an efficient iterative method by incorporating a regularization term in the probability update step.","Via extensive numerical studies, we validate the convergence of our method and demonstrate its advantages compared to the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.18695v1","category":"eess.SY"}
{"created":"2024-03-27 15:41:23","title":"Annolid: Annotate, Segment, and Track Anything You Need","abstract":"Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts.","sentences":["Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis.","Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another.","Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation.","Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts."],"url":"http://arxiv.org/abs/2403.18690v1","category":"cs.CV"}
{"created":"2024-03-27 15:34:46","title":"The Gross--Kohnen--Zagier theorem via $p$-adic uniformization","abstract":"This article gives a new proof of the Gross--Kohnen--Zagier theorem for Shimura curves which exploits the $p$-adic uniformization of Cerednik--Drinfeld. The explicit description of CM points via this uniformization leads to an expression relating the Gross--Kohnen--Zagier generating series to the ordinary projection of the first derivative, with respect to a weight variable, of a $p$-adic family of positive definite ternary theta series.","sentences":["This article gives a new proof of the Gross--Kohnen--Zagier theorem for Shimura curves which exploits the $p$-adic uniformization of Cerednik--Drinfeld.","The explicit description of CM points via this uniformization leads to an expression relating the Gross--Kohnen--Zagier generating series to the ordinary projection of the first derivative, with respect to a weight variable, of a $p$-adic family of positive definite ternary theta series."],"url":"http://arxiv.org/abs/2403.18688v1","category":"math.NT"}
{"created":"2024-03-27 15:34:27","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","abstract":"Neural networks were used to classify infrasound data. Two different approaches were compared. One based on the direct classification of time series data, using a custom implementation of the InceptionTime network. For the other approach, we generated 2D images of the wavelet transformation of the signals, which were subsequently classified using a ResNet implementation. Choosing appropriate hyperparameter settings, both achieve a classification accuracy of above 90 %, with the direct approach reaching 95.2 %.","sentences":["Neural networks were used to classify infrasound data.","Two different approaches were compared.","One based on the direct classification of time series data, using a custom implementation of the InceptionTime network.","For the other approach, we generated 2D images of the wavelet transformation of the signals, which were subsequently classified using a ResNet implementation.","Choosing appropriate hyperparameter settings, both achieve a classification accuracy of above 90 %, with the direct approach reaching 95.2 %."],"url":"http://arxiv.org/abs/2403.18687v1","category":"cs.LG"}
{"created":"2024-03-27 15:29:08","title":"Representatividad Muestral en la Incertidumbre Sim\u00e9trica Multivariada para la Selecci\u00f3n de Atributos","abstract":"In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   --   En el presente trabajo hemos analizado el comportamiento de una versi\\'on multivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de simulaci\\'on estad\\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria. Los experimentos muestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no muestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on de resultados hemos propuesto una condici\\'on que preserva una buena calidad en el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on de dimensionalidad.","sentences":["In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features.","Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU.","In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   ","--   En el presente trabajo hemos analizado el comportamiento de una versi\\'on multivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de simulaci\\'on estad\\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria.","Los experimentos muestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no muestral afectan al MSU como medida.","En esta tesis, mediante la observaci\\'on de resultados hemos propuesto una condici\\'on que preserva una buena calidad en el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on de dimensionalidad."],"url":"http://arxiv.org/abs/2403.18685v1","category":"cs.IT"}
{"created":"2024-03-27 15:27:36","title":"Scaling Laws For Dense Retrieval","abstract":"Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amounts of annotated data. Results indicate that, under our settings, the performance of dense retrieval models follows a precise power-law scaling related to the model size and the number of annotations. Additionally, we examine scaling with prevalent data augmentation methods to assess the impact of annotation quality, and apply the scaling law to find the best resource allocation strategy under a budget constraint. We believe that these insights will significantly contribute to understanding the scaling effect of dense retrieval models and offer meaningful guidance for future research endeavors.","sentences":["Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation.","Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size.","This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive.","Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks.","In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models.","We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amounts of annotated data.","Results indicate that, under our settings, the performance of dense retrieval models follows a precise power-law scaling related to the model size and the number of annotations.","Additionally, we examine scaling with prevalent data augmentation methods to assess the impact of annotation quality, and apply the scaling law to find the best resource allocation strategy under a budget constraint.","We believe that these insights will significantly contribute to understanding the scaling effect of dense retrieval models and offer meaningful guidance for future research endeavors."],"url":"http://arxiv.org/abs/2403.18684v1","category":"cs.IR"}
{"created":"2024-03-27 15:26:09","title":"JumpBackHash: Say Goodbye to the Modulo Operation to Distribute Keys Uniformly to Buckets","abstract":"The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage. A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets. Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests. Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries. This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator. Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability. A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library.","sentences":["The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage.","A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets.","Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests.","Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries.","This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator.","Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability.","A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library."],"url":"http://arxiv.org/abs/2403.18682v1","category":"cs.DS"}
{"created":"2024-03-27 15:24:54","title":"TransFusion: Contrastive Learning with Transformers","abstract":"This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy in downstream tasks.","sentences":["This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable.","TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output.","The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes.","The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning.","Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy in downstream tasks."],"url":"http://arxiv.org/abs/2403.18681v1","category":"cs.LG"}
{"created":"2024-03-27 15:21:58","title":"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project","abstract":"Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice. Software engineering educators must teach future software engineers how to use such tools well. As of yet, there have been few studies that report on the use of LLMs in the classroom. It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms.   Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project.   Design/Method: We collected data from a senior-level software engineering course at Purdue University. This course uses a project-based learning (PBL) design. The students used LLMs such as ChatGPT and Copilot in their projects. A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester. We analyzed the data to identify themes related to students' usage patterns and learning outcomes.   Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications. In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks. However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes. Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes.","sentences":["Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice.","Software engineering educators must teach future software engineers how to use such tools well.","As of yet, there have been few studies that report on the use of LLMs in the classroom.","It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms.   ","Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project.   ","Design/Method: We collected data from a senior-level software engineering course at Purdue University.","This course uses a project-based learning (PBL) design.","The students used LLMs such as ChatGPT and Copilot in their projects.","A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester.","We analyzed the data to identify themes related to students' usage patterns and learning outcomes.   ","Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications.","In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks.","However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes.","Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes."],"url":"http://arxiv.org/abs/2403.18679v1","category":"cs.SE"}
{"created":"2024-03-27 15:20:38","title":"Mollow-like triplets in ultra-fast resonant absorption","abstract":"We show that resonant absorption of smooth laser fields can yield Mollow-like triplet patterns. General conditions for such triplets are derived and illustrated with a super-Gaussian pulse sequence. Gaussian pulses can not exhibit triplets, super-Gaussian pulses can form triplets depending on the pulse area and flat-top pulses can produce absorption triplets after one Rabi cycle. Our results are compared side-by-side with resonance fluorescence to emphasize similarities and differences between these unlike observables. In the high-intensity limit, we show that the central absorption peak is asymmetric, which we attribute to non-linear photoionization, beyond two-level atomic physics.","sentences":["We show that resonant absorption of smooth laser fields can yield Mollow-like triplet patterns.","General conditions for such triplets are derived and illustrated with a super-Gaussian pulse sequence.","Gaussian pulses can not exhibit triplets, super-Gaussian pulses can form triplets depending on the pulse area and flat-top pulses can produce absorption triplets after one Rabi cycle.","Our results are compared side-by-side with resonance fluorescence to emphasize similarities and differences between these unlike observables.","In the high-intensity limit, we show that the central absorption peak is asymmetric, which we attribute to non-linear photoionization, beyond two-level atomic physics."],"url":"http://arxiv.org/abs/2403.18676v1","category":"quant-ph"}
{"created":"2024-03-27 15:17:10","title":"Deep Learning for Robust and Explainable Models in Computer Vision","abstract":"Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing. These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry. However, as more and more models are deployed and used in practice, many challenges have emerged. This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice.   Robustness and reliability are the critical components of any model before certification and deployment in practice. Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature. In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori.   This thesis presents developments in computer vision models' robustness and explainability. Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research. Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness. In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing.","sentences":["Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing.","These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry.","However, as more and more models are deployed and used in practice, many challenges have emerged.","This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice.   ","Robustness and reliability are the critical components of any model before certification and deployment in practice.","Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature.","In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori.   ","This thesis presents developments in computer vision models' robustness and explainability.","Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research.","Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness.","In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing."],"url":"http://arxiv.org/abs/2403.18674v1","category":"cs.CV"}
{"created":"2024-03-27 15:17:05","title":"Unveiling the inconsistency of the Proca theory with non-minimal coupling to gravity","abstract":"We study the degrees of freedom of the Proca theory, non-minimally coupled to gravity. In the Minkowski background, this theory propagates five degrees of freedom - a massive longitudinal mode, two massive vector ones, and two massless tensor modes. At first sight, the non-linear coupling between the metric perturbations and the vector field indicates that both longitudinal and tensor modes become strongly coupled, at the same scale. This would imply that no matter how small the photon mass is if non-minimal coupling is taken into account, gravitational waves would necessarily be strongly coupled. We show that the way out of this inconsistency is through the introduction of the disformal coupling to the metric perturbations that resemble the vector-type disformal transformations. This way, the unphysical coupling between the two types of modes can be avoided, rendering the model consistent. As a result, we show that only the longitudinal modes enter a strong coupling regime, while both tensor and transverse modes remain weakly coupled at all scales up to the Planck length. Finally, we show how this cures recently reported runaway modes in curved spacetime.","sentences":["We study the degrees of freedom of the Proca theory, non-minimally coupled to gravity.","In the Minkowski background, this theory propagates five degrees of freedom - a massive longitudinal mode, two massive vector ones, and two massless tensor modes.","At first sight, the non-linear coupling between the metric perturbations and the vector field indicates that both longitudinal and tensor modes become strongly coupled, at the same scale.","This would imply that no matter how small the photon mass is if non-minimal coupling is taken into account, gravitational waves would necessarily be strongly coupled.","We show that the way out of this inconsistency is through the introduction of the disformal coupling to the metric perturbations that resemble the vector-type disformal transformations.","This way, the unphysical coupling between the two types of modes can be avoided, rendering the model consistent.","As a result, we show that only the longitudinal modes enter a strong coupling regime, while both tensor and transverse modes remain weakly coupled at all scales up to the Planck length.","Finally, we show how this cures recently reported runaway modes in curved spacetime."],"url":"http://arxiv.org/abs/2403.18673v1","category":"gr-qc"}
{"created":"2024-03-27 15:15:14","title":"Fact Checking Beyond Training Set","abstract":"Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.","sentences":["Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise.","We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain.","Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem.","We propose an adversarial algorithm to make the retriever component robust against distribution shift.","Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data.","We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents.","Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift.","To our knowledge, there is no publicly available multi-topic fact checking dataset.","Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets.","We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data."],"url":"http://arxiv.org/abs/2403.18671v1","category":"cs.CL"}
{"created":"2024-03-27 15:11:07","title":"Aiming for Relevance","abstract":"Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care. 10 pages, 9 figures.","sentences":["Vital signs are crucial in intensive care units (ICUs).","They are used to track the patient's state and to identify clinically significant changes.","Predicting vital sign trajectories is valuable for early detection of adverse events.","However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions.","We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations.","These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians.","We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU).","Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events.","This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care.","10 pages, 9 figures."],"url":"http://arxiv.org/abs/2403.18668v1","category":"cs.LG"}
{"created":"2024-03-27 15:09:46","title":"FluxGAT: Integrating Flux Sampling with Graph Neural Networks for Unbiased Gene Essentiality Classification","abstract":"Gene essentiality, the necessity of a specific gene for the survival of an organism, is crucial to our understanding of cellular processes and identifying drug targets. Experimental determination of gene essentiality requires large growth screens that are time-consuming and expensive, motivating the development of in-silico approaches. Existing methods predominantly utilise flux balance analysis (FBA), a constraint-based optimisation algorithm; however, they are fundamentally limited by the necessity of a predefined cellular objective function. This requirement introduces an element of observer bias, as the objective function often reflects the researcher's assumptions rather than the cell's biological goals. Here, we present FluxGAT, a graph neural network (GNN) model capable of predicting gene essentiality directly from graphical representations of flux sampling data. Flux sampling removes the need for objective functions, thereby eliminating observer bias. FluxGAT leverages the unique strengths of GNNs in learning representations of complex relationships within metabolic reaction networks. The success of our approach in predicting experimentally determined gene essentiality, with almost double the sensitivity of FBA, explores the possibility of predicting cellular phenotypes in cases when objectives are less understood. Thus, we demonstrate a method for more general gene essentiality predictions across a broader spectrum of biological systems and environments.","sentences":["Gene essentiality, the necessity of a specific gene for the survival of an organism, is crucial to our understanding of cellular processes and identifying drug targets.","Experimental determination of gene essentiality requires large growth screens that are time-consuming and expensive, motivating the development of in-silico approaches.","Existing methods predominantly utilise flux balance analysis (FBA), a constraint-based optimisation algorithm; however, they are fundamentally limited by the necessity of a predefined cellular objective function.","This requirement introduces an element of observer bias, as the objective function often reflects the researcher's assumptions rather than the cell's biological goals.","Here, we present FluxGAT, a graph neural network (GNN) model capable of predicting gene essentiality directly from graphical representations of flux sampling data.","Flux sampling removes the need for objective functions, thereby eliminating observer bias.","FluxGAT leverages the unique strengths of GNNs in learning representations of complex relationships within metabolic reaction networks.","The success of our approach in predicting experimentally determined gene essentiality, with almost double the sensitivity of FBA, explores the possibility of predicting cellular phenotypes in cases when objectives are less understood.","Thus, we demonstrate a method for more general gene essentiality predictions across a broader spectrum of biological systems and environments."],"url":"http://arxiv.org/abs/2403.18666v1","category":"q-bio.QM"}
{"created":"2024-03-27 15:08:19","title":"Lipschitz-type estimate for the frog model with Bernoulli initial configuration","abstract":"We consider the frog model with Bernoulli initial configuration, which is an interacting particle system on the multidimensional lattice consisting of two states of particles: active and sleeping. Active particles perform independent simple random walks. On the other hand, although sleeping particles do not move at first, they become active and can move around when touched by active particles. Initially, only the origin has one active particle, and the other sites have sleeping particles according to a Bernoulli distribution. Then, starting from the original active particle, active ones are gradually generated and propagate across the lattice, with time. It is of interest to know how the propagation of active particles behaves as the parameter of the Bernoulli distribution varies. In this paper, we treat the so-called time constant describing the speed of propagation, and prove that the absolute difference between the time constants for parameters $p,q \\in (0,1]$ is bounded from above and below by multiples of $|p-q|$.","sentences":["We consider the frog model with Bernoulli initial configuration, which is an interacting particle system on the multidimensional lattice consisting of two states of particles: active and sleeping.","Active particles perform independent simple random walks.","On the other hand, although sleeping particles do not move at first, they become active and can move around when touched by active particles.","Initially, only the origin has one active particle, and the other sites have sleeping particles according to a Bernoulli distribution.","Then, starting from the original active particle, active ones are gradually generated and propagate across the lattice, with time.","It is of interest to know how the propagation of active particles behaves as the parameter of the Bernoulli distribution varies.","In this paper, we treat the so-called time constant describing the speed of propagation, and prove that the absolute difference between the time constants for parameters $p,q \\in (0,1]$ is bounded from above and below by multiples of $|p-q|$."],"url":"http://arxiv.org/abs/2403.18665v1","category":"math.PR"}
{"created":"2024-03-27 15:05:55","title":"Benchmarking Quantum Generative Learning: A Study on Scalability and Noise Resilience using QUARK","abstract":"Quantum computing promises a disruptive impact on machine learning algorithms, taking advantage of the exponentially large Hilbert space available. However, it is not clear how to scale quantum machine learning (QML) to industrial-level applications. This paper investigates the scalability and noise resilience of quantum generative learning applications. We consider the training performance in the presence of statistical noise due to finite-shot noise statistics and quantum noise due to decoherence to analyze the scalability of QML methods. We employ rigorous benchmarking techniques to track progress and identify challenges in scaling QML algorithms, and show how characterization of QML systems can be accelerated, simplified, and made reproducible when the QUARK framework is used. We show that QGANs are not as affected by the curse of dimensionality as QCBMs and to which extent QCBMs are resilient to noise.","sentences":["Quantum computing promises a disruptive impact on machine learning algorithms, taking advantage of the exponentially large Hilbert space available.","However, it is not clear how to scale quantum machine learning (QML) to industrial-level applications.","This paper investigates the scalability and noise resilience of quantum generative learning applications.","We consider the training performance in the presence of statistical noise due to finite-shot noise statistics and quantum noise due to decoherence to analyze the scalability of QML methods.","We employ rigorous benchmarking techniques to track progress and identify challenges in scaling QML algorithms, and show how characterization of QML systems can be accelerated, simplified, and made reproducible when the QUARK framework is used.","We show that QGANs are not as affected by the curse of dimensionality as QCBMs and to which extent QCBMs are resilient to noise."],"url":"http://arxiv.org/abs/2403.18662v1","category":"quant-ph"}
{"created":"2024-03-27 15:04:15","title":"A machine-learning pipeline for real-time detection of gravitational waves from compact binary coalescences","abstract":"The promise of multi-messenger astronomy relies on the rapid detection of gravitational waves at very low latencies ($\\mathcal{O}$(1\\,s)) in order to maximize the amount of time available for follow-up observations. In recent years, neural-networks have demonstrated robust non-linear modeling capabilities and millisecond-scale inference at a comparatively small computational footprint, making them an attractive family of algorithms in this context. However, integration of these algorithms into the gravitational-wave astrophysics research ecosystem has proven non-trivial. Here, we present the first fully machine learning-based pipeline for the detection of gravitational waves from compact binary coalescences (CBCs) running in low-latency. We demonstrate this pipeline to have a fraction of the latency of traditional matched filtering search pipelines while achieving state-of-the-art sensitivity to higher-mass stellar binary black holes.","sentences":["The promise of multi-messenger astronomy relies on the rapid detection of gravitational waves at very low latencies ($\\mathcal{O}$(1\\,s)) in order to maximize the amount of time available for follow-up observations.","In recent years, neural-networks have demonstrated robust non-linear modeling capabilities and millisecond-scale inference at a comparatively small computational footprint, making them an attractive family of algorithms in this context.","However, integration of these algorithms into the gravitational-wave astrophysics research ecosystem has proven non-trivial.","Here, we present the first fully machine learning-based pipeline for the detection of gravitational waves from compact binary coalescences (CBCs) running in low-latency.","We demonstrate this pipeline to have a fraction of the latency of traditional matched filtering search pipelines while achieving state-of-the-art sensitivity to higher-mass stellar binary black holes."],"url":"http://arxiv.org/abs/2403.18661v1","category":"gr-qc"}
{"created":"2024-03-27 15:03:38","title":"InstructBrush: Learning Attention-based Instruction Optimization for Image Editing","abstract":"In recent years, instruction-based image editing methods have garnered significant attention in image editing. However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging to accurately describe through language. We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap. It extracts editing effects from exemplar image pairs as editing instructions, which are further applied for image editing. Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization. To explore the ability of instruction inversion methods to guide image editing in open scenarios, we establish a TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set of scenes and editing types. The creation of this benchmark paves the way for further exploration of instruction inversion. Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects.","sentences":["In recent years, instruction-based image editing methods have garnered significant attention in image editing.","However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging to accurately describe through language.","We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap.","It extracts editing effects from exemplar image pairs as editing instructions, which are further applied for image editing.","Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization.","To explore the ability of instruction inversion methods to guide image editing in open scenarios, we establish a TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set of scenes and editing types.","The creation of this benchmark paves the way for further exploration of instruction inversion.","Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects."],"url":"http://arxiv.org/abs/2403.18660v1","category":"cs.GR"}
{"created":"2024-03-27 15:03:33","title":"INEXA: Interactive and Explainable Process Model Abstraction Through Object-Centric Process Mining","abstract":"Process events are recorded by multiple information systems at different granularity levels. Based on the resulting event logs, process models are discovered at different granularity levels, as well. Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements. The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs. Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log. Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level. To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process model abstraction method that keeps the link to the event log. As a starting point, INEXA aggregates large process models to a \"displayable\" size, e.g., for the manufacturing use case to a process model with 58 model elements. Then, the process analyst can explore granularity levels interactively, while applied abstractions are automatically traced in the event log for explainability.","sentences":["Process events are recorded by multiple information systems at different granularity levels.","Based on the resulting event logs, process models are discovered at different granularity levels, as well.","Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements.","The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs.","Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log.","Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level.","To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process model abstraction method that keeps the link to the event log.","As a starting point, INEXA aggregates large process models to a \"displayable\" size, e.g., for the manufacturing use case to a process model with 58 model elements.","Then, the process analyst can explore granularity levels interactively, while applied abstractions are automatically traced in the event log for explainability."],"url":"http://arxiv.org/abs/2403.18659v1","category":"cs.AI"}
{"created":"2024-03-27 15:03:29","title":"Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator","abstract":"This work analyzes the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. It assumes a weak inlier-outlier model and allows the fraction of inliers to be smaller than a fraction that leads to computational hardness of the robust subspace recovery problem. It shows that in this setting, if the initialization of STE, which is an iterative algorithm, satisfies a certain condition, then STE can effectively recover the underlying subspace. It further shows that under the generalized haystack model, STE initialized by the Tyler's M-estimator (TME), can recover the subspace when the fraction of iniliers is too small for TME to handle.","sentences":["This work analyzes the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers.","It assumes a weak inlier-outlier model and allows the fraction of inliers to be smaller than a fraction that leads to computational hardness of the robust subspace recovery problem.","It shows that in this setting, if the initialization of STE, which is an iterative algorithm, satisfies a certain condition, then STE can effectively recover the underlying subspace.","It further shows that under the generalized haystack model, STE initialized by the Tyler's M-estimator (TME), can recover the subspace when the fraction of iniliers is too small for TME to handle."],"url":"http://arxiv.org/abs/2403.18658v1","category":"math.ST"}
{"created":"2024-03-27 15:02:31","title":"Late-forming black holes and the antiproton, gamma-ray, and anti-helium excesses","abstract":"Black holes long-lived enough to be the dark matter have temperatures below the MeV. Since Hawking evaporation is a quasi-thermal process, no GeV emission is predicted to be produced by black holes if they are part, or all, of the cosmological dark matter. However, black holes could be ``spawned'' at late times with masses that correspond to short lifetimes, and as such be significantly hotter and produce particles well in excess of the GeV. Here, we show that such late-forming black holes could, at once, explain the tantalizing excesses found in the gamma radiation from the Galactic center, in the flux of cosmic-ray antiproton, and in the few tentative antihelium events reported by the anti-matter spectrometer AMS-02. We compute accurate predictions for the anti-deuteron, high-energy neutrino, and positron fluxes if this scenario is realized in nature. We find that while the neutrino and positron fluxes are too small compared to the expected background, a significant number of anti-deuteron events is expected both at AMS-02 and at the future General AntiParticle Spectrometer (GAPS).","sentences":["Black holes long-lived enough to be the dark matter have temperatures below the MeV.","Since Hawking evaporation is a quasi-thermal process, no GeV emission is predicted to be produced by black holes if they are part, or all, of the cosmological dark matter.","However, black holes could be ``spawned'' at late times with masses that correspond to short lifetimes, and as such be significantly hotter and produce particles well in excess of the GeV. Here, we show that such late-forming black holes could, at once, explain the tantalizing excesses found in the gamma radiation from the Galactic center, in the flux of cosmic-ray antiproton, and in the few tentative antihelium events reported by the anti-matter spectrometer AMS-02.","We compute accurate predictions for the anti-deuteron, high-energy neutrino, and positron fluxes if this scenario is realized in nature.","We find that while the neutrino and positron fluxes are too small compared to the expected background, a significant number of anti-deuteron events is expected both at AMS-02 and at the future General AntiParticle Spectrometer (GAPS)."],"url":"http://arxiv.org/abs/2403.18656v1","category":"astro-ph.CO"}
{"created":"2024-03-27 15:01:51","title":"A Dimca-Greuel type inequality for foliations","abstract":"Let $\\mathcal{F}$ be a holomorphic foliation at $p\\in \\mathbb{C}^2$, and $B$ be a separatrix of $\\mathcal{F}$. We prove the following Dimca-Greuel type inequality $\\frac{\\mu_p(\\mathcal{F},B)}{\\tau_p(\\mathcal{F},B)}<4/3$, where $\\mu_p(\\mathcal{F},B)$ is the multiplicity of $\\mathcal{F}$ along $B$ and $\\tau_p(\\mathcal{F},B)$ is the dimension of the quotient of $\\mathbb{C}[[x,y]]$ by the ideal generated by the components of any $1$-form defining $\\mathcal{F}$ and any equation of $B$. As a consequence, we provide a new proof of the $\\frac{4}{3}$-Dimca-Greuel's conjecture for singularities of irreducible plane curve germs, with foliations ingredients, that differs from those given by Alberich-Carrami\\~nana, Almir\\'on, Blanco, Melle-Hern\\'andez and Genzmer-Hernandes but it is in line with the idea developed by Wang.","sentences":["Let $\\mathcal{F}$ be a holomorphic foliation at $p\\in \\mathbb{C}^2$, and $B$ be a separatrix of $\\mathcal{F}$. We prove the following Dimca-Greuel type inequality $\\frac{\\mu_p(\\mathcal{F},B)}{\\tau_p(\\mathcal{F},B)}<4/3$, where $\\mu_p(\\mathcal{F},B)$ is the multiplicity of $\\mathcal{F}$ along $B$ and $\\tau_p(\\mathcal{F},B)$ is the dimension of the quotient of $\\mathbb{C}[[x,y]]$ by the ideal generated by the components of any $1$-form defining $\\mathcal{F}$ and any equation of $B$. As a consequence, we provide a new proof of the $\\frac{4}{3}$-Dimca-Greuel's conjecture for singularities of irreducible plane curve germs, with foliations ingredients, that differs from those given by Alberich-Carrami\\~nana, Almir\\'on, Blanco, Melle-Hern\\'andez and Genzmer-Hernandes but it is in line with the idea developed by Wang."],"url":"http://arxiv.org/abs/2403.18654v1","category":"math.CV"}
{"created":"2024-03-27 14:58:11","title":"MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections","abstract":"The paper focuses on the design of a control strategy for safety-critical remote teleoperation. The main goal is to make the controlled system track the desired velocity specified by an operator while avoiding obstacles despite communication delays. Control Barrier Functions (CBFs) are used to define the safety constraints that the system has to respect to avoid obstacles, while Model Predictive Control (MPC) provides the framework for adjusting the desired input, taking the constraints into account. The resulting input is sent to the remote system, where appropriate low-level velocity controllers translate it into system-specific commands. The main novelty of the paper is a method to make the CBFs robust against the uncertainties caused by the network delays affecting the system's state and do so in a less conservative manner. The results show how the proposed method successfully solves the safety-critical teleoperation problem, making the controlled systems avoid obstacles with different types of network delay. The controller has also been tested in simulation and on a real manipulator, demonstrating its general applicability when reliable low-level velocity controllers are available.","sentences":["The paper focuses on the design of a control strategy for safety-critical remote teleoperation.","The main goal is to make the controlled system track the desired velocity specified by an operator while avoiding obstacles despite communication delays.","Control Barrier Functions (CBFs) are used to define the safety constraints that the system has to respect to avoid obstacles, while Model Predictive Control (MPC) provides the framework for adjusting the desired input, taking the constraints into account.","The resulting input is sent to the remote system, where appropriate low-level velocity controllers translate it into system-specific commands.","The main novelty of the paper is a method to make the CBFs robust against the uncertainties caused by the network delays affecting the system's state and do so in a less conservative manner.","The results show how the proposed method successfully solves the safety-critical teleoperation problem, making the controlled systems avoid obstacles with different types of network delay.","The controller has also been tested in simulation and on a real manipulator, demonstrating its general applicability when reliable low-level velocity controllers are available."],"url":"http://arxiv.org/abs/2403.18650v1","category":"cs.SY"}
{"created":"2024-03-27 14:54:27","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens","abstract":"We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.","sentences":["We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT).","The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy.","The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens.","2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead.","3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling.","Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively.","Please refer to https://github.com/hasuoshenyun/SDSAT."],"url":"http://arxiv.org/abs/2403.18647v1","category":"cs.CL"}
{"created":"2024-03-27 14:46:54","title":"Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks","abstract":"Existing approaches to trajectory planning for autonomous racing employ sampling-based methods, generating numerous jerk-optimal trajectories and selecting the most favorable feasible trajectory based on a cost function penalizing deviations from an offline-calculated racing line. While successful on oval tracks, these methods face limitations on complex circuits due to the simplistic geometry of jerk-optimal edges failing to capture the complexity of the racing line. Additionally, they only consider two-dimensional tracks, potentially neglecting or surpassing the actual dynamic potential. In this paper, we present a sampling-based local trajectory planning approach for autonomous racing that can maintain the lap time of the racing line even on complex race tracks and consider the race track's three-dimensional effects. In simulative experiments, we demonstrate that our approach achieves lower lap times and improved utilization of dynamic limits compared to existing approaches. We also investigate the impact of online racing line generation, in which the time-optimal solution is planned from the current vehicle state for a limited spatial horizon, in contrast to a closed racing line calculated offline. We show that combining the sampling-based planner with the online racing line generation can significantly reduce lap times in multi-vehicle scenarios.","sentences":["Existing approaches to trajectory planning for autonomous racing employ sampling-based methods, generating numerous jerk-optimal trajectories and selecting the most favorable feasible trajectory based on a cost function penalizing deviations from an offline-calculated racing line.","While successful on oval tracks, these methods face limitations on complex circuits due to the simplistic geometry of jerk-optimal edges failing to capture the complexity of the racing line.","Additionally, they only consider two-dimensional tracks, potentially neglecting or surpassing the actual dynamic potential.","In this paper, we present a sampling-based local trajectory planning approach for autonomous racing that can maintain the lap time of the racing line even on complex race tracks and consider the race track's three-dimensional effects.","In simulative experiments, we demonstrate that our approach achieves lower lap times and improved utilization of dynamic limits compared to existing approaches.","We also investigate the impact of online racing line generation, in which the time-optimal solution is planned from the current vehicle state for a limited spatial horizon, in contrast to a closed racing line calculated offline.","We show that combining the sampling-based planner with the online racing line generation can significantly reduce lap times in multi-vehicle scenarios."],"url":"http://arxiv.org/abs/2403.18643v1","category":"cs.RO"}
{"created":"2024-03-27 14:45:43","title":"Collective schedules: axioms and algorithms","abstract":"The collective schedules problem consists in computing a schedule of tasks shared between individuals. Tasks may have different duration, and individuals have preferences over the order of the shared tasks. This problem has numerous applications since tasks may model public infrastructure projects, events taking place in a shared room, or work done by co-workers. Our aim is, given the preferred schedules of individuals (voters), to return a consensus schedule. We propose an axiomatic study of the collective schedule problem, by using classic axioms in computational social choice and new axioms that take into account the duration of the tasks. We show that some axioms are incompatible, and we study the axioms fulfilled by three rules: one which has been studied in the seminal paper on collective schedules (Pascual et al. 2018), one which generalizes the Kemeny rule, and one which generalizes Spearman's footrule. From an algorithmic point of view, we show that these rules solve NP-hard problems, but that it is possible to solve optimally these problems for small but realistic size instances, and we give an efficient heuristic for large instances. We conclude this paper with experiments.","sentences":["The collective schedules problem consists in computing a schedule of tasks shared between individuals.","Tasks may have different duration, and individuals have preferences over the order of the shared tasks.","This problem has numerous applications since tasks may model public infrastructure projects, events taking place in a shared room, or work done by co-workers.","Our aim is, given the preferred schedules of individuals (voters), to return a consensus schedule.","We propose an axiomatic study of the collective schedule problem, by using classic axioms in computational social choice and new axioms that take into account the duration of the tasks.","We show that some axioms are incompatible, and we study the axioms fulfilled by three rules: one which has been studied in the seminal paper on collective schedules (Pascual et al. 2018), one which generalizes the Kemeny rule, and one which generalizes Spearman's footrule.","From an algorithmic point of view, we show that these rules solve NP-hard problems, but that it is possible to solve optimally these problems for small but realistic size instances, and we give an efficient heuristic for large instances.","We conclude this paper with experiments."],"url":"http://arxiv.org/abs/2403.18642v1","category":"cs.GT"}
{"created":"2024-03-27 14:41:39","title":"A Diffusion-Based Generative Equalizer for Music Restoration","abstract":"This paper presents a novel approach to audio restoration, focusing on the enhancement of low-quality music recordings, and in particular historical ones. Building upon a previous algorithm called BABE, or Blind Audio Bandwidth Extension, we introduce BABE-2, which presents a series of significant improvements. This research broadens the concept of bandwidth extension to \\emph{generative equalization}, a novel task that, to the best of our knowledge, has not been explicitly addressed in previous studies. BABE-2 is built around an optimization algorithm utilizing priors from diffusion models, which are trained or fine-tuned using a curated set of high-quality music tracks. The algorithm simultaneously performs two critical tasks: estimation of the filter degradation magnitude response and hallucination of the restored audio. The proposed method is objectively evaluated on historical piano recordings, showing a marked enhancement over the prior version. The method yields similarly impressive results in rejuvenating the works of renowned vocalists Enrico Caruso and Nellie Melba. This research represents an advancement in the practical restoration of historical music.","sentences":["This paper presents a novel approach to audio restoration, focusing on the enhancement of low-quality music recordings, and in particular historical ones.","Building upon a previous algorithm called BABE, or Blind Audio Bandwidth Extension, we introduce BABE-2, which presents a series of significant improvements.","This research broadens the concept of bandwidth extension to \\emph{generative equalization}, a novel task that, to the best of our knowledge, has not been explicitly addressed in previous studies.","BABE-2 is built around an optimization algorithm utilizing priors from diffusion models, which are trained or fine-tuned using a curated set of high-quality music tracks.","The algorithm simultaneously performs two critical tasks: estimation of the filter degradation magnitude response and hallucination of the restored audio.","The proposed method is objectively evaluated on historical piano recordings, showing a marked enhancement over the prior version.","The method yields similarly impressive results in rejuvenating the works of renowned vocalists Enrico Caruso and Nellie Melba.","This research represents an advancement in the practical restoration of historical music."],"url":"http://arxiv.org/abs/2403.18636v1","category":"eess.AS"}
{"created":"2024-03-27 14:39:46","title":"A Hilbert metric for bounded symmetric domains","abstract":"Bounded symmetric domains carry several natural invariant metrics, for example the Carath\\'eodory, Kobayashi or the Bergman metric. We define another natural metric, from generalized Hilbert metric defined in [FGW20], by considering the Borel embedding of the domain as an open subset of its dual compact Hermitian symmetric space and then its Harish-Chandra realization in projective spaces. We describe this construction on the four classical families of bounded symmetric domains and compute both this metric and its associated Finsler metric. We compare it to Carath\\'eodory and Bergman metrics and show that, except for the complex hyperbolic space, those metrics differ.","sentences":["Bounded symmetric domains carry several natural invariant metrics, for example the Carath\\'eodory, Kobayashi or the Bergman metric.","We define another natural metric, from generalized Hilbert metric defined in [FGW20], by considering the Borel embedding of the domain as an open subset of its dual compact Hermitian symmetric space and then its Harish-Chandra realization in projective spaces.","We describe this construction on the four classical families of bounded symmetric domains and compute both this metric and its associated Finsler metric.","We compare it to Carath\\'eodory and Bergman metrics and show that, except for the complex hyperbolic space, those metrics differ."],"url":"http://arxiv.org/abs/2403.18634v1","category":"math.DG"}
{"created":"2024-03-27 14:38:22","title":"Optimal Control Synthesis of Markov Decision Processes for Efficiency with Surveillance Tasks","abstract":"We investigate the problem of optimal control synthesis for Markov Decision Processes (MDPs), addressing both qualitative and quantitative objectives. Specifically, we require the system to fulfill a qualitative surveillance task in the sense that a specific region of interest can be visited infinitely often with probability one. Furthermore, to quantify the performance of the system, we consider the concept of efficiency, which is defined as the ratio between rewards and costs. This measure is more general than the standard long-run average reward metric as it aims to maximize the reward obtained per unit cost. Our objective is to synthesize a control policy that ensures the surveillance task while maximizes the efficiency. We provide an effective approach to synthesize a stationary control policy achieving $\\epsilon$-optimality by integrating state classifications of MDPs and perturbation analysis in a novel manner. Our results generalize existing works on efficiency-optimal control synthesis for MDP by incorporating qualitative surveillance tasks. A robot motion planning case study is provided to illustrate the proposed algorithm.","sentences":["We investigate the problem of optimal control synthesis for Markov Decision Processes (MDPs), addressing both qualitative and quantitative objectives.","Specifically, we require the system to fulfill a qualitative surveillance task in the sense that a specific region of interest can be visited infinitely often with probability one.","Furthermore, to quantify the performance of the system, we consider the concept of efficiency, which is defined as the ratio between rewards and costs.","This measure is more general than the standard long-run average reward metric as it aims to maximize the reward obtained per unit cost.","Our objective is to synthesize a control policy that ensures the surveillance task while maximizes the efficiency.","We provide an effective approach to synthesize a stationary control policy achieving $\\epsilon$-optimality by integrating state classifications of MDPs and perturbation analysis in a novel manner.","Our results generalize existing works on efficiency-optimal control synthesis for MDP by incorporating qualitative surveillance tasks.","A robot motion planning case study is provided to illustrate the proposed algorithm."],"url":"http://arxiv.org/abs/2403.18632v1","category":"eess.SY"}
{"created":"2024-03-27 14:38:02","title":"First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques","abstract":"Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great classification power, with good values for the metrics under consideration. Given the lack of this type of tool in Argentina, this work represents the first step towards the development of more sophisticated models.","sentences":["Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors.","Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another.","In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed.","First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables.","After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models.","In particular, RF, DT, and ANN demonstrated great classification power, with good values for the metrics under consideration.","Given the lack of this type of tool in Argentina, this work represents the first step towards the development of more sophisticated models."],"url":"http://arxiv.org/abs/2403.18631v1","category":"cs.LG"}
{"created":"2024-03-27 14:37:46","title":"Flavored leptogenesis from a sudden mass gain of right-handed neutrinos","abstract":"In this paper, we would like to point out that in the scenario that the right-handed neutrinos suddenly gain some masses much larger than the temperature of the Universe at that time so that the washout effects for the generated lepton asymmetry can be safely neglected, the flavored leptogenesis (which crucially rely on the flavor non-universality of the washout effects) cannot work in the usual way any more. For this problem, we put forward that the flavor non-universality of the conversion efficiencies from the lepton asymmetry to the baryon asymmetry via the sphaleron processes may play a crucial role. And we will study if the requisite baryon asymmetry can be successfully reproduced from such a mechanism in the scenarios that the right-handed neutrino masses are hierarchical and nearly degenerate, respectively. A detailed study shows that this mechanism can be viable in both these two scenarios.","sentences":["In this paper, we would like to point out that in the scenario that the right-handed neutrinos suddenly gain some masses much larger than the temperature of the Universe at that time so that the washout effects for the generated lepton asymmetry can be safely neglected, the flavored leptogenesis (which crucially rely on the flavor non-universality of the washout effects) cannot work in the usual way any more.","For this problem, we put forward that the flavor non-universality of the conversion efficiencies from the lepton asymmetry to the baryon asymmetry via the sphaleron processes may play a crucial role.","And we will study if the requisite baryon asymmetry can be successfully reproduced from such a mechanism in the scenarios that the right-handed neutrino masses are hierarchical and nearly degenerate, respectively.","A detailed study shows that this mechanism can be viable in both these two scenarios."],"url":"http://arxiv.org/abs/2403.18630v1","category":"hep-ph"}
{"created":"2024-03-27 14:37:41","title":"Uncertainty quantification in $(p,n)$ reactions","abstract":"Charge-exchange reactions are versatile probes for nuclear structure. In particular, when populating isobaric analog states, these reactions are used to study isovector nuclear densities and neutron skins. The quality of the information extracted from charge-exchange data depends on the accuracy of the reaction models and their inputs; this work addresses these two points. First, we quantify the uncertainties due to effective nucleon-nucleus interactions by propagating the parameter posterior distributions of the recent global optical model KDUQ [1] to $(p,n)$ reaction observables populating the isobaric analogue state, at beam energies in the range of $25-160$ MeV. Our analysis, focusing on $^{48}$Ca, shows that the total parametric uncertainties on the cross sections are around 60-100%. The source of this uncertainty is mainly the transition operator as the uncertainties from the distorted waves alone are less than about 15%. Second, we perform a comparison between two- and three-body models that both describe the dynamics of the reaction within the DWBA. The predictions from these two models are similar and generally agree with the available data, suggesting that 1-step DWBA is sufficient to describe the reaction process. Only at a beam energy of 25 MeV there are possibly signs that a 1-step assumption is not fully correct. This work provides motivation for the quantification of uncertainties associated with the transition operator in three-body model. It also suggests that further constraint of the optical potential parameters is needed for increased model precision.","sentences":["Charge-exchange reactions are versatile probes for nuclear structure.","In particular, when populating isobaric analog states, these reactions are used to study isovector nuclear densities and neutron skins.","The quality of the information extracted from charge-exchange data depends on the accuracy of the reaction models and their inputs; this work addresses these two points.","First, we quantify the uncertainties due to effective nucleon-nucleus interactions by propagating the parameter posterior distributions of the recent global optical model KDUQ [1] to $(p,n)$ reaction observables populating the isobaric analogue state, at beam energies in the range of $25-160$ MeV. Our analysis, focusing on $^{48}$Ca, shows that the total parametric uncertainties on the cross sections are around 60-100%.","The source of this uncertainty is mainly the transition operator as the uncertainties from the distorted waves alone are less than about 15%.","Second, we perform a comparison between two- and three-body models that both describe the dynamics of the reaction within the DWBA.","The predictions from these two models are similar and generally agree with the available data, suggesting that 1-step DWBA is sufficient to describe the reaction process.","Only at a beam energy of 25 MeV there are possibly signs that a 1-step assumption is not fully correct.","This work provides motivation for the quantification of uncertainties associated with the transition operator in three-body model.","It also suggests that further constraint of the optical potential parameters is needed for increased model precision."],"url":"http://arxiv.org/abs/2403.18629v1","category":"nucl-th"}
{"created":"2024-03-27 14:36:57","title":"Gravitational helicity flux density from two-body systems","abstract":"The helicity flux density is a novel quantity which characterizes the angular distribution of the helicity of radiative gravitons and it may be tested by gravitational wave experiments in the future. We derive a quadrupole formula for the helicity flux density due to gravitational radiation in the slow motion and the weak field limit. We apply the formula to the bound and unbound orbits in two-body systems and find that the total radiative helicity fluxes are always zero. However, the helicity flux density still has non-trivial dependence on the angle. We also find a formula for the total helicity flux by including all contributions of the higher multipoles.","sentences":["The helicity flux density is a novel quantity which characterizes the angular distribution of the helicity of radiative gravitons and it may be tested by gravitational wave experiments in the future.","We derive a quadrupole formula for the helicity flux density due to gravitational radiation in the slow motion and the weak field limit.","We apply the formula to the bound and unbound orbits in two-body systems and find that the total radiative helicity fluxes are always zero.","However, the helicity flux density still has non-trivial dependence on the angle.","We also find a formula for the total helicity flux by including all contributions of the higher multipoles."],"url":"http://arxiv.org/abs/2403.18627v1","category":"gr-qc"}
{"created":"2024-03-27 14:33:11","title":"Triple product $p$-adic $L$-functions for finite slope families and a $p$-adic Gross-Zagier formula","abstract":"In this paper, we generalize two results of H. Darmon and V. Rotger on triple product $p$-adic $L$-functions associated with Hida families to finite slope families. We first prove a $p$-adic Gross-Zagier formula, then demonstrate an application to a special case of the equivariant Birch and Swinnerton-Dyer conjecture for supersingular elliptic curves.","sentences":["In this paper, we generalize two results of H. Darmon and V. Rotger on triple product $p$-adic $L$-functions associated with Hida families to finite slope families.","We first prove a $p$-adic Gross-Zagier formula, then demonstrate an application to a special case of the equivariant Birch and Swinnerton-Dyer conjecture for supersingular elliptic curves."],"url":"http://arxiv.org/abs/2403.18620v1","category":"math.NT"}
{"created":"2024-03-27 14:32:03","title":"Quantum concentration inequalities and equivalence of the thermodynamical ensembles: an optimal mass transport approach","abstract":"We prove new concentration inequalities for quantum spin systems which apply to any local observable measured on any product state or on any state with exponentially decaying correlations. Our results do not require the spins to be arranged in a regular lattice, and cover the case of observables that contain terms acting on spins at arbitrary distance. Moreover, we introduce a local W1 distance, which quantifies the distinguishability of two states with respect to local observables. We prove a transportation-cost inequality stating that the local W1 distance between a generic state and a state with exponentially decaying correlations is upper bounded by a function of their relative entropy. Finally, we apply such inequality to prove the equivalence between the canonical and microcanonical ensembles of quantum statistical mechanics and the weak eigenstate thermalization hypothesis for the Hamiltonians whose Gibbs states have exponentially decaying correlations.","sentences":["We prove new concentration inequalities for quantum spin systems which apply to any local observable measured on any product state or on any state with exponentially decaying correlations.","Our results do not require the spins to be arranged in a regular lattice, and cover the case of observables that contain terms acting on spins at arbitrary distance.","Moreover, we introduce a local W1 distance, which quantifies the distinguishability of two states with respect to local observables.","We prove a transportation-cost inequality stating that the local W1 distance between a generic state and a state with exponentially decaying correlations is upper bounded by a function of their relative entropy.","Finally, we apply such inequality to prove the equivalence between the canonical and microcanonical ensembles of quantum statistical mechanics and the weak eigenstate thermalization hypothesis for the Hamiltonians whose Gibbs states have exponentially decaying correlations."],"url":"http://arxiv.org/abs/2403.18617v1","category":"math-ph"}
{"created":"2024-03-27 14:29:36","title":"Energy-ordered resource stratification as an agnostic signature of life","abstract":"The search for extraterrestrial life hinges on identifying biosignatures, often focusing on gaseous metabolic byproducts as indicators. However, most such biosignatures require assuming specific metabolic processes. It is widely recognized that life on other planets may not resemble that of Earth, but identifying biosignatures ``agnostic'' to such assumptions has remained a challenge. Here, we propose a novel approach by considering the generic outcome of life: the formation of competing ecosystems. We use a minimal model to argue that the presence of ecosystem-level dynamics, characterized by ecological interactions and resource competition, may yield biosignatures independent of specific metabolic activities. Specifically, we propose the emergent stratification of chemical resources in order of decreasing energy content as a candidate new biosignature. While likely inaccessible to remote sensing, this signature could be relevant for sample return missions, or for detection of ancient signatures of life on Earth itself.","sentences":["The search for extraterrestrial life hinges on identifying biosignatures, often focusing on gaseous metabolic byproducts as indicators.","However, most such biosignatures require assuming specific metabolic processes.","It is widely recognized that life on other planets may not resemble that of Earth, but identifying biosignatures ``agnostic'' to such assumptions has remained a challenge.","Here, we propose a novel approach by considering the generic outcome of life: the formation of competing ecosystems.","We use a minimal model to argue that the presence of ecosystem-level dynamics, characterized by ecological interactions and resource competition, may yield biosignatures independent of specific metabolic activities.","Specifically, we propose the emergent stratification of chemical resources in order of decreasing energy content as a candidate new biosignature.","While likely inaccessible to remote sensing, this signature could be relevant for sample return missions, or for detection of ancient signatures of life on Earth itself."],"url":"http://arxiv.org/abs/2403.18614v1","category":"q-bio.PE"}
{"created":"2024-03-27 14:27:10","title":"More on maximal line-free sets in $\\mathbb{F}_p^n$","abstract":"For a prime $p$ we construct a subset of $\\mathbb{F}_p^{(k^2-k)/2}$ of size $p^{(k^2-k)/2-1}$ that does not contain progressions of length $k$. More generally, we show that for any prime power $q$ there is a subset of $\\mathbb{F}_q^{(k^2-k)/2}$ of size $q^{(k^2-k)/2-1}$ that does not contain $k$ points on a line. This yields the first asympotic lower bounds $c^n$ for the size of $p$-progression-free sets in $\\mathbb{F}_p^{n}$ with $c=p-o(1)$, as $p$ tends to infinity.","sentences":["For a prime $p$ we construct a subset of $\\mathbb{F}_p^{(k^2-k)/2}$ of size $p^{(k^2-k)/2-1}$ that does not contain progressions of length $k$. More generally, we show that for any prime power $q$ there is a subset of $\\mathbb{F}_q^{(k^2-k)/2}$ of size $q^{(k^2-k)/2-1}$ that does not contain $k$ points on a line.","This yields the first asympotic lower bounds $c^n$ for the size of $p$-progression-free sets in $\\mathbb{F}_p^{n}$ with $c=p-o(1)$, as $p$ tends to infinity."],"url":"http://arxiv.org/abs/2403.18611v1","category":"math.CO"}
{"created":"2024-03-27 14:26:44","title":"Cavity Detection of Gravitational Waves: Where Do We Stand?","abstract":"High frequency gravitational waves (HFGWs) are predicted in various exotic scenarios involving both cosmological and astrophysical sources. These elusive signals have recently sparked the interest of a diverse community of researchers, due to the possibility of HFGW detection in the laboratory through graviton-photon conversion in strong magnetic fields. Notable examples include the redesign of the resonant cavities currently under development to detect the cosmic axion. In this work, we derive the sensitivities of some existing and planned resonant cavities to detect a HFGW background. As a concrete scenario, we consider the collective signals that originate from the merging of compact objects, such as two primordial black holes (PBHs) in the asteroid mass window. Our findings improve over existing work by explicitly discussing and quantifying the loss in the experimental reach due to the actual coherence of the source. We elucidate on the approach we adopt in relation with recent literature on the topic. Most notably, we give a recipe for the estimate of the stochastic background that focuses on the presence of the signal in the cavity at all times and showing that, in the relevant PBH mass region, the signal is dominated by coherent binary mergers.","sentences":["High frequency gravitational waves (HFGWs) are predicted in various exotic scenarios involving both cosmological and astrophysical sources.","These elusive signals have recently sparked the interest of a diverse community of researchers, due to the possibility of HFGW detection in the laboratory through graviton-photon conversion in strong magnetic fields.","Notable examples include the redesign of the resonant cavities currently under development to detect the cosmic axion.","In this work, we derive the sensitivities of some existing and planned resonant cavities to detect a HFGW background.","As a concrete scenario, we consider the collective signals that originate from the merging of compact objects, such as two primordial black holes (PBHs) in the asteroid mass window.","Our findings improve over existing work by explicitly discussing and quantifying the loss in the experimental reach due to the actual coherence of the source.","We elucidate on the approach we adopt in relation with recent literature on the topic.","Most notably, we give a recipe for the estimate of the stochastic background that focuses on the presence of the signal in the cavity at all times and showing that, in the relevant PBH mass region, the signal is dominated by coherent binary mergers."],"url":"http://arxiv.org/abs/2403.18610v1","category":"gr-qc"}
{"created":"2024-03-27 14:26:41","title":"A survey on learning models of spiking neural membrane systems and spiking neural networks","abstract":"Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties. In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning. In SNN, communication between neurons takes place through the spikes and spike trains. This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals. Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory. In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS. A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both SNN and SNPS formalisms.","sentences":["Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties.","In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning.","In SNN, communication between neurons takes place through the spikes and spike trains.","This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals.","Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory.","In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS.","A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both SNN and SNPS formalisms."],"url":"http://arxiv.org/abs/2403.18609v1","category":"cs.NE"}
{"created":"2024-03-27 14:25:02","title":"Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices","abstract":"Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers. Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks. Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration.","sentences":["Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks.","The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data.","However, in FedNL, unknown threats may be hidden in time-varying spike signals.","In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices.","In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers.","Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks.","Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration."],"url":"http://arxiv.org/abs/2403.18607v1","category":"cs.CR"}
{"created":"2024-03-27 14:24:43","title":"Test the Loop Quantum Gravity Theory via the Lensing Effect","abstract":"Recently, scholars such as Lewandowski, Ma, and Yang have successfully derived a quantum-corrected black hole model in loop quantum gravity (Phys.Rev.Lett.130.101501 (2023)), which is a modification of the Schwarzschild black hole. In this paper, we calculate the strong/weak gravitational lensing effects under the quantum-corrected black hole model, taking the supermassive black holes M87* and SgrA* as the subjects of study to explore the impact of the quantum correction parameter $\\alpha$ on the positions of images and the Einstein ring. Our calculations show that in the case of strong gravitational lensing, the lensing coefficient $\\bar{a}$ increases with an increase in the quantum correction parameter, while the deflection angle and the lensing coefficient $\\bar{b}$ decrease with an increase in the quantum correction parameter. The quantum correction parameter $\\alpha$ has a significant impact in the context of supermassive black holes. In weak gravitational lensing, the quantum correction parameter $\\alpha$ plays a suppressive role. Our theoretical analysis suggests that with advancements in future astrophysical observation techniques, it might be possible to test loop quantum gravity theory using the gravitational lensing effect.","sentences":["Recently, scholars such as Lewandowski, Ma, and Yang have successfully derived a quantum-corrected black hole model in loop quantum gravity (Phys.Rev.Lett.130.101501 (2023)), which is a modification of the Schwarzschild black hole.","In this paper, we calculate the strong/weak gravitational lensing effects under the quantum-corrected black hole model, taking the supermassive black holes M87* and SgrA* as the subjects of study to explore the impact of the quantum correction parameter $\\alpha$ on the positions of images and the Einstein ring.","Our calculations show that in the case of strong gravitational lensing, the lensing coefficient $\\bar{a}$ increases with an increase in the quantum correction parameter, while the deflection angle and the lensing coefficient $\\bar{b}$ decrease with an increase in the quantum correction parameter.","The quantum correction parameter $\\alpha$ has a significant impact in the context of supermassive black holes.","In weak gravitational lensing, the quantum correction parameter $\\alpha$ plays a suppressive role.","Our theoretical analysis suggests that with advancements in future astrophysical observation techniques, it might be possible to test loop quantum gravity theory using the gravitational lensing effect."],"url":"http://arxiv.org/abs/2403.18606v1","category":"gr-qc"}
{"created":"2024-03-27 14:24:28","title":"Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems","abstract":"In an era of information overload and complex decision-making processes, Recommender Systems (RS) have emerged as indispensable tools across diverse domains, particularly travel and tourism. These systems simplify trip planning by offering personalized recommendations that consider individual preferences and address broader challenges like seasonality, travel regulations, and capacity constraints. The intricacies of the tourism domain, characterized by multiple stakeholders, including consumers, item providers, platforms, and society, underscore the complexity of achieving balance among diverse interests. Although previous research has focused on fairness in Tourism Recommender Systems (TRS) from a multistakeholder perspective, limited work has focused on generating sustainable recommendations.   Our paper introduces a novel approach for assigning a sustainability indicator (SF index) for city trips accessible from the users' starting point, integrating Co2e analysis, destination popularity, and seasonal demand. Our methodology involves comprehensive data gathering on transportation modes and emissions, complemented by analyses of destination popularity and seasonal demand. A user study validates our index, showcasing its practicality and efficacy in providing well-rounded and sustainable city trip recommendations. Our findings contribute significantly to the evolution of responsible tourism strategies, harmonizing the interests of tourists, local communities, and the environment while paving the way for future research in responsible and equitable tourism practices.","sentences":["In an era of information overload and complex decision-making processes, Recommender Systems (RS) have emerged as indispensable tools across diverse domains, particularly travel and tourism.","These systems simplify trip planning by offering personalized recommendations that consider individual preferences and address broader challenges like seasonality, travel regulations, and capacity constraints.","The intricacies of the tourism domain, characterized by multiple stakeholders, including consumers, item providers, platforms, and society, underscore the complexity of achieving balance among diverse interests.","Although previous research has focused on fairness in Tourism Recommender Systems (TRS) from a multistakeholder perspective, limited work has focused on generating sustainable recommendations.   ","Our paper introduces a novel approach for assigning a sustainability indicator (SF index) for city trips accessible from the users' starting point, integrating Co2e analysis, destination popularity, and seasonal demand.","Our methodology involves comprehensive data gathering on transportation modes and emissions, complemented by analyses of destination popularity and seasonal demand.","A user study validates our index, showcasing its practicality and efficacy in providing well-rounded and sustainable city trip recommendations.","Our findings contribute significantly to the evolution of responsible tourism strategies, harmonizing the interests of tourists, local communities, and the environment while paving the way for future research in responsible and equitable tourism practices."],"url":"http://arxiv.org/abs/2403.18604v1","category":"cs.IR"}
{"created":"2024-03-27 14:23:59","title":"Collaborative graphical lasso","abstract":"In recent years, the availability of multi-omics data has increased substantially. Multi-omics data integration methods mainly aim to leverage different molecular data sets to gain a complete molecular description of biological processes. An attractive integration approach is the reconstruction of multi-omics networks. However, the development of effective multi-omics network reconstruction strategies lags behind. This hinders maximizing the potential of multi-omics data sets. With this study, we advance the frontier of multi-omics network reconstruction by introducing \"collaborative graphical lasso\" as a novel strategy. Our proposed algorithm synergizes \"graphical lasso\" with the concept of \"collaboration\", effectively harmonizing multi-omics data sets integration, thereby enhancing the accuracy of network inference. Besides, to tackle model selection in this framework, we designed an ad hoc procedure based on network stability. We assess the performance of collaborative graphical lasso and the corresponding model selection procedure through simulations, and we apply them to publicly available multi-omics data. This demonstrated collaborative graphical lasso is able to reconstruct known biological connections and suggest previously unknown and biologically coherent interactions, enabling the generation of novel hypotheses. We implemented collaborative graphical lasso as an R package, available on CRAN as coglasso.","sentences":["In recent years, the availability of multi-omics data has increased substantially.","Multi-omics data integration methods mainly aim to leverage different molecular data sets to gain a complete molecular description of biological processes.","An attractive integration approach is the reconstruction of multi-omics networks.","However, the development of effective multi-omics network reconstruction strategies lags behind.","This hinders maximizing the potential of multi-omics data sets.","With this study, we advance the frontier of multi-omics network reconstruction by introducing \"collaborative graphical lasso\" as a novel strategy.","Our proposed algorithm synergizes \"graphical lasso\" with the concept of \"collaboration\", effectively harmonizing multi-omics data sets integration, thereby enhancing the accuracy of network inference.","Besides, to tackle model selection in this framework, we designed an ad hoc procedure based on network stability.","We assess the performance of collaborative graphical lasso and the corresponding model selection procedure through simulations, and we apply them to publicly available multi-omics data.","This demonstrated collaborative graphical lasso is able to reconstruct known biological connections and suggest previously unknown and biologically coherent interactions, enabling the generation of novel hypotheses.","We implemented collaborative graphical lasso as an R package, available on CRAN as coglasso."],"url":"http://arxiv.org/abs/2403.18602v1","category":"stat.ME"}
{"created":"2024-03-27 14:22:40","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos","abstract":"Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined. To address these challenges we introduce Retrieval-Augmented Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture. For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures. To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps. Experiments on CrossTask and COIN benchmarks show the superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning.","sentences":["Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states.","Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies.","(2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans.","(3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.","In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the procedure length is not fixed or pre-determined.","To address these challenges we introduce Retrieval-Augmented Planner (RAP) model.","Specifically, for adaptive procedures, RAP adaptively determines the conclusion of actions using an auto-regressive model architecture.","For temporal relation, RAP establishes an external memory module to explicitly retrieve the most relevant state-action pairs from the training videos and revises the generated procedures.","To tackle high annotation cost, RAP utilizes a weakly-supervised learning manner to expand the training dataset to other task-relevant, unannotated videos by generating pseudo labels for action steps.","Experiments on CrossTask and COIN benchmarks show the superiority of RAP over traditional fixed-length models, establishing it as a strong baseline solution for adaptive procedure planning."],"url":"http://arxiv.org/abs/2403.18600v1","category":"cs.CV"}
{"created":"2024-03-27 14:20:27","title":"Proving correctness for SQL implementations of OCL constraints","abstract":"In the context of the model-driven development of data-centric applications, OCL constraints play a major role in adding precision to the source models (e.g., data models and security models). Several code-generators have been proposed to bridge the gap between source models with OCL constraints and their corresponding database implementations. However, the database queries produced by these code-generators are significantly less efficient -- from the point of view of execution-time performance -- than the implementations manually written by database experts. In this paper, we propose a different approach to bridge the gap between models with OCL constraints and their corresponding database implementations. In particular, we introduce a model-based methodology for proving the correctness of manually written SQL implementations of OCL constraints. This methodology is based on a novel mapping from a significant subset of the SQL language into many-sorted first-order logic. Moreover, by leveraging on an already existing mapping from the OCL language into many-sorted first-order logic, we can use SMT solvers to automatically prove the correctness of SQL implementations of OCL constraints. To illustrate and show the applicability of our approach, we include in the paper a number of non-trivial examples. Finally, we report on the status of a suite of tools supporting our approach.","sentences":["In the context of the model-driven development of data-centric applications, OCL constraints play a major role in adding precision to the source models (e.g., data models and security models).","Several code-generators have been proposed to bridge the gap between source models with OCL constraints and their corresponding database implementations.","However, the database queries produced by these code-generators are significantly less efficient -- from the point of view of execution-time performance -- than the implementations manually written by database experts.","In this paper, we propose a different approach to bridge the gap between models with OCL constraints and their corresponding database implementations.","In particular, we introduce a model-based methodology for proving the correctness of manually written SQL implementations of OCL constraints.","This methodology is based on a novel mapping from a significant subset of the SQL language into many-sorted first-order logic.","Moreover, by leveraging on an already existing mapping from the OCL language into many-sorted first-order logic, we can use SMT solvers to automatically prove the correctness of SQL implementations of OCL constraints.","To illustrate and show the applicability of our approach, we include in the paper a number of non-trivial examples.","Finally, we report on the status of a suite of tools supporting our approach."],"url":"http://arxiv.org/abs/2403.18599v1","category":"cs.DB"}
{"created":"2024-03-27 14:18:09","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding","abstract":"The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds within the same SIR. To achieve adaptability, the OVM defines a variable number of learnable vectors as cross-attention queries, allowing for the adjustment of token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19 classification dataset, and GID5 segmentation dataset for sparse and dense tasks. The results demonstrate that the visual tokens obtained by HOOK correspond to individual objects, which demonstrates homogeneity. HOOK outperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved state-of-the-art performance compared to the baselines used for comparison. Compared to Patch Embed, which requires more than one hundred tokens for one image, HOOK requires only 6 and 8 tokens for sparse and dense tasks, respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The code is available at https://github.com/GeoX-Lab/Hook.","sentences":["The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks.","One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language.","In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language.","Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision.","We designed a simple HOmogeneous visual tOKenizer: HOOK.","HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM).","To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to perceive SIRs.","The OVM employs cross-attention to merge seeds within the same SIR.","To achieve adaptability, the OVM defines a variable number of learnable vectors as cross-attention queries, allowing for the adjustment of token quantity.","We conducted experiments on the NWPU-RESISC45, WHU-RS19 classification dataset, and GID5 segmentation dataset for sparse and dense tasks.","The results demonstrate that the visual tokens obtained by HOOK correspond to individual objects, which demonstrates homogeneity.","HOOK outperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved state-of-the-art performance compared to the baselines used for comparison.","Compared to Patch Embed, which requires more than one hundred tokens for one image, HOOK requires only 6 and 8 tokens for sparse and dense tasks, respectively, resulting in efficiency improvements of 1.5 to 2.8 times.","The code is available at https://github.com/GeoX-Lab/Hook."],"url":"http://arxiv.org/abs/2403.18593v1","category":"cs.CV"}
{"created":"2024-03-27 14:17:33","title":"Safety Verification of Wait-Only Non-Blocking Broadcast Protocols","abstract":"We study networks of processes that all execute the same finite protocol and communicate synchronously in two different ways: a process can broadcast one message to all other processes or send it to at most one other process. In both cases, if no process can receive the message, it will still be sent. We establish a precise complexity class for two coverability problems with a parameterised number of processes: the state coverability problem and the configuration coverability problem. It is already known that these problems are Ackermann-hard (but decidable) in the general case. We show that when the protocol is Wait-Only, i.e., it has no state from which a process can send and receive messages, the complexity drops to P and PSPACE, respectively.","sentences":["We study networks of processes that all execute the same finite protocol and communicate synchronously in two different ways: a process can broadcast one message to all other processes or send it to at most one other process.","In both cases, if no process can receive the message, it will still be sent.","We establish a precise complexity class for two coverability problems with a parameterised number of processes: the state coverability problem and the configuration coverability problem.","It is already known that these problems are Ackermann-hard (but decidable) in the general case.","We show that when the protocol is Wait-Only, i.e., it has no state from which a process can send and receive messages, the complexity drops to P and PSPACE, respectively."],"url":"http://arxiv.org/abs/2403.18591v1","category":"cs.LO"}
{"created":"2024-03-27 13:59:21","title":"MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction","abstract":"The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset.   This paper proposes \"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.","sentences":["The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets.","These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs.","Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access.","This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset.   ","This paper proposes \"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD.","The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers.","The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries.","Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings."],"url":"http://arxiv.org/abs/2403.18580v1","category":"cs.CR"}
{"created":"2024-03-27 13:59:05","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","abstract":"Generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small. Here, we tackle the problem of graph generation from only one observed graph. The classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. Generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples. Our proposed generating procedure, SteinGen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from Stein's method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model. SteinGen uses the Glauber dynamics associated with an estimated Stein operator to generate a sample, and re-estimates the Stein operator from the sample after every sampling step. We show that on a class of exponential random graph models this novel \"estimation and re-estimation\" generation strategy yields high distributional similarity (high fidelity) to the original data, combined with high sample diversity.","sentences":["Generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small.","Here, we tackle the problem of graph generation from only one observed graph.","The classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants.","Generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples.","Our proposed generating procedure, SteinGen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from Stein's method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model.","SteinGen uses the Glauber dynamics associated with an estimated Stein operator to generate a sample, and re-estimates the Stein operator from the sample after every sampling step.","We show that on a class of exponential random graph models this novel \"estimation and re-estimation\" generation strategy yields high distributional similarity (high fidelity) to the original data, combined with high sample diversity."],"url":"http://arxiv.org/abs/2403.18578v1","category":"stat.ML"}
{"created":"2024-03-27 13:56:08","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions","abstract":"Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.","sentences":["Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets.","While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage.","In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples.","First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free.","Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set.","Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks.","Our code will be released on https://github.com/hxwork/HandBooster_Pytorch."],"url":"http://arxiv.org/abs/2403.18575v1","category":"cs.CV"}
{"created":"2024-03-27 13:54:17","title":"ACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds","abstract":"Automated Audio Captioning is a multimodal task that aims to convert audio content into natural language. The assessment of audio captioning systems is typically based on quantitative metrics applied to text data. Previous studies have employed metrics derived from machine translation and image captioning to evaluate the quality of generated audio captions. Drawing inspiration from auditory cognitive neuroscience research, we introduce a novel metric approach -- Audio Captioning Evaluation on Semantics of Sound (ACES). ACES takes into account how human listeners parse semantic information from sounds, providing a novel and comprehensive evaluation perspective for automated audio captioning systems. ACES combines semantic similarities and semantic entity labeling. ACES outperforms similar automated audio captioning metrics on the Clotho-Eval FENSE benchmark in two evaluation categories.","sentences":["Automated Audio Captioning is a multimodal task that aims to convert audio content into natural language.","The assessment of audio captioning systems is typically based on quantitative metrics applied to text data.","Previous studies have employed metrics derived from machine translation and image captioning to evaluate the quality of generated audio captions.","Drawing inspiration from auditory cognitive neuroscience research, we introduce a novel metric approach -- Audio Captioning Evaluation on Semantics of Sound (ACES).","ACES takes into account how human listeners parse semantic information from sounds, providing a novel and comprehensive evaluation perspective for automated audio captioning systems.","ACES combines semantic similarities and semantic entity labeling.","ACES outperforms similar automated audio captioning metrics on the Clotho-Eval FENSE benchmark in two evaluation categories."],"url":"http://arxiv.org/abs/2403.18572v1","category":"cs.SD"}
{"created":"2024-03-27 13:51:26","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","abstract":"Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information. Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS. Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets.","sentences":["Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development.","As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - \"Clean water and sanitation for all\".","In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS.","Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing.","Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner.","To the best of our knowledge, this is the first DL approach to emulate the popular hydraulic simulator EPANET, utilizing no additional information.","Like most DL models and unlike the hydraulic simulator, our model demonstrates vastly faster emulation times that do not increase drastically with the size of the WDS.","Moreover, we achieve high accuracy on the ground truth and very similar results compared to the hydraulic simulator as demonstrated through experiments on five real-world WDS datasets."],"url":"http://arxiv.org/abs/2403.18570v1","category":"cs.LG"}
{"created":"2024-03-27 13:50:13","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction","abstract":"IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above features during the learning process. Several key designs are presented to make the dynamic IR drop prediction highly effective and interpretable. We are the first work to apply graph structure to deep-learning based dynamic IR drop prediction method. Experiments show that PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3% reduction in prediction error and achieves 545x speedup compared to the commercial tool, which demonstrates the superiority of our method.","sentences":["IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption.","As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution.","Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible.","In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure.","Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation.","We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above features during the learning process.","Several key designs are presented to make the dynamic IR drop prediction highly effective and interpretable.","We are the first work to apply graph structure to deep-learning based dynamic IR drop prediction method.","Experiments show that PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3% reduction in prediction error and achieves 545x speedup compared to the commercial tool, which demonstrates the superiority of our method."],"url":"http://arxiv.org/abs/2403.18569v1","category":"cs.LG"}
{"created":"2024-03-27 13:47:17","title":"The effect of Relativistic Aberration on Cosmological Distances","abstract":"Aims: we propose that the condition of relative motion between us and the objects that we observe in the Universe should generate relativistic aberration on the photons that such objects emit, varying the observed flux similarly to the cases of blazars and neutron stars, with instead a decrease of this radiative flux. Methods: we follow some important papers and textbook used in modern cosmology, adding the effects of relativistic aberration in the theoretical description of the Cosmos. Results: New definitions for the luminosity distance and the angular distance arise, with the consequence of changing the cosmological parameters measured in the last years. A qualitative description of all of this is also performed on the multipole analysis of the CMB.","sentences":["Aims: we propose that the condition of relative motion between us and the objects that we observe in the Universe should generate relativistic aberration on the photons that such objects emit, varying the observed flux similarly to the cases of blazars and neutron stars, with instead a decrease of this radiative flux.","Methods: we follow some important papers and textbook used in modern cosmology, adding the effects of relativistic aberration in the theoretical description of the Cosmos.","Results: New definitions for the luminosity distance and the angular distance arise, with the consequence of changing the cosmological parameters measured in the last years.","A qualitative description of all of this is also performed on the multipole analysis of the CMB."],"url":"http://arxiv.org/abs/2403.18567v1","category":"astro-ph.CO"}
{"created":"2024-03-27 13:46:01","title":"Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images with Deep Learning -- A Review","abstract":"Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations.","sentences":["Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics.","In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature.","In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact.","We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms.","Research gaps are identified to suggest avenues for future exploration.","One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations."],"url":"http://arxiv.org/abs/2403.18565v1","category":"cs.CV"}
{"created":"2024-03-27 13:45:45","title":"Formal Verification with Constrained Polynomial Logical Zonotope","abstract":"In this paper, we propose using constrained polynomial logical zonotopes for formal verification of logical systems. We perform reachability analysis to compute the set of states that could be reached. To do this, we utilize a recently introduced set representation called polynomial logical zonotopes for performing computationally efficient and exact reachability analysis on logical systems. Notably, polynomial logical zonotopes address the \"curse of dimensionality\" when analyzing the reachability of logical systems since the set representation can represent 2^n binary vectors using n generators. After finishing the reachability analysis, the formal verification involves verifying whether the intersection of the calculated reachable set and the unsafe set is empty or not. However, polynomial logical zonotopes are not closed under intersections. To address this, we formulate constrained polynomial logical zonotopes, which maintain the computational efficiency and exactness of polynomial logical zonotopes for reachability analysis while supporting exact intersections. Furthermore, we present an extensive empirical study illustrating and verifying the benefits of using constrained polynomial logical zonotopes for the formal verification of logical systems.","sentences":["In this paper, we propose using constrained polynomial logical zonotopes for formal verification of logical systems.","We perform reachability analysis to compute the set of states that could be reached.","To do this, we utilize a recently introduced set representation called polynomial logical zonotopes for performing computationally efficient and exact reachability analysis on logical systems.","Notably, polynomial logical zonotopes address the \"curse of dimensionality\" when analyzing the reachability of logical systems since the set representation can represent 2^n binary vectors using n generators.","After finishing the reachability analysis, the formal verification involves verifying whether the intersection of the calculated reachable set and the unsafe set is empty or not.","However, polynomial logical zonotopes are not closed under intersections.","To address this, we formulate constrained polynomial logical zonotopes, which maintain the computational efficiency and exactness of polynomial logical zonotopes for reachability analysis while supporting exact intersections.","Furthermore, we present an extensive empirical study illustrating and verifying the benefits of using constrained polynomial logical zonotopes for the formal verification of logical systems."],"url":"http://arxiv.org/abs/2403.18564v1","category":"eess.SY"}
{"created":"2024-03-27 13:33:14","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection","abstract":"Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.","sentences":["Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images.","Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction.","The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects.","In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement.","Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification.","For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples.","For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition.","Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise.","The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly."],"url":"http://arxiv.org/abs/2403.18554v1","category":"cs.CV"}
{"created":"2024-03-27 13:32:12","title":"Generalized convergence of the deep BSDE method: a step towards fully-coupled FBSDEs and applications in stochastic control","abstract":"We are concerned with high-dimensional coupled FBSDE systems approximated by the deep BSDE method of Han et al. (2018). It was shown by Han and Long (2020) that the errors induced by the deep BSDE method admit a posteriori estimate depending on the loss function, whenever the backward equation only couples into the forward diffusion through the Y process. We generalize this result to fully-coupled drift coefficients, and give sufficient conditions for convergence under standard assumptions. The resulting conditions are directly verifiable for any equation. Consequently, unlike in earlier theory, our convergence analysis enables the treatment of FBSDEs stemming from stochastic optimal control problems. In particular, we provide a theoretical justification for the non-convergence of the deep BSDE method observed in recent literature, and present direct guidelines for when convergence can be guaranteed in practice. Our theoretical findings are supported by several numerical experiments in high-dimensional settings.","sentences":["We are concerned with high-dimensional coupled FBSDE systems approximated by the deep BSDE method of Han et al.","(2018).","It was shown by Han and Long (2020) that the errors induced by the deep BSDE method admit a posteriori estimate depending on the loss function, whenever the backward equation only couples into the forward diffusion through the Y process.","We generalize this result to fully-coupled drift coefficients, and give sufficient conditions for convergence under standard assumptions.","The resulting conditions are directly verifiable for any equation.","Consequently, unlike in earlier theory, our convergence analysis enables the treatment of FBSDEs stemming from stochastic optimal control problems.","In particular, we provide a theoretical justification for the non-convergence of the deep BSDE method observed in recent literature, and present direct guidelines for when convergence can be guaranteed in practice.","Our theoretical findings are supported by several numerical experiments in high-dimensional settings."],"url":"http://arxiv.org/abs/2403.18552v1","category":"math.NA"}
{"created":"2024-03-27 13:31:39","title":"Attention Calibration for Disentangled Text-to-Image Personalization","abstract":"Recent thrilling progress in large-scale text-to-image (T2I) models has unlocked unprecedented synthesis quality of AI-generated content (AIGC) including image generation, 3D and video composition. Further, personalized techniques enable appealing customized production of a novel concept given only several images as reference. However, an intriguing problem persists: Is it possible to capture multiple, novel concepts from one single reference image? In this paper, we identify that existing approaches fail to preserve visual consistency with the reference image and eliminate cross-influence from concepts. To alleviate this, we propose an attention calibration mechanism to improve the concept-level understanding of the T2I model. Specifically, we first introduce new learnable modifiers bound with classes to capture attributes of multiple concepts. Then, the classes are separated and strengthened following the activation of the cross-attention operation, ensuring comprehensive and self-contained concepts. Additionally, we suppress the attention activation of different classes to mitigate mutual influence among concepts. Together, our proposed method, dubbed DisenDiff, can learn disentangled multiple concepts from one single image and produce novel customized images with learned concepts. We demonstrate that our method outperforms the current state of the art in both qualitative and quantitative evaluations. More importantly, our proposed techniques are compatible with LoRA and inpainting pipelines, enabling more interactive experiences.","sentences":["Recent thrilling progress in large-scale text-to-image (T2I) models has unlocked unprecedented synthesis quality of AI-generated content (AIGC) including image generation, 3D and video composition.","Further, personalized techniques enable appealing customized production of a novel concept given only several images as reference.","However, an intriguing problem persists: Is it possible to capture multiple, novel concepts from one single reference image?","In this paper, we identify that existing approaches fail to preserve visual consistency with the reference image and eliminate cross-influence from concepts.","To alleviate this, we propose an attention calibration mechanism to improve the concept-level understanding of the T2I model.","Specifically, we first introduce new learnable modifiers bound with classes to capture attributes of multiple concepts.","Then, the classes are separated and strengthened following the activation of the cross-attention operation, ensuring comprehensive and self-contained concepts.","Additionally, we suppress the attention activation of different classes to mitigate mutual influence among concepts.","Together, our proposed method, dubbed DisenDiff, can learn disentangled multiple concepts from one single image and produce novel customized images with learned concepts.","We demonstrate that our method outperforms the current state of the art in both qualitative and quantitative evaluations.","More importantly, our proposed techniques are compatible with LoRA and inpainting pipelines, enabling more interactive experiences."],"url":"http://arxiv.org/abs/2403.18551v1","category":"cs.CV"}
{"created":"2024-03-27 13:30:48","title":"OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning","abstract":"Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which the problem space expands with limited data. FSCIL methods inherently face the challenge of catastrophic forgetting as data arrives incrementally, making models susceptible to overwriting previously acquired knowledge. Moreover, given the scarcity of labeled samples available at any given time, models may be prone to overfitting and find it challenging to strike a balance between extensive pretraining and the limited incremental data. To address these challenges, we propose the OrCo framework built on two core principles: features' orthogonality in the representation space, and contrastive learning. In particular, we improve the generalization of the embedding space by employing a combination of supervised and self-supervised contrastive losses during the pretraining phase. Additionally, we introduce OrCo loss to address challenges arising from data limitations during incremental sessions. Through feature space perturbations and orthogonality between classes, the OrCo loss maximizes margins and reserves space for the following incremental data. This, in turn, ensures the accommodation of incoming classes in the feature space without compromising previously acquired knowledge. Our experimental results showcase state-of-the-art performance across three benchmark datasets, including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at https://github.com/noorahmedds/OrCo","sentences":["Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which the problem space expands with limited data.","FSCIL methods inherently face the challenge of catastrophic forgetting as data arrives incrementally, making models susceptible to overwriting previously acquired knowledge.","Moreover, given the scarcity of labeled samples available at any given time, models may be prone to overfitting and find it challenging to strike a balance between extensive pretraining and the limited incremental data.","To address these challenges, we propose the OrCo framework built on two core principles: features' orthogonality in the representation space, and contrastive learning.","In particular, we improve the generalization of the embedding space by employing a combination of supervised and self-supervised contrastive losses during the pretraining phase.","Additionally, we introduce OrCo loss to address challenges arising from data limitations during incremental sessions.","Through feature space perturbations and orthogonality between classes, the OrCo loss maximizes margins and reserves space for the following incremental data.","This, in turn, ensures the accommodation of incoming classes in the feature space without compromising previously acquired knowledge.","Our experimental results showcase state-of-the-art performance across three benchmark datasets, including mini-ImageNet, CIFAR100, and CUB datasets.","Code is available at https://github.com/noorahmedds/OrCo"],"url":"http://arxiv.org/abs/2403.18550v1","category":"cs.CV"}
{"created":"2024-03-27 13:25:43","title":"Neural Architecture Search for Sentence Classification with BERT","abstract":"Pre training of language models on large text corpora is common practice in Natural Language Processing. Following, fine tuning of these models is performed to achieve the best results on a variety of tasks. In this paper we question the common practice of only adding a single output layer as a classification head on top of the network. We perform an AutoML search to find architectures that outperform the current single layer at only a small compute cost. We validate our classification architecture on a variety of NLP benchmarks from the GLUE dataset.","sentences":["Pre training of language models on large text corpora is common practice in Natural Language Processing.","Following, fine tuning of these models is performed to achieve the best results on a variety of tasks.","In this paper we question the common practice of only adding a single output layer as a classification head on top of the network.","We perform an AutoML search to find architectures that outperform the current single layer at only a small compute cost.","We validate our classification architecture on a variety of NLP benchmarks from the GLUE dataset."],"url":"http://arxiv.org/abs/2403.18547v1","category":"cs.AI"}
{"created":"2024-03-27 13:24:58","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","abstract":"Fast and robust object grasping in clutter is a crucial component of robotics. Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance. In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation. Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way. Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information. Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity. Benefiting from the high-efficiency encoding in the image space and focusing on points in local graspable regions, our framework can perform high-quality grasp detection in real-time and achieve state-of-the-art results. In addition, real robot experiments demonstrate the effectiveness of our method with a success rate of 94% and a clutter completion rate of 100%. Our code is available at https://github.com/THU-VCLab/HGGD.","sentences":["Fast and robust object grasping in clutter is a crucial component of robotics.","Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance.","In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation.","Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way.","Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information.","Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity.","Benefiting from the high-efficiency encoding in the image space and focusing on points in local graspable regions, our framework can perform high-quality grasp detection in real-time and achieve state-of-the-art results.","In addition, real robot experiments demonstrate the effectiveness of our method with a success rate of 94% and a clutter completion rate of 100%.","Our code is available at https://github.com/THU-VCLab/HGGD."],"url":"http://arxiv.org/abs/2403.18546v1","category":"cs.RO"}
{"created":"2024-03-27 13:23:07","title":"Accurate analytical modeling of light rays in spherically symmetric spacetimes: Applications in the study of black hole accretion disks and polarimetry","abstract":"We present new, simple analytical formulas to accurately describe light rays in spherically symmetric static spacetimes. These formulas extend those introduced by Beloborodov and refined by Poutanen for the Schwarzschild metric. Our enhanced formulas are designed to be applicable to a broader range of spacetimes, making them particularly valuable for describing phenomena around compact objects like neutron stars and black holes. As an illustration of their application, we present analytical studies of images of thin accretion disks surrounding black holes and explore their associated polarimetry.","sentences":["We present new, simple analytical formulas to accurately describe light rays in spherically symmetric static spacetimes.","These formulas extend those introduced by Beloborodov and refined by Poutanen for the Schwarzschild metric.","Our enhanced formulas are designed to be applicable to a broader range of spacetimes, making them particularly valuable for describing phenomena around compact objects like neutron stars and black holes.","As an illustration of their application, we present analytical studies of images of thin accretion disks surrounding black holes and explore their associated polarimetry."],"url":"http://arxiv.org/abs/2403.18543v1","category":"gr-qc"}
{"created":"2024-03-27 13:19:12","title":"Thin accretion disk around Kerr-Sen black hole in Einstein-Maxwell-dilaton-axion gravity","abstract":"We investigate the accretion process in a thin disk surrounding a supermassive black hole in Einstein-Maxwell-dilaton-axion gravity by using the Novikov-Thorne model. Our exploration focuses on understanding the impact of the model parameter on various physical properties of the disk. The results reveal that as the dilaton parameter $r_2$ increases, the energy flux, radiation temperature, spectra luminosity, and conversion efficiency of the disk all increase. By narrowing down the dilaton parameter range to $0\\leqslant r_2\\leqslant0.4$ M, we discover that in the high-frequency region, the Kerr-Sen black hole demonstrates higher energy output compared to the Kerr black hole. This facilitates a more straightforward examination of the observational results for two black holes.","sentences":["We investigate the accretion process in a thin disk surrounding a supermassive black hole in Einstein-Maxwell-dilaton-axion gravity by using the Novikov-Thorne model.","Our exploration focuses on understanding the impact of the model parameter on various physical properties of the disk.","The results reveal that as the dilaton parameter $r_2$ increases, the energy flux, radiation temperature, spectra luminosity, and conversion efficiency of the disk all increase.","By narrowing down the dilaton parameter range to $0\\leqslant r_2\\leqslant0.4$ M, we discover that in the high-frequency region, the Kerr-Sen black hole demonstrates higher energy output compared to the Kerr black hole.","This facilitates a more straightforward examination of the observational results for two black holes."],"url":"http://arxiv.org/abs/2403.18541v1","category":"gr-qc"}
{"created":"2024-03-27 13:12:57","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks","abstract":"Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using large language models (LLMs), expert legal systems known as legal decision paths, and Bayesian networks. We then show how the proposed method could be applied to extant regulation in matters of autonomous cars, such as the California Vehicle Code.","sentences":["Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways.","It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment.","The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device).","This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law.","In this paper, we sketch a proof of principle for such a method using large language models (LLMs), expert legal systems known as legal decision paths, and Bayesian networks.","We then show how the proposed method could be applied to extant regulation in matters of autonomous cars, such as the California Vehicle Code."],"url":"http://arxiv.org/abs/2403.18537v1","category":"cs.AI"}
{"created":"2024-03-27 13:12:41","title":"A Novel Behavior-Based Recommendation System for E-commerce","abstract":"The majority of existing recommender systems rely on user ratings, which are limited by the lack of user collaboration and the sparsity problem. To address these issues, this study proposes a behavior-based recommender system that leverages customers' natural behaviors, such as browsing and clicking, on e-commerce platforms. The proposed recommendation system involves clustering active customers, determining neighborhoods, collecting similar users, calculating product reputation based on similar users, and recommending high-reputation products. To overcome the complexity of customer behaviors and traditional clustering methods, an unsupervised clustering approach based on product categories is developed to enhance the recommendation methodology. This study makes notable contributions in several aspects. Firstly, a groundbreaking behavior-based recommendation methodology is developed, incorporating customer behavior to generate accurate and tailored recommendations leading to improved customer satisfaction and engagement. Secondly, an original unsupervised clustering method, focusing on product categories, enables more precise clustering and facilitates accurate recommendations. Finally, an approach to determine neighborhoods for active customers within clusters is established, ensuring grouping of customers with similar behavioral patterns to enhance recommendation accuracy and relevance. The proposed recommendation methodology and clustering method contribute to improved recommendation performance, offering valuable insights for researchers and practitioners in the field of e-commerce recommendation systems. Additionally, the proposed method outperforms benchmark methods in experiments conducted using a behavior dataset from the well-known e-commerce site Alibaba.","sentences":["The majority of existing recommender systems rely on user ratings, which are limited by the lack of user collaboration and the sparsity problem.","To address these issues, this study proposes a behavior-based recommender system that leverages customers' natural behaviors, such as browsing and clicking, on e-commerce platforms.","The proposed recommendation system involves clustering active customers, determining neighborhoods, collecting similar users, calculating product reputation based on similar users, and recommending high-reputation products.","To overcome the complexity of customer behaviors and traditional clustering methods, an unsupervised clustering approach based on product categories is developed to enhance the recommendation methodology.","This study makes notable contributions in several aspects.","Firstly, a groundbreaking behavior-based recommendation methodology is developed, incorporating customer behavior to generate accurate and tailored recommendations leading to improved customer satisfaction and engagement.","Secondly, an original unsupervised clustering method, focusing on product categories, enables more precise clustering and facilitates accurate recommendations.","Finally, an approach to determine neighborhoods for active customers within clusters is established, ensuring grouping of customers with similar behavioral patterns to enhance recommendation accuracy and relevance.","The proposed recommendation methodology and clustering method contribute to improved recommendation performance, offering valuable insights for researchers and practitioners in the field of e-commerce recommendation systems.","Additionally, the proposed method outperforms benchmark methods in experiments conducted using a behavior dataset from the well-known e-commerce site Alibaba."],"url":"http://arxiv.org/abs/2403.18536v1","category":"cs.IR"}
{"created":"2024-03-27 13:11:09","title":"Innermost stable circular orbits around a spinning black hole binary","abstract":"Using the exact solution that describes multi-centered rotating black holes, recently discovered by Teo and Wan, we investigate the innermost stable circular orbit (ISCO) for massive particles and the circular orbit for massless particles moving around a spinning black hole binary. We assume equal masses $M_1 = M_2=m$ and equal spin angular momenta $|J_1| = |J_2|$ for both black holes. Firstly, we examine the case where two black holes are spinning in the same direction ($J_1=J_2$). We clarify that that for particles rotating in the same direction as (opposite directions to) black holes' spin, the greater the spin angular momenta of the black holes, the more the radii of the ISCO for massive particles and the circular orbit for massless particles decrease (increase). We show that distinct ISCO transitions occur for particles rotating in the same direction as the black holes in three ranges of spin angular momenta: $0<J_1/m^2=J_2/m^2< 0.395...$, $0.395...<J_1/m^2=J_2/m^2< 0.483...$, and $0.483...<J_1/m^2=J_2/m^2<0.5$. Conversely, particles rotating in the opposite direction to the black holes exhibit a consistent transition pattern for the case $0<J_1/m^2=J_2/m^2<0.5$. Secondly, we study the situation where binary black holes are spinning in opposite directions ($J_1=-J_2$). We clarify that for large (small) separations between black holes, the ISCO appears near the black hole that is spinning in the same (opposite) direction as particles' rotation. Additionally, we show that different ISCO transitions occur in the three angular momentum ranges: $0<J_1/m^2=-J_2/m^2< 0.160...$, $0.160...<J_1/m^2=-J_2/m^2< 0.467...$, and $0.467...<J_1/m^2=-J_2/m^2<0.5$.","sentences":["Using the exact solution that describes multi-centered rotating black holes, recently discovered by Teo and Wan, we investigate the innermost stable circular orbit (ISCO) for massive particles and the circular orbit for massless particles moving around a spinning black hole binary.","We assume equal masses $M_1 = M_2=m$ and equal spin angular momenta $|J_1| = |J_2|$ for both black holes.","Firstly, we examine the case where two black holes are spinning in the same direction ($J_1=J_2$).","We clarify that that for particles rotating in the same direction as (opposite directions to) black holes' spin, the greater the spin angular momenta of the black holes, the more the radii of the ISCO for massive particles and the circular orbit for massless particles decrease (increase).","We show that distinct ISCO transitions occur for particles rotating in the same direction as the black holes in three ranges of spin angular momenta: $0<J_1/m^2=J_2/m^2< 0.395...$, $0.395...","<J_1/m^2=J_2/m^2< 0.483...$, and $0.483...<J_1/m^2=J_2/m^2<0.5$. Conversely",", particles rotating in the opposite direction to the black holes exhibit a consistent transition pattern for the case $0<J_1/m^2=J_2/m^2<0.5$. Secondly, we study the situation where binary black holes are spinning in opposite directions ($J_1=-J_2$).","We clarify that for large (small) separations between black holes, the ISCO appears near the black hole that is spinning in the same (opposite) direction as particles' rotation.","Additionally, we show that different ISCO transitions occur in the three angular momentum ranges: $0<J_1/m^2=-J_2/m^2< 0.160...$, $0.160...<J_1/m^2=-J_2/m^2< 0.467...$, and $0.467...<J_1/m^2=-J_2/m^2<0.5$."],"url":"http://arxiv.org/abs/2403.18533v1","category":"gr-qc"}
{"created":"2024-03-27 13:01:14","title":"Velocity Distribution of Dark Matter Spike around Schwarzschild Black Holes and Effects on Gravitational Waves from EMRIs","abstract":"Dark matter (DM) constitutes the predominant portion of matter in our universe. Despite compelling evidence, the precise characteristics of DM remain elusive. Among the leading DM candidates are weakly interacting massive particles, which may aggregate into steep concentrations around the central black holes of galaxies, forming dense spikes. Employing Schwarzschild geometry, we assess the density and velocity distribution of DM within such spikes. Through variations in black hole masses and dark halo parameters, we identify universal features in the density profile of DM and fit them with Gaussian distributions. Additionally, we investigate the impact of dynamical friction on gravitational waves (GWs) generated by extreme-mass-ratio inspirals (EMRIs) within DM spikes. Our findings uncover phase shifts in the time-domain waveform, potentially providing significant insights for the GW-based detection of DM in galactic centers.","sentences":["Dark matter (DM) constitutes the predominant portion of matter in our universe.","Despite compelling evidence, the precise characteristics of DM remain elusive.","Among the leading DM candidates are weakly interacting massive particles, which may aggregate into steep concentrations around the central black holes of galaxies, forming dense spikes.","Employing Schwarzschild geometry, we assess the density and velocity distribution of DM within such spikes.","Through variations in black hole masses and dark halo parameters, we identify universal features in the density profile of DM and fit them with Gaussian distributions.","Additionally, we investigate the impact of dynamical friction on gravitational waves (GWs) generated by extreme-mass-ratio inspirals (EMRIs) within DM spikes.","Our findings uncover phase shifts in the time-domain waveform, potentially providing significant insights for the GW-based detection of DM in galactic centers."],"url":"http://arxiv.org/abs/2403.18529v1","category":"astro-ph.GA"}
{"created":"2024-03-27 12:59:44","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP","abstract":"Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the scale and diversity of training data and language supervision play a key role in unlocking the compositional generalization abilities of vision-language models.","sentences":["Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts.","Recent studies attempted to investigate the leading cause of this capability.","In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes.","We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets.","We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M.","Our results provide evidence that the scale and diversity of training data and language supervision play a key role in unlocking the compositional generalization abilities of vision-language models."],"url":"http://arxiv.org/abs/2403.18525v1","category":"cs.CV"}
{"created":"2024-03-27 12:54:56","title":"Exact Solution of Bipartite Fluctuations in One-Dimensional Fermions","abstract":"Emergence of hydrodynamics in quantum many-body systems has recently garnered growing interest. The recent experiment of ultracold atoms [J. F. Wienand et al., arXiv:2306.11457] studied emergent hydrodynamics in hard-core bosons using a bipartite fluctuation, which quantifies how the particle number fluctuates in a subsystem. In this Letter, we theoretically study the variance of a bipartite fluctuation in one-dimensional noninteracting fermionic dynamics starting from an alternating state, deriving the exact solution of the variance and its asymptotic linear growth law for the long-time dynamics. To compare the theoretical prediction with the experiment, we generalize our exact solution by incorporating the incompleteness of the initial alternating state, deriving the general linear growth law analytically. We find that it quantitatively describes the experimentally observed variance growth without any fitting parameters.","sentences":["Emergence of hydrodynamics in quantum many-body systems has recently garnered growing interest.","The recent experiment of ultracold atoms [J. F. Wienand et al., arXiv:2306.11457] studied emergent hydrodynamics in hard-core bosons using a bipartite fluctuation, which quantifies how the particle number fluctuates in a subsystem.","In this Letter, we theoretically study the variance of a bipartite fluctuation in one-dimensional noninteracting fermionic dynamics starting from an alternating state, deriving the exact solution of the variance and its asymptotic linear growth law for the long-time dynamics.","To compare the theoretical prediction with the experiment, we generalize our exact solution by incorporating the incompleteness of the initial alternating state, deriving the general linear growth law analytically.","We find that it quantitatively describes the experimentally observed variance growth without any fitting parameters."],"url":"http://arxiv.org/abs/2403.18523v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-27 12:51:17","title":"Redirecting Flows -- Navigating the Future of the Amazon","abstract":"The Amazon Basin, and the Latin America and Caribbean (LAC) region more broadly, stands at a critical juncture, grappling with pressing environmental challenges while holding immense potential for transformative change through innovative solutions. This report illuminates the diverse landscape of social-ecological issues, technological advancements, community-led initiatives, and strategic actions that could help foster biosphere-based sustainability and resilience across the region.","sentences":["The Amazon Basin, and the Latin America and Caribbean (LAC) region more broadly, stands at a critical juncture, grappling with pressing environmental challenges while holding immense potential for transformative change through innovative solutions.","This report illuminates the diverse landscape of social-ecological issues, technological advancements, community-led initiatives, and strategic actions that could help foster biosphere-based sustainability and resilience across the region."],"url":"http://arxiv.org/abs/2403.18521v1","category":"econ.GN"}
{"created":"2024-03-27 12:50:58","title":"Global convergence of iterative solvers for problems of nonlinear magnetostatics","abstract":"We consider the convergence of iterative solvers for problems of nonlinear magnetostatics. Using the equivalence to an underlying minimization problem, we can establish global linear convergence of a large class of methods, including the damped Newton-method, fixed-point iteration, and the Kacanov iteration, which can all be interpreted as generalized gradient descent methods. Armijo backtracking isconsidered for an adaptive choice of the stepsize. The general assumptions required for our analysis cover inhomogeneous, nonlinear, and anisotropic materials, as well as permanent magnets. The main results are proven on the continuous level, but they carry over almost verbatim to various approximation schemes, including finite elements and isogeometric analysis, leading to bounds on the iteration numbers, which are independent of the particular discretization. The theoretical results are illustrated by numerical tests for a typical benchmark problem.","sentences":["We consider the convergence of iterative solvers for problems of nonlinear magnetostatics.","Using the equivalence to an underlying minimization problem, we can establish global linear convergence of a large class of methods, including the damped Newton-method, fixed-point iteration, and the Kacanov iteration, which can all be interpreted as generalized gradient descent methods.","Armijo backtracking isconsidered for an adaptive choice of the stepsize.","The general assumptions required for our analysis cover inhomogeneous, nonlinear, and anisotropic materials, as well as permanent magnets.","The main results are proven on the continuous level, but they carry over almost verbatim to various approximation schemes, including finite elements and isogeometric analysis, leading to bounds on the iteration numbers, which are independent of the particular discretization.","The theoretical results are illustrated by numerical tests for a typical benchmark problem."],"url":"http://arxiv.org/abs/2403.18520v1","category":"math.NA"}
{"created":"2024-03-27 12:50:27","title":"Improving Line Search Methods for Large Scale Neural Network Training","abstract":"In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which provides a hyperparameter free Pytorch optimizer.","sentences":["In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule.","In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness.","We test these methods on larger datasets and more complex data domains than before.","Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods.","Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam.","Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data.","Our work is publicly available as a Python package, which provides a hyperparameter free Pytorch optimizer."],"url":"http://arxiv.org/abs/2403.18519v1","category":"cs.LG"}
{"created":"2024-03-27 12:49:53","title":"Non-empirical prediction of the length-dependent ionization potential in molecular chains","abstract":"The ionization potential of molecular chains is well-known to be a tunable nano-scale property that exhibits clear quantum confinement effects. State-of-the-art methods can accurately predict the ionization potential in the small molecule limit and in the solid-state limit, but for intermediate, nano-sized systems prediction of the evolution of the electronic structure between the two limits is more difficult. Recently, optimal tuning of range-separated hybrid functionals has emerged as a highly accurate method for predicting ionization potentials. This was first achieved for molecules using the ionization potential theorem (IPT) and more recently extended to solid-state systems, based on an \\textit{ansatz} that generalizes the IPT to the removal of charge from a localized Wannier function. Here, we study one-dimensional molecular chains of increasing size, from the monomer limit to the infinite polymer limit using this approach. By comparing our results with other localization-based methods and where available with experiment, we demonstrate that Wannier-localization-based optimal tuning is highly accurate in predicting ionization potentials for any chain length, including the nano-scale regime.","sentences":["The ionization potential of molecular chains is well-known to be a tunable nano-scale property that exhibits clear quantum confinement effects.","State-of-the-art methods can accurately predict the ionization potential in the small molecule limit and in the solid-state limit, but for intermediate, nano-sized systems prediction of the evolution of the electronic structure between the two limits is more difficult.","Recently, optimal tuning of range-separated hybrid functionals has emerged as a highly accurate method for predicting ionization potentials.","This was first achieved for molecules using the ionization potential theorem (IPT) and more recently extended to solid-state systems, based on an \\textit{ansatz} that generalizes the IPT to the removal of charge from a localized Wannier function.","Here, we study one-dimensional molecular chains of increasing size, from the monomer limit to the infinite polymer limit using this approach.","By comparing our results with other localization-based methods and where available with experiment, we demonstrate that Wannier-localization-based optimal tuning is highly accurate in predicting ionization potentials for any chain length, including the nano-scale regime."],"url":"http://arxiv.org/abs/2403.18518v1","category":"physics.chem-ph"}
{"created":"2024-03-27 12:49:14","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models","abstract":"Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyperparameters, and to design balancing strategies to enhance the convergence speed of dedicated optimization algorithms. Some of these results were already known but restricted to specific instances of regularized low-rank approximations. We also derive a generic Majorization Minimization algorithm that handles many regularized nonnegative low-rank approximations, with convergence guarantees. We showcase our contributions on sparse Nonnegative Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and sparse Nonnegative Tucker Decomposition.","sentences":["Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability.","However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices.","This paper aims at improving upon these issues.","By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects.","This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyperparameters, and to design balancing strategies to enhance the convergence speed of dedicated optimization algorithms.","Some of these results were already known but restricted to specific instances of regularized low-rank approximations.","We also derive a generic Majorization Minimization algorithm that handles many regularized nonnegative low-rank approximations, with convergence guarantees.","We showcase our contributions on sparse Nonnegative Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and sparse Nonnegative Tucker Decomposition."],"url":"http://arxiv.org/abs/2403.18517v1","category":"cs.LG"}
{"created":"2024-03-27 12:45:12","title":"Autonomous Quantum Heat Engine Based on Non-Markovian Dynamics of an Optomechanical Hamiltonian","abstract":"We propose a recipe for demonstrating an autonomous quantum heat engine where the working fluid consists of a harmonic oscillator, the frequency of which is tuned by a driving mode. The working fluid is coupled two heat reservoirs each exhibiting a peaked power spectrum, a hot reservoir peaked at a higher frequency than the cold reservoir. Provided that the driving mode is initialized in a coherent state with a high enough amplitude and the parameters of the utilized optomechanical Hamiltonian and the reservoirs are appropriate, the driving mode induces an approximate Otto cycle for the working fluid and consequently its oscillation amplitude begins to increase in time. We build both an analytical and a non-Markovian quasiclassical model for this quantum heat engine and show that reasonably powerful coherent fields can be generated as the output of the quantum heat engine. This general theoretical proposal heralds the in-depth studies of quantum heat engines in the non-Markovian regime. Further, it paves the way for specific physical realizations, such as those in optomechanical systems, and for the subsequent experimental realization of an autonomous quantum heat engine.","sentences":["We propose a recipe for demonstrating an autonomous quantum heat engine where the working fluid consists of a harmonic oscillator, the frequency of which is tuned by a driving mode.","The working fluid is coupled two heat reservoirs each exhibiting a peaked power spectrum, a hot reservoir peaked at a higher frequency than the cold reservoir.","Provided that the driving mode is initialized in a coherent state with a high enough amplitude and the parameters of the utilized optomechanical Hamiltonian and the reservoirs are appropriate, the driving mode induces an approximate Otto cycle for the working fluid and consequently its oscillation amplitude begins to increase in time.","We build both an analytical and a non-Markovian quasiclassical model for this quantum heat engine and show that reasonably powerful coherent fields can be generated as the output of the quantum heat engine.","This general theoretical proposal heralds the in-depth studies of quantum heat engines in the non-Markovian regime.","Further, it paves the way for specific physical realizations, such as those in optomechanical systems, and for the subsequent experimental realization of an autonomous quantum heat engine."],"url":"http://arxiv.org/abs/2403.18515v1","category":"quant-ph"}
{"created":"2024-03-27 12:41:30","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","abstract":"We study a challenging task: text-to-motion synthesis, aiming to generate motions that align with textual descriptions and exhibit coordinated movements. Currently, the part-based methods introduce part partition into the motion synthesis process to achieve finer-grained generation. However, these methods encounter challenges such as the lack of coordination between different part motions and difficulties for networks to understand part concepts. Moreover, introducing finer-grained part concepts poses computational complexity challenges. In this paper, we propose Part-Coordinating Text-to-Motion Synthesis (ParCo), endowed with enhanced capabilities for understanding part motions and communication among different part motion generators, ensuring a coordinated and fined-grained motion synthesis. Specifically, we discretize whole-body motion into multiple part motions to establish the prior concept of different parts. Afterward, we employ multiple lightweight generators designed to synthesize different part motions and coordinate them through our part coordination module. Our approach demonstrates superior performance on common benchmarks with economic computations, including HumanML3D and KIT-ML, providing substantial evidence of its effectiveness. Code is available at https://github.com/qrzou/ParCo .","sentences":["We study a challenging task: text-to-motion synthesis, aiming to generate motions that align with textual descriptions and exhibit coordinated movements.","Currently, the part-based methods introduce part partition into the motion synthesis process to achieve finer-grained generation.","However, these methods encounter challenges such as the lack of coordination between different part motions and difficulties for networks to understand part concepts.","Moreover, introducing finer-grained part concepts poses computational complexity challenges.","In this paper, we propose Part-Coordinating Text-to-Motion Synthesis (ParCo), endowed with enhanced capabilities for understanding part motions and communication among different part motion generators, ensuring a coordinated and fined-grained motion synthesis.","Specifically, we discretize whole-body motion into multiple part motions to establish the prior concept of different parts.","Afterward, we employ multiple lightweight generators designed to synthesize different part motions and coordinate them through our part coordination module.","Our approach demonstrates superior performance on common benchmarks with economic computations, including HumanML3D and KIT-ML, providing substantial evidence of its effectiveness.","Code is available at https://github.com/qrzou/ParCo ."],"url":"http://arxiv.org/abs/2403.18512v1","category":"cs.CV"}
{"created":"2024-03-27 12:40:00","title":"Bolzano's Conjecture: Measuring the Numerosity of Infinite Sets","abstract":"Bolzano and Cantor were the first mathematicians to make significant attempts to measure the size (numerosity) of different infinite collections. They differed in their methodological approaches, with Cantor's prevailing. This led to the foundation of the theory of sets as well as Cantor's transfinite arithmetic. This paper argues that Bolzano's conjecture is correct and that Euclid's principle, 'that the whole is greater than a part', should be considered as a necessary condition for the quantification of infinite sets (rather than bijection). Cantor had concluded that the rational and the algebraic numbers were of the same size as the natural numbers, whilst, in contrast, the real numbers were a larger set. Using Cantor's methods it is shown in this paper that the rational numbers are of larger size than the natural numbers, thus showing that bijection is not a reliable measure of the size of infinite sets. It is also concluded, using mathematical induction, that different 'countably' infinite sets can have various different sizes. The implications for theorems using bijection as a measure of size is then briefly discussed. There already exist new methods of measuring numerosity, based on Euclid's principle, which may develop a consistent system of infinite arithmetic.","sentences":["Bolzano and Cantor were the first mathematicians to make significant attempts to measure the size (numerosity) of different infinite collections.","They differed in their methodological approaches, with Cantor's prevailing.","This led to the foundation of the theory of sets as well as Cantor's transfinite arithmetic.","This paper argues that Bolzano's conjecture is correct and that Euclid's principle, 'that the whole is greater than a part', should be considered as a necessary condition for the quantification of infinite sets (rather than bijection).","Cantor had concluded that the rational and the algebraic numbers were of the same size as the natural numbers, whilst, in contrast, the real numbers were a larger set.","Using Cantor's methods it is shown in this paper that the rational numbers are of larger size than the natural numbers, thus showing that bijection is not a reliable measure of the size of infinite sets.","It is also concluded, using mathematical induction, that different 'countably' infinite sets can have various different sizes.","The implications for theorems using bijection as a measure of size is then briefly discussed.","There already exist new methods of measuring numerosity, based on Euclid's principle, which may develop a consistent system of infinite arithmetic."],"url":"http://arxiv.org/abs/2403.18511v1","category":"math.GM"}
{"created":"2024-03-27 12:39:50","title":"Observation of vortices in a dipolar supersolid","abstract":"Supersolids are states of matter that spontaneously break two continuous symmetries: translational invariance due to the appearance of a crystal structure and phase invariance due to phase locking of single-particle wave functions, responsible for superfluid phenomena. While originally predicted to be present in solid helium, ultracold quantum gases provided a first platform to observe supersolids, with particular success coming from dipolar atoms. Phase locking in dipolar supersolids has been probed through e.g. measurements of the phase coherence and gapless Goldstone modes, but quantized vortices, a hydrodynamic fingerprint of superfluidity, have not yet been observed. Here, with the prerequisite pieces at our disposal, namely a method to generate vortices in dipolar gases and supersolids with two-dimensional crystalline order, we report on the theoretical investigation and experimental observation of vortices in the supersolid phase. Our work reveals a fundamental difference in vortex seeding dynamics between unmodulated and modulated quantum fluids. This opens the door to study the hydrodynamic properties of exotic quantum systems with multiple spontaneously broken symmetries, in disparate domains such as quantum crystals and neutron stars.","sentences":["Supersolids are states of matter that spontaneously break two continuous symmetries: translational invariance due to the appearance of a crystal structure and phase invariance due to phase locking of single-particle wave functions, responsible for superfluid phenomena.","While originally predicted to be present in solid helium, ultracold quantum gases provided a first platform to observe supersolids, with particular success coming from dipolar atoms.","Phase locking in dipolar supersolids has been probed through e.g. measurements of the phase coherence and gapless Goldstone modes, but quantized vortices, a hydrodynamic fingerprint of superfluidity, have not yet been observed.","Here, with the prerequisite pieces at our disposal, namely a method to generate vortices in dipolar gases and supersolids with two-dimensional crystalline order, we report on the theoretical investigation and experimental observation of vortices in the supersolid phase.","Our work reveals a fundamental difference in vortex seeding dynamics between unmodulated and modulated quantum fluids.","This opens the door to study the hydrodynamic properties of exotic quantum systems with multiple spontaneously broken symmetries, in disparate domains such as quantum crystals and neutron stars."],"url":"http://arxiv.org/abs/2403.18510v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-27 12:35:23","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","abstract":"Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.","sentences":["Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures","[1], [2].","In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing.","More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units.","Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases.","Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures."],"url":"http://arxiv.org/abs/2403.18506v1","category":"cs.LG"}
{"created":"2024-03-27 12:24:20","title":"HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with Dual-Branch Pix2pix Generator","abstract":"Computational analysis of multiplexed immunofluorescence histology data is emerging as an important method for understanding the tumour micro-environment in cancer. This work presents HEMIT, a dataset designed for translating Hematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC) images, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC images are multi-component and cellular-level aligned with H&E, enriching supervised stain translation tasks. To our knowledge, HEMIT is the first publicly available cellular-level aligned dataset that enables H&E to multi-target mIHC image translation. This dataset provides the computer vision community with a valuable resource to develop novel computational methods which have the potential to gain new insights from H&E slide archives.   We also propose a new dual-branch generator architecture, using residual Convolutional Neural Networks (CNNs) and Swin Transformers which achieves better translation outcomes than other popular algorithms. When evaluated on HEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the highest overall score on key metrics including the Structural Similarity Index Measure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio (PSNR). Additionally, downstream analysis has been used to further validate the quality of the generated mIHC images. These results set a new benchmark in the field of stain translation tasks.","sentences":["Computational analysis of multiplexed immunofluorescence histology data is emerging as an important method for understanding the tumour micro-environment in cancer.","This work presents HEMIT, a dataset designed for translating Hematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC) images, featuring DAPI, CD3, and panCK markers.","Distinctively, HEMIT's mIHC images are multi-component and cellular-level aligned with H&E, enriching supervised stain translation tasks.","To our knowledge, HEMIT is the first publicly available cellular-level aligned dataset that enables H&E to multi-target mIHC image translation.","This dataset provides the computer vision community with a valuable resource to develop novel computational methods which have the potential to gain new insights from H&E slide archives.   ","We also propose a new dual-branch generator architecture, using residual Convolutional Neural Networks (CNNs) and Swin Transformers which achieves better translation outcomes than other popular algorithms.","When evaluated on HEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the highest overall score on key metrics including the Structural Similarity Index Measure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio (PSNR).","Additionally, downstream analysis has been used to further validate the quality of the generated mIHC images.","These results set a new benchmark in the field of stain translation tasks."],"url":"http://arxiv.org/abs/2403.18501v1","category":"eess.IV"}
{"created":"2024-03-27 12:20:42","title":"Mechanisms of THz Radiation in Laser-Plasma Interactions","abstract":"The exploration of Terahertz (THz) waves has captivated researchers across diverse scientific disciplines such as physics, spectroscopy, chemistry, biology, and engineering, driven by the myriad applications these waves offer. Within this expansive landscape, the development of efficient and reliable THz sources stands as a paramount objective. In the pursuit of this goal, a multitude of approaches have been undertaken, with a notable contender emerging in the form of laser-induced plasma. Harnessing the advancements in ultrafast pulses, laser-induced plasma has proven to be a promising tool for generating THz waves. Its appeal lies in the robust attributes of a high power threshold, intense THz signal, and an broadband THz spectrum. This paper delves into a comprehensive review of the physics and progress underlying THz generation from laser-induced plasmas, exploring scenarios where plasmas are induced in gases, liquids, and solids. The interactions between lasers and plasmas involve complex physical processes, resulting in a variety of laser plasma scenarios for THz generation. In this review, the focus is specifically placed on classifying THz generation based on different physical mechanisms and also examines the characteristics of the emitted THz waves. By categorizing the processes, a deeper understanding of the underlying principles can be attained.","sentences":["The exploration of Terahertz (THz) waves has captivated researchers across diverse scientific disciplines such as physics, spectroscopy, chemistry, biology, and engineering, driven by the myriad applications these waves offer.","Within this expansive landscape, the development of efficient and reliable THz sources stands as a paramount objective.","In the pursuit of this goal, a multitude of approaches have been undertaken, with a notable contender emerging in the form of laser-induced plasma.","Harnessing the advancements in ultrafast pulses, laser-induced plasma has proven to be a promising tool for generating THz waves.","Its appeal lies in the robust attributes of a high power threshold, intense THz signal, and an broadband THz spectrum.","This paper delves into a comprehensive review of the physics and progress underlying THz generation from laser-induced plasmas, exploring scenarios where plasmas are induced in gases, liquids, and solids.","The interactions between lasers and plasmas involve complex physical processes, resulting in a variety of laser plasma scenarios for THz generation.","In this review, the focus is specifically placed on classifying THz generation based on different physical mechanisms and also examines the characteristics of the emitted THz waves.","By categorizing the processes, a deeper understanding of the underlying principles can be attained."],"url":"http://arxiv.org/abs/2403.18499v1","category":"physics.plasm-ph"}
{"created":"2024-03-27 12:19:57","title":"Global Representation Ring and Knutson Index","abstract":"We follow on from the pioneering paper by the second author where he introduced the Knutson index for the character ring of a finite group. We introduce the Knutson index for general commutative rings. Then we study it for Burnside rings and global representation rings. We also introduce the global table of a finite group, that encompasses both the character table and the Burnside table of marks.","sentences":["We follow on from the pioneering paper by the second author where he introduced the Knutson index for the character ring of a finite group.","We introduce the Knutson index for general commutative rings.","Then we study it for Burnside rings and global representation rings.","We also introduce the global table of a finite group, that encompasses both the character table and the Burnside table of marks."],"url":"http://arxiv.org/abs/2403.18498v1","category":"math.RA"}
{"created":"2024-03-27 12:18:10","title":"Nijenhuis deformations of Poisson algebras and $F$-manifold algebras","abstract":"The notion of pre-Poisson algebras was introduced by Aguiar in his study of zinbiel algebras and pre-Lie algebras. In this paper, we first introduce NS-Poisson algebras as a generalization of both Poisson algebras and pre-Poisson algebras. An NS-Poisson algebra has an associated sub-adjacent Poisson algebra. We show that a Nijenhuis operator on a Poisson algebra deforms the structure into an NS-Poisson algebra. The semi-classical limit of an NS-algebra deformation and a suitable filtration of an NS-algebra produce NS-Poisson algebras. On the other hand, $F$-manifold algebras were introduced by Dotsenko as the underlying algebraic structure of $F$-manifolds. We also introduce NS-$F$-manifold algebras as a simultaneous generalization of NS-Poisson algebras, $F$-manifold algebras and pre-$F$-manifold algebras. In the end, we show that Nijenhuis deformations of $F$-manifold algebras and the semi-classical limits of NS-pre-Lie algebra deformations have NS-$F$-manifold algebra structures.","sentences":["The notion of pre-Poisson algebras was introduced by Aguiar in his study of zinbiel algebras and pre-Lie algebras.","In this paper, we first introduce NS-Poisson algebras as a generalization of both Poisson algebras and pre-Poisson algebras.","An NS-Poisson algebra has an associated sub-adjacent Poisson algebra.","We show that a Nijenhuis operator on a Poisson algebra deforms the structure into an NS-Poisson algebra.","The semi-classical limit of an NS-algebra deformation and a suitable filtration of an NS-algebra produce NS-Poisson algebras.","On the other hand, $F$-manifold algebras were introduced by Dotsenko as the underlying algebraic structure of $F$-manifolds.","We also introduce NS-$F$-manifold algebras as a simultaneous generalization of NS-Poisson algebras, $F$-manifold algebras and pre-$F$-manifold algebras.","In the end, we show that Nijenhuis deformations of $F$-manifold algebras and the semi-classical limits of NS-pre-Lie algebra deformations have NS-$F$-manifold algebra structures."],"url":"http://arxiv.org/abs/2403.18496v1","category":"math.RA"}
{"created":"2024-03-27 12:10:30","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","abstract":"We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion\", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization.","sentences":["We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives.","By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion\", characterized by equilibrium in the learning rates and homogeneous gradients.","This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence.","We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization.","We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss.","Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization."],"url":"http://arxiv.org/abs/2403.18494v1","category":"cs.LG"}
{"created":"2024-03-27 12:08:41","title":"VersaT2I: Improving Text-to-Image Models with Versatile Reward","abstract":"Recent text-to-image (T2I) models have benefited from large-scale and high-quality data, demonstrating impressive performance. However, these T2I models still struggle to produce images that are aesthetically pleasing, geometrically accurate, faithful to text, and of good low-level quality. We present VersaT2I, a versatile training framework that can boost the performance with multiple rewards of any T2I model. We decompose the quality of the image into several aspects such as aesthetics, text-image alignment, geometry, low-level quality, etc. Then, for every quality aspect, we select high-quality images in this aspect generated by the model as the training set to finetune the T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a gating function to combine multiple quality aspects, which can avoid conflicts between different quality aspects. Our method is easy to extend and does not require any manual annotation, reinforcement learning, or model architecture changes. Extensive experiments demonstrate that VersaT2I outperforms the baseline methods across various quality criteria.","sentences":["Recent text-to-image (T2I) models have benefited from large-scale and high-quality data, demonstrating impressive performance.","However, these T2I models still struggle to produce images that are aesthetically pleasing, geometrically accurate, faithful to text, and of good low-level quality.","We present VersaT2I, a versatile training framework that can boost the performance with multiple rewards of any T2I model.","We decompose the quality of the image into several aspects such as aesthetics, text-image alignment, geometry, low-level quality, etc.","Then, for every quality aspect, we select high-quality images in this aspect generated by the model as the training set to finetune the T2I model using the Low-Rank Adaptation (LoRA).","Furthermore, we introduce a gating function to combine multiple quality aspects, which can avoid conflicts between different quality aspects.","Our method is easy to extend and does not require any manual annotation, reinforcement learning, or model architecture changes.","Extensive experiments demonstrate that VersaT2I outperforms the baseline methods across various quality criteria."],"url":"http://arxiv.org/abs/2403.18493v1","category":"cs.CV"}
{"created":"2024-03-27 12:01:51","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models","abstract":"Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop. The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature. To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR). One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days. A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty. For this reason, several ET0 estimation models using machine and deep learning were developed and presented in the literature, that use as input features a reduced set of carefully selected weather parameters, that are compatible with common freely available WFSs. However, most studies on this topic have only evaluated model performance using data from weather stations (WSs), without considering the effect of using weather forecast data. In this study, the performance of authors' previous models is evaluated when using weather forecast data from two online WFSs, in the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii) estimate SR by ANN model, and then use that estimation for ET0 computation, using the FAO56-PM method. Employing data collected from two WFSs and a WS located in Vale do Lobo, Portugal, the latter approach achieved the best result, with a coefficient of determination (R2) ranging between 0.893 and 0.667, when considering forecasts up to 15 days.","sentences":["Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop.","The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature.","To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR).","One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days.","A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty.","For this reason, several ET0 estimation models using machine and deep learning were developed and presented in the literature, that use as input features a reduced set of carefully selected weather parameters, that are compatible with common freely available WFSs.","However, most studies on this topic have only evaluated model performance using data from weather stations (WSs), without considering the effect of using weather forecast data.","In this study, the performance of authors' previous models is evaluated when using weather forecast data from two online WFSs, in the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii) estimate SR by ANN model, and then use that estimation for ET0 computation, using the FAO56-PM method.","Employing data collected from two WFSs and a WS located in Vale do Lobo, Portugal, the latter approach achieved the best result, with a coefficient of determination (R2) ranging between 0.893 and 0.667, when considering forecasts up to 15 days."],"url":"http://arxiv.org/abs/2403.18489v1","category":"cs.AI"}
{"created":"2024-03-27 11:58:45","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models","abstract":"Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.","sentences":["Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models.","While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data.","To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data.","In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples.","The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class."],"url":"http://arxiv.org/abs/2403.18486v1","category":"cs.LG"}
{"created":"2024-03-27 11:58:04","title":"Stellar wind bubbles of OB stars as Galactic cosmic-ray re-accelerators","abstract":"Cosmic rays are highly energetic messengers propagating in magnetized plasma, which are, possibly but not exclusively, accelerated at astrophysical shocks. Amongst the variety of astrophysical objects presenting shocks, the huge circumstellar stellar wind bubbles forming around very massive stars, are potential non thermal emitters. We present the 1D magnetohydrodynamical simulation of the evolving magnetized surroundings of a single, OB type main sequence 60 Mo star, which is post processed to calculate the re-acceleration of preexisting non-thermal particles of the Galactic cosmic ray background. It is found that the forward shock of such circumstellar bubble can, during the early phase (1 Myr) of its expansion, act as a substantial reaccelerator of pre existing interstellar cosmic rays. This results in an increasing excess emission flux by a factor of 5, the hadronic component producing gamma-rays by pion0 decay being more important than those by synchrotron and inverse Compton radiation mechanisms. We propose that this effect is at work in the circumstellar environments of massive stars in general and we conjecture that other nebulae such as the stellar wind bow shocks of runaway massive stars also act as Galactic cosmic-ray re-accelerators. Particularly, this study supports the interpretation of the enhanced hadronic emission flux measured from the surroundings of kappa Ori as originating from the acceleration of pre-existing particles at the forward shock of its wind bubble.","sentences":["Cosmic rays are highly energetic messengers propagating in magnetized plasma, which are, possibly but not exclusively, accelerated at astrophysical shocks.","Amongst the variety of astrophysical objects presenting shocks, the huge circumstellar stellar wind bubbles forming around very massive stars, are potential non thermal emitters.","We present the 1D magnetohydrodynamical simulation of the evolving magnetized surroundings of a single, OB type main sequence 60 Mo star, which is post processed to calculate the re-acceleration of preexisting non-thermal particles of the Galactic cosmic ray background.","It is found that the forward shock of such circumstellar bubble can, during the early phase (1 Myr) of its expansion, act as a substantial reaccelerator of pre existing interstellar cosmic rays.","This results in an increasing excess emission flux by a factor of 5, the hadronic component producing gamma-rays by pion0 decay being more important than those by synchrotron and inverse Compton radiation mechanisms.","We propose that this effect is at work in the circumstellar environments of massive stars in general and we conjecture that other nebulae such as the stellar wind bow shocks of runaway massive stars also act as Galactic cosmic-ray re-accelerators.","Particularly, this study supports the interpretation of the enhanced hadronic emission flux measured from the surroundings of kappa Ori as originating from the acceleration of pre-existing particles at the forward shock of its wind bubble."],"url":"http://arxiv.org/abs/2403.18484v1","category":"astro-ph.HE"}
{"created":"2024-03-27 11:56:08","title":"UVL Sentinel: a tool for parsing and syntactic correction of UVL datasets","abstract":"Feature models have become a de facto standard for representing variability in software product lines. UVL (Universal Variability Language) is a language which expresses the features, dependencies, and constraints between them. This language is written in plain text and follows a syntactic structure that needs to be processed by a parser. This parser is software with specific syntactic rules that the language must comply with to be processed correctly. Researchers have datasets with numerous feature models. The language description form of these feature models is tied to a version of the parser language. When the parser is updated to support new features or correct previous ones, these feature models are often no longer compatible, generating incompatibilities and inconsistency within the dataset. In this paper, we present UVL Sentinel. This tool analyzes a dataset of feature models in UVL format, generating error analysis reports, describing those errors and, eventually, a syntactic processing that applies the most common solutions. This tool can detect the incompatibilities of the feature models of a dataset when the parser is updated and tries to correct the most common syntactic errors, facilitating the management of the dataset and the adaptation of their models to the new version of the parser. Our tool was evaluated using a dataset of 1,479 UVL models from different sources and helped semi-automatically fix 185 warnings and syntax errors.","sentences":["Feature models have become a de facto standard for representing variability in software product lines.","UVL (Universal Variability Language) is a language which expresses the features, dependencies, and constraints between them.","This language is written in plain text and follows a syntactic structure that needs to be processed by a parser.","This parser is software with specific syntactic rules that the language must comply with to be processed correctly.","Researchers have datasets with numerous feature models.","The language description form of these feature models is tied to a version of the parser language.","When the parser is updated to support new features or correct previous ones, these feature models are often no longer compatible, generating incompatibilities and inconsistency within the dataset.","In this paper, we present UVL Sentinel.","This tool analyzes a dataset of feature models in UVL format, generating error analysis reports, describing those errors and, eventually, a syntactic processing that applies the most common solutions.","This tool can detect the incompatibilities of the feature models of a dataset when the parser is updated and tries to correct the most common syntactic errors, facilitating the management of the dataset and the adaptation of their models to the new version of the parser.","Our tool was evaluated using a dataset of 1,479 UVL models from different sources and helped semi-automatically fix 185 warnings and syntax errors."],"url":"http://arxiv.org/abs/2403.18482v1","category":"cs.SE"}
{"created":"2024-03-27 11:49:58","title":"Enhanced Generative Recommendation via Content and Collaboration Integration","abstract":"Generative recommendation has emerged as a promising paradigm aimed at augmenting recommender systems with recent advancements in generative artificial intelligence. This task has been formulated as a sequence-to-sequence generation process, wherein the input sequence encompasses data pertaining to the user's previously interacted items, and the output sequence denotes the generative identifier for the suggested item. However, existing generative recommendation approaches still encounter challenges in (i) effectively integrating user-item collaborative signals and item content information within a unified generative framework, and (ii) executing an efficient alignment between content information and collaborative signals.   In this paper, we introduce content-based collaborative generation for recommender systems, denoted as ColaRec. To capture collaborative signals, the generative item identifiers are derived from a pretrained collaborative filtering model, while the user is represented through the aggregation of interacted items' content. Subsequently, the aggregated textual description of items is fed into a language model to encapsulate content information. This integration enables ColaRec to amalgamate collaborative signals and content information within an end-to-end framework. Regarding the alignment, we propose an item indexing task to facilitate the mapping between the content-based semantic space and the interaction-based collaborative space. Additionally, a contrastive loss is introduced to ensure that items with similar collaborative GIDs possess comparable content representations, thereby enhancing alignment. To validate the efficacy of ColaRec, we conduct experiments on three benchmark datasets. Empirical results substantiate the superior performance of ColaRec.","sentences":["Generative recommendation has emerged as a promising paradigm aimed at augmenting recommender systems with recent advancements in generative artificial intelligence.","This task has been formulated as a sequence-to-sequence generation process, wherein the input sequence encompasses data pertaining to the user's previously interacted items, and the output sequence denotes the generative identifier for the suggested item.","However, existing generative recommendation approaches still encounter challenges in (i) effectively integrating user-item collaborative signals and item content information within a unified generative framework, and (ii) executing an efficient alignment between content information and collaborative signals.   ","In this paper, we introduce content-based collaborative generation for recommender systems, denoted as ColaRec.","To capture collaborative signals, the generative item identifiers are derived from a pretrained collaborative filtering model, while the user is represented through the aggregation of interacted items' content.","Subsequently, the aggregated textual description of items is fed into a language model to encapsulate content information.","This integration enables ColaRec to amalgamate collaborative signals and content information within an end-to-end framework.","Regarding the alignment, we propose an item indexing task to facilitate the mapping between the content-based semantic space and the interaction-based collaborative space.","Additionally, a contrastive loss is introduced to ensure that items with similar collaborative GIDs possess comparable content representations, thereby enhancing alignment.","To validate the efficacy of ColaRec, we conduct experiments on three benchmark datasets.","Empirical results substantiate the superior performance of ColaRec."],"url":"http://arxiv.org/abs/2403.18480v1","category":"cs.IR"}
{"created":"2024-03-27 11:49:03","title":"A consistent kinetic Fokker-Planck model for gas mixtures","abstract":"Rarefied gas dynamics is usually described by the Boltzmann equation. Unfortunately, the expense of evaluating this operator can be very prohibitive. This made it worthwhile to look for approximations that convey essentially an equivalent amount of physical information. One widely known approximative collision operator is the Bathnagar-Gross-Krook (BGK) operator. However, recently, the Foker-Planck approximation has become increasingly popular. Nevertheless, the modeling of gas mixtures in the context of the kinetic Fokker-Planck equation has so far only been addressed in a very few papers. In this paper, we propose a general multi-species Fokker-Planck model. We prove consistency of our model: conservation properties, positivity of all temperatures, H-Theorem and the shape of equilibrium as Maxwell distributions with the same mean velocity and temperature. Moreover, we derive the usual macroscopic equations.","sentences":["Rarefied gas dynamics is usually described by the Boltzmann equation.","Unfortunately, the expense of evaluating this operator can be very prohibitive.","This made it worthwhile to look for approximations that convey essentially an equivalent amount of physical information.","One widely known approximative collision operator is the Bathnagar-Gross-Krook (BGK) operator.","However, recently, the Foker-Planck approximation has become increasingly popular.","Nevertheless, the modeling of gas mixtures in the context of the kinetic Fokker-Planck equation has so far only been addressed in a very few papers.","In this paper, we propose a general multi-species Fokker-Planck model.","We prove consistency of our model: conservation properties, positivity of all temperatures, H-Theorem and the shape of equilibrium as Maxwell distributions with the same mean velocity and temperature.","Moreover, we derive the usual macroscopic equations."],"url":"http://arxiv.org/abs/2403.18478v1","category":"math.AP"}
{"created":"2024-03-27 11:37:59","title":"DMRG-tailored coupled cluster method in the 4c-relativistic domain: General implementation and application to the NUHFI and NUF$_3$ molecules","abstract":"Heavy atom compounds represent a challenge for computational chemistry, due to the need for simultaneous treatment of relativistic and correlation effects. Often such systems exhibit also strong correlation which hampers the application of perturbation theory or single-reference coupled cluster (CC) methods. As a viable alternative, we have proposed to externally correct the CC method using the density matrix renormalization group (DMRG) wave functions, yielding the DMRG-tailored CC method. In a previous paper [J. Chem. Phys. {\\bf 152}, 174107 (2020)] we have reported a first implementation of this method in the relativistic context, which was restricted to molecules with real double group symmetry. In this work we present a fully general implementation of the method, covering complex and quaternion double groups as well. The 4c-TCC method thus becomes applicable to polyatomic molecules including heavy atoms. For assessment of the method, we performed calculations of the chiral uranium compound NUHFI, which was previously studied in the context of the enhancement of parity violation effects. In particular, we performed calculations of a cut of the potential energy surface of this molecule along the dissociation of the N-U bond, where the system exhibits a strong multireference character. Since there are no experimental data for NUHFI, we have performed also an analogous study of the (more symmetric) NUF$_3$ molecule, where the vibrational frequency of the N-U bond can be compared with spectroscopic data.","sentences":["Heavy atom compounds represent a challenge for computational chemistry, due to the need for simultaneous treatment of relativistic and correlation effects.","Often such systems exhibit also strong correlation which hampers the application of perturbation theory or single-reference coupled cluster (CC) methods.","As a viable alternative, we have proposed to externally correct the CC method using the density matrix renormalization group (DMRG) wave functions, yielding the DMRG-tailored CC method.","In a previous paper [J. Chem. Phys. {\\bf 152}, 174107 (2020)] we have reported a first implementation of this method in the relativistic context, which was restricted to molecules with real double group symmetry.","In this work we present a fully general implementation of the method, covering complex and quaternion double groups as well.","The 4c-TCC method thus becomes applicable to polyatomic molecules including heavy atoms.","For assessment of the method, we performed calculations of the chiral uranium compound NUHFI, which was previously studied in the context of the enhancement of parity violation effects.","In particular, we performed calculations of a cut of the potential energy surface of this molecule along the dissociation of the N-U bond, where the system exhibits a strong multireference character.","Since there are no experimental data for NUHFI, we have performed also an analogous study of the (more symmetric) NUF$_3$ molecule, where the vibrational frequency of the N-U bond can be compared with spectroscopic data."],"url":"http://arxiv.org/abs/2403.18473v1","category":"physics.chem-ph"}
{"created":"2024-03-27 11:32:53","title":"Computational decomposition and composition technique for approximate solution of nonstationary problems","abstract":"Stable computational algorithms for the approximate solution of the Cauchy problem for nonstationary problems are based on implicit time approximations. Computational costs for boundary value problems for systems of coupled multidimensional equations can be reduced by additive decomposition of the problem operator(s) and composition of the approximate solution using particular explicit-implicit time approximations. Such a technique is currently applied in conditions where the decomposition step is uncomplicated. A general approach is proposed to construct decomposition-composition algorithms for evolution equations in finite-dimensional Hilbert spaces. It is based on two main variants of the decomposition of the unit operator in the corresponding spaces at the decomposition stage and the application of additive operator-difference schemes at the composition stage. The general results are illustrated on the boundary value problem for a second-order parabolic equation by constructing standard splitting schemes on spatial variables and region-additive schemes (domain decomposition schemes).","sentences":["Stable computational algorithms for the approximate solution of the Cauchy problem for nonstationary problems are based on implicit time approximations.","Computational costs for boundary value problems for systems of coupled multidimensional equations can be reduced by additive decomposition of the problem operator(s) and composition of the approximate solution using particular explicit-implicit time approximations.","Such a technique is currently applied in conditions where the decomposition step is uncomplicated.","A general approach is proposed to construct decomposition-composition algorithms for evolution equations in finite-dimensional Hilbert spaces.","It is based on two main variants of the decomposition of the unit operator in the corresponding spaces at the decomposition stage and the application of additive operator-difference schemes at the composition stage.","The general results are illustrated on the boundary value problem for a second-order parabolic equation by constructing standard splitting schemes on spatial variables and region-additive schemes (domain decomposition schemes)."],"url":"http://arxiv.org/abs/2403.18472v1","category":"math.NA"}
{"created":"2024-03-27 11:32:44","title":"DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face Forgery Analysis","abstract":"The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models' effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \\url{https://github.com/Rapisurazurite/DiffFace}.","sentences":["The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks.","Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques.","To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms.","Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation.","Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models' effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes.","The dataset is available for download at \\url{https://github.com/Rapisurazurite/DiffFace}."],"url":"http://arxiv.org/abs/2403.18471v1","category":"cs.CV"}
{"created":"2024-03-27 11:29:30","title":"Generalized bulk-boundary correspondence in periodically driven non-Hermitian systems","abstract":"We present a pedagogical review of the periodically driven non-Hermitian systems, particularly on the rich interplay between the non-Hermitian skin effect and the topology. We start by reviewing the non-Bloch band theory of the static non-Hermitian systems and discuss the establishment of its generalized bulk-boundary correspondence. Ultimately, we focus on the non-Bloch band theory of two typical periodically driven non-Hermitian systems: harmonically driven non-Hermitian system and periodically quenched non-Hermitian system. The non-Bloch topological invariants were defined on the generalized Brillouin zone and the real space wave functions to characterize the Floquet non-Hermtian topological phases. Then, the generalized bulk-boundary correspondence was established for the two typical periodically driven non-Hermitian systems. Additionally, we review novel phenomena in the higher-dimensional periodically driven non-Hermitian systems, including Floquet non-Hermitian higher-order topological phases and Floquet hybrid skin-topological modes. The experimental realizations and recent advances have also been surveyed. Finally, we end with a summarization and hope this pedagogical review can motivate further research on Floquet non-Hermtian topological physics.","sentences":["We present a pedagogical review of the periodically driven non-Hermitian systems, particularly on the rich interplay between the non-Hermitian skin effect and the topology.","We start by reviewing the non-Bloch band theory of the static non-Hermitian systems and discuss the establishment of its generalized bulk-boundary correspondence.","Ultimately, we focus on the non-Bloch band theory of two typical periodically driven non-Hermitian systems: harmonically driven non-Hermitian system and periodically quenched non-Hermitian system.","The non-Bloch topological invariants were defined on the generalized Brillouin zone and the real space wave functions to characterize the Floquet non-Hermtian topological phases.","Then, the generalized bulk-boundary correspondence was established for the two typical periodically driven non-Hermitian systems.","Additionally, we review novel phenomena in the higher-dimensional periodically driven non-Hermitian systems, including Floquet non-Hermitian higher-order topological phases and Floquet hybrid skin-topological modes.","The experimental realizations and recent advances have also been surveyed.","Finally, we end with a summarization and hope this pedagogical review can motivate further research on Floquet non-Hermtian topological physics."],"url":"http://arxiv.org/abs/2403.18470v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 11:28:57","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds","abstract":"3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-training method with source-aware consistency loss is proposed in stage two to narrow the domain gap further. Experiments on two synthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and SynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms state-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements, respectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.","sentences":["3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains.","Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies.","In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST.","First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level.","Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer.","Finally, by leveraging the designs above, a domain-mixed self-training method with source-aware consistency loss is proposed in stage two to narrow the domain gap further.","Experiments on two synthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and SynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms state-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements, respectively.","Code is available at \\url{https://github.com/yuan-zm/DGT-ST}."],"url":"http://arxiv.org/abs/2403.18469v1","category":"cs.CV"}
{"created":"2024-03-27 11:28:32","title":"Deep Learning Segmentation and Classification of Red Blood Cells Using a Large Multi-Scanner Dataset","abstract":"Digital pathology has recently been revolutionized by advancements in artificial intelligence, deep learning, and high-performance computing. With its advanced tools, digital pathology can help improve and speed up the diagnostic process, reduce human errors, and streamline the reporting step. In this paper, we report a new large red blood cell (RBC) image dataset and propose a two-stage deep learning framework for RBC image segmentation and classification. The dataset is a highly diverse dataset of more than 100K RBCs containing eight different classes. The dataset, which is considerably larger than any publicly available hematopathology dataset, was labeled independently by two hematopathologists who also manually created masks for RBC cell segmentation. Subsequently, in the proposed framework, first, a U-Net model was trained to achieve automatic RBC image segmentation. Second, an EfficientNetB0 model was trained to classify RBC images into one of the eight classes using a transfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03% and an average classification accuracy of 96.5% were attained on the test set. Moreover, we have performed experimental comparisons against several prominent CNN models. These comparisons show the superiority of the proposed model with a good balance between performance and computational cost.","sentences":["Digital pathology has recently been revolutionized by advancements in artificial intelligence, deep learning, and high-performance computing.","With its advanced tools, digital pathology can help improve and speed up the diagnostic process, reduce human errors, and streamline the reporting step.","In this paper, we report a new large red blood cell (RBC) image dataset and propose a two-stage deep learning framework for RBC image segmentation and classification.","The dataset is a highly diverse dataset of more than 100K RBCs containing eight different classes.","The dataset, which is considerably larger than any publicly available hematopathology dataset, was labeled independently by two hematopathologists who also manually created masks for RBC cell segmentation.","Subsequently, in the proposed framework, first, a U-Net model was trained to achieve automatic RBC image segmentation.","Second, an EfficientNetB0 model was trained to classify RBC images into one of the eight classes using a transfer learning approach with a 5X2 cross-validation scheme.","An IoU of 98.03% and an average classification accuracy of 96.5% were attained on the test set.","Moreover, we have performed experimental comparisons against several prominent CNN models.","These comparisons show the superiority of the proposed model with a good balance between performance and computational cost."],"url":"http://arxiv.org/abs/2403.18468v1","category":"eess.IV"}
{"created":"2024-03-27 11:23:17","title":"Variational principles and apllications to symmetric PDEs","abstract":"In this paper, we explore various equivalences of Ekeland's variational principle within the framework of group-invariant mappings. We introduce and analyze several key theorems, including the Drop theorem, the Petal theorem, Caristi-Kirk fxed-point theorem, and Takahashi's theorem, all of them within this context. Moreover, we extend the classical Drop theorem and Petal theorem to a more generalized setting. We also demonstrate the practical signifcance of these findings through numerous applications to diverse areas of mathematics. In particular, in the context of partial differential equations, we explore their implications on the solution of the Plateau problem, and in control theory. We also extend the classical Pontyargin maximum principle.","sentences":["In this paper, we explore various equivalences of Ekeland's variational principle within the framework of group-invariant mappings.","We introduce and analyze several key theorems, including the Drop theorem, the Petal theorem, Caristi-Kirk fxed-point theorem, and Takahashi's theorem, all of them within this context.","Moreover, we extend the classical Drop theorem and Petal theorem to a more generalized setting.","We also demonstrate the practical signifcance of these findings through numerous applications to diverse areas of mathematics.","In particular, in the context of partial differential equations, we explore their implications on the solution of the Plateau problem, and in control theory.","We also extend the classical Pontyargin maximum principle."],"url":"http://arxiv.org/abs/2403.18467v1","category":"math.FA"}
{"created":"2024-03-27 11:22:26","title":"Kinetic data-driven approach to turbulence subgrid modeling","abstract":"Numerical simulations of turbulent flows are well known to pose extreme computational challenges due to the huge number of dynamical degrees of freedom required to correctly describe the complex multi-scale statistical correlations of the velocity. On the other hand, kinetic mesoscale approaches based on the Boltzmann equation, have the potential to describe a broad range of flows, stretching well beyond the special case of gases close to equilibrium, which results in the ordinary Navier-Stokes dynamics. Here we demonstrate that, by properly tuning, a kinetic approach can statistically reproduce the quantitative dynamics of the larger scales in turbulence, thereby providing an alternative, computationally efficient and physically rooted approach towards subgrid scale (SGS) modeling in turbulence. More specifically we show that by leveraging on data from fully resolved Direct Numerical Simulation (DNS) data we can learn a collision operator for the discretized Boltzmann equation solver (the lattice Boltzmann method), which effectively implies a turbulence subgrid closure model. The mesoscopic nature of our formulation makes the learning problem fully local in both space and time, leading to reduced computational costs and enhanced generalization capabilities. We show that the model offers superior performance compared to traditional methods, such as the Smagorinsky model, being less dissipative and, therefore, being able to more closely capture the intermittency of higher-order velocity correlations.","sentences":["Numerical simulations of turbulent flows are well known to pose extreme computational challenges due to the huge number of dynamical degrees of freedom required to correctly describe the complex multi-scale statistical correlations of the velocity.","On the other hand, kinetic mesoscale approaches based on the Boltzmann equation, have the potential to describe a broad range of flows, stretching well beyond the special case of gases close to equilibrium, which results in the ordinary Navier-Stokes dynamics.","Here we demonstrate that, by properly tuning, a kinetic approach can statistically reproduce the quantitative dynamics of the larger scales in turbulence, thereby providing an alternative, computationally efficient and physically rooted approach towards subgrid scale (SGS) modeling in turbulence.","More specifically we show that by leveraging on data from fully resolved Direct Numerical Simulation (DNS) data we can learn a collision operator for the discretized Boltzmann equation solver (the lattice Boltzmann method), which effectively implies a turbulence subgrid closure model.","The mesoscopic nature of our formulation makes the learning problem fully local in both space and time, leading to reduced computational costs and enhanced generalization capabilities.","We show that the model offers superior performance compared to traditional methods, such as the Smagorinsky model, being less dissipative and, therefore, being able to more closely capture the intermittency of higher-order velocity correlations."],"url":"http://arxiv.org/abs/2403.18466v1","category":"physics.flu-dyn"}
{"created":"2024-03-27 11:22:11","title":"Posets of finite GK-dimensional graded pre-Nichols algebras of diagonal type","abstract":"We classify graded pre-Nichols algebras of diagonal type with finite Gelfand-Kirillov dimension. The characterization is made through an isomorphism of posets with the family of appropriate subsets of the set of positive roots coming from central extensions of Nichols algebras of diagonal type, generalizing the corresponding extensions for small quantum groups in de Concini-Kac-Procesi forms of quantum groups.   On the way to achieving this result, we also classify graded quotients of algebras of functions of unipotent algebraic groups attached to semisimple Lie algebras.","sentences":["We classify graded pre-Nichols algebras of diagonal type with finite Gelfand-Kirillov dimension.","The characterization is made through an isomorphism of posets with the family of appropriate subsets of the set of positive roots coming from central extensions of Nichols algebras of diagonal type, generalizing the corresponding extensions for small quantum groups in de Concini-Kac-Procesi forms of quantum groups.   ","On the way to achieving this result, we also classify graded quotients of algebras of functions of unipotent algebraic groups attached to semisimple Lie algebras."],"url":"http://arxiv.org/abs/2403.18465v1","category":"math.RT"}
{"created":"2024-03-27 11:18:44","title":"Probing the quantum nature of gravity using a Bose-Einstein condensate; \"Erste Abhandlung\"","abstract":"The effect of noise induced by gravitons has been investigated using a Bose-Einstein condensate. The gravitational wave perturbation is then considerd as a sum of discrete Fourier modes in the momentum space. Coming to an operatorial representation and quantizing the phase space variables via appropriately introduced canonincal commutation relations between the canonically conjugate variables corresponding to the graviton and bosonic part of the total system, one obtains a proper quantum gravity setup. Then we obtain the Bogoliubov coefficients from the solution of the time-dependent part of the pseudo-Goldstone boson and construct the covariance metric for the bosons initially being in a squeezed state. Using the stochastic average of the Fisher information, we obtain a lower bound on the amplitude parameter of the gravitational wave. As the entire calculation is done at zero temperature, the bosonic system, by construction, will behave as a Bose-Einstein condensate. For a Bose-Einstein condensate with a single mode, we observe that the lower bound of the expectation value of the square of the uncertainty in the amplitude measurement does not become infinite when the total observational term approaches zero. In order to sum over all possible momentum modes, we next consider a noise term with a suitable Gaussian weight factor which decays over time. We then obtain the lower bound on the final expectation value of the square of the variance in the amplitude parameter. Because of the noise induced by the graviton, there is a minimum value of the measurement time below which it is impossible to detect any gravitational wave using a Bose-Einstein condensate. Finally, we consider interaction between the phonon modes of the Bose-Einstein condensate which results in a decoherence. We observe that the decoherence effect becomes significant for gravitons with minimal squeezing.","sentences":["The effect of noise induced by gravitons has been investigated using a Bose-Einstein condensate.","The gravitational wave perturbation is then considerd as a sum of discrete Fourier modes in the momentum space.","Coming to an operatorial representation and quantizing the phase space variables via appropriately introduced canonincal commutation relations between the canonically conjugate variables corresponding to the graviton and bosonic part of the total system, one obtains a proper quantum gravity setup.","Then we obtain the Bogoliubov coefficients from the solution of the time-dependent part of the pseudo-Goldstone boson and construct the covariance metric for the bosons initially being in a squeezed state.","Using the stochastic average of the Fisher information, we obtain a lower bound on the amplitude parameter of the gravitational wave.","As the entire calculation is done at zero temperature, the bosonic system, by construction, will behave as a Bose-Einstein condensate.","For a Bose-Einstein condensate with a single mode, we observe that the lower bound of the expectation value of the square of the uncertainty in the amplitude measurement does not become infinite when the total observational term approaches zero.","In order to sum over all possible momentum modes, we next consider a noise term with a suitable Gaussian weight factor which decays over time.","We then obtain the lower bound on the final expectation value of the square of the variance in the amplitude parameter.","Because of the noise induced by the graviton, there is a minimum value of the measurement time below which it is impossible to detect any gravitational wave using a Bose-Einstein condensate.","Finally, we consider interaction between the phonon modes of the Bose-Einstein condensate which results in a decoherence.","We observe that the decoherence effect becomes significant for gravitons with minimal squeezing."],"url":"http://arxiv.org/abs/2403.18460v1","category":"hep-th"}
{"created":"2024-03-27 11:18:01","title":"CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration","abstract":"Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic simulation study with 56000 experiments. We outperform all baselines by a margin of 4-10%. Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising.","sentences":["Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated.","Fixed robot programs leave no room to diverge from a fixed protocol.","Working on such a process can be stressful for the user and lead to ineffective behavior or failure.","We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS.","This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human).","The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task.","In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios.","We evaluate our algorithm using a probabilistic simulation study with 56000 experiments.","We outperform all baselines by a margin of 4-10%.","Initial real robot experiments using a Franka Emika Panda robot and human tracking based on HTC Vive VR gloves look promising."],"url":"http://arxiv.org/abs/2403.18459v1","category":"cs.RO"}
{"created":"2024-03-27 11:16:04","title":"Inverse kinematics learning of a continuum manipulator using limited real time data","abstract":"Data driven control of a continuum manipulator requires a lot of data for training but generating sufficient amount of real time data is not cost efficient. Random actuation of the manipulator can also be unsafe sometimes. Meta learning has been used successfully to adapt to a new environment. Hence, this paper tries to solve the above mentioned problem using meta learning. We consider two cases for that. First, this paper proposes a method to use simulation data for training the model using MAML(Model-Agnostic Meta-Learning). Then, it adapts to the real world using gradient steps. Secondly,if the simulation model is not available or difficult to formulate, then we propose a CGAN(Conditional Generative adversial network)-MAML based method for it. The model is trained using a small amount of real time data and augmented data for different loading conditions. Then, adaptation is done in the real environment. It has been found out from the experiments that the relative positioning error for both the cases are below 3%. The proposed models are experimentally verified on a real continuum manipulator.","sentences":["Data driven control of a continuum manipulator requires a lot of data for training but generating sufficient amount of real time data is not cost efficient.","Random actuation of the manipulator can also be unsafe sometimes.","Meta learning has been used successfully to adapt to a new environment.","Hence, this paper tries to solve the above mentioned problem using meta learning.","We consider two cases for that.","First, this paper proposes a method to use simulation data for training the model using MAML(Model-Agnostic Meta-Learning).","Then, it adapts to the real world using gradient steps.","Secondly,if the simulation model is not available or difficult to formulate, then we propose a CGAN(Conditional Generative adversial network)-MAML based method for it.","The model is trained using a small amount of real time data and augmented data for different loading conditions.","Then, adaptation is done in the real environment.","It has been found out from the experiments that the relative positioning error for both the cases are below 3%.","The proposed models are experimentally verified on a real continuum manipulator."],"url":"http://arxiv.org/abs/2403.18456v1","category":"cs.RO"}
{"created":"2024-03-27 11:11:37","title":"Annotating Slack Directly on Your Verilog: Fine-Grained RTL Timing Evaluation for Early Optimization","abstract":"In digital IC design, compared with post-synthesis netlists or layouts, the early register-transfer level (RTL) stage offers greater optimization flexibility for both designers and EDA tools. However, timing information is typically unavailable at this early stage. Some recent machine learning (ML) solutions propose to predict the total negative slack (TNS) and worst negative slack (WNS) of an entire design at the RTL stage, but the fine-grained timing information of individual registers remains unavailable. In this work, we address the unique challenges of RTL timing prediction and introduce our solution named RTL-Timer. To the best of our knowledge, this is the first fine-grained general timing estimator applicable to any given design. RTL-Timer explores multiple promising RTL representations and proposes customized loss functions to capture the maximum arrival time at register endpoints. RTL-Timer's fine-grained predictions are further applied to guide optimization in a standard synthesis flow. The average results on unknown test designs demonstrate a correlation above 0.89, contributing around 3% WNS and 10% TNS improvement after optimization.","sentences":["In digital IC design, compared with post-synthesis netlists or layouts, the early register-transfer level (RTL) stage offers greater optimization flexibility for both designers and EDA tools.","However, timing information is typically unavailable at this early stage.","Some recent machine learning (ML) solutions propose to predict the total negative slack (TNS) and worst negative slack (WNS) of an entire design at the RTL stage, but the fine-grained timing information of individual registers remains unavailable.","In this work, we address the unique challenges of RTL timing prediction and introduce our solution named RTL-Timer.","To the best of our knowledge, this is the first fine-grained general timing estimator applicable to any given design.","RTL-Timer explores multiple promising RTL representations and proposes customized loss functions to capture the maximum arrival time at register endpoints.","RTL-Timer's fine-grained predictions are further applied to guide optimization in a standard synthesis flow.","The average results on unknown test designs demonstrate a correlation above 0.89, contributing around 3% WNS and 10% TNS improvement after optimization."],"url":"http://arxiv.org/abs/2403.18453v1","category":"cs.AR"}
{"created":"2024-03-27 11:11:08","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","abstract":"There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot. These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embedding space. We next propose an adaptive anchor working in the Singular space. Unlike traditional fixed anchor methods that sometimes yield unacceptable paths, our adaptive anchor enables correct anchors, which are put into a wrong location, based on a traversability map. Finally, we adopt a diffusion-based predictor to further enhance the prototype paths using a cascaded denoising process. Our unified framework ensures the generality across various benchmark settings such as input modality, and trajectory lengths. Extensive experiments on five public benchmarks demonstrate that SingularTrajectory substantially outperforms existing models, highlighting its effectiveness in estimating general dynamics of human movements. Code is publicly available at https://github.com/inhwanbae/SingularTrajectory .","sentences":["There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot.","These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods.","Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary.","For the other task, generality issues can lead to sub-optimal performances.","In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks.","The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks.","To do this, we first build a Singular space to project all types of motion patterns from each task into one embedding space.","We next propose an adaptive anchor working in the Singular space.","Unlike traditional fixed anchor methods that sometimes yield unacceptable paths, our adaptive anchor enables correct anchors, which are put into a wrong location, based on a traversability map.","Finally, we adopt a diffusion-based predictor to further enhance the prototype paths using a cascaded denoising process.","Our unified framework ensures the generality across various benchmark settings such as input modality, and trajectory lengths.","Extensive experiments on five public benchmarks demonstrate that SingularTrajectory substantially outperforms existing models, highlighting its effectiveness in estimating general dynamics of human movements.","Code is publicly available at https://github.com/inhwanbae/SingularTrajectory ."],"url":"http://arxiv.org/abs/2403.18452v1","category":"cs.CV"}
{"created":"2024-03-27 11:11:06","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT","abstract":"Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to offer context-aware insights for localized client tasks through FM-powered global representation learning. Our evaluation on real-world weather dataset demonstrates CoRAST's ability to exploit correlated heterogeneous data through environmental representation learning to reduce the forecast errors by up to 50.3% compared to the baselines.","sentences":["Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets.","Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings.","This process facilitates the integration of information from various modalities and the application of prior learning to new domains.","However, deploying FMs in resource-constrained edge systems poses significant challenges.","To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data.","Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data.","This enables CoRAST to offer context-aware insights for localized client tasks through FM-powered global representation learning.","Our evaluation on real-world weather dataset demonstrates CoRAST's ability to exploit correlated heterogeneous data through environmental representation learning to reduce the forecast errors by up to 50.3% compared to the baselines."],"url":"http://arxiv.org/abs/2403.18451v1","category":"cs.LG"}
{"created":"2024-03-27 11:09:27","title":"Loop homology of moment-angle complexes in the flag case","abstract":"We develop a general homological approach to presentations of connected graded associative algebras, and apply it to loop homology of moment-angle complexes $Z_K$ that correspond to flag simplicial complexes $K$. For arbitrary coefficient ring, we describe generators of the Pontryagin algebra $H_*(\\Omega Z_K)$ and defining relations between them. We prove that such moment-angle complexes are coformal over $\\mathbb{Q},$ give a necessary condition for rational formality, and compute their homotopy groups in terms of homotopy groups of spheres.","sentences":["We develop a general homological approach to presentations of connected graded associative algebras, and apply it to loop homology of moment-angle complexes $Z_K$ that correspond to flag simplicial complexes $K$. For arbitrary coefficient ring, we describe generators of the Pontryagin algebra $H_*(\\Omega Z_K)$ and defining relations between them.","We prove that such moment-angle complexes are coformal over $\\mathbb{Q},$ give a necessary condition for rational formality, and compute their homotopy groups in terms of homotopy groups of spheres."],"url":"http://arxiv.org/abs/2403.18450v1","category":"math.AT"}
{"created":"2024-03-27 11:08:43","title":"Generalizations of free monoids","abstract":"We generalize free monoids by defining $k$-monoids. These are nothing other than the one-vertex higher-rank graphs used in $C^{\\ast}$-algebra theory with the cardinality requirement waived. The $1$-monoids are precisely the free monoids. We then take the next step and generalize $k$-monoids in such a way that self-similar group actions yield monoids of this type.","sentences":["We generalize free monoids by defining $k$-monoids.","These are nothing other than the one-vertex higher-rank graphs used in $C^{\\ast}$-algebra theory with the cardinality requirement waived.","The $1$-monoids are precisely the free monoids.","We then take the next step and generalize $k$-monoids in such a way that self-similar group actions yield monoids of this type."],"url":"http://arxiv.org/abs/2403.18449v1","category":"math.CT"}
{"created":"2024-03-27 11:06:44","title":"Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction","abstract":"Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering. We then train a numerical tokenizer with the prompt data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts. Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at https://github.com/inhwanbae/LMTrajectory .","sentences":["Language models have demonstrated impressive ability in context understanding and generative performance.","Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem.","Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts.","Specially, we first transform an input space for the trajectory coordinate into the natural language space.","Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning.","The transformed numerical and image data are then wrapped into the question-answering template for use in a language model.","Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering.","We then train a numerical tokenizer with the prompt data.","We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model.","Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts.","Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences.","Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods.","Code is publicly available at https://github.com/inhwanbae/LMTrajectory ."],"url":"http://arxiv.org/abs/2403.18447v1","category":"cs.CL"}
{"created":"2024-03-27 11:05:26","title":"Cosmic Inflation From Fluctuating Baby-Skyrme Brane","abstract":"In this work, we explore the inflationary dynamics induced by small fluctuations on the Skyrme brane, characterized by a time-dependent perturbative function $\\tilde{\\phi}$. In the low-energy regime, the model successfully reproduces standard inflation, with a potential term dictated by the Skyrmion at the brane. Gravity localization is achieved at the brane, and the lowest energy scale is established at the asymptotic boundary. The model demonstrates the capability to emulate standard inflation dynamics, resembling $\\tilde{\\phi}^4$ potential characteristics under certain conditions. At higher energy levels, the behaviour of $\\tilde{\\phi}$ is contingent upon the Skyrme term coupling constant $\\lambda$, influencing reheating phases. The wave-like nature of fluctuations allows for energy transfer, resulting in a possibly lower reheating temperature. We also discuss the prospect of $\\lambda$ changing sign during inflation, presenting a non-standard coupling dependent on the matter field.","sentences":["In this work, we explore the inflationary dynamics induced by small fluctuations on the Skyrme brane, characterized by a time-dependent perturbative function $\\tilde{\\phi}$.","In the low-energy regime, the model successfully reproduces standard inflation, with a potential term dictated by the Skyrmion at the brane.","Gravity localization is achieved at the brane, and the lowest energy scale is established at the asymptotic boundary.","The model demonstrates the capability to emulate standard inflation dynamics, resembling $\\tilde{\\phi}^4$ potential characteristics under certain conditions.","At higher energy levels, the behaviour of $\\tilde{\\phi}$ is contingent upon the Skyrme term coupling constant $\\lambda$, influencing reheating phases.","The wave-like nature of fluctuations allows for energy transfer, resulting in a possibly lower reheating temperature.","We also discuss the prospect of $\\lambda$ changing sign during inflation, presenting a non-standard coupling dependent on the matter field."],"url":"http://arxiv.org/abs/2403.18446v1","category":"gr-qc"}
{"created":"2024-03-27 11:00:53","title":"FRESCO: Federated Reinforcement Energy System for Cooperative Optimization","abstract":"The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results.","sentences":["The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid.","This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning.","The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives.","This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results."],"url":"http://arxiv.org/abs/2403.18444v1","category":"cs.LG"}
{"created":"2024-03-27 11:00:33","title":"$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation via Optical Flow Consistency and Feature Map Synthesis","abstract":"Self-supervised monocular depth estimation methods have been increasingly given much attention due to the benefit of not requiring large, labelled datasets. Such self-supervised methods require high-quality salient features and consequently suffer from severe performance drop for indoor scenes, where low-textured regions dominant in the scenes are almost indiscriminative. To address the issue, we propose a self-supervised indoor monocular depth estimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow estimation network is introduced to supervise depth learning. To improve optical flow estimation performance in low-textured areas, only some patches of points with more discriminative features are adopted for finetuning based on our well-designed patch-based photometric loss. The finetuned optical flow estimation network generates high-accuracy optical flow as a supervisory signal for depth estimation. Correspondingly, an optical flow consistency loss is designed. Multi-scale feature maps produced by finetuned optical flow estimation network perform warping to compute feature map synthesis loss as another supervisory signal for depth learning. Experimental results on the NYU Depth V2 dataset demonstrate the effectiveness of the framework and our proposed losses. To evaluate the generalization ability of our $\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of approximately 1500 points selected from 99 images in 18 scenes. Zero-shot generalization experiments on 7-Scenes dataset and Campus Indoor achieve $\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show that our model can generalize well to monocular images captured in unknown indoor scenes.","sentences":["Self-supervised monocular depth estimation methods have been increasingly given much attention due to the benefit of not requiring large, labelled datasets.","Such self-supervised methods require high-quality salient features and consequently suffer from severe performance drop for indoor scenes, where low-textured regions dominant in the scenes are almost indiscriminative.","To address the issue, we propose a self-supervised indoor monocular depth estimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow estimation network is introduced to supervise depth learning.","To improve optical flow estimation performance in low-textured areas, only some patches of points with more discriminative features are adopted for finetuning based on our well-designed patch-based photometric loss.","The finetuned optical flow estimation network generates high-accuracy optical flow as a supervisory signal for depth estimation.","Correspondingly, an optical flow consistency loss is designed.","Multi-scale feature maps produced by finetuned optical flow estimation network perform warping to compute feature map synthesis loss as another supervisory signal for depth learning.","Experimental results on the NYU Depth V2 dataset demonstrate the effectiveness of the framework and our proposed losses.","To evaluate the generalization ability of our $\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of approximately 1500 points selected from 99 images in 18 scenes.","Zero-shot generalization experiments on 7-Scenes dataset and Campus Indoor achieve $\\delta_1$ accuracy of 75.8% and 76.0% respectively.","The accuracy results show that our model can generalize well to monocular images captured in unknown indoor scenes."],"url":"http://arxiv.org/abs/2403.18443v1","category":"cs.CV"}
{"created":"2024-03-27 10:47:06","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","abstract":"The smart grid domain requires bolstering the capabilities of existing energy management systems; Federated Learning (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models. This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs. Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data. Experimental results validate the robustness of our approach, affirming its proficiency in effectively learning policy models for smart grid challenges.","sentences":["The smart grid domain requires bolstering the capabilities of existing energy management systems; Federated Learning (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models.","This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs.","Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data.","Experimental results validate the robustness of our approach, affirming its proficiency in effectively learning policy models for smart grid challenges."],"url":"http://arxiv.org/abs/2403.18439v1","category":"cs.LG"}
{"created":"2024-03-27 10:43:12","title":"Coherent Collections of Rules Describing Exceptional Materials Identified with a Multi-Objective Optimization of Subgroups","abstract":"Using a modest amount of data from a large population, subgroup discovery (SGD) identifies outstanding subsets of data with respect to a certain property of interest of that population. The SGs are described by \"rules\". These are constraints on key descriptive parameters that characterize the material or the environment. These parameters and constraints are obtained by maximizing a quality function that establishes a tradeoff between SG size and utility, i.e., between generality and exceptionality. The utility function measures how outstanding a SG is. However, this approach does not give a unique solution, but typically many SGs have similar quality-function values. Here, we identify coherent collections of SGs of a \"Pareto region\" presenting various size-utility tradeoffs and define a SG similarity measure based on the Jaccard index, which allows us to hierarchically cluster these optimal SGs. These concepts are demonstrated by learning rules that describe perovskites with high bulk modulus. We show that SGs focusing on exceptional materials exhibit a high quality-function value but do not necessarily maximize it. We compare the mean shift with the cumulative Jensen-Shannon divergence ($D_{sJS}$) as utility functions and show that the SG rules obtained with $D_{cJS}$ are more focused than those obtained with the mean shift.","sentences":["Using a modest amount of data from a large population, subgroup discovery (SGD) identifies outstanding subsets of data with respect to a certain property of interest of that population.","The SGs are described by \"rules\".","These are constraints on key descriptive parameters that characterize the material or the environment.","These parameters and constraints are obtained by maximizing a quality function that establishes a tradeoff between SG size and utility, i.e., between generality and exceptionality.","The utility function measures how outstanding a SG is.","However, this approach does not give a unique solution, but typically many SGs have similar quality-function values.","Here, we identify coherent collections of SGs of a \"Pareto region\" presenting various size-utility tradeoffs and define a SG similarity measure based on the Jaccard index, which allows us to hierarchically cluster these optimal SGs.","These concepts are demonstrated by learning rules that describe perovskites with high bulk modulus.","We show that SGs focusing on exceptional materials exhibit a high quality-function value but do not necessarily maximize it.","We compare the mean shift with the cumulative Jensen-Shannon divergence ($D_{sJS}$) as utility functions and show that the SG rules obtained with $D_{cJS}$ are more focused than those obtained with the mean shift."],"url":"http://arxiv.org/abs/2403.18437v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 10:27:28","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","abstract":"Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators' results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.","sentences":["Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions.","In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial.","This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution.","We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.","Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints.","To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints.","The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively.","Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators' results.","Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment."],"url":"http://arxiv.org/abs/2403.18426v1","category":"cs.CL"}
{"created":"2024-03-27 10:26:42","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","abstract":"Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently capturing both local and global features, as well as spatial correlations between pixels. Moreover, we propose the addition of a sketch simplification network that offers the user the choice of preprocessing and simplifying input sketches for enhanced outputs. The experimental results, corroborated by user feedback, demonstrate that our proposed U-Net latent edge predictor leads to more realistic results, that are better aligned with the spatial outlines of the reference sketches, while drastically reducing the number of required denoising steps and, consequently, the overall execution time.","sentences":["Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts.","Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches.","Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed.","Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency.","To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently capturing both local and global features, as well as spatial correlations between pixels.","Moreover, we propose the addition of a sketch simplification network that offers the user the choice of preprocessing and simplifying input sketches for enhanced outputs.","The experimental results, corroborated by user feedback, demonstrate that our proposed U-Net latent edge predictor leads to more realistic results, that are better aligned with the spatial outlines of the reference sketches, while drastically reducing the number of required denoising steps and, consequently, the overall execution time."],"url":"http://arxiv.org/abs/2403.18425v1","category":"cs.CV"}
{"created":"2024-03-27 10:24:25","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks","abstract":"Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model's high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.","sentences":["Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern.","While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited.","In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs.","Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain.","Our method learns a robust representation that bridges these two domains.","We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness.","We align the domains by incorporating a new distance-based objective.","With this, our model is able to learn more generalized representations by aligning the model's high-level output features and therefore better handling unseen adversarial samples.","This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels.","To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets.","The results demonstrate promising state-of-the-art robustness."],"url":"http://arxiv.org/abs/2403.18423v1","category":"cs.CL"}
{"created":"2024-03-27 10:18:21","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","abstract":"Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. The model is available on the Hugging Face Hub: https://huggingface.co/stanford-crfm/BioMedLM.","sentences":["Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks.","However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources.","Can smaller, more targeted models compete?","To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles.","When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam.","BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics.","This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine.","The model is available on the Hugging Face Hub: https://huggingface.co/stanford-crfm/BioMedLM."],"url":"http://arxiv.org/abs/2403.18421v1","category":"cs.CL"}
{"created":"2024-03-27 10:13:35","title":"Bounded islands in dS$_{2}$ multiverse model","abstract":"The cosmological event horizons are observer-dependent, which might bring a paradox. As an example, in dS$_{2}$ multiverse model there are entanglement islands in crunching regions encoding the information of regions near future infinity of inflating or Minkowski bubbles, however, for two observers in different bubbles, since their island regions overlap, both observers will be able to get access to the information encoded in the overlapping region, indicating a violation of no-cloning theorem. In this paper, we present a different resolution to this paradox. Based on the Petz $\\mathrm{R\\acute{e}nyi}$ mutual information, we show that besides the quantum extremal surfaces there might be another boundary for the island in corresponding spacetime so that the island regions are bounded by ``division points\" rather than extending to the rest of the entire spacetime. We also discuss the implications of our result.","sentences":["The cosmological event horizons are observer-dependent, which might bring a paradox.","As an example, in dS$_{2}$ multiverse model there are entanglement islands in crunching regions encoding the information of regions near future infinity of inflating or Minkowski bubbles, however, for two observers in different bubbles, since their island regions overlap, both observers will be able to get access to the information encoded in the overlapping region, indicating a violation of no-cloning theorem.","In this paper, we present a different resolution to this paradox.","Based on the Petz $\\mathrm{R\\acute{e}nyi}$ mutual information, we show that besides the quantum extremal surfaces there might be another boundary for the island in corresponding spacetime so that the island regions are bounded by ``division points\" rather than extending to the rest of the entire spacetime.","We also discuss the implications of our result."],"url":"http://arxiv.org/abs/2403.18420v1","category":"hep-th"}
{"created":"2024-03-27 10:09:38","title":"ECNet: Effective Controllable Text-to-Image Diffusion Models","abstract":"The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models.","sentences":["The conditional text-to-image diffusion models have garnered significant attention in recent years.","However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss.","To address the challenges, we introduce two innovative solutions.","Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information.","This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model.","Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step.","This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output.","The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision.","We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects.","The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models."],"url":"http://arxiv.org/abs/2403.18417v1","category":"cs.CV"}
{"created":"2024-03-27 10:08:48","title":"A Delaunay Refinement Algorithm for the Particle Finite Element Method applied to Free Surface Flows","abstract":"This paper proposes two contributions to the calculation of free surface flows using the particle finite element method (PFEM). The PFEM is based on a Lagrangian approach: a set of particles defines the fluid. Then, unlike a pure Lagrangian method, all the particles are connected by a triangular mesh. The difficulty lies in locating the free surface from this mesh. It is a matter of deciding which of the elements in the mesh are part of the fluid domain, and to define a boundary - the free surface. Then, the incompressible Navier-Stokes equations are solved on the fluid domain and the particles' position is updated using the resulting velocity vector. Our first contribution is to propose an approach to adapt the mesh with theoretical guarantees of quality: the mesh generation community has acquired a lot of experience and understanding about mesh adaptation approaches with guarantees of quality on the final mesh. We use here a Delaunay refinement strategy, allowing to insert and remove nodes while gradually improving mesh quality. We show that this allows to create stable and smooth free surface geometries. Our PFEM approach models the topological evolution of one fluid. It is nevertheless necessary to apply conditions on the domain boundaries. When a boundary is a free surface, the flow on the other side is not modelled, it is represented by an external pressure. On the external free surface boundary, atmospheric pressure can be imposed. Nevertheless, there may be internal free surfaces: the fluid can fully encapsulate cavities to form bubbles. The pressure required to maintain the volume of those bubbles is a priori unknown. We propose a multi-point constraint approach to enforce global incompressibility of those empty bubbles. This approach allows to accurately model bubbly flows that involve two fluids with large density differences, while only modelling the heavier fluid.","sentences":["This paper proposes two contributions to the calculation of free surface flows using the particle finite element method (PFEM).","The PFEM is based on a Lagrangian approach: a set of particles defines the fluid.","Then, unlike a pure Lagrangian method, all the particles are connected by a triangular mesh.","The difficulty lies in locating the free surface from this mesh.","It is a matter of deciding which of the elements in the mesh are part of the fluid domain, and to define a boundary - the free surface.","Then, the incompressible Navier-Stokes equations are solved on the fluid domain and the particles' position is updated using the resulting velocity vector.","Our first contribution is to propose an approach to adapt the mesh with theoretical guarantees of quality: the mesh generation community has acquired a lot of experience and understanding about mesh adaptation approaches with guarantees of quality on the final mesh.","We use here a Delaunay refinement strategy, allowing to insert and remove nodes while gradually improving mesh quality.","We show that this allows to create stable and smooth free surface geometries.","Our PFEM approach models the topological evolution of one fluid.","It is nevertheless necessary to apply conditions on the domain boundaries.","When a boundary is a free surface, the flow on the other side is not modelled, it is represented by an external pressure.","On the external free surface boundary, atmospheric pressure can be imposed.","Nevertheless, there may be internal free surfaces: the fluid can fully encapsulate cavities to form bubbles.","The pressure required to maintain the volume of those bubbles is a priori unknown.","We propose a multi-point constraint approach to enforce global incompressibility of those empty bubbles.","This approach allows to accurately model bubbly flows that involve two fluids with large density differences, while only modelling the heavier fluid."],"url":"http://arxiv.org/abs/2403.18416v1","category":"cs.CE"}
{"created":"2024-03-27 10:06:29","title":"Approximation of functions of many variables from the generalization Nikol'skii-Besov type classes in the uniform and integral metrics","abstract":"We obtain the exact order estimates of the approximation of the functions of many variables from the generalized Nikol'skii-Besov classes $B^{\\Omega}_{p,\\theta}(\\mathbb{R}^d)$ by sums of de la Vallee Poussin type in the metrics space $L_{\\infty}(\\mathbb{R}^d)$ and $L_{1}(\\mathbb{R}^d)$. These classes of functions for some given $\\Omega$ coincide with the well-known classical Nikol'skii-Besov isotropic classes.","sentences":["We obtain the exact order estimates of the approximation of the functions of many variables from the generalized Nikol'skii-Besov classes $B^{\\Omega}_{p,\\theta}(\\mathbb{R}^d)$ by sums of de la Vallee Poussin type in the metrics space $L_{\\infty}(\\mathbb{R}^d)$ and $L_{1}(\\mathbb{R}^d)$. These classes of functions for some given $\\Omega$ coincide with the well-known classical Nikol'skii-Besov isotropic classes."],"url":"http://arxiv.org/abs/2403.18414v1","category":"math.CA"}
{"created":"2024-03-27 10:01:14","title":"HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion Planning Algorithm for Hybrid Systems","abstract":"This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems. The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backward propagation results is detected. Then, HyRRT-Connect constructs a motion plan through the reversal and concatenation of functions defined on hybrid time domains, ensuring the motion plan thoroughly satisfies the given hybrid dynamics. To address the potential discontinuity along the flow caused by tolerating some distance between the forward and backward partial motion plans, we reconstruct the backward partial motion plan by a forward-in-hybrid-time simulation from the final state of the forward partial motion plan. By applying the reversed input of the backward partial motion plan, the reconstruction process effectively eliminates the discontinuity and ensures that as the tolerance distance decreases to zero, the distance between the endpoint of the reconstructed motion plan and the final state set approaches zero. The proposed algorithm is applied to an actuated bouncing ball example and a walking robot example so as to highlight its generality and computational improvement.","sentences":["This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems.","The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backward propagation results is detected.","Then, HyRRT-Connect constructs a motion plan through the reversal and concatenation of functions defined on hybrid time domains, ensuring the motion plan thoroughly satisfies the given hybrid dynamics.","To address the potential discontinuity along the flow caused by tolerating some distance between the forward and backward partial motion plans, we reconstruct the backward partial motion plan by a forward-in-hybrid-time simulation from the final state of the forward partial motion plan.","By applying the reversed input of the backward partial motion plan, the reconstruction process effectively eliminates the discontinuity and ensures that as the tolerance distance decreases to zero, the distance between the endpoint of the reconstructed motion plan and the final state set approaches zero.","The proposed algorithm is applied to an actuated bouncing ball example and a walking robot example so as to highlight its generality and computational improvement."],"url":"http://arxiv.org/abs/2403.18413v1","category":"cs.RO"}
{"created":"2024-03-27 09:54:21","title":"Chiral Virasoro algebra from a single wavefunction","abstract":"Chiral edges of 2+1D systems can have very robust emergent conformal symmetry. When the edge is purely chiral, the Hilbert space of low-energy edge excitations can form a representation of a single Virasoro algebra. We propose a method to systematically extract the generators of the Virasoro algebra from a single ground state wavefunction, using entanglement bootstrap and an input from the edge conformal field theory. We corroborate our construction by numerically verifying the commutation relations of the generators. We also study the unitary flows generated by these operators, whose properties (such as energy and state overlap) are shown numerically to agree with our analytical predictions.","sentences":["Chiral edges of 2+1D systems can have very robust emergent conformal symmetry.","When the edge is purely chiral, the Hilbert space of low-energy edge excitations can form a representation of a single Virasoro algebra.","We propose a method to systematically extract the generators of the Virasoro algebra from a single ground state wavefunction, using entanglement bootstrap and an input from the edge conformal field theory.","We corroborate our construction by numerically verifying the commutation relations of the generators.","We also study the unitary flows generated by these operators, whose properties (such as energy and state overlap) are shown numerically to agree with our analytical predictions."],"url":"http://arxiv.org/abs/2403.18410v1","category":"quant-ph"}
{"created":"2024-03-27 09:53:52","title":"Effect of Magnetic diffusion in the Chromosphere on the Solar Wind","abstract":"We investigate non-ideal magnetohydrodynamical (MHD) effects in the chromosphere on the solar wind by performing MHD simulations for Alfv\\'en-wave driven winds with explicitly including Ohmic and ambipolar diffusion. We find that MHD waves are significantly damped in the chromosphere by ambipolar diffusion so that the Alfv\\'enic Poynting flux that reaches the corona is substantially reduced. As a result, the coronal temperature and the mass loss rate of the solar wind are considerably reduced, compared with those obtained from an ideal MHD case, which is indicative of a great importance of the non-ideal MHD effects in the solar atmosphere. However, the temperature and the mass loss rate are recovered by a small increase in the convection-originated velocity perturbation at the photosphere because of the sensitive dependence of the ambipolar diffusion and reflection of Alfv\\'en waves on the physical properties of the chromosphere. We also find that density perturbations in the corona are reduced by the ambipolar diffusion of Alfv\\'en waves in the chromosphere because the nonlinear generation of compressible perturbations is suppressed.","sentences":["We investigate non-ideal magnetohydrodynamical (MHD) effects in the chromosphere on the solar wind by performing MHD simulations for Alfv\\'en-wave driven winds with explicitly including Ohmic and ambipolar diffusion.","We find that MHD waves are significantly damped in the chromosphere by ambipolar diffusion so that the Alfv\\'enic Poynting flux that reaches the corona is substantially reduced.","As a result, the coronal temperature and the mass loss rate of the solar wind are considerably reduced, compared with those obtained from an ideal MHD case, which is indicative of a great importance of the non-ideal MHD effects in the solar atmosphere.","However, the temperature and the mass loss rate are recovered by a small increase in the convection-originated velocity perturbation at the photosphere because of the sensitive dependence of the ambipolar diffusion and reflection of Alfv\\'en waves on the physical properties of the chromosphere.","We also find that density perturbations in the corona are reduced by the ambipolar diffusion of Alfv\\'en waves in the chromosphere because the nonlinear generation of compressible perturbations is suppressed."],"url":"http://arxiv.org/abs/2403.18409v1","category":"astro-ph.SR"}
{"created":"2024-03-27 09:49:37","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification","abstract":"Semi-supervised learning (SSL) is a practical challenge in computer vision. Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of The Art (SOTA) performances in SSL. These approaches employ a threshold-to-pseudo-label (T2L) process to generate PLs by truncating the confidence scores of unlabeled data predicted by the self-training method. However, self-trained models typically yield biased and high-variance predictions, especially in the scenarios when a little labeled data are supplied. To address this issue, we propose a lightweight channel-based ensemble method to effectively consolidate multiple inferior PLs into the theoretically guaranteed unbiased and low-variance one. Importantly, our approach can be readily extended to any SSL framework, such as FixMatch or FreeMatch. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques on CIFAR10/100 in terms of effectiveness and efficiency.","sentences":["Semi-supervised learning (SSL) is a practical challenge in computer vision.","Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of The Art (SOTA) performances in SSL.","These approaches employ a threshold-to-pseudo-label (T2L) process to generate PLs by truncating the confidence scores of unlabeled data predicted by the self-training method.","However, self-trained models typically yield biased and high-variance predictions, especially in the scenarios when a little labeled data are supplied.","To address this issue, we propose a lightweight channel-based ensemble method to effectively consolidate multiple inferior PLs into the theoretically guaranteed unbiased and low-variance one.","Importantly, our approach can be readily extended to any SSL framework, such as FixMatch or FreeMatch.","Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques on CIFAR10/100 in terms of effectiveness and efficiency."],"url":"http://arxiv.org/abs/2403.18407v1","category":"cs.CV"}
{"created":"2024-03-27 09:48:23","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM","abstract":"Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot video question answering benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks.","sentences":["Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised.","A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs.","Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging.","In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized.","Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information.","The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame.","Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout.","The resulting single image is termed as an image grid.","This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure.","Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training.","Our extensive experimental analysis across ten zero-shot video question answering benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks."],"url":"http://arxiv.org/abs/2403.18406v1","category":"cs.CV"}
{"created":"2024-03-27 09:46:56","title":"Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval","abstract":"Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments. By comparing the relevance judgments of LLMs and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow. Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model.","sentences":["Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task.","Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments.","With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment.","Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored.","To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases.","The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments.","By comparing the relevance judgments of LLMs and human experts, we empirically show that we can obtain reliable relevance judgments with the proposed workflow.","Furthermore, we demonstrate the capacity to augment existing legal case retrieval models through the synthesis of data generated by the large language model."],"url":"http://arxiv.org/abs/2403.18405v1","category":"cs.AI"}
{"created":"2024-03-27 09:44:50","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency","abstract":"The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems. Here, a novel approach for power grid classification is developed, leveraging ENF. Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers. Four traditional machine learning classifiers plus a Convolutional Neural Network (CNN), optimized using Neural Architecture Search, are developed for One-vs-All classification. This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample. Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the effectiveness and robustness of the proposed methodology.","sentences":["The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems.","Here, a novel approach for power grid classification is developed, leveraging ENF.","Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers.","Four traditional machine learning classifiers plus a Convolutional Neural Network (CNN), optimized using Neural Architecture Search, are developed for One-vs-All classification.","This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample.","Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the effectiveness and robustness of the proposed methodology."],"url":"http://arxiv.org/abs/2403.18402v1","category":"cs.LG"}
{"created":"2024-03-27 09:42:56","title":"Force generation by a cylindrical cell under stationary osmolytes synthesis","abstract":"Turgor is the driving force of plant growth, making possible for roots to overcome soil resistance or for stems to counteract gravity. Maintaining a constant growth rate while avoiding the cell content dilution, which would progressively stop the inward water flux, imposes the production or import of osmolytes in proportion to the increase of volume. We coin this phenomenon stationary osmoregulation. The article explores the quantitative consequences of this hypothesis on the interaction of a cylindrical cell growing axially against an obstacle.   An instantaneous axial compression of a pressurized cylindrical cell generates a force and a pressure jump which both decrease toward a lower value once water has flowed out of the cell to reach the water potential equilibrium. In a first part, the article derives analytical formula for these force and over-pressure both before and after relaxation. In a second part, we describe how the coupling of the Lockhart's growth law with the stationary osmoregulation hypothesis predicts a transient slowdown in growth due to contact before a re-acceleration in growth. We finally compare these predictions with the output of an elastic growth model which ignores the osmotic origin of growth: models only match in the early phase of contact for high stiffness obstacle.","sentences":["Turgor is the driving force of plant growth, making possible for roots to overcome soil resistance or for stems to counteract gravity.","Maintaining a constant growth rate while avoiding the cell content dilution, which would progressively stop the inward water flux, imposes the production or import of osmolytes in proportion to the increase of volume.","We coin this phenomenon stationary osmoregulation.","The article explores the quantitative consequences of this hypothesis on the interaction of a cylindrical cell growing axially against an obstacle.   ","An instantaneous axial compression of a pressurized cylindrical cell generates a force and a pressure jump which both decrease toward a lower value once water has flowed out of the cell to reach the water potential equilibrium.","In a first part, the article derives analytical formula for these force and over-pressure both before and after relaxation.","In a second part, we describe how the coupling of the Lockhart's growth law with the stationary osmoregulation hypothesis predicts a transient slowdown in growth due to contact before a re-acceleration in growth.","We finally compare these predictions with the output of an elastic growth model which ignores the osmotic origin of growth: models only match in the early phase of contact for high stiffness obstacle."],"url":"http://arxiv.org/abs/2403.18401v1","category":"q-bio.SC"}
{"created":"2024-03-27 09:35:56","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks","abstract":"Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem.","sentences":["Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist.","Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning.","This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN).","GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space.","However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls.","This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation.","The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns.","The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing.","Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference.","These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem."],"url":"http://arxiv.org/abs/2403.18397v1","category":"cs.CV"}
{"created":"2024-03-27 09:35:42","title":"Asymptotic symmetries from the string worldsheet","abstract":"In IIB string theory on AdS$_3$ background with NS-NS fluxes, we show that Brown-Henneaux asymptotic Killing vectors can be derived by requiring both the worldsheet equations of motion and Virasoro constraints are preserved near the asymptotic boundary of the target spacetime. The charges on the worldsheet that generate the corresponding transformations can be written down in both the Lagrangian formalism and Hamiltonian formalism. This provides a method of studying asymptotic symmetry of the target spacetime directly from worldsheet string theories, without using results from the supergravity limit. As an example, we apply this method to flat spacetime in three dimensions and obtain the BMS$_3$ generators on the worldsheet theory.","sentences":["In IIB string theory on AdS$_3$ background with NS-NS fluxes, we show that Brown-Henneaux asymptotic Killing vectors can be derived by requiring both the worldsheet equations of motion and Virasoro constraints are preserved near the asymptotic boundary of the target spacetime.","The charges on the worldsheet that generate the corresponding transformations can be written down in both the Lagrangian formalism and Hamiltonian formalism.","This provides a method of studying asymptotic symmetry of the target spacetime directly from worldsheet string theories, without using results from the supergravity limit.","As an example, we apply this method to flat spacetime in three dimensions and obtain the BMS$_3$ generators on the worldsheet theory."],"url":"http://arxiv.org/abs/2403.18396v1","category":"hep-th"}
{"created":"2024-03-27 09:25:20","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","abstract":"Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes. However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation -- stemming from the temporal dynamics of spiking neurons and their discrete signal processing -- which necessitates alternative ways of training, most notably through ANN-SNN conversion. In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead. We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step. We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus eliminating the computational burden of backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet datasets, achieving a notable increase in accuracy on all datasets. Codes are released at a GitHub repository.","sentences":["Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes.","However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation -- stemming from the temporal dynamics of spiking neurons and their discrete signal processing -- which necessitates alternative ways of training, most notably through ANN-SNN conversion.","In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead.","We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step.","We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus eliminating the computational burden of backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet datasets, achieving a notable increase in accuracy on all datasets.","Codes are released at a GitHub repository."],"url":"http://arxiv.org/abs/2403.18388v1","category":"cs.AI"}
{"created":"2024-03-27 09:21:07","title":"Generative Multi-modal Models are Good Class-Incremental Learners","abstract":"In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction. In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios. Under the Few-shot CIL setting, we have improved by at least 14\\% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at \\url{https://github.com/DoubleClass/GMM}.","sentences":["In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge.","It is mainly caused by the characteristic of discriminative models.","With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL.","However, transitioning from discriminative to generative models requires addressing two key challenges.","The primary challenge lies in transferring the generated textual information into the classification of distinct categories.","Additionally, it requires formulating the task of CIL within a generative framework.","To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning.","Our approach directly generates labels for images using an adapted generative model.","After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction.","In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios.","Under the Few-shot CIL setting, we have improved by at least 14\\% accuracy over all the current state-of-the-art methods with significantly less forgetting.","Our code is available at \\url{https://github.com/DoubleClass/GMM}."],"url":"http://arxiv.org/abs/2403.18383v1","category":"cs.CV"}
{"created":"2024-03-27 09:19:13","title":"Improving Attributed Text Generation of Large Language Models via Preference Learning","abstract":"Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information. Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality.","sentences":["Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content.","Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations).","However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility.","In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework.","First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets.","Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs.","Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information.","Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality."],"url":"http://arxiv.org/abs/2403.18381v1","category":"cs.CL"}
{"created":"2024-03-27 09:17:50","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction","abstract":"Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing MLP, capturing local temporal patterns in the short-term period, and the inter-patch mixing MLP, capturing global temporal patterns in the long-term period. Notably, to address the varying importance of features in RUL prediction, we introduce a weighted loss function in the MLP-Mixer-based architecture, marking the first time such an approach has been employed. Our experiments demonstrate that IIP-Mixer achieves competitive performance in battery RUL prediction, outperforming other popular time-series frameworks","sentences":["Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems.","However, this task is often challenging due to the complex temporal dynamics involved.","Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting.","Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns.","To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction.","The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing MLP, capturing local temporal patterns in the short-term period, and the inter-patch mixing MLP, capturing global temporal patterns in the long-term period.","Notably, to address the varying importance of features in RUL prediction, we introduce a weighted loss function in the MLP-Mixer-based architecture, marking the first time such an approach has been employed.","Our experiments demonstrate that IIP-Mixer achieves competitive performance in battery RUL prediction, outperforming other popular time-series frameworks"],"url":"http://arxiv.org/abs/2403.18379v1","category":"cs.LG"}
{"created":"2024-03-27 09:08:20","title":"Bulk-entanglement-spectrum correspondence in $PT$- and $PC$-symmetric topological insulators","abstract":"In this study, we discuss a new type of bulk-boundary correspondence which holds for topological insulators when the parity-time ($PT$) and/or parity-particle-hole ($PC$) symmetry are present. In these systems, even when the bulk topology is nontrivial, the edge spectrum is generally gapped, and thus the conventional bulk-boundary correspondence does not hold. We find that, instead of the edge spectrum, the single-particle entanglement spectrum becomes gapless when the bulk topology is nontrivial: i.e., the {\\it bulk-entanglement-spectrum correspondence} holds in $PT$- and/or $PC$-symmetric topological insulators. After showing the correspondence using $K$-theoretic approach, we provide concrete models for each symmetry class up to three dimensions where nontrivial topology due to $PT$ and/or $PC$ is expected. An implication of our results is that, when the bulk topology under $PT$ and/or $PC$ symmetry is nontrivial, the many-body entanglement spectrum is multiply degenerate in one dimension and is gapless in two or higher dimensions.","sentences":["In this study, we discuss a new type of bulk-boundary correspondence which holds for topological insulators when the parity-time ($PT$) and/or parity-particle-hole ($PC$) symmetry are present.","In these systems, even when the bulk topology is nontrivial, the edge spectrum is generally gapped, and thus the conventional bulk-boundary correspondence does not hold.","We find that, instead of the edge spectrum, the single-particle entanglement spectrum becomes gapless when the bulk topology is nontrivial: i.e., the {\\it bulk-entanglement-spectrum correspondence} holds in $PT$- and/or $PC$-symmetric topological insulators.","After showing the correspondence using $K$-theoretic approach, we provide concrete models for each symmetry class up to three dimensions where nontrivial topology due to $PT$ and/or $PC$ is expected.","An implication of our results is that, when the bulk topology under $PT$ and/or $PC$ symmetry is nontrivial, the many-body entanglement spectrum is multiply degenerate in one dimension and is gapless in two or higher dimensions."],"url":"http://arxiv.org/abs/2403.18372v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 09:06:36","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","abstract":"In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: https://github.com/LuigiSigillo/ShipinSight .","sentences":["In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution.","A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images.","In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance.","We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned.","In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image.","Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\\footnote{\\url{www.shipspotting.com}} website.","Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed.","Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario.","Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks.","The code is available at: https://github.com/LuigiSigillo/ShipinSight ."],"url":"http://arxiv.org/abs/2403.18370v1","category":"cs.CV"}
{"created":"2024-03-27 17:44:29","title":"SolderlessPCB: Reusing Electronic Components in PCB Prototyping through Detachable 3D Printed Housings","abstract":"The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste. This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components. Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering. We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability. To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes. We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission. The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges.","sentences":["The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste.","This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components.","Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering.","We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability.","To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes.","We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission.","The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges."],"url":"http://arxiv.org/abs/2403.18797v1","category":"cs.HC"}
{"created":"2024-03-27 16:22:45","title":"A vascular synthetic model for improved aneurysm segmentation and detection via Deep Neural Networks","abstract":"We hereby present a full synthetic model, able to mimic the various constituents of the cerebral vascular tree: the cerebral arteries, the bifurcations and the intracranial aneurysms. By building this model, our goal was to provide a substantial dataset of brain arteries which could be used by a 3D Convolutional Neural Network (CNN) to either segment or detect/recognize various vascular diseases (such as artery dissection/thrombosis) or even some portions of the cerebral vasculature, such as the bifurcations or aneurysms. In this study, we will particularly focus on Intra-Cranial Aneurysm (ICA) detection and segmentation. The cerebral aneurysms most often occur on a particular structure of the vascular tree named the Circle of Willis. Various studies have been conducted to detect and monitor the ICAs and those based on Deep Learning (DL) achieve the best performances. Specifically, in this work, we propose a full synthetic 3D model able to mimic the brain vasculature as acquired by Magnetic Resonance Angiography (MRA), and more particularly the Time Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF allows to have a relatively good rendering of the blood vessels and is non-invasive (no contrast liquid injection). Our model has been designed to simultaneously mimic the arteries geometry, the ICA shape and the background noise. The geometry of the vascular tree is modeled thanks to an interpolation with 3D Spline functions, and the statistical properties of the background MRI noise is collected from MRA acquisitions and reproduced within the model. In this work, we thoroughly describe the synthetic vasculature model, we build up a neural network designed for ICA segmentation and detection, and finally, we carry out an in-depth evaluation of the performance gap gained thanks to the synthetic model data augmentation.","sentences":["We hereby present a full synthetic model, able to mimic the various constituents of the cerebral vascular tree: the cerebral arteries, the bifurcations and the intracranial aneurysms.","By building this model, our goal was to provide a substantial dataset of brain arteries which could be used by a 3D Convolutional Neural Network (CNN) to either segment or detect/recognize various vascular diseases (such as artery dissection/thrombosis) or even some portions of the cerebral vasculature, such as the bifurcations or aneurysms.","In this study, we will particularly focus on Intra-Cranial Aneurysm (ICA) detection and segmentation.","The cerebral aneurysms most often occur on a particular structure of the vascular tree named the Circle of Willis.","Various studies have been conducted to detect and monitor the ICAs and those based on Deep Learning (DL) achieve the best performances.","Specifically, in this work, we propose a full synthetic 3D model able to mimic the brain vasculature as acquired by Magnetic Resonance Angiography (MRA), and more particularly the Time Of Flight (TOF) principle.","Among the various MRI modalities, the MRA-TOF allows to have a relatively good rendering of the blood vessels and is non-invasive (no contrast liquid injection).","Our model has been designed to simultaneously mimic the arteries geometry, the ICA shape and the background noise.","The geometry of the vascular tree is modeled thanks to an interpolation with 3D Spline functions, and the statistical properties of the background MRI noise is collected from MRA acquisitions and reproduced within the model.","In this work, we thoroughly describe the synthetic vasculature model, we build up a neural network designed for ICA segmentation and detection, and finally, we carry out an in-depth evaluation of the performance gap gained thanks to the synthetic model data augmentation."],"url":"http://arxiv.org/abs/2403.18734v1","category":"eess.IV"}
{"created":"2024-03-27 15:48:35","title":"Collective excitations in competing phases in two and three dimensions","abstract":"We investigate the superconducting (SC), charge-density wave (CDW), and antiferromagnetic (AFM) phases in the extended Hubbard model at zero temperature and half-filling. We employ the iterated equations of motion approach to compute the two-particle Green's functions and their spectral densities. This renders a comprehensive analysis of the behavior of collective excitations possible as the model's parameters are tuned across phase transitions. We identify the well-known amplitude (Higgs) and phase (Anderson-Bogoliubov) modes within the superconducting phase and observe a similar excitation (cooperon) in the CDW phase which shifts towards zero energy as the system approaches the phase transition to the SC phase. In the CDW phase, close to the phase transition to the AFM phase, we find a collective mode, an exciton, that does not change significantly and another mode, a longitudinal magnon, that emerges from the two-particle continuum as the system approaches the phase transition to the AFM phase. It becomes identical with the former at the transition. In the AFM phase, their roles are reversed. Additionally, we find a transversal Goldstone magnon located at zero energy.","sentences":["We investigate the superconducting (SC), charge-density wave (CDW), and antiferromagnetic (AFM) phases in the extended Hubbard model at zero temperature and half-filling.","We employ the iterated equations of motion approach to compute the two-particle Green's functions and their spectral densities.","This renders a comprehensive analysis of the behavior of collective excitations possible as the model's parameters are tuned across phase transitions.","We identify the well-known amplitude (Higgs) and phase (Anderson-Bogoliubov) modes within the superconducting phase and observe a similar excitation (cooperon) in the CDW phase which shifts towards zero energy as the system approaches the phase transition to the SC phase.","In the CDW phase, close to the phase transition to the AFM phase, we find a collective mode, an exciton, that does not change significantly and another mode, a longitudinal magnon, that emerges from the two-particle continuum as the system approaches the phase transition to the AFM phase.","It becomes identical with the former at the transition.","In the AFM phase, their roles are reversed.","Additionally, we find a transversal Goldstone magnon located at zero energy."],"url":"http://arxiv.org/abs/2403.18701v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 14:56:44","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution for Scania Collected Datasets","abstract":"Data annotation in autonomous vehicles is a critical step in the development of Deep Neural Network (DNN) based models or the performance evaluation of the perception system. This often takes the form of adding 3D bounding boxes on time-sequential and registered series of point-sets captured from active sensors like Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR). When annotating multiple active sensors, there is a need to motion compensate and translate the points to a consistent coordinate frame and timestamp respectively. However, highly dynamic objects pose a unique challenge, as they can appear at different timestamps in each sensor's data. Without knowing the speed of the objects, their position appears to be different in different sensor outputs. Thus, even after motion compensation, highly dynamic objects are not matched from multiple sensors in the same frame, and human annotators struggle to add unique bounding boxes that capture all objects. This article focuses on addressing this challenge, primarily within the context of Scania collected datasets. The proposed solution takes a track of an annotated object as input and uses the Moving Horizon Estimation (MHE) to robustly estimate its speed. The estimated speed profile is utilized to correct the position of the annotated box and add boxes to object clusters missed by the original annotation.","sentences":["Data annotation in autonomous vehicles is a critical step in the development of Deep Neural Network (DNN) based models or the performance evaluation of the perception system.","This often takes the form of adding 3D bounding boxes on time-sequential and registered series of point-sets captured from active sensors like Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR).","When annotating multiple active sensors, there is a need to motion compensate and translate the points to a consistent coordinate frame and timestamp respectively.","However, highly dynamic objects pose a unique challenge, as they can appear at different timestamps in each sensor's data.","Without knowing the speed of the objects, their position appears to be different in different sensor outputs.","Thus, even after motion compensation, highly dynamic objects are not matched from multiple sensors in the same frame, and human annotators struggle to add unique bounding boxes that capture all objects.","This article focuses on addressing this challenge, primarily within the context of Scania collected datasets.","The proposed solution takes a track of an annotated object as input and uses the Moving Horizon Estimation (MHE) to robustly estimate its speed.","The estimated speed profile is utilized to correct the position of the annotated box and add boxes to object clusters missed by the original annotation."],"url":"http://arxiv.org/abs/2403.18649v1","category":"cs.CV"}
{"created":"2024-03-27 14:28:44","title":"Scalable Lipschitz Estimation for CNNs","abstract":"Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate an enhanced scalability and comparable accuracy to existing baselines through a range of experiments.","sentences":["Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness.","Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications.","However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs.","To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs.","The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks.","We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks.","Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation.","We demonstrate an enhanced scalability and comparable accuracy to existing baselines through a range of experiments."],"url":"http://arxiv.org/abs/2403.18613v1","category":"cs.LG"}
{"created":"2024-03-27 13:59:09","title":"On Optimizing Hyperparameters for Quantum Neural Networks","abstract":"The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training. Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law. Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power. However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models. In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models. We compare different configurations and provide researchers with performance data and concrete suggestions for hyperparameter selection.","sentences":["The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training.","Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law.","Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint.","Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power.","However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models.","In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models.","We compare different configurations and provide researchers with performance data and concrete suggestions for hyperparameter selection."],"url":"http://arxiv.org/abs/2403.18579v1","category":"cs.LG"}
{"created":"2024-03-27 13:42:19","title":"A Dynamic Programming Approach for Road Traffic Estimation","abstract":"We consider a road network represented by a directed graph. We assume to collect many measurements of traffic flows on all the network arcs, or on a subset of them. We assume that the users are divided into different groups. Each group follows a different path. The flows of all user groups are modeled as a set of independent Poisson processes. Our focus is estimating the paths followed by each user group, and the means of the associated Poisson processes. We present a possible solution based on a Dynamic Programming algorithm. The method relies on the knowledge of high order cumulants. We discuss the theoretical properties of the introduced method. Finally, we present some numerical tests on well-known benchmark networks, using synthetic data.","sentences":["We consider a road network represented by a directed graph.","We assume to collect many measurements of traffic flows on all the network arcs, or on a subset of them.","We assume that the users are divided into different groups.","Each group follows a different path.","The flows of all user groups are modeled as a set of independent Poisson processes.","Our focus is estimating the paths followed by each user group, and the means of the associated Poisson processes.","We present a possible solution based on a Dynamic Programming algorithm.","The method relies on the knowledge of high order cumulants.","We discuss the theoretical properties of the introduced method.","Finally, we present some numerical tests on well-known benchmark networks, using synthetic data."],"url":"http://arxiv.org/abs/2403.18561v1","category":"eess.SY"}
{"created":"2024-03-27 11:22:00","title":"Cumulative Incidence Function Estimation Based on Population-Based Biobank Data","abstract":"Many countries have established population-based biobanks, which are being used increasingly in epidemiolgical and clinical research. These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies. However, using biobank data poses new challenges. Typically, biobank data is collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between $c_L$ and $c_U$. This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period. We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency, and (2) CIF estimation for ages before the lower limit, $c_L$.","sentences":["Many countries have established population-based biobanks, which are being used increasingly in epidemiolgical and clinical research.","These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies.","However, using biobank data poses new challenges.","Typically, biobank data is collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between $c_L$ and $c_U$. This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period.","We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency, and (2) CIF estimation for ages before the lower limit, $c_L$."],"url":"http://arxiv.org/abs/2403.18464v1","category":"stat.ME"}
{"created":"2024-03-27 11:13:20","title":"Scaling Vision-and-Language Navigation With Offline RL","abstract":"The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal offline trajectories. Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data. We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments.","sentences":["The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them.","On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky.","In contrast, it is easy to access large repositories of suboptimal offline trajectories.","Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data.","We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area.","We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments.","Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments."],"url":"http://arxiv.org/abs/2403.18454v1","category":"cs.CV"}
{"created":"2024-03-27 10:47:09","title":"Connections between metric differentiability and rectifiability","abstract":"We combine Kirchheim's metric differentials with Cheeger charts in order to establish a non-embeddability principle for any collection $\\mathcal C$ of Banach (or metric) spaces: if a metric measure space $X$ bi-Lipschitz embeds in some element in $\\mathcal C$, and if every Lipschitz map $X\\to Y\\in \\mathcal C$ is differentiable, then $X$ is rectifiable. This gives a simple proof of the rectifiability of Lipschitz differentiability spaces that are bi-Lipschitz embeddable in Euclidean space, due to Kell--Mondino. Our principle also implies a converse to Kirchheim's theorem: if all Lipschitz maps from a domain space to arbitrary targets are metrically differentiable, the domain is rectifiable. We moreover establish the compatibility of metric and w$^*$-differentials of maps from metric spaces in the spirit of Ambrosio--Kirchheim.","sentences":["We combine Kirchheim's metric differentials with Cheeger charts in order to establish a non-embeddability principle for any collection $\\mathcal C$ of Banach (or metric) spaces: if a metric measure space $X$ bi-Lipschitz embeds in some element in $\\mathcal C$, and if every Lipschitz map $X\\to Y\\in \\mathcal C$ is differentiable, then $X$ is rectifiable.","This gives a simple proof of the rectifiability of Lipschitz differentiability spaces that are bi-Lipschitz embeddable in Euclidean space, due to Kell--Mondino.","Our principle also implies a converse to Kirchheim's theorem: if all Lipschitz maps from a domain space to arbitrary targets are metrically differentiable, the domain is rectifiable.","We moreover establish the compatibility of metric and w$^*$-differentials of maps from metric spaces in the spirit of Ambrosio--Kirchheim."],"url":"http://arxiv.org/abs/2403.18440v1","category":"math.MG"}
{"created":"2024-03-27 10:40:27","title":"Collaborative Active Learning in Conditional Trust Environment","abstract":"In this paper, we investigate collaborative active learning, a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models. Instead, the collaborators share prediction results from the new domain and newly acquired labels. This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs. To realize these benefits, we introduce a collaborative active learning framework designed to fulfill the aforementioned objectives. We validate the effectiveness of the proposed framework through simulations. The results demonstrate that collaboration leads to higher AUC scores compared to independent efforts, highlighting the framework's ability to overcome the limitations of individual models. These findings support the use of collaborative approaches in active learning, emphasizing their potential to enhance outcomes through collective expertise and shared resources. Our work provides a foundation for further research on collaborative active learning and its practical applications in various domains where data privacy, cost efficiency, and model performance are critical considerations.","sentences":["In this paper, we investigate collaborative active learning, a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models.","Instead, the collaborators share prediction results from the new domain and newly acquired labels.","This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs.","To realize these benefits, we introduce a collaborative active learning framework designed to fulfill the aforementioned objectives.","We validate the effectiveness of the proposed framework through simulations.","The results demonstrate that collaboration leads to higher AUC scores compared to independent efforts, highlighting the framework's ability to overcome the limitations of individual models.","These findings support the use of collaborative approaches in active learning, emphasizing their potential to enhance outcomes through collective expertise and shared resources.","Our work provides a foundation for further research on collaborative active learning and its practical applications in various domains where data privacy, cost efficiency, and model performance are critical considerations."],"url":"http://arxiv.org/abs/2403.18436v1","category":"cs.LG"}
{"created":"2024-03-27 10:40:08","title":"iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing","abstract":"Hand-over-face gestures can provide important implicit interactions during conversations, such as frustration or excitement. However, in situations where interlocutors are not visible, such as phone calls or textual communication, the potential meaning contained in the hand-over-face gestures is lost. In this work, we present iFace, an unobtrusive, wearable impedance-sensing solution for recognizing different hand-over-face gestures. In contrast to most existing works, iFace does not require the placement of sensors on the user's face or hands. Instead, we proposed a novel sensing configuration, the shoulders, which remains invisible to both the user and outside observers. The system can monitor the shoulder-to-shoulder impedance variation caused by gestures through electrodes attached to each shoulder. We evaluated iFace in a user study with eight participants, collecting six kinds of hand-over-face gestures with different meanings. Using a convolutional neural network and a user-dependent classification, iFace reaches 82.58 \\% macro F1 score. We discuss potential application scenarios of iFace as an implicit interaction interface.","sentences":["Hand-over-face gestures can provide important implicit interactions during conversations, such as frustration or excitement.","However, in situations where interlocutors are not visible, such as phone calls or textual communication, the potential meaning contained in the hand-over-face gestures is lost.","In this work, we present iFace, an unobtrusive, wearable impedance-sensing solution for recognizing different hand-over-face gestures.","In contrast to most existing works, iFace does not require the placement of sensors on the user's face or hands.","Instead, we proposed a novel sensing configuration, the shoulders, which remains invisible to both the user and outside observers.","The system can monitor the shoulder-to-shoulder impedance variation caused by gestures through electrodes attached to each shoulder.","We evaluated iFace in a user study with eight participants, collecting six kinds of hand-over-face gestures with different meanings.","Using a convolutional neural network and a user-dependent classification, iFace reaches 82.58 \\% macro F1 score.","We discuss potential application scenarios of iFace as an implicit interaction interface."],"url":"http://arxiv.org/abs/2403.18433v1","category":"cs.HC"}
{"created":"2024-03-27 10:38:32","title":"$l^2$ decoupling theorem for surfaces in $\\mathbb{R}^3$","abstract":"We identify a new way to divide the $\\delta$-neighborhood of surfaces $\\mathcal{M}\\subset\\mathbb{R}^3$ into a finitely-overlapping collection of rectangular boxes $S$. We obtain a sharp $(l^2,L^p)$ decoupling estimate using this decomposition, for the sharp range of exponents $2\\leq p\\leq 4$. Our decoupling inequality leads to new exponential sum estimates where the frequencies lie on surfaces which do not contain a line.","sentences":["We identify a new way to divide the $\\delta$-neighborhood of surfaces $\\mathcal{M}\\subset\\mathbb{R}^3$ into a finitely-overlapping collection of rectangular boxes $S$. We obtain a sharp $(l^2,L^p)$ decoupling estimate using this decomposition, for the sharp range of exponents $2\\leq p\\leq","4$.","Our decoupling inequality leads to new exponential sum estimates where the frequencies lie on surfaces which do not contain a line."],"url":"http://arxiv.org/abs/2403.18431v1","category":"math.CA"}
{"created":"2024-03-27 09:46:22","title":"Convexity of near-optimal orthogonal-pair-free sets on the unit sphere","abstract":"A subset $S$ of the unit sphere $\\mathbb{S}^2$ is called orthogonal-pair-free if and only if there do not exist two distinct points $u, v \\in S$ at distance $\\frac{\\pi}{2}$ from each other. Witsenhausen \\cite{witsenhausen} asked the following question: {\\it What is the least upper bound $\\alpha_3$ on the Lesbegue measure of any measurable orthogonal-pair-free subset of $\\mathbb{S}^2$?} We prove the following result in this paper: Let $\\mathcal{A}$ be the collection of all orthogonal-pair-free sets $S$ such that $S$ consists of a finite number of mutually disjoint convex sets. Then, $\\alpha_3 = \\limsup_{S \\in \\mathcal{A}} \\mu(S)$. Thus, if the double cap conjecture \\cite{kalai1} is not true, there is a set in $\\mathcal{A}$ with measure strictly greater than the measure of the double cap.","sentences":["A subset $S$ of the unit sphere $\\mathbb{S}^2$ is called orthogonal-pair-free if and only if there do not exist two distinct points $u, v \\in S$ at distance $\\frac{\\pi}{2}$ from each other.","Witsenhausen \\cite{witsenhausen} asked the following question: {\\it What is the least upper bound $\\alpha_3$ on the Lesbegue measure of any measurable orthogonal-pair-free subset of $\\mathbb{S}^2$?}","We prove the following result in this paper: Let $\\mathcal{A}$ be the collection of all orthogonal-pair-free sets $S$ such that $S$ consists of a finite number of mutually disjoint convex sets.","Then, $\\alpha_3 = \\limsup_{S \\in \\mathcal{A}} \\mu(S)$. Thus, if the double cap conjecture \\cite{kalai1} is not true, there is a set in $\\mathcal{A}$ with measure strictly greater than the measure of the double cap."],"url":"http://arxiv.org/abs/2403.18404v1","category":"cs.CG"}
{"created":"2024-03-27 08:57:15","title":"Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR","abstract":"We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed scheduler also outperforms the contention-free and contention-based schemes in maximizing the number of successfully computed tasks.","sentences":["We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival.","A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs.","The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals.","Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy.","Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches.","The proposed scheduler also outperforms the contention-free and contention-based schemes in maximizing the number of successfully computed tasks."],"url":"http://arxiv.org/abs/2403.18364v1","category":"cs.IT"}
{"created":"2024-03-27 08:48:47","title":"MonoHair: High-Fidelity Hair Modeling from a Monocular Video","abstract":"Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic expression, and immersion in computer graphics. While existing 3D hair modeling methods have achieved impressive performance, the challenge of achieving high-quality hair reconstruction persists: they either require strict capture conditions, making practical applications difficult, or heavily rely on learned prior data, obscuring fine-grained details in images. To address these challenges, we propose MonoHair,a generic framework to achieve high-fidelity hair reconstruction from a monocular video, without specific requirements for environments. Our approach bifurcates the hair modeling process into two main stages: precise exterior reconstruction and interior structure inference. The exterior is meticulously crafted using our Patch-based Multi-View Optimization (PMVO). This method strategically collects and integrates hair information from multiple views, independent of prior data, to produce a high-fidelity exterior 3D line map. This map not only captures intricate details but also facilitates the inference of the hair's inner structure. For the interior, we employ a data-driven, multi-view 3D hair reconstruction method. This method utilizes 2D structural renderings derived from the reconstructed exterior, mirroring the synthetic 2D inputs used during training. This alignment effectively bridges the domain gap between our training data and real-world data, thereby enhancing the accuracy and reliability of our interior structure inference. Lastly, we generate a strand model and resolve the directional ambiguity by our hair growth algorithm. Our experiments demonstrate that our method exhibits robustness across diverse hairstyles and achieves state-of-the-art performance. For more results, please refer to our project page https://keyuwu-cs.github.io/MonoHair/.","sentences":["Undoubtedly, high-fidelity 3D hair is crucial for achieving realism, artistic expression, and immersion in computer graphics.","While existing 3D hair modeling methods have achieved impressive performance, the challenge of achieving high-quality hair reconstruction persists: they either require strict capture conditions, making practical applications difficult, or heavily rely on learned prior data, obscuring fine-grained details in images.","To address these challenges, we propose MonoHair,a generic framework to achieve high-fidelity hair reconstruction from a monocular video, without specific requirements for environments.","Our approach bifurcates the hair modeling process into two main stages: precise exterior reconstruction and interior structure inference.","The exterior is meticulously crafted using our Patch-based Multi-View Optimization (PMVO).","This method strategically collects and integrates hair information from multiple views, independent of prior data, to produce a high-fidelity exterior 3D line map.","This map not only captures intricate details but also facilitates the inference of the hair's inner structure.","For the interior, we employ a data-driven, multi-view 3D hair reconstruction method.","This method utilizes 2D structural renderings derived from the reconstructed exterior, mirroring the synthetic 2D inputs used during training.","This alignment effectively bridges the domain gap between our training data and real-world data, thereby enhancing the accuracy and reliability of our interior structure inference.","Lastly, we generate a strand model and resolve the directional ambiguity by our hair growth algorithm.","Our experiments demonstrate that our method exhibits robustness across diverse hairstyles and achieves state-of-the-art performance.","For more results, please refer to our project page https://keyuwu-cs.github.io/MonoHair/."],"url":"http://arxiv.org/abs/2403.18356v1","category":"cs.CV"}
{"created":"2024-03-27 08:42:47","title":"Generating Diverse Agricultural Data for Vision-Based Farming Applications","abstract":"We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective solution for generating high-quality, diverse data but also addresses specific needs in agricultural vision tasks that are not fully covered by general-purpose models.","sentences":["We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds.","This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions.","The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data.","Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control.","We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture.","This approach not only provides a cost-effective solution for generating high-quality, diverse data but also addresses specific needs in agricultural vision tasks that are not fully covered by general-purpose models."],"url":"http://arxiv.org/abs/2403.18351v1","category":"cs.CV"}
{"created":"2024-03-27 08:38:56","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes","abstract":"The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics. Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems. For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches. However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs. In-spite of this, fast and accurate detection of CHs are till a major issues. Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region. The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the segmented image based on image morphological operation. In the work, quantum computing has been used to optimize the cost function of the fast fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm (QAOA) has been used to optimize the quadratic part of the cost function. The proposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image datasets and has been compared with the existing techniques. The outcome shows the comparable performance of the proposed method with the existing one within a very lesser time.","sentences":["The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics.","Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems.","For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches.","However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs.","In-spite of this, fast and accurate detection of CHs are till a major issues.","Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region.","The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the segmented image based on image morphological operation.","In the work, quantum computing has been used to optimize the cost function of the fast fuzzy c-mean (FFCM)","algorithm, where quantum approximate optimization algorithm (QAOA) has been used to optimize the quadratic part of the cost function.","The proposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image datasets and has been compared with the existing techniques.","The outcome shows the comparable performance of the proposed method with the existing one within a very lesser time."],"url":"http://arxiv.org/abs/2403.18347v1","category":"astro-ph.SR"}
{"created":"2024-03-27 08:34:55","title":"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models","abstract":"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM's powerful common sense reasoning abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions. Furthermore, we incorporate explanatory requirements into the prompts in the inference stage. Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability. Extensive experiments on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can encode comprehensive interaction information for driving behavior understanding.","sentences":["To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories.","Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability.","In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs).","Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task.","This allows us to utilize the LLM's powerful common sense reasoning abilities to understand complex interactive information, thereby improving the accuracy of long-term predictions.","Furthermore, we incorporate explanatory requirements into the prompts in the inference stage.","Therefore, our LC-LLM model not only can predict lane change intentions and trajectories but also provides explanations for its predictions, enhancing the interpretability.","Extensive experiments on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task.","To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior.","Our study shows that LLMs can encode comprehensive interaction information for driving behavior understanding."],"url":"http://arxiv.org/abs/2403.18344v1","category":"cs.AI"}
{"created":"2024-03-27 08:25:28","title":"mALBERT: Is a Compact Multilingual BERT Model Still Worth It?","abstract":"Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical andecological impact of such models. In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural LanguageUnderstanding, classification, Question--Answering tasks. PLMs also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model. We also evaluate the model against classicalmultilingual PLMs in classical NLP tasks. Finally, this paper proposes a rare study on the subword tokenizationimpact on language performances.","sentences":["Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical andecological impact of such models.","In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these PLM.","However,PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural LanguageUnderstanding, classification, Question--Answering tasks.","PLMs also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist.","Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model.","We also evaluate the model against classicalmultilingual PLMs in classical NLP tasks.","Finally, this paper proposes a rare study on the subword tokenizationimpact on language performances."],"url":"http://arxiv.org/abs/2403.18338v1","category":"cs.AI"}
{"created":"2024-03-27 08:11:45","title":"Neighbor-Environment Observer: An Intelligent Agent for Immersive Working Companionship","abstract":"Human-computer symbiosis is a crucial direction for the development of artificial intelligence. As intelligent systems become increasingly prevalent in our work and personal lives, it is important to develop strategies to support users across physical and virtual environments. While technological advances in personal digital devices, such as personal computers and virtual reality devices, can provide immersive experiences, they can also disrupt users' awareness of their surroundings and enhance the frustration caused by disturbances. In this paper, we propose a joint observation strategy for artificial agents to support users across virtual and physical environments. We introduce a prototype system, neighbor-environment observer (NEO), that utilizes non-invasive sensors to assist users in dealing with disruptions to their immersive experience. System experiments evaluate NEO from different perspectives and demonstrate the effectiveness of the joint observation strategy. A user study is conducted to evaluate its usability. The results show that NEO could lessen users' workload with the learned user preference. We suggest that the proposed strategy can be applied to various smart home scenarios.","sentences":["Human-computer symbiosis is a crucial direction for the development of artificial intelligence.","As intelligent systems become increasingly prevalent in our work and personal lives, it is important to develop strategies to support users across physical and virtual environments.","While technological advances in personal digital devices, such as personal computers and virtual reality devices, can provide immersive experiences, they can also disrupt users' awareness of their surroundings and enhance the frustration caused by disturbances.","In this paper, we propose a joint observation strategy for artificial agents to support users across virtual and physical environments.","We introduce a prototype system, neighbor-environment observer (NEO), that utilizes non-invasive sensors to assist users in dealing with disruptions to their immersive experience.","System experiments evaluate NEO from different perspectives and demonstrate the effectiveness of the joint observation strategy.","A user study is conducted to evaluate its usability.","The results show that NEO could lessen users' workload with the learned user preference.","We suggest that the proposed strategy can be applied to various smart home scenarios."],"url":"http://arxiv.org/abs/2403.18331v1","category":"cs.HC"}
{"created":"2024-03-27 08:08:00","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications","abstract":"Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.","sentences":["Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs.","This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications.","Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets.","We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input.","Our approach generates formal syntax using language grammars to automatically generate a dataset.","We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems."],"url":"http://arxiv.org/abs/2403.18327v1","category":"cs.CL"}
{"created":"2024-03-27 07:52:10","title":"Quantum Algorithms: A New Frontier in Financial Crime Prevention","abstract":"Financial crimes fast proliferation and sophistication require novel approaches that provide robust and effective solutions. This paper explores the potential of quantum algorithms in combating financial crimes. It highlights the advantages of quantum computing by examining traditional and Machine Learning (ML) techniques alongside quantum approaches. The study showcases advanced methodologies such as Quantum Machine Learning (QML) and Quantum Artificial Intelligence (QAI) as powerful solutions for detecting and preventing financial crimes, including money laundering, financial crime detection, cryptocurrency attacks, and market manipulation. These quantum approaches leverage the inherent computational capabilities of quantum computers to overcome limitations faced by classical methods. Furthermore, the paper illustrates how quantum computing can support enhanced financial risk management analysis. Financial institutions can improve their ability to identify and mitigate risks, leading to more robust risk management strategies by exploiting the quantum advantage. This research underscores the transformative impact of quantum algorithms on financial risk management. By embracing quantum technologies, organisations can enhance their capabilities to combat evolving threats and ensure the integrity and stability of financial systems.","sentences":["Financial crimes fast proliferation and sophistication require novel approaches that provide robust and effective solutions.","This paper explores the potential of quantum algorithms in combating financial crimes.","It highlights the advantages of quantum computing by examining traditional and Machine Learning (ML) techniques alongside quantum approaches.","The study showcases advanced methodologies such as Quantum Machine Learning (QML) and Quantum Artificial Intelligence (QAI) as powerful solutions for detecting and preventing financial crimes, including money laundering, financial crime detection, cryptocurrency attacks, and market manipulation.","These quantum approaches leverage the inherent computational capabilities of quantum computers to overcome limitations faced by classical methods.","Furthermore, the paper illustrates how quantum computing can support enhanced financial risk management analysis.","Financial institutions can improve their ability to identify and mitigate risks, leading to more robust risk management strategies by exploiting the quantum advantage.","This research underscores the transformative impact of quantum algorithms on financial risk management.","By embracing quantum technologies, organisations can enhance their capabilities to combat evolving threats and ensure the integrity and stability of financial systems."],"url":"http://arxiv.org/abs/2403.18322v1","category":"cs.LG"}
{"created":"2024-03-27 07:34:44","title":"Chinese Offensive Language Detection:Current Status and Future Directions","abstract":"Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary objective of this survey is to explore the existing techniques and identify potential avenues for further research that can address the cultural and linguistic complexities of Chinese.","sentences":["Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge.","Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time.","However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically.","This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language.","The primary objective of this survey is to explore the existing techniques and identify potential avenues for further research that can address the cultural and linguistic complexities of Chinese."],"url":"http://arxiv.org/abs/2403.18314v1","category":"cs.CL"}
{"created":"2024-03-27 07:22:32","title":"A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites","abstract":"This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic loading-unloading experimental tests. Numerical examples show that the PIDL model can accurately predict the mechanical behavior of epoxy-based nanocomposites for different volume fractions of fibers and nanoparticles under various hygrothermal conditions.","sentences":["This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.","The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model.","To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials.","In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system.","The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model.","The model is then trained by extracting the data directly from cyclic loading-unloading experimental tests.","Numerical examples show that the PIDL model can accurately predict the mechanical behavior of epoxy-based nanocomposites for different volume fractions of fibers and nanoparticles under various hygrothermal conditions."],"url":"http://arxiv.org/abs/2403.18310v1","category":"cs.LG"}
{"created":"2024-03-27 07:10:09","title":"Mutual Information Optimization for SIM-Based Holographic MIMO Systems","abstract":"In the context of emerging stacked intelligent metasurface (SIM)-based holographic MIMO (HMIMO) systems, a fundamental problem is to study the mutual information (MI) between transmitted and received signals to establish their capacity. However, direct optimization or analytical evaluation of the MI, particularly for discrete signaling, is often intractable. To address this challenge, we adopt the channel cutoff rate (CR) as an alternative optimization metric for the MI maximization. In this regard, we propose an alternating projected gradient method (APGM), which optimizes the CR of a SIM-based HMIMO system by adjusting signal precoding and the phase shifts across the transmit and receive SIMs in a layer-by-layer basis. Simulation results indicate that the proposed algorithm significantly enhances the CR, achieving substantial gains proportional to those observed for the corresponding MI. This justifies the effectiveness of using the channel CR for the MI optimization. Moreover, we demonstrate that the integration of digital precoding, even on a modest scale, has a significant impact on the ultimate performance of SIM-aided systems.","sentences":["In the context of emerging stacked intelligent metasurface (SIM)-based holographic MIMO (HMIMO) systems, a fundamental problem is to study the mutual information (MI) between transmitted and received signals to establish their capacity.","However, direct optimization or analytical evaluation of the MI, particularly for discrete signaling, is often intractable.","To address this challenge, we adopt the channel cutoff rate (CR) as an alternative optimization metric for the MI maximization.","In this regard, we propose an alternating projected gradient method (APGM), which optimizes the CR of a SIM-based HMIMO system by adjusting signal precoding and the phase shifts across the transmit and receive SIMs in a layer-by-layer basis.","Simulation results indicate that the proposed algorithm significantly enhances the CR, achieving substantial gains proportional to those observed for the corresponding MI.","This justifies the effectiveness of using the channel CR for the MI optimization.","Moreover, we demonstrate that the integration of digital precoding, even on a modest scale, has a significant impact on the ultimate performance of SIM-aided systems."],"url":"http://arxiv.org/abs/2403.18307v1","category":"cs.IT"}
{"created":"2024-03-27 07:03:17","title":"Sm-Nd Isotope Data Compilation from Geoscientific Literature Using an Automated Tabular Extraction Method","abstract":"The rare earth elements Sm and Nd significantly address fundamental questions about crustal growth, such as its spatiotemporal evolution and the interplay between orogenesis and crustal accretion. Their relative immobility during high-grade metamorphism makes the Sm-Nd isotopic system crucial for inferring crustal formation times. Historically, data have been disseminated sporadically in the scientific literature due to complicated and costly sampling procedures, resulting in a fragmented knowledge base. However, the scattering of critical geoscience data across multiple publications poses significant challenges regarding human capital and time. In response, we present an automated tabular extraction method for harvesting tabular geoscience data. We collect 10,624 Sm-Nd data entries from 9,138 tables in over 20,000 geoscience publications using this method. We manually selected 2,118 data points from it to supplement our previously constructed global Sm-Nd dataset, increasing its sample count by over 20\\%. Our automatic data collection methodology enhances the efficiency of data acquisition processes spanning various scientific domains. Furthermore, the constructed Sm-Nd isotopic dataset should motivate the research of classifying global orogenic belts.","sentences":["The rare earth elements Sm and Nd significantly address fundamental questions about crustal growth, such as its spatiotemporal evolution and the interplay between orogenesis and crustal accretion.","Their relative immobility during high-grade metamorphism makes the Sm-Nd isotopic system crucial for inferring crustal formation times.","Historically, data have been disseminated sporadically in the scientific literature due to complicated and costly sampling procedures, resulting in a fragmented knowledge base.","However, the scattering of critical geoscience data across multiple publications poses significant challenges regarding human capital and time.","In response, we present an automated tabular extraction method for harvesting tabular geoscience data.","We collect 10,624 Sm-Nd data entries from 9,138 tables in over 20,000 geoscience publications using this method.","We manually selected 2,118 data points from it to supplement our previously constructed global Sm-Nd dataset, increasing its sample count by over 20\\%.","Our automatic data collection methodology enhances the efficiency of data acquisition processes spanning various scientific domains.","Furthermore, the constructed Sm-Nd isotopic dataset should motivate the research of classifying global orogenic belts."],"url":"http://arxiv.org/abs/2403.18306v1","category":"cs.DB"}
{"created":"2024-03-27 06:59:39","title":"A Recommender System for NFT Collectibles with Item Feature","abstract":"Recommender systems have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market. This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences. We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the performance of the graph-based recommender system improves significantly after utilizing all types of item features as side information, thereby outperforming all other baselines.","sentences":["Recommender systems have been actively studied and applied in various domains to deal with information overload.","Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market.","This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences.","We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure.","Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature.","Numerical experiments verify the performance of the graph-based recommender system improves significantly after utilizing all types of item features as side information, thereby outperforming all other baselines."],"url":"http://arxiv.org/abs/2403.18305v1","category":"cs.IR"}
{"created":"2024-03-27 06:58:01","title":"Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network","abstract":"Image super-resolution has been an important subject in image processing and recognition. Here, we present an attention-aided convolutional neural network (CNN) for solar image super-resolution. Our method, named SolarCNN, aims to enhance the quality of line-of-sight (LOS) magnetograms of solar active regions (ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and Heliospheric Observatory (SOHO). The ground-truth labels used for training SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist of strong magnetic fields in which magnetic energy can suddenly be released to produce extreme space weather events, such as solar flares, coronal mass ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI magnetograms allow for better understanding and forecasting of violent events of space weather. Experimental results show that SolarCNN improves the quality of SOHO/MDI magnetograms in terms of the structural similarity index measure (SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise ratio (PSNR).","sentences":["Image super-resolution has been an important subject in image processing and recognition.","Here, we present an attention-aided convolutional neural network (CNN) for solar image super-resolution.","Our method, named SolarCNN, aims to enhance the quality of line-of-sight (LOS) magnetograms of solar active regions (ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and Heliospheric Observatory (SOHO).","The ground-truth labels used for training SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO).","Solar ARs consist of strong magnetic fields in which magnetic energy can suddenly be released to produce extreme space weather events, such as solar flares, coronal mass ejections, and solar energetic particles.","SOHO/MDI covers Solar Cycle 23, which is stronger with more eruptive events than Cycle 24.","Enhanced SOHO/MDI magnetograms allow for better understanding and forecasting of violent events of space weather.","Experimental results show that SolarCNN improves the quality of SOHO/MDI magnetograms in terms of the structural similarity index measure (SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise ratio (PSNR)."],"url":"http://arxiv.org/abs/2403.18302v1","category":"astro-ph.SR"}
{"created":"2024-03-27 06:55:23","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","abstract":"The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective. We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.","sentences":["The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models.","However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness.","We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives.","On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective.","To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective.","The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective.","We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification.","We find that proposed SelMix fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks."],"url":"http://arxiv.org/abs/2403.18301v1","category":"cs.LG"}
{"created":"2024-03-27 06:46:59","title":"GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm","abstract":"Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's performance by varying the number of nodes, revealing its versatility as a new paradigm for semantic communication. Additionally, we show GeNet's robustness to geometric transformations by testing it with different rotation angles, without resorting to data augmentation.","sentences":["Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise.","However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources.","In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC).","We propose a novel approach where we first transform the input data image into graph structures.","Then we leverage a GNN-based encoder to extract semantic information from the source data.","This extracted semantic information is then transmitted through the channel.","At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC.","Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoupling the SNR dependency.","We further evaluate GeNet's performance by varying the number of nodes, revealing its versatility as a new paradigm for semantic communication.","Additionally, we show GeNet's robustness to geometric transformations by testing it with different rotation angles, without resorting to data augmentation."],"url":"http://arxiv.org/abs/2403.18296v1","category":"cs.LG"}
{"created":"2024-03-27 06:25:40","title":"Few-Shot Recalibration of Language Models","abstract":"Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.","sentences":["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct.","However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate).","To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration.","Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice.","Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice.","This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain.","Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."],"url":"http://arxiv.org/abs/2403.18286v1","category":"cs.CL"}
{"created":"2024-03-27 06:13:39","title":"Identification and Uses of Deep Learning Backbones via Pattern Mining","abstract":"Deep learning is extensively used in many areas of data mining as a black-box method with impressive results. However, understanding the core mechanism of how deep learning makes predictions is a relatively understudied problem. Here we explore the notion of identifying a backbone of deep learning for a given group of instances. A group here can be instances of the same class or even misclassified instances of the same class. We view each instance for a given group as activating a subset of neurons and attempt to find a subgraph of neurons associated with a given concept/group. We formulate this problem as a set cover style problem and show it is intractable and presents a highly constrained integer linear programming (ILP) formulation. As an alternative, we explore a coverage-based heuristic approach related to pattern mining, and show it converges to a Pareto equilibrium point of the ILP formulation. Experimentally we explore these backbones to identify mistakes and improve performance, explanation, and visualization. We demonstrate application-based results using several challenging data sets, including Bird Audio Detection (BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic MNIST data.","sentences":["Deep learning is extensively used in many areas of data mining as a black-box method with impressive results.","However, understanding the core mechanism of how deep learning makes predictions is a relatively understudied problem.","Here we explore the notion of identifying a backbone of deep learning for a given group of instances.","A group here can be instances of the same class or even misclassified instances of the same class.","We view each instance for a given group as activating a subset of neurons and attempt to find a subgraph of neurons associated with a given concept/group.","We formulate this problem as a set cover style problem and show it is intractable and presents a highly constrained integer linear programming (ILP) formulation.","As an alternative, we explore a coverage-based heuristic approach related to pattern mining, and show it converges to a Pareto equilibrium point of the ILP formulation.","Experimentally we explore these backbones to identify mistakes and improve performance, explanation, and visualization.","We demonstrate application-based results using several challenging data sets, including Bird Audio Detection (BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic MNIST data."],"url":"http://arxiv.org/abs/2403.18278v1","category":"cs.AI"}
{"created":"2024-03-27 05:41:50","title":"DSF-GAN: DownStream Feedback Generative Adversarial Network","abstract":"Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising real samples. All code and datasets used in this research will be made openly available for ease of reproduction.","sentences":["Utility and privacy are two crucial measurements of the quality of synthetic tabular data.","While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging.","To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN).","This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information.","Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples.","To evaluate our method, we tested it using two popular datasets.","Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback.","The evaluation was conducted on the same validation set comprising real samples.","All code and datasets used in this research will be made openly available for ease of reproduction."],"url":"http://arxiv.org/abs/2403.18267v1","category":"cs.LG"}
{"created":"2024-03-27 05:10:38","title":"Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach","abstract":"This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning. Through our experiments, we have found that integrating the forgetting mechanisms significantly enhances the models' performance in acquiring new knowledge, underscoring the positive role that strategic forgetting plays in the process of continual learning.","sentences":["This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data.","GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models.","The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans.","However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated.","In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning.","Through our experiments, we have found that integrating the forgetting mechanisms significantly enhances the models' performance in acquiring new knowledge, underscoring the positive role that strategic forgetting plays in the process of continual learning."],"url":"http://arxiv.org/abs/2403.18258v1","category":"cs.CV"}
{"created":"2024-03-27 04:56:48","title":"Manipulating Neural Path Planners via Slight Perturbations","abstract":"Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes. Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnoticeable object), that can nonetheless significantly compromise their integrity. We also discuss potential techniques to identify these backdoors aimed at alleviating such risks. We demonstrate our approach on both sampling-based and search-based neural path planners.","sentences":["Data-driven neural path planners are attracting increasing interest in the robotics community.","However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes.","Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors.","For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region.","In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners.","Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnoticeable object), that can nonetheless significantly compromise their integrity.","We also discuss potential techniques to identify these backdoors aimed at alleviating such risks.","We demonstrate our approach on both sampling-based and search-based neural path planners."],"url":"http://arxiv.org/abs/2403.18256v1","category":"cs.RO"}
{"created":"2024-03-27 04:49:23","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","abstract":"Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our model can consistently outperform the state-of-the-art (SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone visual representations, our model can closely match or even beat the SOTA MLLMs that are built on CLIP visual embeddings. Our code is available at https://github.com/LaVi-Lab/Visual-Table.","sentences":["Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet.","Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning.","In this work, we propose Visual Table, a novel visual representation tailored for MLLMs.","It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level.","We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our model can consistently outperform the state-of-the-art (SOTA) MLLMs across diverse benchmarks.","When visual tables serve as standalone visual representations, our model can closely match or even beat the SOTA MLLMs that are built on CLIP visual embeddings.","Our code is available at https://github.com/LaVi-Lab/Visual-Table."],"url":"http://arxiv.org/abs/2403.18252v1","category":"cs.CV"}
{"created":"2024-03-27 04:39:18","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges","abstract":"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.","sentences":["Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare.","Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored.","Thus, this work aims to determine whether prompting strategies can effectively narrow this gap.","Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency.","Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt).","Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.","To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts.","Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings."],"url":"http://arxiv.org/abs/2403.18249v1","category":"cs.CL"}
{"created":"2024-03-27 04:20:18","title":"Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check","abstract":"Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines. Moreover, we also release a Chinese CQA dataset with new features including reformulated question, extracted keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in RAG enhanced CQA.","sentences":["Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge.","Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied.","In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA).","In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings.","Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines.","Moreover, we also release a Chinese CQA dataset with new features including reformulated question, extracted keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in RAG enhanced CQA."],"url":"http://arxiv.org/abs/2403.18243v1","category":"cs.AI"}
{"created":"2024-03-27 04:09:34","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation","abstract":"3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis.","sentences":["3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints.","Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency.","As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints.","In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling.","To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes.","Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes.","This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis."],"url":"http://arxiv.org/abs/2403.18241v1","category":"cs.CV"}
{"created":"2024-03-27 04:07:10","title":"Determination of output composition in reaction-advection-diffusion systems on network reactors","abstract":"We consider reaction-transport processes in open reactors in which systems of first order reactions involving a number of gas species and solid catalysts can occur at localized active regions. Reaction products flow out of the reactor into vacuum conditions and are collected at an exit boundary. The output composition problem (OCP) is to determine the composition (molar fractions) of the collected gas after the reactor is fully emptied. We provide a solution to this problem in the form of a boundary-value problem for a system of time-independent partial differential equations. We then consider network-like reactors, which can be approximated by a network consisting of a collection of nodes and 1-dimensional branches, with reactions taking place at nodes. For these, it is possible to solve the OCP in a simple and effective way, giving explicit formulas for the output composition as a function of the reaction coefficients and parameters associated with the geometric configuration of the system. Several examples are given to illustrate the method.","sentences":["We consider reaction-transport processes in open reactors in which systems of first order reactions involving a number of gas species and solid catalysts can occur at localized active regions.","Reaction products flow out of the reactor into vacuum conditions and are collected at an exit boundary.","The output composition problem (OCP) is to determine the composition (molar fractions) of the collected gas after the reactor is fully emptied.","We provide a solution to this problem in the form of a boundary-value problem for a system of time-independent partial differential equations.","We then consider network-like reactors, which can be approximated by a network consisting of a collection of nodes and 1-dimensional branches, with reactions taking place at nodes.","For these, it is possible to solve the OCP in a simple and effective way, giving explicit formulas for the output composition as a function of the reaction coefficients and parameters associated with the geometric configuration of the system.","Several examples are given to illustrate the method."],"url":"http://arxiv.org/abs/2403.18239v1","category":"physics.chem-ph"}
{"created":"2024-03-27 03:33:32","title":"Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation","abstract":"Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the reasoning abilities of LLMs in complex human systems.","sentences":["Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge.","In this paper, we delve into the reasoning abilities of LLMs within complex human systems.","We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique.","In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation.","It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system.","We conduct the experiments within a communication game that mirrors real-world security scenarios.","The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the reasoning abilities of LLMs in complex human systems."],"url":"http://arxiv.org/abs/2403.18230v1","category":"cs.AI"}
{"created":"2024-03-27 03:25:45","title":"A Transformer-Based Framework for Payload Malware Detection and Classification","abstract":"As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attention mechanism. Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle. The payload bytes are used to detect malicious packets and classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our transformer-based model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79\\% using binary classification and 72\\% on the multi-classification experiment, both using solely payload bytes.","sentences":["As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial.","Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats.","IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity.","Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network.","In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head.","Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attention mechanism.","Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle.","The payload bytes are used to detect malicious packets and classify their types.","Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our transformer-based model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79\\% using binary classification and 72\\% on the multi-classification experiment, both using solely payload bytes."],"url":"http://arxiv.org/abs/2403.18223v1","category":"cs.CR"}
{"created":"2024-03-27 03:07:18","title":"From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries","abstract":"Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical analysis, the study evaluates agents' learning trajectories and adaptation processes, revealing insights into the efficacy of RL algorithms in navigating complex, multi-dimensional spaces. Reflections on the findings prompt considerations for future research, particularly in understanding the dynamics of learning in higher-dimensional environments.","sentences":["Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms.","This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions.","A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics.","The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension.","The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings?","Through empirical analysis, the study evaluates agents' learning trajectories and adaptation processes, revealing insights into the efficacy of RL algorithms in navigating complex, multi-dimensional spaces.","Reflections on the findings prompt considerations for future research, particularly in understanding the dynamics of learning in higher-dimensional environments."],"url":"http://arxiv.org/abs/2403.18219v1","category":"cs.LG"}
{"created":"2024-03-27 03:04:21","title":"Leveraging Large Language Models for Fuzzy String Matching in Political Science","abstract":"Fuzzy string matching remains a key issue when political scientists combine data from different sources. Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity. As such, they are inherently incapable of matching strings that refer to the same entity with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and ''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In this letter, we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner. Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists. Moreover, our results are robust against various temperatures. We further note that enhanced prompting can lead to additional performance improvements.","sentences":["Fuzzy string matching remains a key issue when political scientists combine data from different sources.","Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity.","As such, they are inherently incapable of matching strings that refer to the same entity with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and ''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''.","In this letter, we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner.","Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists.","Moreover, our results are robust against various temperatures.","We further note that enhanced prompting can lead to additional performance improvements."],"url":"http://arxiv.org/abs/2403.18218v1","category":"cs.AI"}
{"created":"2024-03-27 02:46:09","title":"Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies","abstract":"Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions. In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation. Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches. We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions.","sentences":["Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes.","In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals.","Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$).","To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP.","Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP.","To synthesize a most preferred policy, our technical approach includes two key steps.","In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions.","In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation.","Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches.","We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions."],"url":"http://arxiv.org/abs/2403.18212v1","category":"cs.RO"}
{"created":"2024-03-27 02:41:52","title":"Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving","abstract":"Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the vehicle throughout the decision-making process. In addition, we develop a safe RL method with dual-constraint optimization based on the Lagrange multiplier to optimize the training process for end-to-end autonomous driving. Comprehensive experiments were conducted on the MetaDrive simulator. Experimental results demonstrate that the proposed method achieves higher safety in continuous state and action tasks, and exhibits higher exploration performance in long-distance decision-making tasks compared with state-of-the-art methods.","sentences":["Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving.","Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks.","Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms.","In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL.","The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the vehicle throughout the decision-making process.","In addition, we develop a safe RL method with dual-constraint optimization based on the Lagrange multiplier to optimize the training process for end-to-end autonomous driving.","Comprehensive experiments were conducted on the MetaDrive simulator.","Experimental results demonstrate that the proposed method achieves higher safety in continuous state and action tasks, and exhibits higher exploration performance in long-distance decision-making tasks compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.18209v1","category":"cs.LG"}
{"created":"2024-03-27 02:39:23","title":"An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition","abstract":"Hand gesture recognition (HGR) based on multimodal data has attracted considerable attention owing to its great potential in applications. Various manually designed multimodal deep networks have performed well in multimodal HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials. To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS). Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the multimodal data, allowing for the automatic construction of multimodal networks with different architectures through decoding. Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to various datasets, the ENAS framework is designed to automatically search a MHGR network with appropriate fusion positions and ratios. To the best of our knowledge, this is the first time that ENAS has been utilized in MHGR to tackle issues related to the fusion position and ratio of multimodal data. Experimental results demonstrate that AMF-ENAS achieves state-of-the-art performance on the Ninapro DB2, DB3, and DB7 datasets.","sentences":["Hand gesture recognition (HGR) based on multimodal data has attracted considerable attention owing to its great potential in applications.","Various manually designed multimodal deep networks have performed well in multimodal HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials.","To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS).","Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the multimodal data, allowing for the automatic construction of multimodal networks with different architectures through decoding.","Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC.","To automatically adapt to various datasets, the ENAS framework is designed to automatically search a MHGR network with appropriate fusion positions and ratios.","To the best of our knowledge, this is the first time that ENAS has been utilized in MHGR to tackle issues related to the fusion position and ratio of multimodal data.","Experimental results demonstrate that AMF-ENAS achieves state-of-the-art performance on the Ninapro DB2, DB3, and DB7 datasets."],"url":"http://arxiv.org/abs/2403.18208v1","category":"cs.CV"}
{"created":"2024-03-27 02:31:54","title":"Exploring the Privacy Protection Capabilities of Chinese Large Language Models","abstract":"Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.","sentences":["Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence.","Yet, these advancements have raised growing concerns about privacy and security implications.","To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems.","This framework consists of progressively complex and in-depth privacy test tasks at each tier.","Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios.","This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches.","Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings.","It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models."],"url":"http://arxiv.org/abs/2403.18205v1","category":"cs.AI"}
{"created":"2024-03-27 02:24:38","title":"EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications","abstract":"Artificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different 'omics' studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preprocessing, training, evaluating, and visualizing machine learning (ML) models without manual intervention or coding expertise. By integrating traditional machine learning and deep neural network models with visualizations, our library assists in recognizing, classifying, clustering, and predicting a wide range of multi-modal, multi-sensor datasets, including images, languages, and one-dimensional numerical data, for drug discovery, pathogen classification, and medical diagnostics.","sentences":["Artificial intelligence (AI) techniques are widely applied in the life sciences.","However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages.","An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community.","With easy access to different sequencing technologies and increased interest in different 'omics' studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding.","The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills.","In this research, we propose a web-based end-to-end pipeline that is capable of preprocessing, training, evaluating, and visualizing machine learning (ML) models without manual intervention or coding expertise.","By integrating traditional machine learning and deep neural network models with visualizations, our library assists in recognizing, classifying, clustering, and predicting a wide range of multi-modal, multi-sensor datasets, including images, languages, and one-dimensional numerical data, for drug discovery, pathogen classification, and medical diagnostics."],"url":"http://arxiv.org/abs/2403.18203v1","category":"cs.AI"}
{"created":"2024-03-27 02:24:00","title":"Few-shot Online Anomaly Detection and Segmentation","abstract":"Detecting anomaly patterns from images is a crucial artificial intelligence technique in industrial applications. Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model's performance. Consequently, this paper focuses on addressing the challenging yet practical few-shot online anomaly detection and segmentation (FOADS) task. Under the FOADS framework, models are trained on a few-shot normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously.   To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow. In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a CNN pre-trained on ImageNet to obtain a robust representation. Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples. Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets.","sentences":["Detecting anomaly patterns from images is a crucial artificial intelligence technique in industrial applications.","Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model's performance.","Consequently, this paper focuses on addressing the challenging yet practical few-shot online anomaly detection and segmentation (FOADS) task.","Under the FOADS framework, models are trained on a few-shot normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously.   ","To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow.","In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a CNN pre-trained on ImageNet to obtain a robust representation.","Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples.","Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets."],"url":"http://arxiv.org/abs/2403.18201v1","category":"cs.CV"}
{"created":"2024-03-27 02:13:20","title":"Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities","abstract":"There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods.","sentences":["There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays.","Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups.","In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification.","Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness.","We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups.","Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings.","The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods."],"url":"http://arxiv.org/abs/2403.18196v1","category":"cs.LG"}
{"created":"2024-03-27 02:08:12","title":"SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network","abstract":"Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary. Finally, we utilize SCANet to correct the assembly results of MEPNet. Experimental results demonstrate that SCANet can identify and correct MEPNet's misassembled results, significantly improving the correctness of assembly. Our code and dataset are available at https://github.com/Yaser-wyx/SCANet.","sentences":["Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness.","Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images.","However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning.","Concurrently, we observe that integrating a self-correction module can partially alleviate such issues.","Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components.","To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures.","Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task.","SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary.","Finally, we utilize SCANet to correct the assembly results of MEPNet.","Experimental results demonstrate that SCANet can identify and correct MEPNet's misassembled results, significantly improving the correctness of assembly.","Our code and dataset are available at https://github.com/Yaser-wyx/SCANet."],"url":"http://arxiv.org/abs/2403.18195v1","category":"cs.RO"}
{"created":"2024-03-27 01:44:39","title":"Integrating urban digital twins with cloud-based geospatial dashboards for coastal resilience planning: A case study in Florida","abstract":"Coastal communities are confronted with a growing incidence of climate-induced flooding, necessitating adaptation measures for resilience. In this paper, we introduce a framework that integrates an urban digital twin with a geospatial dashboard to allow visualization of the vulnerabilities within critical infrastructure across a range of spatial and temporal scales. The synergy between these two technologies fosters heightened community awareness about increased flood risks to establish a unified understanding, the foundation for collective decision-making in adaptation plans. The paper also elucidates ethical considerations while developing the platform, including ensuring accessibility, promoting transparency and equity, and safeguarding individual privacy.","sentences":["Coastal communities are confronted with a growing incidence of climate-induced flooding, necessitating adaptation measures for resilience.","In this paper, we introduce a framework that integrates an urban digital twin with a geospatial dashboard to allow visualization of the vulnerabilities within critical infrastructure across a range of spatial and temporal scales.","The synergy between these two technologies fosters heightened community awareness about increased flood risks to establish a unified understanding, the foundation for collective decision-making in adaptation plans.","The paper also elucidates ethical considerations while developing the platform, including ensuring accessibility, promoting transparency and equity, and safeguarding individual privacy."],"url":"http://arxiv.org/abs/2403.18188v1","category":"cs.CY"}
{"created":"2024-03-27 01:21:48","title":"Can AI Models Appreciate Document Aesthetics? An Exploration of Legibility and Layout Quality in Relation to Prediction Confidence","abstract":"A well-designed document communicates not only through its words but also through its visual eloquence. Authors utilize aesthetic elements such as colors, fonts, graphics, and layouts to shape the perception of information. Thoughtful document design, informed by psychological insights, enhances both the visual appeal and the comprehension of the content. While state-of-the-art document AI models demonstrate the benefits of incorporating layout and image data, it remains unclear whether the nuances of document aesthetics are effectively captured. To bridge the gap between human cognition and AI interpretation of aesthetic elements, we formulated hypotheses concerning AI behavior in document understanding tasks, specifically anchored in document design principles. With a focus on legibility and layout quality, we tested four aspects of aesthetic effects: noise, font-size contrast, alignment, and complexity, on model confidence using correlational analysis. The results and observations highlight the value of model analysis rooted in document design theories. Our work serves as a trailhead for further studies and we advocate for continued research in this topic to deepen our understanding of how AI interprets document aesthetics.","sentences":["A well-designed document communicates not only through its words but also through its visual eloquence.","Authors utilize aesthetic elements such as colors, fonts, graphics, and layouts to shape the perception of information.","Thoughtful document design, informed by psychological insights, enhances both the visual appeal and the comprehension of the content.","While state-of-the-art document AI models demonstrate the benefits of incorporating layout and image data, it remains unclear whether the nuances of document aesthetics are effectively captured.","To bridge the gap between human cognition and AI interpretation of aesthetic elements, we formulated hypotheses concerning AI behavior in document understanding tasks, specifically anchored in document design principles.","With a focus on legibility and layout quality, we tested four aspects of aesthetic effects: noise, font-size contrast, alignment, and complexity, on model confidence using correlational analysis.","The results and observations highlight the value of model analysis rooted in document design theories.","Our work serves as a trailhead for further studies and we advocate for continued research in this topic to deepen our understanding of how AI interprets document aesthetics."],"url":"http://arxiv.org/abs/2403.18183v1","category":"cs.AI"}
{"created":"2024-03-27 00:23:03","title":"Mechanisms of non-factual hallucinations in language models","abstract":"State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics for the two mechanistic causes of hallucinations. We also highlight how attribution features from our causal analysis can effectively construct hallucination detectors. Our work proposes a mechanistic understanding of LM factual errors.","sentences":["State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge.","Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive.","Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries.","With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs.","These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness.","Additionally, we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics for the two mechanistic causes of hallucinations.","We also highlight how attribution features from our causal analysis can effectively construct hallucination detectors.","Our work proposes a mechanistic understanding of LM factual errors."],"url":"http://arxiv.org/abs/2403.18167v1","category":"cs.CL"}
{"created":"2024-03-26 23:51:44","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models","abstract":"Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that ov-freeze results in near float-point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks.","sentences":["Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively.","However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices.","In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications.","To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors.","Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process.","Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that ov-freeze results in near float-point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks."],"url":"http://arxiv.org/abs/2403.18159v1","category":"cs.LG"}
{"created":"2024-03-26 23:32:52","title":"Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency","abstract":"Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.","sentences":["Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them.","While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored.","To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents.","We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers.","We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers.","We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount.","Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention.","Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings."],"url":"http://arxiv.org/abs/2403.18152v1","category":"cs.CL"}
{"created":"2024-03-26 23:32:29","title":"Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study","abstract":"Cytology plays a crucial role in lung cancer diagnosis. Pulmonary cytology involves cell morphological characterization in the specimen and reporting the corresponding findings, which are extremely burdensome tasks. In this study, we propose a report-generation technique for lung cytology images. In total, 71 benign and 135 malignant pulmonary cytology specimens were collected. Patch images were extracted from the captured specimen images, and the findings were assigned to each image as a dataset for report generation. The proposed method consists of a vision model and a text decoder. In the former, a convolutional neural network (CNN) is used to classify a given image as benign or malignant, and the features related to the image are extracted from the intermediate layer. Independent text decoders for benign and malignant cells are prepared for text generation, and the text decoder switches according to the CNN classification results. The text decoder is configured using a Transformer that uses the features obtained from the CNN for report generation. Based on the evaluation results, the sensitivity and specificity were 100% and 96.4%, respectively, for automated benign and malignant case classification, and the saliency map indicated characteristic benign and malignant areas. The grammar and style of the generated texts were confirmed as correct and in better agreement with gold standard compared to existing LLM-based image-captioning methods and single-text-decoder ablation model. These results indicate that the proposed method is useful for pulmonary cytology classification and reporting.","sentences":["Cytology plays a crucial role in lung cancer diagnosis.","Pulmonary cytology involves cell morphological characterization in the specimen and reporting the corresponding findings, which are extremely burdensome tasks.","In this study, we propose a report-generation technique for lung cytology images.","In total, 71 benign and 135 malignant pulmonary cytology specimens were collected.","Patch images were extracted from the captured specimen images, and the findings were assigned to each image as a dataset for report generation.","The proposed method consists of a vision model and a text decoder.","In the former, a convolutional neural network (CNN) is used to classify a given image as benign or malignant, and the features related to the image are extracted from the intermediate layer.","Independent text decoders for benign and malignant cells are prepared for text generation, and the text decoder switches according to the CNN classification results.","The text decoder is configured using a Transformer that uses the features obtained from the CNN for report generation.","Based on the evaluation results, the sensitivity and specificity were 100% and 96.4%, respectively, for automated benign and malignant case classification, and the saliency map indicated characteristic benign and malignant areas.","The grammar and style of the generated texts were confirmed as correct and in better agreement with gold standard compared to existing LLM-based image-captioning methods and single-text-decoder ablation model.","These results indicate that the proposed method is useful for pulmonary cytology classification and reporting."],"url":"http://arxiv.org/abs/2403.18151v1","category":"eess.IV"}
{"created":"2024-03-26 23:14:34","title":"Large Language Models Produce Responses Perceived to be Empathic","abstract":"Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles\", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is important.","sentences":["Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy.","Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations.","Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be.","We found that LLM-generated responses were consistently rated as more empathic than human-written responses.","Linguistic analyses also show that these models write in distinct, predictable ``styles\", in terms of their use of punctuation, emojis, and certain words.","These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is important."],"url":"http://arxiv.org/abs/2403.18148v1","category":"cs.CL"}
{"created":"2024-03-26 23:10:41","title":"A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution","abstract":"One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution. One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location. In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders. We prove the optimality of SES and evaluate its efficiency via simulations. The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems.","sentences":["One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution.","One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location.","In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders.","We prove the optimality of SES and evaluate its efficiency via simulations.","The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems."],"url":"http://arxiv.org/abs/2403.18145v1","category":"cs.AI"}
{"created":"2024-03-26 22:54:12","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","abstract":"The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.","sentences":["The high computational cost associated with pretraining large language models limits their research.","Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data.","To explore these strategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams.","Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data.","However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language.","This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost."],"url":"http://arxiv.org/abs/2403.18140v1","category":"cs.CL"}
{"created":"2024-03-26 22:41:41","title":"Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs","abstract":"Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding GNNs against backdoor attacks.","sentences":["Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application.","The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking.","Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete.","To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks.","Additionally, we develop an adaptive attack to rigorously evaluate our approach.","We test our method on multiple benchmark datasets and examine its efficacy against various attack models.","Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding GNNs against backdoor attacks."],"url":"http://arxiv.org/abs/2403.18136v1","category":"cs.LG"}
{"created":"2024-03-26 22:28:43","title":"AE SemRL: Learning Semantic Association Rules with Autoencoders","abstract":"Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster execution time than state-of-the-art ARM approaches in many scenarios. We believe that this study advances a new way of extracting associations from representations and has the potential to inspire more research in this field.","sentences":["Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules.","Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task.","In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL).","Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules.","Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible.","Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster execution time than state-of-the-art ARM approaches in many scenarios.","We believe that this study advances a new way of extracting associations from representations and has the potential to inspire more research in this field."],"url":"http://arxiv.org/abs/2403.18133v1","category":"cs.LG"}
{"created":"2024-03-26 22:26:39","title":"Recommendation of data-free class-incremental learning algorithms by simulating future data","abstract":"Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting. This work contributes to facilitate the practical deployment of incremental learning.","sentences":["Class-incremental learning deals with sequential data streams composed of batches of classes.","Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored.","However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings.","To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream.","Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain.","We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting.","We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings.","Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting.","This work contributes to facilitate the practical deployment of incremental learning."],"url":"http://arxiv.org/abs/2403.18132v1","category":"cs.LG"}
{"created":"2024-03-26 22:01:13","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization","abstract":"Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.","sentences":["Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems.","However, they still make unjustified logical and computational errors in their reasoning steps and answers.","In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency.","This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.","We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K.","In our experiments it improves results consistently across all datasets and LLM model sizes.","The code can be found at https://github.com/jinpz/dtv."],"url":"http://arxiv.org/abs/2403.18120v1","category":"cs.AI"}
{"created":"2024-03-26 21:48:27","title":"EgoLifter: Open-world 3D Segmentation for Egocentric Perception","abstract":"In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.","sentences":["In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects.","The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion.","EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy.","To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction.","The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene.","We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input.","We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale."],"url":"http://arxiv.org/abs/2403.18118v1","category":"cs.CV"}
{"created":"2024-03-26 21:45:29","title":"QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1","abstract":"Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas. In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view. Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis.","sentences":["Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process.","The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage).","Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations.","Using social media images in crisis management has proven effective in various situations.","However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area.","Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively.","The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas.","In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view.","Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis."],"url":"http://arxiv.org/abs/2403.18116v1","category":"cs.CV"}
{"created":"2024-03-26 21:04:29","title":"Large Language Models for Education: A Survey and Outlook","abstract":"The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.","sentences":["The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education.","This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools.","We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education.","Furthermore, we outline future research opportunities, highlighting the potential promising directions.","Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment."],"url":"http://arxiv.org/abs/2403.18105v1","category":"cs.CL"}
{"created":"2024-03-26 21:00:06","title":"Towards Explainable Clustering: A Constrained Declarative based Approach","abstract":"The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clustering with Explainabilty-driven Cluster Selection that integrates structural or domain expert knowledge expressed by means of constraints. It is based on the notion of coverage and discrimination that are formalized at different levels (cluster / clustering), each allowing for exceptions through parameterized thresholds. Our method relies on four steps: generation of a set of partitions, computation of frequent patterns for each cluster, pruning clusters that violates some constraints, and selection of clusters and associated patterns to build an interpretable clustering. This last step is combinatorial and we have developed a Constraint-Programming (CP) model to solve it. The method can integrate prior knowledge in the form of user constraints, both before or in the CP model.","sentences":["The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert.","We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering.","We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination).","Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations.","In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clustering with Explainabilty-driven Cluster Selection that integrates structural or domain expert knowledge expressed by means of constraints.","It is based on the notion of coverage and discrimination that are formalized at different levels (cluster / clustering), each allowing for exceptions through parameterized thresholds.","Our method relies on four steps: generation of a set of partitions, computation of frequent patterns for each cluster, pruning clusters that violates some constraints, and selection of clusters and associated patterns to build an interpretable clustering.","This last step is combinatorial and we have developed a Constraint-Programming (CP) model to solve it.","The method can integrate prior knowledge in the form of user constraints, both before or in the CP model."],"url":"http://arxiv.org/abs/2403.18101v1","category":"cs.AI"}
{"created":"2024-03-26 20:59:48","title":"Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning","abstract":"This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficiency, and enhance security, thereby driving the development of intelligent systems. The paper also introduces the development of iot monitoring and control technology, the application of edge computing in iot monitoring and control, and the role of machine learning in data analysis and fault detection. Finally, the application and effect of intelligent Internet of Things monitoring and control system in industry, agriculture, medical and other fields are demonstrated through practical cases and experimental studies.","sentences":["This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning.","As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives.","However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations.","Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis.","The combination of iot and edge computing can reduce latency, improve efficiency, and enhance security, thereby driving the development of intelligent systems.","The paper also introduces the development of iot monitoring and control technology, the application of edge computing in iot monitoring and control, and the role of machine learning in data analysis and fault detection.","Finally, the application and effect of intelligent Internet of Things monitoring and control system in industry, agriculture, medical and other fields are demonstrated through practical cases and experimental studies."],"url":"http://arxiv.org/abs/2403.18100v1","category":"cs.AI"}
{"created":"2024-03-26 20:47:32","title":"GPTs and Language Barrier: A Cross-Lingual Legal QA Examination","abstract":"In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.","sentences":["In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset.","In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task.","By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain."],"url":"http://arxiv.org/abs/2403.18098v1","category":"cs.CL"}
{"created":"2024-03-26 20:30:55","title":"A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury","abstract":"Hand function is critical for our interactions and quality of life. Spinal cord injuries (SCI) can impair hand function, reducing independence. A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function. Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data. This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic clustering. Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance. A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful clusters in video content. This methodology provides a flexible and effective strategy to analyze hand function in the wild. It offers researchers and clinicians an efficient tool for evaluating hand function, aiding sensitive assessments and tailored intervention plans.","sentences":["Hand function is critical for our interactions and quality of life.","Spinal cord injuries (SCI) can impair hand function, reducing independence.","A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function.","Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data.","This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic clustering.","Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance.","A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy.","Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy.","Qualitative assessment revealed meaningful clusters in video content.","This methodology provides a flexible and effective strategy to analyze hand function in the wild.","It offers researchers and clinicians an efficient tool for evaluating hand function, aiding sensitive assessments and tailored intervention plans."],"url":"http://arxiv.org/abs/2403.18094v1","category":"cs.CV"}
{"created":"2024-03-26 20:25:53","title":"Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models","abstract":"Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.","sentences":["Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent.","Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems.","Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles.","This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking.","Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy.","However, error analysis reveals several existing issues in the retrieval system that still need resolution."],"url":"http://arxiv.org/abs/2403.18093v1","category":"cs.CL"}
{"created":"2024-03-26 20:18:11","title":"Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new advance and generalization of the RIS technique. BD-RIS breaks through the isolation between RIS elements by creatively introducing inter-element connections, thereby enabling smarter wave manipulation and enlarging coverage. However, exploring proper channel estimation schemes suitable for BD-RIS aided communication systems still remains an open problem. In this paper, we study channel estimation and beamforming design for BD-RIS aided multi-antenna systems. We first describe the channel estimation strategy based on the least square (LS) method, derive the mean square error (MSE) of the LS estimation, and formulate the joint pilot sequence and BD-RIS design problem with unique constraints induced by BD-RIS architectures. Specifically, we propose an efficient pilot sequence and BD-RIS design which theoretically guarantees to achieve the minimum MSE. With the estimated channel, we then consider two BD-RIS scenarios and propose beamforming design algorithms. Finally, we provide simulation results to verify the effectiveness of the proposed channel estimation scheme and beamforming design algorithms. We also show that more interelement connections in BD-RIS improves the performance while increasing the training overhead for channel estimation.","sentences":["Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new advance and generalization of the RIS technique.","BD-RIS breaks through the isolation between RIS elements by creatively introducing inter-element connections, thereby enabling smarter wave manipulation and enlarging coverage.","However, exploring proper channel estimation schemes suitable for BD-RIS aided communication systems still remains an open problem.","In this paper, we study channel estimation and beamforming design for BD-RIS aided multi-antenna systems.","We first describe the channel estimation strategy based on the least square (LS) method, derive the mean square error (MSE) of the LS estimation, and formulate the joint pilot sequence and BD-RIS design problem with unique constraints induced by BD-RIS architectures.","Specifically, we propose an efficient pilot sequence and BD-RIS design which theoretically guarantees to achieve the minimum MSE.","With the estimated channel, we then consider two BD-RIS scenarios and propose beamforming design algorithms.","Finally, we provide simulation results to verify the effectiveness of the proposed channel estimation scheme and beamforming design algorithms.","We also show that more interelement connections in BD-RIS improves the performance while increasing the training overhead for channel estimation."],"url":"http://arxiv.org/abs/2403.18087v1","category":"eess.SP"}
{"created":"2024-03-26 20:10:59","title":"Collective modes and Raman response in Ta$_2$NiSe$_5$","abstract":"We explore the collective response in an excitonic insulator phase in Ta$_2$NiSe$_5$ using a semirealistic model including relevant lattice and electronic instabilities. We calculate order-parameter susceptibility and Raman response within a time-dependent Hartree-Fock approach. Contrary to the standard expectations, the amplitude mode frequency does not coincide with the single-particle gap but has a higher frequency. We find a phase mode that is massive because the excitonic condensation breaks a discrete symmetry only and that becomes heavier as the electron-lattice coupling is increased. These features are expected to apply to generic realistic excitonic insulators. We discuss scenarios under which the phase mode does not appear as a sharp in-gap resonance.","sentences":["We explore the collective response in an excitonic insulator phase in Ta$_2$NiSe$_5$ using a semirealistic model including relevant lattice and electronic instabilities.","We calculate order-parameter susceptibility and Raman response within a time-dependent Hartree-Fock approach.","Contrary to the standard expectations, the amplitude mode frequency does not coincide with the single-particle gap but has a higher frequency.","We find a phase mode that is massive because the excitonic condensation breaks a discrete symmetry only and that becomes heavier as the electron-lattice coupling is increased.","These features are expected to apply to generic realistic excitonic insulators.","We discuss scenarios under which the phase mode does not appear as a sharp in-gap resonance."],"url":"http://arxiv.org/abs/2403.18083v1","category":"cond-mat.str-el"}
{"created":"2024-03-26 19:58:39","title":"Paths to Equilibrium in Normal-Form Games","abstract":"In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for mixed extensions of finite normal-form games.%","sentences":["In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles.","This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration.","Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms.","A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy?","The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms.","We answer this question in the affirmative for mixed extensions of finite normal-form games.%"],"url":"http://arxiv.org/abs/2403.18079v1","category":"cs.GT"}
{"created":"2024-03-26 19:36:50","title":"State of the art applications of deep learning within tracking and detecting marine debris: A survey","abstract":"Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations and open challenges.","sentences":["Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years.","We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris.","From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning.","Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database.","We conclude this survey with over 40 future research recommendations and open challenges."],"url":"http://arxiv.org/abs/2403.18067v1","category":"cs.CV"}
{"created":"2024-03-26 19:29:21","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer","abstract":"Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance over the state-of-the-art methods while reducing the number of parameters. Through extensive experiments, we show that SCT-C-small gives state-of-the-art performance on the ImageNet dataset and reaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge reaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project page is available on this webpage.\\url{https://github.com/badripatro/sct}","sentences":["Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin.","These have worked to improve the attention mechanism and make it more efficient.","Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer.","We advocate combining three diverse views of data - local, global, and long-range dependence.","We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform.","We use a convolutional operator in the initial layers to capture local information.","Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance over the state-of-the-art methods while reducing the number of parameters.","Through extensive experiments, we show that SCT-C-small gives state-of-the-art performance on the ImageNet dataset and reaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge reaches 86.4\\%.","We evaluate SCT on transfer learning on datasets such as CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car.","We also evaluate SCT on downstream tasks i.e. instance segmentation on the MSCOCO dataset.","The project page is available on this webpage.\\url{https://github.com/badripatro/sct}"],"url":"http://arxiv.org/abs/2403.18063v1","category":"cs.CV"}
{"created":"2024-03-26 19:26:53","title":"ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition","abstract":"Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/.","sentences":["Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments.","Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships.","Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping.","We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task.","Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate.","Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/."],"url":"http://arxiv.org/abs/2403.18062v1","category":"cs.RO"}
{"created":"2024-03-26 19:24:18","title":"COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning","abstract":"Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset. Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets. We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA","sentences":["Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language.","These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency.","However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning.","The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks.","Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users.","To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset.","Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions.","To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&A communities, Wikis, examinations, and existing NLP datasets.","This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset.","Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses.","The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets.","We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks.","Data are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA"],"url":"http://arxiv.org/abs/2403.18058v1","category":"cs.CL"}
{"created":"2024-03-26 19:21:50","title":"Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems","abstract":"Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agents. Next, we use Unreal Engine to design a large-scale heterogeneous cooperation benchmark named Large-Scale Multiagent Operation (LSMO), which is a complex two-team competition scenario that requires collaboration from both ground and airborne agents. We use experiments to show that PHLRL outperforms state-of-the-art methods, including QTRAN and QPLEX in LSMO.","sentences":["Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost.","In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages.","Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types.","We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems.","PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization.","Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agents.","Next, we use Unreal Engine to design a large-scale heterogeneous cooperation benchmark named Large-Scale Multiagent Operation (LSMO), which is a complex two-team competition scenario that requires collaboration from both ground and airborne agents.","We use experiments to show that PHLRL outperforms state-of-the-art methods, including QTRAN and QPLEX in LSMO."],"url":"http://arxiv.org/abs/2403.18057v1","category":"cs.AI"}
{"created":"2024-03-26 19:19:16","title":"Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph","abstract":"Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates.","sentences":["Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges.","However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors.","The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge.","This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems.","HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators.","HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks.","ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer.","To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically.","The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface.","In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards.","We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates."],"url":"http://arxiv.org/abs/2403.18056v1","category":"cs.AI"}
{"created":"2024-03-26 19:08:20","title":"Supervisory Prompt Training","abstract":"The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \\textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT advances LLMs by refining prompts to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model fine-tuning.","sentences":["The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable.","We propose a novel approach, Supervisory Prompt Training (SPT).","SPT automates the generation of highly effective prompts using a dual LLM system.","In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts.","In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time.","We also introduce the concept of \\textit{impact scores} to measure the sentence-level effectiveness of the prompts.","Our method was tested on four benchmarks, testing the level of hallucinations in LLMs.","Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase).","SPT advances LLMs by refining prompts to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model fine-tuning."],"url":"http://arxiv.org/abs/2403.18051v1","category":"cs.CL"}
{"created":"2024-03-26 18:57:41","title":"MUSE view of PDS 456: kpc-scale wind, extended ionized gas and close environment","abstract":"PDS 456 is the most luminous RQQ at z<0.3 and can be regarded as a local counterpart of the powerful QSOs shining at Cosmic Noon. It hosts a strong nuclear X-ray ultra-fast outflow, and a massive and clumpy CO(3-2) molecular outflow extending up to 5 kpc from the nucleus. We analyzed the first MUSE WFM and AO-NFM optical integral field spectroscopic observations of PDS456. The AO-NFM observations provide an unprecedented spatial resolution, reaching up to 280 pc. Our findings reveal a complex circumgalactic medium around PDS 456, extending up to a maximum projected size of ~46 kpc. This includes a reservoir of gas with a mass of ~1e7-1e8 Modot, along with eight companion galaxies, and a multi-phase outflow. WFM and NFM MUSE data reveal an outflow on a large scale (~12 kpc from the quasar) in [OIII], and on smaller scales (within 3 kpc) with higher resolution (about 280 pc) in Halpha, respectively. The [OIII] outflow mass rate is 2.3 +/- 0.2 Modot/yr which is significantly lower than those typically found in other luminous quasars. Remarkably, the Ha outflow shows a similar scale, morphology, and kinematics to the CO(3-2) molecular outflow, with the latter dominating in terms of kinetic energy and mass outflow rate by two and one orders of magnitude, respectively. Our results therefore indicate that mergers, powerful AGN activity, and feedback through AGN-driven winds will collectively contribute to shaping the host galaxy evolution of PDS 456, and likely, that of similar objects at the brightest end of the AGN luminosity function across all redshifts. Moreover, the finding that the momentum boost of the total outflow deviates from the expected energy-conserving expansion for large-scale outflows highlights the need of novel AGN-driven outflow models to comprehensively interpret these phenomena.","sentences":["PDS 456 is the most luminous RQQ at z<0.3 and can be regarded as a local counterpart of the powerful QSOs shining at Cosmic Noon.","It hosts a strong nuclear X-ray ultra-fast outflow, and a massive and clumpy CO(3-2) molecular outflow extending up to 5 kpc from the nucleus.","We analyzed the first MUSE WFM and AO-NFM optical integral field spectroscopic observations of PDS456.","The AO-NFM observations provide an unprecedented spatial resolution, reaching up to 280 pc.","Our findings reveal a complex circumgalactic medium around PDS 456, extending up to a maximum projected size of ~46 kpc.","This includes a reservoir of gas with a mass of ~1e7-1e8 Modot, along with eight companion galaxies, and a multi-phase outflow.","WFM and NFM MUSE data reveal an outflow on a large scale (~12 kpc from the quasar) in [OIII], and on smaller scales (within 3 kpc) with higher resolution (about 280 pc) in Halpha, respectively.","The [OIII] outflow mass rate is 2.3 +/- 0.2 Modot/yr which is significantly lower than those typically found in other luminous quasars.","Remarkably, the Ha outflow shows a similar scale, morphology, and kinematics to the CO(3-2) molecular outflow, with the latter dominating in terms of kinetic energy and mass outflow rate by two and one orders of magnitude, respectively.","Our results therefore indicate that mergers, powerful AGN activity, and feedback through AGN-driven winds will collectively contribute to shaping the host galaxy evolution of PDS 456, and likely, that of similar objects at the brightest end of the AGN luminosity function across all redshifts.","Moreover, the finding that the momentum boost of the total outflow deviates from the expected energy-conserving expansion for large-scale outflows highlights the need of novel AGN-driven outflow models to comprehensively interpret these phenomena."],"url":"http://arxiv.org/abs/2403.18043v1","category":"astro-ph.GA"}
{"created":"2024-03-26 18:49:56","title":"TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images","abstract":"Line detection is a classic and essential problem in image processing, computer vision and machine intelligence. Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery). Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking. In this paper, we proposed and implemented a topological graph-guided algorithm, named TGGLinesPlus, for line detection. Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm. We also benchmarked our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus. We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters.","sentences":["Line detection is a classic and essential problem in image processing, computer vision and machine intelligence.","Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery).","Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking.","In this paper, we proposed and implemented a topological graph-guided algorithm, named TGGLinesPlus, for line detection.","Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm.","We also benchmarked our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus.","We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters."],"url":"http://arxiv.org/abs/2403.18038v1","category":"cs.CV"}
{"created":"2024-03-26 18:39:38","title":"SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation","abstract":"The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead.","sentences":["The increase in non-biodegradable waste is a worldwide concern.","Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation.","In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging.","Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios.","First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images.","This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse.","Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives.","Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead."],"url":"http://arxiv.org/abs/2403.18033v1","category":"cs.CV"}
{"created":"2024-03-26 18:38:00","title":"EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions","abstract":"Tensor Networks are graph representations of summation expressions in which vertices represent tensors and edges represent tensor indices or vector spaces. In this work, we present EinExprs.jl, a Julia package for contraction path optimization that offers state-of-art optimizers. We propose a representation of the contraction path of a Tensor Network based on symbolic expressions. Using this package the user may choose among a collection of different methods such as Greedy algorithms, or an approach based on the hypergraph partitioning problem. We benchmark this library with examples obtained from the simulation of Random Quantum Circuits (RQC), a well known example where Tensor Networks provide state-of-the-art methods.","sentences":["Tensor Networks are graph representations of summation expressions in which vertices represent tensors and edges represent tensor indices or vector spaces.","In this work, we present EinExprs.jl, a Julia package for contraction path optimization that offers state-of-art optimizers.","We propose a representation of the contraction path of a Tensor Network based on symbolic expressions.","Using this package the user may choose among a collection of different methods such as Greedy algorithms, or an approach based on the hypergraph partitioning problem.","We benchmark this library with examples obtained from the simulation of Random Quantum Circuits (RQC), a well known example where Tensor Networks provide state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.18030v1","category":"quant-ph"}
{"created":"2024-03-26 18:29:39","title":"Predicting species occurrence patterns from partial observations","abstract":"To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial information both within a taxon (birds) and across taxa (birds and butterflies). Our approach opens new perspectives to leveraging insights from species with abundant data to other species with scarce data, by modelling the ecosystems in which they co-occur.","sentences":["To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing.","However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups.","We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species.","To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data.","To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found.","We find that R-Tran outperforms other methods in predicting species encounter rates with partial information both within a taxon (birds) and across taxa (birds and butterflies).","Our approach opens new perspectives to leveraging insights from species with abundant data to other species with scarce data, by modelling the ecosystems in which they co-occur."],"url":"http://arxiv.org/abs/2403.18028v1","category":"cs.LG"}
{"created":"2024-03-26 18:23:16","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER","abstract":"Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.","sentences":["Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data.","Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task.","Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains.","For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern.","To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning.","MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words.","Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms.","We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences.","Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking."],"url":"http://arxiv.org/abs/2403.18025v1","category":"cs.CL"}
{"created":"2024-03-26 14:47:05","title":"Semi-Supervised Image Captioning Considering Wasserstein Graph Matching","abstract":"Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the \"Semi-Supervised Image Captioning\". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately comparable information among heterogeneous modalities. In this paper, SSIC-WGM adopts the successful scene graphs as intermediate information, and constrains the generated sentences from two aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of the raw image and generated sentence respectively, then employs the wasserstein distance to better measure the similarity between region embeddings of different graphs. 2) intra-modal consistency. SSIC-WGM takes the data augmentation techniques for the raw images, then constrains the consistency among augmented images and generated sentences. Consequently, SSIC-WGM combines the cross-modal pseudo supervision and structure invariant measure for efficiently using the undescribed images, and learns more reasonable mapping function.","sentences":["Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features.","Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set.","However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications.","Thereby, a dilemma is the \"Semi-Supervised Image Captioning\".","To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences.","Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately comparable information among heterogeneous modalities.","In this paper, SSIC-WGM adopts the successful scene graphs as intermediate information, and constrains the generated sentences from two aspects: 1) inter-modal consistency.","SSIC-WGM constructs the scene graphs of the raw image and generated sentence respectively, then employs the wasserstein distance to better measure the similarity between region embeddings of different graphs.","2) intra-modal consistency.","SSIC-WGM takes the data augmentation techniques for the raw images, then constrains the consistency among augmented images and generated sentences.","Consequently, SSIC-WGM combines the cross-modal pseudo supervision and structure invariant measure for efficiently using the undescribed images, and learns more reasonable mapping function."],"url":"http://arxiv.org/abs/2403.17995v1","category":"cs.CV"}
{"created":"2024-03-27 17:59:18","title":"A dynamical interpretation of the connection map of an attractor-repeller decomposition","abstract":"In Conley index theory one may study an invariant set $S$ by decomposing it into an attractor $A$, a repeller $R$, and the orbits connecting the two. The Conley indices of $S$, $A$ and $R$ fit into an exact sequence where a certain connection homomorphism $\\Gamma$ plays an important role. In this paper we provide a dynamical interpretation of this map. Roughly, $R$ \"emits\" an element of its Conley index as a \"wavefront\", part of which intersects the connecting orbits in $S$. This subset of the wavefront evolves towards $A$ and is then \"received\" by it to produce an element in its Conley index.","sentences":["In Conley index theory one may study an invariant set $S$ by decomposing it into an attractor $A$, a repeller $R$, and the orbits connecting the two.","The Conley indices of $S$, $A$ and $R$ fit into an exact sequence where a certain connection homomorphism $\\Gamma$ plays an important role.","In this paper we provide a dynamical interpretation of this map.","Roughly, $R$ \"emits\" an element of its Conley index as a \"wavefront\", part of which intersects the connecting orbits in $S$. This subset of the wavefront evolves towards $A$ and is then \"received\" by it to produce an element in its Conley index."],"url":"http://arxiv.org/abs/2403.18815v1","category":"math.DS"}
{"created":"2024-03-27 17:57:16","title":"On the Communication Complexity of Approximate Pattern Matching","abstract":"The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$.   The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and Porat [SODA 2019] proved that $\\Omega(n/m \\cdot k \\log(m/k))$ bits are necessary and $O(n/m \\cdot k\\log (m|\\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact.   Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\\Omega(n/m \\cdot k\\log(m/k))$ bits and an upper bound of $O(n/m \\cdot k^3\\log m)$ bits stemming from previous research. In this work, we prove an upper bound of $O(n/m \\cdot k \\log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors. We also show that $O(n/m \\cdot k \\log m \\log (m|\\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact.   We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits.","sentences":["The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$.   The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and","Porat [SODA 2019] proved that $\\Omega(n/m \\cdot k \\log(m/k))$ bits are necessary and $O(n/m \\cdot k\\log (m|\\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact.   ","Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\\Omega(n/m \\cdot k\\log(m/k))$ bits and an upper bound of $O(n/m \\cdot k^3\\log m)$ bits stemming from previous research.","In this work, we prove an upper bound of $O(n/m \\cdot k \\log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors.","We also show that $O(n/m \\cdot k \\log m \\log (m|\\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact.   ","We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits."],"url":"http://arxiv.org/abs/2403.18812v1","category":"cs.DS"}
{"created":"2024-03-27 17:51:00","title":"Using an invariant knot of a flow to find additional invariant structure","abstract":"Consider a continuous flow in $\\mathbb{R}^3$ or any orientable $3$-manifold. Let $(Q_1, Q_0)$ be an index pair in the sense of Conley and consider the region $N := \\overline{Q_1 - Q_0}$. (An example of this is a compact $3$-manifold $N$ such that trajectories of the flow cross $\\partial N$ inwards or outwards transversally, or bounce off it from the outside). Suppose we know there is an invariant knot or link $K$ in the interior of $N$. We prove the following: if $K$ is contractible and nontrivial (in the sense of knot theory) in $N$, then every neighbourhood $U$ of $K$ contains a point $p \\in N - K$ such that the whole trajectory of $p$ is contained in $N$. In other words, the presence of $K$ forces the existence of additional invariant structure in $N$ (besides $K$), and the latter can actually be found arbitrarily close to $K$.   To prove this result we develop a ``coloured'' handle theory which may be of independent interest to study flows in $3$-manifolds.","sentences":["Consider a continuous flow in $\\mathbb{R}^3$ or any orientable $3$-manifold.","Let $(Q_1, Q_0)$ be an index pair in the sense of Conley and consider the region $N := \\overline{Q_1 - Q_0}$. (An example of this is a compact $3$-manifold $N$ such that trajectories of the flow cross $\\partial N$ inwards or outwards transversally, or bounce off it from the outside).","Suppose we know there is an invariant knot or link $K$ in the interior of $N$. We prove the following: if $K$ is contractible and nontrivial (in the sense of knot theory) in $N$, then every neighbourhood $U$ of $K$ contains a point $p \\in N - K$ such that the whole trajectory of $p$ is contained in $N$. In other words, the presence of $K$ forces the existence of additional invariant structure in $N$ (besides $K$), and the latter can actually be found arbitrarily close to $K$.   To prove this result we develop a ``coloured'' handle theory which may be of independent interest to study flows in $3$-manifolds."],"url":"http://arxiv.org/abs/2403.18805v1","category":"math.DS"}
{"created":"2024-03-27 17:50:00","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation","abstract":"The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity. The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.","sentences":["The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications.","Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups.","However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM).","This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve.","We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones.","In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs.","Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity.","The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity."],"url":"http://arxiv.org/abs/2403.18804v1","category":"cs.CL"}
{"created":"2024-03-27 17:48:54","title":"Nonstandard Lagrangians and branched Hamiltonians: A brief review","abstract":"Time and again, non-conventional forms of Lagrangians have found attention in the literature. For one thing, such Lagrangians have deep connections with several aspects of nonlinear dynamics including specifically the types of the Li\\'{e}nard class; for another, very often the problem of their quantization opens up multiple branches of the corresponding Hamiltonians, ending up with the presence of singularities in the associated eigenfunctions. In this article, we furnish a brief review of the classical theory of such Lagrangians and the associated branched Hamiltonians, starting with the example of Li\\'{e}nard-type systems. We then take up other cases where the Lagrangians depend upon the velocity with powers greater than two while still having a tractable mathematical structure, while also describing the associated branched Hamiltonians for such systems. For various examples, we emphasize upon the emergence of the notion of momentum-dependent mass in the theory of branched Hamiltonians.","sentences":["Time and again, non-conventional forms of Lagrangians have found attention in the literature.","For one thing, such Lagrangians have deep connections with several aspects of nonlinear dynamics including specifically the types of the Li\\'{e}nard class; for another, very often the problem of their quantization opens up multiple branches of the corresponding Hamiltonians, ending up with the presence of singularities in the associated eigenfunctions.","In this article, we furnish a brief review of the classical theory of such Lagrangians and the associated branched Hamiltonians, starting with the example of Li\\'{e}nard-type systems.","We then take up other cases where the Lagrangians depend upon the velocity with powers greater than two while still having a tractable mathematical structure, while also describing the associated branched Hamiltonians for such systems.","For various examples, we emphasize upon the emergence of the notion of momentum-dependent mass in the theory of branched Hamiltonians."],"url":"http://arxiv.org/abs/2403.18801v1","category":"math-ph"}
{"created":"2024-03-27 17:33:55","title":"Peregrine: ML-based Malicious Traffic Detection for Terabit Networks","abstract":"Malicious traffic detectors leveraging machine learning (ML), namely those incorporating deep learning techniques, exhibit impressive detection capabilities across multiple attacks. However, their effectiveness becomes compromised when deployed in networks handling Terabit-speed traffic. In practice, these systems require substantial traffic sampling to reconcile the high data plane packet rates with the comparatively slower processing speeds of ML detection. As sampling significantly reduces traffic observability, it fundamentally undermines their detection capability.   We present Peregrine, an ML-based malicious traffic detector for Terabit networks. The key idea is to run the detection process partially in the network data plane. Specifically, we offload the detector's ML feature computation to a commodity switch. The Peregrine switch processes a diversity of features per-packet, at Tbps line rates - three orders of magnitude higher than the fastest detector - to feed the ML-based component in the control plane. Our offloading approach presents a distinct advantage. While, in practice, current systems sample raw traffic, in Peregrine sampling occurs after feature computation. This essential trait enables computing features over all traffic, significantly enhancing detection performance. The Peregrine detector is not only effective for Terabit networks, but it is also energy- and cost-efficient. Further, by shifting a compute-heavy component to the switch, it saves precious CPU cycles and improves detection throughput.","sentences":["Malicious traffic detectors leveraging machine learning (ML), namely those incorporating deep learning techniques, exhibit impressive detection capabilities across multiple attacks.","However, their effectiveness becomes compromised when deployed in networks handling Terabit-speed traffic.","In practice, these systems require substantial traffic sampling to reconcile the high data plane packet rates with the comparatively slower processing speeds of ML detection.","As sampling significantly reduces traffic observability, it fundamentally undermines their detection capability.   ","We present Peregrine, an ML-based malicious traffic detector for Terabit networks.","The key idea is to run the detection process partially in the network data plane.","Specifically, we offload the detector's ML feature computation to a commodity switch.","The Peregrine switch processes a diversity of features per-packet, at Tbps line rates - three orders of magnitude higher than the fastest detector - to feed the ML-based component in the control plane.","Our offloading approach presents a distinct advantage.","While, in practice, current systems sample raw traffic, in Peregrine sampling occurs after feature computation.","This essential trait enables computing features over all traffic, significantly enhancing detection performance.","The Peregrine detector is not only effective for Terabit networks, but it is also energy- and cost-efficient.","Further, by shifting a compute-heavy component to the switch, it saves precious CPU cycles and improves detection throughput."],"url":"http://arxiv.org/abs/2403.18788v1","category":"cs.NI"}
{"created":"2024-03-27 17:31:31","title":"Beyond boundaries: Gary Lorden's groundbreaking contributions to sequential analysis","abstract":"Gary Lorden provided a number of fundamental and novel insights to sequential hypothesis testing and changepoint detection. In this article we provide an overview of Lorden's contributions in the context of existing results in those areas, and some extensions made possible by Lorden's work, mentioning also areas of application including threat detection in physical-computer systems, near-Earth space informatics, epidemiology, clinical trials, and finance.","sentences":["Gary Lorden provided a number of fundamental and novel insights to sequential hypothesis testing and changepoint detection.","In this article we provide an overview of Lorden's contributions in the context of existing results in those areas, and some extensions made possible by Lorden's work, mentioning also areas of application including threat detection in physical-computer systems, near-Earth space informatics, epidemiology, clinical trials, and finance."],"url":"http://arxiv.org/abs/2403.18782v1","category":"math.ST"}
{"created":"2024-03-27 17:28:37","title":"Universal bounds on the entropy of toroidal attractors","abstract":"A toroidal set is a compactum $K \\subseteq \\mathbb{R}^3$ which has a neighbourhood basis of solid tori. We study the topological entropy of toroidal attractors $K$, bounding it from below in terms of purely topological properties of $K$. In particular, we show that for a toroidal set $K$, either any smooth attracting dynamics on $K$ has an entropy at least $\\log 2$, or (up to continuation) $K$ admits smooth attracting dynamics which are stationary (hence with a zero entropy).","sentences":["A toroidal set is a compactum $K \\subseteq \\mathbb{R}^3$ which has a neighbourhood basis of solid tori.","We study the topological entropy of toroidal attractors $K$, bounding it from below in terms of purely topological properties of $K$. In particular, we show that for a toroidal set $K$, either any smooth attracting dynamics on $K$ has an entropy at least $\\log 2$, or (up to continuation) $K$ admits smooth attracting dynamics which are stationary (hence with a zero entropy)."],"url":"http://arxiv.org/abs/2403.18780v1","category":"math.DS"}
{"created":"2024-03-27 17:28:05","title":"IR Spectroscopy of Carboxylate-Passivated Semiconducting Nanocrystals: Simulation and Experiment","abstract":"Surfaces of colloidal nanocrystals are frequently passivated with carboxylate ligands which exert significant effects on their optoelectronic properties and chemical stability. Experimentally, binding geometries of such ligands are typically investigated using vibrational spectroscopy, but the interpretation of the IR signal is usually not trivial. Here, using machine-learning (ML) algorithms trained on DFT data, we simulate an IR spectrum of a lead-rich PbS nanocrystal passivated with butyrate ligands. We obtain a good agreement with the experimental signal and demonstrate that the observed line shape stems from a very wide range of `tilted-bridge'-type geometries and does not indicate a coexistence of `bridging' and `chelating' binding modes as has been previously assumed. This work illustrates limitations of empirical spectrum assignment and demonstrates the effectiveness of ML-driven molecular dynamics simulations in reproducing IR spectra of nanoscopic systems.","sentences":["Surfaces of colloidal nanocrystals are frequently passivated with carboxylate ligands which exert significant effects on their optoelectronic properties and chemical stability.","Experimentally, binding geometries of such ligands are typically investigated using vibrational spectroscopy, but the interpretation of the IR signal is usually not trivial.","Here, using machine-learning (ML) algorithms trained on DFT data, we simulate an IR spectrum of a lead-rich PbS nanocrystal passivated with butyrate ligands.","We obtain a good agreement with the experimental signal and demonstrate that the observed line shape stems from a very wide range of `tilted-bridge'-type geometries and does not indicate a coexistence of `bridging' and `chelating' binding modes as has been previously assumed.","This work illustrates limitations of empirical spectrum assignment and demonstrates the effectiveness of ML-driven molecular dynamics simulations in reproducing IR spectra of nanoscopic systems."],"url":"http://arxiv.org/abs/2403.18779v1","category":"physics.chem-ph"}
{"created":"2024-03-27 17:26:42","title":"3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation","abstract":"Much worldly semantic knowledge can be encoded in large language models (LLMs). Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language. However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning. The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research. Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics. In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*. We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts. This paper establishes the foundation for LLM-powered path planning for robotic systems.","sentences":["Much worldly semantic knowledge can be encoded in large language models (LLMs).","Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language.","However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment.","This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning.","The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research.","Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics.","In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*.","We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts.","This paper establishes the foundation for LLM-powered path planning for robotic systems."],"url":"http://arxiv.org/abs/2403.18778v1","category":"cs.RO"}
{"created":"2024-03-27 17:23:52","title":"New Graph and Hypergraph Container Lemmas with Applications in Property Testing","abstract":"The graph and hypergraph container methods are powerful tools with a wide range of applications across combinatorics. Recently, Blais and Seth (FOCS 2023) showed that the graph container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental graph properties: having a large independent set and $k$-colorability. In this work, we show that the connection between the container method and property testing extends further along two different directions.   First, we show that the container method can be used to analyze the canonical tester for many other properties of graphs and hypergraphs. We introduce a new hypergraph container lemma and use it to give an upper bound of $\\widetilde{O}(kq^3/\\epsilon)$ on the sample complexity of $\\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet. This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous graph partition property.   Second, we show that the container method can also be used to study the query complexity of (non-canonical) graph property testers. This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets. We use this container lemma to give a new upper bound of $\\widetilde{O}(\\rho^5/\\epsilon^{7/2})$ on the query complexity of $\\epsilon$-testing the $\\rho$-independent set property. This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous graph partition property.","sentences":["The graph and hypergraph container methods are powerful tools with a wide range of applications across combinatorics.","Recently, Blais and Seth (FOCS 2023) showed that the graph container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental graph properties: having a large independent set and $k$-colorability.","In this work, we show that the connection between the container method and property testing extends further along two different directions.   ","First, we show that the container method can be used to analyze the canonical tester for many other properties of graphs and hypergraphs.","We introduce a new hypergraph container lemma and use it to give an upper bound of $\\widetilde{O}(kq^3/\\epsilon)$ on the sample complexity of $\\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet.","This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous graph partition property.   ","Second, we show that the container method can also be used to study the query complexity of (non-canonical) graph property testers.","This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets.","We use this container lemma to give a new upper bound of $\\widetilde{O}(\\rho^5/\\epsilon^{7/2})$ on the query complexity of $\\epsilon$-testing the $\\rho$-independent set property.","This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous graph partition property."],"url":"http://arxiv.org/abs/2403.18777v1","category":"cs.DS"}
{"created":"2024-03-27 17:18:17","title":"Rogue curves in the Davey-Stewartson I equation","abstract":"We report new rogue wave patterns whose wave crests form closed or open curves in the spatial plane, which we call rogue curves, in the Davey-Stewartson I equation. These rogue curves come in various striking shapes, such as rings, double rings, and many others. They emerge from a uniform background (possibly with a few lumps on it), reach high amplitude in such striking shapes, and then disappear into the same background again. We reveal that these rogue curves would arise when an internal parameter in bilinear expressions of the rogue waves is real and large. Analytically, we show that these rogue curves are predicted by root curves of certain types of double-real-variable polynomials. We compare analytical predictions of rogue curves to true solutions and demonstrate good agreement between them.","sentences":["We report new rogue wave patterns whose wave crests form closed or open curves in the spatial plane, which we call rogue curves, in the Davey-Stewartson","I equation.","These rogue curves come in various striking shapes, such as rings, double rings, and many others.","They emerge from a uniform background (possibly with a few lumps on it), reach high amplitude in such striking shapes, and then disappear into the same background again.","We reveal that these rogue curves would arise when an internal parameter in bilinear expressions of the rogue waves is real and large.","Analytically, we show that these rogue curves are predicted by root curves of certain types of double-real-variable polynomials.","We compare analytical predictions of rogue curves to true solutions and demonstrate good agreement between them."],"url":"http://arxiv.org/abs/2403.18770v1","category":"nlin.SI"}
{"created":"2024-03-27 17:13:38","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","abstract":"Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging another line of research -- reflex prediction -- we propose a system in which candidate protoforms from a reconstruction model are reranked by a reflex prediction model. We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets.","sentences":["Protolanguage reconstruction is central to historical linguistics.","The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change.","Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem.","We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms.","Leveraging another line of research -- reflex prediction -- we propose a system in which candidate protoforms from a reconstruction model are reranked by a reflex prediction model.","We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets."],"url":"http://arxiv.org/abs/2403.18769v1","category":"cs.CL"}
{"created":"2024-03-27 17:03:31","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning","abstract":"Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at https://constraints-as-terminations.github.io.","sentences":["Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion.","Yet, current solvers fail to produce efficient policies respecting hard constraints.","In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm.","Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain.","We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization).","Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption.","Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks.","Videos and code are available at https://constraints-as-terminations.github.io."],"url":"http://arxiv.org/abs/2403.18765v1","category":"cs.RO"}
{"created":"2024-03-27 16:57:40","title":"The Fubini--Study metric on an `odd' Grassmannian is rigid","abstract":"Following the ideas of Gasqui and Goldschmidt, we give an explicit description of the infinitesimal Einstein deformations admitted by the Fubini--Study metric on complex Grassmannians $G_{m}(\\mathbb{C}^{n+m})$ with $m,n\\geq 2$. The deformations were first shown to exist by Koiso in the 1980s but it has remained an open question as to whether they can be integrated to give genuine deformations of the Fubini--Study metric. We show that when $n+m$ is odd, the answer is no.","sentences":["Following the ideas of Gasqui and Goldschmidt, we give an explicit description of the infinitesimal Einstein deformations admitted by the Fubini--Study metric on complex Grassmannians $G_{m}(\\mathbb{C}^{n+m})$ with $m,n\\geq 2$.","The deformations were first shown to exist by Koiso in the 1980s but it has remained an open question as to whether they can be integrated to give genuine deformations of the Fubini--Study metric.","We show that when $n+m$ is odd, the answer is no."],"url":"http://arxiv.org/abs/2403.18757v1","category":"math.DG"}
{"created":"2024-03-27 16:53:33","title":"Long gamma-ray burst light curves as the result of a common stochastic pulse-avalanche process","abstract":"Context. The complexity and variety exhibited by the light curves of long gamma-ray bursts (GRBs) enclose a wealth of information that still awaits being fully deciphered. Despite the tremendous advance in the knowledge of the energetics, structure, and composition of the relativistic jet that results from the core collapse of the progenitor star, the nature of the inner engine, how it powers the relativistic outflow, and the dissipation mechanisms remain open issues. Aims. A promising way to gain insights is describing GRB light curves as the result of a common stochastic process. In the Burst And Transient Source Experiment (BATSE) era, a stochastic pulse avalanche model was proposed and tested through the comparison of ensemble-average properties of simulated and real light curves. Here we aim to revive and further test this model. Methods. We apply it to two independent data sets, BATSE and Swift/BAT, through a machine learning approach: the model parameters are optimised using a genetic algorithm. Results. The average properties are successfully reproduced. Notwithstanding the different populations and passbands of both data sets, the corresponding optimal parameters are interestingly similar. In particular, for both sets the dynamics appears to be close to a critical state, which is key to reproduce the observed variety of time profiles. Conclusions. Our results propel the avalanche character in a critical regime as a key trait of the energy release in GRB engines, which underpins some kind of instability.","sentences":["Context.","The complexity and variety exhibited by the light curves of long gamma-ray bursts (GRBs) enclose a wealth of information that still awaits being fully deciphered.","Despite the tremendous advance in the knowledge of the energetics, structure, and composition of the relativistic jet that results from the core collapse of the progenitor star, the nature of the inner engine, how it powers the relativistic outflow, and the dissipation mechanisms remain open issues.","Aims.","A promising way to gain insights is describing GRB light curves as the result of a common stochastic process.","In the Burst And Transient Source Experiment (BATSE) era, a stochastic pulse avalanche model was proposed and tested through the comparison of ensemble-average properties of simulated and real light curves.","Here we aim to revive and further test this model.","Methods.","We apply it to two independent data sets, BATSE and Swift/BAT, through a machine learning approach: the model parameters are optimised using a genetic algorithm.","Results.","The average properties are successfully reproduced.","Notwithstanding the different populations and passbands of both data sets, the corresponding optimal parameters are interestingly similar.","In particular, for both sets the dynamics appears to be close to a critical state, which is key to reproduce the observed variety of time profiles.","Conclusions.","Our results propel the avalanche character in a critical regime as a key trait of the energy release in GRB engines, which underpins some kind of instability."],"url":"http://arxiv.org/abs/2403.18754v1","category":"astro-ph.HE"}
{"created":"2024-03-27 16:49:37","title":"Robust Numerical Algebraic Geometry","abstract":"The field of numerical algebraic geometry consists of algorithms for numerically solving systems of polynomial equations. When the system is exact, such as having rational coefficients, the solution set is well-defined. However, for a member of a parameterized family of polynomial systems where the parameter values may be measured with imprecision or arise from prior numerical computations, uncertainty may arise in the structure of the solution set, including the number of isolated solutions, the existence of higher dimensional solution components, and the number of irreducible components along with their multiplicities. The loci where these structures change form a stratification of exceptional algebraic sets in the space of parameters. We describe methodologies for making the interpretation of numerical results more robust by searching for nearby parameter values on an exceptional set. We demonstrate these techniques on several illustrative examples and then treat several more substantial problems arising from the kinematics of mechanisms and robots.","sentences":["The field of numerical algebraic geometry consists of algorithms for numerically solving systems of polynomial equations.","When the system is exact, such as having rational coefficients, the solution set is well-defined.","However, for a member of a parameterized family of polynomial systems where the parameter values may be measured with imprecision or arise from prior numerical computations, uncertainty may arise in the structure of the solution set, including the number of isolated solutions, the existence of higher dimensional solution components, and the number of irreducible components along with their multiplicities.","The loci where these structures change form a stratification of exceptional algebraic sets in the space of parameters.","We describe methodologies for making the interpretation of numerical results more robust by searching for nearby parameter values on an exceptional set.","We demonstrate these techniques on several illustrative examples and then treat several more substantial problems arising from the kinematics of mechanisms and robots."],"url":"http://arxiv.org/abs/2403.18749v1","category":"math.NA"}
{"created":"2024-03-27 16:48:44","title":"Extended eigenvalues of composition operator on Bergman space and Fock space","abstract":"A complex scalar k is said to be an extended eigenvalue of a bounded linear operator A on a complex Hilbert space if there is a nonzero operator X such that AX=kXA. There are some solutions to the problem of computing the extended eigenvalues for composition operators induced on the Fock space and Bergman space by linear fractional transformationsof the complex plane in this paper.","sentences":["A complex scalar k is said to be an extended eigenvalue of a bounded linear operator A on a complex Hilbert space if there is a nonzero operator X such that AX=kXA.","There are some solutions to the problem of computing the extended eigenvalues for composition operators induced on the Fock space and Bergman space by linear fractional transformationsof the complex plane in this paper."],"url":"http://arxiv.org/abs/2403.18748v1","category":"math.FA"}
{"created":"2024-03-27 16:41:39","title":"Highly confined incident-angle-robust surface phonon polariton bound states in the continuum metasurfaces","abstract":"Squeezing light into subwavelength dimensions is vital for on-chip integration of photonic technologies. One approach to overcome the diffraction limit is coupling light to material excitations, leading to polariton states. Here, we showcase how low-loss mid-infrared surface phonon polaritons enable metasurfaces supporting quasi-bound states in the continuum (qBICs) with deeply subwavelength unit cells. Utilizing 100 nm thick free-standing silicon carbide membranes, we achieve highly confined qBIC states with a unit cell volume ~ 10^4 times smaller than the diffraction limit. This results in remarkable robustness of the platform against the incident angle that is unique among qBIC systems. We also demonstrate vibrational strong coupling with a thin layer of spin-coated molecules, leveraging the small mode volume. This work introduces phononic qBICs as a novel ultra-confined nanophotonic platform, paving a way for the miniaturization of mid-infrared devices for molecular sensing and thermal radiation engineering.","sentences":["Squeezing light into subwavelength dimensions is vital for on-chip integration of photonic technologies.","One approach to overcome the diffraction limit is coupling light to material excitations, leading to polariton states.","Here, we showcase how low-loss mid-infrared surface phonon polaritons enable metasurfaces supporting quasi-bound states in the continuum (qBICs) with deeply subwavelength unit cells.","Utilizing 100 nm thick free-standing silicon carbide membranes, we achieve highly confined qBIC states with a unit cell volume ~ 10^4 times smaller than the diffraction limit.","This results in remarkable robustness of the platform against the incident angle that is unique among qBIC systems.","We also demonstrate vibrational strong coupling with a thin layer of spin-coated molecules, leveraging the small mode volume.","This work introduces phononic qBICs as a novel ultra-confined nanophotonic platform, paving a way for the miniaturization of mid-infrared devices for molecular sensing and thermal radiation engineering."],"url":"http://arxiv.org/abs/2403.18743v1","category":"physics.optics"}
{"created":"2024-03-27 16:32:32","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural Networks","abstract":"Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context. The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots. An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent. However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results. In many cases, the data is not homogeneously sampled and in this case, it is proposed to resample the data to make it homogeneously sampled. How densely the dataset is sampled turns out to be an important parameter; it should be chosen large enough to produce good results, but this also increases the size of the dataset which makes training slow. To reduce the number of samples needed during training, the paper also proposes a technique to, instead of resampling the dataset once before the training starts, randomly resample the dataset at the start of each epoch during the training. The proposed methodology is evaluated on both a simulated dataset and an experimental dataset of starter battery failures. The results show that if the data is homogeneously sampled the methodology works as intended and produces accurate survival models. The results also show that randomly resampling the dataset on each epoch is an effective way to reduce the size of the training data.","sentences":["Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context.","The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots.","An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent.","However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results.","In many cases, the data is not homogeneously sampled and in this case, it is proposed to resample the data to make it homogeneously sampled.","How densely the dataset is sampled turns out to be an important parameter; it should be chosen large enough to produce good results, but this also increases the size of the dataset which makes training slow.","To reduce the number of samples needed during training, the paper also proposes a technique to, instead of resampling the dataset once before the training starts, randomly resample the dataset at the start of each epoch during the training.","The proposed methodology is evaluated on both a simulated dataset and an experimental dataset of starter battery failures.","The results show that if the data is homogeneously sampled the methodology works as intended and produces accurate survival models.","The results also show that randomly resampling the dataset on each epoch is an effective way to reduce the size of the training data."],"url":"http://arxiv.org/abs/2403.18739v1","category":"cs.LG"}
{"created":"2024-03-27 16:22:27","title":"Identifying CP Basis Invariants in SMEFT","abstract":"Building on our automated framework that uses ring diagrams for classifying CP basis invariants [1], this paper broadens the application of the methodology with more extensive examples and a wider scope of theoretical frameworks. Here, we showcase its versatility through detailed analyses in the Standard Model Effective Field Theory (SMEFT) up to dim-2n, and SMEFT with sterile neutrinos ({\\nu}SMEFT) up to dimension-7. By integrating the ring-diagram technique with the Cayley-Hamilton theorem, we have developed a system that not only simplifies the process of identifying basic and joint invariants but also enables the automatic differentiation between CP-even and CP-odd invariants from the lowest orders. Additionally, this work presents a comparison of our results with those derived using the traditional Hilbert-Poincar\\'e series and its Plethystic logarithm. While these conventional approaches primarily yield the numerical count of invariants, our framework provides a complete structure of invariants, thereby surpassing the limitations of these traditional methods.","sentences":["Building on our automated framework that uses ring diagrams for classifying CP basis invariants [1], this paper broadens the application of the methodology with more extensive examples and a wider scope of theoretical frameworks.","Here, we showcase its versatility through detailed analyses in the Standard Model Effective Field Theory (SMEFT) up to dim-2n, and SMEFT with sterile neutrinos ({\\nu}SMEFT) up to dimension-7.","By integrating the ring-diagram technique with the Cayley-Hamilton theorem, we have developed a system that not only simplifies the process of identifying basic and joint invariants but also enables the automatic differentiation between CP-even and CP-odd invariants from the lowest orders.","Additionally, this work presents a comparison of our results with those derived using the traditional Hilbert-Poincar\\'e series and its Plethystic logarithm.","While these conventional approaches primarily yield the numerical count of invariants, our framework provides a complete structure of invariants, thereby surpassing the limitations of these traditional methods."],"url":"http://arxiv.org/abs/2403.18732v1","category":"hep-ph"}
{"created":"2024-03-27 16:20:55","title":"Towards Image Ambient Lighting Normalization","abstract":"Lighting normalization is a crucial but underexplored restoration task with broad applications. However, existing works often simplify this task within the context of shadow removal, limiting the light sources to one and oversimplifying the scene, thus excluding complex self-shadows and restricting surface classes to smooth ones. Although promising, such simplifications hinder generalizability to more realistic settings encountered in daily use. In this paper, we propose a new challenging task termed Ambient Lighting Normalization (ALN), which enables the study of interactions between shadows, unifying image restoration and shadow removal in a broader context. To address the lack of appropriate datasets for ALN, we introduce the large-scale high-resolution dataset Ambient6K, comprising samples obtained from multiple light sources and including self-shadows resulting from complex geometries, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong baseline that maximizes Image-Frequency joint entropy to selectively restore local areas under different lighting conditions, without relying on shadow localization priors. Experiments show that IFBlend achieves SOTA scores on Ambient6K and exhibits competitive performance on conventional shadow removal benchmarks compared to shadow-specific models with mask priors. The dataset, benchmark, and code are available at https://github.com/fvasluianu97/IFBlend.","sentences":["Lighting normalization is a crucial but underexplored restoration task with broad applications.","However, existing works often simplify this task within the context of shadow removal, limiting the light sources to one and oversimplifying the scene, thus excluding complex self-shadows and restricting surface classes to smooth ones.","Although promising, such simplifications hinder generalizability to more realistic settings encountered in daily use.","In this paper, we propose a new challenging task termed Ambient Lighting Normalization (ALN), which enables the study of interactions between shadows, unifying image restoration and shadow removal in a broader context.","To address the lack of appropriate datasets for ALN, we introduce the large-scale high-resolution dataset Ambient6K, comprising samples obtained from multiple light sources and including self-shadows resulting from complex geometries, which is the first of its kind.","For benchmarking, we select various mainstream methods and rigorously evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong baseline that maximizes Image-Frequency joint entropy to selectively restore local areas under different lighting conditions, without relying on shadow localization priors.","Experiments show that IFBlend achieves SOTA scores on Ambient6K and exhibits competitive performance on conventional shadow removal benchmarks compared to shadow-specific models with mask priors.","The dataset, benchmark, and code are available at https://github.com/fvasluianu97/IFBlend."],"url":"http://arxiv.org/abs/2403.18730v1","category":"cs.CV"}
{"created":"2024-03-27 16:14:52","title":"An exactly curl-free finite-volume scheme for a hyperbolic compressible barotropic two-phase model","abstract":"We present a new second order accurate structure-preserving finite volume scheme for the solution of the compressible barotropic two-phase model of Romenski et. al in multiple space dimensions. The governing equations fall into the wider class of symmetric hyperbolic and thermodynamically compatible (SHTC) systems and consist of a set of first-order hyperbolic partial differential equations (PDE). In the absence of algebraic source terms, the model is subject to a curl-free constraint for the relative velocity between the two phases. The main objective of this paper is, therefore, to preserve this structural property exactly also at the discrete level. The new numerical method is based on a staggered grid arrangement where the relative velocity field is stored in the cell vertexes while all the remaining variables are stored in the cell centers. This allows the definition of discretely compatible gradient and curl operators, which ensure that the discrete curl errors of the relative velocity field remain zero up to machine precision. A set of numerical results confirms this property also experimentally.","sentences":["We present a new second order accurate structure-preserving finite volume scheme for the solution of the compressible barotropic two-phase model of Romenski et.","al in multiple space dimensions.","The governing equations fall into the wider class of symmetric hyperbolic and thermodynamically compatible (SHTC) systems and consist of a set of first-order hyperbolic partial differential equations (PDE).","In the absence of algebraic source terms, the model is subject to a curl-free constraint for the relative velocity between the two phases.","The main objective of this paper is, therefore, to preserve this structural property exactly also at the discrete level.","The new numerical method is based on a staggered grid arrangement where the relative velocity field is stored in the cell vertexes while all the remaining variables are stored in the cell centers.","This allows the definition of discretely compatible gradient and curl operators, which ensure that the discrete curl errors of the relative velocity field remain zero up to machine precision.","A set of numerical results confirms this property also experimentally."],"url":"http://arxiv.org/abs/2403.18724v1","category":"math.NA"}
{"created":"2024-03-27 16:13:12","title":"Formally Modelling the Rijkswaterstaat Tunnel Control Systems in a Constrained Industrial Environment","abstract":"Rijkswaterstaat, the National Dutch body responsible for infrastructure, recognised the importance of formal modelling and set up a program to model the control of road tunnels. This is done to improve the standardisation of tunnel control and make communication with suppliers smoother. A subset of SysML is used to formulate the models, which are substantial. In an earlier paper we have shown that these models can be used to prove behavioural properties by manually translating the models to mCRL2. In this paper we report on an automatic translation to mCRL2. As the results of the translation became unwieldy, we also investigated modelling tunnel control in the specification language Dezyne which has built-in verification capabilities and compared the results.","sentences":["Rijkswaterstaat, the National Dutch body responsible for infrastructure, recognised the importance of formal modelling and set up a program to model the control of road tunnels.","This is done to improve the standardisation of tunnel control and make communication with suppliers smoother.","A subset of SysML is used to formulate the models, which are substantial.","In an earlier paper we have shown that these models can be used to prove behavioural properties by manually translating the models to mCRL2.","In this paper we report on an automatic translation to mCRL2.","As the results of the translation became unwieldy, we also investigated modelling tunnel control in the specification language Dezyne which has built-in verification capabilities and compared the results."],"url":"http://arxiv.org/abs/2403.18722v1","category":"cs.LO"}
{"created":"2024-03-27 16:11:49","title":"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations","abstract":"Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.","sentences":["Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning.","This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs.","We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert.","The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support.","We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same.","However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05).","However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05).","Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks.","To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education."],"url":"http://arxiv.org/abs/2403.18721v1","category":"cs.RO"}
{"created":"2024-03-27 15:56:42","title":"Dense Vision Transformer Compression with Few Samples","abstract":"Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels). Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression. But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge. In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes. This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module. DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity. DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants.","sentences":["Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels).","Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression.","But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge.","In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes.","This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module.","DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity.","DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants."],"url":"http://arxiv.org/abs/2403.18708v1","category":"cs.CV"}
{"created":"2024-03-27 15:56:23","title":"$p\\bar\u039b$ final-state interaction in the reactions $e^+e^- \\to K^- p \\bar \u039b$ and $J/\u03c8\\to K^- p \\bar \u039b$","abstract":"Near-threshold $p\\bar\\Lambda$ mass spectra for the reactions $e^+e^- \\to K^- p\\bar\\Lambda$ and $J/\\psi \\to K^- p\\bar\\Lambda$ are investigated with an emphasis on the role played by the interaction in the $p\\bar\\Lambda$ system. As guideline for the $p\\bar\\Lambda$ interaction a variety of $\\Lambda\\bar\\Lambda$ potential models is considered that have been established in the analysis of data on $p\\bar p\\to \\Lambda\\bar\\Lambda$ in the past. Arguments why the properties of the $p\\bar\\Lambda$ and $\\Lambda\\bar\\Lambda$ interactions can be expected to be very similar are provided. It is shown that the near-threshold enhancement in the invariant mass observed for the $e^+e^-$ reaction can be reproduced quantitatively by the assumed $p\\bar\\Lambda$ final-state interaction in the partial wave suggested by an amplitude analysis of the experiment. The effect of the $p\\bar\\Lambda$ final-state interaction in other decays is explored, including the recently measured reactions $B^- \\to J/\\psi\\, \\bar p \\Lambda$ and $B^+ \\to J/\\psi\\, p \\bar\\Lambda$. It is found that the final-state interaction improves the description of the measured invariant mass near threshold in most cases.","sentences":["Near-threshold $p\\bar\\Lambda$ mass spectra for the reactions $e^+e^- \\to K^- p\\bar\\Lambda$ and $J/\\psi \\to K^- p\\bar\\Lambda$ are investigated with an emphasis on the role played by the interaction in the $p\\bar\\Lambda$ system.","As guideline for the $p\\bar\\Lambda$ interaction a variety of $\\Lambda\\bar\\Lambda$ potential models is considered that have been established in the analysis of data on $p\\bar p\\to \\Lambda\\bar\\Lambda$ in the past.","Arguments why the properties of the $p\\bar\\Lambda$ and $\\Lambda\\bar\\Lambda$ interactions can be expected to be very similar are provided.","It is shown that the near-threshold enhancement in the invariant mass observed for the $e^+e^-$ reaction can be reproduced quantitatively by the assumed $p\\bar\\Lambda$ final-state interaction in the partial wave suggested by an amplitude analysis of the experiment.","The effect of the $p\\bar\\Lambda$ final-state interaction in other decays is explored, including the recently measured reactions $B^- \\to J/\\psi\\, \\bar p \\Lambda$ and $B^+ \\to J/\\psi\\, p \\bar\\Lambda$.","It is found that the final-state interaction improves the description of the measured invariant mass near threshold in most cases."],"url":"http://arxiv.org/abs/2403.18706v1","category":"nucl-th"}
{"created":"2024-03-27 15:52:54","title":"Fpga-Based Neural Thrust Controller for UAVs","abstract":"The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA for a popular open-source micro-UAV platform. We successfully validate its functionality by implementing an RL-based low-level controller using real-world experiments.","sentences":["The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms.","To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions.","Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments.","However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs.","This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency.","We propose a novel hardware board equipped with an Artix-7 FPGA for a popular open-source micro-UAV platform.","We successfully validate its functionality by implementing an RL-based low-level controller using real-world experiments."],"url":"http://arxiv.org/abs/2403.18703v1","category":"eess.SY"}
{"created":"2024-03-27 15:50:24","title":"Toward CXL-Native Memory Tiering via Device-Side Profiling","abstract":"The Compute Express Link (CXL) interconnect has provided the ability to integrate diverse memory types into servers via byte-addressable SerDes links. Harnessing the full potential of such heterogeneous memory systems requires efficient memory tiering. However, existing research in this domain has been constrained by low-resolution and high-overhead memory access profiling techniques. To address this critical challenge, we propose to enhance existing memory tiering systems with a novel NeoMem solution. NeoMem offloads memory profiling functions to device-side controllers, integrating a dedicated hardware unit called NeoProf. NeoProf readily tracks memory access and provides the operating system with crucial page hotness statistics and other useful system state information. On the OS kernel side, we introduce a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics. We implement NeoMem on a real CXL-enabled FPGA platform and Linux kernel v6.3. Comprehensive evaluations demonstrate that NeoMem achieves 32% to 67% geomean speedup over several existing memory tiering solutions.","sentences":["The Compute Express Link (CXL) interconnect has provided the ability to integrate diverse memory types into servers via byte-addressable SerDes links.","Harnessing the full potential of such heterogeneous memory systems requires efficient memory tiering.","However, existing research in this domain has been constrained by low-resolution and high-overhead memory access profiling techniques.","To address this critical challenge, we propose to enhance existing memory tiering systems with a novel NeoMem solution.","NeoMem offloads memory profiling functions to device-side controllers, integrating a dedicated hardware unit called NeoProf.","NeoProf readily tracks memory access and provides the operating system with crucial page hotness statistics and other useful system state information.","On the OS kernel side, we introduce a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics.","We implement NeoMem on a real CXL-enabled FPGA platform and Linux kernel v6.3.","Comprehensive evaluations demonstrate that NeoMem achieves 32% to 67% geomean speedup over several existing memory tiering solutions."],"url":"http://arxiv.org/abs/2403.18702v1","category":"cs.AR"}
{"created":"2024-03-27 15:46:25","title":"The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian","abstract":"While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\\% accuracy.   We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian.","sentences":["While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language.","This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   ","This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian.","These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   ","To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models.","We show that this is a challenging benchmark where current language models are bound by 60\\% accuracy.   ","We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian."],"url":"http://arxiv.org/abs/2403.18697v1","category":"cs.CL"}
{"created":"2024-03-27 15:36:47","title":"Supernova Archaeology with X-Ray Binary Winds -- The Case of GRO J1655-40","abstract":"Supernovae are responsible for the elemental enrichment of the galaxy and some are postulated to leave behind a black hole. In a stellar binary system the supernova pollutes its companion, and the black hole can accrete back its own debris and emit X-rays. In this sequence of events, which is only poorly understood, winds are ejected, and observed through X-ray absorption lines. Measuring abundances of elements in the wind can lead to inferences about the historical explosion and possibly identify the long-gone progenitor of the compact object. Here, we re-analyze the uniquely rich X-ray spectrum of the 2005 outburst of GRO J1655-40. We reconstruct the absorption measure distribution (AMD) of the wind, and find that it increases sharply with ionization from H-like O up to H-like Ca, and then flattens out. The AMD is then used to measure relative abundances of 18 different elements. The present abundances are in partial agreement with a previous work with discrepancies mostly for low-Z elements. The overabundance of odd-Z elements hints at a high-metallicity, high-mass ($\\simeq25\\,M_\\odot$) progenitor. Interestingly, the abundances are different from those measured in the companion atmosphere, indicating that the wind entrains lingering ambient supernova debris. This can be expected since the current total stellar mass of the binary ($<10\\,M_\\odot$) is much less than the progenitor mass.","sentences":["Supernovae are responsible for the elemental enrichment of the galaxy and some are postulated to leave behind a black hole.","In a stellar binary system the supernova pollutes its companion, and the black hole can accrete back its own debris and emit X-rays.","In this sequence of events, which is only poorly understood, winds are ejected, and observed through X-ray absorption lines.","Measuring abundances of elements in the wind can lead to inferences about the historical explosion and possibly identify the long-gone progenitor of the compact object.","Here, we re-analyze the uniquely rich X-ray spectrum of the 2005 outburst of GRO J1655-40.","We reconstruct the absorption measure distribution (AMD) of the wind, and find that it increases sharply with ionization from H-like O up to H-like Ca, and then flattens out.","The AMD is then used to measure relative abundances of 18 different elements.","The present abundances are in partial agreement with a previous work with discrepancies mostly for low-Z elements.","The overabundance of odd-Z elements hints at a high-metallicity, high-mass ($\\simeq25\\,M_\\odot$) progenitor.","Interestingly, the abundances are different from those measured in the companion atmosphere, indicating that the wind entrains lingering ambient supernova debris.","This can be expected since the current total stellar mass of the binary ($<10\\,M_\\odot$) is much less than the progenitor mass."],"url":"http://arxiv.org/abs/2403.18689v1","category":"astro-ph.HE"}
{"created":"2024-03-27 15:33:49","title":"Decision-Epoch Matters: Unveiling its Impact on the Stability of Scheduling with Randomly Varying Connectivity","abstract":"A classical queuing theory result states that in a parallel-queue single-server model, the maximum stability region does not depend on the scheduling decision epochs, and in particular is the same for preemptive and non-preemptive systems. We consider here the case in which each of the queues may be connected to the server or not, depending on an exogenous process. In our main result, we show that the maximum stability region now does strongly depend on how the decision epochs are defined. We compare the setting where decisions can be made at any moment in time (the unconstrained setting), to two other settings: decisions are taken either (i) at moments of a departure (non-preemptive scheduling), or (ii) when an exponentially clock rings with rate $\\gamma$. We characterise the maximum stability region for the two constrained configurations, allowing us to observe a reduction compared to the unconstrained configuration. In the non-preemptive setting, the maximum stability region is drastically reduced compared to the unconstrained setting and we conclude that a non-preemptive scheduler cannot take opportunistically advantage (in terms of stability) of the random varying connectivity. Instead, for the $\\gamma$ decision epochs, we observe that the maximum stability region is monotone in the rate of the decision moments $\\gamma$, and that one can be arbitrarily close to the maximum stability region in the unconstrained setting if we choose $\\gamma$ large enough. We further show that Serve Longest Connected (SLC) queue is maximum stable in both constrained settings, within the set of policies that select a queue among the connected ones. From a methodological viewpoint, we introduce a novel theoretical tool termed a ``test for fluid limits'' (TFL) that might be of independent interest. TFL is a simple test that, if satisfied by the fluid limit, allows us to conclude for stability.","sentences":["A classical queuing theory result states that in a parallel-queue single-server model, the maximum stability region does not depend on the scheduling decision epochs, and in particular is the same for preemptive and non-preemptive systems.","We consider here the case in which each of the queues may be connected to the server or not, depending on an exogenous process.","In our main result, we show that the maximum stability region now does strongly depend on how the decision epochs are defined.","We compare the setting where decisions can be made at any moment in time (the unconstrained setting), to two other settings: decisions are taken either (i) at moments of a departure (non-preemptive scheduling), or (ii) when an exponentially clock rings with rate $\\gamma$.","We characterise the maximum stability region for the two constrained configurations, allowing us to observe a reduction compared to the unconstrained configuration.","In the non-preemptive setting, the maximum stability region is drastically reduced compared to the unconstrained setting and we conclude that a non-preemptive scheduler cannot take opportunistically advantage (in terms of stability) of the random varying connectivity.","Instead, for the $\\gamma$ decision epochs, we observe that the maximum stability region is monotone in the rate of the decision moments $\\gamma$, and that one can be arbitrarily close to the maximum stability region in the unconstrained setting if we choose $\\gamma$ large enough.","We further show that Serve Longest Connected (SLC) queue is maximum stable in both constrained settings, within the set of policies that select a queue among the connected ones.","From a methodological viewpoint, we introduce a novel theoretical tool termed a ``test for fluid limits'' (TFL) that might be of independent interest.","TFL is a simple test that, if satisfied by the fluid limit, allows us to conclude for stability."],"url":"http://arxiv.org/abs/2403.18686v1","category":"math.PR"}
{"created":"2024-03-27 15:27:04","title":"Exploring the Berezinskii-Kosterlitz-Thouless Transition in a Two-dimensional Dipolar Bose Gas","abstract":"Long-range and anisotropic dipolar interactions induce complex order in quantum systems. It becomes particularly interesting in two-dimension (2D), where the superfluidity with quasi-long-range order emerges via Berezinskii-Kosterlitz-Thouless (BKT) mechanism, which still remains elusive with dipolar interactions. Here, we observe the BKT transition from a normal gas to the superfluid phase in a quasi-2D dipolar Bose gas of erbium atoms. Controlling the orientation of dipoles, we characterize the transition point by monitoring extended coherence and measuring the equation of state. This allows us to gain a systematic understanding of the BKT transition based on an effective short-range description of dipolar interaction in 2D. Additionally, we observe anisotropic density fluctuations and non-local effects in the superfluid regime, which establishes the dipolar nature of the 2D superfluid. Our results lay the ground for understanding the behavior of dipolar bosons in 2D and open up opportunities for examining complex orders in a dipolar superfluid.","sentences":["Long-range and anisotropic dipolar interactions induce complex order in quantum systems.","It becomes particularly interesting in two-dimension (2D), where the superfluidity with quasi-long-range order emerges via Berezinskii-Kosterlitz-Thouless (BKT) mechanism, which still remains elusive with dipolar interactions.","Here, we observe the BKT transition from a normal gas to the superfluid phase in a quasi-2D dipolar Bose gas of erbium atoms.","Controlling the orientation of dipoles, we characterize the transition point by monitoring extended coherence and measuring the equation of state.","This allows us to gain a systematic understanding of the BKT transition based on an effective short-range description of dipolar interaction in 2D. Additionally, we observe anisotropic density fluctuations and non-local effects in the superfluid regime, which establishes the dipolar nature of the 2D superfluid.","Our results lay the ground for understanding the behavior of dipolar bosons in 2D and open up opportunities for examining complex orders in a dipolar superfluid."],"url":"http://arxiv.org/abs/2403.18683v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-27 15:20:51","title":"A blue repulsive potential for dysprosium Bose-Einstein condensates","abstract":"Short-wavelength repulsive potentials for quantum gases allow to realize new systems and to study new phenomena. Here we report the realization of repulsive optical potentials for dysprosium atoms in the blue region of the spectrum, at wavelengths close to 400 nm. We employ a spectrallyfiltered diode laser system to measure both scalar and tensorial components of the polarizability of dysprosium, which we find in good agreement with the theoretical predictions. We demonstrate the implementation of potential strengths appropriate to manipulate Bose-Einstein condensates, with scattering-limited lifetimes exceeding one second. This type of optical potentials opens interesting directions for the study of dipolar superfluids and supersolids.","sentences":["Short-wavelength repulsive potentials for quantum gases allow to realize new systems and to study new phenomena.","Here we report the realization of repulsive optical potentials for dysprosium atoms in the blue region of the spectrum, at wavelengths close to 400 nm.","We employ a spectrallyfiltered diode laser system to measure both scalar and tensorial components of the polarizability of dysprosium, which we find in good agreement with the theoretical predictions.","We demonstrate the implementation of potential strengths appropriate to manipulate Bose-Einstein condensates, with scattering-limited lifetimes exceeding one second.","This type of optical potentials opens interesting directions for the study of dipolar superfluids and supersolids."],"url":"http://arxiv.org/abs/2403.18677v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-27 15:17:59","title":"Submanifolds with boundary and Stokes' Theorem in Heisenberg groups","abstract":"We introduce and study the notion of $C^1_\\mathbb{H}$-regular submanifold with boundary in sub-Riemannian Heisenberg groups. As an application, we prove a version of Stokes' Theorem for $C^1_\\mathbb{H}$-regular submanifolds with boundary that takes into account Rumin's complex of differential forms in Heisenberg groups.","sentences":["We introduce and study the notion of $C^1_\\mathbb{H}$-regular submanifold with boundary in sub-Riemannian Heisenberg groups.","As an application, we prove a version of Stokes' Theorem for $C^1_\\mathbb{H}$-regular submanifolds with boundary that takes into account Rumin's complex of differential forms in Heisenberg groups."],"url":"http://arxiv.org/abs/2403.18675v1","category":"math.DG"}
{"created":"2024-03-27 15:14:46","title":"Globally integrable quantum systems and their perturbations","abstract":"In this paper we present the notion of globally integrable quantum system that we introduced in [BL22]: we motivate it using the spectral theory of pseudodifferential operators and then we give some results on linear and nonlinear perturbations of a globally integrable quantum system. In particular, we give a spectral result ensuring stability of most of its eigenvalues under relatively bounded perturbations, and two results controlling the growth of Sobolev norms when it is subject either to linear unbounded time dependent perturbations or a small nonlinear Hamiltonian nonlinear perturbation.","sentences":["In this paper we present the notion of globally integrable quantum system that we introduced in [BL22]: we motivate it using the spectral theory of pseudodifferential operators and then we give some results on linear and nonlinear perturbations of a globally integrable quantum system.","In particular, we give a spectral result ensuring stability of most of its eigenvalues under relatively bounded perturbations, and two results controlling the growth of Sobolev norms when it is subject either to linear unbounded time dependent perturbations or a small nonlinear Hamiltonian nonlinear perturbation."],"url":"http://arxiv.org/abs/2403.18670v1","category":"math.AP"}
{"created":"2024-03-27 15:13:26","title":"Orthogonal Polynomials with a Singularly Perturbed Airy Weight","abstract":"We study the monic orthogonal polynomials with respect to a singularly perturbed Airy weight. By using Chen and Ismail's ladder operator approach, we derive a discrete system satisfied by the recurrence coefficients for the orthogonal polynomials. We find that the orthogonal polynomials satisfy a second-order linear ordinary differential equation, whose coefficients are all expressed in terms of the recurrence coefficients. By considering the time evolution, we obtain a system of differential-difference equations satisfied by the recurrence coefficients. Finally, we study the asymptotics of the recurrence coefficients when the degrees of the orthogonal polynomials tend to infinity.","sentences":["We study the monic orthogonal polynomials with respect to a singularly perturbed Airy weight.","By using Chen and Ismail's ladder operator approach, we derive a discrete system satisfied by the recurrence coefficients for the orthogonal polynomials.","We find that the orthogonal polynomials satisfy a second-order linear ordinary differential equation, whose coefficients are all expressed in terms of the recurrence coefficients.","By considering the time evolution, we obtain a system of differential-difference equations satisfied by the recurrence coefficients.","Finally, we study the asymptotics of the recurrence coefficients when the degrees of the orthogonal polynomials tend to infinity."],"url":"http://arxiv.org/abs/2403.18669v1","category":"math.CA"}
{"created":"2024-03-27 15:11:00","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users","abstract":"Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding. Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships between entities within the knowledge graph by utilizing semantic information from text. It leads to more accurate, relevant, and diverse user recommendations and a benefit that extends even to cold-start users who have few interactions with items. We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach. Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective. Furthermore, our results provide evidence that item-based contrastive learning enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment.","sentences":["Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding.","Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals.","A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task.","It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc.","In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions.","We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata.","Our approach allows the model to better understand the relationships between entities within the knowledge graph by utilizing semantic information from text.","It leads to more accurate, relevant, and diverse user recommendations and a benefit that extends even to cold-start users who have few interactions with items.","We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach.","Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective.","Furthermore, our results provide evidence that item-based contrastive learning enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment."],"url":"http://arxiv.org/abs/2403.18667v1","category":"cs.IR"}
{"created":"2024-03-27 15:08:00","title":"Neural Network-Based Piecewise Survival Models","abstract":"In this paper, a family of neural network-based survival models is presented. The models are specified based on piecewise definitions of the hazard function and the density function on a partitioning of the time; both constant and linear piecewise definitions are presented, resulting in a family of four models. The models can be seen as an extension of the commonly used discrete-time and piecewise exponential models and thereby add flexibility to this set of standard models. Using a simulated dataset the models are shown to perform well compared to the highly expressive, state-of-the-art energy-based model, while only requiring a fraction of the computation time.","sentences":["In this paper, a family of neural network-based survival models is presented.","The models are specified based on piecewise definitions of the hazard function and the density function on a partitioning of the time; both constant and linear piecewise definitions are presented, resulting in a family of four models.","The models can be seen as an extension of the commonly used discrete-time and piecewise exponential models and thereby add flexibility to this set of standard models.","Using a simulated dataset the models are shown to perform well compared to the highly expressive, state-of-the-art energy-based model, while only requiring a fraction of the computation time."],"url":"http://arxiv.org/abs/2403.18664v1","category":"stat.ML"}
{"created":"2024-03-27 15:02:25","title":"Full quantitative near-field characterization of strongly coupled exciton-plasmon polaritons in thin-layered WSe2 on a monocrystalline gold platelet","abstract":"Exciton-plasmon polaritons (EPPs) are attractive both for the exploration of fundamental phenomena and applications in nanophotonics. Previous studies of EPPs mainly relied on far-field characterization. Here, using near-field optical microscopy, we quantitatively characterize the dispersion of EPPs existing in 13-nm-thick tungsten diselenide (WSe$_2$) deposited on a monocrystalline gold platelet. We extract from our experimental data a Rabi splitting of 81 meV, and an experimental effective polariton loss of 55 meV, demonstrating that our system is in the strong-coupling regime. Furthermore, we measure for the first time at visible wavelengths the propagation length of these EPPs for each excitation energy of the dispersion relation. To demonstrate the quantitative nature of our near-field method to obtain the full complex-valued wavevector of EPPs, we use our near-field measurements to predict, via the transfer matrix method, the far-field reflectivities across the exciton resonance. These predictions are in excellent agreement with our experimental far-field measurements. Our findings open the door towards the full near-field study of light-manipulating devices at the nanoscale.","sentences":["Exciton-plasmon polaritons (EPPs) are attractive both for the exploration of fundamental phenomena and applications in nanophotonics.","Previous studies of EPPs mainly relied on far-field characterization.","Here, using near-field optical microscopy, we quantitatively characterize the dispersion of EPPs existing in 13-nm-thick tungsten diselenide (WSe$_2$) deposited on a monocrystalline gold platelet.","We extract from our experimental data a Rabi splitting of 81 meV, and an experimental effective polariton loss of 55 meV, demonstrating that our system is in the strong-coupling regime.","Furthermore, we measure for the first time at visible wavelengths the propagation length of these EPPs for each excitation energy of the dispersion relation.","To demonstrate the quantitative nature of our near-field method to obtain the full complex-valued wavevector of EPPs, we use our near-field measurements to predict, via the transfer matrix method, the far-field reflectivities across the exciton resonance.","These predictions are in excellent agreement with our experimental far-field measurements.","Our findings open the door towards the full near-field study of light-manipulating devices at the nanoscale."],"url":"http://arxiv.org/abs/2403.18655v1","category":"physics.optics"}
{"created":"2024-03-27 15:00:46","title":"Indecomposable set-theoretical solutions to the Yang-Baxter equation of size $p^2$","abstract":"The quantum Yang-Baxter equation is a braiding condition on complex vector spaces which is of high relevance in several fields of mathematics, such as knot theory and quantum group theory. A combinatorial approach is the investigation of set-theoretic solutions to the Yang--Baxter equation and their associated algebraic structures. In this article, we focus on indecomposable set-theoretic solutions to the Yang--Baxter equation. More specifically, we give a full classification of those which are of size $p^2$.","sentences":["The quantum Yang-Baxter equation is a braiding condition on complex vector spaces which is of high relevance in several fields of mathematics, such as knot theory and quantum group theory.","A combinatorial approach is the investigation of set-theoretic solutions to the Yang--Baxter equation and their associated algebraic structures.","In this article, we focus on indecomposable set-theoretic solutions to the Yang--Baxter equation.","More specifically, we give a full classification of those which are of size $p^2$."],"url":"http://arxiv.org/abs/2403.18653v1","category":"math.QA"}
{"created":"2024-03-27 14:59:51","title":"The Tolman-Ehrenfest effect for an ideal gas in a background of time-independent electric, magnetic, and gravitational fields","abstract":"The statistical mechanics of an ideal gas of point particles moving in a time independent background metric with $g_{0j}\\neq 0$ is investigated. An explicit calculation shows that when there is no background electrostatic or magnetostatic field the thermodynamic pressure, energy density, and thermally averaged energy-momentum tensor depend on temperature and chemical potential only through the ratios $T_{0}/\\sqrt{g_{00}}$ and $\\mu_{0}/\\sqrt{g_{00}}$. A background magnetostatic field does not change this, however with a background electrostatic field the previous results are multiplied by a factor $\\exp(-eA_{0}/T_{0})$, which is an exception to the strict Tolman-Ehrenfest rule because the system is open.","sentences":["The statistical mechanics of an ideal gas of point particles moving in a time independent background metric with $g_{0j}\\neq 0$ is investigated.","An explicit calculation shows that when there is no background electrostatic or magnetostatic field the thermodynamic pressure, energy density, and thermally averaged energy-momentum tensor depend on temperature and chemical potential only through the ratios $T_{0}/\\sqrt{g_{00}}$ and $\\mu_{0}/\\sqrt{g_{00}}$. A background magnetostatic field does not change this, however with a background electrostatic field the previous results are multiplied by a factor $\\exp(-eA_{0}/T_{0})$, which is an exception to the strict Tolman-Ehrenfest rule because the system is open."],"url":"http://arxiv.org/abs/2403.18652v1","category":"hep-th"}
{"created":"2024-03-27 14:56:39","title":"Can the splashback radius be an observable boundary of galaxy clusters?","abstract":"The splashback radius was proposed as a physically motivated boundary of clusters as it sets the limit between the infalling and the orbitally dominated regions. However, galaxy clusters are complex objects connected to filaments of the cosmic web from which they accrete matter that disturbs them and modifies their morphology. In this context, estimating the splashback radius and the cluster boundary becomes challenging. In this work, we use a constrained hydrodynamical simulation of the Virgo cluster's replica embedded in its large-scale structure to investigate the impact of its local environment on the splashback radius estimate. We identify the splashback radius from 3D radial profiles of dark matter density, baryons density, and pressure in three regions representative of different dynamical states: accretion from spherical collapse, filaments, and matter outflow. We also identify the splashback radius from 2D-projected radial profiles of observation-like quantities: mass surface density, emission measure, and Compton-y. We show that the splashback radius mainly depends on the dynamics in each region and the physical processes traced by the different probes. We find multiple values for the splashback radius ranging from 3.3$\\pm$0.2 to 5.5$\\pm$0.3 Mpc. Particularly, in the regions of collapsing and outflowing material, the splashback radii estimated from baryon density and pressure radial profiles overestimate that of the dark matter density profiles, which is considered the reference value originally defined from dark matter simulations. Consequently, caution is required when using the splashback radius as a boundary of clusters, particularly in the case of highly disturbed clusters like Virgo. We also discuss the detection of the splashback radius from pressure radial profiles, which could be more related to an accretion shock, and its detection from stacked radial profiles.","sentences":["The splashback radius was proposed as a physically motivated boundary of clusters as it sets the limit between the infalling and the orbitally dominated regions.","However, galaxy clusters are complex objects connected to filaments of the cosmic web from which they accrete matter that disturbs them and modifies their morphology.","In this context, estimating the splashback radius and the cluster boundary becomes challenging.","In this work, we use a constrained hydrodynamical simulation of the Virgo cluster's replica embedded in its large-scale structure to investigate the impact of its local environment on the splashback radius estimate.","We identify the splashback radius from 3D radial profiles of dark matter density, baryons density, and pressure in three regions representative of different dynamical states: accretion from spherical collapse, filaments, and matter outflow.","We also identify the splashback radius from 2D-projected radial profiles of observation-like quantities: mass surface density, emission measure, and","Compton-y. We show that the splashback radius mainly depends on the dynamics in each region and the physical processes traced by the different probes.","We find multiple values for the splashback radius ranging from 3.3$\\pm$0.2 to 5.5$\\pm$0.3 Mpc.","Particularly, in the regions of collapsing and outflowing material, the splashback radii estimated from baryon density and pressure radial profiles overestimate that of the dark matter density profiles, which is considered the reference value originally defined from dark matter simulations.","Consequently, caution is required when using the splashback radius as a boundary of clusters, particularly in the case of highly disturbed clusters like Virgo.","We also discuss the detection of the splashback radius from pressure radial profiles, which could be more related to an accretion shock, and its detection from stacked radial profiles."],"url":"http://arxiv.org/abs/2403.18648v1","category":"astro-ph.CO"}
{"created":"2024-03-27 14:44:24","title":"Mind the Domain Gap: a Systematic Analysis on Bioacoustic Sound Event Detection","abstract":"Detecting the presence of animal vocalisations in nature is essential to study animal populations and their behaviors. A recent development in the field is the introduction of the task known as few-shot bioacoustic sound event detection, which aims to train a versatile animal sound detector using only a small set of audio samples. Previous efforts in this area have utilized different architectures and data augmentation techniques to enhance model performance. However, these approaches have not fully bridged the domain gap between source and target distributions, limiting their applicability in real-world scenarios. In this work, we introduce an new dataset designed to augment the diversity and breadth of classes available for few-shot bioacoustic event detection, building on the foundations of our previous datasets. To establish a robust baseline system tailored for the DCASE 2024 Task 5 challenge, we delve into an array of acoustic features and adopt negative hard sampling as our primary domain adaptation strategy. This approach, chosen in alignment with the challenge's guidelines that necessitate the independent treatment of each audio file, sidesteps the use of transductive learning to ensure compliance while aiming to enhance the system's adaptability to domain shifts. Our experiments show that the proposed baseline system achieves a better performance compared with the vanilla prototypical network. The findings also confirm the effectiveness of each domain adaptation method by ablating different components within the networks. This highlights the potential to improve few-shot bioacoustic sound event detection by further reducing the impact of domain shift.","sentences":["Detecting the presence of animal vocalisations in nature is essential to study animal populations and their behaviors.","A recent development in the field is the introduction of the task known as few-shot bioacoustic sound event detection, which aims to train a versatile animal sound detector using only a small set of audio samples.","Previous efforts in this area have utilized different architectures and data augmentation techniques to enhance model performance.","However, these approaches have not fully bridged the domain gap between source and target distributions, limiting their applicability in real-world scenarios.","In this work, we introduce an new dataset designed to augment the diversity and breadth of classes available for few-shot bioacoustic event detection, building on the foundations of our previous datasets.","To establish a robust baseline system tailored for the DCASE 2024 Task 5 challenge, we delve into an array of acoustic features and adopt negative hard sampling as our primary domain adaptation strategy.","This approach, chosen in alignment with the challenge's guidelines that necessitate the independent treatment of each audio file, sidesteps the use of transductive learning to ensure compliance while aiming to enhance the system's adaptability to domain shifts.","Our experiments show that the proposed baseline system achieves a better performance compared with the vanilla prototypical network.","The findings also confirm the effectiveness of each domain adaptation method by ablating different components within the networks.","This highlights the potential to improve few-shot bioacoustic sound event detection by further reducing the impact of domain shift."],"url":"http://arxiv.org/abs/2403.18638v1","category":"eess.AS"}
{"created":"2024-03-27 14:42:08","title":"Transformers-based architectures for stroke segmentation: A review","abstract":"Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes. The emergence of deep learning methodologies has transformed the landscape of medical image analysis. Recently, Transformers, initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis. This comprehensive review aims to provide an in-depth exploration of the cutting-edge Transformer-based architectures applied in the context of stroke segmentation. It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation. Subsequently, the review delves into the fundamental ideas of Transformers, offering detailed insights into their architectural intricacies and the underlying mechanisms that empower them to effectively capture complex spatial information within medical images. The existing literature is systematically categorized and analyzed, discussing various approaches that leverage Transformers for stroke segmentation. A critical assessment is provided, highlighting the strengths and limitations of these methods, including considerations of performance and computational efficiency. Additionally, this review explores potential avenues for future research and development","sentences":["Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes.","The emergence of deep learning methodologies has transformed the landscape of medical image analysis.","Recently, Transformers, initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis.","This comprehensive review aims to provide an in-depth exploration of the cutting-edge Transformer-based architectures applied in the context of stroke segmentation.","It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation.","Subsequently, the review delves into the fundamental ideas of Transformers, offering detailed insights into their architectural intricacies and the underlying mechanisms that empower them to effectively capture complex spatial information within medical images.","The existing literature is systematically categorized and analyzed, discussing various approaches that leverage Transformers for stroke segmentation.","A critical assessment is provided, highlighting the strengths and limitations of these methods, including considerations of performance and computational efficiency.","Additionally, this review explores potential avenues for future research and development"],"url":"http://arxiv.org/abs/2403.18637v1","category":"eess.IV"}
{"created":"2024-03-27 14:40:25","title":"Fusion approaches for emotion recognition from speech using acoustic and text-based features","abstract":"In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions.","sentences":["In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features.","We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings.","We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets.","We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches.","Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results.","In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions."],"url":"http://arxiv.org/abs/2403.18635v1","category":"cs.LG"}
{"created":"2024-03-27 14:37:01","title":"To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models","abstract":"Most current recommender systems primarily focus on what to recommend, assuming users always require personalized recommendations. However, with the widely spread of ChatGPT and other chatbots, a more crucial problem in the context of conversational systems is how to minimize user disruption when we provide recommendation services for users. While previous research has extensively explored different user intents in dialogue systems, fewer efforts are made to investigate whether recommendations should be provided. In this paper, we formally define the recommendability identification problem, which aims to determine whether recommendations are necessary in a specific scenario. First, we propose and define the recommendability identification task, which investigates the need for recommendations in the current conversational context. A new dataset is constructed. Subsequently, we discuss and evaluate the feasibility of leveraging pre-trained language models (PLMs) for recommendability identification. Finally, through comparative experiments, we demonstrate that directly employing PLMs with zero-shot results falls short of meeting the task requirements. Besides, fine-tuning or utilizing soft prompt techniques yields comparable results to traditional classification methods. Our work is the first to study recommendability before recommendation and provides preliminary ways to make it a fundamental component of the future recommendation system.","sentences":["Most current recommender systems primarily focus on what to recommend, assuming users always require personalized recommendations.","However, with the widely spread of ChatGPT and other chatbots, a more crucial problem in the context of conversational systems is how to minimize user disruption when we provide recommendation services for users.","While previous research has extensively explored different user intents in dialogue systems, fewer efforts are made to investigate whether recommendations should be provided.","In this paper, we formally define the recommendability identification problem, which aims to determine whether recommendations are necessary in a specific scenario.","First, we propose and define the recommendability identification task, which investigates the need for recommendations in the current conversational context.","A new dataset is constructed.","Subsequently, we discuss and evaluate the feasibility of leveraging pre-trained language models (PLMs) for recommendability identification.","Finally, through comparative experiments, we demonstrate that directly employing PLMs with zero-shot results falls short of meeting the task requirements.","Besides, fine-tuning or utilizing soft prompt techniques yields comparable results to traditional classification methods.","Our work is the first to study recommendability before recommendation and provides preliminary ways to make it a fundamental component of the future recommendation system."],"url":"http://arxiv.org/abs/2403.18628v1","category":"cs.IR"}
{"created":"2024-03-27 14:33:58","title":"qIoV: A Quantum-Driven Internet-of-Vehicles-Based Approach for Environmental Monitoring and Rapid Response Systems","abstract":"This research addresses the critical necessity for advanced rapid response operations in managing a spectrum of environmental hazards. We propose a novel framework, qIoV that integrates quantum computing with the Internet-of-Vehicles (IoV) to leverage the computational efficiency, parallelism, and entanglement properties of quantum mechanics. Our approach involves the use of environmental sensors mounted on vehicles for precise air quality assessment. These sensors are designed to be highly sensitive and accurate, leveraging the principles of quantum mechanics to detect and measure environmental parameters. A salient feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system designed to dynamically adjust the quantum network topology in accordance with vehicular movements. This capability is critical to maintaining the integrity of quantum states against environmental and vehicular disturbances, thereby ensuring reliable data transmission and processing. Moreover, our methodology is further augmented by the incorporation of a variational quantum classifier (VQC) with advanced quantum entanglement techniques. This integration offers a significant reduction in latency for hazard alert transmission, thus enabling expedited communication of crucial data to emergency response teams and the public. Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system, revealed significant advancements in pair plot analysis, achieving over 90% in precision, recall, and F1-Score metrics and an 83% increase in the speed of toxic gas detection compared to conventional methods.Additionally, theoretical analyses validate the efficiency of quantum rotation, teleportation protocols, and the fidelity of quantum entanglement, further underscoring the potential of quantum computing in enhancing analytical performance.","sentences":["This research addresses the critical necessity for advanced rapid response operations in managing a spectrum of environmental hazards.","We propose a novel framework, qIoV that integrates quantum computing with the Internet-of-Vehicles (IoV) to leverage the computational efficiency, parallelism, and entanglement properties of quantum mechanics.","Our approach involves the use of environmental sensors mounted on vehicles for precise air quality assessment.","These sensors are designed to be highly sensitive and accurate, leveraging the principles of quantum mechanics to detect and measure environmental parameters.","A salient feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system designed to dynamically adjust the quantum network topology in accordance with vehicular movements.","This capability is critical to maintaining the integrity of quantum states against environmental and vehicular disturbances, thereby ensuring reliable data transmission and processing.","Moreover, our methodology is further augmented by the incorporation of a variational quantum classifier (VQC) with advanced quantum entanglement techniques.","This integration offers a significant reduction in latency for hazard alert transmission, thus enabling expedited communication of crucial data to emergency response teams and the public.","Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system, revealed significant advancements in pair plot analysis, achieving over 90% in precision, recall, and F1-Score metrics and an 83% increase in the speed of toxic gas detection compared to conventional methods.","Additionally, theoretical analyses validate the efficiency of quantum rotation, teleportation protocols, and the fidelity of quantum entanglement, further underscoring the potential of quantum computing in enhancing analytical performance."],"url":"http://arxiv.org/abs/2403.18622v1","category":"cs.ET"}
{"created":"2024-03-27 14:29:55","title":"Random Apollonian networks with tailored clustering coefficient","abstract":"We introduce a family of complex networks that interpolates between the Apollonian network and its binary version, recently introduced in [Phys. Rev. E \\textbf{107}, 024305 (2023)], via random removal of nodes. The dilution process allows the clustering coefficient to vary from $C=0.828$ to $C=0$ while maintaining the behavior of average path length and other relevant quantities as in the deterministic Apollonian network. Robustness against the random deletion of nodes is also reported on spectral quantities such as the ground-state localization degree and its energy gap to the first excited state. The loss of the $2\\pi / 3$ rotation symmetry as a tree-like network emerges is investigated in the light of the hub wavefunction amplitude. Our findings expose the interplay between the small-world property and other distinctive traits exhibited by Apollonian networks, as well as their resilience against random attacks.","sentences":["We introduce a family of complex networks that interpolates between the Apollonian network and its binary version, recently introduced in [Phys.","Rev. E \\textbf{107}, 024305 (2023)], via random removal of nodes.","The dilution process allows the clustering coefficient to vary from $C=0.828$ to $C=0$ while maintaining the behavior of average path length and other relevant quantities as in the deterministic Apollonian network.","Robustness against the random deletion of nodes is also reported on spectral quantities such as the ground-state localization degree and its energy gap to the first excited state.","The loss of the $2\\pi / 3$ rotation symmetry as a tree-like network emerges is investigated in the light of the hub wavefunction amplitude.","Our findings expose the interplay between the small-world property and other distinctive traits exhibited by Apollonian networks, as well as their resilience against random attacks."],"url":"http://arxiv.org/abs/2403.18615v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-27 14:28:01","title":"Bowen's formula for a rational graph-directed Markov system","abstract":"We establish Bowen's formula for the Julia set of a non-elementary, expanding, irreducible and aperiodic rational graph-directed Markov system satisfying the backward separating condition. Towards this end, we shall prove that the associated skew product map is topologically exact on the skew product Julia set, and satisfies the density of repelling periodic points. Moreover, we give a criterion for expandingness in terms of hyperbolicity.","sentences":["We establish Bowen's formula for the Julia set of a non-elementary, expanding, irreducible and aperiodic rational graph-directed Markov system satisfying the backward separating condition.","Towards this end, we shall prove that the associated skew product map is topologically exact on the skew product Julia set, and satisfies the density of repelling periodic points.","Moreover, we give a criterion for expandingness in terms of hyperbolicity."],"url":"http://arxiv.org/abs/2403.18612v1","category":"math.DS"}
{"created":"2024-03-27 14:20:11","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue Constitutive Law and Microstructure From Digital Image Correlation Measurements","abstract":"Human tissues are highly organized structures with specific collagen fiber arrangements varying from point to point. The effects of such heterogeneity play an important role for tissue function, and hence it is of critical to discover and understand the distribution of such fiber orientations from experimental measurements, such as the digital image correlation data. To this end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO) approach, for data-driven constitutive modeling of heterogeneous anisotropic materials. The goal is to learn both a nonlocal constitutive law together with the material microstructure, in the form of a heterogeneous fiber orientation field, from loading field-displacement field measurements. To this end, we propose a two-phase learning approach. Firstly, we learn a homogeneous constitutive law in the form of a neural network-based kernel function and a nonlocal bond force, to capture complex homogeneous material responses from data. Then, in the second phase we reinitialize the learnt bond force and the kernel function, and training them together with a fiber orientation field for each material point. Owing to the state-based peridynamic skeleton, our HeteroPNO-learned material models are objective and have the balance of linear and angular momentum guaranteed. Moreover, the effects from heterogeneity and nonlinear constitutive relationship are captured by the kernel function and the bond force respectively, enabling physical interpretability. As a result, our HeteroPNO architecture can learn a constitutive model for a biological tissue with anisotropic heterogeneous response undergoing large deformation regime. Moreover, the framework is capable to provide displacement and stress field predictions for new and unseen loading instances.","sentences":["Human tissues are highly organized structures with specific collagen fiber arrangements varying from point to point.","The effects of such heterogeneity play an important role for tissue function, and hence it is of critical to discover and understand the distribution of such fiber orientations from experimental measurements, such as the digital image correlation data.","To this end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO) approach, for data-driven constitutive modeling of heterogeneous anisotropic materials.","The goal is to learn both a nonlocal constitutive law together with the material microstructure, in the form of a heterogeneous fiber orientation field, from loading field-displacement field measurements.","To this end, we propose a two-phase learning approach.","Firstly, we learn a homogeneous constitutive law in the form of a neural network-based kernel function and a nonlocal bond force, to capture complex homogeneous material responses from data.","Then, in the second phase we reinitialize the learnt bond force and the kernel function, and training them together with a fiber orientation field for each material point.","Owing to the state-based peridynamic skeleton, our HeteroPNO-learned material models are objective and have the balance of linear and angular momentum guaranteed.","Moreover, the effects from heterogeneity and nonlinear constitutive relationship are captured by the kernel function and the bond force respectively, enabling physical interpretability.","As a result, our HeteroPNO architecture can learn a constitutive model for a biological tissue with anisotropic heterogeneous response undergoing large deformation regime.","Moreover, the framework is capable to provide displacement and stress field predictions for new and unseen loading instances."],"url":"http://arxiv.org/abs/2403.18597v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 14:17:58","title":"Contact processes with quenched disorder on $\\mathbb{Z}^d$ and on Erdos-Renyi graphs","abstract":"In real systems impurities and defects play an important role in determining their properties. Here we will consider what probabilists have called the contact process in a random environment and what physicists have more precisely named the contact process with quenched disorder. We will concentrate our efforts on the special case called the random dilution model, in which sites independently and with probability $p$ are active and particles on them give birth at rate $\\lambda$, while the other sites are inert and particles on them do not give birth. We show that the resulting inhomogeniety can make dramatic changes in the behavior in the supercritical, subcritical, and critical behavior. In particular, the usual exponential decay of the desnity of particles in the subcritical phase becomes a power law (the Griffiths phase), and polynomial decay at the critical value becomes a power of $\\log$.","sentences":["In real systems impurities and defects play an important role in determining their properties.","Here we will consider what probabilists have called the contact process in a random environment and what physicists have more precisely named the contact process with quenched disorder.","We will concentrate our efforts on the special case called the random dilution model, in which sites independently and with probability $p$ are active and particles on them give birth at rate $\\lambda$, while the other sites are inert and particles on them do not give birth.","We show that the resulting inhomogeniety can make dramatic changes in the behavior in the supercritical, subcritical, and critical behavior.","In particular, the usual exponential decay of the desnity of particles in the subcritical phase becomes a power law (the Griffiths phase), and polynomial decay at the critical value becomes a power of $\\log$."],"url":"http://arxiv.org/abs/2403.18592v1","category":"math.PR"}
{"created":"2024-03-27 14:11:32","title":"From Virtual Reality to the Emerging Discipline of Perception Engineering","abstract":"This paper makes the case that a powerful new discipline, which we term perception engineering, is steadily emerging. It follows from a progression of ideas that involve creating illusions, from historical paintings and film, to video games and virtual reality in modern times. Rather than creating physical artifacts such as bridges, airplanes, or computers, perception engineers create illusory perceptual experiences. The scope is defined over any agent that interacts with the physical world, including both biological organisms (humans, animals) and engineered systems (robots, autonomous systems). The key idea is that an agent, called a producer, alters the environment with the intent to alter the perceptual experience of another agent, called a receiver. Most importantly, the paper introduces a precise mathematical formulation of this process, based on the von Neumann-Morgenstern notion of information, to help scope and define the discipline. It is then applied to the cases of engineered and biological agents with discussion of its implications on existing fields such as virtual reality, robotics, and even social media. Finally, open challenges and opportunities for involvement are identified.","sentences":["This paper makes the case that a powerful new discipline, which we term perception engineering, is steadily emerging.","It follows from a progression of ideas that involve creating illusions, from historical paintings and film, to video games and virtual reality in modern times.","Rather than creating physical artifacts such as bridges, airplanes, or computers, perception engineers create illusory perceptual experiences.","The scope is defined over any agent that interacts with the physical world, including both biological organisms (humans, animals) and engineered systems (robots, autonomous systems).","The key idea is that an agent, called a producer, alters the environment with the intent to alter the perceptual experience of another agent, called a receiver.","Most importantly, the paper introduces a precise mathematical formulation of this process, based on the von Neumann-Morgenstern notion of information, to help scope and define the discipline.","It is then applied to the cases of engineered and biological agents with discussion of its implications on existing fields such as virtual reality, robotics, and even social media.","Finally, open challenges and opportunities for involvement are identified."],"url":"http://arxiv.org/abs/2403.18588v1","category":"cs.HC"}
{"created":"2024-03-27 14:06:24","title":"Resonances crossing effect and quantum sensor of electric fields","abstract":"While the phenomenon of the exact crossing of energy levels is a rarely occurring event, in the case of quantum resonances associated with metastable states this phenomenon is much more frequent and various scenarios can occur. When there is an exact crossing of the imaginary parts of the resonances in a two-level quantum system subject to an external DC electric field, then a damped beating phenomenon occurs, which is absent if there is no such crossing. This fact, tested numerically on an explicit one-dimensional model, suggests the possibility of designing quantum sensors to determine in a very simple way whether the external field strength has an assigned value or not.","sentences":["While the phenomenon of the exact crossing of energy levels is a rarely occurring event, in the case of quantum resonances associated with metastable states this phenomenon is much more frequent and various scenarios can occur.","When there is an exact crossing of the imaginary parts of the resonances in a two-level quantum system subject to an external DC electric field, then a damped beating phenomenon occurs, which is absent if there is no such crossing.","This fact, tested numerically on an explicit one-dimensional model, suggests the possibility of designing quantum sensors to determine in a very simple way whether the external field strength has an assigned value or not."],"url":"http://arxiv.org/abs/2403.18585v1","category":"quant-ph"}
{"created":"2024-03-27 13:52:41","title":"Bootstrapping Guarantees: Stability and Performance Analysis for Dynamic Encrypted Control","abstract":"Encrypted dynamic controllers that operate for an unlimited time have been a challenging subject of research. The fundamental difficulty is the accumulation of errors and scaling factors in the internal state during operation. Bootstrapping, a technique commonly employed in fully homomorphic cryptosystems, can be used to avoid overflows in the controller state but can potentially introduce significant numerical errors. In this paper, we analyze dynamic encrypted control with explicit consideration of bootstrapping. By recognizing the bootstrapping errors occurring in the controller's state as an uncertainty in the robust control framework, we can provide stability and performance guarantees for the whole encrypted control system. Further, the conservatism of the stability and performance test is reduced by using a lifted version of the control system.","sentences":["Encrypted dynamic controllers that operate for an unlimited time have been a challenging subject of research.","The fundamental difficulty is the accumulation of errors and scaling factors in the internal state during operation.","Bootstrapping, a technique commonly employed in fully homomorphic cryptosystems, can be used to avoid overflows in the controller state but can potentially introduce significant numerical errors.","In this paper, we analyze dynamic encrypted control with explicit consideration of bootstrapping.","By recognizing the bootstrapping errors occurring in the controller's state as an uncertainty in the robust control framework, we can provide stability and performance guarantees for the whole encrypted control system.","Further, the conservatism of the stability and performance test is reduced by using a lifted version of the control system."],"url":"http://arxiv.org/abs/2403.18571v1","category":"eess.SY"}
{"created":"2024-03-27 13:46:32","title":"Computer-Assisted Proofs of Existence of Invariant Tori in Quasi-periodic Systems via Fourier Methods","abstract":"The goal of this paper is to provide a methodology to prove existence of (fiberwise hyperbolic) real-analytic invariant tori in real-analytic quasi-periodic skew-product dynamical systems that present nearly-invariant tori of the same characteristics. The methodology is based on the application of a Newton-Kantorovich theorem whose hypotheses are tested using Fourier analysis methods for a numerical approximation of the parameterization of an invariant torus.","sentences":["The goal of this paper is to provide a methodology to prove existence of (fiberwise hyperbolic) real-analytic invariant tori in real-analytic quasi-periodic skew-product dynamical systems that present nearly-invariant tori of the same characteristics.","The methodology is based on the application of a Newton-Kantorovich theorem whose hypotheses are tested using Fourier analysis methods for a numerical approximation of the parameterization of an invariant torus."],"url":"http://arxiv.org/abs/2403.18566v1","category":"math.DS"}
{"created":"2024-03-27 13:44:20","title":"Natural convection in a vertical channel. Part 2. Oblique solutions and global bifurcations in a spanwise-extended domain","abstract":"Vertical thermal convection is a non-equilibrium system in which both buoyancy and shear forces play a role in driving the convective flow. Beyond the onset of convection, the driven dissipative system exhibits chaotic dynamics and turbulence. In a three-dimensional domain extended in both the vertical and the transverse dimensions, Gao et al. (2018) have observed a variety of convection patterns which are not described by linear stability analysis. We investigate the fully non-linear dynamics of vertical convection using a dynamical-systems approach based on the Oberbeck-Boussinesq equations. We compute the invariant solutions of these equations and the bifurcations that are responsible for the creation and termination of various branches. We map out a sequence of local bifurcations from the laminar base state, including simultaneous bifurcations involving patterned steady states with different symmetries. This atypical phenomenon of multiple branches simultaneously bifurcating from a single parent branch is explained by the role of D4 symmetry. In addition, two global bifurcations are identified: first, a homoclinic cycle from modulated transverse rolls and second, a robust heteroclinic cycle linking two symmetry-related diamond-roll patterns. These are confirmed by phase space projections as well as the functional form of the divergence of the period close to the bifurcation points. The intricacy of this bifurcation diagram highlights the essential role played by dynamical systems theory and computation in hydrodynamic configurations.","sentences":["Vertical thermal convection is a non-equilibrium system in which both buoyancy and shear forces play a role in driving the convective flow.","Beyond the onset of convection, the driven dissipative system exhibits chaotic dynamics and turbulence.","In a three-dimensional domain extended in both the vertical and the transverse dimensions, Gao et al. (2018) have observed a variety of convection patterns which are not described by linear stability analysis.","We investigate the fully non-linear dynamics of vertical convection using a dynamical-systems approach based on the Oberbeck-Boussinesq equations.","We compute the invariant solutions of these equations and the bifurcations that are responsible for the creation and termination of various branches.","We map out a sequence of local bifurcations from the laminar base state, including simultaneous bifurcations involving patterned steady states with different symmetries.","This atypical phenomenon of multiple branches simultaneously bifurcating from a single parent branch is explained by the role of D4 symmetry.","In addition, two global bifurcations are identified: first, a homoclinic cycle from modulated transverse rolls and second, a robust heteroclinic cycle linking two symmetry-related diamond-roll patterns.","These are confirmed by phase space projections as well as the functional form of the divergence of the period close to the bifurcation points.","The intricacy of this bifurcation diagram highlights the essential role played by dynamical systems theory and computation in hydrodynamic configurations."],"url":"http://arxiv.org/abs/2403.18563v1","category":"physics.flu-dyn"}
{"created":"2024-03-27 13:42:14","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","abstract":"Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS. These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and fine-tuning on clean data is superior to supervised learning on clean data across all testing conditions, and superior to supervised MTR for testing conditions of SNR above 5 dB. This indicates that pretraining alone can increase the model's robustness. Finally, it is found that using noisy data for pretraining models, especially with the Data2Vec-denoising approach, significantly enhances the robustness of KWS models in noisy conditions.","sentences":["Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used.","Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance.","Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions.","This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   ","Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS.","These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR).","The results show that pretraining and fine-tuning on clean data is superior to supervised learning on clean data across all testing conditions, and superior to supervised MTR for testing conditions of SNR above 5 dB. This indicates that pretraining alone can increase the model's robustness.","Finally, it is found that using noisy data for pretraining models, especially with the Data2Vec-denoising approach, significantly enhances the robustness of KWS models in noisy conditions."],"url":"http://arxiv.org/abs/2403.18560v1","category":"eess.AS"}
{"created":"2024-03-27 13:39:39","title":"Existence and compactness of global weak solutions of three-dimensional axisymmetric Ericksen-Leslie system","abstract":"In dimension three, the existence of global weak solutions to the axisymmetric simplified Ericksen-Leslie system without swirl is established. This is achieved by analyzing weak convergence of solutions of the axisymmetric Ginzburg-Landau approximated solutions as the penalization parameter $\\varepsilon$ tends to zero. The proof relies on the one hand on the use of a blow-up argument to rule out energy concentration off the $z$-axis, which exploits the topological restrictions of the axisymmetry. On the other hand, possible limiting energy concentrations on the $z$-axis can be dealt by a cancellation argument at the origin. Once more, the axisymmetry plays a substantial role. We will also show that the set of axisymmetric solutions without swirl $(u,d)$ to the simplified Ericksen-Leslie system is compact under weak convergence in $L^\\infty_tL^2_x\\times L^2_tH^1_x$.","sentences":["In dimension three, the existence of global weak solutions to the axisymmetric simplified Ericksen-Leslie system without swirl is established.","This is achieved by analyzing weak convergence of solutions of the axisymmetric Ginzburg-Landau approximated solutions as the penalization parameter $\\varepsilon$ tends to zero.","The proof relies on the one hand on the use of a blow-up argument to rule out energy concentration off the $z$-axis, which exploits the topological restrictions of the axisymmetry.","On the other hand, possible limiting energy concentrations on the $z$-axis can be dealt by a cancellation argument at the origin.","Once more, the axisymmetry plays a substantial role.","We will also show that the set of axisymmetric solutions without swirl $(u,d)$ to the simplified Ericksen-Leslie system is compact under weak convergence in $L^\\infty_tL^2_x\\times L^2_tH^1_x$."],"url":"http://arxiv.org/abs/2403.18559v1","category":"math.AP"}
{"created":"2024-03-27 13:39:06","title":"Stability Properties of the Impulsive Goodwin's Oscillator in 1-cycle","abstract":"The Impulsive Goodwin's Oscillator (IGO) is a mathematical model of a hybrid closed-loop system. It arises by closing a special kind of continuous linear positive time-invariant system with impulsive feedback, which employs both amplitude and frequency pulse modulation. The structure of IGO precludes the existence of equilibria, and all its solutions are oscillatory. With its origin in mathematical biology, the IGO also presents a control paradigm useful in a wide range of applications, in particular dosing of chemicals and medicines. Since the pulse modulation feedback mechanism introduces significant nonlinearity and non-smoothness in the closedloop dynamics, conventional controller design methods fail to apply. However, the hybrid dynamics of IGO reduce to a nonlinear, time-invariant discrete-time system, exhibiting a one-to-one correspondence between periodic solutions of the original IGO and those of the discrete-time system. The paper proposes a design approach that leverages the linearization of the equivalent discrete-time dynamics in the vicinity of a fixed point. A simple and efficient local stability condition of the 1-cycle in terms of the characteristics of the amplitude and frequency modulation functions is obtained.","sentences":["The Impulsive Goodwin's Oscillator (IGO) is a mathematical model of a hybrid closed-loop system.","It arises by closing a special kind of continuous linear positive time-invariant system with impulsive feedback, which employs both amplitude and frequency pulse modulation.","The structure of IGO precludes the existence of equilibria, and all its solutions are oscillatory.","With its origin in mathematical biology, the IGO also presents a control paradigm useful in a wide range of applications, in particular dosing of chemicals and medicines.","Since the pulse modulation feedback mechanism introduces significant nonlinearity and non-smoothness in the closedloop dynamics, conventional controller design methods fail to apply.","However, the hybrid dynamics of IGO reduce to a nonlinear, time-invariant discrete-time system, exhibiting a one-to-one correspondence between periodic solutions of the original IGO and those of the discrete-time system.","The paper proposes a design approach that leverages the linearization of the equivalent discrete-time dynamics in the vicinity of a fixed point.","A simple and efficient local stability condition of the 1-cycle in terms of the characteristics of the amplitude and frequency modulation functions is obtained."],"url":"http://arxiv.org/abs/2403.18557v1","category":"math.OC"}
{"created":"2024-03-27 13:32:37","title":"The Distribution of Highly Red-sloped Asteroids in the Middle and Outer Main Belt","abstract":"Red (S > 10%/0.1 micron) spectral slopes are common among Centaurs and trans-Neptunian objects (TNOs) in the outer solar system. Interior to and co-orbital with Jupiter, the red (S approx. = 10%/0.1 micron) slopes of D-type main-belt and Jupiter Trojan asteroids are thought to reflect their hypothesized shared origin with TNOs beyond the orbit of Jupiter. In order to quantify the abundance of red-sloped asteroids within the main belt, we conducted a survey using the NASA Infrared Telescope Facility and the Lowell Discovery Telescope. We followed up on 32 candidate red objects identified via spectrophotometry from the Sloan Digital Sky Survey's Moving Object Catalog to confirm their steep spectral slopes and determine their taxonomic classifications. We find that our criteria for identifying candidate red objects from the Moving Object Catalog result in an approx. 50% confirmation rate for steeply red-sloped asteroids. We also compare our observations of main-belt asteroids to existing literature spectra of the Jupiter Trojans and steeply red-sloped main-belt asteroids. We show that some red-sloped asteroids have linearly increasing reflectance with increasing wavelength, while other red-sloped asteroids show a flattening in slope at longer near-infrared wavelengths, indicating a diversity among the population of spectrally red main-belt asteroids suggestive of a variety of origins among the population of steep-sloped asteroids.","sentences":["Red (S > 10%/0.1 micron) spectral slopes are common among Centaurs and trans-Neptunian objects (TNOs) in the outer solar system.","Interior to and co-orbital with Jupiter, the red (S approx.","= 10%/0.1 micron) slopes of D-type main-belt and Jupiter Trojan asteroids are thought to reflect their hypothesized shared origin with TNOs beyond the orbit of Jupiter.","In order to quantify the abundance of red-sloped asteroids within the main belt, we conducted a survey using the NASA Infrared Telescope Facility and the Lowell Discovery Telescope.","We followed up on 32 candidate red objects identified via spectrophotometry from the Sloan Digital Sky Survey's Moving Object Catalog to confirm their steep spectral slopes and determine their taxonomic classifications.","We find that our criteria for identifying candidate red objects from the Moving Object Catalog result in an approx.","50% confirmation rate for steeply red-sloped asteroids.","We also compare our observations of main-belt asteroids to existing literature spectra of the Jupiter Trojans and steeply red-sloped main-belt asteroids.","We show that some red-sloped asteroids have linearly increasing reflectance with increasing wavelength, while other red-sloped asteroids show a flattening in slope at longer near-infrared wavelengths, indicating a diversity among the population of spectrally red main-belt asteroids suggestive of a variety of origins among the population of steep-sloped asteroids."],"url":"http://arxiv.org/abs/2403.18553v1","category":"astro-ph.EP"}
{"created":"2024-03-27 13:24:06","title":"On the distribution of the components of multicurves of given type","abstract":"We study the distribution of the individual components of a random multicurve under the action of the mapping class group.","sentences":["We study the distribution of the individual components of a random multicurve under the action of the mapping class group."],"url":"http://arxiv.org/abs/2403.18544v1","category":"math.GT"}
{"created":"2024-03-27 13:14:29","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","abstract":"Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversarial training. Environmental factors, including sim-to-real transfer and domain adaptation, are also scrutinized to understand how RL systems can adapt to diverse and dynamic surroundings. Moreover, human involvement is an integral ingredient of the analysis, acknowledging the broad set of roles that humans can take in this context.   Importantly, to aid practitioners in navigating the complexities of safe and robust RL implementation, this paper introduces a practical checklist derived from the synthesized literature. The checklist encompasses critical aspects of algorithm design, training environment considerations, and ethical guidelines. It will serve as a resource for developers and policymakers alike to ensure the responsible deployment of RL systems in many application domains.","sentences":["Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness.","This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations.","We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   ","After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents.","We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversarial training.","Environmental factors, including sim-to-real transfer and domain adaptation, are also scrutinized to understand how RL systems can adapt to diverse and dynamic surroundings.","Moreover, human involvement is an integral ingredient of the analysis, acknowledging the broad set of roles that humans can take in this context.   ","Importantly, to aid practitioners in navigating the complexities of safe and robust RL implementation, this paper introduces a practical checklist derived from the synthesized literature.","The checklist encompasses critical aspects of algorithm design, training environment considerations, and ethical guidelines.","It will serve as a resource for developers and policymakers alike to ensure the responsible deployment of RL systems in many application domains."],"url":"http://arxiv.org/abs/2403.18539v1","category":"cs.LG"}
{"created":"2024-03-27 13:13:16","title":"Uphill drift in the absence of current in single-file diffusion","abstract":"Single-file diffusion is a paradigmatic model for the transport of Brownian colloidal particles in narrow one-dimensional channels, such as those found in certain porous media, where the particles cannot cross each other. We consider a system where a different external uniform potential is present to the right and left of an origin. For example, this is the case when two channels meeting at the origin have different radii. In equilibrium, the chemical potential of the particles are equal, the density is thus lower in the region with the higher potential, and by definition there is no net current in the system. Remarkably, a single-file tracer particle initially located at the origin, with position denoted by $Y(t)$, exhibits an average {\\em up-hill} drift toward the region of {\\em highest} potential. This drift has the late time behavior $\\langle Y(t)\\rangle= C t^{1/4}$, where the prefactor $C$ depends on the initial particle arrangement. This surprising result is shown analytically by computing the first two moments of $Y(t)$ through a simple and physically-illuminating method, and also via extensive numerical simulations.","sentences":["Single-file diffusion is a paradigmatic model for the transport of Brownian colloidal particles in narrow one-dimensional channels, such as those found in certain porous media, where the particles cannot cross each other.","We consider a system where a different external uniform potential is present to the right and left of an origin.","For example, this is the case when two channels meeting at the origin have different radii.","In equilibrium, the chemical potential of the particles are equal, the density is thus lower in the region with the higher potential, and by definition there is no net current in the system.","Remarkably, a single-file tracer particle initially located at the origin, with position denoted by $Y(t)$, exhibits an average {\\em up-hill} drift toward the region of {\\em highest} potential.","This drift has the late time behavior $\\langle Y(t)\\rangle= C t^{1/4}$, where the prefactor $C$ depends on the initial particle arrangement.","This surprising result is shown analytically by computing the first two moments of $Y(t)$ through a simple and physically-illuminating method, and also via extensive numerical simulations."],"url":"http://arxiv.org/abs/2403.18538v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-27 13:11:34","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","abstract":"Recent studies reveal a significant theoretical link between variational autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to estimate the theoretical upper bound of the information rate-distortion function of images. Such estimated theoretical bounds substantially exceed the performance of existing neural image codecs (NICs). To narrow this gap, we propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The proposed BG-VAE leverages the theoretical bound to guide the NIC model towards enhanced performance. We implement the BG-VAE using Hierarchical VAEs and demonstrate its effectiveness through extensive experiments. Along with advanced neural network blocks, we provide a versatile, variable-rate NIC that outperforms existing methods when considering both rate-distortion performance and computational complexity. The code is available at BG-VAE.","sentences":["Recent studies reveal a significant theoretical link between variational autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to estimate the theoretical upper bound of the information rate-distortion function of images.","Such estimated theoretical bounds substantially exceed the performance of existing neural image codecs (NICs).","To narrow this gap, we propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC.","The proposed BG-VAE leverages the theoretical bound to guide the NIC model towards enhanced performance.","We implement the BG-VAE using Hierarchical VAEs and demonstrate its effectiveness through extensive experiments.","Along with advanced neural network blocks, we provide a versatile, variable-rate NIC that outperforms existing methods when considering both rate-distortion performance and computational complexity.","The code is available at BG-VAE."],"url":"http://arxiv.org/abs/2403.18535v1","category":"eess.IV"}
{"created":"2024-03-27 13:00:10","title":"Limited Attention Allocation in a Stochastic Linear Quadratic System with Multiplicative Noise","abstract":"This study addresses limited attention allocation in a stochastic linear quadratic system with multiplicative noise. Our approach enables strategic resource allocation to enhance noise estimation and improve control decisions. We provide analytical optimal control and propose a numerical method for optimal attention allocation. Additionally, we apply our ffndings to dynamic mean-variance portfolio selection, showing effective resource allocation across time periods and factors, providing valuable insights for investors.","sentences":["This study addresses limited attention allocation in a stochastic linear quadratic system with multiplicative noise.","Our approach enables strategic resource allocation to enhance noise estimation and improve control decisions.","We provide analytical optimal control and propose a numerical method for optimal attention allocation.","Additionally, we apply our ffndings to dynamic mean-variance portfolio selection, showing effective resource allocation across time periods and factors, providing valuable insights for investors."],"url":"http://arxiv.org/abs/2403.18528v1","category":"math.OC"}
{"created":"2024-03-27 12:55:16","title":"Bridging the Gap: Regularized Reinforcement Learning for Improved Classical Motion Planning with Safety Modules","abstract":"Classical navigation planners can provide safe navigation, albeit often suboptimally and with hindered human norm compliance. ML-based, contemporary autonomous navigation algorithms can imitate more natural and humancompliant navigation, but usually require large and realistic datasets and do not always provide safety guarantees. We present an approach that leverages a classical algorithm to guide reinforcement learning. This greatly improves the results and convergence rate of the underlying RL algorithm and requires no human-expert demonstrations to jump-start the process. Additionally, we incorporate a practical fallback system that can switch back to a classical planner to ensure safety. The outcome is a sample efficient ML approach for mobile navigation that builds on classical algorithms, improves them to ensure human compliance, and guarantees safety.","sentences":["Classical navigation planners can provide safe navigation, albeit often suboptimally and with hindered human norm compliance.","ML-based, contemporary autonomous navigation algorithms can imitate more natural and humancompliant navigation, but usually require large and realistic datasets and do not always provide safety guarantees.","We present an approach that leverages a classical algorithm to guide reinforcement learning.","This greatly improves the results and convergence rate of the underlying RL algorithm and requires no human-expert demonstrations to jump-start the process.","Additionally, we incorporate a practical fallback system that can switch back to a classical planner to ensure safety.","The outcome is a sample efficient ML approach for mobile navigation that builds on classical algorithms, improves them to ensure human compliance, and guarantees safety."],"url":"http://arxiv.org/abs/2403.18524v1","category":"cs.RO"}
{"created":"2024-03-27 12:44:27","title":"Realizing temporal transportation trees","abstract":"In this paper, we study the complexity of the \\textit{periodic temporal graph realization} problem with respect to upper bounds on the fastest path durations among its vertices. This constraint with respect to upper bounds appears naturally in transportation network design applications where, for example, a road network is given, and the goal is to appropriately schedule periodic travel routes, while not exceeding some desired upper bounds on the travel times. This approach is in contrast to verification applications of the graph realization problems, where exact values for the distances (respectively, fastest travel times) are given, following some kind of precise measurement. In our work, we focus only on underlying tree topologies, which are fundamental in many transportation network applications.   As it turns out, the periodic upper-bounded temporal tree realization problem (TTR) has a very different computational complexity behavior than both (i) the classic graph realization problem with respect to shortest path distances in static graphs and (ii) the periodic temporal graph realization problem with exact given fastest travel times (which was recently introduced). First, we prove that, surprisingly, TTR is NP-hard, even for a constant period $\\Delta$ and when the input tree $G$ satisfies at least one of the following conditions: (a) $G$ has a constant diameter, or (b) $G$ has constant maximum degree. In contrast, when we are given exact values of the fastest travel delays, the problem is known to be solvable in polynomial time. Second, we prove that TTR is fixed-parameter tractable (FPT) with respect to the number of leaves in the input tree $G$, via a novel combination of techniques for totally unimodular matrices and mixed integer linear programming.","sentences":["In this paper, we study the complexity of the \\textit{periodic temporal graph realization} problem with respect to upper bounds on the fastest path durations among its vertices.","This constraint with respect to upper bounds appears naturally in transportation network design applications where, for example, a road network is given, and the goal is to appropriately schedule periodic travel routes, while not exceeding some desired upper bounds on the travel times.","This approach is in contrast to verification applications of the graph realization problems, where exact values for the distances (respectively, fastest travel times) are given, following some kind of precise measurement.","In our work, we focus only on underlying tree topologies, which are fundamental in many transportation network applications.   ","As it turns out, the periodic upper-bounded temporal tree realization problem (TTR) has a very different computational complexity behavior than both (i) the classic graph realization problem with respect to shortest path distances in static graphs and (ii) the periodic temporal graph realization problem with exact given fastest travel times (which was recently introduced).","First, we prove that, surprisingly, TTR is NP-hard, even for a constant period $\\Delta$ and when the input tree $G$ satisfies at least one of the following conditions: (a) $G$ has a constant diameter, or (b) $G$ has constant maximum degree.","In contrast, when we are given exact values of the fastest travel delays, the problem is known to be solvable in polynomial time.","Second, we prove that TTR is fixed-parameter tractable (FPT) with respect to the number of leaves in the input tree $G$, via a novel combination of techniques for totally unimodular matrices and mixed integer linear programming."],"url":"http://arxiv.org/abs/2403.18513v1","category":"cs.DS"}
{"created":"2024-03-27 12:35:17","title":"Moduli spaces of untwisted wild Riemann surfaces","abstract":"We construct moduli stacks of wild Riemann surfaces in the (pure) untwisted case, for any complex reductive structure group, and we define the corresponding (pure) wild mapping class groups.","sentences":["We construct moduli stacks of wild Riemann surfaces in the (pure) untwisted case, for any complex reductive structure group, and we define the corresponding (pure) wild mapping class groups."],"url":"http://arxiv.org/abs/2403.18505v1","category":"math.AG"}
{"created":"2024-03-27 12:21:19","title":"Global unique solutions to the planar inhomogeneous Navier--Stokes--Maxwell equations","abstract":"The evolution of an electrically conducting imcompressible fluid with nonconstant density can be described by a set of equations combining the continuity, momentum and Maxwell's equations; altogether known as the inhomogeneous Navier--Stokes--Maxwell system.   In this paper, we focus on the global well-posedness of these equations in two dimensions. Specifically, we are able to prove the existence of global energy solutions, provided that the initial velocity field belongs to the Besov space $\\dot{B}^{r}_{p,1}(\\mathbb{R}^2)$, with $r=-1+\\frac{2}{p}$, for some $p\\in (1,2)$, while the initial electromagnetic field enjoys some $H^s(\\mathbb{R}^2)$ Sobolev regularity, for some $s \\geq 2-\\frac{2}{p} \\in (0,1)$, and whenever the initial fluid density is bounded pointwise and close to a nonnegative constant. Moreover, if it is assumed that $s>\\frac{1}{2}$, then the solution is shown to be unique in the class of all energy solutions.   It is to be emphasized that the solutions constructed here are global and uniformly bounded with respect to the speed of light $c\\in (0,\\infty)$. This important fact allows us to derive the inhomogeneous MHD system as the speed of light tends to infinity.","sentences":["The evolution of an electrically conducting imcompressible fluid with nonconstant density can be described by a set of equations combining the continuity, momentum and Maxwell's equations; altogether known as the inhomogeneous Navier--Stokes--Maxwell system.   ","In this paper, we focus on the global well-posedness of these equations in two dimensions.","Specifically, we are able to prove the existence of global energy solutions, provided that the initial velocity field belongs to the Besov space $\\dot{B}^{r}_{p,1}(\\mathbb{R}^2)$, with $r=-1+\\frac{2}{p}$, for some $p\\in (1,2)$, while the initial electromagnetic field enjoys some $H^s(\\mathbb{R}^2)$ Sobolev regularity, for some $s \\geq 2-\\frac{2}{p} \\in (0,1)$, and whenever the initial fluid density is bounded pointwise and close to a nonnegative constant.","Moreover, if it is assumed that $s>\\frac{1}{2}$, then the solution is shown to be unique in the class of all energy solutions.   ","It is to be emphasized that the solutions constructed here are global and uniformly bounded with respect to the speed of light $c\\in (0,\\infty)$. This important fact allows us to derive the inhomogeneous MHD system as the speed of light tends to infinity."],"url":"http://arxiv.org/abs/2403.18500v1","category":"math.AP"}
{"created":"2024-03-27 12:18:56","title":"Minimum sum vertex cover: kernelization and parameterized algorithms","abstract":"Given an ordering of the vertices of a graph, the cost of covering an edge is the smaller number of its two ends. The minimum sum vertex cover problem asks for an ordering that minimizes the total cost of covering all edges. We consider parameterized complexity of this problem, using the largest cost~$k$ of covering a single edge as the parameter. Note that the first $k$ vertices form a (not necessarily minimal) vertex cover of the graph, and ordering of vertices after $k$ is irrelevant. We present a $(2k^2 + 3k)$-vertex kernel and an $O(|E(G)| + 2^kk! k^4)$-time algorithm for the minimum sum vertex cover problem.","sentences":["Given an ordering of the vertices of a graph, the cost of covering an edge is the smaller number of its two ends.","The minimum sum vertex cover problem asks for an ordering that minimizes the total cost of covering all edges.","We consider parameterized complexity of this problem, using the largest cost~$k$ of covering a single edge as the parameter.","Note that the first $k$ vertices form a (not necessarily minimal) vertex cover of the graph, and ordering of vertices after $k$ is irrelevant.","We present a $(2k^2 + 3k)$-vertex kernel and an $O(|E(G)| + 2^kk!","k^4)$-time algorithm for the minimum sum vertex cover problem."],"url":"http://arxiv.org/abs/2403.18497v1","category":"cs.DS"}
{"created":"2024-03-27 12:15:22","title":"Direct mineral content prediction from drill core images via transfer learning","abstract":"Deep subsurface exploration is important for mining, oil and gas industries, as well as in the assessment of geological units for the disposal of chemical or nuclear waste, or the viability of geothermal energy systems. Typically, detailed examinations of subsurface formations or units are performed on cuttings or core materials extracted during drilling campaigns, as well as on geophysical borehole data, which provide detailed information about the petrophysical properties of the rocks. Depending on the volume of rock samples and the analytical program, the laboratory analysis and diagnostics can be very time-consuming. This study investigates the potential of utilizing machine learning, specifically convolutional neural networks (CNN), to assess the lithology and mineral content solely from analysis of drill core images, aiming to support and expedite the subsurface geological exploration. The paper outlines a comprehensive methodology, encompassing data preprocessing, machine learning methods, and transfer learning techniques. The outcome reveals a remarkable 96.7% accuracy in the classification of drill core segments into distinct formation classes. Furthermore, a CNN model was trained for the evaluation of mineral content using a learning data set from multidimensional log analysis data (silicate, total clay, carbonate). When benchmarked against laboratory XRD measurements on samples from the cores, both the advanced multidimensional log analysis model and the neural network approach developed here provide equally good performance. This work demonstrates that deep learning and particularly transfer learning can support extracting petrophysical properties, including mineral content and formation classification, from drill core images, thus offering a road map for enhancing model performance and data set quality in image-based analysis of drill cores.","sentences":["Deep subsurface exploration is important for mining, oil and gas industries, as well as in the assessment of geological units for the disposal of chemical or nuclear waste, or the viability of geothermal energy systems.","Typically, detailed examinations of subsurface formations or units are performed on cuttings or core materials extracted during drilling campaigns, as well as on geophysical borehole data, which provide detailed information about the petrophysical properties of the rocks.","Depending on the volume of rock samples and the analytical program, the laboratory analysis and diagnostics can be very time-consuming.","This study investigates the potential of utilizing machine learning, specifically convolutional neural networks (CNN), to assess the lithology and mineral content solely from analysis of drill core images, aiming to support and expedite the subsurface geological exploration.","The paper outlines a comprehensive methodology, encompassing data preprocessing, machine learning methods, and transfer learning techniques.","The outcome reveals a remarkable 96.7% accuracy in the classification of drill core segments into distinct formation classes.","Furthermore, a CNN model was trained for the evaluation of mineral content using a learning data set from multidimensional log analysis data (silicate, total clay, carbonate).","When benchmarked against laboratory XRD measurements on samples from the cores, both the advanced multidimensional log analysis model and the neural network approach developed here provide equally good performance.","This work demonstrates that deep learning and particularly transfer learning can support extracting petrophysical properties, including mineral content and formation classification, from drill core images, thus offering a road map for enhancing model performance and data set quality in image-based analysis of drill cores."],"url":"http://arxiv.org/abs/2403.18495v1","category":"cs.CV"}
{"created":"2024-03-27 12:01:31","title":"The Guesswork of Ordered Statistics Decoding: Complexity and Practical Design","abstract":"This paper investigates guesswork over ordered statistics and formulates the complexity of ordered statistics decoding (OSD) in binary additive white Gaussian noise (AWGN) channels. It first develops a new upper bound of guesswork for independent sequences, by applying the Holder's inequity to Hamming shell-based subspaces. This upper bound is then extended to the ordered statistics, by constructing the conditionally independent sequences within the ordered statistics sequences. We leverage the established bounds to formulate the best achievable decoding complexity of OSD that ensures no loss in error performance, where OSD stops immediately when the correct codeword estimate is found. We show that the average complexity of OSD at maximum decoding order can be accurately approximated by the modified Bessel function, which increases near-exponentially with code dimension. We also identify a complexity saturation threshold, where increasing the OSD decoding order beyond this threshold improves error performance without further raising decoding complexity. Finally, the paper presents insights on applying these findings to enhance the efficiency of practical decoder implementations.","sentences":["This paper investigates guesswork over ordered statistics and formulates the complexity of ordered statistics decoding (OSD) in binary additive white Gaussian noise (AWGN) channels.","It first develops a new upper bound of guesswork for independent sequences, by applying the Holder's inequity to Hamming shell-based subspaces.","This upper bound is then extended to the ordered statistics, by constructing the conditionally independent sequences within the ordered statistics sequences.","We leverage the established bounds to formulate the best achievable decoding complexity of OSD that ensures no loss in error performance, where OSD stops immediately when the correct codeword estimate is found.","We show that the average complexity of OSD at maximum decoding order can be accurately approximated by the modified Bessel function, which increases near-exponentially with code dimension.","We also identify a complexity saturation threshold, where increasing the OSD decoding order beyond this threshold improves error performance without further raising decoding complexity.","Finally, the paper presents insights on applying these findings to enhance the efficiency of practical decoder implementations."],"url":"http://arxiv.org/abs/2403.18488v1","category":"cs.IT"}
{"created":"2024-03-27 11:52:19","title":"On the properties of free floating planets originating in circumbinary planetary systems","abstract":"Free-floating planets are a new class of planets recently discovered. These planets don't orbit within stellar systems, instead living a nomadic life within the galaxy. How such objects formed remains elusive. Numerous works have explored mechanisms to form such objects, but have not yet provided predictions on their distributions that could differentiate between formation mechanisms. In this work we form these objects within circumbinary systems, where these planets are readily formed and ejected through interactions with the central binary stars. We find significant differences between planets ejected through planet-planet interactions and those by the binary stars. The main differences that arise are in the distributions of excess velocity, where binary stars eject planets with faster velocities. These differences should be observable amongst known free-floating planets in nearby star-forming regions. We predict that targeted observations of directly imaged free-floating planets in these regions should be able to determine their preferred formation pathway, either by planet formation in single or multiple stellar systems, or through processes akin to star formation. Additionally the mass distributions of free-floating planets can yield important insights into the underlying planet populations. We find that for planets more massive than 20 $\\rm M_{\\oplus}$, their frequencies are similar to those planets remaining bound and orbiting near the central binaries. This similarity allows for effective and informative comparisons between mass distributions from microlensing surveys, to those of transit and radial velocities. Ultimately, by observing the velocity dispersion and mass distribution of free-floating planets, it will be possible to effectively compare with predictions from planet formation models, and to further understand the formation and evolution of these exotic worlds.","sentences":["Free-floating planets are a new class of planets recently discovered.","These planets don't orbit within stellar systems, instead living a nomadic life within the galaxy.","How such objects formed remains elusive.","Numerous works have explored mechanisms to form such objects, but have not yet provided predictions on their distributions that could differentiate between formation mechanisms.","In this work we form these objects within circumbinary systems, where these planets are readily formed and ejected through interactions with the central binary stars.","We find significant differences between planets ejected through planet-planet interactions and those by the binary stars.","The main differences that arise are in the distributions of excess velocity, where binary stars eject planets with faster velocities.","These differences should be observable amongst known free-floating planets in nearby star-forming regions.","We predict that targeted observations of directly imaged free-floating planets in these regions should be able to determine their preferred formation pathway, either by planet formation in single or multiple stellar systems, or through processes akin to star formation.","Additionally the mass distributions of free-floating planets can yield important insights into the underlying planet populations.","We find that for planets more massive than 20 $\\rm M_{\\oplus}$, their frequencies are similar to those planets remaining bound and orbiting near the central binaries.","This similarity allows for effective and informative comparisons between mass distributions from microlensing surveys, to those of transit and radial velocities.","Ultimately, by observing the velocity dispersion and mass distribution of free-floating planets, it will be possible to effectively compare with predictions from planet formation models, and to further understand the formation and evolution of these exotic worlds."],"url":"http://arxiv.org/abs/2403.18481v1","category":"astro-ph.EP"}
{"created":"2024-03-27 11:48:45","title":"Thermalization condition for non-Hermitian quantum systems","abstract":"The application of the eigenstate thermalization hypothesis to non-Hermitian quantum systems has become one of the most important topics in dissipative quantum chaos, recently giving rise to intense debates. The process of thermalization is intricate, involving many time-evolution trajectories in the reduced Hilbert space of the system. By considering two different expansion forms of the density matrices adopted in the biorthogonal and right-state time evolutions, we have derived two versions of the Gorini-Kossakowski-Sudarshan-Lindblad master equations describing the non-Hermitian systems coupled to a bosonic heat bath in thermal equilibrium. By solving the equations, we have identified a sufficient condition for thermalization under both time evolutions, resulting in Boltzmann biorthogonal and right-eigenstate statistics, respectively. This finding implies that the recently proposed biorthogonal random matrix theory needs an appropriate revision. Moreover, we have exemplified the precise dynamics of thermalization and thermodynamic properties with test models.","sentences":["The application of the eigenstate thermalization hypothesis to non-Hermitian quantum systems has become one of the most important topics in dissipative quantum chaos, recently giving rise to intense debates.","The process of thermalization is intricate, involving many time-evolution trajectories in the reduced Hilbert space of the system.","By considering two different expansion forms of the density matrices adopted in the biorthogonal and right-state time evolutions, we have derived two versions of the Gorini-Kossakowski-Sudarshan-Lindblad master equations describing the non-Hermitian systems coupled to a bosonic heat bath in thermal equilibrium.","By solving the equations, we have identified a sufficient condition for thermalization under both time evolutions, resulting in Boltzmann biorthogonal and right-eigenstate statistics, respectively.","This finding implies that the recently proposed biorthogonal random matrix theory needs an appropriate revision.","Moreover, we have exemplified the precise dynamics of thermalization and thermodynamic properties with test models."],"url":"http://arxiv.org/abs/2403.18477v1","category":"quant-ph"}
{"created":"2024-03-27 11:44:49","title":"Record cryogenic cooling in ferroelectric hafnia proximity induced via Mott transition","abstract":"On-chip refrigeration at cryogenic temperatures is becoming an important requirement in the context of quantum technologies and nanoelectronics. Ferroic materials with enhanced electrocaloric effects at phase transitions are good material candidates for the same. By exploiting the Mott metal-insulator transition (MIT) of TiOx(Ny), the bottom electrode, we engineer a depolarization field controlled reversible polar to non-polar phase transition in thick La-doped hafnia (40 nm). This transition occurs between ~125 and 140 K and produces giant negative pyroelectric and electrocaloric effects. Refrigeration metrics were estimated between 120 to 200 K, with a peak refrigerant capacity of 25 kJ Kg-1 (2 kJ Kg-1), peak isothermal entropy {\\Delta}S~ 8 kJ Kg-1 K-1 (0.5 kJ Kg-1 K-1) and adiabatic {\\Delta}Tcooling ~ 106 K (11 K) at ~140 K and 5 MV cm-1 (0.5 MV cm-1, and these are the largest reported in any electrocaloric system. Our work fundamentally proposes design guidelines to induce significant solid-state refrigeration through proximity effects, even at cryogenic temperatures relevant to quantum technologies.","sentences":["On-chip refrigeration at cryogenic temperatures is becoming an important requirement in the context of quantum technologies and nanoelectronics.","Ferroic materials with enhanced electrocaloric effects at phase transitions are good material candidates for the same.","By exploiting the Mott metal-insulator transition (MIT) of TiOx(Ny), the bottom electrode, we engineer a depolarization field controlled reversible polar to non-polar phase transition in thick La-doped hafnia (40 nm).","This transition occurs between ~125 and 140 K and produces giant negative pyroelectric and electrocaloric effects.","Refrigeration metrics were estimated between 120 to 200 K, with a peak refrigerant capacity of 25 kJ Kg-1 (2 kJ Kg-1), peak isothermal entropy {\\Delta}S~ 8 kJ Kg-1 K-1 (0.5 kJ Kg-1 K-1) and adiabatic {\\Delta}Tcooling ~ 106 K (11 K) at ~140 K and 5 MV cm-1 (0.5 MV cm-1, and these are the largest reported in any electrocaloric system.","Our work fundamentally proposes design guidelines to induce significant solid-state refrigeration through proximity effects, even at cryogenic temperatures relevant to quantum technologies."],"url":"http://arxiv.org/abs/2403.18475v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 11:43:13","title":"Fermi and eROSITA Bubbles as Persistent Structures of The Milky Way Galaxy","abstract":"The Fermi and eROSITA bubbles, large diffuse structures in our Galaxy, can be the by-products of the steady star formation activity. To simultaneously explain the star formation history of the Milky Way and the metallicity of $\\sim$ Z$_\\odot$ at the Galactic disk, a steady Galactic wind driven by cosmic-rays is required. For tenuous gases with a density of $\\lesssim$10$^{-3}$ cm$^{-3}$, the cosmic-ray heating dominates over radiative cooling, and the gas can maintain the virial temperature of $\\sim$0.3 keV ideal for escape from the Galactic system as the wind. A part of the wind falls back onto the disk like a galactic fountain flow. We model the wind dynamics according to the Galactic evolution scenario and find that the scale height and surface brightness of the X-ray and the hadronic gamma-ray emissions from such fountain flow region can be consistent with the observed properties of the Fermi and eROSITA bubbles. This implies that the bubbles are persistent structures of the Milky Way existing over (at least) the last $\\sim$1 Gyr, rather than evanescent structures formed by non-trivial, $\\sim$10 Myr past Galactic Center transient activities.","sentences":["The Fermi and eROSITA bubbles, large diffuse structures in our Galaxy, can be the by-products of the steady star formation activity.","To simultaneously explain the star formation history of the Milky Way and the metallicity of $\\sim$ Z$_\\odot$ at the Galactic disk, a steady Galactic wind driven by cosmic-rays is required.","For tenuous gases with a density of $\\lesssim$10$^{-3}$ cm$^{-3}$, the cosmic-ray heating dominates over radiative cooling, and the gas can maintain the virial temperature of $\\sim$0.3 keV ideal for escape from the Galactic system as the wind.","A part of the wind falls back onto the disk like a galactic fountain flow.","We model the wind dynamics according to the Galactic evolution scenario and find that the scale height and surface brightness of the X-ray and the hadronic gamma-ray emissions from such fountain flow region can be consistent with the observed properties of the Fermi and eROSITA bubbles.","This implies that the bubbles are persistent structures of the Milky Way existing over (at least) the last $\\sim$1 Gyr, rather than evanescent structures formed by non-trivial, $\\sim$10 Myr past Galactic Center transient activities."],"url":"http://arxiv.org/abs/2403.18474v1","category":"astro-ph.HE"}
{"created":"2024-03-27 11:20:55","title":"Affine Weyl groups and non-Abelian discrete systems: an application to the $d$-Painlev\u00e9 equations","abstract":"A non-abelian generalisation of a birational representation of affine Weyl groups and their application to the discrete dynamical systems is presented. By using this generalisation, non-commutative analogs for the discrete systems of $A_n^{(1)}$, $n \\geq 2$ type and of $d$-Painlev\\'e equations with an additive dynamic were derived. A coalescence cascade of the later is also discussed.","sentences":["A non-abelian generalisation of a birational representation of affine Weyl groups and their application to the discrete dynamical systems is presented.","By using this generalisation, non-commutative analogs for the discrete systems of $A_n^{(1)}$, $n \\geq 2$ type and of $d$-Painlev\\'e equations with an additive dynamic were derived.","A coalescence cascade of the later is also discussed."],"url":"http://arxiv.org/abs/2403.18463v1","category":"nlin.SI"}
{"created":"2024-03-27 11:20:48","title":"Decoy Effect In Search Interaction: Understanding User Behavior and Measuring System Vulnerability","abstract":"This study examines the decoy effect's underexplored influence on user search interactions and methods for measuring information retrieval (IR) systems' vulnerability to this effect. It explores how decoy results alter users' interactions on search engine result pages, focusing on metrics like click-through likelihood, browsing time, and perceived document usefulness. By analyzing user interaction logs from multiple datasets, the study demonstrates that decoy results significantly affect users' behavior and perceptions. Furthermore, it investigates how different levels of task difficulty and user knowledge modify the decoy effect's impact, finding that easier tasks and lower knowledge levels lead to higher engagement with target documents. In terms of IR system evaluation, the study introduces the DEJA-VU metric to assess systems' susceptibility to the decoy effect, testing it on specific retrieval tasks. The results show differences in systems' effectiveness and vulnerability, contributing to our understanding of cognitive biases in search behavior and suggesting pathways for creating more balanced and bias-aware IR evaluations.","sentences":["This study examines the decoy effect's underexplored influence on user search interactions and methods for measuring information retrieval (IR) systems' vulnerability to this effect.","It explores how decoy results alter users' interactions on search engine result pages, focusing on metrics like click-through likelihood, browsing time, and perceived document usefulness.","By analyzing user interaction logs from multiple datasets, the study demonstrates that decoy results significantly affect users' behavior and perceptions.","Furthermore, it investigates how different levels of task difficulty and user knowledge modify the decoy effect's impact, finding that easier tasks and lower knowledge levels lead to higher engagement with target documents.","In terms of IR system evaluation, the study introduces the DEJA-VU metric to assess systems' susceptibility to the decoy effect, testing it on specific retrieval tasks.","The results show differences in systems' effectiveness and vulnerability, contributing to our understanding of cognitive biases in search behavior and suggesting pathways for creating more balanced and bias-aware IR evaluations."],"url":"http://arxiv.org/abs/2403.18462v1","category":"cs.IR"}
{"created":"2024-03-27 11:17:51","title":"Dust Extinction Measures for $z\\sim 8$ Galaxies using Machine Learning on JWST Imaging","abstract":"We present the results of a machine learning study to measure the dust content of galaxies observed with JWST at z > 6 through the use of trained neural networks based on high-resolution IllustrisTNG simulations. Dust is an important unknown in the evolution and observability of distant galaxies and is degenerate with other stellar population features through spectral energy fitting. As such, we develop and test a new SED-independent machine learning method to predict dust attenuation and sSFR of high redshift (z > 6) galaxies. Simulated galaxies were constructed using the IllustrisTNG model, with a variety of dust contents parameterized by E(B-V) and A(V) values, then used to train Convolutional Neural Network (CNN) models using supervised learning through a regression model. We demonstrate that within the context of these simulations, our single and multi-band models are able to predict dust content of distant galaxies to within a 1$\\sigma$ dispersion of A(V) $\\sim 0.1$. Applied to spectroscopically confirmed z > 6 galaxies from the JADES and CEERS programs, our models predicted attenuation values of A(V) < 0.7 for all systems, with a low average (A(V) = 0.28). Our CNN predictions show larger dust attenuation but lower amounts of star formation compared to SED fitted values. Both results show that distant galaxies with confirmed spectroscopy are not extremely dusty, although this sample is potentially significantly biased. We discuss these issues and present ideas on how to accurately measure dust features at the highest redshifts using a combination of machine learning and SED fitting.","sentences":["We present the results of a machine learning study to measure the dust content of galaxies observed with JWST at z > 6 through the use of trained neural networks based on high-resolution IllustrisTNG simulations.","Dust is an important unknown in the evolution and observability of distant galaxies and is degenerate with other stellar population features through spectral energy fitting.","As such, we develop and test a new SED-independent machine learning method to predict dust attenuation and sSFR of high redshift (z > 6) galaxies.","Simulated galaxies were constructed using the IllustrisTNG model, with a variety of dust contents parameterized by E(B-V) and A(V) values, then used to train Convolutional Neural Network (CNN) models using supervised learning through a regression model.","We demonstrate that within the context of these simulations, our single and multi-band models are able to predict dust content of distant galaxies to within a 1$\\sigma$ dispersion of A(V) $\\sim 0.1$. Applied to spectroscopically confirmed z > 6 galaxies from the JADES and CEERS programs, our models predicted attenuation values of A(V) < 0.7 for all systems, with a low average (A(V) = 0.28).","Our CNN predictions show larger dust attenuation but lower amounts of star formation compared to SED fitted values.","Both results show that distant galaxies with confirmed spectroscopy are not extremely dusty, although this sample is potentially significantly biased.","We discuss these issues and present ideas on how to accurately measure dust features at the highest redshifts using a combination of machine learning and SED fitting."],"url":"http://arxiv.org/abs/2403.18458v1","category":"astro-ph.GA"}
{"created":"2024-03-27 11:16:27","title":"Specificity of $\u03c4$ -- approximation for chaotic electron trajectories on complex Fermi surfaces","abstract":"The work examines a special behavior of the magnetic conductivity of metals that arises when chaotic electron trajectories appear on the Fermi surface. This behavior is due to the scattering of electrons at singular points of the dynamic system describing the dynamics of electrons in $\\, {\\bf p}$-space, and caused by small-angle scattering of electrons on phonons. In this situation, the electronic system is described by a \"non-standard\" relaxation time, which plays the main role in a certain range of temperature and magnetic field values.","sentences":["The work examines a special behavior of the magnetic conductivity of metals that arises when chaotic electron trajectories appear on the Fermi surface.","This behavior is due to the scattering of electrons at singular points of the dynamic system describing the dynamics of electrons in $\\, {\\bf p}$-space, and caused by small-angle scattering of electrons on phonons.","In this situation, the electronic system is described by a \"non-standard\" relaxation time, which plays the main role in a certain range of temperature and magnetic field values."],"url":"http://arxiv.org/abs/2403.18457v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-27 11:14:35","title":"Onset of penumbra formation","abstract":"Context. Fully fledged penumbrae have been widely studied both observationally and theoretically. Yet the relatively fast process of penumbra formation has not been studied closely with high spatial resolution. Aims. We investigate the stages previous to and during the formation of penumbral filaments in a developing sunspot. Methods. We analysed Milne-Eddington inversions from spectro-polarimetric data of the leading sunspot of NOAA 11024 during the development of its penumbra. We focused on selected areas of this protospot in which segments of penumbra develop. Results. We find that few types of distinctive flow patterns develop at the protospot limb and centre sides previous to penumbra formation. The flow in the centre side is often characterised by a persistent (>20 min) inflow-outflow pattern extending radially over 4 arcsec at the direct periphery of the protospot umbra. This inflow-outflow system often correlates with elongated granules, as seen in continuum intensity maps, and is also coupled with magnetic bipolar patches at its edges, as seen in magnetograms. The field is close to horizontal between the bipolar patches, which is indicative of its possible loop configuration. All of these aspects are analogous to observations of magnetic flux emergence. In the protospot limb side, however, we observed a mostly regular pattern associated with small granules located near the protospot intensity boundary. Locally, an inflow develops adjacent to an existing penumbral segment, and this inflow is correlated with a single bright penumbral filament that is brighter than filaments containing the Evershed flow. All investigated areas at the centre and limb side eventually develop penumbral filaments with an actual Evershed flow that starts at the umbral boundary and grows outwards radially as the penumbral filaments become longer in time","sentences":["Context.","Fully fledged penumbrae have been widely studied both observationally and theoretically.","Yet the relatively fast process of penumbra formation has not been studied closely with high spatial resolution.","Aims.","We investigate the stages previous to and during the formation of penumbral filaments in a developing sunspot.","Methods.","We analysed Milne-Eddington inversions from spectro-polarimetric data of the leading sunspot of NOAA 11024 during the development of its penumbra.","We focused on selected areas of this protospot in which segments of penumbra develop.","Results.","We find that few types of distinctive flow patterns develop at the protospot limb and centre sides previous to penumbra formation.","The flow in the centre side is often characterised by a persistent (>20 min) inflow-outflow pattern extending radially over 4 arcsec at the direct periphery of the protospot umbra.","This inflow-outflow system often correlates with elongated granules, as seen in continuum intensity maps, and is also coupled with magnetic bipolar patches at its edges, as seen in magnetograms.","The field is close to horizontal between the bipolar patches, which is indicative of its possible loop configuration.","All of these aspects are analogous to observations of magnetic flux emergence.","In the protospot limb side, however, we observed a mostly regular pattern associated with small granules located near the protospot intensity boundary.","Locally, an inflow develops adjacent to an existing penumbral segment, and this inflow is correlated with a single bright penumbral filament that is brighter than filaments containing the Evershed flow.","All investigated areas at the centre and limb side eventually develop penumbral filaments with an actual Evershed flow that starts at the umbral boundary and grows outwards radially as the penumbral filaments become longer in time"],"url":"http://arxiv.org/abs/2403.18455v1","category":"astro-ph.SR"}
{"created":"2024-03-27 10:50:24","title":"Backpropagation-free Network for 3D Test-time Adaptation","abstract":"Real-world systems often encounter new data over time, which leads to experiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods tend to apply computationally heavy and memory-intensive backpropagation-based approaches to handle this. Here, we propose a novel method that uses a backpropagation-free approach for TTA for the specific case of 3D data. Our model uses a two-stream architecture to maintain knowledge about the source domain as well as complementary target-domain-specific information. The backpropagation-free property of our model helps address the well-known forgetting problem and mitigates the error accumulation issue. The proposed method also eliminates the need for the usually noisy process of pseudo-labeling and reliance on costly self-supervised training. Moreover, our method leverages subspace learning, effectively reducing the distribution variance between the two domains. Furthermore, the source-domain-specific and the target-domain-specific streams are aligned using a novel entropy-based adaptive fusion strategy. Extensive experiments on popular benchmarks demonstrate the effectiveness of our method. The code will be available at https://github.com/abie-e/BFTT3D.","sentences":["Real-world systems often encounter new data over time, which leads to experiencing target domain shifts.","Existing Test-Time Adaptation (TTA) methods tend to apply computationally heavy and memory-intensive backpropagation-based approaches to handle this.","Here, we propose a novel method that uses a backpropagation-free approach for TTA for the specific case of 3D data.","Our model uses a two-stream architecture to maintain knowledge about the source domain as well as complementary target-domain-specific information.","The backpropagation-free property of our model helps address the well-known forgetting problem and mitigates the error accumulation issue.","The proposed method also eliminates the need for the usually noisy process of pseudo-labeling and reliance on costly self-supervised training.","Moreover, our method leverages subspace learning, effectively reducing the distribution variance between the two domains.","Furthermore, the source-domain-specific and the target-domain-specific streams are aligned using a novel entropy-based adaptive fusion strategy.","Extensive experiments on popular benchmarks demonstrate the effectiveness of our method.","The code will be available at https://github.com/abie-e/BFTT3D."],"url":"http://arxiv.org/abs/2403.18442v1","category":"cs.CV"}
{"created":"2024-03-27 10:45:16","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","abstract":"Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes. Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity. Motivated by the recent success of Transformer-based Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability. We investigate how the learned global representation of the atmosphere's state can be transferred to model the normalized difference vegetation index (NDVI). Our model globally estimates vegetation activity at a resolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI model from scratch. Additionally, we compare our results to other recent data-driven NDVI modeling approaches from machine learning and ecology literature. We further provide experimental evidence on how much data and training time is necessary to turn FourCastNet into an effective vegetation model. Code and models will be made available upon publication.","sentences":["Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes.","Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity.","Motivated by the recent success of Transformer-based Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability.","We investigate how the learned global representation of the atmosphere's state can be transferred to model the normalized difference vegetation index (NDVI).","Our model globally estimates vegetation activity at a resolution of \\SI{0.25}{\\degree} while relying only on meteorological data.","We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI model from scratch.","Additionally, we compare our results to other recent data-driven NDVI modeling approaches from machine learning and ecology literature.","We further provide experimental evidence on how much data and training time is necessary to turn FourCastNet into an effective vegetation model.","Code and models will be made available upon publication."],"url":"http://arxiv.org/abs/2403.18438v1","category":"cs.LG"}
{"created":"2024-03-27 10:28:57","title":"Fermion integrals for finite spectral triples","abstract":"Fermion functional integrals are calculated for the Dirac operator of a finite real spectral triple. Complex, real and chiral functional integrals are considered for each KO-dimension where they are non-trivial, and phase ambiguities in the definition are noted.","sentences":["Fermion functional integrals are calculated for the Dirac operator of a finite real spectral triple.","Complex, real and chiral functional integrals are considered for each KO-dimension where they are non-trivial, and phase ambiguities in the definition are noted."],"url":"http://arxiv.org/abs/2403.18428v1","category":"math-ph"}
{"created":"2024-03-27 10:21:07","title":"Feedback Linearizable Discretizations of Second Order Mechanical Systems using Retraction Maps","abstract":"Mechanical systems, in nature, are often described by a set of continuous-time, nonlinear, second-order differential equations (SODEs). This has motivated designs of various control laws implemented on digital controllers, consequently requiring numerical discretization schemes. Feedback linearizability of such sampled systems depends on the discretization scheme or map choice. In this article, we utilize retraction maps and their lifts to construct feedback linearizable discretizations for SODEs, which can be applied to various mechanical systems.","sentences":["Mechanical systems, in nature, are often described by a set of continuous-time, nonlinear, second-order differential equations (SODEs).","This has motivated designs of various control laws implemented on digital controllers, consequently requiring numerical discretization schemes.","Feedback linearizability of such sampled systems depends on the discretization scheme or map choice.","In this article, we utilize retraction maps and their lifts to construct feedback linearizable discretizations for SODEs, which can be applied to various mechanical systems."],"url":"http://arxiv.org/abs/2403.18422v1","category":"eess.SY"}
{"created":"2024-03-27 09:59:49","title":"Shear-induced diffusivity in supercooled liquids","abstract":"The Taylor-Aris theory of shear diffusion predicts that the effective diffusivity of a tracer molecule in a sheared liquid is enhanced by a term quadratic in the shear rate. In sheared supercooled liquids, instead, the observed enhancement is linear in the shear rate. This is a fundamental observation for the physics of nonequilibrium liquids for which no theory or fundamental understanding is available. We derive a formula for the effective molecular diffusivity in supercooled liquids under shear flow based on the underlying Smoluchowski equation with shear (Smoluchowski diffusion-convection equation) with an energy barrier representing a crowded energy landscape. The obtained formula correctly recovers the effective diffusivity with a correction term linear in the shear rate, in agreement with results from numerical simulations of different liquids as well as with earlier experimental results on shear melting of colloidal glass. The theory predictions are supported by comparisons with molecular simulations of supercooled water and supercooled Lennard-Jones liquids, which confirm that the predicted enhancement of diffusivity is inversely proportional to temperature and directly proportional to the zero shear viscosity.","sentences":["The Taylor-Aris theory of shear diffusion predicts that the effective diffusivity of a tracer molecule in a sheared liquid is enhanced by a term quadratic in the shear rate.","In sheared supercooled liquids, instead, the observed enhancement is linear in the shear rate.","This is a fundamental observation for the physics of nonequilibrium liquids for which no theory or fundamental understanding is available.","We derive a formula for the effective molecular diffusivity in supercooled liquids under shear flow based on the underlying Smoluchowski equation with shear (Smoluchowski diffusion-convection equation) with an energy barrier representing a crowded energy landscape.","The obtained formula correctly recovers the effective diffusivity with a correction term linear in the shear rate, in agreement with results from numerical simulations of different liquids as well as with earlier experimental results on shear melting of colloidal glass.","The theory predictions are supported by comparisons with molecular simulations of supercooled water and supercooled Lennard-Jones liquids, which confirm that the predicted enhancement of diffusivity is inversely proportional to temperature and directly proportional to the zero shear viscosity."],"url":"http://arxiv.org/abs/2403.18412v1","category":"cond-mat.soft"}
{"created":"2024-03-27 09:45:33","title":"FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs","abstract":"Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.","sentences":["Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task.","Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection.","Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work.","In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries.","In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language.","The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches.","To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database.","In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain.","And an automated method is devised to create semantic labels for extensive binary functions.","Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score.","FoC-Sim outperforms the previous best methods with a 52% higher Recall@1.","Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection."],"url":"http://arxiv.org/abs/2403.18403v1","category":"cs.CR"}
{"created":"2024-03-27 09:37:12","title":"Adaptive Economic Model Predictive Control for linear systems with performance guarantees","abstract":"We present a model predictive control (MPC) formulation to directly optimize economic criteria for linear constrained systems subject to disturbances and uncertain model parameters. The proposed formulation combines a certainty equivalent economic MPC with a simple least-squares parameter adaptation. For the resulting adaptive economic MPC scheme, we derive strong asymptotic and transient performance guarantees. We provide a numerical example involving building temperature control and demonstrate performance benefits of online parameter adaptation.","sentences":["We present a model predictive control (MPC) formulation to directly optimize economic criteria for linear constrained systems subject to disturbances and uncertain model parameters.","The proposed formulation combines a certainty equivalent economic MPC with a simple least-squares parameter adaptation.","For the resulting adaptive economic MPC scheme, we derive strong asymptotic and transient performance guarantees.","We provide a numerical example involving building temperature control and demonstrate performance benefits of online parameter adaptation."],"url":"http://arxiv.org/abs/2403.18398v1","category":"eess.SY"}
{"created":"2024-03-27 09:31:09","title":"Vertex corrections to conductivity in the Holstein model: A numerical-analytical study","abstract":"The optical-conductivity profile carries information on electronic dynamics in interacting quantum many-body systems. Its computation is a formidable task that is usually approached by invoking the single-particle (bubble) approximation and neglecting the vertex corrections. Their importance remains elusive even in model Hamiltonian calculations. Here, we combine analytical arguments with our recent breakthroughs in numerically exact and approximate calculations of finite-temperature real-time correlation functions to thoroughly assess the importance of vertex corrections in the one-dimensional Holstein polaron model. We find, both analytically and numerically, vanishing vertex corrections to optical conductivity in the limits of zero electron--phonon interaction, zero electronic bandwidth, and infinite temperature. Furthermore, our numerical results show that vertex corrections to the electron mobility also vanish in many parameter regimes between these limits. In some of these cases, the vertex corrections still introduce important qualitative changes to the optical-conductivity profile in comparison to the bubble approximation even though the self-energy remains approximately local. We trace these changes back to the bubble approximation not fully capturing a time-limited slow-down of the electron on intermediate time scales between ballistic and diffusive transport. We find that the vertex corrections are overall most pronounced for intermediate electron--phonon interaction and may increase or decrease the bubble-approximation mobility depending on the values of model parameters.","sentences":["The optical-conductivity profile carries information on electronic dynamics in interacting quantum many-body systems.","Its computation is a formidable task that is usually approached by invoking the single-particle (bubble) approximation and neglecting the vertex corrections.","Their importance remains elusive even in model Hamiltonian calculations.","Here, we combine analytical arguments with our recent breakthroughs in numerically exact and approximate calculations of finite-temperature real-time correlation functions to thoroughly assess the importance of vertex corrections in the one-dimensional Holstein polaron model.","We find, both analytically and numerically, vanishing vertex corrections to optical conductivity in the limits of zero electron--phonon interaction, zero electronic bandwidth, and infinite temperature.","Furthermore, our numerical results show that vertex corrections to the electron mobility also vanish in many parameter regimes between these limits.","In some of these cases, the vertex corrections still introduce important qualitative changes to the optical-conductivity profile in comparison to the bubble approximation even though the self-energy remains approximately local.","We trace these changes back to the bubble approximation not fully capturing a time-limited slow-down of the electron on intermediate time scales between ballistic and diffusive transport.","We find that the vertex corrections are overall most pronounced for intermediate electron--phonon interaction and may increase or decrease the bubble-approximation mobility depending on the values of model parameters."],"url":"http://arxiv.org/abs/2403.18394v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 09:24:38","title":"Morrey-Lorentz estimates for Hodge-type systems","abstract":"We prove up to the boundary regularity estimates in Morrey-Lorentz spaces for weak solutions of the linear system of differential forms with regular anisotropic coefficients   \\begin{equation*}   d^{\\ast} \\left( A d\\omega \\right) + B^{\\intercal}d d^{\\ast} \\left( B\\omega \\right) = \\lambda B\\omega + f \\text{ in } \\Omega,   \\end{equation*}   with either $ \\nu\\wedge \\omega$ and $\\nu\\wedge d^{\\ast} \\left( B\\omega \\right)$ or $\\nu\\lrcorner B\\omega$ and   $\\nu\\lrcorner \\left( A d\\omega \\right)$ prescribed on $\\partial\\Omega.$ We derive these estimates from the $L^{p}$ estimates obtained in \\cite{Sil_linearregularity} in the spirit of Campanato's method. Unlike Lorentz spaces, Morrey spaces are neither interpolation spaces nor rearrangement invariant. So Morrey estimates can not be obtained directly from the $L^{p}$ estimates using interpolation. We instead adapt an idea of Lieberman \\cite{Lieberman_morrey_from_Lp} to our setting to derive the estimates. Applications to Hodge decomposition in Morrey-Lorentz spaces, Gaffney type inequalities and estimates for related systems such as Hodge-Maxwell systems and `div-curl' systems are discussed.","sentences":["We prove up to the boundary regularity estimates in Morrey-Lorentz spaces for weak solutions of the linear system of differential forms with regular anisotropic coefficients   \\begin{equation*}   d^{\\ast} \\left( A d\\omega \\right) + B^{\\intercal}d d^{\\ast} \\left( B\\omega \\right) = \\lambda B\\omega +","f \\text{ in } \\Omega,   \\end{equation*}   with either $ \\nu\\wedge \\omega$ and $\\nu\\wedge d^{\\ast} \\left( B\\omega \\right)$ or $\\nu\\lrcorner B\\omega$ and   ","$\\nu\\lrcorner \\left( A d\\omega \\right)$ prescribed on $\\partial\\Omega.$ We derive these estimates from the $L^{p}$ estimates obtained in \\cite{Sil_linearregularity} in the spirit of Campanato's method.","Unlike Lorentz spaces, Morrey spaces are neither interpolation spaces nor rearrangement invariant.","So Morrey estimates can not be obtained directly from the $L^{p}$ estimates using interpolation.","We instead adapt an idea of Lieberman \\cite{Lieberman_morrey_from_Lp} to our setting to derive the estimates.","Applications to Hodge decomposition in Morrey-Lorentz spaces, Gaffney type inequalities and estimates for related systems such as Hodge-Maxwell systems and `div-curl' systems are discussed."],"url":"http://arxiv.org/abs/2403.18387v1","category":"math.AP"}
{"created":"2024-03-27 09:22:02","title":"Distributed Feedback Optimization of Linear Multi-agent Systems","abstract":"Feedback optimization is an increasingly popular control paradigm to optimize dynamical systems, accounting for control objectives that concern the system's operation at steady-state. Existing feedback optimization techniques heavily rely on centralized system and controller architectures, and thus suffer from scalability and privacy issues when systems become large-scale. In this paper, we propose and study a distributed architecture for feedback optimization, in which each agent updates its local control state by combining the average of its neighbors with a local negative-gradient step. Under convexity and smoothness assumptions, we establish convergence of the control method to a fixed point. By reinforcing the assumptions to restricted strong convexity of the cost, we show that our algorithm converges linearly to a neighborhood of the optimal point, where the size of the neighborhood depends on the choice of the stepsize. Simulations corroborate the theoretical results.","sentences":["Feedback optimization is an increasingly popular control paradigm to optimize dynamical systems, accounting for control objectives that concern the system's operation at steady-state.","Existing feedback optimization techniques heavily rely on centralized system and controller architectures, and thus suffer from scalability and privacy issues when systems become large-scale.","In this paper, we propose and study a distributed architecture for feedback optimization, in which each agent updates its local control state by combining the average of its neighbors with a local negative-gradient step.","Under convexity and smoothness assumptions, we establish convergence of the control method to a fixed point.","By reinforcing the assumptions to restricted strong convexity of the cost, we show that our algorithm converges linearly to a neighborhood of the optimal point, where the size of the neighborhood depends on the choice of the stepsize.","Simulations corroborate the theoretical results."],"url":"http://arxiv.org/abs/2403.18386v1","category":"math.OC"}
{"created":"2024-03-27 09:21:50","title":"Global solution of 2D hyperbolic liquid crystal system for small initial data","abstract":"We prove the global stability of small perturbation near the the constant equilibrium for the two dimensional simplified Ericksen-Leslie's hyperbolic system for incompressible liquid crystal model, where the direction function of liquid crystal molecules satisfies a wave map equation with an acoustical metric. This improves the almost global existence result by Huang-Jiang-Zhao. As byproducts, we obtain the sharp (same as the linear solution) decay estimates for the nonlinear velocity and the nonlinear wave part. Moreover the nonlinear wave part scatters to a linear solution as time goes to infinity.   The main novelty of this paper is that we uncover a null structure inside the velocity equation on the Fourier side for the nonlinear interaction between nonlinear heat equation and nonlinear wave equation.","sentences":["We prove the global stability of small perturbation near the the constant equilibrium for the two dimensional simplified Ericksen-Leslie's hyperbolic system for incompressible liquid crystal model, where the direction function of liquid crystal molecules satisfies a wave map equation with an acoustical metric.","This improves the almost global existence result by Huang-Jiang-Zhao.","As byproducts, we obtain the sharp (same as the linear solution) decay estimates for the nonlinear velocity and the nonlinear wave part.","Moreover the nonlinear wave part scatters to a linear solution as time goes to infinity.   ","The main novelty of this paper is that we uncover a null structure inside the velocity equation on the Fourier side for the nonlinear interaction between nonlinear heat equation and nonlinear wave equation."],"url":"http://arxiv.org/abs/2403.18385v1","category":"math.AP"}
{"created":"2024-03-27 09:15:45","title":"Extensible Hook System for Rendesvouz and Docking of a Cubesat Swarm","abstract":"The use of cubesat swarms is being proposed for different missions where cooperation between satellites is required. Commonly, the cube swarm requires formation flight and even rendezvous and docking, which are very challenging tasks since they required more energy and the use of advanced guidance, navigation and control techniques. In this paper, we propose the use of an extensible hook system to mitigate these drawbacks,i.e. it allows to save fuel and reduce the system complexity by including techniques that have been previously demonstrated on Earth. This system is based on a scissor boom structure, which could reach up to five meters for a 4U dimension, including three degrees of freedom to place the end effector at any pose within the system workspace. We simulated the dynamic behaviour of a cubesat with the proposed system, demonstrating the required power for a 16U cubesat equipped with one extensible hook system is considered acceptable according to the current state of the art actuators.","sentences":["The use of cubesat swarms is being proposed for different missions where cooperation between satellites is required.","Commonly, the cube swarm requires formation flight and even rendezvous and docking, which are very challenging tasks since they required more energy and the use of advanced guidance, navigation and control techniques.","In this paper, we propose the use of an extensible hook system to mitigate these drawbacks,i.e. it allows to save fuel and reduce the system complexity by including techniques that have been previously demonstrated on Earth.","This system is based on a scissor boom structure, which could reach up to five meters for a 4U dimension, including three degrees of freedom to place the end effector at any pose within the system workspace.","We simulated the dynamic behaviour of a cubesat with the proposed system, demonstrating the required power for a 16U cubesat equipped with one extensible hook system is considered acceptable according to the current state of the art actuators."],"url":"http://arxiv.org/abs/2403.18376v1","category":"cs.RO"}
{"created":"2024-03-27 09:12:35","title":"Optimizing Communication for Latency Sensitive HPC Applications on up to 48 FPGAs Using ACCL","abstract":"Most FPGA boards in the HPC domain are well-suited for parallel scaling because of the direct integration of versatile and high-throughput network ports. However, the utilization of their network capabilities is often challenging and error-prone because the whole network stack and communication patterns have to be implemented and managed on the FPGAs. Also, this approach conceptually involves a trade-off between the performance potential of improved communication and the impact of resource consumption for communication infrastructure, since the utilized resources on the FPGAs could otherwise be used for computations. In this work, we investigate this trade-off, firstly, by using synthetic benchmarks to evaluate the different configuration options of the communication framework ACCL and their impact on communication latency and throughput. Finally, we use our findings to implement a shallow water simulation whose scalability heavily depends on low-latency communication. With a suitable configuration of ACCL, good scaling behavior can be shown to all 48 FPGAs installed in the system. Overall, the results show that the availability of inter-FPGA communication frameworks as well as the configurability of framework and network stack are crucial to achieve the best application performance with low latency communication.","sentences":["Most FPGA boards in the HPC domain are well-suited for parallel scaling because of the direct integration of versatile and high-throughput network ports.","However, the utilization of their network capabilities is often challenging and error-prone because the whole network stack and communication patterns have to be implemented and managed on the FPGAs.","Also, this approach conceptually involves a trade-off between the performance potential of improved communication and the impact of resource consumption for communication infrastructure, since the utilized resources on the FPGAs could otherwise be used for computations.","In this work, we investigate this trade-off, firstly, by using synthetic benchmarks to evaluate the different configuration options of the communication framework ACCL and their impact on communication latency and throughput.","Finally, we use our findings to implement a shallow water simulation whose scalability heavily depends on low-latency communication.","With a suitable configuration of ACCL, good scaling behavior can be shown to all 48 FPGAs installed in the system.","Overall, the results show that the availability of inter-FPGA communication frameworks as well as the configurability of framework and network stack are crucial to achieve the best application performance with low latency communication."],"url":"http://arxiv.org/abs/2403.18374v1","category":"cs.DC"}
{"created":"2024-03-27 09:10:01","title":"BAM: Box Abstraction Monitors for Real-time OoD Detection in Object Detection","abstract":"Out-of-distribution (OoD) detection techniques for deep neural networks (DNNs) become crucial thanks to their filtering of abnormal inputs, especially when DNNs are used in safety-critical applications and interact with an open and dynamic environment. Nevertheless, integrating OoD detection into state-of-the-art (SOTA) object detection DNNs poses significant challenges, partly due to the complexity introduced by the SOTA OoD construction methods, which require the modification of DNN architecture and the introduction of complex loss functions. This paper proposes a simple, yet surprisingly effective, method that requires neither retraining nor architectural change in object detection DNN, called Box Abstraction-based Monitors (BAM). The novelty of BAM stems from using a finite union of convex box abstractions to capture the learned features of objects for in-distribution (ID) data, and an important observation that features from OoD data are more likely to fall outside of these boxes. The union of convex regions within the feature space allows the formation of non-convex and interpretable decision boundaries, overcoming the limitations of VOS-like detectors without sacrificing real-time performance. Experiments integrating BAM into Faster R-CNN-based object detection DNNs demonstrate a considerably improved performance against SOTA OoD detection techniques.","sentences":["Out-of-distribution (OoD) detection techniques for deep neural networks (DNNs) become crucial thanks to their filtering of abnormal inputs, especially when DNNs are used in safety-critical applications and interact with an open and dynamic environment.","Nevertheless, integrating OoD detection into state-of-the-art (SOTA) object detection DNNs poses significant challenges, partly due to the complexity introduced by the SOTA OoD construction methods, which require the modification of DNN architecture and the introduction of complex loss functions.","This paper proposes a simple, yet surprisingly effective, method that requires neither retraining nor architectural change in object detection DNN, called Box Abstraction-based Monitors (BAM).","The novelty of BAM stems from using a finite union of convex box abstractions to capture the learned features of objects for in-distribution (ID) data, and an important observation that features from OoD data are more likely to fall outside of these boxes.","The union of convex regions within the feature space allows the formation of non-convex and interpretable decision boundaries, overcoming the limitations of VOS-like detectors without sacrificing real-time performance.","Experiments integrating BAM into Faster R-CNN-based object detection DNNs demonstrate a considerably improved performance against SOTA OoD detection techniques."],"url":"http://arxiv.org/abs/2403.18373v1","category":"cs.CV"}
{"created":"2024-03-27 09:08:06","title":"Multivariable control of modular multilevel converters with convergence and safety guarantees","abstract":"Well-designed current control is a key factor in ensuring the efficient and safe operation of modular multilevel converters (MMCs). Even though this control problem involves multiple control objectives, conventional current control schemes are comprised of independently designed decoupled controllers, e.g., proportional-integral (PI) or proportional-resonant (PR). Due to the bilinearity of the MMC dynamics, tuning PI and PR controllers so that good performance and constraint satisfaction are guaranteed is quite challenging. This challenge becomes more relevant in an AC/AC MMC configuration due to the complexity of tracking the single-phase sinusoidal components of the MMC output. In this paper, we propose a method to design a multivariable controller, i.e., a static feedback gain, to regulate the MMC currents. We use a physics-informed transformation to model the MMC dynamics linearly and synthesise the proposed controller. We use this linear model to formulate a linear matrix inequality that computes a feedback gain that guarantees safe and effective operation, including (i) limited tracking error, (ii) stability, and (iii) meeting all constraints. To test the efficacy of our method, we examine its performance in a direct AC/AC MMC simulated in Simulink/PLECS and in a scaled-down AC/AC MMC prototype to investigate the ultra-fast charging of electric vehicles.","sentences":["Well-designed current control is a key factor in ensuring the efficient and safe operation of modular multilevel converters (MMCs).","Even though this control problem involves multiple control objectives, conventional current control schemes are comprised of independently designed decoupled controllers, e.g., proportional-integral (PI) or proportional-resonant (PR).","Due to the bilinearity of the MMC dynamics, tuning PI and PR controllers so that good performance and constraint satisfaction are guaranteed is quite challenging.","This challenge becomes more relevant in an AC/AC MMC configuration due to the complexity of tracking the single-phase sinusoidal components of the MMC output.","In this paper, we propose a method to design a multivariable controller, i.e., a static feedback gain, to regulate the MMC currents.","We use a physics-informed transformation to model the MMC dynamics linearly and synthesise the proposed controller.","We use this linear model to formulate a linear matrix inequality that computes a feedback gain that guarantees safe and effective operation, including (i) limited tracking error, (ii) stability, and (iii) meeting all constraints.","To test the efficacy of our method, we examine its performance in a direct AC/AC MMC simulated in Simulink/PLECS and in a scaled-down AC/AC MMC prototype to investigate the ultra-fast charging of electric vehicles."],"url":"http://arxiv.org/abs/2403.18371v1","category":"eess.SY"}
{"created":"2024-03-27 08:55:08","title":"Fractional variational integrators based on convolution quadrature","abstract":"Fractional dissipation is a powerful tool to study non-local physical phenomena such as damping models. The design of geometric, in particular, variational integrators for the numerical simulation of such systems relies on a variational formulation of the model. In [19], a new approach is proposed to deal with dissipative systems including fractionally damped systems in a variational way for both, the continuous and discrete setting. It is based on the doubling of variables and their fractional derivatives. The aim of this work is to derive higher-order fractional variational integrators by means of convolution quadrature (CQ) based on backward difference formulas. We then provide numerical methods that are of order 2 improving a previous result in [19]. The convergence properties of the fractional variational integrators and saturation effects due to the approximation of the fractional derivatives by CQ are studied numerically.","sentences":["Fractional dissipation is a powerful tool to study non-local physical phenomena such as damping models.","The design of geometric, in particular, variational integrators for the numerical simulation of such systems relies on a variational formulation of the model.","In [19], a new approach is proposed to deal with dissipative systems including fractionally damped systems in a variational way for both, the continuous and discrete setting.","It is based on the doubling of variables and their fractional derivatives.","The aim of this work is to derive higher-order fractional variational integrators by means of convolution quadrature (CQ) based on backward difference formulas.","We then provide numerical methods that are of order 2 improving a previous result in [19].","The convergence properties of the fractional variational integrators and saturation effects due to the approximation of the fractional derivatives by CQ are studied numerically."],"url":"http://arxiv.org/abs/2403.18362v1","category":"math.NA"}
{"created":"2024-03-27 08:52:05","title":"Dynamical properties of magnetized low angular momentum accretion flow around a Kerr black hole","abstract":"An essential factor in determining the flow characteristics of an accretion flow is its angular momentum. According to the angular momentum of the flow, semi-analytical analysis suggests various types of accretion solutions. It is critical to test it with numerical simulations using the most advanced framework available (general relativistic magnetohydrodynamics) to understand how flow changes with different angular momentum. By changing the initial condition of the accretion torus minimally, we can simulate steady, low angular momentum accretion flow around a Kerr black hole. We focus primarily on the lower limits of angular momentum and come upon that accretion flow with an intermediate range of angular momentum differs significantly from high or very low angular momentum flow. The intermediate angular momentum accretion flow has the highest density, pressure, and temperature near the black hole, making it easier to observe. We find that the density and pressure have power-law scalings $\\rho\\propto r^{n-3/2}$ and $p_g\\propto r^{n-5/2}$ which only hold for very low angular momentum cases. With the increase in flow angular momentum, it develops a non-axisymmetric nature. In this case, simple self-similarity does not hold. We also find that the sonic surface moves away from the innermost stable circular orbit as its angular momentum decreases. Finally, we emphasize that intermediate angular momentum flow could provide a possible solution to explain the complex observation features of the supermassive black hole Sgr~A$^*$ at our galactic center.","sentences":["An essential factor in determining the flow characteristics of an accretion flow is its angular momentum.","According to the angular momentum of the flow, semi-analytical analysis suggests various types of accretion solutions.","It is critical to test it with numerical simulations using the most advanced framework available (general relativistic magnetohydrodynamics) to understand how flow changes with different angular momentum.","By changing the initial condition of the accretion torus minimally, we can simulate steady, low angular momentum accretion flow around a Kerr black hole.","We focus primarily on the lower limits of angular momentum and come upon that accretion flow with an intermediate range of angular momentum differs significantly from high or very low angular momentum flow.","The intermediate angular momentum accretion flow has the highest density, pressure, and temperature near the black hole, making it easier to observe.","We find that the density and pressure have power-law scalings $\\rho\\propto r^{n-3/2}$ and $p_g\\propto r^{n-5/2}$ which only hold for very low angular momentum cases.","With the increase in flow angular momentum, it develops a non-axisymmetric nature.","In this case, simple self-similarity does not hold.","We also find that the sonic surface moves away from the innermost stable circular orbit as its angular momentum decreases.","Finally, we emphasize that intermediate angular momentum flow could provide a possible solution to explain the complex observation features of the supermassive black hole Sgr~A$^*$ at our galactic center."],"url":"http://arxiv.org/abs/2403.18359v1","category":"astro-ph.HE"}
{"created":"2024-03-27 08:48:16","title":"Supervised Multiple Kernel Learning approaches for multi-omics data integration","abstract":"Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our results offer a direction for bio-data mining research and further development of methods for heterogeneous data integration.","sentences":["Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets.","The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics.","Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.","We provide novel MKL approaches based on different kernel fusion strategies.","To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.","We also tested deep learning architectures for kernel fusion and classification.","The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches.","Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data.","Our results offer a direction for bio-data mining research and further development of methods for heterogeneous data integration."],"url":"http://arxiv.org/abs/2403.18355v1","category":"stat.ML"}
{"created":"2024-03-27 08:44:22","title":"Deciphering autism heterogeneity: a molecular stratification approach in four mouse models","abstract":"Autism spectrum disorder (ASD) is a complex neurodevelopmental condition characterized by impairments in social interaction, communication, as well as restrained or stereotyped behaviors. The inherent heterogeneity within the autism spectrum poses challenges for developing effective pharmacological treatments targeting core features. Successful clinical trials require the identification of robust markers to enable patient stratification. In this study, we explored molecular markers within the oxytocin and immediate early gene families across five interconnected brain structures of the social circuit in four distinct ASD mouse models, each exhibiting unique behavioral features along the autism spectrum. While dysregulations in the oxytocin family were model-specific, immediate early genes displayed widespread alterations, reflecting global changes in social plasticity. Through integrative analysis, we identified Egr1, Foxp1, Homer1a, Oxt and Oxtr as five robust and discriminant molecular markers facilitating successful stratification of the four models. Importantly, our stratification demonstrated predictive values when challenged with a fifth mouse model or identifying subgroups of mice potentially responsive to oxytocin treatment. Beyond providing insights into oxytocin and immediate early gene mRNA dynamics, this proof-of-concept study represents a significant step toward potential stratification of individuals with ASD. The implications extend to enhancing the success of clinical trials and guiding personalized medicine for distinct subgroups of individuals with autism.","sentences":["Autism spectrum disorder (ASD) is a complex neurodevelopmental condition characterized by impairments in social interaction, communication, as well as restrained or stereotyped behaviors.","The inherent heterogeneity within the autism spectrum poses challenges for developing effective pharmacological treatments targeting core features.","Successful clinical trials require the identification of robust markers to enable patient stratification.","In this study, we explored molecular markers within the oxytocin and immediate early gene families across five interconnected brain structures of the social circuit in four distinct ASD mouse models, each exhibiting unique behavioral features along the autism spectrum.","While dysregulations in the oxytocin family were model-specific, immediate early genes displayed widespread alterations, reflecting global changes in social plasticity.","Through integrative analysis, we identified Egr1, Foxp1, Homer1a, Oxt and Oxtr as five robust and discriminant molecular markers facilitating successful stratification of the four models.","Importantly, our stratification demonstrated predictive values when challenged with a fifth mouse model or identifying subgroups of mice potentially responsive to oxytocin treatment.","Beyond providing insights into oxytocin and immediate early gene mRNA dynamics, this proof-of-concept study represents a significant step toward potential stratification of individuals with ASD.","The implications extend to enhancing the success of clinical trials and guiding personalized medicine for distinct subgroups of individuals with autism."],"url":"http://arxiv.org/abs/2403.18352v1","category":"q-bio.NC"}
{"created":"2024-03-27 08:42:31","title":"Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language","abstract":"The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).","sentences":["The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search.","However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task.","This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language.","This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic.","Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG)."],"url":"http://arxiv.org/abs/2403.18350v1","category":"cs.CL"}
{"created":"2024-03-27 08:39:42","title":"Sequential Recommendation with Latent Relations based on Large Language Model","abstract":"Sequential recommender systems predict items that may interest users by modeling their preferences based on historical interactions. Traditional sequential recommendation methods rely on capturing implicit collaborative filtering signals among items. Recent relation-aware sequential recommendation models have achieved promising performance by explicitly incorporating item relations into the modeling of user historical sequences, where most relations are extracted from knowledge graphs. However, existing methods rely on manually predefined relations and suffer the sparsity issue, limiting the generalization ability in diverse scenarios with varied item relations. In this paper, we propose a novel relation-aware sequential recommendation framework with Latent Relation Discovery (LRD). Different from previous relation-aware models that rely on predefined rules, we propose to leverage the Large Language Model (LLM) to provide new types of relations and connections between items. The motivation is that LLM contains abundant world knowledge, which can be adopted to mine latent relations of items for recommendation. Specifically, inspired by that humans can describe relations between items using natural language, LRD harnesses the LLM that has demonstrated human-like knowledge to obtain language knowledge representations of items. These representations are fed into a latent relation discovery module based on the discrete state variational autoencoder (DVAE). Then the self-supervised relation discovery tasks and recommendation tasks are jointly optimized. Experimental results on multiple public datasets demonstrate our proposed latent relations discovery method can be incorporated with existing relation-aware sequential recommendation models and significantly improve the performance. Further analysis experiments indicate the effectiveness and reliability of the discovered latent relations.","sentences":["Sequential recommender systems predict items that may interest users by modeling their preferences based on historical interactions.","Traditional sequential recommendation methods rely on capturing implicit collaborative filtering signals among items.","Recent relation-aware sequential recommendation models have achieved promising performance by explicitly incorporating item relations into the modeling of user historical sequences, where most relations are extracted from knowledge graphs.","However, existing methods rely on manually predefined relations and suffer the sparsity issue, limiting the generalization ability in diverse scenarios with varied item relations.","In this paper, we propose a novel relation-aware sequential recommendation framework with Latent Relation Discovery (LRD).","Different from previous relation-aware models that rely on predefined rules, we propose to leverage the Large Language Model (LLM) to provide new types of relations and connections between items.","The motivation is that LLM contains abundant world knowledge, which can be adopted to mine latent relations of items for recommendation.","Specifically, inspired by that humans can describe relations between items using natural language, LRD harnesses the LLM that has demonstrated human-like knowledge to obtain language knowledge representations of items.","These representations are fed into a latent relation discovery module based on the discrete state variational autoencoder (DVAE).","Then the self-supervised relation discovery tasks and recommendation tasks are jointly optimized.","Experimental results on multiple public datasets demonstrate our proposed latent relations discovery method can be incorporated with existing relation-aware sequential recommendation models and significantly improve the performance.","Further analysis experiments indicate the effectiveness and reliability of the discovered latent relations."],"url":"http://arxiv.org/abs/2403.18348v1","category":"cs.IR"}
{"created":"2024-03-27 08:38:49","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective","abstract":"Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA) framework for limited-access MLLMs and the refinement of open-source MLLMs through fine-tuning. Extensive quantitative and qualitative experiments offer valuable insights for future research.","sentences":["Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs).","Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks.","To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems.","Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis.","Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances.","This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases.","Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA) framework for limited-access MLLMs and the refinement of open-source MLLMs through fine-tuning.","Extensive quantitative and qualitative experiments offer valuable insights for future research."],"url":"http://arxiv.org/abs/2403.18346v1","category":"cs.CL"}
{"created":"2024-03-27 08:21:41","title":"Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity","abstract":"To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve. Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces. Within the scope of this study a methodology for the semi-supervised training of deep learning models for fracture surface segmentation on a macroscopic level was established. Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability. The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories. The datasets correspond to typical isolated laboratory conditions, complex real-world circumstances, and a curated subset of the two. We implemented a weak-to-strong consistency regularization for semi-supervised learning. On the heterogeneous dataset we were able to train robust and well-generalizing models that learned feature representations from images across different domains without observing a significant drop in prediction quality. Furthermore, our approach reduced the number of labeled images required for training by a factor of 6. To demonstrate the success of our method and the benefit of our approach for the fracture mechanics assessment, we utilized the models for initial crack size measurements with the area average method. For the laboratory setting, the deep learning assisted measurements proved to have the same quality as manual measurements. For models trained on the heterogeneous dataset, very good measurement accuracies with mean deviations smaller than 1 % could be achieved...","sentences":["To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve.","Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces.","Within the scope of this study a methodology for the semi-supervised training of deep learning models for fracture surface segmentation on a macroscopic level was established.","Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability.","The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories.","The datasets correspond to typical isolated laboratory conditions, complex real-world circumstances, and a curated subset of the two.","We implemented a weak-to-strong consistency regularization for semi-supervised learning.","On the heterogeneous dataset we were able to train robust and well-generalizing models that learned feature representations from images across different domains without observing a significant drop in prediction quality.","Furthermore, our approach reduced the number of labeled images required for training by a factor of 6.","To demonstrate the success of our method and the benefit of our approach for the fracture mechanics assessment, we utilized the models for initial crack size measurements with the area average method.","For the laboratory setting, the deep learning assisted measurements proved to have the same quality as manual measurements.","For models trained on the heterogeneous dataset, very good measurement accuracies with mean deviations smaller than 1 % could be achieved..."],"url":"http://arxiv.org/abs/2403.18337v1","category":"cs.LG"}
{"created":"2024-03-27 08:14:27","title":"THz probing of non-trivial topological states in Co2MnGe Heusler alloy thin films","abstract":"Co2MnGe (CMG) has been demonstrated recently as a half-metallic ferromagnetic Heusler alloy which possesses a topologically non-trivial band structure. This behavior is unique to such systems and hence warrants extensive experimental exploration for potential spintronic and chirality sensitive optoelectonic applications. Here, we demonstrate that an epitaxial thin film of CMG acts as a source of THz radiation upon photoexcitation by optical femtosecond laser pulses. Detailed experiments have revealed that a large contribution to THz emission occurs due to nonmagnetic or spin-independent origin, however, significant contribution in the THz generation is evidenced through excitation light helicity dependent circular photogalvanic effect (CPGE) confirming the presence of topologically non-trivial carriers. Furthermore, we show that not only the topological contribution is easily suppressed but also the overall THz generation efficiency is also affected adversely for the epitaxial films grown at high substrate temperatures.","sentences":["Co2MnGe (CMG) has been demonstrated recently as a half-metallic ferromagnetic Heusler alloy which possesses a topologically non-trivial band structure.","This behavior is unique to such systems and hence warrants extensive experimental exploration for potential spintronic and chirality sensitive optoelectonic applications.","Here, we demonstrate that an epitaxial thin film of CMG acts as a source of THz radiation upon photoexcitation by optical femtosecond laser pulses.","Detailed experiments have revealed that a large contribution to THz emission occurs due to nonmagnetic or spin-independent origin, however, significant contribution in the THz generation is evidenced through excitation light helicity dependent circular photogalvanic effect (CPGE) confirming the presence of topologically non-trivial carriers.","Furthermore, we show that not only the topological contribution is easily suppressed but also the overall THz generation efficiency is also affected adversely for the epitaxial films grown at high substrate temperatures."],"url":"http://arxiv.org/abs/2403.18332v1","category":"physics.app-ph"}
{"created":"2024-03-27 07:56:13","title":"Shaping entangled photons through thick scattering media using an advanced wave beacon","abstract":"Entangled photons provide transformative new paths in the fields of communication, sensing, and computing. However, when entangled photons propagate through a complex medium such as a biological tissue or a turbulent atmosphere, their correlations are scrambled. Using wavefront shaping to compensate for the scattering and retrieve the two-photon correlations is challenging due to the low signal-to-noise ratio of the two-photon signal. While previous works partly addressed this challenge by using feedback from a strong classical laser beam that co-propagates with the entangled photons, such methods frequently depend on assumptions about the complex medium, limiting the applicability of quantum wavefront shaping. In this work, we propose and demonstrate a new feedback mechanism that is inspired by Klyshko's advanced wave picture, in which the classical laser beam counter-propagates with one of the entangled photons and co-propagates with the other. The new Klyshko feedback allows compensation of scattering in thick samples and even in situations where each photon propagates through a different scattering medium. Since the advanced wave picture applies whenever optical reciprocity is valid, such Klyshko optimization can be utilized across a wide range of configurations, offering a robust and alignment-free setup. We therefore believe this protocol will open the door for real-world applications of quantum wavefront shaping.","sentences":["Entangled photons provide transformative new paths in the fields of communication, sensing, and computing.","However, when entangled photons propagate through a complex medium such as a biological tissue or a turbulent atmosphere, their correlations are scrambled.","Using wavefront shaping to compensate for the scattering and retrieve the two-photon correlations is challenging due to the low signal-to-noise ratio of the two-photon signal.","While previous works partly addressed this challenge by using feedback from a strong classical laser beam that co-propagates with the entangled photons, such methods frequently depend on assumptions about the complex medium, limiting the applicability of quantum wavefront shaping.","In this work, we propose and demonstrate a new feedback mechanism that is inspired by Klyshko's advanced wave picture, in which the classical laser beam counter-propagates with one of the entangled photons and co-propagates with the other.","The new Klyshko feedback allows compensation of scattering in thick samples and even in situations where each photon propagates through a different scattering medium.","Since the advanced wave picture applies whenever optical reciprocity is valid, such Klyshko optimization can be utilized across a wide range of configurations, offering a robust and alignment-free setup.","We therefore believe this protocol will open the door for real-world applications of quantum wavefront shaping."],"url":"http://arxiv.org/abs/2403.18324v1","category":"quant-ph"}
{"created":"2024-03-27 07:46:57","title":"Online Prediction for Streaming Tensor Time Series","abstract":"Real-time prediction plays a vital role in various control systems, such as traffic congestion control and wireless channel resource allocation. In these scenarios, the predictor usually needs to track the evolution of the latent statistical patterns in the modern high-dimensional streaming time series continuously and quickly, which presents new challenges for traditional prediction methods. This paper proposes a novel algorithm based on tensor factorization to predict streaming tensor time series online. The proposed algorithm updates the predictor in a low-complexity online manner to adapt to the time-evolving data. Additionally, an automatically adaptive version of the algorithm is presented to mitigate the negative impact of stale data. Simulation results demonstrate that our proposed methods achieve prediction accuracy similar to that of conventional offline tensor prediction methods, while being much faster than them during long-term online prediction. Therefore, our proposed algorithm provides an effective and efficient solution for the online prediction of streaming tensor time series.","sentences":["Real-time prediction plays a vital role in various control systems, such as traffic congestion control and wireless channel resource allocation.","In these scenarios, the predictor usually needs to track the evolution of the latent statistical patterns in the modern high-dimensional streaming time series continuously and quickly, which presents new challenges for traditional prediction methods.","This paper proposes a novel algorithm based on tensor factorization to predict streaming tensor time series online.","The proposed algorithm updates the predictor in a low-complexity online manner to adapt to the time-evolving data.","Additionally, an automatically adaptive version of the algorithm is presented to mitigate the negative impact of stale data.","Simulation results demonstrate that our proposed methods achieve prediction accuracy similar to that of conventional offline tensor prediction methods, while being much faster than them during long-term online prediction.","Therefore, our proposed algorithm provides an effective and efficient solution for the online prediction of streaming tensor time series."],"url":"http://arxiv.org/abs/2403.18320v1","category":"math.OC"}
{"created":"2024-03-27 07:40:51","title":"Uncertainty-Aware SAR ATR: Defending Against Adversarial Attacks via Bayesian Neural Networks","abstract":"Adversarial attacks have demonstrated the vulnerability of Machine Learning (ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) systems. An adversarial attack can deceive the classifier into making incorrect predictions by perturbing the input SAR images, for example, with a few scatterers attached to the on-ground objects. Therefore, it is critical to develop robust SAR ATR systems that can detect potential adversarial attacks by leveraging the inherent uncertainty in ML classifiers, thereby effectively alerting human decision-makers. In this paper, we propose a novel uncertainty-aware SAR ATR for detecting adversarial attacks. Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in performing image classification with quantified epistemic uncertainty to measure the confidence for each input SAR image. By evaluating the uncertainty, our method alerts when the input SAR image is likely to be adversarially generated. Simultaneously, we also generate visual explanations that reveal the specific regions in the SAR image where the adversarial scatterers are likely to to be present, thus aiding human decision-making with hints of evidence of adversarial attacks. Experiments on the MSTAR dataset demonstrate that our approach can identify over 80% adversarial SAR images with fewer than 20% false alarms, and our visual explanations can identify up to over 90% of scatterers in an adversarial SAR image.","sentences":["Adversarial attacks have demonstrated the vulnerability of Machine Learning (ML) image classifiers in Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) systems.","An adversarial attack can deceive the classifier into making incorrect predictions by perturbing the input SAR images, for example, with a few scatterers attached to the on-ground objects.","Therefore, it is critical to develop robust SAR ATR systems that can detect potential adversarial attacks by leveraging the inherent uncertainty in ML classifiers, thereby effectively alerting human decision-makers.","In this paper, we propose a novel uncertainty-aware SAR ATR for detecting adversarial attacks.","Specifically, we leverage the capability of Bayesian Neural Networks (BNNs) in performing image classification with quantified epistemic uncertainty to measure the confidence for each input SAR image.","By evaluating the uncertainty, our method alerts when the input SAR image is likely to be adversarially generated.","Simultaneously, we also generate visual explanations that reveal the specific regions in the SAR image where the adversarial scatterers are likely to to be present, thus aiding human decision-making with hints of evidence of adversarial attacks.","Experiments on the MSTAR dataset demonstrate that our approach can identify over 80% adversarial SAR images with fewer than 20% false alarms, and our visual explanations can identify up to over 90% of scatterers in an adversarial SAR image."],"url":"http://arxiv.org/abs/2403.18318v1","category":"cs.CV"}
{"created":"2024-03-27 07:40:05","title":"A Situation-aware Enhancer for Personalized Recommendation","abstract":"When users interact with Recommender Systems (RecSys), current situations, such as time, location, and environment, significantly influence their preferences. Situations serve as the background for interactions, where relationships between users and items evolve with situation changes. However, existing RecSys treat situations, users, and items on the same level. They can only model the relations between situations and users/items respectively, rather than the dynamic impact of situations on user-item associations (i.e., user preferences). In this paper, we provide a new perspective that takes situations as the preconditions for users' interactions. This perspective allows us to separate situations from user/item representations, and capture situations' influences over the user-item relationship, offering a more comprehensive understanding of situations. Based on it, we propose a novel Situation-Aware Recommender Enhancer (SARE), a pluggable module to integrate situations into various existing RecSys. Since users' perception of situations and situations' impact on preferences are both personalized, SARE includes a Personalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder (UCPE) to model the perception and impact of situations, respectively. We conduct experiments of applying SARE on seven backbones in various settings on two real-world datasets. Experimental results indicate that SARE improves the recommendation performances significantly compared with backbones and SOTA situation-aware baselines.","sentences":["When users interact with Recommender Systems (RecSys), current situations, such as time, location, and environment, significantly influence their preferences.","Situations serve as the background for interactions, where relationships between users and items evolve with situation changes.","However, existing RecSys treat situations, users, and items on the same level.","They can only model the relations between situations and users/items respectively, rather than the dynamic impact of situations on user-item associations (i.e., user preferences).","In this paper, we provide a new perspective that takes situations as the preconditions for users' interactions.","This perspective allows us to separate situations from user/item representations, and capture situations' influences over the user-item relationship, offering a more comprehensive understanding of situations.","Based on it, we propose a novel Situation-Aware Recommender Enhancer (SARE), a pluggable module to integrate situations into various existing RecSys.","Since users' perception of situations and situations' impact on preferences are both personalized, SARE includes a Personalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder (UCPE) to model the perception and impact of situations, respectively.","We conduct experiments of applying SARE on seven backbones in various settings on two real-world datasets.","Experimental results indicate that SARE improves the recommendation performances significantly compared with backbones and SOTA situation-aware baselines."],"url":"http://arxiv.org/abs/2403.18317v1","category":"cs.IR"}
{"created":"2024-03-27 07:27:13","title":"Cepheids with giant companions: A new, abundant source of Cepheid astrophysics","abstract":"We present a progress report of our project aiming to increase the number of known Cepheids in double-lined binary (SB2) systems from six to 100 or more. This will allow us, among other goals, to accurately measure masses for a large sample of Cepheids. Currently, only six accurate Cepheid masses are available, which hinders our understanding of their physical properties and renders the Cepheid mass--luminosity relation poorly constrained. At the same time, Cepheids are widely used for essential measurements (e.g., extragalactic distances, the Hubble constant). To examine Cepheid period--luminosity relations, we selected as binary candidates Cepheids that are too bright for their periods. To date, we have confirmed 56 SB2 systems, including the detection of significant orbital motions of the components for 32. We identified systems with orbital periods up to five times shorter than the shortest reported period to date, as well as systems with mass ratios significantly different from unity (suggesting past merger events). Both features are essential to understand how multiplicity affects the formation and destruction of Cepheid progenitors and what effect this has on global Cepheid properties. We also present eight new systems composed of two Cepheids (only one such system was known before). Among confirmed SB2 Cepheids, there are also several wide-orbit systems. In the future, these may facilitate independent accurate geometric distance measurements to the Large and Small Magellanic Clouds.","sentences":["We present a progress report of our project aiming to increase the number of known Cepheids in double-lined binary (SB2) systems from six to 100 or more.","This will allow us, among other goals, to accurately measure masses for a large sample of Cepheids.","Currently, only six accurate Cepheid masses are available, which hinders our understanding of their physical properties and renders the Cepheid mass--luminosity relation poorly constrained.","At the same time, Cepheids are widely used for essential measurements (e.g., extragalactic distances, the Hubble constant).","To examine Cepheid period--luminosity relations, we selected as binary candidates Cepheids that are too bright for their periods.","To date, we have confirmed 56 SB2 systems, including the detection of significant orbital motions of the components for 32.","We identified systems with orbital periods up to five times shorter than the shortest reported period to date, as well as systems with mass ratios significantly different from unity (suggesting past merger events).","Both features are essential to understand how multiplicity affects the formation and destruction of Cepheid progenitors and what effect this has on global Cepheid properties.","We also present eight new systems composed of two Cepheids (only one such system was known before).","Among confirmed SB2 Cepheids, there are also several wide-orbit systems.","In the future, these may facilitate independent accurate geometric distance measurements to the Large and Small Magellanic Clouds."],"url":"http://arxiv.org/abs/2403.18312v1","category":"astro-ph.SR"}
{"created":"2024-03-27 07:16:05","title":"Comparison of different methods for identification of dominant oscillation mode","abstract":"This paper introduces and compares the various techniques for identification and analysis of low frequency oscillations in a power system. Inter-area electromechanical oscillations are the focus of this paper. After multiresolution decomposition of characteristic signals, physical characteristics of system oscillations in signal components are identified and presented using the Fourier transform, Prony method, Matrix Pencil Analysis Method, S-transform, Global Wavelet Spectrum and Hilbert Huang transform (Hilbert Marginal Spectrum) in time-frequency domain representation. The analyses were performed on real frequency signals obtained from FNET GridEye system during the earthquake that triggered the shutdown of the North Anna Nuclear Generating Station in the east coast of the United States. In addition, according to the obtained results the proposed methods have proven to be reliable for identification of the model parameters of low-frequency oscillation in power systems. The relevant analyses are carried out in MATLAB coding environment.","sentences":["This paper introduces and compares the various techniques for identification and analysis of low frequency oscillations in a power system.","Inter-area electromechanical oscillations are the focus of this paper.","After multiresolution decomposition of characteristic signals, physical characteristics of system oscillations in signal components are identified and presented using the Fourier transform, Prony method, Matrix Pencil Analysis Method, S-transform, Global Wavelet Spectrum and Hilbert Huang transform (Hilbert Marginal Spectrum) in time-frequency domain representation.","The analyses were performed on real frequency signals obtained from FNET GridEye system during the earthquake that triggered the shutdown of the North Anna Nuclear Generating Station in the east coast of the United States.","In addition, according to the obtained results the proposed methods have proven to be reliable for identification of the model parameters of low-frequency oscillation in power systems.","The relevant analyses are carried out in MATLAB coding environment."],"url":"http://arxiv.org/abs/2403.18308v1","category":"cs.CE"}
{"created":"2024-03-27 06:58:50","title":"Classical dynamics and semiclassical analysis of excitons in cuprous oxide","abstract":"Excitons, as bound states of electrons and holes, embody the solid state analogue of the hydrogen atom, whose quantum spectrum is explained within a classical framework by the Bohr-Sommerfeld atomic model. In a first hydrogenlike approximation the spectra of excitons are also well described by a Rydberg series, however, due to the surrounding crystal environment deviations from this series can be observed. A theoretical treatment of excitons in cuprous oxide needs to include the band structure of the crystal, leading to a prominent fine-structure splitting in the quantum spectra. This is achieved by introducing additional spin degrees of freedom into the system, making the existence and meaningfulness of classical exciton orbits in the physical system a non-trivial question. Recently, we have uncovered the contributions of periodic exciton orbits directly in the quantum mechanical recurrence spectra of cuprous oxide [J. Ertl et al., Phys. Rev. Lett. 129, 067401 (2022)] by application of a scaling technique and fixing the energy of the classical dynamics to a value corresponding to a principle quantum number $n=5$ in the hydrogenlike case. Here, we present a comprehensive derivation of the classical and semiclassical theory of excitons in cuprous oxide. In particular, we investigate the energy dependence of the exciton dynamics. Both the semiclassical and quantum mechanical recurrence spectra exhibit stronger deviations from the hydrogenlike behavior with decreasing energy, which is related to a growing influence of the spin-orbit coupling and thus a higher velocity of the secular motion of the exciton orbits. The excellent agreement between semiclassical and quantum mechanical exciton recurrence spectra demonstrates the validity of the classical and semiclassical approach to excitons in cuprous oxide.","sentences":["Excitons, as bound states of electrons and holes, embody the solid state analogue of the hydrogen atom, whose quantum spectrum is explained within a classical framework by the Bohr-Sommerfeld atomic model.","In a first hydrogenlike approximation the spectra of excitons are also well described by a Rydberg series, however, due to the surrounding crystal environment deviations from this series can be observed.","A theoretical treatment of excitons in cuprous oxide needs to include the band structure of the crystal, leading to a prominent fine-structure splitting in the quantum spectra.","This is achieved by introducing additional spin degrees of freedom into the system, making the existence and meaningfulness of classical exciton orbits in the physical system a non-trivial question.","Recently, we have uncovered the contributions of periodic exciton orbits directly in the quantum mechanical recurrence spectra of cuprous oxide [J. Ertl et al., Phys.","Rev. Lett.","129, 067401 (2022)] by application of a scaling technique and fixing the energy of the classical dynamics to a value corresponding to a principle quantum number $n=5$ in the hydrogenlike case.","Here, we present a comprehensive derivation of the classical and semiclassical theory of excitons in cuprous oxide.","In particular, we investigate the energy dependence of the exciton dynamics.","Both the semiclassical and quantum mechanical recurrence spectra exhibit stronger deviations from the hydrogenlike behavior with decreasing energy, which is related to a growing influence of the spin-orbit coupling and thus a higher velocity of the secular motion of the exciton orbits.","The excellent agreement between semiclassical and quantum mechanical exciton recurrence spectra demonstrates the validity of the classical and semiclassical approach to excitons in cuprous oxide."],"url":"http://arxiv.org/abs/2403.18303v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 06:54:56","title":"HotStuff-2 vs. HotStuff: The Difference and Advantage","abstract":"Byzantine consensus protocols are essential in blockchain technology. The widely recognized HotStuff protocol uses cryptographic measures for efficient view changes and reduced communication complexity. Recently, the main authors of HotStuff introduced an advanced iteration named HotStuff-2. This paper aims to compare the principles and analyze the effectiveness of both protocols, hoping to depict their key differences and assess the potential enhancements offered by HotStuff-2.","sentences":["Byzantine consensus protocols are essential in blockchain technology.","The widely recognized HotStuff protocol uses cryptographic measures for efficient view changes and reduced communication complexity.","Recently, the main authors of HotStuff introduced an advanced iteration named HotStuff-2.","This paper aims to compare the principles and analyze the effectiveness of both protocols, hoping to depict their key differences and assess the potential enhancements offered by HotStuff-2."],"url":"http://arxiv.org/abs/2403.18300v1","category":"cs.CR"}
{"created":"2024-03-27 06:54:53","title":"Scaling Enhancement of Photon Blockade in Output Fields","abstract":"Photon blockade enhancement is an exciting and promising subject that has been well studied for photons in cavities. However, whether photon blockade can be enhanced in the output fields remains largely unexplored. We show that photon blockade can be greatly enhanced in the mixing output field of a nonlinear cavity and an auxiliary (linear) cavity, where no direct coupling between the nonlinear and auxiliary cavities is needed. We uncover a biquadratic scaling relation between the second-order correlation of the photons in the output field and intracavity nonlinear interaction strength, in contrast to a quadratic scaling relation for the photons in a nonlinear cavity. We identify that this scaling enhancement of photon blockade in the output field is induced by the destructive interference between two of the paths for two photons passing through the two cavities. We then extend the theory to the experimentally feasible Jaynes-Cummings model consisting of a two-level system strongly coupled to one of the two uncoupled cavities, and also predict a biquadratic scaling law in the mixing output field. Our proposed scheme is universal and can be extended to enhance blockade in other bosonic systems.","sentences":["Photon blockade enhancement is an exciting and promising subject that has been well studied for photons in cavities.","However, whether photon blockade can be enhanced in the output fields remains largely unexplored.","We show that photon blockade can be greatly enhanced in the mixing output field of a nonlinear cavity and an auxiliary (linear) cavity, where no direct coupling between the nonlinear and auxiliary cavities is needed.","We uncover a biquadratic scaling relation between the second-order correlation of the photons in the output field and intracavity nonlinear interaction strength, in contrast to a quadratic scaling relation for the photons in a nonlinear cavity.","We identify that this scaling enhancement of photon blockade in the output field is induced by the destructive interference between two of the paths for two photons passing through the two cavities.","We then extend the theory to the experimentally feasible Jaynes-Cummings model consisting of a two-level system strongly coupled to one of the two uncoupled cavities, and also predict a biquadratic scaling law in the mixing output field.","Our proposed scheme is universal and can be extended to enhance blockade in other bosonic systems."],"url":"http://arxiv.org/abs/2403.18299v1","category":"quant-ph"}
{"created":"2024-03-27 06:53:52","title":"Deciphering Chemical Ordering in High Entropy Materials: A Machine Learning-Accelerated High-throughput Cluster Expansion Approach","abstract":"The Cluster Expansion (CE) Method encounters significant computational challenges in multicomponent systems due to the computational expense of generating training data through density functional theory (DFT) calculations. This work aims to refine the cluster and structure selection processes to mitigate these challenges. We introduce a novel method that significantly reduces the computational load associated with the calculation of fitting parameters. This method employs a Graph Neural Network (GNN) model, leveraging the M3GNet network, which is trained using a select subset of DFT calculations at each ionic step. The trained surrogate model excels in predicting the volume and energy of the final structure for a relaxation run. By employing this model, we sample thousands of structures and fit a CE model to the energies of these GNN-relaxed structures. This approach, utilizing a large training dataset, effectively reduces the risk of overfitting, yielding a CE model with a root-mean-square error (RMSE) below 10 meV/atom. We validate our method's effectiveness in two test cases: the (Cr,Hf,Mo,Ta,Ti,Zr)B$_2$ diboride system and the Refractory High-Entropy Alloy (HEA) AlHfNbTaTiZr system. Our findings demonstrate the significant advantages of integrating a GNN model, specifically the M3GNet network, with CE methods for the efficient predictive analysis of chemical ordering in High Entropy Materials. The accelerating capabilities of the hybrid ML-CE approach to investigate the evolution of Short Range Ordering (SRO) in a large number of stoichiometric systems. Finally, we show how it is possible to correlate the strength of chemical ordering to easily accessible alloy parameters.","sentences":["The Cluster Expansion (CE) Method encounters significant computational challenges in multicomponent systems due to the computational expense of generating training data through density functional theory (DFT) calculations.","This work aims to refine the cluster and structure selection processes to mitigate these challenges.","We introduce a novel method that significantly reduces the computational load associated with the calculation of fitting parameters.","This method employs a Graph Neural Network (GNN) model, leveraging the M3GNet network, which is trained using a select subset of DFT calculations at each ionic step.","The trained surrogate model excels in predicting the volume and energy of the final structure for a relaxation run.","By employing this model, we sample thousands of structures and fit a CE model to the energies of these GNN-relaxed structures.","This approach, utilizing a large training dataset, effectively reduces the risk of overfitting, yielding a CE model with a root-mean-square error (RMSE) below 10 meV/atom.","We validate our method's effectiveness in two test cases: the (Cr,Hf,Mo,Ta,Ti,Zr)B$_2$ diboride system and the Refractory High-Entropy Alloy (HEA) AlHfNbTaTiZr system.","Our findings demonstrate the significant advantages of integrating a GNN model, specifically the M3GNet network, with CE methods for the efficient predictive analysis of chemical ordering in High Entropy Materials.","The accelerating capabilities of the hybrid ML-CE approach to investigate the evolution of Short Range Ordering (SRO) in a large number of stoichiometric systems.","Finally, we show how it is possible to correlate the strength of chemical ordering to easily accessible alloy parameters."],"url":"http://arxiv.org/abs/2403.18298v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 06:21:31","title":"A new dual spectral projected gradient method for log-determinant semidefinite programming with hidden clustering structures","abstract":"In this paper, we propose a new efficient method for a sparse Gaussian graphical model with hidden clustering structures by extending a dual spectral projected gradient (DSPG) method proposed by Nakagaki et al.~(2020). We establish the global convergence of the proposed method to an optimal solution, and we show that the projection onto the feasible region can be solved with a low computational complexity by the use of the pool-adjacent-violators algorithm. Numerical experiments on synthesis data and real data demonstrate the efficiency of the proposed method. The proposed method takes 0.91 seconds to achieve a similar solution to the direct application of the DSPG method which takes 4361 seconds.","sentences":["In this paper, we propose a new efficient method for a sparse Gaussian graphical model with hidden clustering structures by extending a dual spectral projected gradient (DSPG) method proposed by Nakagaki et al.~(2020).","We establish the global convergence of the proposed method to an optimal solution, and we show that the projection onto the feasible region can be solved with a low computational complexity by the use of the pool-adjacent-violators algorithm.","Numerical experiments on synthesis data and real data demonstrate the efficiency of the proposed method.","The proposed method takes 0.91 seconds to achieve a similar solution to the direct application of the DSPG method which takes 4361 seconds."],"url":"http://arxiv.org/abs/2403.18284v1","category":"math.OC"}
{"created":"2024-03-27 06:19:35","title":"PT-symmetric dynamical confinement: Fermi acceleration, quantum force and Berry phase","abstract":"We consider a quantum particle under the dynamical confinement caused by PT-symmetric box with a moving wall. The latter is described in terms of the time-dependent Schr\\\"{o}dinger equation obeying the time-dependent PT-symmetric boundary conditions. The class of the functions, describing time-dependence of the wall's position and keeping the system as PT-symmetric is found. Physically observable characteristics, such as average kinetic energy and the average quantum force are calculated as a function of time. Also, geometric phase is calculated for the harmonically oscillating wall regime. Experimental realization of the proposed model is discussed.","sentences":["We consider a quantum particle under the dynamical confinement caused by PT-symmetric box with a moving wall.","The latter is described in terms of the time-dependent Schr\\\"{o}dinger equation obeying the time-dependent PT-symmetric boundary conditions.","The class of the functions, describing time-dependence of the wall's position and keeping the system as PT-symmetric is found.","Physically observable characteristics, such as average kinetic energy and the average quantum force are calculated as a function of time.","Also, geometric phase is calculated for the harmonically oscillating wall regime.","Experimental realization of the proposed model is discussed."],"url":"http://arxiv.org/abs/2403.18283v1","category":"quant-ph"}
{"created":"2024-03-27 06:16:14","title":"Improving Out-of-Vocabulary Handling in Recommendation Systems","abstract":"Recommendation systems (RS) are an increasingly relevant area for both academic and industry researchers, given their widespread impact on the daily online experiences of billions of users. One common issue in real RS is the cold-start problem, where users and items may not contain enough information to produce high-quality recommendations. This work focuses on a complementary problem: recommending new users and items unseen (out-of-vocabulary, or OOV) at training time. This setting is known as the inductive setting and is especially problematic for factorization-based models, which rely on encoding only those users/items seen at training time with fixed parameter vectors. Many existing solutions applied in practice are often naive, such as assigning OOV users/items to random buckets. In this work, we tackle this problem and propose approaches that better leverage available user/item features to improve OOV handling at the embedding table level. We discuss general-purpose plug-and-play approaches that are easily applicable to most RS models and improve inductive performance without negatively impacting transductive model performance. We extensively evaluate 9 OOV embedding methods on 5 models across 4 datasets (spanning different domains). One of these datasets is a proprietary production dataset from a prominent RS employed by a large social platform serving hundreds of millions of daily active users. In our experiments, we find that several proposed methods that exploit feature similarity using LSH consistently outperform alternatives on most model-dataset combinations, with the best method showing a mean improvement of 3.74% over the industry standard baseline in inductive performance. We release our code and hope our work helps practitioners make more informed decisions when handling OOV for their RS and further inspires academic research into improving OOV support in RS.","sentences":["Recommendation systems (RS) are an increasingly relevant area for both academic and industry researchers, given their widespread impact on the daily online experiences of billions of users.","One common issue in real RS is the cold-start problem, where users and items may not contain enough information to produce high-quality recommendations.","This work focuses on a complementary problem: recommending new users and items unseen (out-of-vocabulary, or OOV) at training time.","This setting is known as the inductive setting and is especially problematic for factorization-based models, which rely on encoding only those users/items seen at training time with fixed parameter vectors.","Many existing solutions applied in practice are often naive, such as assigning OOV users/items to random buckets.","In this work, we tackle this problem and propose approaches that better leverage available user/item features to improve OOV handling at the embedding table level.","We discuss general-purpose plug-and-play approaches that are easily applicable to most RS models and improve inductive performance without negatively impacting transductive model performance.","We extensively evaluate 9 OOV embedding methods on 5 models across 4 datasets (spanning different domains).","One of these datasets is a proprietary production dataset from a prominent RS employed by a large social platform serving hundreds of millions of daily active users.","In our experiments, we find that several proposed methods that exploit feature similarity using LSH consistently outperform alternatives on most model-dataset combinations, with the best method showing a mean improvement of 3.74% over the industry standard baseline in inductive performance.","We release our code and hope our work helps practitioners make more informed decisions when handling OOV for their RS and further inspires academic research into improving OOV support in RS."],"url":"http://arxiv.org/abs/2403.18280v1","category":"cs.IR"}
{"created":"2024-03-27 06:15:50","title":"Forbidden complexes for the 3-sphere","abstract":"A simplicial complex is said to be {\\em critical} (or {\\em forbidden}) for the 3-sphere $S^3$ if it cannot be embedded in $S^3$ but after removing any one point, it can be embedded.   We show that if a multibranched surface cannot be embedded in $S^3$, it contains a critical complex which is a union of a multibranched surface and a (possibly empty) graph. We exhibit all critical complexes for $S^3$ which are contained in $K_5 \\times S^1$ and $K_{3,3} \\times S^1$ families. We also classify all critical complexes for $S^3$ which can be decomposed into $G\\times S^1$ and $H$, where $G$ and $H$ are graphs.   In spite of the above property, there exist complexes which cannot be embedded in $S^3$, but they do not contain any critical complexes. From the property of those examples, we define an equivalence relation on all simplicial complexes $\\mathcal{C}$ and a partially ordered set of complexes $(\\mathcal{C}/\\mathord\\sim; \\subseteqq)$, and refine the definition of critical. According to the refined definition of critical, we show that if a complex $X$ cannot be embedded in $S^3$, then there exists $[X']\\subseteqq [X]$ such that $[X']$ is critical for $[S^3]$.","sentences":["A simplicial complex is said to be {\\em critical} (or {\\em forbidden}) for the 3-sphere $S^3$ if it cannot be embedded in $S^3$ but after removing any one point, it can be embedded.   ","We show that if a multibranched surface cannot be embedded in $S^3$, it contains a critical complex which is a union of a multibranched surface and a (possibly empty) graph.","We exhibit all critical complexes for $S^3$ which are contained in $K_5 \\times S^1$ and $K_{3,3} \\times S^1$ families.","We also classify all critical complexes for $S^3$ which can be decomposed into $G\\times S^1$ and $H$, where $G$ and $H$ are graphs.   ","In spite of the above property, there exist complexes which cannot be embedded in $S^3$, but they do not contain any critical complexes.","From the property of those examples, we define an equivalence relation on all simplicial complexes $\\mathcal{C}$ and a partially ordered set of complexes $(\\mathcal{C}/\\mathord\\sim; \\subseteqq)$, and refine the definition of critical.","According to the refined definition of critical, we show that if a complex $X$ cannot be embedded in $S^3$, then there exists $[X']\\subseteqq","[X]$ such that $[X']$ is critical for $[S^3]$."],"url":"http://arxiv.org/abs/2403.18279v1","category":"math.GT"}
{"created":"2024-03-27 06:13:04","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","abstract":"Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at https://github.com/HYU-NLP/BlendX.","sentences":["Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent.","However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance.","While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation.","To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity.","For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection.","To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage.","Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field.","The dataset is available at https://github.com/HYU-NLP/BlendX."],"url":"http://arxiv.org/abs/2403.18277v1","category":"cs.CL"}
{"created":"2024-03-27 06:07:05","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers","abstract":"Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \\mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language model's ability to comprehend lengthy contextual inputs and to capture the interaction between query and document tokens. We find that (1) Mamba models achieve competitive performance compared to transformer-based models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient transformer implementations such as flash attention. We hope this study can serve as a starting point to explore Mamba models in other classical IR tasks. Our code implementation and trained checkpoints are made public to facilitate reproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.","sentences":["Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR).","Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference.","Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention.","A different line of work aims to design new mechanisms to replace attention.","Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   ","In this work, we examine \\mamba's efficacy through the lens of a classical IR task -- document ranking.","A reranker model takes a query and a document as input, and predicts a scalar relevance score.","This task demands the language model's ability to comprehend lengthy contextual inputs and to capture the interaction between query and document tokens.","We find that (1) Mamba models achieve competitive performance compared to transformer-based models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient transformer implementations such as flash attention.","We hope this study can serve as a starting point to explore Mamba models in other classical IR tasks.","Our code implementation and trained checkpoints are made public to facilitate reproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}."],"url":"http://arxiv.org/abs/2403.18276v1","category":"cs.IR"}
{"created":"2024-03-27 17:58:20","title":"Equivalence Checking of Quantum Circuits by Model Counting","abstract":"Verifying equivalence between two quantum circuits is a hard problem, that is nonetheless crucial in compiling and optimizing quantum algorithms for real-world devices. This paper gives a Turing reduction of the (universal) quantum circuits equivalence problem to weighted model counting (WMC). Our starting point is a folklore theorem showing that equivalence checking of quantum circuits can be done in the so-called Pauli-basis. We combine this insight with a WMC encoding of quantum circuit simulation, which we extend with support for the Toffoli gate. Finally, we prove that the weights computed by the model counter indeed realize the reduction. With an open-source implementation, we demonstrate that this novel approach can outperform a state-of-the-art equivalence-checking tool based on ZX calculus and decision diagrams.","sentences":["Verifying equivalence between two quantum circuits is a hard problem, that is nonetheless crucial in compiling and optimizing quantum algorithms for real-world devices.","This paper gives a Turing reduction of the (universal) quantum circuits equivalence problem to weighted model counting (WMC).","Our starting point is a folklore theorem showing that equivalence checking of quantum circuits can be done in the so-called Pauli-basis.","We combine this insight with a WMC encoding of quantum circuit simulation, which we extend with support for the Toffoli gate.","Finally, we prove that the weights computed by the model counter indeed realize the reduction.","With an open-source implementation, we demonstrate that this novel approach can outperform a state-of-the-art equivalence-checking tool based on ZX calculus and decision diagrams."],"url":"http://arxiv.org/abs/2403.18813v1","category":"quant-ph"}
{"created":"2024-03-27 17:49:31","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language Models","abstract":"Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.","sentences":["Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings.","More recently, attention has shifted to debiasing pre-trained language models.","We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations.","Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters.","We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned.","To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures.","We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated.","This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model."],"url":"http://arxiv.org/abs/2403.18803v1","category":"cs.CL"}
{"created":"2024-03-27 17:45:29","title":"Dimension-independent functional inequalities by tensorization and projection arguments","abstract":"We study stability under tensorization and projection-type operations of gradient-type estimates and other functional inequalities for Markov semigroups on metric spaces. Using transportation-type inequalities obtained by F. Baudoin and N. Eldredge in 2021, we prove that constants in the gradient estimates can be chosen to be independent of the dimension. Our results are applicable to hypoelliptic diffusions on sub-Riemannian manifolds and some hypocoercive diffusions. As a byproduct, we obtain dimension-independent reverse Poincar\\'{e}, reverse logarithmic Sobolev, and gradient bounds for Lie groups with a transverse symmetry and for non-isotropic Heisenberg groups.","sentences":["We study stability under tensorization and projection-type operations of gradient-type estimates and other functional inequalities for Markov semigroups on metric spaces.","Using transportation-type inequalities obtained by F. Baudoin and N. Eldredge in 2021, we prove that constants in the gradient estimates can be chosen to be independent of the dimension.","Our results are applicable to hypoelliptic diffusions on sub-Riemannian manifolds and some hypocoercive diffusions.","As a byproduct, we obtain dimension-independent reverse Poincar\\'{e}, reverse logarithmic Sobolev, and gradient bounds for Lie groups with a transverse symmetry and for non-isotropic Heisenberg groups."],"url":"http://arxiv.org/abs/2403.18799v1","category":"math.PR"}
{"created":"2024-03-27 16:44:39","title":"Fast Decision Algorithms for Efficient Access Point Assignment in SDN-Controlled Wireless Access Networks","abstract":"Global optimization of access point (AP) assignment to user terminals requires efficient monitoring of user behavior, fast decision algorithms, efficient control signaling, and fast AP reassignment mechanisms. In this scenario, software defined networking (SDN) technology may be suitable for network monitoring, signaling, and control. We recently proposed embedding virtual switches in user terminals for direct management by an SDN controller, further contributing to SDN-oriented access network optimization. However, since users may restrict terminal-side traffic monitoring for privacy reasons (a common assumption by previous authors), we infer user traffic classes at the APs. On the other hand, since handovers will be more frequent in dense small-cell networks (e.g., mmWave-based 5G deployments will require dense network topologies with inter-site distances of ~150-200 m), the delay to take assignment decisions should be minimal. To this end, we propose taking fast decisions based exclusively on extremely simple network-side application flow-type predictions based on past user behavior. Using real data we show that a centralized allocation algorithm based on those predictions achieves network utilization levels that approximate those of optimal allocations. We also test a distributed version of this algorithm. Finally, we quantify the elapsed time since a user traffic event takes place until its terminal is assigned an AP, when needed.","sentences":["Global optimization of access point (AP) assignment to user terminals requires efficient monitoring of user behavior, fast decision algorithms, efficient control signaling, and fast AP reassignment mechanisms.","In this scenario, software defined networking (SDN) technology may be suitable for network monitoring, signaling, and control.","We recently proposed embedding virtual switches in user terminals for direct management by an SDN controller, further contributing to SDN-oriented access network optimization.","However, since users may restrict terminal-side traffic monitoring for privacy reasons (a common assumption by previous authors), we infer user traffic classes at the APs.","On the other hand, since handovers will be more frequent in dense small-cell networks (e.g., mmWave-based 5G deployments will require dense network topologies with inter-site distances of ~150-200 m), the delay to take assignment decisions should be minimal.","To this end, we propose taking fast decisions based exclusively on extremely simple network-side application flow-type predictions based on past user behavior.","Using real data we show that a centralized allocation algorithm based on those predictions achieves network utilization levels that approximate those of optimal allocations.","We also test a distributed version of this algorithm.","Finally, we quantify the elapsed time since a user traffic event takes place until its terminal is assigned an AP, when needed."],"url":"http://arxiv.org/abs/2403.18745v1","category":"cs.NI"}
{"created":"2024-03-27 16:25:57","title":"Optimal Rebalancing in Dynamic AMMs","abstract":"Dynamic AMM pools, as found in Temporal Function Market Making, rebalance their holdings to a new desired ratio (e.g. moving from being 50-50 between two assets to being 90-10 in favour of one of them) by introducing an arbitrage opportunity that disappears when their holdings are in line with their target. Structuring this arbitrage opportunity reduces to the problem of choosing the sequence of portfolio weights the pool exposes to the market via its trading function. Linear interpolation from start weights to end weights has been used to reduce the cost paid by pools to arbitrageurs to rebalance. Here we obtain the $\\textit{optimal}$ interpolation in the limit of small weight changes (which has the downside of requiring a call to a transcendental function) and then obtain a cheap-to-compute approximation to that optimal approach that gives almost the same performance improvement. We then demonstrate this method on a range of market backtests, including simulating pool performance when trading fees are present, finding that the new approximately-optimal method of changing weights gives robust increases in pool performance. For a BTC-ETH-DAI pool from July 2022 to June 2023, the increases of pool P\\&L from approximately-optimal weight changes is $\\sim25\\%$ for a range of different strategies and trading fees.","sentences":["Dynamic AMM pools, as found in Temporal Function Market Making, rebalance their holdings to a new desired ratio (e.g. moving from being 50-50 between two assets to being 90-10 in favour of one of them) by introducing an arbitrage opportunity that disappears when their holdings are in line with their target.","Structuring this arbitrage opportunity reduces to the problem of choosing the sequence of portfolio weights the pool exposes to the market via its trading function.","Linear interpolation from start weights to end weights has been used to reduce the cost paid by pools to arbitrageurs to rebalance.","Here we obtain the $\\textit{optimal}$ interpolation in the limit of small weight changes (which has the downside of requiring a call to a transcendental function) and then obtain a cheap-to-compute approximation to that optimal approach that gives almost the same performance improvement.","We then demonstrate this method on a range of market backtests, including simulating pool performance when trading fees are present, finding that the new approximately-optimal method of changing weights gives robust increases in pool performance.","For a BTC-ETH-DAI pool from July 2022 to June 2023, the increases of pool P\\&L from approximately-optimal weight changes is $\\sim25\\%$ for a range of different strategies and trading fees."],"url":"http://arxiv.org/abs/2403.18737v1","category":"q-fin.TR"}
{"created":"2024-03-27 15:53:42","title":"Convergence rates under a range invariance condition with application to electrical impedance tomography","abstract":"This paper is devoted to proving convergence rates of variational and iterative regularization methods under variational source conditions VSCs for inverse problems whose linearization satisfies a range invariance condition. In order to achieve this, often an appropriate relaxation of the problem needs to be found that is usually based on an augmentation of the set of unknowns and leads to a particularly structured reformulation of the inverse problem. We analyze three approaches that make use of this structure, namely a variational and a Newton type scheme, whose convergence without rates has already been established in \\cite{rangeinvar}; additionally we propose a split minimization approach that can be show to satisfy the same rates results. \\\\ The range invariance condition has been verified for several coefficient identification problems for partial differential equations from boundary observations as relevant in a variety of tomographic imaging modalities. Our motivation particularly comes from the by now classical inverse problem of electrical impedance tomography EIT and we study both the original formulation by a diffusion type equation and its reformulation as a Schr\\\"odinger equation. For both of them we find relaxations that can be proven to satisfy the range invariance condition. Combining results on VSCs from \\cite{Diss-Weidling} with the abstract framework for the three approaches mentioned above, we arrive at convergence rates results for the variational, split minimization and Newton type method in EIT.","sentences":["This paper is devoted to proving convergence rates of variational and iterative regularization methods under variational source conditions VSCs for inverse problems whose linearization satisfies a range invariance condition.","In order to achieve this, often an appropriate relaxation of the problem needs to be found that is usually based on an augmentation of the set of unknowns and leads to a particularly structured reformulation of the inverse problem.","We analyze three approaches that make use of this structure, namely a variational and a Newton type scheme, whose convergence without rates has already been established in \\cite{rangeinvar}; additionally we propose a split minimization approach that can be show to satisfy the same rates results.","\\\\","The range invariance condition has been verified for several coefficient identification problems for partial differential equations from boundary observations as relevant in a variety of tomographic imaging modalities.","Our motivation particularly comes from the by now classical inverse problem of electrical impedance tomography EIT and we study both the original formulation by a diffusion type equation and its reformulation as a Schr\\\"odinger equation.","For both of them we find relaxations that can be proven to satisfy the range invariance condition.","Combining results on VSCs from \\cite{Diss-Weidling} with the abstract framework for the three approaches mentioned above, we arrive at convergence rates results for the variational, split minimization and Newton type method in EIT."],"url":"http://arxiv.org/abs/2403.18704v1","category":"math.NA"}
{"created":"2024-03-27 15:43:09","title":"Designing Simple Mechanisms","abstract":"Which mechanisms are simple to play? When is it easy for participants to see that a mechanism is incentive-compatible? I will start by explaining how and why economists came to ask these questions. Then I will discuss three recent answers, that capture different aspects of what makes a mechanism simple.","sentences":["Which mechanisms are simple to play?","When is it easy for participants to see that a mechanism is incentive-compatible?","I will start by explaining how and why economists came to ask these questions.","Then I will discuss three recent answers, that capture different aspects of what makes a mechanism simple."],"url":"http://arxiv.org/abs/2403.18694v1","category":"econ.TH"}
{"created":"2024-03-27 15:42:36","title":"Anomalous friction of supercooled glycerol on mica","abstract":"The fundamental understanding of friction of liquids on solid surfaces remains one of the key knowledge gaps in the transport of fluids. While the standard perspective emphasizes the role of wettability and commensurability, recent works have unveiled the crucial role of the solid's internal excitations, whether electronic or phononic, on liquid-solid dissipation. In this work, we take advantage of the considerable variation of the molecular timescales of supercooled glycerol under mild change of temperature, in order to explore how friction depends on the liquid's molecular dynamics. Using a dedicated tuning-fork-based AFM to measure the hydrodynamic slippage of glycerol on mica, we report a 2-order of magnitude increase of the slip length with decreasing temperature by only 30{\\deg}C. However the solid-liquid friction coefficient is found to be a non monotonous function of the fluid molecular relaxation rate, f{\\alpha}, at odd with an expected Arrhenius behavior. In particular, the linear increase of friction with the liquid molecular rate measured at high temperature cannot be accounted for by existing modelling. We show that this unconventional and non-arrhenian friction is consistent with a contribution of the solid's phonons to the liquid-solid friction. This dynamical friction opens new perspectives to control hydrodynamic flows by properly engineering phononic and electronic excitation spectra in channel walls.","sentences":["The fundamental understanding of friction of liquids on solid surfaces remains one of the key knowledge gaps in the transport of fluids.","While the standard perspective emphasizes the role of wettability and commensurability, recent works have unveiled the crucial role of the solid's internal excitations, whether electronic or phononic, on liquid-solid dissipation.","In this work, we take advantage of the considerable variation of the molecular timescales of supercooled glycerol under mild change of temperature, in order to explore how friction depends on the liquid's molecular dynamics.","Using a dedicated tuning-fork-based AFM to measure the hydrodynamic slippage of glycerol on mica, we report a 2-order of magnitude increase of the slip length with decreasing temperature by only 30{\\deg}C.","However the solid-liquid friction coefficient is found to be a non monotonous function of the fluid molecular relaxation rate, f{\\alpha}, at odd with an expected Arrhenius behavior.","In particular, the linear increase of friction with the liquid molecular rate measured at high temperature cannot be accounted for by existing modelling.","We show that this unconventional and non-arrhenian friction is consistent with a contribution of the solid's phonons to the liquid-solid friction.","This dynamical friction opens new perspectives to control hydrodynamic flows by properly engineering phononic and electronic excitation spectra in channel walls."],"url":"http://arxiv.org/abs/2403.18693v1","category":"cond-mat.soft"}
{"created":"2024-03-27 14:52:44","title":"Synergistic Knowledge","abstract":"In formal epistemology, group knowledge is often modelled as the knowledge that the group would have, if the agents shared all their individual knowledge. However, this interpretation does not account for relations between agents. In this work, we propose the notion of synergistic knowledge which makes it possible to model those relationships.","sentences":["In formal epistemology, group knowledge is often modelled as the knowledge that the group would have, if the agents shared all their individual knowledge.","However, this interpretation does not account for relations between agents.","In this work, we propose the notion of synergistic knowledge which makes it possible to model those relationships."],"url":"http://arxiv.org/abs/2403.18646v1","category":"cs.LO"}
{"created":"2024-03-27 14:32:56","title":"Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures","abstract":"Graphs have become a key tool when modeling and solving problems in different areas. The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a graph and is employed in areas like communication networking, traffic routing, bioinformatics, among others. However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space. As the graph gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range. In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former. To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios. In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms. The experimental results show that all optimizations were beneficial on the two x86 platforms selected. Last, the new optimization proposal improved performance by up to 23%.","sentences":["Graphs have become a key tool when modeling and solving problems in different areas.","The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a graph and is employed in areas like communication networking, traffic routing, bioinformatics, among others.","However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space.","As the graph gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range.","In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former.","To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios.","In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms.","The experimental results show that all optimizations were beneficial on the two x86 platforms selected.","Last, the new optimization proposal improved performance by up to 23%."],"url":"http://arxiv.org/abs/2403.18619v1","category":"cs.DC"}
{"created":"2024-03-27 14:19:36","title":"Conditions for Relativistic Magnetic Reconnection under the Presence of Shear Flow and Guide Field","abstract":"The scaling of the relativistic reconnection outflow speed is studied in the presence of both shear flows parallel to the reconnecting magnetic fields and guide fields pointing out of the reconnection plane. In nonrelativistic reconnection, super-Alfv\\'enic shear flows have been found to suppress reconnection. We extend the analytical model of this phenomenon to the relativistic regime and find similar behavior, which is confirmed by particle-in-cell simulations. Unlike the nonrelativistic limit, the addition of a guide field lowers the in-plane Alfv\\'en velocity, contributing to slower outflow jets and the more efficient suppression of reconnection in strongly magnetized plasmas.","sentences":["The scaling of the relativistic reconnection outflow speed is studied in the presence of both shear flows parallel to the reconnecting magnetic fields and guide fields pointing out of the reconnection plane.","In nonrelativistic reconnection, super-Alfv\\'enic shear flows have been found to suppress reconnection.","We extend the analytical model of this phenomenon to the relativistic regime and find similar behavior, which is confirmed by particle-in-cell simulations.","Unlike the nonrelativistic limit, the addition of a guide field lowers the in-plane Alfv\\'en velocity, contributing to slower outflow jets and the more efficient suppression of reconnection in strongly magnetized plasmas."],"url":"http://arxiv.org/abs/2403.18595v1","category":"physics.plasm-ph"}
{"created":"2024-03-27 14:11:23","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision","abstract":"Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normalization, and ReLU activation. Based on these insights, we propose two new simple, yet effective strategies for crafting sponge examples: sampling images from a probability distribution and identifying dense, yet inconspicuous inputs in natural datasets. We empirically examine our findings in a comprehensive evaluation with multiple image classification models and show that our attack achieves the same sparsity effect as prior sponge-example methods, but at a fraction of computation effort. We also show that our sponge examples transfer between different neural networks. Finally, we discuss applications of our findings for the good by improving efficiency by increasing sparsity.","sentences":["Resource efficiency plays an important role for machine learning nowadays.","The energy and decision latency are two critical aspects to ensure a sustainable and practical application.","Unfortunately, the energy consumption and decision latency are not robust against adversaries.","Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks.","In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation.","In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity.","In particular, we find that input uniformity is a key enabler.","A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normalization, and ReLU activation.","Based on these insights, we propose two new simple, yet effective strategies for crafting sponge examples: sampling images from a probability distribution and identifying dense, yet inconspicuous inputs in natural datasets.","We empirically examine our findings in a comprehensive evaluation with multiple image classification models and show that our attack achieves the same sparsity effect as prior sponge-example methods, but at a fraction of computation effort.","We also show that our sponge examples transfer between different neural networks.","Finally, we discuss applications of our findings for the good by improving efficiency by increasing sparsity."],"url":"http://arxiv.org/abs/2403.18587v1","category":"cs.CR"}
{"created":"2024-03-27 14:06:00","title":"Erosion rate of lunar soil under a landing rocket, part 2: benchmarking and predictions","abstract":"In the companion paper (\"Erosion rate of lunar soil under a landing rocket, part 1: identifying the rate-limiting physics\", this issue) an equation was developed for the rate that lunar soil erodes under the exhaust of a landing rocket. That equation has only one parameter that is not calibrated from first principles, so here it is calibrated by the blowing soil's optical density curve during an Apollo landing. An excellent fit is obtained, helping validate the equation. However, when extrapolating the erosion rate all the way to touchdown on the lunar surface, a soil model is needed to handle the increased resistance to erosion as the deeper, more compacted soil is exposed. Relying on models derived from Apollo measurements and from Lunar Reconnaissance Orbiter (LRO) Diviner thermal inertia measurements, only one additional soil parameter is unknown: the scale of increasing cohesive energy with soil compaction. Treating this as an additional fitting parameter results in some degeneracy in the solutions, but the depth of erosion scour in the post-landing imagery provides an additional constraint on the solution. The results show that about 4 to 10 times more soil was blown in each Apollo landing than previously believed, so the potential for sandblasting damage is worse than prior estimates. This also shows that, with further development, instruments to measure the soil erosion during lunar landings can constrain the soil column's density profile complementary to the thermal inertia measurements, providing insight into the landing site's geology.","sentences":["In the companion paper (\"Erosion rate of lunar soil under a landing rocket, part 1: identifying the rate-limiting physics\", this issue) an equation was developed for the rate that lunar soil erodes under the exhaust of a landing rocket.","That equation has only one parameter that is not calibrated from first principles, so here it is calibrated by the blowing soil's optical density curve during an Apollo landing.","An excellent fit is obtained, helping validate the equation.","However, when extrapolating the erosion rate all the way to touchdown on the lunar surface, a soil model is needed to handle the increased resistance to erosion as the deeper, more compacted soil is exposed.","Relying on models derived from Apollo measurements and from Lunar Reconnaissance Orbiter (LRO) Diviner thermal inertia measurements, only one additional soil parameter is unknown: the scale of increasing cohesive energy with soil compaction.","Treating this as an additional fitting parameter results in some degeneracy in the solutions, but the depth of erosion scour in the post-landing imagery provides an additional constraint on the solution.","The results show that about 4 to 10 times more soil was blown in each Apollo landing than previously believed, so the potential for sandblasting damage is worse than prior estimates.","This also shows that, with further development, instruments to measure the soil erosion during lunar landings can constrain the soil column's density profile complementary to the thermal inertia measurements, providing insight into the landing site's geology."],"url":"http://arxiv.org/abs/2403.18584v1","category":"astro-ph.EP"}
{"created":"2024-03-27 14:05:48","title":"Erosion rate of lunar soil under a landing rocket, part 1: identifying the rate-limiting physics","abstract":"Multiple nations are planning activity on the Moon's surface, and to deconflict lunar operations we must understand the sandblasting damage from rocket exhaust blowing soil. Prior research disagreed over the scaling of the erosion rate, which determines the magnitude of the damage. Reduced gravity experiments and two other lines of evidence now indicate that the erosion rate scales with the kinetic energy flux at the bottom of the laminar sublayer of the gas. Because the rocket exhaust is so fast, eroded particles lifted higher in the boundary layer do not impact the surface for kilometers (if at all; some leave the Moon entirely), so there is no saltation in the vicinity of the gas. As a result, there is little transport of gas kinetic energy from higher in the boundary layer down to the surface, so the emission of soil into the gas is a surprisingly low energy process. In low lunar gravity, a dominant source of resistance to this small energy flux turns out to be the cohesive energy density of the lunar soil, which arises primarily from particles in the 0.3 to 3 micron size range. These particles constitute only a tiny fraction of the mass of lunar soil and have been largely ignored in most studies, so they are poorly characterized.","sentences":["Multiple nations are planning activity on the Moon's surface, and to deconflict lunar operations we must understand the sandblasting damage from rocket exhaust blowing soil.","Prior research disagreed over the scaling of the erosion rate, which determines the magnitude of the damage.","Reduced gravity experiments and two other lines of evidence now indicate that the erosion rate scales with the kinetic energy flux at the bottom of the laminar sublayer of the gas.","Because the rocket exhaust is so fast, eroded particles lifted higher in the boundary layer do not impact the surface for kilometers (if at all; some leave the Moon entirely), so there is no saltation in the vicinity of the gas.","As a result, there is little transport of gas kinetic energy from higher in the boundary layer down to the surface, so the emission of soil into the gas is a surprisingly low energy process.","In low lunar gravity, a dominant source of resistance to this small energy flux turns out to be the cohesive energy density of the lunar soil, which arises primarily from particles in the 0.3 to 3 micron size range.","These particles constitute only a tiny fraction of the mass of lunar soil and have been largely ignored in most studies, so they are poorly characterized."],"url":"http://arxiv.org/abs/2403.18583v1","category":"astro-ph.EP"}
{"created":"2024-03-27 14:01:58","title":"Qubit teleportation between a memory-compatible photonic time-bin qubit and a solid-state quantum network node","abstract":"We report on a quantum interface linking a diamond NV center quantum network node and 795nm photonic time-bin qubits compatible with Thulium and Rubidium quantum memories. The interface makes use of two-stage low-noise quantum frequency conversion and waveform shaping to match temporal and spectral photon profiles. Two-photon quantum interference shows high indistinguishability of (89.5 $\\pm$ 1.9)% between converted 795nm photons and the native NV center photons. We use the interface to demonstrate quantum teleportation including real-time feedforward from an unbiased set of 795nm photonic qubit input states to the NV center spin qubit, achieving a teleportation fidelity of (75.5 $\\pm$ 1.0)%. This proof-of-concept experiment shows the feasibility of interconnecting different quantum network hardware.","sentences":["We report on a quantum interface linking a diamond NV center quantum network node and 795nm photonic time-bin qubits compatible with Thulium and Rubidium quantum memories.","The interface makes use of two-stage low-noise quantum frequency conversion and waveform shaping to match temporal and spectral photon profiles.","Two-photon quantum interference shows high indistinguishability of (89.5 $\\pm$ 1.9)% between converted 795nm photons and the native NV center photons.","We use the interface to demonstrate quantum teleportation including real-time feedforward from an unbiased set of 795nm photonic qubit input states to the NV center spin qubit, achieving a teleportation fidelity of (75.5 $\\pm$ 1.0)%.","This proof-of-concept experiment shows the feasibility of interconnecting different quantum network hardware."],"url":"http://arxiv.org/abs/2403.18581v1","category":"quant-ph"}
{"created":"2024-03-27 13:54:45","title":"Classifying symmetric and symmetry-broken spin chain phases with anomalous group actions","abstract":"We consider the classification problem of quantum spin chains invariant under local decomposable group actions, covering matrix product unitaries (MPUs), using an operator algebraic approach. We focus on finite group symmetries hosting both symmetric and symmetry broken phases. The local-decomposable group actions we consider have a 3-cocycle class of the symmetry group associated to them. We derive invariants for our classification that naturally cover one-dimensional symmetry protected topological (SPT) phases. We prove that these invariants coincide with the ones of [J. Garre Rubio et al, Quantum 7, 927 (2023)] using matrix product states (MPSs) techniques, by explicitly working out the GNS representation of MPSs and MPUs, resulting in a useful dictionary between both approaches that could be of independent interest.","sentences":["We consider the classification problem of quantum spin chains invariant under local decomposable group actions, covering matrix product unitaries (MPUs), using an operator algebraic approach.","We focus on finite group symmetries hosting both symmetric and symmetry broken phases.","The local-decomposable group actions we consider have a 3-cocycle class of the symmetry group associated to them.","We derive invariants for our classification that naturally cover one-dimensional symmetry protected topological (SPT) phases.","We prove that these invariants coincide with the ones of [J. Garre Rubio et al, Quantum 7, 927 (2023)] using matrix product states (MPSs) techniques, by explicitly working out the GNS representation of MPSs and MPUs, resulting in a useful dictionary between both approaches that could be of independent interest."],"url":"http://arxiv.org/abs/2403.18573v1","category":"quant-ph"}
{"created":"2024-03-27 13:30:21","title":"A communication-efficient, online changepoint detection method for monitoring distributed sensor networks","abstract":"We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud. We propose an online, communication-efficient method to detect such changes. The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively. We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes. Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs.","sentences":["We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud.","We propose an online, communication-efficient method to detect such changes.","The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively.","We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes.","Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs."],"url":"http://arxiv.org/abs/2403.18549v1","category":"stat.ME"}
{"created":"2024-03-27 13:22:38","title":"Attention-aware semantic relevance predicting Chinese sentence reading","abstract":"In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the presence of semantic preview benefits in Chinese naturalistic reading. Furthermore, the attention-aware metrics of semantic relevance, being memory-based, possess high interpretability from both linguistic and cognitive standpoints, making them a valuable computational tool for modeling eye-movements in reading and further gaining insight into the process of language comprehension. Our approach underscores the potential of these metrics to advance our comprehension of how humans understand and process language, ultimately leading to a better understanding of language comprehension and processing.","sentences":["In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence.","One particularly promising approach is contextual semantic similarity.","Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance.","This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully.","The attention-aware approach also facilitates the simulation of existing reading models and evaluate them.","The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches.","The study's findings further provide strong support for the presence of semantic preview benefits in Chinese naturalistic reading.","Furthermore, the attention-aware metrics of semantic relevance, being memory-based, possess high interpretability from both linguistic and cognitive standpoints, making them a valuable computational tool for modeling eye-movements in reading and further gaining insight into the process of language comprehension.","Our approach underscores the potential of these metrics to advance our comprehension of how humans understand and process language, ultimately leading to a better understanding of language comprehension and processing."],"url":"http://arxiv.org/abs/2403.18542v1","category":"cs.CL"}
{"created":"2024-03-27 12:59:57","title":"Number of aftershocks in epidemic-type seismicity models","abstract":"Let V(M, m0) be the number of m>M aftershocks caused by m0 event. We consider the V(M, m0) distribution within epidemic-type seismicity models, ETAS(F). These models include the Gutenberg-Richter law for magnitude and Utsu law for average productivity of m0, but differ in the type of F distribution for the number v(m0) of direct aftershocks. The class of F is quite broad and includes both the Poisson distribution, which is the basis for the regular ETAS model, and its possible alternative, the Geometric distribution. Instead of the traditional threshold M=m0-d we consider M=ma-d, where ma is the mode in the distribution of the strongest aftershock. Under these conditions we find the limit V(M, m0) distribution at m0>>1. In the subcritical case, the limit distribution is extremely simple and identical to the v(m) distribution with a suitable magnitude m=m(d). Theoretical results of this kind are lacking even for the regular ETAS model. Our results provide an additional opportunity to test the type of F-distribution, Bath law, and the very concept of epidemic-type clustering.","sentences":["Let V(M, m0) be the number of m>M aftershocks caused by m0 event.","We consider the V(M, m0) distribution within epidemic-type seismicity models, ETAS(F).","These models include the Gutenberg-Richter law for magnitude and Utsu law for average productivity of m0, but differ in the type of F distribution for the number v(m0) of direct aftershocks.","The class of F is quite broad and includes both the Poisson distribution, which is the basis for the regular ETAS model, and its possible alternative, the Geometric distribution.","Instead of the traditional threshold M=m0-d we consider M=ma-d, where ma is the mode in the distribution of the strongest aftershock.","Under these conditions we find the limit V(M, m0) distribution at m0>>1.","In the subcritical case, the limit distribution is extremely simple and identical to the v(m) distribution with a suitable magnitude m=m(d).","Theoretical results of this kind are lacking even for the regular ETAS model.","Our results provide an additional opportunity to test the type of F-distribution, Bath law, and the very concept of epidemic-type clustering."],"url":"http://arxiv.org/abs/2403.18526v1","category":"physics.geo-ph"}
{"created":"2024-03-27 11:45:08","title":"Modeling uncertainty for Gaussian Splatting","abstract":"We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications.","sentences":["We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS).","GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF).","However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs.","To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS.","Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction.","Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy.","Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications."],"url":"http://arxiv.org/abs/2403.18476v1","category":"cs.CV"}
{"created":"2024-03-27 11:01:15","title":"Asymptotic Analysis of Synchronous Signal Processing","abstract":"This paper extends various theoretical results from stationary data processing to cyclostationary (CS) processes under a unified framework. We first derive their asymptotic eigenbasis, which provides a link between their Fourier and Karhunen-Lo\\`eve (KL) expansions, through a unitary transformation dictated by the cyclic spectrum. By exploiting this connection and the optimalities offered by the KL representation, we study the asymptotic performance of smoothing, filtering and prediction of CS processes, without the need for deriving explicit implementations. We obtain minimum mean squared error expressions that depend on the cyclic spectrum and include classical limits based on the power spectral density as particular cases. We conclude this work by applying the results to a practical scenario, in order to quantify the achievable gains of synchronous signal processing.","sentences":["This paper extends various theoretical results from stationary data processing to cyclostationary (CS) processes under a unified framework.","We first derive their asymptotic eigenbasis, which provides a link between their Fourier and Karhunen-Lo\\`eve (KL) expansions, through a unitary transformation dictated by the cyclic spectrum.","By exploiting this connection and the optimalities offered by the KL representation, we study the asymptotic performance of smoothing, filtering and prediction of CS processes, without the need for deriving explicit implementations.","We obtain minimum mean squared error expressions that depend on the cyclic spectrum and include classical limits based on the power spectral density as particular cases.","We conclude this work by applying the results to a practical scenario, in order to quantify the achievable gains of synchronous signal processing."],"url":"http://arxiv.org/abs/2403.18445v1","category":"eess.SP"}
{"created":"2024-03-27 10:39:45","title":"Poisson Regression in one Covariate on Massive Data","abstract":"The goal of subsampling is to select an informative subset of all observations, when using the full data for statistical analysis is not viable. We construct locally $ D $-optimal subsampling designs under a Poisson regression model with a log link in one covariate. A Representation of the support of locally $ D $-optimal subsampling designs is established. We make statements on scale-location transformations of the covariate that require a simultaneous transformation of the regression parameter. The performance of the methods is demonstrated by illustrating examples. To show the advantage of the optimal subsampling designs, we examine the efficiency of uniform random subsampling as well as of two heuristic designs. Further, the efficiency of locally $ D $-optimal subsampling designs is studied when the parameter is misspecified.","sentences":["The goal of subsampling is to select an informative subset of all observations, when using the full data for statistical analysis is not viable.","We construct locally $ D $-optimal subsampling designs under a Poisson regression model with a log link in one covariate.","A Representation of the support of locally $ D $-optimal subsampling designs is established.","We make statements on scale-location transformations of the covariate that require a simultaneous transformation of the regression parameter.","The performance of the methods is demonstrated by illustrating examples.","To show the advantage of the optimal subsampling designs, we examine the efficiency of uniform random subsampling as well as of two heuristic designs.","Further, the efficiency of locally $ D $-optimal subsampling designs is studied when the parameter is misspecified."],"url":"http://arxiv.org/abs/2403.18432v1","category":"math.ST"}
{"created":"2024-03-27 10:35:41","title":"Reinforcement learning for graph theory, I. Reimplementation of Wagner's approach","abstract":"We reimplement here the recent approach of Adam Zsolt Wagner [arXiv:2104.14516], which applies reinforcement learning to construct (counter)examples in graph theory, in order to make it more readable, more stable and much faster. The presented concepts are illustrated by constructing counterexamples for a number of published conjectured bounds for the Laplacian spectral radius of graphs.","sentences":["We reimplement here the recent approach of Adam Zsolt Wagner [arXiv:2104.14516], which applies reinforcement learning to construct (counter)examples in graph theory, in order to make it more readable, more stable and much faster.","The presented concepts are illustrated by constructing counterexamples for a number of published conjectured bounds for the Laplacian spectral radius of graphs."],"url":"http://arxiv.org/abs/2403.18429v1","category":"math.CO"}
{"created":"2024-03-27 09:51:47","title":"$J_{eff}$ states in a quasi one dimensional antiferromagnetic spin chain hexagonal Iridates Sr$_3$MIrO$_6$ (M=Mg, Zn, Cd): an $ab-initio$ comparative perspective","abstract":"We employ first-principles density-functional theory, to perform a comparative investigation of the effect of the spin-orbit coupling (SOC) on the electronic and magnetic properties of three experimentally synthesized and characterized hexagonal perovskites Sr$_3$MIrO$_6$(M=Mg, Zn, Cd). The electronic structure calculations show that in all the compounds, Ir is the only magnetically active site in +4[5$d^5$] configuration, whereas M$^{+2}$ (M=Cd, Zn, Mg), remains in nonmagnetic states with Cd/Zn and Mg featuring $d^{10}$ and $d^{0}$ electronic configurations, respectively. The insulating gap could be opened by switching on the correlation parameter $U$ for Sr$_3$CdrO$_6$ and Sr$_3$ZnIrO$_6$ which qualifies it to be a correlated Mott insulator. However, in the case of Sr$_3$MgIrO$_6$ both $U$ and antiferromagnetic ordering is not enough and the gap could only be opened by including the SOC which classifies it to fall under the category of a typical SOC Mott insulator. The $j_{eff}$ states are visualized from the orbital projected band structure. The magnetism is studied from the point of view of exchange interactions and magnetocrystalline anisotropy in the presence of the SOC. We also present the comparative analysis of the renormalized impact of SOC on the three compounds, which shows that all the three compounds fall under the $intermediate$ coupling regime, where Sr$_3$MgIrO$_6$ is comparatively closer to the atomic $j_{eff}=\\frac{1}{2}$ picture from the others.","sentences":["We employ first-principles density-functional theory, to perform a comparative investigation of the effect of the spin-orbit coupling (SOC) on the electronic and magnetic properties of three experimentally synthesized and characterized hexagonal perovskites Sr$_3$MIrO$_6$(M=Mg, Zn, Cd).","The electronic structure calculations show that in all the compounds, Ir is the only magnetically active site in +4[5$d^5$] configuration, whereas M$^{+2}$ (M=Cd, Zn, Mg), remains in nonmagnetic states with Cd/Zn and Mg featuring $d^{10}$ and $d^{0}$ electronic configurations, respectively.","The insulating gap could be opened by switching on the correlation parameter $U$ for Sr$_3$CdrO$_6$ and Sr$_3$ZnIrO$_6$ which qualifies it to be a correlated Mott insulator.","However, in the case of Sr$_3$MgIrO$_6$ both $U$ and antiferromagnetic ordering is not enough and the gap could only be opened by including the SOC which classifies it to fall under the category of a typical SOC Mott insulator.","The $j_{eff}$ states are visualized from the orbital projected band structure.","The magnetism is studied from the point of view of exchange interactions and magnetocrystalline anisotropy in the presence of the SOC.","We also present the comparative analysis of the renormalized impact of SOC on the three compounds, which shows that all the three compounds fall under the $intermediate$ coupling regime, where Sr$_3$MgIrO$_6$ is comparatively closer to the atomic $j_{eff}=\\frac{1}{2}$ picture from the others."],"url":"http://arxiv.org/abs/2403.18408v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 09:30:50","title":"Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering","abstract":"Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic structure among data points. By making an assumption that the learned neighbor graph of each view comprises both a consistent graph and a view-specific graph, we formulate a new tensor-based target graph learning paradigm. Owing to the benefits of tensor singular value decomposition (t-SVD) in uncovering high-order correlations, this model is capable of achieving a complete understanding of the target graph. Furthermore, we develop an iterative algorithm to solve the proposed objective optimization problem. Experiments conducted on real-world datasets have demonstrated the superior performance of the proposed method over some state-of-the-art multi-view clustering methods. The source code has been released on https://github.com/lshi91/CSTGL-Code.","sentences":["Graph learning is widely recognized as a crucial technique in multi-view clustering.","Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations.","Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios.","Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information.","In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering.","Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic structure among data points.","By making an assumption that the learned neighbor graph of each view comprises both a consistent graph and a view-specific graph, we formulate a new tensor-based target graph learning paradigm.","Owing to the benefits of tensor singular value decomposition (t-SVD) in uncovering high-order correlations, this model is capable of achieving a complete understanding of the target graph.","Furthermore, we develop an iterative algorithm to solve the proposed objective optimization problem.","Experiments conducted on real-world datasets have demonstrated the superior performance of the proposed method over some state-of-the-art multi-view clustering methods.","The source code has been released on https://github.com/lshi91/CSTGL-Code."],"url":"http://arxiv.org/abs/2403.18393v1","category":"cs.LG"}
{"created":"2024-03-27 09:14:36","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates","abstract":"Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated independently with a different contributing set of users. We provide a theoretical analysis, establishing convergence guarantees for the global model under mild assumptions on the distribution of the participating devices, revealing that SALF converges at the same asymptotic rate as FL with no timing limitations. This insight is matched with empirical observations, demonstrating the performance gains of SALF compared to alternative mechanisms mitigating the device heterogeneity gap in FL.","sentences":["Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning.","It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations.","As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers.","Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints.","In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion.","SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated independently with a different contributing set of users.","We provide a theoretical analysis, establishing convergence guarantees for the global model under mild assumptions on the distribution of the participating devices, revealing that SALF converges at the same asymptotic rate as FL with no timing limitations.","This insight is matched with empirical observations, demonstrating the performance gains of SALF compared to alternative mechanisms mitigating the device heterogeneity gap in FL."],"url":"http://arxiv.org/abs/2403.18375v1","category":"cs.LG"}
{"created":"2024-03-27 08:52:44","title":"Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation","abstract":"Most domain adaptation (DA) methods are based on either a convolutional neural networks (CNNs) or a vision transformers (ViTs). They align the distribution differences between domains as encoders without considering their unique characteristics. For instance, ViT excels in accuracy due to its superior ability to capture global representations, while CNN has an advantage in capturing local representations. This fact has led us to design a hybrid method to fully take advantage of both ViT and CNN, called Explicitly Class-specific Boundaries (ECB). ECB learns CNN on ViT to combine their distinct strengths. In particular, we leverage ViT's properties to explicitly find class-specific decision boundaries by maximizing the discrepancy between the outputs of the two classifiers to detect target samples far from the source support. In contrast, the CNN encoder clusters target features based on the previously defined class-specific boundaries by minimizing the discrepancy between the probabilities of the two classifiers. Finally, ViT and CNN mutually exchange knowledge to improve the quality of pseudo labels and reduce the knowledge discrepancies of these models. Compared to conventional DA methods, our ECB achieves superior performance, which verifies its effectiveness in this hybrid model. The project website can be found https://dotrannhattuong.github.io/ECB/website/.","sentences":["Most domain adaptation (DA) methods are based on either a convolutional neural networks (CNNs) or a vision transformers (ViTs).","They align the distribution differences between domains as encoders without considering their unique characteristics.","For instance, ViT excels in accuracy due to its superior ability to capture global representations, while CNN has an advantage in capturing local representations.","This fact has led us to design a hybrid method to fully take advantage of both ViT and CNN, called Explicitly Class-specific Boundaries (ECB).","ECB learns CNN on ViT to combine their distinct strengths.","In particular, we leverage ViT's properties to explicitly find class-specific decision boundaries by maximizing the discrepancy between the outputs of the two classifiers to detect target samples far from the source support.","In contrast, the CNN encoder clusters target features based on the previously defined class-specific boundaries by minimizing the discrepancy between the probabilities of the two classifiers.","Finally, ViT and CNN mutually exchange knowledge to improve the quality of pseudo labels and reduce the knowledge discrepancies of these models.","Compared to conventional DA methods, our ECB achieves superior performance, which verifies its effectiveness in this hybrid model.","The project website can be found https://dotrannhattuong.github.io/ECB/website/."],"url":"http://arxiv.org/abs/2403.18360v1","category":"cs.CV"}
{"created":"2024-03-27 08:49:30","title":"Imaging radar and LiDAR image translation for 3-DOF extrinsic calibration","abstract":"The integration of sensor data is crucial in the field of robotics to take full advantage of the various sensors employed. One critical aspect of this integration is determining the extrinsic calibration parameters, such as the relative transformation, between each sensor. The use of data fusion between complementary sensors, such as radar and LiDAR, can provide significant benefits, particularly in harsh environments where accurate depth data is required. However, noise included in radar sensor data can make the estimation of extrinsic calibration challenging. To address this issue, we present a novel framework for the extrinsic calibration of radar and LiDAR sensors, utilizing CycleGAN as amethod of image-to-image translation. Our proposed method employs translating radar bird-eye-view images into LiDAR-style images to estimate the 3-DOF extrinsic parameters. The use of image registration techniques, as well as deskewing based on sensor odometry and B-spline interpolation, is employed to address the rolling shutter effect commonly present in spinning sensors. Our method demonstrates a notable improvement in extrinsic calibration compared to filter-based methods using the MulRan dataset.","sentences":["The integration of sensor data is crucial in the field of robotics to take full advantage of the various sensors employed.","One critical aspect of this integration is determining the extrinsic calibration parameters, such as the relative transformation, between each sensor.","The use of data fusion between complementary sensors, such as radar and LiDAR, can provide significant benefits, particularly in harsh environments where accurate depth data is required.","However, noise included in radar sensor data can make the estimation of extrinsic calibration challenging.","To address this issue, we present a novel framework for the extrinsic calibration of radar and LiDAR sensors, utilizing CycleGAN as amethod of image-to-image translation.","Our proposed method employs translating radar bird-eye-view images into LiDAR-style images to estimate the 3-DOF extrinsic parameters.","The use of image registration techniques, as well as deskewing based on sensor odometry and B-spline interpolation, is employed to address the rolling shutter effect commonly present in spinning sensors.","Our method demonstrates a notable improvement in extrinsic calibration compared to filter-based methods using the MulRan dataset."],"url":"http://arxiv.org/abs/2403.18358v1","category":"cs.RO"}
{"created":"2024-03-27 08:49:17","title":"Minimax density estimation in the adversarial framework under local differential privacy","abstract":"We consider the problem of nonparametric density estimation under privacy constraints in an adversarial framework. To this end, we study minimax rates under local differential privacy over Sobolev spaces. We first obtain a lower bound which allows us to quantify the impact of privacy compared with the classical framework. Next, we introduce a new Coordinate block privacy mechanism that guarantees local differential privacy, which, coupled with a projection estimator, achieves the minimax optimal rates.","sentences":["We consider the problem of nonparametric density estimation under privacy constraints in an adversarial framework.","To this end, we study minimax rates under local differential privacy over Sobolev spaces.","We first obtain a lower bound which allows us to quantify the impact of privacy compared with the classical framework.","Next, we introduce a new Coordinate block privacy mechanism that guarantees local differential privacy, which, coupled with a projection estimator, achieves the minimax optimal rates."],"url":"http://arxiv.org/abs/2403.18357v1","category":"math.ST"}
{"created":"2024-03-27 08:44:40","title":"Early Stopping for Ensemble Kalman-Bucy Inversion","abstract":"Bayesian linear inverse problems aim to recover an unknown signal from noisy observations, incorporating prior knowledge. This paper analyses a data dependent method to choose the scale parameter of a Gaussian prior. The method we study arises from early stopping methods, which have been successfully applied to a range of problems for statistical inverse problems in the frequentist setting. These results are extended to the Bayesian setting. We study the use of a discrepancy based stopping rule in the setting of random noise. Our proposed stopping rule results in optimal rates under certain conditions on the prior covariance operator. We furthermore derive for which class of signals this method is adaptive. It is also shown that the associated posterior contracts at the optimal rate and provides a conservative measure of uncertainty. We implement the proposed stopping rule using the continuous-time ensemble Kalman--Bucy filter (EnKBF). The fictitious time parameter replaces the scale parameter, and the ensemble size is appropriately adjusted in order to not lose statistical optimality of the computed estimator. The EnKBF, then, gives a continuous process from the prior distribution to the posterior which is terminated using the proposed stopping rule.","sentences":["Bayesian linear inverse problems aim to recover an unknown signal from noisy observations, incorporating prior knowledge.","This paper analyses a data dependent method to choose the scale parameter of a Gaussian prior.","The method we study arises from early stopping methods, which have been successfully applied to a range of problems for statistical inverse problems in the frequentist setting.","These results are extended to the Bayesian setting.","We study the use of a discrepancy based stopping rule in the setting of random noise.","Our proposed stopping rule results in optimal rates under certain conditions on the prior covariance operator.","We furthermore derive for which class of signals this method is adaptive.","It is also shown that the associated posterior contracts at the optimal rate and provides a conservative measure of uncertainty.","We implement the proposed stopping rule using the continuous-time ensemble Kalman--Bucy filter (EnKBF).","The fictitious time parameter replaces the scale parameter, and the ensemble size is appropriately adjusted in order to not lose statistical optimality of the computed estimator.","The EnKBF, then, gives a continuous process from the prior distribution to the posterior which is terminated using the proposed stopping rule."],"url":"http://arxiv.org/abs/2403.18353v1","category":"math.ST"}
{"created":"2024-03-27 08:09:04","title":"PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans","abstract":"Information from neuroimaging examinations (CT, MRI) is increasingly used to support diagnoses of dementia, e.g., Alzheimer's disease. While current clinical practice is mainly based on visual inspection and feature engineering, Deep Learning approaches can be used to automate the analysis and to discover new image-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to standard blackbox models, and have shown promising results in general computer vision. PP-NN's base their reasoning on prototypical image regions that are learned fully unsupervised, and combined with a simple-to-understand decision layer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from structural Magnetic Resonance Imaging (sMRI). We assess the quality of prototypes under a systematic evaluation framework, propose new metrics to evaluate brain prototypes and perform an evaluation with domain experts. Our results show that PIPNet3D is an interpretable, compact model for Alzheimer's diagnosis with its reasoning well aligned to medical domain knowledge. Notably, PIPNet3D achieves the same accuracy as its blackbox counterpart; and removing the remaining clinically irrelevant prototypes from its decision process does not decrease predictive performance.","sentences":["Information from neuroimaging examinations (CT, MRI) is increasingly used to support diagnoses of dementia, e.g., Alzheimer's disease.","While current clinical practice is mainly based on visual inspection and feature engineering, Deep Learning approaches can be used to automate the analysis and to discover new image-biomarkers.","Part-prototype neural networks (PP-NN) are an alternative to standard blackbox models, and have shown promising results in general computer vision.","PP-NN's base their reasoning on prototypical image regions that are learned fully unsupervised, and combined with a simple-to-understand decision layer.","We present PIPNet3D, a PP-NN for volumetric images.","We apply PIPNet3D to the clinical case study of Alzheimer's Disease diagnosis from structural Magnetic Resonance Imaging (sMRI).","We assess the quality of prototypes under a systematic evaluation framework, propose new metrics to evaluate brain prototypes and perform an evaluation with domain experts.","Our results show that PIPNet3D is an interpretable, compact model for Alzheimer's diagnosis with its reasoning well aligned to medical domain knowledge.","Notably, PIPNet3D achieves the same accuracy as its blackbox counterpart; and removing the remaining clinically irrelevant prototypes from its decision process does not decrease predictive performance."],"url":"http://arxiv.org/abs/2403.18328v1","category":"cs.CV"}
{"created":"2024-03-27 07:50:45","title":"Implementation of the Principal Component Analysis onto High-Performance Computer Facilities for Hyperspectral Dimensionality Reduction: Results and Comparisons","abstract":"Dimensionality reduction represents a critical preprocessing step in order to increase the efficiency and the performance of many hyperspectral imaging algorithms. However, dimensionality reduction algorithms, such as the Principal Component Analysis (PCA), suffer from their computationally demanding nature, becoming advisable for their implementation onto high-performance computer architectures for applications under strict latency constraints. This work presents the implementation of the PCA algorithm onto two different high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and a Kalray manycore, uncovering a highly valuable set of tips and tricks in order to take full advantage of the inherent parallelism of these high-performance computing platforms, and hence, reducing the time that is required to process a given hyperspectral image. Moreover, the achieved results obtained with different hyperspectral images have been compared with the ones that were obtained with a field programmable gate array (FPGA)-based implementation of the PCA algorithm that has been recently published, providing, for the first time in the literature, a comprehensive analysis in order to highlight the pros and cons of each option.","sentences":["Dimensionality reduction represents a critical preprocessing step in order to increase the efficiency and the performance of many hyperspectral imaging algorithms.","However, dimensionality reduction algorithms, such as the Principal Component Analysis (PCA), suffer from their computationally demanding nature, becoming advisable for their implementation onto high-performance computer architectures for applications under strict latency constraints.","This work presents the implementation of the PCA algorithm onto two different high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and a Kalray manycore, uncovering a highly valuable set of tips and tricks in order to take full advantage of the inherent parallelism of these high-performance computing platforms, and hence, reducing the time that is required to process a given hyperspectral image.","Moreover, the achieved results obtained with different hyperspectral images have been compared with the ones that were obtained with a field programmable gate array (FPGA)-based implementation of the PCA algorithm that has been recently published, providing, for the first time in the literature, a comprehensive analysis in order to highlight the pros and cons of each option."],"url":"http://arxiv.org/abs/2403.18321v1","category":"cs.LG"}
{"created":"2024-03-27 07:16:48","title":"Bayesian Learned Models Can Detect Adversarial Malware For Free","abstract":"The vulnerability of machine learning-based malware detectors to adversarial attacks has prompted the need for robust solutions. Adversarial training is an effective method but is computationally expensive to scale up to large datasets and comes at the cost of sacrificing model performance for robustness. We hypothesize that adversarial malware exploits the low-confidence regions of models and can be identified using epistemic uncertainty of ML approaches -- epistemic uncertainty in a machine learning-based malware detector is a result of a lack of similar training samples in regions of the problem space. In particular, a Bayesian formulation can capture the model parameters' distribution and quantify epistemic uncertainty without sacrificing model performance. To verify our hypothesis, we consider Bayesian learning approaches with a mutual information-based formulation to quantify uncertainty and detect adversarial malware in Android, Windows domains and PDF malware. We found, quantifying uncertainty through Bayesian learning methods can defend against adversarial malware. In particular, Bayesian models: (1) are generally capable of identifying adversarial malware in both feature and problem space, (2) can detect concept drift by measuring uncertainty, and (3) with a diversity-promoting approach (or better posterior approximations) lead to parameter instances from the posterior to significantly enhance a detectors' ability.","sentences":["The vulnerability of machine learning-based malware detectors to adversarial attacks has prompted the need for robust solutions.","Adversarial training is an effective method but is computationally expensive to scale up to large datasets and comes at the cost of sacrificing model performance for robustness.","We hypothesize that adversarial malware exploits the low-confidence regions of models and can be identified using epistemic uncertainty of ML approaches -- epistemic uncertainty in a machine learning-based malware detector is a result of a lack of similar training samples in regions of the problem space.","In particular, a Bayesian formulation can capture the model parameters' distribution and quantify epistemic uncertainty without sacrificing model performance.","To verify our hypothesis, we consider Bayesian learning approaches with a mutual information-based formulation to quantify uncertainty and detect adversarial malware in Android, Windows domains and PDF malware.","We found, quantifying uncertainty through Bayesian learning methods can defend against adversarial malware.","In particular, Bayesian models: (1) are generally capable of identifying adversarial malware in both feature and problem space, (2) can detect concept drift by measuring uncertainty, and (3) with a diversity-promoting approach (or better posterior approximations) lead to parameter instances from the posterior to significantly enhance a detectors' ability."],"url":"http://arxiv.org/abs/2403.18309v1","category":"cs.CR"}
{"created":"2024-03-27 06:59:20","title":"Complete moment convergence of moving average processes for $m$-widely acceptable sequence under sub-linear expectations","abstract":"In this article, the complete moment convergence for the partial sum of moving average processes $\\{X_n=\\sum_{i=-\\infty}^{\\infty}a_iY_{i+n},n\\ge 1\\}$ is estabished under some proper conditions, where $\\{Y_i,-\\infty<i<\\infty\\}$ is a sequence of $m$-widely acceptable ($m$-WA) random variables, which is stochastically dominated by a random variable $Y$ in sub-linear expectations space $(\\Omega,\\HH,\\ee)$ and $\\{a_i,-\\infty<i<\\infty\\}$ is an absolutely summable sequence of real numbers. The results extend the relevant results in probability space to those under sub-linear expectations.","sentences":["In this article, the complete moment convergence for the partial sum of moving average processes $\\{X_n=\\sum_{i=-\\infty}^{\\infty}a_iY_{i+n},n\\ge 1\\}$ is estabished under some proper conditions, where $\\{Y_i,-\\infty<i<\\infty\\}$ is a sequence of $m$-widely acceptable ($m$-WA) random variables, which is stochastically dominated by a random variable $Y$ in sub-linear expectations space $(\\Omega,\\HH,\\ee)$ and $\\{a_i,-\\infty<i<\\infty\\}$ is an absolutely summable sequence of real numbers.","The results extend the relevant results in probability space to those under sub-linear expectations."],"url":"http://arxiv.org/abs/2403.18304v1","category":"math.PR"}
{"created":"2024-03-27 06:50:51","title":"A Mean Field Game of Sequential Testing","abstract":"We introduce a mean field game for a family of filtering problems related to the classic sequential testing of the drift of a Brownian motion. To the best of our knowledge this work presents the first treatment of mean field filtering games with stopping and an unobserved common noise in the literature. We show that the game is well-posed, characterize the solution, and establish the existence of an equilibrium under certain assumptions. We also perform numerical studies for several examples of interest.","sentences":["We introduce a mean field game for a family of filtering problems related to the classic sequential testing of the drift of a Brownian motion.","To the best of our knowledge this work presents the first treatment of mean field filtering games with stopping and an unobserved common noise in the literature.","We show that the game is well-posed, characterize the solution, and establish the existence of an equilibrium under certain assumptions.","We also perform numerical studies for several examples of interest."],"url":"http://arxiv.org/abs/2403.18297v1","category":"math.OC"}
{"created":"2024-03-27 06:28:19","title":"Towards Non-Exemplar Semi-Supervised Class-Incremental Learning","abstract":"Deep neural networks perform remarkably well in close-world scenarios. However, novel classes emerged continually in real applications, making it necessary to learn incrementally. Class-incremental learning (CIL) aims to gradually recognize new classes while maintaining the discriminability of old ones. Existing CIL methods have two limitations: a heavy reliance on preserving old data for forgetting mitigation and the need for vast labeled data for knowledge adaptation. To overcome these issues, we propose a non-exemplar semi-supervised CIL framework with contrastive learning and semi-supervised incremental prototype classifier (Semi-IPC). On the one hand, contrastive learning helps the model learn rich representations, easing the trade-off between learning representations of new classes and forgetting that of old classes. On the other hand, Semi-IPC learns a prototype for each class with unsupervised regularization, enabling the model to incrementally learn from partially labeled new data while maintaining the knowledge of old classes. Experiments on benchmark datasets demonstrate the strong performance of our method: without storing any old samples and only using less than 1% of labels, Semi-IPC outperforms advanced exemplar-based methods. We hope our work offers new insights for future CIL research. The code will be made publicly available.","sentences":["Deep neural networks perform remarkably well in close-world scenarios.","However, novel classes emerged continually in real applications, making it necessary to learn incrementally.","Class-incremental learning (CIL) aims to gradually recognize new classes while maintaining the discriminability of old ones.","Existing CIL methods have two limitations: a heavy reliance on preserving old data for forgetting mitigation and the need for vast labeled data for knowledge adaptation.","To overcome these issues, we propose a non-exemplar semi-supervised CIL framework with contrastive learning and semi-supervised incremental prototype classifier (Semi-IPC).","On the one hand, contrastive learning helps the model learn rich representations, easing the trade-off between learning representations of new classes and forgetting that of old classes.","On the other hand, Semi-IPC learns a prototype for each class with unsupervised regularization, enabling the model to incrementally learn from partially labeled new data while maintaining the knowledge of old classes.","Experiments on benchmark datasets demonstrate the strong performance of our method: without storing any old samples and only using less than 1% of labels, Semi-IPC outperforms advanced exemplar-based methods.","We hope our work offers new insights for future CIL research.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2403.18291v1","category":"cs.CV"}
{"created":"2024-03-27 06:25:23","title":"Stability and convergence of the penalty formulation for nonlinear magnetostatics","abstract":"The magnetostatic field distribution in a nonlinear medium amounts to the unique minimizer of the magnetic coenergy over all fields that can be generated by the same current. This is a nonlinear saddlepoint problem whose numerical solution can in principle be achieved by mixed finite element methods and appropriate nonlinear solvers. The saddlepoint structure, however, makes the solution cumbersome. A remedy is to split the magnetic field into a known source field and the gradient of a scalar potential which is governed by a convex minimization problem. The penalty approach avoids the use of artificial potentials and Lagrange multipliers and leads to an unconstrained convex minimization problem involving a large parameter. We provide a rigorous justification of the penalty approach by deriving error estimates for the approximation due to penalization. We further highlight the close connections to the Lagrange-multiplier and scalar potential approach. The theoretical results are illustrated by numerical tests for a typical benchmark problem","sentences":["The magnetostatic field distribution in a nonlinear medium amounts to the unique minimizer of the magnetic coenergy over all fields that can be generated by the same current.","This is a nonlinear saddlepoint problem whose numerical solution can in principle be achieved by mixed finite element methods and appropriate nonlinear solvers.","The saddlepoint structure, however, makes the solution cumbersome.","A remedy is to split the magnetic field into a known source field and the gradient of a scalar potential which is governed by a convex minimization problem.","The penalty approach avoids the use of artificial potentials and Lagrange multipliers and leads to an unconstrained convex minimization problem involving a large parameter.","We provide a rigorous justification of the penalty approach by deriving error estimates for the approximation due to penalization.","We further highlight the close connections to the Lagrange-multiplier and scalar potential approach.","The theoretical results are illustrated by numerical tests for a typical benchmark problem"],"url":"http://arxiv.org/abs/2403.18285v1","category":"math.NA"}
{"created":"2024-03-27 06:18:40","title":"SGDM: Static-Guided Dynamic Module Make Stronger Visual Models","abstract":"The spatial attention mechanism has been widely used to improve object detection performance. However, its operation is currently limited to static convolutions lacking content-adaptive features. This paper innovatively approaches from the perspective of dynamic convolution. We propose Razor Dynamic Convolution (RDConv) to address thetwo flaws in dynamic weight convolution, making it hard to implement in spatial mechanism: 1) it is computation-heavy; 2) when generating weights, spatial information is disregarded. Firstly, by using Razor Operation to generate certain features, we vastly reduce the parameters of the entire dynamic convolution operation. Secondly, we added a spatial branch inside RDConv to generate convolutional kernel parameters with richer spatial information. Embedding dynamic convolution will also bring the problem of sensitivity to high-frequency noise. We propose the Static-Guided Dynamic Module (SGDM) to address this limitation. By using SGDM, we utilize a set of asymmetric static convolution kernel parameters to guide the construction of dynamic convolution. We introduce the mechanism of shared weights in static convolution to solve the problem of dynamic convolution being sensitive to high-frequency noise. Extensive experiments illustrate that multiple different object detection backbones equipped with SGDM achieve a highly competitive boost in performance(e.g., +4% mAP with YOLOv5n on VOC and +1.7% mAP with YOLOv8n on COCO) with negligible parameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n).","sentences":["The spatial attention mechanism has been widely used to improve object detection performance.","However, its operation is currently limited to static convolutions lacking content-adaptive features.","This paper innovatively approaches from the perspective of dynamic convolution.","We propose Razor Dynamic Convolution (RDConv) to address thetwo flaws in dynamic weight convolution, making it hard to implement in spatial mechanism: 1) it is computation-heavy; 2) when generating weights, spatial information is disregarded.","Firstly, by using Razor Operation to generate certain features, we vastly reduce the parameters of the entire dynamic convolution operation.","Secondly, we added a spatial branch inside RDConv to generate convolutional kernel parameters with richer spatial information.","Embedding dynamic convolution will also bring the problem of sensitivity to high-frequency noise.","We propose the Static-Guided Dynamic Module (SGDM) to address this limitation.","By using SGDM, we utilize a set of asymmetric static convolution kernel parameters to guide the construction of dynamic convolution.","We introduce the mechanism of shared weights in static convolution to solve the problem of dynamic convolution being sensitive to high-frequency noise.","Extensive experiments illustrate that multiple different object detection backbones equipped with SGDM achieve a highly competitive boost in performance(e.g., +4% mAP with YOLOv5n on VOC and","+1.7% mAP with YOLOv8n on COCO) with negligible parameter increase(i.e., +0.33M on YOLOv5n and +0.19M on YOLOv8n)."],"url":"http://arxiv.org/abs/2403.18282v1","category":"cs.CV"}
{"created":"2024-03-27 06:02:55","title":"Differentially Private Dual Gradient Tracking for Distributed Resource Allocation","abstract":"This paper investigates privacy issues in distributed resource allocation over directed networks, where each agent holds a private cost function and optimizes its decision subject to a global coupling constraint through local interaction with other agents. Conventional methods for resource allocation over directed networks require all agents to transmit their original data to neighbors, which poses the risk of disclosing sensitive and private information. To address this issue, we propose an algorithm called differentially private dual gradient tracking (DP-DGT) for distributed resource allocation, which obfuscates the exchanged messages using independent Laplacian noise. Our algorithm ensures that the agents' decisions converge to a neighborhood of the optimal solution almost surely. Furthermore, without the assumption of bounded gradients, we prove that the cumulative differential privacy loss under the proposed algorithm is finite even when the number of iterations goes to infinity. To the best of our knowledge, we are the first to simultaneously achieve these two goals in distributed resource allocation problems over directed networks. Finally, numerical simulations on economic dispatch problems within the IEEE 14-bus system illustrate the effectiveness of our proposed algorithm.","sentences":["This paper investigates privacy issues in distributed resource allocation over directed networks, where each agent holds a private cost function and optimizes its decision subject to a global coupling constraint through local interaction with other agents.","Conventional methods for resource allocation over directed networks require all agents to transmit their original data to neighbors, which poses the risk of disclosing sensitive and private information.","To address this issue, we propose an algorithm called differentially private dual gradient tracking (DP-DGT) for distributed resource allocation, which obfuscates the exchanged messages using independent Laplacian noise.","Our algorithm ensures that the agents' decisions converge to a neighborhood of the optimal solution almost surely.","Furthermore, without the assumption of bounded gradients, we prove that the cumulative differential privacy loss under the proposed algorithm is finite even when the number of iterations goes to infinity.","To the best of our knowledge, we are the first to simultaneously achieve these two goals in distributed resource allocation problems over directed networks.","Finally, numerical simulations on economic dispatch problems within the IEEE 14-bus system illustrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2403.18275v1","category":"eess.SY"}
{"created":"2024-03-27 05:15:48","title":"RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation","abstract":"Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration.However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality.Previous methods either regress 3D keypoints directly or utilise a render&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem.This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques.A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions.Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce theNormalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics.Experimental results demonstrate that the proposed method outperforms the state-of-the-art render\\&compare method and achieves higher inference speed.Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities.We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/","sentences":["Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration.","However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality.","Previous methods either regress 3D keypoints directly or utilise a render&compare strategy.","These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem.","This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D.","This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques.","A vital component of our method is the lifting of 2D keypoints to 3D keypoints.","Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions.","Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task.","To bolster cross-camera adaptability, we introduce theNormalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics.","Experimental results demonstrate that the proposed method outperforms the state-of-the-art render\\&compare method and achieves higher inference speed.","Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities.","We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/"],"url":"http://arxiv.org/abs/2403.18259v1","category":"cs.RO"}
{"created":"2024-03-27 04:32:41","title":"An Experimentally Validated Feasible Quantum Protocol for Identity-Based Signature with Application to Secure Email Communication","abstract":"Digital signatures are one of the simplest cryptographic building blocks that provide appealing security characteristics such as authenticity, unforgeability, and undeniability. In 1984, Shamir developed the first Identity-based signature (IBS) to simplify public key infrastructure and circumvent the need for certificates. It makes the process uncomplicated by enabling users to verify digital signatures using only the identifiers of signers, such as email, phone number, etc. Nearly all existing IBS protocols rely on several theoretical assumption-based hard problems. Unfortunately, these hard problems are unsafe and pose a hazard in the quantum realm. Thus, designing IBS algorithms that can withstand quantum attacks and ensure long-term security is an important direction for future research. Quantum cryptography (QC) is one such approach. In this paper, we propose an IBS based on QC. Our scheme's security is based on the laws of quantum mechanics. It thereby achieves long-term security and provides resistance against quantum attacks. We verify the proposed design's correctness and feasibility by simulating it in a prototype quantum device and the IBM Qiskit quantum simulator. The implementation code in qiskit with Jupyternotebook is provided in the Annexure. Moreover, we discuss the application of our design in secure email communication.","sentences":["Digital signatures are one of the simplest cryptographic building blocks that provide appealing security characteristics such as authenticity, unforgeability, and undeniability.","In 1984, Shamir developed the first Identity-based signature (IBS) to simplify public key infrastructure and circumvent the need for certificates.","It makes the process uncomplicated by enabling users to verify digital signatures using only the identifiers of signers, such as email, phone number, etc.","Nearly all existing IBS protocols rely on several theoretical assumption-based hard problems.","Unfortunately, these hard problems are unsafe and pose a hazard in the quantum realm.","Thus, designing IBS algorithms that can withstand quantum attacks and ensure long-term security is an important direction for future research.","Quantum cryptography (QC) is one such approach.","In this paper, we propose an IBS based on QC.","Our scheme's security is based on the laws of quantum mechanics.","It thereby achieves long-term security and provides resistance against quantum attacks.","We verify the proposed design's correctness and feasibility by simulating it in a prototype quantum device and the IBM Qiskit quantum simulator.","The implementation code in qiskit with Jupyternotebook is provided in the Annexure.","Moreover, we discuss the application of our design in secure email communication."],"url":"http://arxiv.org/abs/2403.18247v1","category":"cs.CR"}
{"created":"2024-03-27 03:28:04","title":"Conditional Denoising Diffusion Probabilistic Model for Ground-roll Attenuation","abstract":"Ground-roll attenuation is a challenging seismic processing task in land seismic survey. The ground-roll coherent noise with low frequency and high amplitude seriously contaminate the valuable reflection events, corrupting the quality of seismic data. The transform-based filtering methods leverage the distinct characteristics of the ground roll and seismic reflections within the transform domain to attenuate the ground-roll noise. However, the ground roll and seismic reflections often share overlaps in the transform domain, making it challenging to remove ground-roll noise without also attenuating useful reflections. We propose to apply a conditional diffusion denoising probabilistic model (c-DDPM) to attenuate the ground-roll noise and recover the reflections efficiently. We prepare the training dataset by using the finite-difference modelling method and the convolution modelling method. After the training process, the c-DDPM can generate the clean data given the seismic data as condition. The ground roll obtained by subtracting the clean data from the seismic data might contain some residual reflection energy. Thus, we further improve the c-DDPM to allow for generating the clean data and ground roll simultaneously. We then demonstrate the feasibility and effectiveness of our proposed method by using the synthetic data and the field data. The methods based on the local time-frequency (LTF) transform and U-Net are also applied to these two examples for comparing with our proposed method. The test results show that the proposed method perform better in attenuating the ground-roll noise from the seismic data than the LTF and U-Net methods.","sentences":["Ground-roll attenuation is a challenging seismic processing task in land seismic survey.","The ground-roll coherent noise with low frequency and high amplitude seriously contaminate the valuable reflection events, corrupting the quality of seismic data.","The transform-based filtering methods leverage the distinct characteristics of the ground roll and seismic reflections within the transform domain to attenuate the ground-roll noise.","However, the ground roll and seismic reflections often share overlaps in the transform domain, making it challenging to remove ground-roll noise without also attenuating useful reflections.","We propose to apply a conditional diffusion denoising probabilistic model (c-DDPM) to attenuate the ground-roll noise and recover the reflections efficiently.","We prepare the training dataset by using the finite-difference modelling method and the convolution modelling method.","After the training process, the c-DDPM can generate the clean data given the seismic data as condition.","The ground roll obtained by subtracting the clean data from the seismic data might contain some residual reflection energy.","Thus, we further improve the c-DDPM to allow for generating the clean data and ground roll simultaneously.","We then demonstrate the feasibility and effectiveness of our proposed method by using the synthetic data and the field data.","The methods based on the local time-frequency (LTF) transform and U-Net are also applied to these two examples for comparing with our proposed method.","The test results show that the proposed method perform better in attenuating the ground-roll noise from the seismic data than the LTF and U-Net methods."],"url":"http://arxiv.org/abs/2403.18224v1","category":"physics.geo-ph"}
{"created":"2024-03-27 03:19:36","title":"Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies","abstract":"Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git","sentences":["Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge.","Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents.","Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions.","We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates.","The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git"],"url":"http://arxiv.org/abs/2403.18222v1","category":"cs.RO"}
{"created":"2024-03-27 02:35:36","title":"Road Obstacle Detection based on Unknown Objectness Scores","abstract":"The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.","sentences":["The detection of unknown traffic obstacles is vital to ensure safe autonomous driving.","The standard object-detection methods cannot identify unknown objects that are not included under predefined categories.","This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects.","To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention.","Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples.","However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance.","The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods.","To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores.","Our experimental results show that the objectness scores play an important role in improving the detection performance.","Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score.","Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets."],"url":"http://arxiv.org/abs/2403.18207v1","category":"cs.CV"}
{"created":"2024-03-27 01:28:36","title":"Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting","abstract":"We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.","sentences":["We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes.","Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image.","This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings.","Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics."],"url":"http://arxiv.org/abs/2403.18186v1","category":"cs.CV"}
{"created":"2024-03-27 01:25:26","title":"Learning Optimal Behavior Through Reasoning and Experiences","abstract":"We develop a novel framework of bounded rationality under cognitive frictions that studies learning over optimal behavior through both deliberative reasoning and accumulated experiences. Using both types of information, agents engage in Bayesian non-parametric estimation of the unknown action value function. Reasoning signals are produced internally through mental deliberation, subject to a cognitive cost. Experience signals are the observed utility outcomes at previous actions. Agents' subjective estimation uncertainty, which evolves through information accumulation, modulates the two modes of learning in a state- and history-dependent way. We discuss how the model draws on and bridges conceptual, methodological and empirical insights from both economics and the cognitive sciences literature on reinforcement learning.","sentences":["We develop a novel framework of bounded rationality under cognitive frictions that studies learning over optimal behavior through both deliberative reasoning and accumulated experiences.","Using both types of information, agents engage in Bayesian non-parametric estimation of the unknown action value function.","Reasoning signals are produced internally through mental deliberation, subject to a cognitive cost.","Experience signals are the observed utility outcomes at previous actions.","Agents' subjective estimation uncertainty, which evolves through information accumulation, modulates the two modes of learning in a state- and history-dependent way.","We discuss how the model draws on and bridges conceptual, methodological and empirical insights from both economics and the cognitive sciences literature on reinforcement learning."],"url":"http://arxiv.org/abs/2403.18185v1","category":"econ.TH"}
{"created":"2024-03-27 01:12:31","title":"Online Embedding Multi-Scale CLIP Features into 3D Maps","abstract":"This study introduces a novel approach to online embedding of multi-scale CLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By harnessing CLIP, this methodology surpasses the constraints of conventional vocabulary-limited methods and enables the incorporation of semantic information into the resultant maps. While recent approaches have explored the embedding of multi-modal features in maps, they often impose significant computational costs, lacking practicality for exploring unfamiliar environments in real time. Our approach tackles these challenges by efficiently computing and embedding multi-scale CLIP features, thereby facilitating the exploration of unfamiliar environments through real-time map generation. Moreover, the embedding CLIP features into the resultant maps makes offline retrieval via linguistic queries feasible. In essence, our approach simultaneously achieves real-time object search and mapping of unfamiliar environments. Additionally, we propose a zero-shot object-goal navigation system based on our mapping approach, and we validate its efficacy through object-goal navigation, offline object retrieval, and multi-object-goal navigation in both simulated environments and real robot experiments. The findings demonstrate that our method not only exhibits swifter performance than state-of-the-art mapping methods but also surpasses them in terms of the success rate of object-goal navigation tasks.","sentences":["This study introduces a novel approach to online embedding of multi-scale CLIP (Contrastive Language-Image Pre-Training) features into 3D maps.","By harnessing CLIP, this methodology surpasses the constraints of conventional vocabulary-limited methods and enables the incorporation of semantic information into the resultant maps.","While recent approaches have explored the embedding of multi-modal features in maps, they often impose significant computational costs, lacking practicality for exploring unfamiliar environments in real time.","Our approach tackles these challenges by efficiently computing and embedding multi-scale CLIP features, thereby facilitating the exploration of unfamiliar environments through real-time map generation.","Moreover, the embedding CLIP features into the resultant maps makes offline retrieval via linguistic queries feasible.","In essence, our approach simultaneously achieves real-time object search and mapping of unfamiliar environments.","Additionally, we propose a zero-shot object-goal navigation system based on our mapping approach, and we validate its efficacy through object-goal navigation, offline object retrieval, and multi-object-goal navigation in both simulated environments and real robot experiments.","The findings demonstrate that our method not only exhibits swifter performance than state-of-the-art mapping methods but also surpasses them in terms of the success rate of object-goal navigation tasks."],"url":"http://arxiv.org/abs/2403.18178v1","category":"cs.RO"}
{"created":"2024-03-27 01:05:45","title":"Mistake, Manipulation and Margin Guarantees in Online Strategic Classification","abstract":"We consider an online strategic classification problem where each arriving agent can manipulate their true feature vector to obtain a positive predicted label, while incurring a cost that depends on the amount of manipulation. The learner seeks to predict the agent's true label given access to only the manipulated features. After the learner releases their prediction, the agent's true label is revealed. Previous algorithms such as the strategic perceptron guarantee finitely many mistakes under a margin assumption on agents' true feature vectors. However, these are not guaranteed to encourage agents to be truthful. Promoting truthfulness is intimately linked to obtaining adequate margin on the predictions, thus we provide two new algorithms aimed at recovering the maximum margin classifier in the presence of strategic agent behavior. We prove convergence, finite mistake and finite manipulation guarantees for a variety of agent cost structures. We also provide generalized versions of the strategic perceptron with mistake guarantees for different costs. Our numerical study on real and synthetic data demonstrates that the new algorithms outperform previous ones in terms of margin, number of manipulation and number of mistakes.","sentences":["We consider an online strategic classification problem where each arriving agent can manipulate their true feature vector to obtain a positive predicted label, while incurring a cost that depends on the amount of manipulation.","The learner seeks to predict the agent's true label given access to only the manipulated features.","After the learner releases their prediction, the agent's true label is revealed.","Previous algorithms such as the strategic perceptron guarantee finitely many mistakes under a margin assumption on agents' true feature vectors.","However, these are not guaranteed to encourage agents to be truthful.","Promoting truthfulness is intimately linked to obtaining adequate margin on the predictions, thus we provide two new algorithms aimed at recovering the maximum margin classifier in the presence of strategic agent behavior.","We prove convergence, finite mistake and finite manipulation guarantees for a variety of agent cost structures.","We also provide generalized versions of the strategic perceptron with mistake guarantees for different costs.","Our numerical study on real and synthetic data demonstrates that the new algorithms outperform previous ones in terms of margin, number of manipulation and number of mistakes."],"url":"http://arxiv.org/abs/2403.18176v1","category":"cs.LG"}
{"created":"2024-03-27 00:39:52","title":"Observation of nodal-arcs, effect of axial strain on the nodal-lines & Weyl nodes and low-temperature finite value of Seebeck coefficient in TaAs class of Weyl semimetals","abstract":"This work verifies the SOC-induced evolution of \\textit{nodal-arcs} into Weyl nodes under the effect of spin-orbit coupling (SOC) in NbAs \\& NbP. The obtained features mimics the observations as reported for TaAs \\& TaP in our previous work\\cite{pandey2023existence}. In addition, this work reports that the number of nodes in TaAs class of Weyl semimetals (WSMs) can be altered via creating strain along $a$ or $c$ direction of the crystal. For instance, the number of nodes in NbAs under SOC-effect along with 2\\% (3\\%) compressive-strain in $a$ direction is found to be 20 (28). Besides the nodes, such strain are found to have considerable impact on the nodal-rings of these WSMs when effect of SOC is ignored. Apart from this, the work discusses the role of Weyl physics in affecting the Seebeck coefficient of any WSM. In this direction, it is discussed that how a symmetric Weyl cone, even if tilted, have no contribution to the Seebeck of WSMs. Furthermore, the work highlights the conditions under which a Weyl cone can contribute to the Seebeck coefficient of a given WSM. Lastly, the discussion of Weyl contribution to Seebeck is validated over TaAs class of WSMs via investigating the features of its Weyl cones and calculating the contributions of these cones to the Seebeck coefficient of these semimetals. The value of $S$ contributed from Weyl cone is found to be as large as $\\sim$65 $\\mu$\\textit{V}/\\textit{K} below 25 K in case of TaAs. The findings of this work present a possibility of engineering the topological properties of TaAs class of WSMs via creating strain in their crystal. It also makes the picture of Weyl physics\\textquoteright\\hspace{0.1cm} impact on the Seebeck coefficient of WSMs a more clear.","sentences":["This work verifies the SOC-induced evolution of \\textit{nodal-arcs} into Weyl nodes under the effect of spin-orbit coupling (SOC) in NbAs \\& NbP.","The obtained features mimics the observations as reported for TaAs \\& TaP in our previous work\\cite{pandey2023existence}.","In addition, this work reports that the number of nodes in TaAs class of Weyl semimetals (WSMs) can be altered via creating strain along $a$ or $c$ direction of the crystal.","For instance, the number of nodes in NbAs under SOC-effect along with 2\\% (3\\%) compressive-strain in $a$ direction is found to be 20 (28).","Besides the nodes, such strain are found to have considerable impact on the nodal-rings of these WSMs when effect of SOC is ignored.","Apart from this, the work discusses the role of Weyl physics in affecting the Seebeck coefficient of any WSM.","In this direction, it is discussed that how a symmetric Weyl cone, even if tilted, have no contribution to the Seebeck of WSMs.","Furthermore, the work highlights the conditions under which a Weyl cone can contribute to the Seebeck coefficient of a given WSM.","Lastly, the discussion of Weyl contribution to Seebeck is validated over TaAs class of WSMs via investigating the features of its Weyl cones and calculating the contributions of these cones to the Seebeck coefficient of these semimetals.","The value of $S$ contributed from Weyl cone is found to be as large as $\\sim$65 $\\mu$\\textit{V}/\\textit{K} below 25 K in case of TaAs.","The findings of this work present a possibility of engineering the topological properties of TaAs class of WSMs via creating strain in their crystal.","It also makes the picture of Weyl physics\\textquoteright\\hspace{0.1cm} impact on the Seebeck coefficient of WSMs a more clear."],"url":"http://arxiv.org/abs/2403.18169v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 00:16:17","title":"Incentive Designs for Learning Agents to Stabilize Coupled Exogenous Systems","abstract":"We consider a large population of learning agents noncooperatively selecting strategies from a common set, influencing the dynamics of an exogenous system (ES) we seek to stabilize at a desired equilibrium. Our approach is to design a dynamic payoff mechanism capable of shaping the population's strategy profile, thus affecting the ES's state, by offering incentives for specific strategies within budget limits. Employing system-theoretic passivity concepts, we establish conditions under which a payoff mechanism can be systematically constructed to ensure the global asymptotic stabilization of the ES's equilibrium. In comparison to previous approaches originally studied in the context of the so-called epidemic population games, the method proposed here allows for more realistic epidemic models and other types of ES, such as predator-prey dynamics. Stabilization is established with the support of a Lyapunov function, which provides useful bounds on the transients.","sentences":["We consider a large population of learning agents noncooperatively selecting strategies from a common set, influencing the dynamics of an exogenous system (ES) we seek to stabilize at a desired equilibrium.","Our approach is to design a dynamic payoff mechanism capable of shaping the population's strategy profile, thus affecting the ES's state, by offering incentives for specific strategies within budget limits.","Employing system-theoretic passivity concepts, we establish conditions under which a payoff mechanism can be systematically constructed to ensure the global asymptotic stabilization of the ES's equilibrium.","In comparison to previous approaches originally studied in the context of the so-called epidemic population games, the method proposed here allows for more realistic epidemic models and other types of ES, such as predator-prey dynamics.","Stabilization is established with the support of a Lyapunov function, which provides useful bounds on the transients."],"url":"http://arxiv.org/abs/2403.18164v1","category":"eess.SY"}
{"created":"2024-03-27 00:12:51","title":"A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks","abstract":"In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population -- and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations.","sentences":["In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views.","Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information.","The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance.","Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population -- and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate.","This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network.","We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations."],"url":"http://arxiv.org/abs/2403.18163v1","category":"cs.SI"}
{"created":"2024-03-27 00:04:46","title":"Non-Resonant Picosecond Three-Wave Mixing in the Gas Phase","abstract":"We report on the experimental observation of non-resonant, second-order optical Sum-Frequency Generation (SFG) in five different atomic and molecular gases. The measured signal is attributed to a SFG process by characterizing its intensity scaling and its polarization behavior. We show that the electric quadrupole mechanism cannot explain the observed trends. Our results demonstrate that SFG in the gas phase is about four orders of magnitude stronger than Third Harmonic Generation (THG) and independent from any externally-applied electric fields. These features make this method suitable for gas number density measurements at the picosecond timescale in reactive flows and plasmas.","sentences":["We report on the experimental observation of non-resonant, second-order optical Sum-Frequency Generation (SFG) in five different atomic and molecular gases.","The measured signal is attributed to a SFG process by characterizing its intensity scaling and its polarization behavior.","We show that the electric quadrupole mechanism cannot explain the observed trends.","Our results demonstrate that SFG in the gas phase is about four orders of magnitude stronger than Third Harmonic Generation (THG) and independent from any externally-applied electric fields.","These features make this method suitable for gas number density measurements at the picosecond timescale in reactive flows and plasmas."],"url":"http://arxiv.org/abs/2403.18161v1","category":"physics.optics"}
{"created":"2024-03-26 23:17:05","title":"Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC","abstract":"Conic constraints appear in many important control applications like legged locomotion, robotic manipulation, and autonomous rocket landing. However, current solvers for conic optimization problems have relatively heavy computational demands in terms of both floating-point operations and memory footprint, making them impractical for use on small embedded devices. We extend TinyMPC, an open-source, high-speed solver targeting low-power embedded control applications, to handle second-order cone constraints. We also present code-generation software to enable deployment of TinyMPC on a variety of microcontrollers. We benchmark our generated code against state-of-the-art embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed increase over ECOS while consuming less memory. Finally, we demonstrate TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained quadrotor with fast dynamics. TinyMPC and its code-generation tools are publicly available at https://tinympc.org.","sentences":["Conic constraints appear in many important control applications like legged locomotion, robotic manipulation, and autonomous rocket landing.","However, current solvers for conic optimization problems have relatively heavy computational demands in terms of both floating-point operations and memory footprint, making them impractical for use on small embedded devices.","We extend TinyMPC, an open-source, high-speed solver targeting low-power embedded control applications, to handle second-order cone constraints.","We also present code-generation software to enable deployment of TinyMPC on a variety of microcontrollers.","We benchmark our generated code against state-of-the-art embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed increase over ECOS while consuming less memory.","Finally, we demonstrate TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained quadrotor with fast dynamics.","TinyMPC and its code-generation tools are publicly available at https://tinympc.org."],"url":"http://arxiv.org/abs/2403.18149v1","category":"cs.RO"}
{"created":"2024-03-26 23:14:15","title":"Divide, Conquer, Combine Bayesian Decision Tree Sampling","abstract":"Decision trees are commonly used predictive models due to their flexibility and interpretability. This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach. This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure. This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate. Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions. A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling. This paper takes a different approach, where each distinct tree structure is associated with a unique set of decision parameters. The proposed approach, entitled DCC-Tree, is inspired by the work in Zhou et al. [23] for probabilistic programs and Cochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for decision trees. Results show that DCC-Tree performs comparably to other HMC-based methods and better than existing Bayesian tree methods while improving on consistency and reducing the per-proposal complexity.","sentences":["Decision trees are commonly used predictive models due to their flexibility and interpretability.","This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach.","This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure.","This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate.","Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions.","A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling.","This paper takes a different approach, where each distinct tree structure is associated with a unique set of decision parameters.","The proposed approach, entitled DCC-Tree, is inspired by the work in Zhou et al.","[23] for probabilistic programs and Cochrane et al.","[4] for Hamiltonian Monte Carlo (HMC) based sampling for decision trees.","Results show that DCC-Tree performs comparably to other HMC-based methods and better than existing Bayesian tree methods while improving on consistency and reducing the per-proposal complexity."],"url":"http://arxiv.org/abs/2403.18147v1","category":"cs.LG"}
{"created":"2024-03-26 23:12:12","title":"Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach","abstract":"True-time delayers (TTDs) are popular analog devices for facilitating near-field wideband beamforming subject to the spatial-wideband effect. In this paper, an adaptive TTD configuration is proposed for short-range TTDs. Compared to the existing TTD configurations, the proposed one can effectively combat the spatial-widebandd effect for arbitrary user locations and array shapes with the aid of a switch network. A novel end-to-end deep neural network is proposed to optimize the hybrid beamforming with adaptive TTDs for maximizing spectral efficiency. 1) First, based on the U-Net architecture, a near-field channel learning module (NFC-LM) is proposed for adaptive beamformer design through extracting the latent channel response features of various users across different frequencies. In the NFC-LM, an improved cross attention (CA) is introduced to further optimize beamformer design by enhancing the latent feature connection between near-field channel and different beamformers. 2) Second, a switch multi-user transformer (S-MT) is proposed to adaptively control the connection between TTDs and phase shifters (PSs). In the S-MT, an improved multi-head attention, namely multi-user attention (MSA), is introduced to optimize the switch network through exploring the latent channel relations among various users. 3) Third, a multi feature cross attention (MCA) is introduced to simultaneously optimize the NFC-LM and S-MT by enhancing the latent feature correlation between beamformers and switch network. Numerical simulation results show that 1) the proposed adaptive TTD configuration effectively eliminates the spatial-wideband effect under uniform linear array (ULA) and uniform circular array (UCA) architectures, and 2) the proposed deep neural network can provide near optimal spectral efficiency, and solve the multi-user bemformer design and dynamical connection problem in real-time.","sentences":["True-time delayers (TTDs) are popular analog devices for facilitating near-field wideband beamforming subject to the spatial-wideband effect.","In this paper, an adaptive TTD configuration is proposed for short-range TTDs.","Compared to the existing TTD configurations, the proposed one can effectively combat the spatial-widebandd effect for arbitrary user locations and array shapes with the aid of a switch network.","A novel end-to-end deep neural network is proposed to optimize the hybrid beamforming with adaptive TTDs for maximizing spectral efficiency.","1) First, based on the U-Net architecture, a near-field channel learning module (NFC-LM) is proposed for adaptive beamformer design through extracting the latent channel response features of various users across different frequencies.","In the NFC-LM, an improved cross attention (CA) is introduced to further optimize beamformer design by enhancing the latent feature connection between near-field channel and different beamformers.","2) Second, a switch multi-user transformer (S-MT) is proposed to adaptively control the connection between TTDs and phase shifters (PSs).","In the S-MT, an improved multi-head attention, namely multi-user attention (MSA), is introduced to optimize the switch network through exploring the latent channel relations among various users.","3) Third, a multi feature cross attention (MCA) is introduced to simultaneously optimize the NFC-LM and S-MT by enhancing the latent feature correlation between beamformers and switch network.","Numerical simulation results show that 1) the proposed adaptive TTD configuration effectively eliminates the spatial-wideband effect under uniform linear array (ULA) and uniform circular array (UCA) architectures, and 2) the proposed deep neural network can provide near optimal spectral efficiency, and solve the multi-user bemformer design and dynamical connection problem in real-time."],"url":"http://arxiv.org/abs/2403.18146v1","category":"cs.IT"}
{"created":"2024-03-26 22:24:11","title":"Convergence of Iterative Quadratic Programming for Robust Fixed-Endpoint Transfer of Bilinear Systems","abstract":"We present a computational method for open-loop minimum-norm control synthesis for fixed-endpoint transfer of bilinear ensemble systems that are indexed by two continuously varying parameters. We suppose that one ensemble parameter scales the homogeneous, linear part of the dynamics, and the second parameter scales the effect of the applied control inputs on the inhomogeneous, bilinear dynamics. This class of dynamical systems is motivated by robust quantum control pulse synthesis, where the ensemble parameters correspond to uncertainty in the free Hamiltonian and inhomogeneity in the control Hamiltonian, respectively. Our computational method is based on polynomial approximation of the ensemble state in parameter space and discretization of the evolution equations in the time domain using a product of matrix exponentials corresponding to zero-order hold controls over the time intervals. The dynamics are successively linearized about control and trajectory iterates to formulate a sequence of quadratic programs for computing perturbations to the control that successively improve the objective until the iteration converges. We use a two-stage computation to first ensure transfer to the desired terminal state, and then minimize the norm of the control function. The method is demonstrated for the canonical uniform transfer problem for the Bloch system that appears in nuclear magnetic resonance, as well as the matter-wave splitting problem for the Raman-Nath system that appears in ultra-cold atom interferometry.","sentences":["We present a computational method for open-loop minimum-norm control synthesis for fixed-endpoint transfer of bilinear ensemble systems that are indexed by two continuously varying parameters.","We suppose that one ensemble parameter scales the homogeneous, linear part of the dynamics, and the second parameter scales the effect of the applied control inputs on the inhomogeneous, bilinear dynamics.","This class of dynamical systems is motivated by robust quantum control pulse synthesis, where the ensemble parameters correspond to uncertainty in the free Hamiltonian and inhomogeneity in the control Hamiltonian, respectively.","Our computational method is based on polynomial approximation of the ensemble state in parameter space and discretization of the evolution equations in the time domain using a product of matrix exponentials corresponding to zero-order hold controls over the time intervals.","The dynamics are successively linearized about control and trajectory iterates to formulate a sequence of quadratic programs for computing perturbations to the control that successively improve the objective until the iteration converges.","We use a two-stage computation to first ensure transfer to the desired terminal state, and then minimize the norm of the control function.","The method is demonstrated for the canonical uniform transfer problem for the Bloch system that appears in nuclear magnetic resonance, as well as the matter-wave splitting problem for the Raman-Nath system that appears in ultra-cold atom interferometry."],"url":"http://arxiv.org/abs/2403.18131v1","category":"math.OC"}
{"created":"2024-03-26 22:15:19","title":"Revisiting Elastic String Models of Forward Interest Rates","abstract":"Twenty five years ago, several authors proposed to model the forward interest rate curve (FRC) as an elastic string along which idiosyncratic shocks propagate, accounting for the peculiar structure of the return correlation across different maturities. In this paper, we revisit the specific \"stiff'' elastic string field theory of Baaquie and Bouchaud (2004) in a way that makes its micro-foundation more transparent. Our model can be interpreted as capturing the effect of market forces that set the rates of nearby tenors in a self-referential fashion. The model is parsimonious and accurately reproduces the whole correlation structure of the FRC over the time period 1994-2023, with an error below 2%. We need only two parameters, the values of which being very stable except perhaps during the Quantitative Easing period 2009-2014. The dependence of correlation on time resolution (also called the Epps effect) is also faithfully reproduced within the model and leads to a cross-tenor information propagation time of 10 minutes. Finally, we confirm that the perceived time in interest rate markets is a strongly sub-linear function of real time, as surmised by Baaquie and Bouchaud (2004). In fact, our results are fully compatible with hyperbolic discounting, in line with the recent behavioural literature (Farmer and Geanakoplos, 2009).","sentences":["Twenty five years ago, several authors proposed to model the forward interest rate curve (FRC) as an elastic string along which idiosyncratic shocks propagate, accounting for the peculiar structure of the return correlation across different maturities.","In this paper, we revisit the specific \"stiff'' elastic string field theory of Baaquie and Bouchaud (2004) in a way that makes its micro-foundation more transparent.","Our model can be interpreted as capturing the effect of market forces that set the rates of nearby tenors in a self-referential fashion.","The model is parsimonious and accurately reproduces the whole correlation structure of the FRC over the time period 1994-2023, with an error below 2%.","We need only two parameters, the values of which being very stable except perhaps during the Quantitative Easing period 2009-2014.","The dependence of correlation on time resolution (also called the Epps effect) is also faithfully reproduced within the model and leads to a cross-tenor information propagation time of 10 minutes.","Finally, we confirm that the perceived time in interest rate markets is a strongly sub-linear function of real time, as surmised by Baaquie and Bouchaud (2004).","In fact, our results are fully compatible with hyperbolic discounting, in line with the recent behavioural literature (Farmer and Geanakoplos, 2009)."],"url":"http://arxiv.org/abs/2403.18126v1","category":"q-fin.ST"}
{"created":"2024-03-26 22:07:29","title":"Stochastic Finite Volume Method for Uncertainty Management in Gas Pipeline Network Flows","abstract":"Natural gas consumption by users of pipeline networks is subject to increasing uncertainty that originates from the intermittent nature of electric power loads serviced by gas-fired generators. To enable computationally efficient optimization of gas network flows subject to uncertainty, we develop a finite volume representation of stochastic solutions of hyperbolic partial differential equation (PDE) systems on graph-connected domains with nodal coupling and boundary conditions. The representation is used to express the physical constraints in stochastic optimization problems for gas flow allocation subject to uncertain parameters. The method is based on the stochastic finite volume approach that was recently developed for uncertainty quantification in transient flows represented by hyperbolic PDEs on graphs. In this study, we develop optimization formulations for steady-state gas flow over actuated transport networks subject to probabilistic constraints. In addition to the distributions for the physical solutions, we examine the dual variables that are produced by way of the optimization, and interpret them as price distributions that quantify the financial volatility that arises through demand uncertainty modeled in an optimization-driven gas market mechanism. We demonstrate the computation and distributional analysis using a single-pipe example and a small test network.","sentences":["Natural gas consumption by users of pipeline networks is subject to increasing uncertainty that originates from the intermittent nature of electric power loads serviced by gas-fired generators.","To enable computationally efficient optimization of gas network flows subject to uncertainty, we develop a finite volume representation of stochastic solutions of hyperbolic partial differential equation (PDE) systems on graph-connected domains with nodal coupling and boundary conditions.","The representation is used to express the physical constraints in stochastic optimization problems for gas flow allocation subject to uncertain parameters.","The method is based on the stochastic finite volume approach that was recently developed for uncertainty quantification in transient flows represented by hyperbolic PDEs on graphs.","In this study, we develop optimization formulations for steady-state gas flow over actuated transport networks subject to probabilistic constraints.","In addition to the distributions for the physical solutions, we examine the dual variables that are produced by way of the optimization, and interpret them as price distributions that quantify the financial volatility that arises through demand uncertainty modeled in an optimization-driven gas market mechanism.","We demonstrate the computation and distributional analysis using a single-pipe example and a small test network."],"url":"http://arxiv.org/abs/2403.18124v1","category":"math.OC"}
{"created":"2024-03-26 22:03:20","title":"Planck data revisited: low-noise synchrotron polarisation maps from the WMAP and Planck space missions","abstract":"Observations of cosmic microwave background polarisation, essential for probing a potential phase of inflation in the early universe, suffer from contamination by polarised emission from the Galactic interstellar medium. This work combines existing observations from the WMAP and Planck space missions to make a low-noise map of polarised synchrotron emission that can be used to clean forthcoming CMB observations. We combine WMAP K, Ka and Q maps with Planck LFI 30~GHz and 44~GHz maps using weights that near-optimally combine the observations as a function of sky direction, angular scale, and polarisation orientation. We publish well-characterised maps of synchrotron Q and U Stokes parameters at nu = 30GHz and 1 degree angular resolution. A statistical description of uncertainties is provided with Monte-Carlo simulations of additive and multiplicative errors. Our maps are the most sensitive full-sky maps of synchrotron polarisation to date, and are made available to the scientific community on a dedicated web site.","sentences":["Observations of cosmic microwave background polarisation, essential for probing a potential phase of inflation in the early universe, suffer from contamination by polarised emission from the Galactic interstellar medium.","This work combines existing observations from the WMAP and Planck space missions to make a low-noise map of polarised synchrotron emission that can be used to clean forthcoming CMB observations.","We combine WMAP K, Ka and Q maps with Planck LFI 30~GHz and 44~GHz maps using weights that near-optimally combine the observations as a function of sky direction, angular scale, and polarisation orientation.","We publish well-characterised maps of synchrotron Q and U Stokes parameters at nu = 30GHz and 1 degree angular resolution.","A statistical description of uncertainties is provided with Monte-Carlo simulations of additive and multiplicative errors.","Our maps are the most sensitive full-sky maps of synchrotron polarisation to date, and are made available to the scientific community on a dedicated web site."],"url":"http://arxiv.org/abs/2403.18123v1","category":"astro-ph.CO"}
{"created":"2024-03-26 21:50:09","title":"Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems","abstract":"In this paper we develop a multiple model reference adaptive controller (MMRAC) with blending. The systems under consideration are non-square, i.e., the number of inputs is not equal to the number of states; multi-input, linear, time-invariant with uncertain parameters that lie inside of a known, compact, and convex set. Moreover, the full state of the plant is available for feedback. A multiple model online identification scheme for the plant's state and input matrices is developed that guarantees the estimated parameters converge to the underlying plant model under the assumption of persistence of excitation. Using an exact matching condition, the parameter estimates are used in a control law such that the plant's states asymptotically track the reference signal generated by a state-space model reference. The control architecture is proven to provide boundedness of all closed-loop signals and to asymptotically drive the state tracking error to zero. Numerical simulations illustrate the stability and efficacy of the proposed MMRAC scheme.","sentences":["In this paper we develop a multiple model reference adaptive controller (MMRAC) with blending.","The systems under consideration are non-square, i.e., the number of inputs is not equal to the number of states; multi-input, linear, time-invariant with uncertain parameters that lie inside of a known, compact, and convex set.","Moreover, the full state of the plant is available for feedback.","A multiple model online identification scheme for the plant's state and input matrices is developed that guarantees the estimated parameters converge to the underlying plant model under the assumption of persistence of excitation.","Using an exact matching condition, the parameter estimates are used in a control law such that the plant's states asymptotically track the reference signal generated by a state-space model reference.","The control architecture is proven to provide boundedness of all closed-loop signals and to asymptotically drive the state tracking error to zero.","Numerical simulations illustrate the stability and efficacy of the proposed MMRAC scheme."],"url":"http://arxiv.org/abs/2403.18119v1","category":"eess.SY"}
{"created":"2024-03-26 21:38:50","title":"Assessing COVID-19 Vaccine Effectiveness in Observational Studies via Nested Trial Emulation","abstract":"Observational data are often used to estimate real-world effectiveness and durability of coronavirus disease 2019 (COVID-19) vaccines. A sequence of nested trials can be emulated to draw inference from such data while minimizing selection bias, immortal time bias, and confounding. Typically, when nested trial emulation (NTE) is employed, effect estimates are pooled across trials to increase statistical efficiency. However, such pooled estimates may lack a clear interpretation when the treatment effect is heterogeneous across trials. In the context of COVID-19, vaccine effectiveness quite plausibly will vary over calendar time due to newly emerging variants of the virus. This manuscript considers a NTE inverse probability weighted estimator of vaccine effectiveness that may vary over calendar time, time since vaccination, or both. Statistical testing of the trial effect homogeneity assumption is considered. Simulation studies are presented examining the finite-sample performance of these methods under a variety of scenarios. The methods are used to estimate vaccine effectiveness against COVID-19 outcomes using observational data on over 120,000 residents of Abruzzo, Italy during 2021.","sentences":["Observational data are often used to estimate real-world effectiveness and durability of coronavirus disease 2019 (COVID-19) vaccines.","A sequence of nested trials can be emulated to draw inference from such data while minimizing selection bias, immortal time bias, and confounding.","Typically, when nested trial emulation (NTE) is employed, effect estimates are pooled across trials to increase statistical efficiency.","However, such pooled estimates may lack a clear interpretation when the treatment effect is heterogeneous across trials.","In the context of COVID-19, vaccine effectiveness quite plausibly will vary over calendar time due to newly emerging variants of the virus.","This manuscript considers a NTE inverse probability weighted estimator of vaccine effectiveness that may vary over calendar time, time since vaccination, or both.","Statistical testing of the trial effect homogeneity assumption is considered.","Simulation studies are presented examining the finite-sample performance of these methods under a variety of scenarios.","The methods are used to estimate vaccine effectiveness against COVID-19 outcomes using observational data on over 120,000 residents of Abruzzo, Italy during 2021."],"url":"http://arxiv.org/abs/2403.18115v1","category":"stat.ME"}
{"created":"2024-03-26 21:32:07","title":"Chiral Corrections to the Gell-Mann-Oakes-Renner Relation from QCD Sum Rules","abstract":"We calculate the next-to-leading order corrections to the $SU(2)\\otimes SU(2)$ and $SU(3)\\otimes SU(3)$ Gell-Mann-Oakes-Renner relations. We use a pseudoscalar correlator calculated from Perturbative QCD up to five loops and use the QCD Finite Energy Sum Rules with integration kernels tuned to suppress the importance of the hadronic resonances. This leads to a substantial reduction in the systematic uncertainties from the experimentally unknown resonance spectral function. We use the method of Fixed Order and Fixed Renormalization Scale Perturbation Theory to compute the integrals. We calculate these corrections to be $\\delta_\\pi = 0.060 \\pm 0.014$ and $\\delta _K =0.64 \\pm 0.24$. As a result of these new values, we predict the value of the light quark condensate $\\left\\langle {0|\\bar qq|0} \\right\\rangle = - \\left( {266 \\pm 5{\\text{ MeV}}} \\right)^3$ and the Chiral Perturbation Theory low energy constant $H_2^r = - \\left( {4.9 \\pm 1.8} \\right) \\times 10^{ - 3}$. Results from this work have been published as: J. Bordes, C.A. Dominguez, P. Moodley, J. Pe$\\widetilde{\\text{n}}$arrocha and K. Schilcher, J. High Ener. Phys. 05 (2010) 064.","sentences":["We calculate the next-to-leading order corrections to the $SU(2)\\otimes SU(2)$ and $SU(3)\\otimes SU(3)$ Gell-Mann-Oakes-Renner relations.","We use a pseudoscalar correlator calculated from Perturbative QCD up to five loops and use the QCD Finite Energy Sum Rules with integration kernels tuned to suppress the importance of the hadronic resonances.","This leads to a substantial reduction in the systematic uncertainties from the experimentally unknown resonance spectral function.","We use the method of Fixed Order and Fixed Renormalization Scale Perturbation Theory to compute the integrals.","We calculate these corrections to be $\\delta_\\pi = 0.060 \\pm 0.014$ and $\\delta _","K =0.64 \\pm 0.24$.","As a result of these new values, we predict the value of the light quark condensate $\\left\\langle {0|\\bar qq|0} \\right\\rangle = - \\left( {266 \\pm 5{\\text{ MeV}}} \\right)^3$ and the Chiral Perturbation Theory low energy constant $H_2^r = - \\left( {4.9 \\pm 1.8} \\right) \\times 10^{ - 3}$. Results from this work have been published as: J. Bordes, C.A. Dominguez, P. Moodley, J. Pe$\\widetilde{\\text{n}}$arrocha and K. Schilcher, J. High Ener.","Phys. 05","(2010) 064."],"url":"http://arxiv.org/abs/2403.18112v1","category":"hep-ph"}
{"created":"2024-03-26 21:29:48","title":"Scrolly2Reel: Turning News Graphics into TikToks by Adjusting Narrative Beats and Pacing","abstract":"As media evolves, storytelling evolves. In 2012, newspapers introduced scrollytelling sequences, or \"scrollies,\" to make news more immersive and interactive on the web. As users scroll through an article, graphics like animation, charts, and 3D visualizations appear to provide visual dynamics to the story. Today, news consumption is shifting to short-video platforms like TikTok, particularly among younger audiences. We propose repurposing the assets from scrollies and computationally transform them into videos. By shortening the original written text and precisely synchronizing the timing of audio narrative with features in the visual scrolling assets, we can create reels with dynamic pacing and narrative beats. We argue that text shortening is essential to producing fast paced videos that are compelling and visually interesting, and show that when beats are preserved in the output reel, topical alignment between them and the visual assets is crucial to the viewing experience. Understanding narrative pacing and beats in creative forms is key to user experience of media. They are an important primitive to effective editing, repurposing, and retargeting content while maintaining a cohesive narrative.","sentences":["As media evolves, storytelling evolves.","In 2012, newspapers introduced scrollytelling sequences, or \"scrollies,\" to make news more immersive and interactive on the web.","As users scroll through an article, graphics like animation, charts, and 3D visualizations appear to provide visual dynamics to the story.","Today, news consumption is shifting to short-video platforms like TikTok, particularly among younger audiences.","We propose repurposing the assets from scrollies and computationally transform them into videos.","By shortening the original written text and precisely synchronizing the timing of audio narrative with features in the visual scrolling assets, we can create reels with dynamic pacing and narrative beats.","We argue that text shortening is essential to producing fast paced videos that are compelling and visually interesting, and show that when beats are preserved in the output reel, topical alignment between them and the visual assets is crucial to the viewing experience.","Understanding narrative pacing and beats in creative forms is key to user experience of media.","They are an important primitive to effective editing, repurposing, and retargeting content while maintaining a cohesive narrative."],"url":"http://arxiv.org/abs/2403.18111v1","category":"cs.HC"}
{"created":"2024-03-26 21:04:18","title":"Mathematical Foundation and Corrections for Full Range Head Pose Estimation","abstract":"Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from provided code. This paper presents (1) code and algorithms for inferring coordinate system from provided source code, code for Euler angle application order and extracting precise rotation matrices and the Euler angles, (2) code and algorithms for converting poses from one rotation system to another, (3) novel formulae for 2D augmentations of the rotation matrices, and (4) derivations and code for the correct drawing routines for rotation matrices and poses. This paper also addresses the feasibility of defining rotations with right-handed coordinate system in Wikipedia and SciPy, which makes the Euler angle extraction much easier for full-range head pose research.","sentences":["Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region.","However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use.","It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order.","Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works.","In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles.","When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from provided code.","This paper presents (1) code and algorithms for inferring coordinate system from provided source code, code for Euler angle application order and extracting precise rotation matrices and the Euler angles, (2) code and algorithms for converting poses from one rotation system to another, (3) novel formulae for 2D augmentations of the rotation matrices, and (4) derivations and code for the correct drawing routines for rotation matrices and poses.","This paper also addresses the feasibility of defining rotations with right-handed coordinate system in Wikipedia and SciPy, which makes the Euler angle extraction much easier for full-range head pose research."],"url":"http://arxiv.org/abs/2403.18104v1","category":"cs.CV"}
{"created":"2024-03-26 19:53:28","title":"Workflow Mini-Apps: Portable, Scalable, Tunable & Faithful Representations of Scientific Workflows","abstract":"Workflows are critical for scientific discovery. However, the sophistication, heterogeneity, and scale of workflows make building, testing, and optimizing them increasingly challenging. Furthermore, their complexity and heterogeneity make performance reproducibility hard. In this paper, we propose workflow mini-apps as a tool to address the challenges in building and testing workflows while controlling the fidelity of representing realworld workflows. Workflow mini-apps are deployed and run on various HPC systems and architectures without workflow-specific constraints. We offer insight into their design and implementation, providing an analysis of their performance and reproducibility. Workflow mini-apps thus advance the science of workflows by providing simple, portable, and managed (fidelity) representations of otherwise complex and difficult-to-control real workflows.","sentences":["Workflows are critical for scientific discovery.","However, the sophistication, heterogeneity, and scale of workflows make building, testing, and optimizing them increasingly challenging.","Furthermore, their complexity and heterogeneity make performance reproducibility hard.","In this paper, we propose workflow mini-apps as a tool to address the challenges in building and testing workflows while controlling the fidelity of representing realworld workflows.","Workflow mini-apps are deployed and run on various HPC systems and architectures without workflow-specific constraints.","We offer insight into their design and implementation, providing an analysis of their performance and reproducibility.","Workflow mini-apps thus advance the science of workflows by providing simple, portable, and managed (fidelity) representations of otherwise complex and difficult-to-control real workflows."],"url":"http://arxiv.org/abs/2403.18073v1","category":"cs.DC"}
{"created":"2024-03-26 19:49:58","title":"Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo","abstract":"Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.","sentences":["Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data.","Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters.","However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner.","We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs.","In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive.","The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization.","We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field."],"url":"http://arxiv.org/abs/2403.18072v1","category":"stat.CO"}
{"created":"2024-03-26 19:41:55","title":"On the boundedness of solutions of a forced discontinuous oscillator","abstract":"We study the global boundedness of the solutions of a non-smooth forced oscillator with a periodic and real analytic forcing. We show that the impact map associated with this discontinuous equation becomes a real analytic and exact symplectic map when written in suitable canonical coordinates. By an accurate study of the behaviour of the map for large amplitudes and by employing a parametrization KAM theorem, we show that the periodic solutions of the unperturbed oscillator persist as two-dimensional tori under conditions that depend on the Diophantine conditions of the frequency, but are independent on both the amplitude of the orbit and of the specific value of the frequency. This allows the construction of a sequence of nested invariant tori of increasing amplitude that confine the solutions within them, ensuring their boundedness. The same construction may be useful to address such persistence problem for a larger class of non-smooth forced oscillators.","sentences":["We study the global boundedness of the solutions of a non-smooth forced oscillator with a periodic and real analytic forcing.","We show that the impact map associated with this discontinuous equation becomes a real analytic and exact symplectic map when written in suitable canonical coordinates.","By an accurate study of the behaviour of the map for large amplitudes and by employing a parametrization KAM theorem, we show that the periodic solutions of the unperturbed oscillator persist as two-dimensional tori under conditions that depend on the Diophantine conditions of the frequency, but are independent on both the amplitude of the orbit and of the specific value of the frequency.","This allows the construction of a sequence of nested invariant tori of increasing amplitude that confine the solutions within them, ensuring their boundedness.","The same construction may be useful to address such persistence problem for a larger class of non-smooth forced oscillators."],"url":"http://arxiv.org/abs/2403.18068v1","category":"math.DS"}
{"created":"2024-03-26 19:17:03","title":"Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing","abstract":"We study in this paper boundary stabilization, in the L2 sense, of the one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing. We assume that we measure the state of this spatio-temporal equation on a given spatial subdomain during certain intervals of time, while we measure the state on the remaining spatial subdomain during the remaining intervals of time. As a result, we assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains. Throughout the study, the destabilizing coefficient is assumed to be space-dependent and bounded but unknown. Adaptive boundary controllers are designed under different assumptions on the forcing term. In particular, when the forcing term is null, we guarantee global exponential stability of the origin. Furthermore, when the forcing term is bounded and admits a known upper bound, we guarantee input-to-state stability, and only global uniform ultimate boundedness is guaranteed when the upper bound is unknown. Numerical simulations are performed to illustrate our results","sentences":["We study in this paper boundary stabilization, in the L2 sense, of the one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing.","We assume that we measure the state of this spatio-temporal equation on a given spatial subdomain during certain intervals of time, while we measure the state on the remaining spatial subdomain during the remaining intervals of time.","As a result, we assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains.","Throughout the study, the destabilizing coefficient is assumed to be space-dependent and bounded but unknown.","Adaptive boundary controllers are designed under different assumptions on the forcing term.","In particular, when the forcing term is null, we guarantee global exponential stability of the origin.","Furthermore, when the forcing term is bounded and admits a known upper bound, we guarantee input-to-state stability, and only global uniform ultimate boundedness is guaranteed when the upper bound is unknown.","Numerical simulations are performed to illustrate our results"],"url":"http://arxiv.org/abs/2403.18055v1","category":"eess.SY"}
{"created":"2024-03-26 19:10:08","title":"R2D2 image reconstruction with model uncertainty quantification in radio astronomy","abstract":"The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imaging, and adopting a telescope-specific approach, multiple R2D2 instances were trained to encompass the most general observation setting of the Very Large Array (VLA). Simulations and real-data experiments confirm that: (i) R2D2's image estimation capability is superior to that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction capability (arising from series with only few DNNs) makes the computation of multiple reconstruction samples and of uncertainty maps practical even at large image dimension; (iii) it is characterized by a very low model uncertainty.","sentences":["The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy.","R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs.","In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models.","Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration.","The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality.","Focusing on RI imaging, and adopting a telescope-specific approach, multiple R2D2 instances were trained to encompass the most general observation setting of the Very Large Array (VLA).","Simulations and real-data experiments confirm that: (i) R2D2's image estimation capability is superior to that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction capability (arising from series with only few DNNs) makes the computation of multiple reconstruction samples and of uncertainty maps practical even at large image dimension; (iii) it is characterized by a very low model uncertainty."],"url":"http://arxiv.org/abs/2403.18052v1","category":"astro-ph.IM"}
{"created":"2024-03-26 18:52:50","title":"Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes","abstract":"Control barrier functions (CBFs) have recently been introduced as a systematic tool to ensure safety by establishing set invariance. When combined with a control Lyapunov function (CLF), they form a safety-critical control mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the system model. In practice, model uncertainty can jeopardize safety and stability guarantees and may lead to undesirable performance. In this paper, we develop a safe learning-based control strategy for switching systems in the face of uncertainty. We focus on the case that a nominal model is available for a true underlying switching system. This uncertainty results in piecewise residuals for each switching surface, impacting the CLF and CBF constraints. We introduce a batch multi-output Gaussian process (MOGP) framework to approximate these piecewise residuals, thereby mitigating the adverse effects of uncertainty. A particular structure of the covariance function enables us to convert the MOGP-based chance constraints CLF and CBF into second-order cone constraints, which leads to a convex optimization. We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility. The effectiveness of the proposed strategy is validated through a simulation of a switching adaptive cruise control system.","sentences":["Control barrier functions (CBFs) have recently been introduced as a systematic tool to ensure safety by establishing set invariance.","When combined with a control Lyapunov function (CLF), they form a safety-critical control mechanism.","However, the effectiveness of CBFs and CLFs is closely tied to the system model.","In practice, model uncertainty can jeopardize safety and stability guarantees and may lead to undesirable performance.","In this paper, we develop a safe learning-based control strategy for switching systems in the face of uncertainty.","We focus on the case that a nominal model is available for a true underlying switching system.","This uncertainty results in piecewise residuals for each switching surface, impacting the CLF and CBF constraints.","We introduce a batch multi-output Gaussian process (MOGP) framework to approximate these piecewise residuals, thereby mitigating the adverse effects of uncertainty.","A particular structure of the covariance function enables us to convert the MOGP-based chance constraints CLF and CBF into second-order cone constraints, which leads to a convex optimization.","We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility.","The effectiveness of the proposed strategy is validated through a simulation of a switching adaptive cruise control system."],"url":"http://arxiv.org/abs/2403.18041v1","category":"eess.SY"}
{"created":"2024-03-26 18:50:34","title":"Doubly robust causal inference through penalized bias-reduced estimation: combining non-probability samples with designed surveys","abstract":"Causal inference on the average treatment effect (ATE) using non-probability samples, such as electronic health records (EHR), faces challenges from sample selection bias and high-dimensional covariates. This requires considering a selection model alongside treatment and outcome models that are typical ingredients in causal inference. This paper considers integrating large non-probability samples with external probability samples from a design survey, addressing moderately high-dimensional confounders and variables that influence selection. In contrast to the two-step approach that separates variable selection and debiased estimation, we propose a one-step plug-in doubly robust (DR) estimator of the ATE. We construct a novel penalized estimating equation by minimizing the squared asymptotic bias of the DR estimator. Our approach facilitates ATE inference in high-dimensional settings by ignoring the variability in estimating nuisance parameters, which is not guaranteed in conventional likelihood approaches with non-differentiable L1-type penalties. We provide a consistent variance estimator for the DR estimator. Simulation studies demonstrate the double robustness of our estimator under misspecification of either the outcome model or the selection and treatment models, as well as the validity of statistical inference under penalized estimation. We apply our method to integrate EHR data from the Michigan Genomics Initiative with an external probability sample.","sentences":["Causal inference on the average treatment effect (ATE) using non-probability samples, such as electronic health records (EHR), faces challenges from sample selection bias and high-dimensional covariates.","This requires considering a selection model alongside treatment and outcome models that are typical ingredients in causal inference.","This paper considers integrating large non-probability samples with external probability samples from a design survey, addressing moderately high-dimensional confounders and variables that influence selection.","In contrast to the two-step approach that separates variable selection and debiased estimation, we propose a one-step plug-in doubly robust (DR) estimator of the ATE.","We construct a novel penalized estimating equation by minimizing the squared asymptotic bias of the DR estimator.","Our approach facilitates ATE inference in high-dimensional settings by ignoring the variability in estimating nuisance parameters, which is not guaranteed in conventional likelihood approaches with non-differentiable L1-type penalties.","We provide a consistent variance estimator for the DR estimator.","Simulation studies demonstrate the double robustness of our estimator under misspecification of either the outcome model or the selection and treatment models, as well as the validity of statistical inference under penalized estimation.","We apply our method to integrate EHR data from the Michigan Genomics Initiative with an external probability sample."],"url":"http://arxiv.org/abs/2403.18039v1","category":"stat.ME"}
{"created":"2024-03-26 18:41:07","title":"Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance","abstract":"Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.","sentences":["Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges.","These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets.","To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation.","Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions.","By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs.","Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE.","Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes."],"url":"http://arxiv.org/abs/2403.18036v1","category":"cs.CV"}
{"created":"2024-03-26 18:38:14","title":"The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation","abstract":"Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised methods based on back-translation. Overall, the success of unsupervised machine translation was far from being analytically guaranteed. Instead, it is another proof that languages of the world share deep similarities, and we hope to show how to identify which of these similarities can serve the development of unsupervised, cross-linguistic tools.","sentences":["Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation.","Theoretically, however, the method should not work in general.","We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties.","We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation.","We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation.","We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised methods based on back-translation.","Overall, the success of unsupervised machine translation was far from being analytically guaranteed.","Instead, it is another proof that languages of the world share deep similarities, and we hope to show how to identify which of these similarities can serve the development of unsupervised, cross-linguistic tools."],"url":"http://arxiv.org/abs/2403.18031v1","category":"cs.CL"}
{"created":"2024-03-26 18:04:17","title":"Equilibration of objective observables in a dynamical model of quantum measurements","abstract":"The challenge of understanding quantum measurement persists as a fundamental issue in modern physics. Particularly, the abrupt and energy-non-conserving collapse of the wave function appears to contradict classical thermodynamic laws. The contradiction can be resolved by considering measurement itself to be an entropy-increasing process, driven by the second law of thermodynamics. This proposal, dubbed the Measurement-Equilibration Hypothesis, builds on the Quantum Darwinism framework derived to explain the emergence of the classical world. Measurement outcomes thus emerge objectively from unitary dynamics via closed-system equilibration. Working within this framework, we construct the set of \\textit{`objectifying observables'} that best encode the measurement statistics of a system in an objective manner, and establish a measurement error bound to quantify the probability an observer will obtain an incorrect measurement outcome. Using this error bound, we show that the objectifying observables readily equilibrate on average under the set of Hamiltonians which preserve the outcome statistics on the measured system. Using a random matrix model for this set, we numerically determine the measurement error bound, finding that the error only approaches zero with increasing environment size when the environment is coarse-grained into so-called observer systems. This indicates the necessity of coarse-graining an environment for the emergence of objective measurement outcomes.","sentences":["The challenge of understanding quantum measurement persists as a fundamental issue in modern physics.","Particularly, the abrupt and energy-non-conserving collapse of the wave function appears to contradict classical thermodynamic laws.","The contradiction can be resolved by considering measurement itself to be an entropy-increasing process, driven by the second law of thermodynamics.","This proposal, dubbed the Measurement-Equilibration Hypothesis, builds on the Quantum Darwinism framework derived to explain the emergence of the classical world.","Measurement outcomes thus emerge objectively from unitary dynamics via closed-system equilibration.","Working within this framework, we construct the set of \\textit{`objectifying observables'} that best encode the measurement statistics of a system in an objective manner, and establish a measurement error bound to quantify the probability an observer will obtain an incorrect measurement outcome.","Using this error bound, we show that the objectifying observables readily equilibrate on average under the set of Hamiltonians which preserve the outcome statistics on the measured system.","Using a random matrix model for this set, we numerically determine the measurement error bound, finding that the error only approaches zero with increasing environment size when the environment is coarse-grained into so-called observer systems.","This indicates the necessity of coarse-graining an environment for the emergence of objective measurement outcomes."],"url":"http://arxiv.org/abs/2403.18016v1","category":"quant-ph"}
{"created":"2024-03-26 18:04:10","title":"A Constructive Method for Designing Safe Multirate Controllers for Differentially-Flat Systems","abstract":"We present a multi-rate control architecture that leverages fundamental properties of differential flatness to synthesize controllers for safety-critical nonlinear dynamical systems. We propose a two-layer architecture, where the high-level generates reference trajectories using a linear Model Predictive Controller, and the low-level tracks this reference using a feedback controller. The novelty lies in how we couple these layers, to achieve formal guarantees on recursive feasibility of the MPC problem, and safety of the nonlinear system. Furthermore, using differential flatness, we provide a constructive means to synthesize the multi-rate controller, thereby removing the need to search for suitable Lyapunov or barrier functions, or to approximately linearize/discretize nonlinear dynamics. We show the synthesized controller is a convex optimization problem, making it amenable to real-time implementations. The method is demonstrated experimentally on a ground rover and a quadruped robotic system.","sentences":["We present a multi-rate control architecture that leverages fundamental properties of differential flatness to synthesize controllers for safety-critical nonlinear dynamical systems.","We propose a two-layer architecture, where the high-level generates reference trajectories using a linear Model Predictive Controller, and the low-level tracks this reference using a feedback controller.","The novelty lies in how we couple these layers, to achieve formal guarantees on recursive feasibility of the MPC problem, and safety of the nonlinear system.","Furthermore, using differential flatness, we provide a constructive means to synthesize the multi-rate controller, thereby removing the need to search for suitable Lyapunov or barrier functions, or to approximately linearize/discretize nonlinear dynamics.","We show the synthesized controller is a convex optimization problem, making it amenable to real-time implementations.","The method is demonstrated experimentally on a ground rover and a quadruped robotic system."],"url":"http://arxiv.org/abs/2403.18015v1","category":"cs.SY"}
{"created":"2024-03-26 18:01:58","title":"Rindler Fluids from Gravitational Shockwaves","abstract":"We study a correspondence between gravitational shockwave geometry and its fluid description near a Rindler horizon in Minkowski spacetime. Utilizing the Petrov classification that describes algebraic symmetries for Lorentzian spaces, we establish an explicit mapping between a potential fluid and the shockwave metric perturbation, where the Einstein equation for the shockwave geometry is equivalent to the incompressibility condition of the fluid, augmented by a shockwave source. Then we consider an Ansatz of a stochastic quantum source for the potential fluid, which has the physical interpretation of shockwaves created by vacuum energy fluctuations. Under such circumstance, the Einstein equation, or equivalently, the incompressibility condition for the fluid, becomes a stochastic differential equation. By smearing the quantum source on a stretched horizon in a Lorentz invariant manner with a Planckian width (similarly to the membrane paradigm), we integrate fluctuations near the Rindler horizon to find an accumulated effect of the variance in the round-trip time of a photon traversing the horizon of a causal diamond.","sentences":["We study a correspondence between gravitational shockwave geometry and its fluid description near a Rindler horizon in Minkowski spacetime.","Utilizing the Petrov classification that describes algebraic symmetries for Lorentzian spaces, we establish an explicit mapping between a potential fluid and the shockwave metric perturbation, where the Einstein equation for the shockwave geometry is equivalent to the incompressibility condition of the fluid, augmented by a shockwave source.","Then we consider an Ansatz of a stochastic quantum source for the potential fluid, which has the physical interpretation of shockwaves created by vacuum energy fluctuations.","Under such circumstance, the Einstein equation, or equivalently, the incompressibility condition for the fluid, becomes a stochastic differential equation.","By smearing the quantum source on a stretched horizon in a Lorentz invariant manner with a Planckian width (similarly to the membrane paradigm), we integrate fluctuations near the Rindler horizon to find an accumulated effect of the variance in the round-trip time of a photon traversing the horizon of a causal diamond."],"url":"http://arxiv.org/abs/2403.18013v1","category":"hep-th"}
{"created":"2024-03-26 18:01:36","title":"A closer scrutiny of cosmic ray proton energy losses","abstract":"The percent-level precision attained by modern cosmic ray (CR) observations motivates reaching a comparable or better control of theoretical uncertainties. Here we focus on energy-loss processes affecting low-energy CR protons ($\\sim 0.1-5$ GeV), where the experimental errors are small and collisional effects play a comparatively larger role with respect to collisionless transport ones. We study three aspects of the problem: i) We quantitatively assess the role of the nuclear elastic cross-section, for the first time, providing analytical formulae for the stopping power and inelasticity. ii) We discuss the error arising from treating both elastic and pion production inelastic interactions as continuous energy loss processes, as opposed to catastrophic ones. The former is the approximation used in virtually all modern numerical calculations. iii) We consider sub-leading effects such as relativistic corrections, radiative and medium processes in ionization energy-losses. Our analysis reveals that neglecting i) leads to errors close to 1%, notably around and below 1 GeV; neglecting ii) leads to errors reaching about 3% within the considered energy range; iii) contributes to a minor effect, gauged at the level of 0.1%. Consequently, while iii) can currently be neglected, ii) warrants consideration, and we also recommend incorporating i) into computations. We conclude with some perspectives on further steps to be taken towards a high-precision goal of theoretical CR predictions regarding the treatment of energy-losses.","sentences":["The percent-level precision attained by modern cosmic ray (CR) observations motivates reaching a comparable or better control of theoretical uncertainties.","Here we focus on energy-loss processes affecting low-energy CR protons ($\\sim 0.1-5$ GeV), where the experimental errors are small and collisional effects play a comparatively larger role with respect to collisionless transport ones.","We study three aspects of the problem: i) We quantitatively assess the role of the nuclear elastic cross-section, for the first time, providing analytical formulae for the stopping power and inelasticity.","ii)","We discuss the error arising from treating both elastic and pion production inelastic interactions as continuous energy loss processes, as opposed to catastrophic ones.","The former is the approximation used in virtually all modern numerical calculations.","iii)","We consider sub-leading effects such as relativistic corrections, radiative and medium processes in ionization energy-losses.","Our analysis reveals that neglecting i) leads to errors close to 1%, notably around and below 1 GeV; neglecting ii) leads to errors reaching about 3% within the considered energy range; iii) contributes to a minor effect, gauged at the level of 0.1%.","Consequently, while iii) can currently be neglected, ii) warrants consideration, and we also recommend incorporating i) into computations.","We conclude with some perspectives on further steps to be taken towards a high-precision goal of theoretical CR predictions regarding the treatment of energy-losses."],"url":"http://arxiv.org/abs/2403.18012v1","category":"hep-ph"}
{"created":"2024-03-26 18:01:31","title":"On AdS$_4$ deformations of celestial symmetries","abstract":"Celestial holography has led to the discovery of new symmetry algebras arising from the study of collinear limits of perturbative gravity amplitudes in flat space. We explain from the twistor perspective how a non-vanishing cosmological constant $\\Lambda$ naturally modifies the celestial chiral algebra. The cosmological constant deforms the Poisson bracket on twistor space, so the corresponding deformed algebra of Hamiltonians under the new bracket is automatically consistent. This algebra is equivalent to that recently found by Taylor and Zhu. We find a number of variations of the deformed algebra. We give the Noether charges arising from the expression of this algebra as a symmetry of the twistor action for self-dual gravity with cosmological constant.","sentences":["Celestial holography has led to the discovery of new symmetry algebras arising from the study of collinear limits of perturbative gravity amplitudes in flat space.","We explain from the twistor perspective how a non-vanishing cosmological constant $\\Lambda$ naturally modifies the celestial chiral algebra.","The cosmological constant deforms the Poisson bracket on twistor space, so the corresponding deformed algebra of Hamiltonians under the new bracket is automatically consistent.","This algebra is equivalent to that recently found by Taylor and Zhu.","We find a number of variations of the deformed algebra.","We give the Noether charges arising from the expression of this algebra as a symmetry of the twistor action for self-dual gravity with cosmological constant."],"url":"http://arxiv.org/abs/2403.18011v1","category":"hep-th"}
{"created":"2024-03-26 18:00:05","title":"Typical thermalization of low-entanglement states","abstract":"Proving thermalization from the unitary evolution of a closed quantum system is one of the oldest questions that is still nowadays only partially resolved. Several efforts have led to various formulations of what is called the eigenstate thermalization hypothesis. The latter, however, assume initial states which are highly concentrated around a specific energy window and, as such, cannot account for a large class of states that are of paramount importance and that are operationally accessible in natural physical settings, including many experimental schemes for testing thermalization and for quantum simulation: low-entanglement states. In this work, we prove thermalization of these states under precise conditions that have operational significance. More specifically, we define a random energy smoothing - motivated by arguments of unavoidable finite resolution - on local Hamiltonians that lead to local thermalization when the initial state has low entanglement. Finally we show that such transformation affects neither the Gibbs state locally nor, under a mild condition, the short time dynamics.","sentences":["Proving thermalization from the unitary evolution of a closed quantum system is one of the oldest questions that is still nowadays only partially resolved.","Several efforts have led to various formulations of what is called the eigenstate thermalization hypothesis.","The latter, however, assume initial states which are highly concentrated around a specific energy window and, as such, cannot account for a large class of states that are of paramount importance and that are operationally accessible in natural physical settings, including many experimental schemes for testing thermalization and for quantum simulation: low-entanglement states.","In this work, we prove thermalization of these states under precise conditions that have operational significance.","More specifically, we define a random energy smoothing - motivated by arguments of unavoidable finite resolution - on local Hamiltonians that lead to local thermalization when the initial state has low entanglement.","Finally we show that such transformation affects neither the Gibbs state locally nor, under a mild condition, the short time dynamics."],"url":"http://arxiv.org/abs/2403.18007v1","category":"quant-ph"}
{"created":"2024-03-26 18:00:00","title":"The Relevance of Dynamical Friction for the MW/LMC/SMC Triple System","abstract":"Simulations of structure formation in the standard cold dark matter cosmological model quantify the dark matter halos of galaxies. Taking into account dynamical friction between the dark matter halos, we investigate the past orbital dynamical evolution of the Magellanic Clouds in the presence of the Galaxy. Our calculations are based on a three-body model of rigid Navarro-Frenk-White profiles for the dark matter halos, but were verified in a previous publication by comparison to high-resolution N-body simulations of live self-consistent systems. Under the requirement that the LMC and SMC had an encounter within 20 kpc between 1 and 4 Gyr ago, in order to allow the development of the Magellanic Stream, and using the latest astrometric data, the dynamical evolution of the MW/LMC/SMC system is calculated backwards in time. With the employment of the genetic algorithm and a Markov-Chain Monte-Carlo method, the present state of this system is unlikely with a probability of <10^{-9} (6 sigma complement), because solutions found do not fit into the error bars for the observed plane-of-sky velocity components of the Magellanic Clouds. This implies that orbital solutions that assume dark matter halos according to cosmological structure formation theory to exist around the Magellanic Clouds and the Milky Way are not possible with a confidence of more than 6 sigma","sentences":["Simulations of structure formation in the standard cold dark matter cosmological model quantify the dark matter halos of galaxies.","Taking into account dynamical friction between the dark matter halos, we investigate the past orbital dynamical evolution of the Magellanic Clouds in the presence of the Galaxy.","Our calculations are based on a three-body model of rigid Navarro-Frenk-White profiles for the dark matter halos, but were verified in a previous publication by comparison to high-resolution N-body simulations of live self-consistent systems.","Under the requirement that the LMC and SMC had an encounter within 20 kpc between 1 and 4 Gyr ago, in order to allow the development of the Magellanic Stream, and using the latest astrometric data, the dynamical evolution of the MW/LMC/SMC system is calculated backwards in time.","With the employment of the genetic algorithm and a Markov-Chain Monte-Carlo method, the present state of this system is unlikely with a probability of <10^{-9} (6 sigma complement), because solutions found do not fit into the error bars for the observed plane-of-sky velocity components of the Magellanic Clouds.","This implies that orbital solutions that assume dark matter halos according to cosmological structure formation theory to exist around the Magellanic Clouds and the Milky Way are not possible with a confidence of more than 6 sigma"],"url":"http://arxiv.org/abs/2403.17999v1","category":"astro-ph.GA"}
{"created":"2024-03-26 18:00:00","title":"Testing kinematic distances under a realistic Galactic potential","abstract":"Obtaining reliable distance estimates to gas clouds within the Milky Way is challenging in the absence of certain tracers. The kinematic distance approach has been used as an alternative, derived from the assumption of circular trajectories around the Galactic centre. Consequently, significant errors are expected in regions where gas flow deviates from purely circular motions. We aim to quantify the systematic errors that arise from the kinematic distance method in the presence of a Galactic potential that is non-axisymmetric. We investigate how these errors differ in certain regions of the Galaxy and how they relate to the underlying dynamics. We perform 2D isothermal hydrodynamical simulation of the gas disk with the moving-mesh code Arepo, adding the capability of using an external potential provided by the Agama library for galactic dynamics. We introduce a new analytic potential of the Milky Way, taking elements from existing models and adjusting parameters to match recent observational constraints. We find significant errors in the kinematic distance estimate for gas close to the Sun, along sight lines towards the Galactic centre and anti-centre, and significant deviations associated with the Galactic bar. Kinematic distance errors are low within the spiral arms as gas resides close to local potential minima and the resulting line-of-sight velocity is close to what is expected for an axisymmetric potential. Interarm regions exhibit large deviations at any given Galactic radius. This is caused by the gas being sped up or slowed down as it travels into or out of the spiral arm. We are able to define 'zones of avoidance' in the lv-diagram, where the kinematic distance method is particularly unreliable and should only be used with caution. We report a power law relation between the kinematic distance error and the deviation of the project line-of-sight velocity from circular motion.","sentences":["Obtaining reliable distance estimates to gas clouds within the Milky Way is challenging in the absence of certain tracers.","The kinematic distance approach has been used as an alternative, derived from the assumption of circular trajectories around the Galactic centre.","Consequently, significant errors are expected in regions where gas flow deviates from purely circular motions.","We aim to quantify the systematic errors that arise from the kinematic distance method in the presence of a Galactic potential that is non-axisymmetric.","We investigate how these errors differ in certain regions of the Galaxy and how they relate to the underlying dynamics.","We perform 2D isothermal hydrodynamical simulation of the gas disk with the moving-mesh code Arepo, adding the capability of using an external potential provided by the Agama library for galactic dynamics.","We introduce a new analytic potential of the Milky Way, taking elements from existing models and adjusting parameters to match recent observational constraints.","We find significant errors in the kinematic distance estimate for gas close to the Sun, along sight lines towards the Galactic centre and anti-centre, and significant deviations associated with the Galactic bar.","Kinematic distance errors are low within the spiral arms as gas resides close to local potential minima and the resulting line-of-sight velocity is close to what is expected for an axisymmetric potential.","Interarm regions exhibit large deviations at any given Galactic radius.","This is caused by the gas being sped up or slowed down as it travels into or out of the spiral arm.","We are able to define 'zones of avoidance' in the lv-diagram, where the kinematic distance method is particularly unreliable and should only be used with caution.","We report a power law relation between the kinematic distance error and the deviation of the project line-of-sight velocity from circular motion."],"url":"http://arxiv.org/abs/2403.18000v1","category":"astro-ph.GA"}
{"created":"2024-03-26 17:59:52","title":"Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval","abstract":"The increasing prevalence of video clips has sparked growing interest in text-video retrieval. Recent advances focus on establishing a joint embedding space for text and video, relying on consistent embedding representations to compute similarity. However, the text content in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video. Correspondingly, a single text embedding may be less expressive to capture the video embedding and empower the retrieval. In this study, we propose a new stochastic text modeling method T-MASS, i.e., text is modeled as a stochastic embedding, to enrich text embedding with a flexible and resilient semantic range, yielding a text mass. To be specific, we introduce a similarity-aware radius module to adapt the scale of the text mass upon the given text-video pairs. Plus, we design and develop a support text regularization to further control the text mass during the training. The inference pipeline is also tailored to fully exploit the text mass for accurate retrieval. Empirical evidence suggests that T-MASS not only effectively attracts relevant text-video pairs while distancing irrelevant ones, but also enables the determination of precise text embeddings for relevant pairs. Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.","sentences":["The increasing prevalence of video clips has sparked growing interest in text-video retrieval.","Recent advances focus on establishing a joint embedding space for text and video, relying on consistent embedding representations to compute similarity.","However, the text content in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video.","Correspondingly, a single text embedding may be less expressive to capture the video embedding and empower the retrieval.","In this study, we propose a new stochastic text modeling method T-MASS, i.e., text is modeled as a stochastic embedding, to enrich text embedding with a flexible and resilient semantic range, yielding a text mass.","To be specific, we introduce a similarity-aware radius module to adapt the scale of the text mass upon the given text-video pairs.","Plus, we design and develop a support text regularization to further control the text mass during the training.","The inference pipeline is also tailored to fully exploit the text mass for accurate retrieval.","Empirical evidence suggests that T-MASS not only effectively attracts relevant text-video pairs while distancing irrelevant ones, but also enables the determination of precise text embeddings for relevant pairs.","Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1).","Also, T-MASS achieves state-of-the-art performance on five benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades."],"url":"http://arxiv.org/abs/2403.17998v1","category":"cs.CV"}
{"created":"2024-03-27 17:23:45","title":"Breaking the Limitations with Sparse Inputs by Variational Frameworks (BLIss) in Terahertz Super-Resolution 3D Reconstruction","abstract":"Data acquisition, image processing, and image quality are the long-lasting issues for terahertz (THz) 3D reconstructed imaging. Existing methods are primarily designed for 2D scenarios, given the challenges associated with obtaining super-resolution (SR) data and the absence of an efficient SR 3D reconstruction framework in conventional computed tomography (CT). Here, we demonstrate BLIss, a new approach for THz SR 3D reconstruction with sparse 2D data input. BLIss seamlessly integrates conventional CT techniques and variational framework with the core of the adapted Euler-Elastica-based model. The quantitative 3D image evaluation metrics, including the standard deviation of Gaussian, mean curvatures, and the multi-scale structural similarity index measure (MS-SSIM), validate the superior smoothness and fidelity achieved with our variational framework approach compared with conventional THz CT modal. Beyond its contributions to advancing THz SR 3D reconstruction, BLIss demonstrates potential applicability in other imaging modalities, such as X-ray and MRI. This suggests extensive impacts on the broader field of imaging applications.","sentences":["Data acquisition, image processing, and image quality are the long-lasting issues for terahertz (THz) 3D reconstructed imaging.","Existing methods are primarily designed for 2D scenarios, given the challenges associated with obtaining super-resolution (SR) data and the absence of an efficient SR 3D reconstruction framework in conventional computed tomography (CT).","Here, we demonstrate BLIss, a new approach for THz SR 3D reconstruction with sparse 2D data input.","BLIss seamlessly integrates conventional CT techniques and variational framework with the core of the adapted Euler-Elastica-based model.","The quantitative 3D image evaluation metrics, including the standard deviation of Gaussian, mean curvatures, and the multi-scale structural similarity index measure (MS-SSIM), validate the superior smoothness and fidelity achieved with our variational framework approach compared with conventional THz CT modal.","Beyond its contributions to advancing THz SR 3D reconstruction, BLIss demonstrates potential applicability in other imaging modalities, such as X-ray and MRI.","This suggests extensive impacts on the broader field of imaging applications."],"url":"http://arxiv.org/abs/2403.18776v1","category":"physics.optics"}
{"created":"2024-03-27 16:50:12","title":"Full counting statistics of 1d short-range Riesz gases in confinement","abstract":"We investigate the full counting statistics (FCS) of a harmonically confined 1d short-range Riesz gas consisting of $N$ particles in equilibrium at finite temperature. The particles interact with each other through a repulsive power-law interaction with an exponent $k>1$ which includes the Calogero-Moser model for $k=2$. We examine the probability distribution of the number of particles in a finite domain $[-W, W]$ called number distribution, denoted by $\\mathcal{N}(W, N)$. We analyze the probability distribution of $\\mathcal{N}(W, N)$ and show that it exhibits a large deviation form for large $N$ characterised by a speed $N^{\\frac{3k+2}{k+2}}$ and by a large deviation function of the fraction $c = \\mathcal{N}(W, N)/N$ of the particles inside the domain and $W$. We show that the density profiles that create the large deviations display interesting shape transitions as one varies $c$ and $W$. This is manifested by a third-order phase transition exhibited by the large deviation function that has discontinuous third derivatives. Monte-Carlo (MC) simulations show good agreement with our analytical expressions for the corresponding density profiles. We find that the typical fluctuations of $\\mathcal{N}(W, N)$, obtained from our field theoretic calculations are Gaussian distributed with a variance that scales as $N^{\\nu_k}$, with $\\nu_k = (2-k)/(2+k)$. We also present some numerical findings on the mean and the variance. Furthermore, we adapt our formalism to study the index distribution (where the domain is semi-infinite $(-\\infty, W])$, linear statistics (the variance), thermodynamic pressure and bulk modulus.","sentences":["We investigate the full counting statistics (FCS) of a harmonically confined 1d short-range Riesz gas consisting of $N$ particles in equilibrium at finite temperature.","The particles interact with each other through a repulsive power-law interaction with an exponent $k>1$ which includes the Calogero-Moser model for $k=2$. We examine the probability distribution of the number of particles in a finite domain $[-W, W]$ called number distribution, denoted by $\\mathcal{N}(W, N)$. We analyze the probability distribution of $\\mathcal{N}(W, N)$ and show that it exhibits a large deviation form for large $N$ characterised by a speed $N^{\\frac{3k+2}{k+2}}$ and by a large deviation function of the fraction $c = \\mathcal{N}(W, N)/N$ of the particles inside the domain and $W$. We show that the density profiles that create the large deviations display interesting shape transitions as one varies $c$ and $W$. This is manifested by a third-order phase transition exhibited by the large deviation function that has discontinuous third derivatives.","Monte-Carlo (MC) simulations show good agreement with our analytical expressions for the corresponding density profiles.","We find that the typical fluctuations of $\\mathcal{N}(W, N)$, obtained from our field theoretic calculations are Gaussian distributed with a variance that scales as $N^{\\nu_k}$, with $\\nu_k = (2-k)/(2+k)$.","We also present some numerical findings on the mean and the variance.","Furthermore, we adapt our formalism to study the index distribution (where the domain is semi-infinite $(-\\infty, W])$, linear statistics (the variance), thermodynamic pressure and bulk modulus."],"url":"http://arxiv.org/abs/2403.18750v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-27 14:24:30","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image Editing","abstract":"Our work addresses limitations seen in previous approaches for object-centric editing problems, such as unrealistic results due to shape discrepancies and limited control in object replacement or insertion. To this end, we introduce FlexEdit, a flexible and controllable editing framework for objects where we iteratively adjust latents at each denoising step using our FlexEdit block. Initially, we optimize latents at test time to align with specified object constraints. Then, our framework employs an adaptive mask, automatically extracted during denoising, to protect the background while seamlessly blending new content into the target image. We demonstrate the versatility of FlexEdit in various object editing tasks and curate an evaluation test suite with samples from both real and synthetic images, along with novel evaluation metrics designed for object-centric editing. We conduct extensive experiments on different editing scenarios, demonstrating the superiority of our editing framework over recent advanced text-guided image editing methods. Our project page is published at https://flex-edit.github.io/.","sentences":["Our work addresses limitations seen in previous approaches for object-centric editing problems, such as unrealistic results due to shape discrepancies and limited control in object replacement or insertion.","To this end, we introduce FlexEdit, a flexible and controllable editing framework for objects where we iteratively adjust latents at each denoising step using our FlexEdit block.","Initially, we optimize latents at test time to align with specified object constraints.","Then, our framework employs an adaptive mask, automatically extracted during denoising, to protect the background while seamlessly blending new content into the target image.","We demonstrate the versatility of FlexEdit in various object editing tasks and curate an evaluation test suite with samples from both real and synthetic images, along with novel evaluation metrics designed for object-centric editing.","We conduct extensive experiments on different editing scenarios, demonstrating the superiority of our editing framework over recent advanced text-guided image editing methods.","Our project page is published at https://flex-edit.github.io/."],"url":"http://arxiv.org/abs/2403.18605v1","category":"cs.CV"}
{"created":"2024-03-27 13:00:06","title":"Wirtinger gradient descent methods for low-dose Poisson phase retrieval","abstract":"The problem of phase retrieval has many applications in the field of optical imaging. Motivated by imaging experiments with biological specimens, we primarily consider the setting of low-dose illumination where Poisson noise plays the dominant role. In this paper, we discuss gradient descent algorithms based on different loss functions adapted to data affected by Poisson noise, in particular in the low-dose regime. Starting from the maximum log-likelihood function for the Poisson distribution, we investigate different regularizations and approximations of the problem to design an algorithm that meets the requirements that are faced in applications. In the course of this, we focus on low-count measurements. For all suggested loss functions, we study the convergence of the respective gradient descent algorithms to stationary points and find constant step sizes that guarantee descent of the loss in each iteration. Numerical experiments in the low-dose regime are performed to corroborate the theoretical observations.","sentences":["The problem of phase retrieval has many applications in the field of optical imaging.","Motivated by imaging experiments with biological specimens, we primarily consider the setting of low-dose illumination where Poisson noise plays the dominant role.","In this paper, we discuss gradient descent algorithms based on different loss functions adapted to data affected by Poisson noise, in particular in the low-dose regime.","Starting from the maximum log-likelihood function for the Poisson distribution, we investigate different regularizations and approximations of the problem to design an algorithm that meets the requirements that are faced in applications.","In the course of this, we focus on low-count measurements.","For all suggested loss functions, we study the convergence of the respective gradient descent algorithms to stationary points and find constant step sizes that guarantee descent of the loss in each iteration.","Numerical experiments in the low-dose regime are performed to corroborate the theoretical observations."],"url":"http://arxiv.org/abs/2403.18527v1","category":"math.NA"}
{"created":"2024-03-27 09:20:30","title":"Joint distribution of $L$-values and orders of Sha groups of quadratic twists of elliptic curves","abstract":"We study the joint distribution of central $L$-values and orders of Tate-Shafarevich groups of quadratic twists of elliptic curves. In particular, adapting Radziwill and Soundararajan's principles of establishing upper and lower bounds for the distribution of central values in families of $L$-functions, we obtain conditional upper and lower bounds for such a joint distribution for rank zero twists. These lead us to a conjecture on the full asymptotic for the joint distribution.","sentences":["We study the joint distribution of central $L$-values and orders of Tate-Shafarevich groups of quadratic twists of elliptic curves.","In particular, adapting Radziwill and Soundararajan's principles of establishing upper and lower bounds for the distribution of central values in families of $L$-functions, we obtain conditional upper and lower bounds for such a joint distribution for rank zero twists.","These lead us to a conjecture on the full asymptotic for the joint distribution."],"url":"http://arxiv.org/abs/2403.18382v1","category":"math.NT"}
{"created":"2024-03-27 08:57:21","title":"BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models","abstract":"Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM. Extensive experiments conducted on public legal and medical benchmarks reveal that BLADE significantly outperforms existing approaches. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains.","sentences":["Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks.","However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc.","To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs.","Unfortunately, these strategies are either cost-intensive or unreliable in practical applications.","To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models.","BLADE consists of a black-box LLM and a small domain-specific LM.","The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities.","Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM.","Extensive experiments conducted on public legal and medical benchmarks reveal that BLADE significantly outperforms existing approaches.","This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains."],"url":"http://arxiv.org/abs/2403.18365v1","category":"cs.CL"}
{"created":"2024-03-27 08:53:13","title":"ViTAR: Vision Transformer with Any Resolution","abstract":"his paper tackles a significant challenge faced by Vision Transformers (ViTs): their constrained scalability across different image resolutions. Typically, ViTs experience a performance decline when processing resolutions different from those seen during training. Our work introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. Our resulting model, ViTAR (Vision Transformer with Any Resolution), demonstrates impressive adaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and 80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational costs. ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with self-supervised learning techniques like Masked AutoEncoder. Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing.","sentences":["his paper tackles a significant challenge faced by Vision Transformers (ViTs): their constrained scalability across different image resolutions.","Typically, ViTs experience a performance decline when processing resolutions different from those seen during training.","Our work introduces two key innovations to address this issue.","Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration.","Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution.","Our resulting model, ViTAR (Vision Transformer with Any Resolution), demonstrates impressive adaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and 80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational costs.","ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with self-supervised learning techniques like Masked AutoEncoder.","Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing."],"url":"http://arxiv.org/abs/2403.18361v1","category":"cs.CV"}
{"created":"2024-03-27 08:28:14","title":"H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for Tumor Segmentation in PET/CT Images","abstract":"Positron emission tomography (PET) combined with computed tomography (CT) imaging is routinely used in cancer diagnosis and prognosis by providing complementary information. Automatically segmenting tumors in PET/CT images can significantly improve examination efficiency. Traditional multi-modal segmentation solutions mainly rely on concatenation operations for modality fusion, which fail to effectively model the non-linear dependencies between PET and CT modalities. Recent studies have investigated various approaches to optimize the fusion of modality-specific features for enhancing joint representations. However, modality-specific encoders used in these methods operate independently, inadequately leveraging the synergistic relationships inherent in PET and CT modalities, for example, the complementarity between semantics and structure. To address these issues, we propose a Hierarchical Adaptive Interaction and Weighting Network termed H2ASeg to explore the intrinsic cross-modal correlations and transfer potential complementary information. Specifically, we design a Modality-Cooperative Spatial Attention (MCSA) module that performs intra- and inter-modal interactions globally and locally. Additionally, a Target-Aware Modality Weighting (TAMW) module is developed to highlight tumor-related features within multi-modal features, thereby refining tumor segmentation. By embedding these modules across different layers, H2ASeg can hierarchically model cross-modal correlations, enabling a nuanced understanding of both semantic and structural tumor features. Extensive experiments demonstrate the superiority of H2ASeg, outperforming state-of-the-art methods on AutoPet-II and Hecktor2022 benchmarks. The code is released at https://github.com/G14nTDo4/H2ASeg.","sentences":["Positron emission tomography (PET) combined with computed tomography (CT) imaging is routinely used in cancer diagnosis and prognosis by providing complementary information.","Automatically segmenting tumors in PET/CT images can significantly improve examination efficiency.","Traditional multi-modal segmentation solutions mainly rely on concatenation operations for modality fusion, which fail to effectively model the non-linear dependencies between PET and CT modalities.","Recent studies have investigated various approaches to optimize the fusion of modality-specific features for enhancing joint representations.","However, modality-specific encoders used in these methods operate independently, inadequately leveraging the synergistic relationships inherent in PET and CT modalities, for example, the complementarity between semantics and structure.","To address these issues, we propose a Hierarchical Adaptive Interaction and Weighting Network termed H2ASeg to explore the intrinsic cross-modal correlations and transfer potential complementary information.","Specifically, we design a Modality-Cooperative Spatial Attention (MCSA) module that performs intra- and inter-modal interactions globally and locally.","Additionally, a Target-Aware Modality Weighting (TAMW) module is developed to highlight tumor-related features within multi-modal features, thereby refining tumor segmentation.","By embedding these modules across different layers, H2ASeg can hierarchically model cross-modal correlations, enabling a nuanced understanding of both semantic and structural tumor features.","Extensive experiments demonstrate the superiority of H2ASeg, outperforming state-of-the-art methods on AutoPet-II and Hecktor2022 benchmarks.","The code is released at https://github.com/G14nTDo4/H2ASeg."],"url":"http://arxiv.org/abs/2403.18339v1","category":"eess.IV"}
{"created":"2024-03-27 08:16:33","title":"DODA: Diffusion for Object-detection Domain Adaptation in Agriculture","abstract":"The diverse and high-quality content generated by recent generative models demonstrates the great potential of using synthetic data to train downstream models. However, in vision, especially in objection detection, related areas are not fully explored, the synthetic images are merely used to balance the long tails of existing datasets, and the accuracy of the generated labels is low, the full potential of generative models has not been exploited. In this paper, we propose DODA, a data synthesizer that can generate high-quality object detection data for new domains in agriculture. Specifically, we improve the controllability of layout-to-image through encoding layout as an image, thereby improving the quality of labels, and use a visual encoder to provide visual clues for the diffusion model to decouple visual features from the diffusion model, and empowering the model the ability to generate data in new domains. On the Global Wheat Head Detection (GWHD) Dataset, which is the largest dataset in agriculture and contains diverse domains, using the data synthesized by DODA improves the performance of the object detector by 12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the training data.","sentences":["The diverse and high-quality content generated by recent generative models demonstrates the great potential of using synthetic data to train downstream models.","However, in vision, especially in objection detection, related areas are not fully explored, the synthetic images are merely used to balance the long tails of existing datasets, and the accuracy of the generated labels is low, the full potential of generative models has not been exploited.","In this paper, we propose DODA, a data synthesizer that can generate high-quality object detection data for new domains in agriculture.","Specifically, we improve the controllability of layout-to-image through encoding layout as an image, thereby improving the quality of labels, and use a visual encoder to provide visual clues for the diffusion model to decouple visual features from the diffusion model, and empowering the model the ability to generate data in new domains.","On the Global Wheat Head Detection (GWHD) Dataset, which is the largest dataset in agriculture and contains diverse domains, using the data synthesized by DODA improves the performance of the object detector by 12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the training data."],"url":"http://arxiv.org/abs/2403.18334v1","category":"cs.CV"}
{"created":"2024-03-27 07:52:51","title":"How to Cache Important Contents for Multi-modal Service in Dynamic Networks: A DRL-based Caching Scheme","abstract":"With the continuous evolution of networking technologies, multi-modal services that involve video, audio, and haptic contents are expected to become the dominant multimedia service in the near future. Edge caching is a key technology that can significantly reduce network load and content transmission latency, which is critical for the delivery of multi-modal contents. However, existing caching approaches only rely on a limited number of factors, e.g., popularity, to evaluate their importance for caching, which is inefficient for caching multi-modal contents, especially in dynamic network environments. To overcome this issue, we propose a content importance-based caching scheme which consists of a content importance evaluation model and a caching model. By leveraging dueling double deep Q networks (D3QN) model, the content importance evaluation model can adaptively evaluate contents' importance in dynamic networks. Based on the evaluated contents' importance, the caching model can easily cache and evict proper contents to improve caching efficiency. The simulation results show that the proposed content importance-based caching scheme outperforms existing caching schemes in terms of caching hit ratio (at least 15% higher), reduced network load (up to 22% reduction), average number of hops (up to 27% lower), and unsatisfied requests ratio (more than 47% reduction).","sentences":["With the continuous evolution of networking technologies, multi-modal services that involve video, audio, and haptic contents are expected to become the dominant multimedia service in the near future.","Edge caching is a key technology that can significantly reduce network load and content transmission latency, which is critical for the delivery of multi-modal contents.","However, existing caching approaches only rely on a limited number of factors, e.g., popularity, to evaluate their importance for caching, which is inefficient for caching multi-modal contents, especially in dynamic network environments.","To overcome this issue, we propose a content importance-based caching scheme which consists of a content importance evaluation model and a caching model.","By leveraging dueling double deep Q networks (D3QN) model, the content importance evaluation model can adaptively evaluate contents' importance in dynamic networks.","Based on the evaluated contents' importance, the caching model can easily cache and evict proper contents to improve caching efficiency.","The simulation results show that the proposed content importance-based caching scheme outperforms existing caching schemes in terms of caching hit ratio (at least 15% higher), reduced network load (up to 22% reduction), average number of hops (up to 27% lower), and unsatisfied requests ratio (more than 47% reduction)."],"url":"http://arxiv.org/abs/2403.18323v1","category":"cs.NI"}
{"created":"2024-03-27 06:37:51","title":"Efficient Test-Time Adaptation of Vision-Language Models","abstract":"Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA's superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in \\url{https://kdiaaa.github.io/tda/}.","sentences":["Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time.","Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation.","We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models.","TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys.","Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation.","In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions.","Extensive experiments over two benchmarks demonstrate TDA's superior effectiveness and efficiency as compared with the state-of-the-art.","The code has been released in \\url{https://kdiaaa.github.io/tda/}."],"url":"http://arxiv.org/abs/2403.18293v1","category":"cs.CV"}
{"created":"2024-03-27 06:17:21","title":"AIR-HLoc: Adaptive Image Retrieval for Efficient Visual Localisation","abstract":"State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on image retrieval (IR) techniques to establish 2D-3D correspondences by selecting the $k$ most similar images from a reference image database for a given query image. Although higher values of $k$ enhance localisation robustness, the computational cost for feature matching increases linearly with $k$. In this paper, we observe that queries that are the most similar to images in the database result in a higher proportion of feature matches and, thus, more accurate positioning. Thus, a small number of images is sufficient for queries very similar to images in the reference database. We then propose a novel approach, AIR-HLoc, which divides query images into different localisation difficulty levels based on their similarity to the reference image database. We consider an image with high similarity to the reference image as an easy query and an image with low similarity as a hard query. Easy queries show a limited improvement in accuracy when increasing $k$. Conversely, higher values of $k$ significantly improve accuracy for hard queries. Given the limited improvement in accuracy when increasing $k$ for easy queries and the significant improvement for hard queries, we adapt the value of $k$ to the query's difficulty level. Therefore, AIR-HLoc optimizes processing time by adaptively assigning different values of $k$ based on the similarity between the query and reference images without losing accuracy. Our extensive experiments on the Cambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate our algorithm's efficacy, reducing 30\\%, 26\\%, and 11\\% in computational overhead while maintaining SOTA accuracy compared to HLoc with fixed image retrieval.","sentences":["State-of-the-art (SOTA) hierarchical localisation pipelines (HLoc) rely on image retrieval (IR) techniques to establish 2D-3D correspondences by selecting the $k$ most similar images from a reference image database for a given query image.","Although higher values of $k$ enhance localisation robustness, the computational cost for feature matching increases linearly with $k$. In this paper, we observe that queries that are the most similar to images in the database result in a higher proportion of feature matches and, thus, more accurate positioning.","Thus, a small number of images is sufficient for queries very similar to images in the reference database.","We then propose a novel approach, AIR-HLoc, which divides query images into different localisation difficulty levels based on their similarity to the reference image database.","We consider an image with high similarity to the reference image as an easy query and an image with low similarity as a hard query.","Easy queries show a limited improvement in accuracy when increasing $k$. Conversely, higher values of $k$ significantly improve accuracy for hard queries.","Given the limited improvement in accuracy when increasing $k$ for easy queries and the significant improvement for hard queries, we adapt the value of $k$ to the query's difficulty level.","Therefore, AIR-HLoc optimizes processing time by adaptively assigning different values of $k$ based on the similarity between the query and reference images without losing accuracy.","Our extensive experiments on the Cambridge Landmarks, 7Scenes, and Aachen Day-Night-v1.1 datasets demonstrate our algorithm's efficacy, reducing 30\\%, 26\\%, and 11\\% in computational overhead while maintaining SOTA accuracy compared to HLoc with fixed image retrieval."],"url":"http://arxiv.org/abs/2403.18281v1","category":"cs.CV"}
{"created":"2024-03-27 05:57:45","title":"DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment","abstract":"Information inside visual and LiDAR data is well complementary derived from the fine-grained texture of images and massive geometric information in point clouds. However, it remains challenging to explore effective visual-LiDAR fusion, mainly due to the intrinsic data structure inconsistency between two modalities: Images are regular and dense, but LiDAR points are unordered and sparse. To address the problem, we propose a local-to-global fusion network with bi-directional structure alignment. To obtain locally fused features, we project points onto image plane as cluster centers and cluster image pixels around each center. Image pixels are pre-organized as pseudo points for image-to-point structure alignment. Then, we convert points to pseudo images by cylindrical projection (point-to-image structure alignment) and perform adaptive global feature fusion between point features with local fused features. Our method achieves state-of-the-art performance on KITTI odometry and FlyingThings3D scene flow datasets compared to both single-modal and multi-modal methods. Codes will be released later.","sentences":["Information inside visual and LiDAR data is well complementary derived from the fine-grained texture of images and massive geometric information in point clouds.","However, it remains challenging to explore effective visual-LiDAR fusion, mainly due to the intrinsic data structure inconsistency between two modalities: Images are regular and dense, but LiDAR points are unordered and sparse.","To address the problem, we propose a local-to-global fusion network with bi-directional structure alignment.","To obtain locally fused features, we project points onto image plane as cluster centers and cluster image pixels around each center.","Image pixels are pre-organized as pseudo points for image-to-point structure alignment.","Then, we convert points to pseudo images by cylindrical projection (point-to-image structure alignment) and perform adaptive global feature fusion between point features with local fused features.","Our method achieves state-of-the-art performance on KITTI odometry and FlyingThings3D scene flow datasets compared to both single-modal and multi-modal methods.","Codes will be released later."],"url":"http://arxiv.org/abs/2403.18274v1","category":"cs.CV"}
{"created":"2024-03-27 05:55:16","title":"Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding","abstract":"The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM's original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semi-supervised models relying on extensive unlabeled training data across various medical datasets. Our code is available at https://github.com/Cccccczh404/H-SAM.","sentences":["The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface.","However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance.","This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure.","In the initial stage, H-SAM employs SAM's original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage.","Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask.","Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details.","This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples.","Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices.","Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semi-supervised models relying on extensive unlabeled training data across various medical datasets.","Our code is available at https://github.com/Cccccczh404/H-SAM."],"url":"http://arxiv.org/abs/2403.18271v1","category":"cs.CV"}
{"created":"2024-03-27 05:38:48","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning","abstract":"Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need of modifying the original methods, retaining old data or models. We validate our method through incremental experiments on various benchmark datasets, demonstrating its effectiveness and practical value in real-world scenarios. We hope our work offers new insights for future continual self-supervised learning research. The code will be made publicly available.","sentences":["Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data.","However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining.","This poses a challenge in striking a balance between stability and plasticity when adapting to new information.","In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity.","Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL.","Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need of modifying the original methods, retaining old data or models.","We validate our method through incremental experiments on various benchmark datasets, demonstrating its effectiveness and practical value in real-world scenarios.","We hope our work offers new insights for future continual self-supervised learning research.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2403.18266v1","category":"cs.LG"}
{"created":"2024-03-27 04:03:55","title":"TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes","abstract":"As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.","sentences":["As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing.","The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response.","Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation.","To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target.","Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states.","Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion.","Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens.","Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content.","Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness."],"url":"http://arxiv.org/abs/2403.18238v1","category":"cs.CV"}
{"created":"2024-03-27 02:42:52","title":"NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation","abstract":"Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Project page: https://jingyanghuo.github.io/neuropictor/.","sentences":["Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models.","These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation.","In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals.","Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual.","NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions.","By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets.","Project page: https://jingyanghuo.github.io/neuropictor/."],"url":"http://arxiv.org/abs/2403.18211v1","category":"cs.CV"}
{"created":"2024-03-27 02:13:24","title":"LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators","abstract":"Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.","sentences":["Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments.","Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks.","However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces.","In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments.","By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace.","To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators.","Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation."],"url":"http://arxiv.org/abs/2403.18197v1","category":"cs.RO"}
{"created":"2024-03-27 02:06:25","title":"Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking","abstract":"RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges. We pioneer the use of the middle fusion framework for RGB-T tracking, which achieves a balance between performance and efficiency. Furthermore, we incorporate the pre-trained RGB tracking model into the framework and utilize multiple flexible prompt strategies to adapt the pre-trained model to the comprehensive exploration of uni-modal patterns and the improved modeling of fusion-modal features, harnessing the potential of prompt learning in RGB-T tracking. Our method outperforms the state-of-the-art methods on four challenging benchmarks, while attaining 46.1 fps inference speed.","sentences":["RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years.","Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data.","To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner.","However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios.","We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges.","We pioneer the use of the middle fusion framework for RGB-T tracking, which achieves a balance between performance and efficiency.","Furthermore, we incorporate the pre-trained RGB tracking model into the framework and utilize multiple flexible prompt strategies to adapt the pre-trained model to the comprehensive exploration of uni-modal patterns and the improved modeling of fusion-modal features, harnessing the potential of prompt learning in RGB-T tracking.","Our method outperforms the state-of-the-art methods on four challenging benchmarks, while attaining 46.1 fps inference speed."],"url":"http://arxiv.org/abs/2403.18193v1","category":"cs.CV"}
{"created":"2024-03-27 02:00:18","title":"Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples","abstract":"Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validated in multi-label data. In this study, we introduce a simple yet effective adaptive batch selection algorithm tailored to multi-label deep learning models. It adaptively selects each batch by prioritizing hard samples related to minority labels. A variant of our method also takes informative label correlations into consideration. Comprehensive experiments combining five multi-label deep learning models on thirteen benchmark datasets show that our method converges faster and performs better than random batch selection.","sentences":["Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains.","Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches.","However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch.","Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses.","Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data.","However, batch selection methods have not yet been applied and validated in multi-label data.","In this study, we introduce a simple yet effective adaptive batch selection algorithm tailored to multi-label deep learning models.","It adaptively selects each batch by prioritizing hard samples related to minority labels.","A variant of our method also takes informative label correlations into consideration.","Comprehensive experiments combining five multi-label deep learning models on thirteen benchmark datasets show that our method converges faster and performs better than random batch selection."],"url":"http://arxiv.org/abs/2403.18192v1","category":"cs.LG"}
{"created":"2024-03-27 01:15:05","title":"Multi-Layer Dense Attention Decoder for Polyp Segmentation","abstract":"Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer. This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area. Recently, vision Transformers have shown robust abilities in modeling global context for polyp segmentation. However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequate feature aggregation in the decoder. To address these issues, we propose a novel decoder architecture aimed at hierarchically aggregating locally enhanced multi-level dense features. Specifically, we introduce a novel module named Dense Attention Gate (DAG), which adaptively fuses all previous layers' features to establish local feature relations among all layers. Furthermore, we propose a novel nested decoder architecture that hierarchically aggregates decoder features, thereby enhancing semantic features. We incorporate our novel dense decoder with the PVT backbone network and conduct evaluations on five polyp segmentation datasets: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS. Our experiments and comparisons with nine competing segmentation models demonstrate that the proposed architecture achieves state-of-the-art performance and outperforms the previous models on four datasets. The source code is available at: https://github.com/krushi1992/Dense-Decoder.","sentences":["Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer.","This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area.","Recently, vision Transformers have shown robust abilities in modeling global context for polyp segmentation.","However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequate feature aggregation in the decoder.","To address these issues, we propose a novel decoder architecture aimed at hierarchically aggregating locally enhanced multi-level dense features.","Specifically, we introduce a novel module named Dense Attention Gate (DAG), which adaptively fuses all previous layers' features to establish local feature relations among all layers.","Furthermore, we propose a novel nested decoder architecture that hierarchically aggregates decoder features, thereby enhancing semantic features.","We incorporate our novel dense decoder with the PVT backbone network and conduct evaluations on five polyp segmentation datasets: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS.","Our experiments and comparisons with nine competing segmentation models demonstrate that the proposed architecture achieves state-of-the-art performance and outperforms the previous models on four datasets.","The source code is available at: https://github.com/krushi1992/Dense-Decoder."],"url":"http://arxiv.org/abs/2403.18180v1","category":"cs.CV"}
{"created":"2024-03-27 01:02:27","title":"Local (coarse) correlated equilibria in non-concave games","abstract":"We investigate local notions of correlated equilibria, distributions of actions for smooth games such that players do not incur any regret against modifications of their strategies along a set of continuous vector fields. Our analysis shows that such equilibria are intrinsically linked to the projected gradient dynamics of the game. We identify the equivalent of coarse equilibria in this setting when no regret is incurred against any gradient field of a differentiable function. As a result, such equilibria are approximable when all players employ online (projected) gradient ascent with equal step-sizes as learning algorithms, and when their compact and convex action sets either (1) possess a smooth boundary, or (2) are polyhedra over which linear optimisation is ``trivial''. As a consequence, primal-dual proofs of performance guarantees for local coarse equilibria take the form of a generalised Lyapunov function for the gradient dynamics of the game. Adapting the regret matching framework to our setting, we also show that general local correlated equilibria are approximable when the set of vector fields is finite, given access to a fixed-point oracle for linear or conical combinations. For the class of affine-linear vector fields, which subsumes correlated equilibria of normal form games as a special case, such a fixed-point turns out to be the solution of a convex quadratic minimisation problem. Our results are independent of concavity assumptions on players' utilities.","sentences":["We investigate local notions of correlated equilibria, distributions of actions for smooth games such that players do not incur any regret against modifications of their strategies along a set of continuous vector fields.","Our analysis shows that such equilibria are intrinsically linked to the projected gradient dynamics of the game.","We identify the equivalent of coarse equilibria in this setting when no regret is incurred against any gradient field of a differentiable function.","As a result, such equilibria are approximable when all players employ online (projected) gradient ascent with equal step-sizes as learning algorithms, and when their compact and convex action sets either (1) possess a smooth boundary, or (2) are polyhedra over which linear optimisation is ``trivial''.","As a consequence, primal-dual proofs of performance guarantees for local coarse equilibria take the form of a generalised Lyapunov function for the gradient dynamics of the game.","Adapting the regret matching framework to our setting, we also show that general local correlated equilibria are approximable when the set of vector fields is finite, given access to a fixed-point oracle for linear or conical combinations.","For the class of affine-linear vector fields, which subsumes correlated equilibria of normal form games as a special case, such a fixed-point turns out to be the solution of a convex quadratic minimisation problem.","Our results are independent of concavity assumptions on players' utilities."],"url":"http://arxiv.org/abs/2403.18174v1","category":"cs.GT"}
{"created":"2024-03-26 23:03:06","title":"HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks","abstract":"As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized graph Laplacians that ensures tighter bounds for our algorithm than existing spectral sparsifiers do. Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers.","sentences":["As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs.","Nevertheless, they still suffer from scalability challenges when it comes to the training cost.","Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees.","Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs.","In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee.","Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs.","Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized graph Laplacians that ensures tighter bounds for our algorithm than existing spectral sparsifiers do.","Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers."],"url":"http://arxiv.org/abs/2403.18142v1","category":"cs.LG"}
{"created":"2024-03-26 22:01:14","title":"Adaptive Loss Weighting for Machine Learning Interatomic Potentials","abstract":"Training machine learning interatomic potentials often requires optimizing a loss function composed of three variables: potential energies, forces, and stress. The contribution of each variable to the total loss is typically weighted using fixed coefficients. Identifying these coefficients usually relies on iterative or heuristic methods, which may yield sub-optimal   results. To address this issue, we propose an adaptive loss weighting algorithm that automatically adjusts the loss weights of these variables during the training of potentials, dynamically adapting to the characteristics of the training dataset. The comparative analysis of models trained with fixed and adaptive loss weights demonstrates that the adaptive method not only achieves a more balanced predictions across the three variables but also improves overall prediction accuracy.","sentences":["Training machine learning interatomic potentials often requires optimizing a loss function composed of three variables: potential energies, forces, and stress.","The contribution of each variable to the total loss is typically weighted using fixed coefficients.","Identifying these coefficients usually relies on iterative or heuristic methods, which may yield sub-optimal   results.","To address this issue, we propose an adaptive loss weighting algorithm that automatically adjusts the loss weights of these variables during the training of potentials, dynamically adapting to the characteristics of the training dataset.","The comparative analysis of models trained with fixed and adaptive loss weights demonstrates that the adaptive method not only achieves a more balanced predictions across the three variables but also improves overall prediction accuracy."],"url":"http://arxiv.org/abs/2403.18122v1","category":"physics.comp-ph"}
{"created":"2024-03-26 20:20:19","title":"Diffuse radio emission from merger shocks in simulated galaxy clusters","abstract":"Galaxy clusters are the largest gravitationally-bound structures in the Universe. As such, during merger events with similar systems, they release an enormous amount of energy that is dissipated through the formation of shock waves and turbulence in the intracluster medium (ICM), the hot ionised plasma permeating the cluster volume. These shock waves are believed to be ideal sites for electron acceleration that, in the presence of ubiquitous magnetic fields in the ICM, are capable of producing elongated non-thermal radio features typically observed in the outskirts of dynamically disturbed clusters, also known as radio relics. In this work, we analyse a hydrodynamical re-simulation of merging galaxy clusters, extracted from a large set of zoom-in cosmological simulations of The Three Hundred Project, to study the evolution and diversity of merger shocks and their associated diffuse radio emission within the framework of the diffusive shock acceleration scenario.","sentences":["Galaxy clusters are the largest gravitationally-bound structures in the Universe.","As such, during merger events with similar systems, they release an enormous amount of energy that is dissipated through the formation of shock waves and turbulence in the intracluster medium (ICM), the hot ionised plasma permeating the cluster volume.","These shock waves are believed to be ideal sites for electron acceleration that, in the presence of ubiquitous magnetic fields in the ICM, are capable of producing elongated non-thermal radio features typically observed in the outskirts of dynamically disturbed clusters, also known as radio relics.","In this work, we analyse a hydrodynamical re-simulation of merging galaxy clusters, extracted from a large set of zoom-in cosmological simulations of The Three Hundred Project, to study the evolution and diversity of merger shocks and their associated diffuse radio emission within the framework of the diffusive shock acceleration scenario."],"url":"http://arxiv.org/abs/2403.18089v1","category":"astro-ph.CO"}
{"created":"2024-03-26 19:24:40","title":"Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability","abstract":"We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years. For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline benchmark in the adversarial arrival model and in the unknown IID stochastic arrival model. In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits.","sentences":["We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years.","For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline benchmark in the adversarial arrival model and in the unknown IID stochastic arrival model.","In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits."],"url":"http://arxiv.org/abs/2403.18059v1","category":"cs.DS"}
{"created":"2024-03-26 12:23:34","title":"S+t-SNE -- Bringing dimensionality reduction to data streams","abstract":"We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.","sentences":["We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams.","The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios.","By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations.","Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics.","Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE.","The results highlight its ability to capture patterns in a streaming scenario.","We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data."],"url":"http://arxiv.org/abs/2403.17643v1","category":"cs.AI"}
{"created":"2024-03-25 11:47:53","title":"Graph Augmentation for Recommendation","abstract":"Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.","sentences":["Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited.","However, directly applying existing GCL models to real-world recommendation environments poses challenges.","There are two primary issues to address.","Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance.","Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing.","To address these challenges, we propose a principled framework called GraphAug.","This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems.","The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation.","Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model.","The outcomes consistently unveil its superiority over existing baseline methods.","The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug."],"url":"http://arxiv.org/abs/2403.16656v1","category":"cs.LG"}
{"created":"2024-03-25 11:37:15","title":"CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment","abstract":"Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``Helpful and Harmless'' dataset.","sentences":["Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users.","However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.","To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly.","CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process.","Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences.","Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``Helpful and Harmless'' dataset."],"url":"http://arxiv.org/abs/2403.16649v2","category":"cs.AI"}
{"created":"2024-03-25 11:29:19","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution","abstract":"Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details. While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures. Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process. In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts. Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts. Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image. Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism. It dynamically computes a reality score, enhancing the sharpness of the realistic latent. These alternating mechanisms collectively achieve artifact-free super-resolution. Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.","sentences":["Artifact-free super-resolution (SR) aims to translate low-resolution images into their high-resolution counterparts with a strict integrity of the original content, eliminating any distortions or synthetic details.","While traditional diffusion-based SR techniques have demonstrated remarkable abilities to enhance image detail, they are prone to artifact introduction during iterative procedures.","Such artifacts, ranging from trivial noise to unauthentic textures, deviate from the true structure of the source image, thus challenging the integrity of the super-resolution process.","In this work, we propose Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that delves into the latent space to effectively identify and mitigate the propagation of artifacts.","Our SARGD begins by using an artifact detector to identify implausible pixels, creating a binary mask that highlights artifacts.","Following this, the Reality Guidance Refinement (RGR) process refines artifacts by integrating this mask with realistic latent representations, improving alignment with the original image.","Nonetheless, initial realistic-latent representations from lower-quality images result in over-smoothing in the final output.","To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.","It dynamically computes a reality score, enhancing the sharpness of the realistic latent.","These alternating mechanisms collectively achieve artifact-free super-resolution.","Extensive experiments demonstrate the superiority of our method, delivering detailed artifact-free high-resolution images while reducing sampling steps by 2X. We release our code at https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git."],"url":"http://arxiv.org/abs/2403.16643v1","category":"eess.IV"}
{"created":"2024-03-25 09:43:56","title":"SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging","abstract":"Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.","sentences":["Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement.","Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance.","For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation.","Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset).","Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks.","This indicates that SegICL effectively address new segmentation tasks based on contextual information.","Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks.","Our code will be released soon."],"url":"http://arxiv.org/abs/2403.16578v1","category":"cs.CV"}
{"created":"2024-03-25 09:36:51","title":"NSINA: A News Corpus for Sinhala","abstract":"The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.","sentences":["The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources.","This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets.","In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation.","The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language.","NSINA is the largest news corpus for Sinhala, available up to date."],"url":"http://arxiv.org/abs/2403.16571v1","category":"cs.CL"}
{"created":"2024-03-25 09:18:48","title":"Active Admittance Control with Iterative Learning for General-Purpose Contact-Rich Manipulation","abstract":"Force interaction is inevitable when robots face multiple operation scenarios. How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem. Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism. The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model. The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility. Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method. Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively.","sentences":["Force interaction is inevitable when robots face multiple operation scenarios.","How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem.","Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism.","The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model.","The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility.","Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method.","Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively."],"url":"http://arxiv.org/abs/2403.16560v1","category":"cs.RO"}
{"created":"2024-03-25 08:36:06","title":"Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning","abstract":"Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.","sentences":["Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification.","Representations of textual data extract rich information spanning the domain, entities, and relations.","In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning.","While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped.","To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.","Our method employs contrastive learning to extract complementary discriminative information from these individual representations.","This is particularly relevant in low-resource settings where information is scarce.","Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available.","We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints."],"url":"http://arxiv.org/abs/2403.16543v1","category":"cs.CL"}
{"created":"2024-03-27 16:24:26","title":"Nonlinear model reduction for operator learning","abstract":"Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.","sentences":["Operator learning provides methods to approximate mappings between infinite-dimensional function spaces.","Deep operator networks (DeepONets) are a notable architecture in this field.","Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests.","We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning.","Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet."],"url":"http://arxiv.org/abs/2403.18735v1","category":"cs.LG"}
{"created":"2024-03-27 16:08:41","title":"On the scaling of random Tamari intervals and Schnyder woods of random triangulations (with an asymptotic D-finite trick)","abstract":"We consider a Tamari interval of size $n$ (i.e., a pair of Dyck paths which are comparable for the Tamari relation) chosen uniformly at random. We show that the height of a uniformly chosen vertex on the upper or lower path scales as $n^{3/4}$, and has an explicit limit law. By the Bernardi-Bonichon bijection, this result also describes the height of points in the canonical Schnyder trees of a uniform random plane triangulation of size $n$.   The exact solution of the model is based on polynomial equations with one and two catalytic variables. To prove the convergence from the exact solution, we use a version of moment pumping based on D-finiteness, which is essentially automatic and should apply to many other models. We are not sure to have seen this simple trick used before.   It would be interesting to study the universality of this convergence for decomposition trees associated to positive Bousquet-M\\'elou--Jehanne equations.","sentences":["We consider a Tamari interval of size $n$ (i.e., a pair of Dyck paths which are comparable for the Tamari relation) chosen uniformly at random.","We show that the height of a uniformly chosen vertex on the upper or lower path scales as $n^{3/4}$, and has an explicit limit law.","By the Bernardi-Bonichon bijection, this result also describes the height of points in the canonical Schnyder trees of a uniform random plane triangulation of size $n$.   The exact solution of the model is based on polynomial equations with one and two catalytic variables.","To prove the convergence from the exact solution, we use a version of moment pumping based on D-finiteness, which is essentially automatic and should apply to many other models.","We are not sure to have seen this simple trick used before.   ","It would be interesting to study the universality of this convergence for decomposition trees associated to positive Bousquet-M\\'elou--Jehanne equations."],"url":"http://arxiv.org/abs/2403.18719v1","category":"math.CO"}
{"created":"2024-03-27 15:58:33","title":"Phasing segmented telescopes via deep learning methods: application to a deployable CubeSat","abstract":"Capturing high resolution imagery of the Earth's surface often calls for a telescope of considerable size, even from Low Earth Orbits (LEO). A large aperture often requires large and expensive platforms. For instance, achieving a resolution of 1m at visible wavelengths from LEO typically requires an aperture diameter of at least 30cm. Additionally, ensuring high revisit times often prompts the use of multiple satellites. In light of these challenges, a small, segmented, deployable CubeSat telescope was recently proposed creating the additional need of phasing the telescope's mirrors. Phasing methods on compact platforms are constrained by the limited volume and power available, excluding solutions that rely on dedicated hardware or demand substantial computational resources. Neural Network (NN) are known for their computationally efficient inference and reduced on board requirements. Therefore we developed a NN based method to measure co phasing errors inherent to a deployable telescope. The proposed technique demonstrates its ability to detect phasing error at the targeted performance level (typically a wavefront error (WFE) below 15 nm RMS for a visible imager operating at the diffraction limit) using a point source. The robustness of the NN method is verified in presence of high order aberrations or noise and the results are compared against existing state of the art techniques. The developed NN model ensures its feasibility and provides a realistic pathway towards achieving diffraction limited images.","sentences":["Capturing high resolution imagery of the Earth's surface often calls for a telescope of considerable size, even from Low Earth Orbits (LEO).","A large aperture often requires large and expensive platforms.","For instance, achieving a resolution of 1m at visible wavelengths from LEO typically requires an aperture diameter of at least 30cm.","Additionally, ensuring high revisit times often prompts the use of multiple satellites.","In light of these challenges, a small, segmented, deployable CubeSat telescope was recently proposed creating the additional need of phasing the telescope's mirrors.","Phasing methods on compact platforms are constrained by the limited volume and power available, excluding solutions that rely on dedicated hardware or demand substantial computational resources.","Neural Network (NN) are known for their computationally efficient inference and reduced on board requirements.","Therefore we developed a NN based method to measure co phasing errors inherent to a deployable telescope.","The proposed technique demonstrates its ability to detect phasing error at the targeted performance level (typically a wavefront error (WFE) below 15 nm RMS for a visible imager operating at the diffraction limit) using a point source.","The robustness of the NN method is verified in presence of high order aberrations or noise and the results are compared against existing state of the art techniques.","The developed NN model ensures its feasibility and provides a realistic pathway towards achieving diffraction limited images."],"url":"http://arxiv.org/abs/2403.18712v1","category":"astro-ph.IM"}
{"created":"2024-03-27 14:45:05","title":"Lattice study of disordering of inhomogeneous condensates and the Quantum Pion Liquid in effective $O(N)$ model","abstract":"In this talk, we study a scalar $O(N)$ model with a so-called moat regime -- a regime with negative bosonic wave function renormalization -- using lattice field theory. For negative bare wave function renormalization, inhomogeneous condensates are solutions of the classical equations of motions. Using hybrid Monte Carlo simulations we demonstrate how bosonic quantum fluctuations disorder the inhomogeneous condensate. Instead, one finds a so-called Quantum Pion Liquid, where bosonic correlation functions are spatially oscillating, but also exponentially decaying.","sentences":["In this talk, we study a scalar $O(N)$ model with a so-called moat regime -- a regime with negative bosonic wave function renormalization -- using lattice field theory.","For negative bare wave function renormalization, inhomogeneous condensates are solutions of the classical equations of motions.","Using hybrid Monte Carlo simulations we demonstrate how bosonic quantum fluctuations disorder the inhomogeneous condensate.","Instead, one finds a so-called Quantum Pion Liquid, where bosonic correlation functions are spatially oscillating, but also exponentially decaying."],"url":"http://arxiv.org/abs/2403.18640v1","category":"hep-lat"}
{"created":"2024-03-27 14:24:13","title":"Negative radiation pressure in the abelian Higgs model","abstract":"Interactions of small-amplitude monochromatic plane waves with domain walls in (1+1) dimensional abelian Higgs model with a sextic potential were studied. The effective force exerted on a domain wall was derived from a linearized equation and compared with numerical simulations of the original model. It was shown that the domain walls always accelerate in one direction, regardless of the direction of the incoming wave. This implies that in some cases an effect called negative radiation pressure is observed, i.e. instead of pushing, the wave pulls the soliton.","sentences":["Interactions of small-amplitude monochromatic plane waves with domain walls in (1+1) dimensional abelian Higgs model with a sextic potential were studied.","The effective force exerted on a domain wall was derived from a linearized equation and compared with numerical simulations of the original model.","It was shown that the domain walls always accelerate in one direction, regardless of the direction of the incoming wave.","This implies that in some cases an effect called negative radiation pressure is observed, i.e. instead of pushing, the wave pulls the soliton."],"url":"http://arxiv.org/abs/2403.18603v1","category":"hep-th"}
{"created":"2024-03-27 14:19:52","title":"Remarks on a theorem of Eells and Sampson","abstract":"We prove an extension of Eells and Sampson's rigidity theorem for harmonic maps from a closed manifold of non-negative Ricci curvature to a manifold of non-positive sectional curvature. We give an application of our result in the setting of harmonic-Einstein (or Ricci-harmonic) metrics and as a consequence we recover a classical rigidity result of Hamilton for the problem of prescribed positive definite Ricci curvature.","sentences":["We prove an extension of Eells and Sampson's rigidity theorem for harmonic maps from a closed manifold of non-negative Ricci curvature to a manifold of non-positive sectional curvature.","We give an application of our result in the setting of harmonic-Einstein (or Ricci-harmonic) metrics and as a consequence we recover a classical rigidity result of Hamilton for the problem of prescribed positive definite Ricci curvature."],"url":"http://arxiv.org/abs/2403.18596v1","category":"math.DG"}
{"created":"2024-03-27 14:19:17","title":"Boundary scattering in massless $AdS_3$","abstract":"We study the boundary integrability problem of the massless sector of $AdS_3 \\times S^3 \\times T^4 $ string theory. Exploiting the difference-form of the massless scattering theory, we find a very simple and exhaustive list of reflection matrices for all the possible boundary coideal subalgebras - singlet and vector representations, right and left boundary - and check basic properties of our solutions, primarily the boundary Yang-Baxter equation, for all possible combinations of scattering particles.","sentences":["We study the boundary integrability problem of the massless sector of $AdS_3 \\times S^3 \\times T^4 $ string theory.","Exploiting the difference-form of the massless scattering theory, we find a very simple and exhaustive list of reflection matrices for all the possible boundary coideal subalgebras - singlet and vector representations, right and left boundary - and check basic properties of our solutions, primarily the boundary Yang-Baxter equation, for all possible combinations of scattering particles."],"url":"http://arxiv.org/abs/2403.18594v1","category":"hep-th"}
{"created":"2024-03-27 13:42:49","title":"Construction of Gross-Neveu model using Polchinski flow equation","abstract":"The Gross-Neveu model is a quantum field theory model of Dirac fermions in two dimensions with a quartic interaction term. Like Yang-Mills theory in four dimensions, the model is renormalizable (but not super-renormalizable) and asymptotically free (i.e. its short-distance behaviour is governed by the free theory). We give a new construction of the massive Euclidean Gross-Neveu model in infinite volume based on the renormalization group flow equation. The construction does not involve cluster expansion or discretization of phase-space. We express the Schwinger functions of the Gross-Neveu model in terms of the effective potential and construct the effective potential by solving the flow equation using the Banach fixed point theorem. Since we use crucially the fact that fermionic fields can be represented as bounded operators our construction does not extend to models including bosons. However, it is applicable to other asymptotically free purely fermionic theories such as the symplectic fermion model.","sentences":["The Gross-Neveu model is a quantum field theory model of Dirac fermions in two dimensions with a quartic interaction term.","Like Yang-Mills theory in four dimensions, the model is renormalizable (but not super-renormalizable) and asymptotically free (i.e. its short-distance behaviour is governed by the free theory).","We give a new construction of the massive Euclidean Gross-Neveu model in infinite volume based on the renormalization group flow equation.","The construction does not involve cluster expansion or discretization of phase-space.","We express the Schwinger functions of the Gross-Neveu model in terms of the effective potential and construct the effective potential by solving the flow equation using the Banach fixed point theorem.","Since we use crucially the fact that fermionic fields can be represented as bounded operators our construction does not extend to models including bosons.","However, it is applicable to other asymptotically free purely fermionic theories such as the symplectic fermion model."],"url":"http://arxiv.org/abs/2403.18562v1","category":"math-ph"}
{"created":"2024-03-27 12:48:38","title":"Multi-View Deep Learning for Imaging Atmospheric Cherenkov Telescopes","abstract":"This research note concerns the application of deep-learning-based multi-view-imaging techniques to data from the H.E.S.S. Imaging Atmospheric Cherenkov Telescope array. We find that the earlier the fusion of layer information from different views takes place in the neural network, the better our model performs with this data. Our analysis shows that the point in the network where the information from the different views is combined is far more important for the model performance than the method used to combine the information.","sentences":["This research note concerns the application of deep-learning-based multi-view-imaging techniques to data from the H.E.S.S. Imaging Atmospheric Cherenkov Telescope array.","We find that the earlier the fusion of layer information from different views takes place in the neural network, the better our model performs with this data.","Our analysis shows that the point in the network where the information from the different views is combined is far more important for the model performance than the method used to combine the information."],"url":"http://arxiv.org/abs/2403.18516v1","category":"astro-ph.IM"}
{"created":"2024-03-27 12:25:08","title":"A Novel Proton Decay Signature at DUNE, JUNO, and Hyper-K","abstract":"Proton decay, although unobserved so far, is a natural expectation when attempting to explain the baryon asymmetry of the universe. $p\\to K^+\\bar{\\nu}$ or $p\\to K^+\\tilde{\\chi}_1^0$, with $\\tilde{\\chi}_1^0$ a light exotic neutral particle, represent possible decay channels achievable in models of physics beyond the Standard Model, such as the MSSM with trilinear R-parity-violating terms, or the Standard Model extended by a heavy neutral lepton. Among the decay products of these modes, the neutral fermions would typically appear as missing energy in collider searches. The present study considers how such decay modes could be differentiated in experimental settings, as the exotic $\\tilde{\\chi}_1^0$ may further decay if it is not protected by a symmetry (such as R-parity in the MSSM). We assess the detection prospects of the proposed experiments DUNE, JUNO and Hyper-K in this context.","sentences":["Proton decay, although unobserved so far, is a natural expectation when attempting to explain the baryon asymmetry of the universe.","$p\\to K^+\\bar{\\nu}$ or $p\\to K^+\\tilde{\\chi}_1^0$, with $\\tilde{\\chi}_1^0$ a light exotic neutral particle, represent possible decay channels achievable in models of physics beyond the Standard Model, such as the MSSM with trilinear R-parity-violating terms, or the Standard Model extended by a heavy neutral lepton.","Among the decay products of these modes, the neutral fermions would typically appear as missing energy in collider searches.","The present study considers how such decay modes could be differentiated in experimental settings, as the exotic $\\tilde{\\chi}_1^0$ may further decay if it is not protected by a symmetry (such as R-parity in the MSSM).","We assess the detection prospects of the proposed experiments DUNE, JUNO and Hyper-K in this context."],"url":"http://arxiv.org/abs/2403.18502v1","category":"hep-ph"}
{"created":"2024-03-27 11:49:55","title":"Lightweight Embeddings for Graph Collaborative Filtering","abstract":"Graph neural networks (GNNs) are currently one of the most performant collaborative filtering methods. Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, GNN-based recommenders have inherited the long-standing defect of parameter inefficiency. As a common practice for scalable embeddings, parameter sharing enables the use of fewer embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user's/item's ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings. However, in the context of GNN-based collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction graph, leading to suboptimal recommendation performance. To this end, we propose Lightweight Embeddings for Graph Collaborative Filtering (LEGCF), a parameter-efficient embedding framework dedicated to GNN-based recommenders. LEGCF innovatively introduces an assignment matrix as an extra learnable component on top of meta-embeddings. To jointly optimize these two heavily entangled components, aside from learning the meta-embeddings by minimizing the recommendation loss, LEGCF further performs efficient assignment update by enforcing a novel semantic similarity constraint and finding its closed-form solution based on matrix pseudo-inverse. The meta-embeddings and assignment matrix are alternately updated, where the latter is sparsified on the fly to ensure negligible storage overhead. Extensive experiments on three benchmark datasets have verified LEGCF's smallest trade-off between size and performance, with consistent accuracy gain over state-of-the-art baselines. The codebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.","sentences":["Graph neural networks (GNNs) are currently one of the most performant collaborative filtering methods.","Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, GNN-based recommenders have inherited the long-standing defect of parameter inefficiency.","As a common practice for scalable embeddings, parameter sharing enables the use of fewer embedding vectors (i.e., meta-embeddings).","When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user's/item's ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings.","However, in the context of GNN-based collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction graph, leading to suboptimal recommendation performance.","To this end, we propose Lightweight Embeddings for Graph Collaborative Filtering (LEGCF), a parameter-efficient embedding framework dedicated to GNN-based recommenders.","LEGCF innovatively introduces an assignment matrix as an extra learnable component on top of meta-embeddings.","To jointly optimize these two heavily entangled components, aside from learning the meta-embeddings by minimizing the recommendation loss, LEGCF further performs efficient assignment update by enforcing a novel semantic similarity constraint and finding its closed-form solution based on matrix pseudo-inverse.","The meta-embeddings and assignment matrix are alternately updated, where the latter is sparsified on the fly to ensure negligible storage overhead.","Extensive experiments on three benchmark datasets have verified LEGCF's smallest trade-off between size and performance, with consistent accuracy gain over state-of-the-art baselines.","The codebase of LEGCF is available in https://github.com/xurong-liang/LEGCF."],"url":"http://arxiv.org/abs/2403.18479v1","category":"cs.IR"}
{"created":"2024-03-27 10:06:33","title":"The Topos of Transformer Networks","abstract":"The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.","sentences":["The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models.","We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory.","From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion.","In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners.","Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents."],"url":"http://arxiv.org/abs/2403.18415v1","category":"cs.LG"}
{"created":"2024-03-27 09:17:38","title":"Improvements to the theoretical estimates of the Schwarz preconditioner with $\u0394$-GenEO coarse space for the indefinite Helmholtz problem","abstract":"The purpose of this work is to improve the estimates for the $\\Delta$-GenEO method from the paper \"Overlapping Schwarz methods with GenEO coarse spaces for indefinite and nonself-adjoint problems\" by N. Bootland, V. Dolean, I. G Graham, C. Ma, R. Scheichl (https://doi.org/10.1093/imanum/drac036) when applied to the indefinite Helmholtz equation. We derive k-dependent estimates of quantities of interest ensuring the robustness of the method.","sentences":["The purpose of this work is to improve the estimates for the $\\Delta$-GenEO method from the paper \"Overlapping Schwarz methods with GenEO coarse spaces for indefinite and nonself-adjoint problems\" by N. Bootland, V. Dolean, I. G Graham, C. Ma, R. Scheichl (https://doi.org/10.1093/imanum/drac036) when applied to the indefinite Helmholtz equation.","We derive k-dependent estimates of quantities of interest ensuring the robustness of the method."],"url":"http://arxiv.org/abs/2403.18378v1","category":"math.NA"}
{"created":"2024-03-27 08:58:32","title":"Merits of Time-Domain Computing for VMM -- A Quantitative Comparison","abstract":"Vector-matrix-multiplication (VMM) accel-erators have gained a lot of traction, especially due to therise of convolutional neural networks (CNNs) and the desireto compute them on the edge. Besides the classical digitalapproach, analog computing has gone through a renais-sance to push energy efficiency further. A more recent ap-proach is called time-domain (TD) computing. In contrastto analog computing, TD computing permits easy technol-ogy as well as voltage scaling. As it has received limitedresearch attention, it is not yet clear which scenarios aremost suitable to be computed in the TD. In this work, weinvestigate these scenarios, focussing on energy efficiencyconsidering approximative computations that preserve ac-curacy. Both goals are addressed by a novel efficiency met-ric, which is used to find a baseline design. We use SPICEsimulation data which is fed into a python framework toevaluate how performance scales for VMM computation.We see that TD computing offers best energy efficiency forsmall to medium sized arrays. With throughput and sili-con footprint we investigate two additional metrics, givinga holistic comparison.","sentences":["Vector-matrix-multiplication (VMM) accel-erators have gained a lot of traction, especially due to therise of convolutional neural networks (CNNs) and the desireto compute them on the edge.","Besides the classical digitalapproach, analog computing has gone through a renais-sance to push energy efficiency further.","A more recent ap-proach is called time-domain (TD) computing.","In contrastto analog computing, TD computing permits easy technol-ogy as well as voltage scaling.","As it has received limitedresearch attention, it is not yet clear which scenarios aremost suitable to be computed in the TD.","In this work, weinvestigate these scenarios, focussing on energy efficiencyconsidering approximative computations that preserve ac-curacy.","Both goals are addressed by a novel efficiency met-ric, which is used to find a baseline design.","We use SPICEsimulation data which is fed into a python framework toevaluate how performance scales for VMM computation.","We see that TD computing offers best energy efficiency forsmall to medium sized arrays.","With throughput and sili-con footprint we investigate two additional metrics, givinga holistic comparison."],"url":"http://arxiv.org/abs/2403.18367v1","category":"cs.AR"}
{"created":"2024-03-27 08:34:39","title":"The Artificial Neural Twin -- Process Optimization and Continual Learning in Distributed Process Chains","abstract":"Industrial process optimization and control is crucial to increase economic and ecologic efficiency. However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation. Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular fine-tuning to accommodate distribution drifts. We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues. Our approach introduces differentiable data fusion to estimate the state of distributed process steps and their dependence on input data. By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model fine-tuning to process parameters or AI models respectively. The concept is demonstrated on a virtual machine park simulated in Unity, consisting of bulk material processes in plastic recycling.","sentences":["Industrial process optimization and control is crucial to increase economic and ecologic efficiency.","However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation.","Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular fine-tuning to accommodate distribution drifts.","We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues.","Our approach introduces differentiable data fusion to estimate the state of distributed process steps and their dependence on input data.","By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model fine-tuning to process parameters or AI models respectively.","The concept is demonstrated on a virtual machine park simulated in Unity, consisting of bulk material processes in plastic recycling."],"url":"http://arxiv.org/abs/2403.18343v1","category":"cs.LG"}
{"created":"2024-03-27 07:45:04","title":"Doppler-assisted quantum resonances through swappable excitation pathways in Potassium vapor","abstract":"We report the observation of two additional sub-natural line width quantum interference in the $D_2$ manifold of $^{39}K$ vapor, in addition to the usual single Electromagnetically induced transparency peak. The other two features appear exclusively because $^{39}K$ ground hyperfine splitting is smaller than the Doppler broadened absorption profile. This allows probe and control beams to swap their transition pathways. The control beam detuning captures the nature of the coherence, therefore an unusual phenomenon of conversion from perfect transparency to enhanced absorption is observed and explained by utilizing adiabatic elimination of the excited state in the Master equation. Controlling such dark and bright resonances leads to new applications in quantum technologies viz. frequency offset laser stabilization and long-lived quantum memory.","sentences":["We report the observation of two additional sub-natural line width quantum interference in the $D_2$ manifold of $^{39}K$ vapor, in addition to the usual single Electromagnetically induced transparency peak.","The other two features appear exclusively because $^{39}K$ ground hyperfine splitting is smaller than the Doppler broadened absorption profile.","This allows probe and control beams to swap their transition pathways.","The control beam detuning captures the nature of the coherence, therefore an unusual phenomenon of conversion from perfect transparency to enhanced absorption is observed and explained by utilizing adiabatic elimination of the excited state in the Master equation.","Controlling such dark and bright resonances leads to new applications in quantum technologies viz.","frequency offset laser stabilization and long-lived quantum memory."],"url":"http://arxiv.org/abs/2403.18319v1","category":"physics.atom-ph"}
{"created":"2024-03-27 06:40:26","title":"Multi-scale Unified Network for Image Classification","abstract":"Convolutional Neural Networks (CNNs) have advanced significantly in visual representation learning and recognition. However, they face notable challenges in performance and computational efficiency when dealing with real-world, multi-scale image inputs. Conventional methods rescale all input images into a fixed size, wherein a larger fixed size favors performance but rescaling small size images to a larger size incurs digitization noise and increased computation cost. In this work, we carry out a comprehensive, layer-wise investigation of CNN models in response to scale variation, based on Centered Kernel Alignment (CKA) analysis. The observations reveal lower layers are more sensitive to input image scale variations than high-level layers. Inspired by this insight, we propose Multi-scale Unified Network (MUSN) consisting of multi-scale subnets, a unified network, and scale-invariant constraint. Our method divides the shallow layers into multi-scale subnets to enable feature extraction from multi-scale inputs, and the low-level features are unified in deep layers for extracting high-level semantic features. A scale-invariant constraint is posed to maintain feature consistency across different scales. Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate that MSUN achieves significant improvements in both model performance and computational efficiency. Particularly, MSUN yields an accuracy increase up to 44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios.","sentences":["Convolutional Neural Networks (CNNs) have advanced significantly in visual representation learning and recognition.","However, they face notable challenges in performance and computational efficiency when dealing with real-world, multi-scale image inputs.","Conventional methods rescale all input images into a fixed size, wherein a larger fixed size favors performance but rescaling small size images to a larger size incurs digitization noise and increased computation cost.","In this work, we carry out a comprehensive, layer-wise investigation of CNN models in response to scale variation, based on Centered Kernel Alignment (CKA) analysis.","The observations reveal lower layers are more sensitive to input image scale variations than high-level layers.","Inspired by this insight, we propose Multi-scale Unified Network (MUSN) consisting of multi-scale subnets, a unified network, and scale-invariant constraint.","Our method divides the shallow layers into multi-scale subnets to enable feature extraction from multi-scale inputs, and the low-level features are unified in deep layers for extracting high-level semantic features.","A scale-invariant constraint is posed to maintain feature consistency across different scales.","Extensive experiments on ImageNet and other scale-diverse datasets, demonstrate that MSUN achieves significant improvements in both model performance and computational efficiency.","Particularly, MSUN yields an accuracy increase up to 44.53% and diminishes FLOPs by 7.01-16.13% in multi-scale scenarios."],"url":"http://arxiv.org/abs/2403.18294v1","category":"cs.CV"}
{"created":"2024-03-27 06:27:25","title":"Exploring the pre-inflationary dynamics in loop quantum cosmology with a DBI scalar field","abstract":"Loop quantum cosmology is a symmetry-reduced application of loop quantum gravity. The theory predicts a bounce for the universe at the Planck scale and resolves the singularity of standard cosmology. The dynamics is also governed by an effective Hamiltonian, which predicts a modified Friedmann equation containing the quadratic terms of the energy density. The term plays an essential role in the high energy regime, but the equations return to the standard form in the low energy regime. The evolution of the universe in the pre-inflationary period is studied in the framework of loop quantum cosmology, where the DBI scalar field is assumed to be the dominant component of the universe. Using the numerical method, we provide the evolution of the DBI field. The background evolution shows that there are three phases as: bouncing phase, transition phase and slow-roll inflationary phase. There is also a short period of super-inflation just at the beginning of the bounce phase. The field first climbs the potential and then reaches the turning point where $\\dot{\\phi}$ disappears and the potential energy becomes the dominant part of the energy density. This is the time when the slow roll inflation begins and the field slowly rolls down the potential. The results indicate that there are a few e-fold expansions in the bounce phase, about $N = $, and the universe experiences about $N = 59$ e-fold expansions in the slow-roll inflation phase.","sentences":["Loop quantum cosmology is a symmetry-reduced application of loop quantum gravity.","The theory predicts a bounce for the universe at the Planck scale and resolves the singularity of standard cosmology.","The dynamics is also governed by an effective Hamiltonian, which predicts a modified Friedmann equation containing the quadratic terms of the energy density.","The term plays an essential role in the high energy regime, but the equations return to the standard form in the low energy regime.","The evolution of the universe in the pre-inflationary period is studied in the framework of loop quantum cosmology, where the DBI scalar field is assumed to be the dominant component of the universe.","Using the numerical method, we provide the evolution of the DBI field.","The background evolution shows that there are three phases as: bouncing phase, transition phase and slow-roll inflationary phase.","There is also a short period of super-inflation just at the beginning of the bounce phase.","The field first climbs the potential and then reaches the turning point where $\\dot{\\phi}$ disappears and the potential energy becomes the dominant part of the energy density.","This is the time when the slow roll inflation begins and the field slowly rolls down the potential.","The results indicate that there are a few e-fold expansions in the bounce phase, about $N = $, and the universe experiences about $N = 59$ e-fold expansions in the slow-roll inflation phase."],"url":"http://arxiv.org/abs/2403.18289v1","category":"gr-qc"}
{"created":"2024-03-27 06:26:27","title":"Frozen Gaussian approximation for the fractional Schr\u00f6dinger equation","abstract":"We develop the frozen Gaussian approximation (FGA) for the fractional Schr\\\"odinger equation in the semi-classical regime, where the solution is highly oscillatory when the scaled Planck constant $\\varepsilon$ is small. This method approximates the solution to the Schr\\\"odinger equation by an integral representation based on asymptotic analysis and provides a highly efficient computational method for high-frequency wave function evolution. In particular, we revise the standard FGA formula to address the singularities arising in the higher-order derivatives of coefficients of the associated Hamiltonian flow that are second-order continuously differentiable or smooth in conventional FGA analysis. We then establish its convergence to the true solution. Additionally, we provide some numerical examples to verify the accuracy and convergence behavior of the frozen Gaussian approximation method.","sentences":["We develop the frozen Gaussian approximation (FGA) for the fractional Schr\\\"odinger equation in the semi-classical regime, where the solution is highly oscillatory when the scaled Planck constant $\\varepsilon$ is small.","This method approximates the solution to the Schr\\\"odinger equation by an integral representation based on asymptotic analysis and provides a highly efficient computational method for high-frequency wave function evolution.","In particular, we revise the standard FGA formula to address the singularities arising in the higher-order derivatives of coefficients of the associated Hamiltonian flow that are second-order continuously differentiable or smooth in conventional FGA analysis.","We then establish its convergence to the true solution.","Additionally, we provide some numerical examples to verify the accuracy and convergence behavior of the frozen Gaussian approximation method."],"url":"http://arxiv.org/abs/2403.18287v1","category":"math.NA"}
{"created":"2024-03-27 05:36:03","title":"A Deep Learning Framework for Disentangling Triangle Singularity and Pole-Based Enhancements","abstract":"Enhancements in the invariant mass distribution or scattering cross-section are usually associated with resonances. However, the nature of exotic signals found near hadron-hadron thresholds remain a puzzle today. In fact, a purely kinematic triangle mechanism is also capable of producing similar structures, but do not correspond to any unstable quantum state. In this paper, we report for the first time, that a deep neural network can be trained to distinguish triangle singularity from pole-based enhancements. We also identify the type of dynamic pole structure that can be misidentified as triangle enhancement. We apply our method to confirm that the $P_\\psi^N(4312)^+$ state is not due to a single triangle singularity, and the pole-based interpretation is more favored for this signal. Lastly, we argue how our method can be used as a model-selection framework to help in classifying other exotic hadron candidates.","sentences":["Enhancements in the invariant mass distribution or scattering cross-section are usually associated with resonances.","However, the nature of exotic signals found near hadron-hadron thresholds remain a puzzle today.","In fact, a purely kinematic triangle mechanism is also capable of producing similar structures, but do not correspond to any unstable quantum state.","In this paper, we report for the first time, that a deep neural network can be trained to distinguish triangle singularity from pole-based enhancements.","We also identify the type of dynamic pole structure that can be misidentified as triangle enhancement.","We apply our method to confirm that the $P_\\psi^N(4312)^+$ state is not due to a single triangle singularity, and the pole-based interpretation is more favored for this signal.","Lastly, we argue how our method can be used as a model-selection framework to help in classifying other exotic hadron candidates."],"url":"http://arxiv.org/abs/2403.18265v1","category":"hep-ph"}
{"created":"2024-03-27 05:34:29","title":"Analytic Approach for Computation of Topological Number of Integrable Vortex on Torus","abstract":"An analytic method to calculate the vortex number on a torus is constructed, focusing on analytic vortex solutions to the Chern-Simons-Higgs theory, whose governing equation is the so-called Jackiw-Pi equation. The equation is one of the integrable vortex equations and is reduced to Liouville's equation. The requirement of continuity of the Higgs field strongly restricts the characteristics and the fundamental domain of the vortices. Also considered are the decompactification limits of the vortices on a torus, in which \"flux loss\" phenomena occasionally occur.","sentences":["An analytic method to calculate the vortex number on a torus is constructed, focusing on analytic vortex solutions to the Chern-Simons-Higgs theory, whose governing equation is the so-called Jackiw-Pi equation.","The equation is one of the integrable vortex equations and is reduced to Liouville's equation.","The requirement of continuity of the Higgs field strongly restricts the characteristics and the fundamental domain of the vortices.","Also considered are the decompactification limits of the vortices on a torus, in which \"flux loss\" phenomena occasionally occur."],"url":"http://arxiv.org/abs/2403.18264v1","category":"hep-th"}
{"created":"2024-03-27 04:54:23","title":"Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications","abstract":"This paper proposes a new distributed nonconvex stochastic optimization algorithm that can achieve privacy protection, communication efficiency and convergence simultaneously. Specifically, each node adds time-varying privacy noises to its local state to avoid information leakage, and then quantizes its noise-perturbed state before transmitting to improve communication efficiency. By employing the subsampling method controlled through the sample-size parameter, the proposed algorithm reduces the impact of privacy noises, and enhances the differential privacy level. When the global cost function satisfies the Polyak-Lojasiewicz condition, the mean and high-probability convergence rate and the oracle complexity of the proposed algorithm are given. Importantly, the proposed algorithm achieves both the mean convergence and a finite cumulative differential privacy budget over infinite iterations as the sample-size goes to infinity. A numerical example of the distributed training on the \"MNIST\" dataset is given to show the effectiveness of the algorithm.","sentences":["This paper proposes a new distributed nonconvex stochastic optimization algorithm that can achieve privacy protection, communication efficiency and convergence simultaneously.","Specifically, each node adds time-varying privacy noises to its local state to avoid information leakage, and then quantizes its noise-perturbed state before transmitting to improve communication efficiency.","By employing the subsampling method controlled through the sample-size parameter, the proposed algorithm reduces the impact of privacy noises, and enhances the differential privacy level.","When the global cost function satisfies the Polyak-Lojasiewicz condition, the mean and high-probability convergence rate and the oracle complexity of the proposed algorithm are given.","Importantly, the proposed algorithm achieves both the mean convergence and a finite cumulative differential privacy budget over infinite iterations as the sample-size goes to infinity.","A numerical example of the distributed training on the \"MNIST\" dataset is given to show the effectiveness of the algorithm."],"url":"http://arxiv.org/abs/2403.18254v1","category":"eess.SY"}
{"created":"2024-03-27 04:39:13","title":"Statistical Inference of Optimal Allocations I: Regularities and their Implications","abstract":"In this paper, we develp a functional differentiability approach for solving statistical optimal allocation problems. We first derive Hadamard differentiability of the value function through a detailed analysis of the general properties of the sorting operator. Central to our framework are the concept of Hausdorff measure and the area and coarea integration formulas from geometric measure theory. Building on our Hadamard differentiability results, we demonstrate how the functional delta method can be used to directly derive the asymptotic properties of the value function process for binary constrained optimal allocation problems, as well as the two-step ROC curve estimator. Moreover, leveraging profound insights from geometric functional analysis on convex and local Lipschitz functionals, we obtain additional generic Fr\\'echet differentiability results for the value functions of optimal allocation problems. These compelling findings motivate us to study carefully the first order approximation of the optimal social welfare. In this paper, we then present a double / debiased estimator for the value functions. Importantly, the conditions outlined in the Hadamard differentiability section validate the margin assumption from the statistical classification literature employing plug-in methods that justifies a faster convergence rate.","sentences":["In this paper, we develp a functional differentiability approach for solving statistical optimal allocation problems.","We first derive Hadamard differentiability of the value function through a detailed analysis of the general properties of the sorting operator.","Central to our framework are the concept of Hausdorff measure and the area and coarea integration formulas from geometric measure theory.","Building on our Hadamard differentiability results, we demonstrate how the functional delta method can be used to directly derive the asymptotic properties of the value function process for binary constrained optimal allocation problems, as well as the two-step ROC curve estimator.","Moreover, leveraging profound insights from geometric functional analysis on convex and local Lipschitz functionals, we obtain additional generic Fr\\'echet differentiability results for the value functions of optimal allocation problems.","These compelling findings motivate us to study carefully the first order approximation of the optimal social welfare.","In this paper, we then present a double / debiased estimator for the value functions.","Importantly, the conditions outlined in the Hadamard differentiability section validate the margin assumption from the statistical classification literature employing plug-in methods that justifies a faster convergence rate."],"url":"http://arxiv.org/abs/2403.18248v1","category":"econ.EM"}
{"created":"2024-03-27 04:18:16","title":"Flows in the Space of Interacting Chiral Boson Theories","abstract":"We study interacting theories of $N$ left-moving and $\\overline{N}$ right-moving Floreanini-Jackiw bosons in two dimensions. A parameterized family of such theories is shown to enjoy (non-manifest) Lorentz invariance if and only if its Lagrangian obeys a flow equation driven by a function of the energy-momentum tensor. We discuss the canonical quantization of such theories along classical stress tensor flows, focusing on the case of the root-$T \\overline{T}$ deformation, where we obtain perturbative results for the deformed spectrum in a certain large-momentum limit. In the special case $N = \\overline{N}$, we consider the quantum effective action for the root-$T \\overline{T}$-deformed theory by expanding around a general classical background, and we find that the one-loop contribution vanishes for backgrounds with constant scalar gradients. Our analysis can also be interpreted via dual $U(1)$ Chern-Simons theories in three dimensions, which might be used to describe deformations of charged $\\mathrm{AdS}_3$ black holes or quantum Hall systems.","sentences":["We study interacting theories of $N$ left-moving and $\\overline{N}$ right-moving Floreanini-Jackiw bosons in two dimensions.","A parameterized family of such theories is shown to enjoy (non-manifest)","Lorentz invariance if and only if its Lagrangian obeys a flow equation driven by a function of the energy-momentum tensor.","We discuss the canonical quantization of such theories along classical stress tensor flows, focusing on the case of the root-$T \\overline{T}$ deformation, where we obtain perturbative results for the deformed spectrum in a certain large-momentum limit.","In the special case $N = \\overline{N}$, we consider the quantum effective action for the root-$T \\overline{T}$-deformed theory by expanding around a general classical background, and we find that the one-loop contribution vanishes for backgrounds with constant scalar gradients.","Our analysis can also be interpreted via dual $U(1)$ Chern-Simons theories in three dimensions, which might be used to describe deformations of charged $\\mathrm{AdS}_3$ black holes or quantum Hall systems."],"url":"http://arxiv.org/abs/2403.18242v1","category":"hep-th"}
{"created":"2024-03-27 04:07:21","title":"A deep neural network for positioning and inter-crystal scatter identification in multiplexed PET detectors","abstract":"Objective: Conventional event positioning algorithms in light-sharing PET detectors are often limited by edge effects and the impact of inter-crystal scattering (ICS). This study explores the feasibility of deep neural network (DNN) techniques for more precise event positioning in finely segmented and highly multiplexed PET detectors with light-sharing. Approach: A DNN was designed for crystal localisation, and trained/tested with light distributions of photoelectric (P) and Compton/photoelectric (CP) events simulated using optical GATE and an efficient analytical method. Using the statistical properties of ICS events from simulation, an energy-guided positioning algorithm was built into the DNN, enabling selection of the unique or first crystal of interaction in P and CP events, respectively. Performance of the DNN was compared with Anger logic using light distributions from simulated 511-keV point sources near the PET detector. Results: Despite coarse photodetector data due to signal multiplexing, the DNN demonstrated a crystal classification accuracy of 90% for P events and 82% for CP events. For crystal positioning, the DNN outperformed Anger logic by at least 34% and 14% for P and CP events, respectively. Further improvement is somewhat constrained by the physics, specifically, the ratio of backward to forward scattering of gamma rays within the crystal array being close to 1. This prevents selecting the first crystal of interaction in CP events with a high degree of certainty. Significance: Light-sharing and multiplexed PET detectors are common in high-resolution PET, yet event positioning can be poor due to edge effects and ICS events. Our study shows that DNN-based event positioning can enhance 2D coincidence event positioning accuracy by nearly a factor of 2 compared to Anger logic. However, further improvements are difficult to foresee without timing information.","sentences":["Objective: Conventional event positioning algorithms in light-sharing PET detectors are often limited by edge effects and the impact of inter-crystal scattering (ICS).","This study explores the feasibility of deep neural network (DNN) techniques for more precise event positioning in finely segmented and highly multiplexed PET detectors with light-sharing.","Approach:","A DNN was designed for crystal localisation, and trained/tested with light distributions of photoelectric (P) and Compton/photoelectric (CP) events simulated using optical GATE and an efficient analytical method.","Using the statistical properties of ICS events from simulation, an energy-guided positioning algorithm was built into the DNN, enabling selection of the unique or first crystal of interaction in P and CP events, respectively.","Performance of the DNN was compared with Anger logic using light distributions from simulated 511-keV point sources near the PET detector.","Results: Despite coarse photodetector data due to signal multiplexing, the DNN demonstrated a crystal classification accuracy of 90% for P events and 82% for CP events.","For crystal positioning, the DNN outperformed Anger logic by at least 34% and 14% for P and CP events, respectively.","Further improvement is somewhat constrained by the physics, specifically, the ratio of backward to forward scattering of gamma rays within the crystal array being close to 1.","This prevents selecting the first crystal of interaction in CP events with a high degree of certainty.","Significance: Light-sharing and multiplexed PET detectors are common in high-resolution PET, yet event positioning can be poor due to edge effects and ICS events.","Our study shows that DNN-based event positioning can enhance 2D coincidence event positioning accuracy by nearly a factor of 2 compared to Anger logic.","However, further improvements are difficult to foresee without timing information."],"url":"http://arxiv.org/abs/2403.18240v1","category":"physics.med-ph"}
{"created":"2024-03-27 03:56:14","title":"Analytical computation of bifurcation of orbits near collinear libration point in the restricted three-body problem","abstract":"A unified analytical solution is presented for constructing the phase space near collinear libration points in the Circular Restricted Three-body Problem (CRTBP), encompassing Lissajous orbits and quasihalo orbits, their invariant manifolds, as well as transit and non-transit orbits. Traditional methods could only derive separate analytical solutions for the invariant manifolds of Lissajous orbits and halo orbits, falling short for the invariant manifolds of quasihalo orbits. By introducing a coupling coefficient {\\eta} and a bifurcation equation, a unified series solution for these orbits is systematically developed using a coupling-induced bifurcation mechanism and Lindstedt-Poincar\\'e method. Analyzing the third-order bifurcation equation reveals bifurcation conditions for halo orbits, quasihalo orbits, and their invariant manifolds. Furthermore, new families of periodic orbits similar to halo orbits are discovered, and non-periodic/quasi-periodic orbits, such as transit orbits and non-transit orbits, are found to undergo bifurcations. When {\\eta} = 0, the series solution describes Lissajous orbits and their invariant manifolds, transit, and non-transit orbits. As {\\eta} varies from zero to non-zero values, the solution seamlessly transitions to describe quasihalo orbits and their invariant manifolds, as well as newly bifurcated transit and non-transit orbits. This unified analytical framework provides a more comprehensive understanding of the complex phase space structures near collinear libration points in the CRTBP.","sentences":["A unified analytical solution is presented for constructing the phase space near collinear libration points in the Circular Restricted Three-body Problem (CRTBP), encompassing Lissajous orbits and quasihalo orbits, their invariant manifolds, as well as transit and non-transit orbits.","Traditional methods could only derive separate analytical solutions for the invariant manifolds of Lissajous orbits and halo orbits, falling short for the invariant manifolds of quasihalo orbits.","By introducing a coupling coefficient {\\eta} and a bifurcation equation, a unified series solution for these orbits is systematically developed using a coupling-induced bifurcation mechanism and Lindstedt-Poincar\\'e method.","Analyzing the third-order bifurcation equation reveals bifurcation conditions for halo orbits, quasihalo orbits, and their invariant manifolds.","Furthermore, new families of periodic orbits similar to halo orbits are discovered, and non-periodic/quasi-periodic orbits, such as transit orbits and non-transit orbits, are found to undergo bifurcations.","When {\\eta} = 0, the series solution describes Lissajous orbits and their invariant manifolds, transit, and non-transit orbits.","As {\\eta} varies from zero to non-zero values, the solution seamlessly transitions to describe quasihalo orbits and their invariant manifolds, as well as newly bifurcated transit and non-transit orbits.","This unified analytical framework provides a more comprehensive understanding of the complex phase space structures near collinear libration points in the CRTBP."],"url":"http://arxiv.org/abs/2403.18237v1","category":"math-ph"}
{"created":"2024-03-27 03:53:30","title":"Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters","abstract":"The Reinforcement Learning (RL) algorithm, renowned for its robust learning capability and search stability, has garnered significant attention and found extensive application in Automated Guided Vehicle (AGV) path planning. However, RL planning algorithms encounter challenges stemming from the substantial variance of neural networks caused by environmental instability and significant fluctuations in system structure. These challenges manifest in slow convergence speed and low learning efficiency. To tackle this issue, this paper presents the Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which incorporates the Particle Filter (PF) into multi-AGV reinforcement learning path planning. The PF-DDQN method leverages the imprecise weight values of the network as state values to formulate the state space equation. Through the iterative fusion process of neural networks and particle filters, the DDQN model is optimized to acquire the optimal true weight values, thus enhancing the algorithm's efficiency. The proposed method's effectiveness and superiority are validated through numerical simulations. Overall, the simulation results demonstrate that the proposed algorithm surpasses the traditional DDQN algorithm in terms of path planning superiority and training time indicators by 92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses the challenges encountered by RL planning algorithms in AGV path planning. By integrating the Particle Filter and optimizing the DDQN model, the proposed method achieves enhanced efficiency and outperforms the traditional DDQN algorithm in terms of path planning superiority and training time indicators.","sentences":["The Reinforcement Learning (RL) algorithm, renowned for its robust learning capability and search stability, has garnered significant attention and found extensive application in Automated Guided Vehicle (AGV) path planning.","However, RL planning algorithms encounter challenges stemming from the substantial variance of neural networks caused by environmental instability and significant fluctuations in system structure.","These challenges manifest in slow convergence speed and low learning efficiency.","To tackle this issue, this paper presents the Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which incorporates the Particle Filter (PF) into multi-AGV reinforcement learning path planning.","The PF-DDQN method leverages the imprecise weight values of the network as state values to formulate the state space equation.","Through the iterative fusion process of neural networks and particle filters, the DDQN model is optimized to acquire the optimal true weight values, thus enhancing the algorithm's efficiency.","The proposed method's effectiveness and superiority are validated through numerical simulations.","Overall, the simulation results demonstrate that the proposed algorithm surpasses the traditional DDQN algorithm in terms of path planning superiority and training time indicators by 92.62% and 76.88%, respectively.","In conclusion, the PF-DDQN method addresses the challenges encountered by RL planning algorithms in AGV path planning.","By integrating the Particle Filter and optimizing the DDQN model, the proposed method achieves enhanced efficiency and outperforms the traditional DDQN algorithm in terms of path planning superiority and training time indicators."],"url":"http://arxiv.org/abs/2403.18236v1","category":"cs.RO"}
{"created":"2024-03-27 03:32:15","title":"A Comprehensive Overview of the Lebesgue Differentiation Theorem in Coq","abstract":"Formalization of real analysis offers a chance to rebuild traditional proofs of important theorems as unambiguous theories that can be interactively explored. This paper provides a comprehensive overview of the Lebesgue Differentiation Theorem formalized in the Coq proof assistant, from which the first Fundamental Theorem of Calculus (FTC) for the Lebesgue integral is obtained as a corollary. Proving the first FTC in this way has the advantage of decomposing into loosely-coupled theories of moderate size and of independent interest that lend themselves well to incremental and collaborative development. We explain how we formalize all the topological constructs and all the standard lemmas needed to eventually relate the definitions of derivability and of Lebesgue integration of MathComp-Analysis, a formalization of analysis developed on top of the Mathematical Components library. In the course of this experiment, we substantially enrich MathComp-Analysis and even devise a new proof for Urysohn's lemma.","sentences":["Formalization of real analysis offers a chance to rebuild traditional proofs of important theorems as unambiguous theories that can be interactively explored.","This paper provides a comprehensive overview of the Lebesgue Differentiation Theorem formalized in the Coq proof assistant, from which the first Fundamental Theorem of Calculus (FTC) for the Lebesgue integral is obtained as a corollary.","Proving the first FTC in this way has the advantage of decomposing into loosely-coupled theories of moderate size and of independent interest that lend themselves well to incremental and collaborative development.","We explain how we formalize all the topological constructs and all the standard lemmas needed to eventually relate the definitions of derivability and of Lebesgue integration of MathComp-Analysis, a formalization of analysis developed on top of the Mathematical Components library.","In the course of this experiment, we substantially enrich MathComp-Analysis and even devise a new proof for Urysohn's lemma."],"url":"http://arxiv.org/abs/2403.18229v1","category":"cs.LO"}
{"created":"2024-03-27 03:31:16","title":"Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification","abstract":"Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$ for training and $19\\%$-$70\\%$ for inference), reduced theoretical energy consumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$), compared to the standard spikformer. Our result indicates the continuous refinement of new Transformers, that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising.","sentences":["Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost.","However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners.","In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation.","Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets.","The FWformer can achieve comparable or even higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$ for training and $19\\%$-$70\\%$ for inference), reduced theoretical energy consumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$), compared to the standard spikformer.","Our result indicates the continuous refinement of new Transformers, that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising."],"url":"http://arxiv.org/abs/2403.18228v1","category":"cs.CV"}
{"created":"2024-03-27 02:16:04","title":"Generative Medical Segmentation","abstract":"Rapid advancements in medical image segmentation performance have been significantly driven by the development of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, these models introduce high computational demands and often have limited ability to generalize across diverse medical imaging datasets. In this manuscript, we introduce Generative Medical Segmentation (GMS), a novel approach leveraging a generative model for image segmentation. Concretely, GMS employs a robust pre-trained Variational Autoencoder (VAE) to derive latent representations of both images and masks, followed by a mapping model that learns the transition from image to mask in the latent space. This process culminates in generating a precise segmentation mask within the image space using the pre-trained VAE decoder. The design of GMS leads to fewer learnable parameters in the model, resulting in a reduced computational burden and enhanced generalization capability. Our extensive experimental analysis across five public datasets in different medical imaging domains demonstrates GMS outperforms existing discriminative segmentation models and has remarkable domain generalization. Our experiments suggest GMS could set a new benchmark for medical image segmentation, offering a scalable and effective solution. GMS implementation and model weights are available at https://github.com/King-HAW/GMS.","sentences":["Rapid advancements in medical image segmentation performance have been significantly driven by the development of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).","However, these models introduce high computational demands and often have limited ability to generalize across diverse medical imaging datasets.","In this manuscript, we introduce Generative Medical Segmentation (GMS), a novel approach leveraging a generative model for image segmentation.","Concretely, GMS employs a robust pre-trained Variational Autoencoder (VAE) to derive latent representations of both images and masks, followed by a mapping model that learns the transition from image to mask in the latent space.","This process culminates in generating a precise segmentation mask within the image space using the pre-trained VAE decoder.","The design of GMS leads to fewer learnable parameters in the model, resulting in a reduced computational burden and enhanced generalization capability.","Our extensive experimental analysis across five public datasets in different medical imaging domains demonstrates GMS outperforms existing discriminative segmentation models and has remarkable domain generalization.","Our experiments suggest GMS could set a new benchmark for medical image segmentation, offering a scalable and effective solution.","GMS implementation and model weights are available at https://github.com/King-HAW/GMS."],"url":"http://arxiv.org/abs/2403.18198v1","category":"eess.IV"}
{"created":"2024-03-27 01:50:37","title":"Green Functions in Small Characteristic","abstract":"The values of the ordinary Green functions are known for almost all groups of Lie type, a long term achievement by various authors.   In this note we solve the last open cases, which are for exceptional groups of type $E_8(q)$ where $q$ is a power of $2$, $3$ or $5$.","sentences":["The values of the ordinary Green functions are known for almost all groups of Lie type, a long term achievement by various authors.   ","In this note we solve the last open cases, which are for exceptional groups of type $E_8(q)$ where $q$ is a power of $2$, $3$ or $5$."],"url":"http://arxiv.org/abs/2403.18190v1","category":"math.RT"}
{"created":"2024-03-27 01:49:33","title":"Interfacial magnetic spin Hall effect in van der Waals Fe3GeTe2/MoTe2 heterostructure","abstract":"The spin Hall effect (SHE) allows efficient generation of spin polarization or spin current through charge current and plays a crucial role in the development of spintronics. While SHE typically occurs in non-magnetic materials and is time-reversal even, exploring time-reversal-odd (T-odd) SHE, which couples SHE to magnetization in ferromagnetic materials, offers a new charge-spin conversion mechanism with new functionalities. Here, we report the observation of giant T-odd SHE in Fe3GeTe2/MoTe2 van der Waals heterostructure, representing a previously unidentified interfacial magnetic spin Hall effect (interfacial-MSHE). Through rigorous symmetry analysis and theoretical calculations, we attribute the interfacial-MSHE to a symmetry-breaking induced spin current dipole at the vdW interface. Furthermore, we show that this linear effect can be used for implementing multiply-accumulate operations and binary convolutional neural networks with cascaded multi-terminal devices. Our findings uncover an interfacial T-odd charge-spin conversion mechanism with promising potential for energy-efficient in-memory computing.","sentences":["The spin Hall effect (SHE) allows efficient generation of spin polarization or spin current through charge current and plays a crucial role in the development of spintronics.","While SHE typically occurs in non-magnetic materials and is time-reversal even, exploring time-reversal-odd (T-odd) SHE, which couples SHE to magnetization in ferromagnetic materials, offers a new charge-spin conversion mechanism with new functionalities.","Here, we report the observation of giant T-odd SHE in Fe3GeTe2/MoTe2 van der Waals heterostructure, representing a previously unidentified interfacial magnetic spin Hall effect (interfacial-MSHE).","Through rigorous symmetry analysis and theoretical calculations, we attribute the interfacial-MSHE to a symmetry-breaking induced spin current dipole at the vdW interface.","Furthermore, we show that this linear effect can be used for implementing multiply-accumulate operations and binary convolutional neural networks with cascaded multi-terminal devices.","Our findings uncover an interfacial T-odd charge-spin conversion mechanism with promising potential for energy-efficient in-memory computing."],"url":"http://arxiv.org/abs/2403.18189v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 01:12:45","title":"Tagged particles and size-biased dynamics in mean-field interacting particle systems","abstract":"We establish a connection between tagged particles and size-biased empirical processes in interacting particle systems, in analogy to classical results on the propagation of chaos. In a mean-field scaling limit, the evolution of the occupation number on the tagged particle site converges to a time-inhomogeneous Markov process with non-linear master equation given by the law of large numbers of size-biased empirical measures. The latter are important in recent efforts to understand the dynamics of condensation in interacting particle systems.","sentences":["We establish a connection between tagged particles and size-biased empirical processes in interacting particle systems, in analogy to classical results on the propagation of chaos.","In a mean-field scaling limit, the evolution of the occupation number on the tagged particle site converges to a time-inhomogeneous Markov process with non-linear master equation given by the law of large numbers of size-biased empirical measures.","The latter are important in recent efforts to understand the dynamics of condensation in interacting particle systems."],"url":"http://arxiv.org/abs/2403.18179v1","category":"math.PR"}
{"created":"2024-03-27 00:58:31","title":"Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models","abstract":"In minimally invasive telesurgery, obtaining accurate force information is difficult due to the complexities of in-vivo end effector force sensing. This constrains development and implementation of haptic feedback and force-based automated performance metrics, respectively. Vision-based force sensing approaches using deep learning are a promising alternative to intrinsic end effector force sensing. However, they have limited ability to generalize to novel scenarios, and require learning on high-quality force sensor training data that can be difficult to obtain. To address these challenges, this paper presents a novel vision-based contact-conditional approach for force estimation in telesurgical environments. Our method leverages supervised learning with human labels and end effector position data to train deep neural networks. Predictions from these trained models are optionally combined with robot joint torque information to estimate forces indirectly from visual data. We benchmark our method against ground truth force sensor data and demonstrate generality by fine-tuning to novel surgical scenarios in a data-efficient manner. Our methods demonstrated greater than 90% accuracy on contact detection and less than 10% force prediction error. These results suggest potential usefulness of contact-conditional force estimation for sensory substitution haptic feedback and tissue handling skill evaluation in clinical settings.","sentences":["In minimally invasive telesurgery, obtaining accurate force information is difficult due to the complexities of in-vivo end effector force sensing.","This constrains development and implementation of haptic feedback and force-based automated performance metrics, respectively.","Vision-based force sensing approaches using deep learning are a promising alternative to intrinsic end effector force sensing.","However, they have limited ability to generalize to novel scenarios, and require learning on high-quality force sensor training data that can be difficult to obtain.","To address these challenges, this paper presents a novel vision-based contact-conditional approach for force estimation in telesurgical environments.","Our method leverages supervised learning with human labels and end effector position data to train deep neural networks.","Predictions from these trained models are optionally combined with robot joint torque information to estimate forces indirectly from visual data.","We benchmark our method against ground truth force sensor data and demonstrate generality by fine-tuning to novel surgical scenarios in a data-efficient manner.","Our methods demonstrated greater than 90% accuracy on contact detection and less than 10% force prediction error.","These results suggest potential usefulness of contact-conditional force estimation for sensory substitution haptic feedback and tissue handling skill evaluation in clinical settings."],"url":"http://arxiv.org/abs/2403.18172v1","category":"cs.RO"}
{"created":"2024-03-27 00:53:06","title":"Formal deformations, cohomology theory and $L_\\infty[1]$-structures for differential Lie algebras of arbitrary weight","abstract":"Generalising a previous work of Jiang and Sheng, a cohomology theory for differential Lie algebras of arbitrary weight is introduced. The underlying $L_\\infty[1]$-structure on the cochain complex is also determined via a generalised version of higher derived brackets. The equivalence between $L_\\infty[1]$-structures for absolute and relative differential Lie algebras are established. Formal deformations and abelian extensions are interpreted by using lower degree cohomology groups. Also we introduce the homotopy differential Lie algebras. In a forthcoming paper, we will show that the operad of homotopy (relative) differential Lie algebras is the minimal model of the operad of (relative) differential Lie algebras.","sentences":["Generalising a previous work of Jiang and Sheng, a cohomology theory for differential Lie algebras of arbitrary weight is introduced.","The underlying $L_\\infty[1]$-structure on the cochain complex is also determined via a generalised version of higher derived brackets.","The equivalence between $L_\\infty[1]$-structures for absolute and relative differential Lie algebras are established.","Formal deformations and abelian extensions are interpreted by using lower degree cohomology groups.","Also we introduce the homotopy differential Lie algebras.","In a forthcoming paper, we will show that the operad of homotopy (relative) differential Lie algebras is the minimal model of the operad of (relative) differential Lie algebras."],"url":"http://arxiv.org/abs/2403.18170v1","category":"math.RA"}
{"created":"2024-03-27 00:17:59","title":"A Generic Nonlinear Evolution Equation of Magnetic Type II. Particular Solutions","abstract":"We consider a matrix nonlinear partial differential equation that generalizes Heisenberg ferromagnet equation. This generalized Heisenberg ferromagnet equation is completely integrable with a linear bundle Lax pair related to the pseudo-unitary algebra. This allows us to explicitly derive particular solutions by using dressing technique. We shall discuss two classes of solutions over constant background: soliton-like solutions and quasi-rational solutions. Both classes have their analogues in the case of the Heisenberg ferromagnet equation related to the same Lie algebra.","sentences":["We consider a matrix nonlinear partial differential equation that generalizes Heisenberg ferromagnet equation.","This generalized Heisenberg ferromagnet equation is completely integrable with a linear bundle Lax pair related to the pseudo-unitary algebra.","This allows us to explicitly derive particular solutions by using dressing technique.","We shall discuss two classes of solutions over constant background: soliton-like solutions and quasi-rational solutions.","Both classes have their analogues in the case of the Heisenberg ferromagnet equation related to the same Lie algebra."],"url":"http://arxiv.org/abs/2403.18165v1","category":"nlin.SI"}
{"created":"2024-03-27 00:05:48","title":"Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys","abstract":"Microsoft Active Directory (AD) is the default security management system for Window domain network. We study the problem of placing decoys in AD network to detect potential attacks. We model the problem as a Stackelberg game between an attacker and a defender on AD attack graphs where the defender employs a set of decoys to detect the attacker on their way to Domain Admin (DA). Contrary to previous works, we consider time-varying (temporal) attack graphs. We proposed a novel metric called response time, to measure the effectiveness of our decoy placement in temporal attack graphs. Response time is defined as the duration from the moment attackers trigger the first decoy to when they compromise the DA. Our goal is to maximize the defender's response time to the worst-case attack paths. We establish the NP-hard nature of the defender's optimization problem, leading us to develop Evolutionary Diversity Optimization (EDO) algorithms. EDO algorithms identify diverse sets of high-quality solutions for the optimization problem. Despite the polynomial nature of the fitness function, it proves experimentally slow for larger graphs. To enhance scalability, we proposed an algorithm that exploits the static nature of AD infrastructure in the temporal setting. Then, we introduce tailored repair operations, ensuring the convergence to better results while maintaining scalability for larger graphs.","sentences":["Microsoft Active Directory (AD) is the default security management system for Window domain network.","We study the problem of placing decoys in AD network to detect potential attacks.","We model the problem as a Stackelberg game between an attacker and a defender on AD attack graphs where the defender employs a set of decoys to detect the attacker on their way to Domain Admin (DA).","Contrary to previous works, we consider time-varying (temporal) attack graphs.","We proposed a novel metric called response time, to measure the effectiveness of our decoy placement in temporal attack graphs.","Response time is defined as the duration from the moment attackers trigger the first decoy to when they compromise the DA.","Our goal is to maximize the defender's response time to the worst-case attack paths.","We establish the NP-hard nature of the defender's optimization problem, leading us to develop Evolutionary Diversity Optimization (EDO) algorithms.","EDO algorithms identify diverse sets of high-quality solutions for the optimization problem.","Despite the polynomial nature of the fitness function, it proves experimentally slow for larger graphs.","To enhance scalability, we proposed an algorithm that exploits the static nature of AD infrastructure in the temporal setting.","Then, we introduce tailored repair operations, ensuring the convergence to better results while maintaining scalability for larger graphs."],"url":"http://arxiv.org/abs/2403.18162v1","category":"cs.CR"}
{"created":"2024-03-26 23:40:13","title":"An inexact infeasible arc-search interior-point method for linear programming problems","abstract":"Inexact interior-point methods (IPMs) are a type of interior-point methods that inexactly solve the linear equation system for obtaining the search direction. On the other hand,arc-search IPMs approximate the central path with an ellipsoidal arc obtained by solving two linear equation systems in each iteration, while conventional line-search IPMs solve one linear system, therefore, the improvement due to the inexact solutions of the linear equation systems can be more beneficial in arc-search IPMs than conventional IPMs. In this paper, we propose an inexact infeasible arc-search interior-point method.We establish that the proposed method is a polynomial-time algorithm through its convergence analysis. The numerical experiments with the conjugate gradient method show that the proposed method can reduce the number of iterations compared to an existing method for benchmark problems; the numbers of iterations are reduced to two-thirds for more than 70% of the problems.","sentences":["Inexact interior-point methods (IPMs) are a type of interior-point methods that inexactly solve the linear equation system for obtaining the search direction.","On the other hand,arc-search IPMs approximate the central path with an ellipsoidal arc obtained by solving two linear equation systems in each iteration, while conventional line-search IPMs solve one linear system, therefore, the improvement due to the inexact solutions of the linear equation systems can be more beneficial in arc-search IPMs than conventional IPMs.","In this paper, we propose an inexact infeasible arc-search interior-point method.","We establish that the proposed method is a polynomial-time algorithm through its convergence analysis.","The numerical experiments with the conjugate gradient method show that the proposed method can reduce the number of iterations compared to an existing method for benchmark problems; the numbers of iterations are reduced to two-thirds for more than 70% of the problems."],"url":"http://arxiv.org/abs/2403.18155v1","category":"math.OC"}
{"created":"2024-03-26 22:19:39","title":"Generalized Maximum Entropy Differential Dynamic Programming","abstract":"We present a sampling-based trajectory optimization method derived from the maximum entropy formulation of Differential Dynamic Programming with Tsallis entropy. This method can be seen as a generalization of the legacy work with Shannon entropy, which leads to a Gaussian optimal control policy for exploration during optimization. With the Tsallis entropy, the optimal control policy takes the form of $q$-Gaussian, which further encourages exploration with its heavy-tailed shape. Moreover, in our formulation, the exploration variance, which was scaled by a fixed constant inverse temperature in the original formulation with Shannon entropy, is automatically scaled based on the value function of the trajectory. Due to this property, our algorithms can promote exploration when necessary, that is, the cost of the trajectory is high, rather than using the same scaling factor. The simulation results demonstrate the properties of the proposed algorithm described above.","sentences":["We present a sampling-based trajectory optimization method derived from the maximum entropy formulation of Differential Dynamic Programming with Tsallis entropy.","This method can be seen as a generalization of the legacy work with Shannon entropy, which leads to a Gaussian optimal control policy for exploration during optimization.","With the Tsallis entropy, the optimal control policy takes the form of $q$-Gaussian, which further encourages exploration with its heavy-tailed shape.","Moreover, in our formulation, the exploration variance, which was scaled by a fixed constant inverse temperature in the original formulation with Shannon entropy, is automatically scaled based on the value function of the trajectory.","Due to this property, our algorithms can promote exploration when necessary, that is, the cost of the trajectory is high, rather than using the same scaling factor.","The simulation results demonstrate the properties of the proposed algorithm described above."],"url":"http://arxiv.org/abs/2403.18130v1","category":"math.OC"}
{"created":"2024-03-26 20:41:35","title":"Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot Interaction","abstract":"Although mobile robots have on-board sensors to perform navigation, their efficiency in completing paths can be enhanced by planning to avoid human interaction. Infrastructure cameras can capture human activity continuously for the purpose of compiling activity analytics to choose efficient times and routes. We describe a cascade temporal filtering method to efficiently extract short- and long-term activity in two time dimensions, isochronal and chronological, for use in global path planning and local navigation respectively. The temporal filter has application either independently, or, if object recognition is also required, it can be used as a pre-filter to perform activity-gating of the more computationally expensive neural network processing. For a testbed 32-camera network, we show how this hybrid approach can achieve over 8 times improvement in frames per second throughput and 6.5 times reduction of system power use. We also show how the cost map of static objects in the ROS robot software development framework is augmented with dynamic regions determined from the temporal filter.","sentences":["Although mobile robots have on-board sensors to perform navigation, their efficiency in completing paths can be enhanced by planning to avoid human interaction.","Infrastructure cameras can capture human activity continuously for the purpose of compiling activity analytics to choose efficient times and routes.","We describe a cascade temporal filtering method to efficiently extract short- and long-term activity in two time dimensions, isochronal and chronological, for use in global path planning and local navigation respectively.","The temporal filter has application either independently, or, if object recognition is also required, it can be used as a pre-filter to perform activity-gating of the more computationally expensive neural network processing.","For a testbed 32-camera network, we show how this hybrid approach can achieve over 8 times improvement in frames per second throughput and 6.5 times reduction of system power use.","We also show how the cost map of static objects in the ROS robot software development framework is augmented with dynamic regions determined from the temporal filter."],"url":"http://arxiv.org/abs/2403.18096v1","category":"cs.RO"}
{"created":"2024-03-26 20:20:29","title":"Atomistic Descriptor Optimization Using Complementary Euclidean and Geodesic Distance Information","abstract":"Descriptors are physically-inspired schemes for representing atomistic systems that play a central role in the construction of models of potential energy surfaces. Although physical intuition can be flexibly encoded into descriptor schemes, they are generally ultimately guided only by the spatial or topological arrangement of atoms in the system. Here, we propose a novel approach for the optimization of descriptors based on encoding information about geodesic distances along potential energy manifolds into the hyperparameters of commonly used descriptor schemes. To accomplish this, we combine two ideas: (1) a differential-geometric approach for the fast estimation of approximate geodesic distances; and (2) an information-theoretic evaluation metric - information imbalance - for measuring the shared information between two distance measures. Using the MD22 datasets of ethanol, malonaldehyde, and aspirin, we first show that Euclidean (in Cartesian coordinates) and geodesic distances are inequivalent distance measures, indicating the need for updated ground-truth distance measures that go beyond the Euclidean distance. We then utilize a Bayesian optimization framework to show that descriptors (in this case, atom-centered symmetry functions) can be optimized to maximally express a certain type of distance information, such as Euclidean or geodesic information. We also show that modifying the Bayesian optimization algorithm to minimize a combined Euclidean+geodesic objective function can yield descriptors that not only express both Euclidean and geodesic distance information simultaneously, but in fact resolve substantial disagreements between descriptors optimized to encode only one type of distance measure. We discuss the relevance of our approach to the design of more physically rich and informative descriptors that can encode useful, alternative information about molecular systems.","sentences":["Descriptors are physically-inspired schemes for representing atomistic systems that play a central role in the construction of models of potential energy surfaces.","Although physical intuition can be flexibly encoded into descriptor schemes, they are generally ultimately guided only by the spatial or topological arrangement of atoms in the system.","Here, we propose a novel approach for the optimization of descriptors based on encoding information about geodesic distances along potential energy manifolds into the hyperparameters of commonly used descriptor schemes.","To accomplish this, we combine two ideas: (1) a differential-geometric approach for the fast estimation of approximate geodesic distances; and (2) an information-theoretic evaluation metric - information imbalance - for measuring the shared information between two distance measures.","Using the MD22 datasets of ethanol, malonaldehyde, and aspirin, we first show that Euclidean (in Cartesian coordinates) and geodesic distances are inequivalent distance measures, indicating the need for updated ground-truth distance measures that go beyond the Euclidean distance.","We then utilize a Bayesian optimization framework to show that descriptors (in this case, atom-centered symmetry functions) can be optimized to maximally express a certain type of distance information, such as Euclidean or geodesic information.","We also show that modifying the Bayesian optimization algorithm to minimize a combined Euclidean+geodesic objective function can yield descriptors that not only express both Euclidean and geodesic distance information simultaneously, but in fact resolve substantial disagreements between descriptors optimized to encode only one type of distance measure.","We discuss the relevance of our approach to the design of more physically rich and informative descriptors that can encode useful, alternative information about molecular systems."],"url":"http://arxiv.org/abs/2403.18090v1","category":"physics.chem-ph"}
{"created":"2024-03-26 20:19:51","title":"Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation","abstract":"We propose a new neural network based large eddy simulation framework for the incompressible Navier-Stokes equations based on the paradigm \"discretize first, filter and close next\". This leads to full model-data consistency and allows for employing neural closure models in the same environment as where they have been trained. Since the LES discretization error is included in the learning process, the closure models can learn to account for the discretization.   Furthermore, we introduce a new divergence-consistent discrete filter defined through face-averaging. The new filter preserves the discrete divergence-free constraint by construction, unlike general discrete filters such as volume-averaging filters. We show that using a divergence-consistent LES formulation coupled with a convolutional neural closure model produces stable and accurate results for both a-priori and a-posteriori training, while a general (divergence-inconsistent) LES model requires a-posteriori training or other stability-enforcing measures.","sentences":["We propose a new neural network based large eddy simulation framework for the incompressible Navier-Stokes equations based on the paradigm \"discretize first, filter and close next\".","This leads to full model-data consistency and allows for employing neural closure models in the same environment as where they have been trained.","Since the LES discretization error is included in the learning process, the closure models can learn to account for the discretization.   ","Furthermore, we introduce a new divergence-consistent discrete filter defined through face-averaging.","The new filter preserves the discrete divergence-free constraint by construction, unlike general discrete filters such as volume-averaging filters.","We show that using a divergence-consistent LES formulation coupled with a convolutional neural closure model produces stable and accurate results for both a-priori and a-posteriori training, while a general (divergence-inconsistent) LES model requires a-posteriori training or other stability-enforcing measures."],"url":"http://arxiv.org/abs/2403.18088v1","category":"math.NA"}
{"created":"2024-03-26 19:58:12","title":"The equivalence of smooth and synthetic notions of timelike sectional curvature bounds","abstract":"Timelike sectional curvature bounds play an important role in spacetime geometry, both for the understanding of classical smooth spacetimes and for the study of Lorentzian (pre-)length spaces introduced in \\cite{kunzinger2018lorentzian}. In the smooth setting, a bound on the sectional curvature of timelike planes can be formulated via the Riemann curvature tensor. In the synthetic setting, bounds are formulated by comparing various geometric configurations to the corresponding ones in constant curvature spaces. The first link between these notions in the Lorentzian context was established in \\cite{harris1982triangle}, which was instrumental in the proof of powerful results in spacetime geometry \\cite{beem1985toponogov, beem1985decomposition, galloway2018existence}. For general semi-Riemannian manifolds, the equivalence between sectional curvature bounds and synthetic bounds was established in \\cite{alexander2008triangle}, however in this approach the sectional curvatures of both timelike and spacelike planes have to be considered. In this article, we fill a gap in the literature by proving the full equivalence between sectional curvature bounds on timelike planes and synthetic timelike bounds on strongly causal spacetimes. As an essential tool, we establish Hessian comparison for the time separation and signed distance functions.","sentences":["Timelike sectional curvature bounds play an important role in spacetime geometry, both for the understanding of classical smooth spacetimes and for the study of Lorentzian (pre-)length spaces introduced in \\cite{kunzinger2018lorentzian}.","In the smooth setting, a bound on the sectional curvature of timelike planes can be formulated via the Riemann curvature tensor.","In the synthetic setting, bounds are formulated by comparing various geometric configurations to the corresponding ones in constant curvature spaces.","The first link between these notions in the Lorentzian context was established in \\cite{harris1982triangle}, which was instrumental in the proof of powerful results in spacetime geometry \\cite{beem1985toponogov, beem1985decomposition, galloway2018existence}.","For general semi-Riemannian manifolds, the equivalence between sectional curvature bounds and synthetic bounds was established in \\cite{alexander2008triangle}, however in this approach the sectional curvatures of both timelike and spacelike planes have to be considered.","In this article, we fill a gap in the literature by proving the full equivalence between sectional curvature bounds on timelike planes and synthetic timelike bounds on strongly causal spacetimes.","As an essential tool, we establish Hessian comparison for the time separation and signed distance functions."],"url":"http://arxiv.org/abs/2403.18077v1","category":"math.DG"}
{"created":"2024-03-26 19:06:48","title":"Quillen (co)homology of divided power algebras over an operad","abstract":"Barr--Beck cohomology, put into the framework of model categories by Quillen, provides a cohomology theory for any algebraic structure, for example Andr\\'e--Quillen cohomology of commutative rings. Quillen cohomology has been studied notably for divided power algebras and restricted Lie algebras, both of which are instances of divided power algebras over an operad $\\mathcal P$: the commutative and Lie operad respectively. In this paper, we investigate the Quillen cohomology of divided power algebras over an operad $\\mathcal P$, identifying Beck modules, derivations, and K\\\"ahler differentials in that setup. We also compare the cohomology of divided power algebras over $\\mathcal P$ with that of $\\mathcal P$-algebras, and work out some examples.","sentences":["Barr--Beck cohomology, put into the framework of model categories by Quillen, provides a cohomology theory for any algebraic structure, for example Andr\\'e--Quillen cohomology of commutative rings.","Quillen cohomology has been studied notably for divided power algebras and restricted Lie algebras, both of which are instances of divided power algebras over an operad $\\mathcal P$: the commutative and Lie operad respectively.","In this paper, we investigate the Quillen cohomology of divided power algebras over an operad $\\mathcal P$, identifying Beck modules, derivations, and K\\\"ahler differentials in that setup.","We also compare the cohomology of divided power algebras over $\\mathcal P$ with that of $\\mathcal P$-algebras, and work out some examples."],"url":"http://arxiv.org/abs/2403.18049v1","category":"math.RA"}
{"created":"2024-03-26 19:03:11","title":"A new look at AdS black holes with conformal scalar hair","abstract":"We revisit static, spherically symmetric solutions to AdS-Einstein gravity with a conformally coupled scalar field (and no self-interaction potential) in four dimensions. We first observe that a convenient choice of coordinates leads to a significant simplification of the field equations, which enables one to identify various roots of the indicial equations and thus distinct branches of solutions. Next, we construct an explicit 2-parameter hairy black hole solution in terms of an infinite power series around the event horizon. The black hole is non-extremal with a regular scalar field on and outside the event horizon, and it reduces to the Schwarzschild-AdS metric in the limit of vanishing hair. Its properties are illustrated for various values of the parameters and compared with previous numerical results by other authors. In addition, the analysis reveals the presence of a photon sphere and how the scalar field affects its size, in agreement with certain general bounds obtained in the literature. The thermodynamics of the solution is also briefly discussed.","sentences":["We revisit static, spherically symmetric solutions to AdS-Einstein gravity with a conformally coupled scalar field (and no self-interaction potential) in four dimensions.","We first observe that a convenient choice of coordinates leads to a significant simplification of the field equations, which enables one to identify various roots of the indicial equations and thus distinct branches of solutions.","Next, we construct an explicit 2-parameter hairy black hole solution in terms of an infinite power series around the event horizon.","The black hole is non-extremal with a regular scalar field on and outside the event horizon, and it reduces to the Schwarzschild-AdS metric in the limit of vanishing hair.","Its properties are illustrated for various values of the parameters and compared with previous numerical results by other authors.","In addition, the analysis reveals the presence of a photon sphere and how the scalar field affects its size, in agreement with certain general bounds obtained in the literature.","The thermodynamics of the solution is also briefly discussed."],"url":"http://arxiv.org/abs/2403.18048v1","category":"gr-qc"}
{"created":"2024-03-26 18:57:56","title":"Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design","abstract":"Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.","sentences":["Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope.","For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation.","In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort.","We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study."],"url":"http://arxiv.org/abs/2403.18044v1","category":"math.OC"}
{"created":"2024-03-26 18:40:36","title":"Bidirectional Consistency Models","abstract":"Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework. Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error. Furthermore, by leveraging our model's bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content. We further showcase our model's capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending black-box adversarial attacks.","sentences":["Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE).","Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing.","However, the iterative nature of this process restricts its speed, hindering its broader application.","Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate.","Yet, the absence of an explicit ODE solver complicates the inversion process.","To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework.","Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error.","Furthermore, by leveraging our model's bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content.","We further showcase our model's capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending black-box adversarial attacks."],"url":"http://arxiv.org/abs/2403.18035v1","category":"cs.LG"}
{"created":"2024-03-26 18:38:30","title":"A Note on Almost Everywhere Convergence Along Tangential Curves to the Schr\u00f6dinger Equation Initial Datum","abstract":"In this short note, we give an easy proof of the following result: for $ n\\geq 2, $ $\\underset{t\\to0}{\\lim} \\,e^{it\\Delta }f\\left(x+\\gamma(t)\\right) = f(x) $ almost everywhere whenever $ \\gamma $ is an $ \\alpha- $H\\\"older curve with $ \\frac12\\leq \\alpha\\leq 1 $ and $ f\\in H^s(\\mathbb{R}^n) $, with $ s > \\frac{n}{2(n+1)} $. This is the optimal range of regularity up to the endpoint.","sentences":["In this short note, we give an easy proof of the following result: for $ n\\geq 2, $ $\\underset{t\\to0}{\\lim} \\,e^{it\\Delta }f\\left(x+\\gamma(t)\\right) = f(x) $ almost everywhere whenever $ \\gamma $ is an $ \\alpha- $H\\\"older curve with $ \\frac12\\leq \\alpha\\leq 1 $ and $ f\\in H^s(\\mathbb{R}^n) $, with $ s > \\frac{n}{2(n+1)} $.","This is the optimal range of regularity up to the endpoint."],"url":"http://arxiv.org/abs/2403.18032v1","category":"math.CA"}
{"created":"2024-03-26 18:13:20","title":"A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots","abstract":"We report results obtained and insights gained while answering the following question: how effective is it to use a simulator to establish path following control policies for an autonomous ground robot? While the quality of the simulator conditions the answer to this question, we found that for the simulation platform used herein, producing four control policies for path planning was straightforward once a digital twin of the controlled robot was available. The control policies established in simulation and subsequently demonstrated in the real world are PID control, MPC, and two neural network (NN) based controllers. Training the two NN controllers via imitation learning was accomplished expeditiously using seven simple maneuvers: follow three circles clockwise, follow the same circles counter-clockwise, and drive straight. A test randomization process that employs random micro-simulations is used to rank the ``goodness'' of the four control policies. The policy ranking noted in simulation correlates well with the ranking observed when the control policies were tested in the real world. The simulation platform used is publicly available and BSD3-released as open source; a public Docker image is available for reproducibility studies. It contains a dynamics engine, a sensor simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in the simulator and the real world experiments.","sentences":["We report results obtained and insights gained while answering the following question: how effective is it to use a simulator to establish path following control policies for an autonomous ground robot?","While the quality of the simulator conditions the answer to this question, we found that for the simulation platform used herein, producing four control policies for path planning was straightforward once a digital twin of the controlled robot was available.","The control policies established in simulation and subsequently demonstrated in the real world are PID control, MPC, and two neural network (NN) based controllers.","Training the two NN controllers via imitation learning was accomplished expeditiously using seven simple maneuvers: follow three circles clockwise, follow the same circles counter-clockwise, and drive straight.","A test randomization process that employs random micro-simulations is used to rank the ``goodness'' of the four control policies.","The policy ranking noted in simulation correlates well with the ranking observed when the control policies were tested in the real world.","The simulation platform used is publicly available and BSD3-released as open source; a public Docker image is available for reproducibility studies.","It contains a dynamics engine, a sensor simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in the simulator and the real world experiments."],"url":"http://arxiv.org/abs/2403.18021v1","category":"cs.RO"}
{"created":"2024-03-26 18:05:39","title":"Nonsingularity of unsymmetric Kansa matrices: random collocation by MultiQuadrics and Inverse MultiQuadrics","abstract":"Unisolvence of unsymmetric Kansa collocation is still a substantially open problem. We prove that Kansa matrices with MultiQuadrics and Inverse MultiQuadrics for the Dirichlet problem of the Poisson equation are almost surely nonsingular, when the collocation points are chosen by any continuous random distribution in the domain interior and arbitrarily on its boundary.","sentences":["Unisolvence of unsymmetric Kansa collocation is still a substantially open problem.","We prove that Kansa matrices with MultiQuadrics and Inverse MultiQuadrics for the Dirichlet problem of the Poisson equation are almost surely nonsingular, when the collocation points are chosen by any continuous random distribution in the domain interior and arbitrarily on its boundary."],"url":"http://arxiv.org/abs/2403.18017v1","category":"math.NA"}
{"created":"2024-03-26 17:43:19","title":"Quasar Island -- Three new $z\\sim6$ quasars, including a lensed candidate, identified with contrastive learning","abstract":"Of the hundreds of $z\\gtrsim6$ quasars discovered to date, only one is known to be gravitationally lensed, despite the high lensing optical depth expected at $z\\gtrsim6$. High-redshift quasars are typically identified in large-scale surveys by applying strict photometric selection criteria, in particular by imposing non-detections in bands blueward of the Lyman-$\\alpha$ line. Such procedures by design prohibit the discovery of lensed quasars, as the lensing foreground galaxy would contaminate the photometry of the quasar. We present a novel quasar selection methodology, applying contrastive learning (an unsupervised machine learning technique) to Dark Energy Survey imaging data. We describe the use of this technique to train a neural network which isolates an 'island' of 11 sources, of which 7 are known $z\\sim6$ quasars. Of the remaining four, three are newly discovered quasars (J0109-5424, $z=6.07$; J0122-4609, $z=5.99$; J0603-3923, $z=5.94$), as confirmed by follow-up Gemini-South/GMOS and archival NTT/EFOSC2 spectroscopy, implying a 91 per cent efficiency for our novel selection method; the final object on the island is a brown dwarf. In one case (J0109-5424), emission below the Lyman limit unambiguously indicates the presence of a foreground source, though high-resolution optical/near-infrared imaging is still needed to confirm the quasar's lensed (multiply-imaged) nature. Detection in the g band has led this quasar to escape selection by traditional colour cuts. Our findings demonstrate that machine learning techniques can thus play a key role in unveiling populations of quasars missed by traditional methods.","sentences":["Of the hundreds of $z\\gtrsim6$ quasars discovered to date, only one is known to be gravitationally lensed, despite the high lensing optical depth expected at $z\\gtrsim6$. High-redshift quasars are typically identified in large-scale surveys by applying strict photometric selection criteria, in particular by imposing non-detections in bands blueward of the Lyman-$\\alpha$ line.","Such procedures by design prohibit the discovery of lensed quasars, as the lensing foreground galaxy would contaminate the photometry of the quasar.","We present a novel quasar selection methodology, applying contrastive learning (an unsupervised machine learning technique) to Dark Energy Survey imaging data.","We describe the use of this technique to train a neural network which isolates an 'island' of 11 sources, of which 7 are known $z\\sim6$ quasars.","Of the remaining four, three are newly discovered quasars (J0109-5424, $z=6.07$; J0122-4609, $z=5.99$; J0603-3923, $z=5.94$), as confirmed by follow-up Gemini-South/GMOS and archival NTT/EFOSC2 spectroscopy, implying a 91 per cent efficiency for our novel selection method; the final object on the island is a brown dwarf.","In one case (J0109-5424), emission below the Lyman limit unambiguously indicates the presence of a foreground source, though high-resolution optical/near-infrared imaging is still needed to confirm the quasar's lensed (multiply-imaged) nature.","Detection in the g band has led this quasar to escape selection by traditional colour cuts.","Our findings demonstrate that machine learning techniques can thus play a key role in unveiling populations of quasars missed by traditional methods."],"url":"http://arxiv.org/abs/2403.17903v1","category":"astro-ph.GA"}
{"created":"2024-03-27 16:22:35","title":"Light-cone feature selection for quantum machine learning","abstract":"Feature selection plays an essential role in improving the predictive performance and interpretability of trained models in classical machine learning. On the other hand, the usability of conventional feature selection could be limited for quantum machine learning tasks; the technique might not provide a clear interpretation on embedding quantum circuits for classical data tasks and, more importantly, is not applicable to quantum data tasks. In this work, we propose a feature selection method with a specific focus on quantum machine learning. Our scheme treats the light-cones (i.e., subspace) of quantum models as features and then select relevant ones through training of the corresponding local quantum kernels. We numerically demonstrate its versatility for four different applications using toy tasks: (1) feature selection of classical inputs, (2) circuit architecture search for data embedding, (3) compression of quantum machine learning models and (4) subspace selection for quantum data. The proposed framework paves the way towards applications of quantum machine learning to practical tasks. Also, this technique could be used to practically test if the quantum machine learning tasks really need quantumness, while it is beyond the scope of this work.","sentences":["Feature selection plays an essential role in improving the predictive performance and interpretability of trained models in classical machine learning.","On the other hand, the usability of conventional feature selection could be limited for quantum machine learning tasks; the technique might not provide a clear interpretation on embedding quantum circuits for classical data tasks and, more importantly, is not applicable to quantum data tasks.","In this work, we propose a feature selection method with a specific focus on quantum machine learning.","Our scheme treats the light-cones (i.e., subspace) of quantum models as features and then select relevant ones through training of the corresponding local quantum kernels.","We numerically demonstrate its versatility for four different applications using toy tasks: (1) feature selection of classical inputs, (2) circuit architecture search for data embedding, (3) compression of quantum machine learning models and (4) subspace selection for quantum data.","The proposed framework paves the way towards applications of quantum machine learning to practical tasks.","Also, this technique could be used to practically test if the quantum machine learning tasks really need quantumness, while it is beyond the scope of this work."],"url":"http://arxiv.org/abs/2403.18733v1","category":"quant-ph"}
{"created":"2024-03-27 15:42:01","title":"Teaching Introductory HRI: UChicago Course \"Human-Robot Interaction: Research and Practice\"","abstract":"In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago. Since 2020, I have taught and refined this course each academic year. Human-Robot Interaction: Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI. Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications. Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot. In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course.","sentences":["In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago.","Since 2020, I have taught and refined this course each academic year.","Human-Robot Interaction:","Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI.","Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications.","Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot.","In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course."],"url":"http://arxiv.org/abs/2403.18692v1","category":"cs.RO"}
{"created":"2024-03-27 15:22:16","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method","abstract":"Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence).","sentences":["Large Language Models (LLM) are prone to returning false information.","It constitutes one of major challenges in the AI field.","In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI).","In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful).","Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads.","We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI).","NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results.","NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B.","Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence)."],"url":"http://arxiv.org/abs/2403.18680v1","category":"cs.CL"}
{"created":"2024-03-27 14:30:56","title":"Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics","abstract":"This paper presents findings from an exploratory needfinding study investigating the research current status and potential participation of the competitions on the robotics community towards four human-centric topics: safety, privacy, explainability, and federated learning. We conducted a survey with 34 participants across three distinguished European robotics consortia, nearly 60% of whom possessed over five years of research experience in robotics. Our qualitative and quantitative analysis revealed that current mainstream robotic researchers prioritize safety and explainability, expressing a greater willingness to invest in further research in these areas. Conversely, our results indicate that privacy and federated learning garner less attention and are perceived to have lower potential. Additionally, the study suggests a lack of enthusiasm within the robotics community for participating in competitions related to these topics. Based on these findings, we recommend targeting other communities, such as the machine learning community, for future competitions related to these four human-centric topics.","sentences":["This paper presents findings from an exploratory needfinding study investigating the research current status and potential participation of the competitions on the robotics community towards four human-centric topics: safety, privacy, explainability, and federated learning.","We conducted a survey with 34 participants across three distinguished European robotics consortia, nearly 60% of whom possessed over five years of research experience in robotics.","Our qualitative and quantitative analysis revealed that current mainstream robotic researchers prioritize safety and explainability, expressing a greater willingness to invest in further research in these areas.","Conversely, our results indicate that privacy and federated learning garner less attention and are perceived to have lower potential.","Additionally, the study suggests a lack of enthusiasm within the robotics community for participating in competitions related to these topics.","Based on these findings, we recommend targeting other communities, such as the machine learning community, for future competitions related to these four human-centric topics."],"url":"http://arxiv.org/abs/2403.18616v1","category":"cs.HC"}
{"created":"2024-03-27 14:03:41","title":"One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch","abstract":"Simulated events are key ingredients in almost all high-energy physics analyses. However, imperfections in the simulation can lead to sizeable differences between the observed data and simulated events. The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations. We introduce a correction method that transforms one multidimensional distribution (simulation) into another one (data) using a simple architecture based on a single normalising flow with a boolean condition. We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations.","sentences":["Simulated events are key ingredients in almost all high-energy physics analyses.","However, imperfections in the simulation can lead to sizeable differences between the observed data and simulated events.","The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations.","We introduce a correction method that transforms one multidimensional distribution (simulation) into another one (data) using a simple architecture based on a single normalising flow with a boolean condition.","We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations."],"url":"http://arxiv.org/abs/2403.18582v1","category":"hep-ph"}
{"created":"2024-03-27 13:34:59","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","abstract":"Over the last years, various sentence embedders have been an integral part in the success of current machine learning approaches to Natural Language Processing (NLP). Unfortunately, multiple sources have shown that the bias, inherent in the datasets upon which these embedding methods are trained, is learned by them. A variety of different approaches to remove biases in embeddings exists in the literature. Most of these approaches are applicable to word embeddings and in fewer cases to sentence embeddings. It is problematic that most debiasing approaches are directly transferred from word embeddings, therefore these approaches fail to take into account the nonlinear nature of sentence embedders and the embeddings they produce. It has been shown in literature that bias information is still present if sentence embeddings are debiased using such methods. In this contribution, we explore an approach to remove linear and nonlinear bias information for NLP solutions, without impacting downstream performance. We compare our approach to common debiasing methods on classical bias metrics and on bias metrics which take nonlinear information into account.","sentences":["Over the last years, various sentence embedders have been an integral part in the success of current machine learning approaches to Natural Language Processing (NLP).","Unfortunately, multiple sources have shown that the bias, inherent in the datasets upon which these embedding methods are trained, is learned by them.","A variety of different approaches to remove biases in embeddings exists in the literature.","Most of these approaches are applicable to word embeddings and in fewer cases to sentence embeddings.","It is problematic that most debiasing approaches are directly transferred from word embeddings, therefore these approaches fail to take into account the nonlinear nature of sentence embedders and the embeddings they produce.","It has been shown in literature that bias information is still present if sentence embeddings are debiased using such methods.","In this contribution, we explore an approach to remove linear and nonlinear bias information for NLP solutions, without impacting downstream performance.","We compare our approach to common debiasing methods on classical bias metrics and on bias metrics which take nonlinear information into account."],"url":"http://arxiv.org/abs/2403.18555v1","category":"cs.CL"}
{"created":"2024-03-27 13:27:02","title":"A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint","abstract":"Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing. First, the spatial attention and frequency spectrum filtering are implemented as a spatial-frequency domain information interaction module to handle the first issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source code and Supplementary Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.","sentences":["Existing research based on deep learning has extensively explored the problem of daytime image dehazing.","However, few studies have considered the characteristics of nighttime hazy scenes.","There are two distinctions between nighttime and daytime haze.","First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics.","Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data.","To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing.","First, the spatial attention and frequency spectrum filtering are implemented as a spatial-frequency domain information interaction module to handle the first issue.","Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness.","Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods.","The source code and Supplementary Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD."],"url":"http://arxiv.org/abs/2403.18548v1","category":"cs.CV"}
{"created":"2024-03-27 13:24:56","title":"Optimal Resource Efficiency with Fairness in Heterogeneous GPU Clusters","abstract":"Ensuring the highest training throughput to maximize resource efficiency, while maintaining fairness among users, is critical for deep learning (DL) training in heterogeneous GPU clusters. However, current DL schedulers provide only limited fairness properties and suboptimal training throughput, impeding tenants from effectively leveraging heterogeneous resources. The underlying design challenge stems from inherent conflicts between efficiency and fairness properties.   In this paper, we introduce OEF, a new resource allocation framework specifically developed for achieving optimal resource efficiency and ensuring diverse fairness properties in heterogeneous GPU clusters. By integrating resource efficiency and fairness within a global optimization framework, OEF is capable of providing users with maximized overall efficiency, as well as various guarantees of fairness, in both cooperative and non-cooperative environments. We have implemented OEF in a cluster resource manager and conducted large-scale experiments, showing that OEF can improve the overall training throughput by up to 32% while improving fairness compared to state-of-the-art heterogeneity-aware schedulers.","sentences":["Ensuring the highest training throughput to maximize resource efficiency, while maintaining fairness among users, is critical for deep learning (DL) training in heterogeneous GPU clusters.","However, current DL schedulers provide only limited fairness properties and suboptimal training throughput, impeding tenants from effectively leveraging heterogeneous resources.","The underlying design challenge stems from inherent conflicts between efficiency and fairness properties.   ","In this paper, we introduce OEF, a new resource allocation framework specifically developed for achieving optimal resource efficiency and ensuring diverse fairness properties in heterogeneous GPU clusters.","By integrating resource efficiency and fairness within a global optimization framework, OEF is capable of providing users with maximized overall efficiency, as well as various guarantees of fairness, in both cooperative and non-cooperative environments.","We have implemented OEF in a cluster resource manager and conducted large-scale experiments, showing that OEF can improve the overall training throughput by up to 32% while improving fairness compared to state-of-the-art heterogeneity-aware schedulers."],"url":"http://arxiv.org/abs/2403.18545v1","category":"cs.DC"}
{"created":"2024-03-27 13:17:15","title":"skscope: Fast Sparsity-Constrained Optimization in Python","abstract":"Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its source code is available at: https://github.com/abess-team/skscope.","sentences":["Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact.","In the paper, the library skscope is introduced to overcome such an obstacle.","With skscope, users can solve the SCO by just programming the objective function.","The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code.","More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space.","Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver.","skscope is published on the Python Package Index (PyPI) and Conda, and its source code is available at: https://github.com/abess-team/skscope."],"url":"http://arxiv.org/abs/2403.18540v1","category":"stat.ML"}
{"created":"2024-03-27 12:44:57","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans","abstract":"Unsupervised pathology detection can be implemented by training a model on healthy data only and measuring the deviation from the training set upon inference, for example with CNN-based feature extraction and one-class classifiers, or reconstruction-score-based methods such as AEs, GANs and Diffusion models. Normalizing Flows (NF) have the ability to directly learn the probability distribution of training examples through an invertible architecture. We leverage this property in a novel 3D NF-based model named CT-3DFlow, specifically tailored for patient-level pulmonary pathology detection in chest CT data. Our model is trained unsupervised on healthy 3D pulmonary CT patches, and detects deviations from its log-likelihood distribution as anomalies. We aggregate patches-level likelihood values from a patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction. Out-of-distribution detection performance is evaluated using expert annotations on a separate chest CT test dataset, outperforming other state-of-the-art methods.","sentences":["Unsupervised pathology detection can be implemented by training a model on healthy data only and measuring the deviation from the training set upon inference, for example with CNN-based feature extraction and one-class classifiers, or reconstruction-score-based methods such as AEs, GANs and Diffusion models.","Normalizing Flows (NF) have the ability to directly learn the probability distribution of training examples through an invertible architecture.","We leverage this property in a novel 3D NF-based model named CT-3DFlow, specifically tailored for patient-level pulmonary pathology detection in chest CT data.","Our model is trained unsupervised on healthy 3D pulmonary CT patches, and detects deviations from its log-likelihood distribution as anomalies.","We aggregate patches-level likelihood values from a patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.","Out-of-distribution detection performance is evaluated using expert annotations on a separate chest CT test dataset, outperforming other state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.18514v1","category":"eess.IV"}
{"created":"2024-03-27 12:39:16","title":"Distributed Maximum Consensus over Noisy Links","abstract":"We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.","sentences":["We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links.","Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers.","Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency.","To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates.","Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms."],"url":"http://arxiv.org/abs/2403.18509v1","category":"cs.DC"}
{"created":"2024-03-27 10:40:14","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment","abstract":"Recent research demonstrates the effectiveness of using pre-trained language models for legal case retrieval. Most of the existing works focus on improving the representation ability for the contextualized embedding of the [CLS] token and calculate relevance using textual semantic similarity. However, in the legal domain, textual semantic similarity does not always imply that the cases are relevant enough. Instead, relevance in legal cases primarily depends on the similarity of key facts that impact the final judgment. Without proper treatments, the discriminative ability of learned representations could be limited since legal cases are lengthy and contain numerous non-key facts. To this end, we introduce DELTA, a discriminative model designed for legal case retrieval. The basic idea involves pinpointing key facts in legal cases and pulling the contextualized embedding of the [CLS] token closer to the key facts while pushing away from the non-key facts, which can warm up the case embedding space in an unsupervised manner. To be specific, this study brings the word alignment mechanism to the contextual masked auto-encoder. First, we leverage shallow decoders to create information bottlenecks, aiming to enhance the representation ability. Second, we employ the deep decoder to enable translation between different structures, with the goal of pinpointing key facts to enhance discriminative ability. Comprehensive experiments conducted on publicly available legal benchmarks show that our approach can outperform existing state-of-the-art methods in legal case retrieval. It provides a new perspective on the in-depth understanding and processing of legal case documents.","sentences":["Recent research demonstrates the effectiveness of using pre-trained language models for legal case retrieval.","Most of the existing works focus on improving the representation ability for the contextualized embedding of the [CLS] token and calculate relevance using textual semantic similarity.","However, in the legal domain, textual semantic similarity does not always imply that the cases are relevant enough.","Instead, relevance in legal cases primarily depends on the similarity of key facts that impact the final judgment.","Without proper treatments, the discriminative ability of learned representations could be limited since legal cases are lengthy and contain numerous non-key facts.","To this end, we introduce DELTA, a discriminative model designed for legal case retrieval.","The basic idea involves pinpointing key facts in legal cases and pulling the contextualized embedding of the [CLS] token closer to the key facts while pushing away from the non-key facts, which can warm up the case embedding space in an unsupervised manner.","To be specific, this study brings the word alignment mechanism to the contextual masked auto-encoder.","First, we leverage shallow decoders to create information bottlenecks, aiming to enhance the representation ability.","Second, we employ the deep decoder to enable translation between different structures, with the goal of pinpointing key facts to enhance discriminative ability.","Comprehensive experiments conducted on publicly available legal benchmarks show that our approach can outperform existing state-of-the-art methods in legal case retrieval.","It provides a new perspective on the in-depth understanding and processing of legal case documents."],"url":"http://arxiv.org/abs/2403.18435v1","category":"cs.IR"}
{"created":"2024-03-27 08:39:56","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback","abstract":"Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.","sentences":["Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope.","While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms.","In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics.","These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations.","To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF).","RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions.","Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability."],"url":"http://arxiv.org/abs/2403.18349v1","category":"cs.CL"}
{"created":"2024-03-27 08:32:48","title":"Learning Inclusion Matching for Animation Paint Bucket Colorization","abstract":"Colorizing line art is a pivotal task in the production of hand-drawn cel animation. This typically involves digital painters using a paint bucket tool to manually color each segment enclosed by lines, based on RGB values predetermined by a color designer. This frame-by-frame process is both arduous and time-intensive. Current automated methods mainly focus on segment matching. This technique migrates colors from a reference to the target frame by aligning features within line-enclosed segments across frames. However, issues like occlusion and wrinkles in animations often disrupt these direct correspondences, leading to mismatches. In this work, we introduce a new learning-based inclusion matching pipeline, which directs the network to comprehend the inclusion relationships between segments rather than relying solely on direct visual correspondences. Our method features a two-stage pipeline that integrates a coarse color warping module with an inclusion matching module, enabling more nuanced and accurate colorization. To facilitate the training of our network, we also develope a unique dataset, referred to as PaintBucket-Character. This dataset includes rendered line arts alongside their colorized counterparts, featuring various 3D characters. Extensive experiments demonstrate the effectiveness and superiority of our method over existing techniques.","sentences":["Colorizing line art is a pivotal task in the production of hand-drawn cel animation.","This typically involves digital painters using a paint bucket tool to manually color each segment enclosed by lines, based on RGB values predetermined by a color designer.","This frame-by-frame process is both arduous and time-intensive.","Current automated methods mainly focus on segment matching.","This technique migrates colors from a reference to the target frame by aligning features within line-enclosed segments across frames.","However, issues like occlusion and wrinkles in animations often disrupt these direct correspondences, leading to mismatches.","In this work, we introduce a new learning-based inclusion matching pipeline, which directs the network to comprehend the inclusion relationships between segments rather than relying solely on direct visual correspondences.","Our method features a two-stage pipeline that integrates a coarse color warping module with an inclusion matching module, enabling more nuanced and accurate colorization.","To facilitate the training of our network, we also develope a unique dataset, referred to as PaintBucket-Character.","This dataset includes rendered line arts alongside their colorized counterparts, featuring various 3D characters.","Extensive experiments demonstrate the effectiveness and superiority of our method over existing techniques."],"url":"http://arxiv.org/abs/2403.18342v1","category":"cs.CV"}
{"created":"2024-03-27 08:32:19","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","abstract":"With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\\%$ in harmlessness.","sentences":["With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial.","Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment.","However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming.","To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign.","IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM.","These constitutions are then used to guide self-correction of the base LLM.","Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM.","Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\\%$ in harmlessness."],"url":"http://arxiv.org/abs/2403.18341v1","category":"cs.CL"}
{"created":"2024-03-27 08:21:01","title":"A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages","abstract":"User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.","sentences":["User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world.","However, the existing clinical corpora predominantly revolve around scientific articles in English.","This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese.","Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types.","It contributes to the development of real-world multilingual language models for healthcare.","We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages."],"url":"http://arxiv.org/abs/2403.18336v1","category":"cs.CL"}
{"created":"2024-03-27 08:11:25","title":"Tracking-Assisted Object Detection with Event Cameras","abstract":"Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies for pseudo-occluded objects to maintain their permanence and retain their bounding boxes, even when features have not been available for a very long time. These strategies can be treated as an explicit-learned memory guided by the tracking objective to record the displacements of objects across frames. Lastly, we propose a spatio-temporal feature aggregation module to enrich the latent features and a consistency loss to increase the robustness of the overall pipeline. We conduct comprehensive experiments to verify our method's effectiveness where still objects are retained but real occluded objects are discarded. The results demonstrate that (1) the additional visibility labels can assist in supervised training, and (2) our method outperforms state-of-the-art approaches with a significant improvement of 7.9% absolute mAP.","sentences":["Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur.","However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task.","Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues.","While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively.","In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features.","Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset.","Secondly, we exploit tracking strategies for pseudo-occluded objects to maintain their permanence and retain their bounding boxes, even when features have not been available for a very long time.","These strategies can be treated as an explicit-learned memory guided by the tracking objective to record the displacements of objects across frames.","Lastly, we propose a spatio-temporal feature aggregation module to enrich the latent features and a consistency loss to increase the robustness of the overall pipeline.","We conduct comprehensive experiments to verify our method's effectiveness where still objects are retained but real occluded objects are discarded.","The results demonstrate that (1) the additional visibility labels can assist in supervised training, and (2) our method outperforms state-of-the-art approaches with a significant improvement of 7.9% absolute mAP."],"url":"http://arxiv.org/abs/2403.18330v1","category":"cs.CV"}
{"created":"2024-03-27 08:07:07","title":"Privacy-Preserving Distributed Nonnegative Matrix Factorization","abstract":"Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decryption. Simulation results conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc networks.","sentences":["Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning.","However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents.","To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy.","It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data.","To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decryption.","Simulation results conducted on synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc networks."],"url":"http://arxiv.org/abs/2403.18326v1","category":"cs.CR"}
{"created":"2024-03-27 07:38:36","title":"Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications","abstract":"Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities. While prior works have successfully leveraged multiple modalities in supervised settings, we apply advanced self-supervised multi-modal contrastive learning techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks. We introduce a loss function Multi-Modal Neighborhood Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and zero-shot performance of our approach.","sentences":["Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities.","While prior works have successfully leveraged multiple modalities in supervised settings, we apply advanced self-supervised multi-modal contrastive learning techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks.","We introduce a loss function Multi-Modal Neighborhood Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and zero-shot performance of our approach."],"url":"http://arxiv.org/abs/2403.18316v1","category":"cs.LG"}
{"created":"2024-03-27 05:52:39","title":"Image Deraining via Self-supervised Reinforcement Learning","abstract":"The quality of images captured outdoors is often affected by the weather. One factor that interferes with sight is rain, which can obstruct the view of observers and computer vision applications that rely on those images. The work aims to recover rain images by removing rain streaks via Self-supervised Reinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain streak pixels from the input rain image via dictionary learning and use pixel-wise RL agents to take multiple inpainting actions to remove rain progressively. To our knowledge, this work is the first attempt where self-supervised RL is applied to image deraining. Experimental results on several benchmark image-deraining datasets show that the proposed SRL-Derain performs favorably against state-of-the-art few-shot and self-supervised deraining and denoising methods.","sentences":["The quality of images captured outdoors is often affected by the weather.","One factor that interferes with sight is rain, which can obstruct the view of observers and computer vision applications that rely on those images.","The work aims to recover rain images by removing rain streaks via Self-supervised Reinforcement Learning (RL) for image deraining (SRL-Derain).","We locate rain streak pixels from the input rain image via dictionary learning and use pixel-wise RL agents to take multiple inpainting actions to remove rain progressively.","To our knowledge, this work is the first attempt where self-supervised RL is applied to image deraining.","Experimental results on several benchmark image-deraining datasets show that the proposed SRL-Derain performs favorably against state-of-the-art few-shot and self-supervised deraining and denoising methods."],"url":"http://arxiv.org/abs/2403.18270v1","category":"cs.CV"}
{"created":"2024-03-27 05:50:23","title":"Clustering Change Sign Detection by Fusing Mixture Complexity","abstract":"This paper proposes an early detection method for cluster structural changes. Cluster structure refers to discrete structural characteristics, such as the number of clusters, when data are represented using finite mixture models, such as Gaussian mixture models. We focused on scenarios in which the cluster structure gradually changed over time. For finite mixture models, the concept of mixture complexity (MC) measures the continuous cluster size by considering the cluster proportion bias and overlap between clusters. In this paper, we propose MC fusion as an extension of MC to handle situations in which multiple mixture numbers are possible in a finite mixture model. By incorporating the fusion of multiple models, our approach accurately captured the cluster structure during transitional periods of gradual change. Moreover, we introduce a method for detecting changes in the cluster structure by examining the transition of MC fusion. We demonstrate the effectiveness of our method through empirical analysis using both artificial and real-world datasets.","sentences":["This paper proposes an early detection method for cluster structural changes.","Cluster structure refers to discrete structural characteristics, such as the number of clusters, when data are represented using finite mixture models, such as Gaussian mixture models.","We focused on scenarios in which the cluster structure gradually changed over time.","For finite mixture models, the concept of mixture complexity (MC) measures the continuous cluster size by considering the cluster proportion bias and overlap between clusters.","In this paper, we propose MC fusion as an extension of MC to handle situations in which multiple mixture numbers are possible in a finite mixture model.","By incorporating the fusion of multiple models, our approach accurately captured the cluster structure during transitional periods of gradual change.","Moreover, we introduce a method for detecting changes in the cluster structure by examining the transition of MC fusion.","We demonstrate the effectiveness of our method through empirical analysis using both artificial and real-world datasets."],"url":"http://arxiv.org/abs/2403.18269v1","category":"stat.ML"}
{"created":"2024-03-27 04:51:42","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","abstract":"Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection. Specifically, we devise a prompt learning template tailored for the metaphor detection task. By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection. Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model. The inclusion of soft labels, akin to label smoothing, helps alleviate the model's tendency towards over-confidence and effectively addresses the challenge of data sparsity. Experimental results demonstrate that our proposed model achieves state-of-the-art performance across multiple datasets.","sentences":["Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge.","Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity.","To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection.","Specifically, we devise a prompt learning template tailored for the metaphor detection task.","By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words.","This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection.","Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model.","The inclusion of soft labels, akin to label smoothing, helps alleviate the model's tendency towards over-confidence and effectively addresses the challenge of data sparsity.","Experimental results demonstrate that our proposed model achieves state-of-the-art performance across multiple datasets."],"url":"http://arxiv.org/abs/2403.18253v1","category":"cs.CL"}
{"created":"2024-03-27 03:39:57","title":"Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data","abstract":"PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL). In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to further mitigate label noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer classification, then use the strongest model to tune a multi-scale classifier with MIL. We train our MIL models using our novel multi-objective learning strategy and compare our results to existing baselines. RESULTS: We find that for both ROI-scale and multi-scale PCa detection, image transformer backbones lag behind their CNN counterparts. This deficit in performance is even more noticeable for larger models. When using multi-objective learning, we can improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for modelling sparse datasets of prostate ultrasounds, producing more robust features than transformers in PCa detection. Multi-scale methods remain the best architecture for this task, with multi-objective learning presenting an effective way to improve performance.","sentences":["PURPOSE:","Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region.","However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs.","Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL).","In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification.","We also design a novel multi-objective learning strategy that combines both ROI and core predictions to further mitigate label noise.","METHODS:","We evaluate 3 image transformers on ROI-scale cancer classification, then use the strongest model to tune a multi-scale classifier with MIL.","We train our MIL models using our novel multi-objective learning strategy and compare our results to existing baselines.","RESULTS:","We find that for both ROI-scale and multi-scale PCa detection, image transformer backbones lag behind their CNN counterparts.","This deficit in performance is even more noticeable for larger models.","When using multi-objective learning, we can improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%.","CONCLUSION:","Convolutional networks are better suited for modelling sparse datasets of prostate ultrasounds, producing more robust features than transformers in PCa detection.","Multi-scale methods remain the best architecture for this task, with multi-objective learning presenting an effective way to improve performance."],"url":"http://arxiv.org/abs/2403.18233v1","category":"eess.IV"}
{"created":"2024-03-27 02:59:04","title":"Minimax Optimal Fair Classification with Bounded Demographic Disparity","abstract":"Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring fairness. While extensive research aims to reduce disparity, the effect of using a \\emph{finite dataset} -- as opposed to the entire population -- remains unclear. This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups. Although fairness may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds. We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold. To quantify the impact of fairness constraints, we introduce a novel measure called \\emph{fairness-aware excess risk} and derive a minimax lower bound on this measure that all classifiers must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding method with an offset that we show attains the minimax lower bound. Our lower bound proofs involve several innovations. Experiments support that FairBayes-DDP+ controls disparity at the user-specified level, while being faster and having a more favorable fairness-accuracy tradeoff than several baselines.","sentences":["Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring fairness.","While extensive research aims to reduce disparity, the effect of using a \\emph{finite dataset} -- as opposed to the entire population -- remains unclear.","This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups.","Although fairness may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds.","We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold.","To quantify the impact of fairness constraints, we introduce a novel measure called \\emph{fairness-aware excess risk} and derive a minimax lower bound on this measure that all classifiers must satisfy.","Furthermore, we propose FairBayes-DDP+, a group-wise thresholding method with an offset that we show attains the minimax lower bound.","Our lower bound proofs involve several innovations.","Experiments support that FairBayes-DDP+ controls disparity at the user-specified level, while being faster and having a more favorable fairness-accuracy tradeoff than several baselines."],"url":"http://arxiv.org/abs/2403.18216v1","category":"stat.ML"}
{"created":"2024-03-27 01:40:21","title":"LayoutFlow: Flow Matching for Layout Generation","abstract":"Finding a suitable layout represents a crucial task for diverse applications in graphic design. Motivated by simpler and smoother sampling trajectories, we explore the use of Flow Matching as an alternative to current diffusion-based layout generation models. Specifically, we propose LayoutFlow, an efficient flow-based model capable of generating high-quality layouts. Instead of progressively denoising the elements of a noisy layout, our method learns to gradually move, or flow, the elements of an initial sample until it reaches its final prediction. In addition, we employ a conditioning scheme that allows us to handle various generation tasks with varying degrees of conditioning with a single model. Empirically, LayoutFlow performs on par with state-of-the-art models while being significantly faster.","sentences":["Finding a suitable layout represents a crucial task for diverse applications in graphic design.","Motivated by simpler and smoother sampling trajectories, we explore the use of Flow Matching as an alternative to current diffusion-based layout generation models.","Specifically, we propose LayoutFlow, an efficient flow-based model capable of generating high-quality layouts.","Instead of progressively denoising the elements of a noisy layout, our method learns to gradually move, or flow, the elements of an initial sample until it reaches its final prediction.","In addition, we employ a conditioning scheme that allows us to handle various generation tasks with varying degrees of conditioning with a single model.","Empirically, LayoutFlow performs on par with state-of-the-art models while being significantly faster."],"url":"http://arxiv.org/abs/2403.18187v1","category":"cs.CV"}
{"created":"2024-03-27 01:18:00","title":"Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering","abstract":"Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using hierarchical clustering. Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions.","sentences":["Machine learning methods allow the prediction of nonlinear dynamical systems from data alone.","The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems.","The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions.","The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix.","In this work, we propose a method to compress the Koopman matrix using hierarchical clustering.","Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions."],"url":"http://arxiv.org/abs/2403.18181v1","category":"cs.LG"}
{"created":"2024-03-27 00:55:16","title":"Higher order multi-dimension reduction methods via Einstein-product","abstract":"This paper explores the extension of dimension reduction (DR) techniques to the multi-dimension case by using the Einstein product. Our focus lies on graph-based methods, encompassing both linear and nonlinear approaches, within both supervised and unsupervised learning paradigms. Additionally, we investigate variants such as repulsion graphs and kernel methods for linear approaches. Furthermore, we present two generalizations for each method, based on single or multiple weights. We demonstrate the straightforward nature of these generalizations and provide theoretical insights. Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images.","sentences":["This paper explores the extension of dimension reduction (DR) techniques to the multi-dimension case by using the Einstein product.","Our focus lies on graph-based methods, encompassing both linear and nonlinear approaches, within both supervised and unsupervised learning paradigms.","Additionally, we investigate variants such as repulsion graphs and kernel methods for linear approaches.","Furthermore, we present two generalizations for each method, based on single or multiple weights.","We demonstrate the straightforward nature of these generalizations and provide theoretical insights.","Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images."],"url":"http://arxiv.org/abs/2403.18171v1","category":"math.NA"}
{"created":"2024-03-27 14:45:41","title":"Improving Efficiency of Parallel Across the Method Spectral Deferred Corrections","abstract":"Parallel-across-the method time integration can provide small scale parallelism when solving initial value problems. Spectral deferred corrections (SDC) with a diagonal sweeper, which is closely related to iterated Runge-Kutta methods proposed by Van der Houwen and Sommeijer, can use a number of threads equal to the number of quadrature nodes in the underlying collocation method. However, convergence speed, efficiency and stability depends critically on the used coefficients. Previous approaches have used numerical optimization to find good parameters. Instead, we propose an ansatz that allows to find optimal parameters analytically. We show that the resulting parallel SDC methods provide stability domains and convergence order very similar to those of well established serial SDC variants. Using a model for computational cost that assumes 80% efficiency of an implementation of parallel SDC we show that our variants are competitive with serial SDC, previously published parallel SDC coefficients as well as Picard iteration, explicit RKM-4 and an implicit fourth-order diagonally implicit Runge-Kutta method.","sentences":["Parallel-across-the method time integration can provide small scale parallelism when solving initial value problems.","Spectral deferred corrections (SDC) with a diagonal sweeper, which is closely related to iterated Runge-Kutta methods proposed by Van der Houwen and Sommeijer, can use a number of threads equal to the number of quadrature nodes in the underlying collocation method.","However, convergence speed, efficiency and stability depends critically on the used coefficients.","Previous approaches have used numerical optimization to find good parameters.","Instead, we propose an ansatz that allows to find optimal parameters analytically.","We show that the resulting parallel SDC methods provide stability domains and convergence order very similar to those of well established serial SDC variants.","Using a model for computational cost that assumes 80% efficiency of an implementation of parallel SDC we show that our variants are competitive with serial SDC, previously published parallel SDC coefficients as well as Picard iteration, explicit RKM-4 and an implicit fourth-order diagonally implicit Runge-Kutta method."],"url":"http://arxiv.org/abs/2403.18641v1","category":"math.NA"}
{"created":"2024-03-27 14:32:30","title":"Accelerating preconditioned ADMM via degenerate proximal point mappings","abstract":"In this paper, we aim to accelerate a preconditioned alternating direction method of multipliers (pADMM), whose proximal terms are convex quadratic functions, for solving linearly constrained convex optimization problems. To achieve this, we first reformulate the pADMM into a form of proximal point method (PPM) with a positive semidefinite preconditioner which can be degenerate due to the lack of strong convexity of the proximal terms in the pADMM. Then we accelerate the pADMM by accelerating the reformulated degenerate PPM (dPPM). Specifically, we first propose an accelerated dPPM by integrating the Halpern iteration and the fast Krasnosel'ski\\u{i}-Mann iteration into it, achieving asymptotic $o(1/k)$ and non-asymptotic $O(1/k)$ convergence rates. Subsequently, building upon the accelerated dPPM, we develop an accelerated pADMM algorithm that exhibits both asymptotic $o(1/k)$ and non-asymptotic $O(1/k)$ nonergodic convergence rates concerning the Karush-Kuhn-Tucker residual and the primal objective function value gap. Preliminary numerical experiments validate the theoretical findings, demonstrating that the accelerated pADMM outperforms the pADMM in solving convex quadratic programming problems.","sentences":["In this paper, we aim to accelerate a preconditioned alternating direction method of multipliers (pADMM), whose proximal terms are convex quadratic functions, for solving linearly constrained convex optimization problems.","To achieve this, we first reformulate the pADMM into a form of proximal point method (PPM) with a positive semidefinite preconditioner which can be degenerate due to the lack of strong convexity of the proximal terms in the pADMM.","Then we accelerate the pADMM by accelerating the reformulated degenerate PPM (dPPM).","Specifically, we first propose an accelerated dPPM by integrating the Halpern iteration and the fast Krasnosel'ski\\u{i}-Mann iteration into it, achieving asymptotic $o(1/k)$ and non-asymptotic $O(1/k)$ convergence rates.","Subsequently, building upon the accelerated dPPM, we develop an accelerated pADMM algorithm that exhibits both asymptotic $o(1/k)$ and non-asymptotic $O(1/k)$ nonergodic convergence rates concerning the Karush-Kuhn-Tucker residual and the primal objective function value gap.","Preliminary numerical experiments validate the theoretical findings, demonstrating that the accelerated pADMM outperforms the pADMM in solving convex quadratic programming problems."],"url":"http://arxiv.org/abs/2403.18618v1","category":"math.OC"}
{"created":"2024-03-27 14:10:01","title":"Quantum backflow current in a ring: Optimal bounds and fractality","abstract":"The probability density of a quantum particle moving freely within a circular ring can exhibit local flow patterns inconsistent with its angular momentum, a phenomenon known as quantum backflow. In this study, we examine a quantum particle confined to a ring and prepared in a state composed of a fixed (yet arbitrary) number of lowest energy eigenstates with non-negative angular momentum. We investigate the time-dependent behavior of the probability current at a specified point along the ring's circumference. We establish precise lower and upper bounds for this probability current, thereby delineating the exact scope of the quantum backflow effect. We also present an analytical expression for a quantum state that yields a record-high backflow probability transfer, reaching over 95% of the theoretical bound. Furthermore, our investigation yields compelling numerical and analytical evidence supporting the conjecture that the current-versus-time function associated with states maximizing backflow probability transfer forms a fractal curve with a dimension of 7/4. The observed fractality may provide a characteristic, experimentally-relevant signature of quantum backflow near the probability-transfer bound.","sentences":["The probability density of a quantum particle moving freely within a circular ring can exhibit local flow patterns inconsistent with its angular momentum, a phenomenon known as quantum backflow.","In this study, we examine a quantum particle confined to a ring and prepared in a state composed of a fixed (yet arbitrary) number of lowest energy eigenstates with non-negative angular momentum.","We investigate the time-dependent behavior of the probability current at a specified point along the ring's circumference.","We establish precise lower and upper bounds for this probability current, thereby delineating the exact scope of the quantum backflow effect.","We also present an analytical expression for a quantum state that yields a record-high backflow probability transfer, reaching over 95% of the theoretical bound.","Furthermore, our investigation yields compelling numerical and analytical evidence supporting the conjecture that the current-versus-time function associated with states maximizing backflow probability transfer forms a fractal curve with a dimension of 7/4.","The observed fractality may provide a characteristic, experimentally-relevant signature of quantum backflow near the probability-transfer bound."],"url":"http://arxiv.org/abs/2403.18586v1","category":"quant-ph"}
{"created":"2024-03-27 13:37:12","title":"Numerical optimisation of Dirac eigenvalues","abstract":"Motivated by relativistic materials, we develop a numerical scheme to support existing or state new conjectures in the spectral optimisation of eigenvalues of the Dirac operator, subject to infinite-mass boundary conditions. We study the optimality of the regular polygon (respectively, disk) among all polygons of a given number of sides (respectively, arbitrary sets), subject to area or perimeter constraints. We consider the three lowest positive eigenvalues and their ratios. Roughly, we find results analogous to known or expected for the Dirichlet Laplacian, except for the third eigenvalue which does not need to be minimised by the regular polygon (respectively, the disk) for all masses. In addition to the numerical results, a new, mass-dependent upper bound to the lowest eigenvalue in rectangles is proved and its extension to arbitrary quadrilaterals is conjectured.","sentences":["Motivated by relativistic materials, we develop a numerical scheme to support existing or state new conjectures in the spectral optimisation of eigenvalues of the Dirac operator, subject to infinite-mass boundary conditions.","We study the optimality of the regular polygon (respectively, disk) among all polygons of a given number of sides (respectively, arbitrary sets), subject to area or perimeter constraints.","We consider the three lowest positive eigenvalues and their ratios.","Roughly, we find results analogous to known or expected for the Dirichlet Laplacian, except for the third eigenvalue which does not need to be minimised by the regular polygon (respectively, the disk) for all masses.","In addition to the numerical results, a new, mass-dependent upper bound to the lowest eigenvalue in rectangles is proved and its extension to arbitrary quadrilaterals is conjectured."],"url":"http://arxiv.org/abs/2403.18556v1","category":"math.OC"}
{"created":"2024-03-27 11:56:17","title":"Existence and role of low energy charge-paramagnon modes in the strange metal phase of Bi$_2$Sr$_2$CaCu$_2$O$_{8+y}$","abstract":"The strange metal phase is characteristic for the $T$-linear dc resistivity behaviour over a large $T$-range. The effect of the strength of the charge-paramagnon interactions on the charge fluctuations in optimally doped and underdoped regions of Bi$_2$Sr$_2$CaCu$_2$O$_{8+y}$ (Bi-2212) may shine light on the anomalous behaviour of the optical conductivity response in the energy range 50 - 500 meV. We present a preliminary analysis of a single initial run of electron energy loss spectroscopy (EELS) measurements done in a scanning transmission electron microscope (STEM) which exhibit linear dispersive modes separated by 50 meV energy gaps up to 250 meV in optimally doped Bi-2212. Our observations show similarities with the fluctuating stripes as predicted by Zaanen.","sentences":["The strange metal phase is characteristic for the $T$-linear dc resistivity behaviour over a large $T$-range.","The effect of the strength of the charge-paramagnon interactions on the charge fluctuations in optimally doped and underdoped regions of Bi$_2$Sr$_2$CaCu$_2$O$_{8+y}$ (Bi-2212) may shine light on the anomalous behaviour of the optical conductivity response in the energy range 50 - 500 meV.","We present a preliminary analysis of a single initial run of electron energy loss spectroscopy (EELS) measurements done in a scanning transmission electron microscope (STEM) which exhibit linear dispersive modes separated by 50 meV energy gaps up to 250 meV in optimally doped Bi-2212.","Our observations show similarities with the fluctuating stripes as predicted by Zaanen."],"url":"http://arxiv.org/abs/2403.18483v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 10:47:53","title":"Physics and data driven model for prediction of residual stresses in machining","abstract":"Predicting residual stresses has always been a topic of significance due to its implications in the development of enhanced materials and better processing conditions. In this work, an analytical model for prediction of residual stresses is developed for orthogonal machining. It consists of three component models for force, temperature and stress computation. The Oxley force model and Waldorf's slip-line model are employed for obtaining cutting force, thrust force, and temperatures at the shear zone and tool-chip interface for the given parameters. The Komanduri-Hou two heat source model is used for obtaining the temperature distribution in the workpiece. The effect of coolant with differing mass flow rates has also been incorporated. The residual stresses are obtained by combining the mechanical and thermal components, followed by the loading and relaxation of the stresses. Optimal values for unknown parameters are predicted by leveraging a cost function. The residual stress distributions obtained give a tensile region near the surface for Inconel 718, and a compressive region for Ti6Al4V, which are in line with experimental results found in literature.","sentences":["Predicting residual stresses has always been a topic of significance due to its implications in the development of enhanced materials and better processing conditions.","In this work, an analytical model for prediction of residual stresses is developed for orthogonal machining.","It consists of three component models for force, temperature and stress computation.","The Oxley force model and Waldorf's slip-line model are employed for obtaining cutting force, thrust force, and temperatures at the shear zone and tool-chip interface for the given parameters.","The Komanduri-Hou two heat source model is used for obtaining the temperature distribution in the workpiece.","The effect of coolant with differing mass flow rates has also been incorporated.","The residual stresses are obtained by combining the mechanical and thermal components, followed by the loading and relaxation of the stresses.","Optimal values for unknown parameters are predicted by leveraging a cost function.","The residual stress distributions obtained give a tensile region near the surface for Inconel 718, and a compressive region for Ti6Al4V, which are in line with experimental results found in literature."],"url":"http://arxiv.org/abs/2403.18441v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 09:41:51","title":"Reweighted Quasi Norm Regularized Low-Rank Factorization for Matrix Robust PCA","abstract":"Robust Principal Component Analysis (RPCA) and its associated non-convex relaxation methods constitute a significant component of matrix completion problems, wherein matrix factorization strategies effectively reduce dimensionality and enhance computational speed. However, some non-convex factorization forms lack theoretical guarantees. This paper proposes a novel strategy in non-convex quasi-norm representation, introducing a method to obtain weighted matrix quasi-norm factorization forms. Especially, explicit bilinear factor matrix factorization formulations for the weighted logarithmic norm and weighted Schatten-$q$ quasi norms with $q=1, 1/2, 2/3$ are provided, along with the establishment of corresponding matrix completion models. An Alternating Direction Method of Multipliers (ADMM) framework algorithm is employed for solving, and convergence results of the algorithm are presented.","sentences":["Robust Principal Component Analysis (RPCA) and its associated non-convex relaxation methods constitute a significant component of matrix completion problems, wherein matrix factorization strategies effectively reduce dimensionality and enhance computational speed.","However, some non-convex factorization forms lack theoretical guarantees.","This paper proposes a novel strategy in non-convex quasi-norm representation, introducing a method to obtain weighted matrix quasi-norm factorization forms.","Especially, explicit bilinear factor matrix factorization formulations for the weighted logarithmic norm and weighted Schatten-$q$ quasi norms with $q=1, 1/2, 2/3$ are provided, along with the establishment of corresponding matrix completion models.","An Alternating Direction Method of Multipliers (ADMM) framework algorithm is employed for solving, and convergence results of the algorithm are presented."],"url":"http://arxiv.org/abs/2403.18400v1","category":"math.OC"}
{"created":"2024-03-27 09:32:26","title":"Effective Embedding of Integer Linear Inequalities for Variational Quantum Algorithms","abstract":"In variational quantum algorithms, constraints are usually added to the problem objective via penalty terms. For linear inequality constraints, this procedure requires additional slack qubits. Those extra qubits tend to blow up the search space and complicate the parameter landscapes to be navigated by the classical optimizers. In this work, we explore approaches to model linear inequalities for quantum algorithms without these drawbacks. More concretely, our main suggestion is to omit the slack qubits completely and evaluate the inequality classically during parameter tuning. We test our methods on QAOA as well as on Trotterized adiabatic evolution, and present empirical results. As a benchmark problem, we consider different instances of the multi-knapsack problem. Our results show that removing the slack bits from the circuit Hamiltonian and considering them only for the expectation value yields better solution quality than the standard approach. The tests have been carried out using problem sizes up to 26 qubits. Our methods can in principle be applied to any problem with linear inequality constraints, and are suitable for variational as well as digitized versions of adiabatic quantum computing.","sentences":["In variational quantum algorithms, constraints are usually added to the problem objective via penalty terms.","For linear inequality constraints, this procedure requires additional slack qubits.","Those extra qubits tend to blow up the search space and complicate the parameter landscapes to be navigated by the classical optimizers.","In this work, we explore approaches to model linear inequalities for quantum algorithms without these drawbacks.","More concretely, our main suggestion is to omit the slack qubits completely and evaluate the inequality classically during parameter tuning.","We test our methods on QAOA as well as on Trotterized adiabatic evolution, and present empirical results.","As a benchmark problem, we consider different instances of the multi-knapsack problem.","Our results show that removing the slack bits from the circuit Hamiltonian and considering them only for the expectation value yields better solution quality than the standard approach.","The tests have been carried out using problem sizes up to 26 qubits.","Our methods can in principle be applied to any problem with linear inequality constraints, and are suitable for variational as well as digitized versions of adiabatic quantum computing."],"url":"http://arxiv.org/abs/2403.18395v1","category":"quant-ph"}
{"created":"2024-03-27 09:00:02","title":"Damage Mechanics Challenge: Predictions based on the phase field fracture model","abstract":"In this work, we describe our contribution to the Purdue-SANDIA-LLNL \\emph{Damage Mechanics Challenge}. The phase field fracture model is adopted to blindly estimate the failure characteristics of the challenge test, an unconventional three-point bending experiment on an additively manufactured rock resembling a type of gypsum. The model is formulated in a variationally consistent fashion, incorporating a volumetric-deviatoric strain energy decomposition, and the numerical implementation adopts a monolithic unconditionally stable solution scheme. Our focus is on providing an efficient and simple yet rigorous approach capable of delivering accurate predictions based solely on physical parameters. Model inputs are Young's modulus $E$, Poisson's ratio $\\nu$, toughness $G_c$ and strength $\\sigma_c$ (as determined by the choice of phase field length scale $\\ell$). We show that a single mode I three-point bending test is sufficient to calibrate the model, and that the calibrated model can then reliably predict the force versus displacement responses, crack paths and surface crack morphologies of more intricate three-point bending experiments that are inherently mixed-mode. Importantly, our peak load, crack trajectory and crack surface morphology predictions for the challenge test, submitted before the experimental data was released, show a remarkable agreement with experiments. The characteristics of the challenge, and how changes in these can impact the predictive abilities of phase field fracture models, are also discussed.","sentences":["In this work, we describe our contribution to the Purdue-SANDIA-LLNL \\emph{Damage Mechanics Challenge}.","The phase field fracture model is adopted to blindly estimate the failure characteristics of the challenge test, an unconventional three-point bending experiment on an additively manufactured rock resembling a type of gypsum.","The model is formulated in a variationally consistent fashion, incorporating a volumetric-deviatoric strain energy decomposition, and the numerical implementation adopts a monolithic unconditionally stable solution scheme.","Our focus is on providing an efficient and simple yet rigorous approach capable of delivering accurate predictions based solely on physical parameters.","Model inputs are Young's modulus $E$, Poisson's ratio $\\nu$, toughness $G_c$ and strength $\\sigma_c$ (as determined by the choice of phase field length scale $\\ell$).","We show that a single mode I three-point bending test is sufficient to calibrate the model, and that the calibrated model can then reliably predict the force versus displacement responses, crack paths and surface crack morphologies of more intricate three-point bending experiments that are inherently mixed-mode.","Importantly, our peak load, crack trajectory and crack surface morphology predictions for the challenge test, submitted before the experimental data was released, show a remarkable agreement with experiments.","The characteristics of the challenge, and how changes in these can impact the predictive abilities of phase field fracture models, are also discussed."],"url":"http://arxiv.org/abs/2403.18369v1","category":"cs.CE"}
{"created":"2024-03-27 08:58:41","title":"The Mercer-Young Theorem for Matrix-Valued Kernels on Separable Metric Spaces","abstract":"We generalize the characterization theorem going back to Mercer and Young, which states that a symmetric and continuous kernel is positive definite if and only if it is integrally positive definite. More precisely, we extend the result from real-valued kernels on compact intervals to matrix-valued kernels on separable metric spaces. We also demonstrate the applications of the generalized theorem to the field of convex optimization.","sentences":["We generalize the characterization theorem going back to Mercer and Young, which states that a symmetric and continuous kernel is positive definite if and only if it is integrally positive definite.","More precisely, we extend the result from real-valued kernels on compact intervals to matrix-valued kernels on separable metric spaces.","We also demonstrate the applications of the generalized theorem to the field of convex optimization."],"url":"http://arxiv.org/abs/2403.18368v1","category":"math.FA"}
{"created":"2024-03-27 08:57:07","title":"Computing safe bicycle routes -- Berechnung sicherer Fahrradwege","abstract":"The safety of streets is difficult to quantify numerically. However, it is possible to sort streets regarding their safety into ordered categories, like safe, neutral and unsafe. In this paper we model the computation of safe bicycle routes as an optimization problem with ordinal coefficients. We describe an appropriate optimality concept for ordinal optimization problems and introduce a solution strategy for ordinal routing problems. Furthermore, we introduce a concept to incorporate safety preferences by introducing weights such that longer path with a higher safety rating are preferred. We apply the concept of ordinal routing to compute safe bicycle routes in Stuttgart, Germany, based on dates from OpenStreetMaps. We show that the choice of the weights does not only represent the trade-off of safety vs. path length, but has also an impact on the number of alternative solutions and thus on the computation time.   --   Die Sicherheit von Wegen ist nur eingeschr\\\"ankt messbar und daher schwierig zu quantifizieren. Dahingegen ist es verh\\\"altnism\\\"a{\\ss}ig leicht Wege bez\\\"uglich ihrer Sicherheit in geordnete Kategorien, wie beispielsweise sicher, neutral und gef\\\"ahrlich einzuordnen. In diesem Beitrag werden Optimierungsprobleme mit geordneten Kategorien formuliert und Optimalit\\\"at f\\\"ur diese definiert. Daraus wird eine L\\\"osungsstrategie f\\\"ur solche Probleme abgeleitet. Dar\\\"uber hinaus wird erkl\\\"art, wie die Abgrenzung zwischen den Kategorien erh\\\"oht werden kann, sodass l\\\"angere aber daf\\\"ur sicherere Wege mit Hilfe von Gewichten berechnet werden k\\\"onnen. Diese theoretischen Ergebnisse werden in der Praxis angewendet und es werden auf Grundlage von Daten von OpenStreetMaps sichere Fahrradwege in Stuttgart berechnet. Dabei zeigt sich, dass eine gute Wahl der Gewichte zu weniger L\\\"osungen und k\\\"urzeren Rechenzeiten f\\\"uhrt.","sentences":["The safety of streets is difficult to quantify numerically.","However, it is possible to sort streets regarding their safety into ordered categories, like safe, neutral and unsafe.","In this paper we model the computation of safe bicycle routes as an optimization problem with ordinal coefficients.","We describe an appropriate optimality concept for ordinal optimization problems and introduce a solution strategy for ordinal routing problems.","Furthermore, we introduce a concept to incorporate safety preferences by introducing weights such that longer path with a higher safety rating are preferred.","We apply the concept of ordinal routing to compute safe bicycle routes in Stuttgart, Germany, based on dates from OpenStreetMaps.","We show that the choice of the weights does not only represent the trade-off of safety vs. path length, but has also an impact on the number of alternative solutions and thus on the computation time.   ","--   Die Sicherheit von Wegen ist nur eingeschr\\\"ankt messbar und daher schwierig zu quantifizieren.","Dahingegen ist es verh\\\"altnism\\\"a{\\ss}ig leicht Wege bez\\\"uglich ihrer Sicherheit in geordnete Kategorien, wie beispielsweise sicher, neutral und gef\\\"ahrlich einzuordnen.","In diesem Beitrag werden Optimierungsprobleme mit geordneten Kategorien formuliert und Optimalit\\\"at f\\\"ur diese definiert.","Daraus wird eine L\\\"osungsstrategie f\\\"ur solche Probleme abgeleitet.","Dar\\\"uber hinaus wird erkl\\\"art, wie die Abgrenzung zwischen den Kategorien erh\\\"oht werden kann, sodass l\\\"angere aber daf\\\"ur sicherere Wege mit","Hilfe von Gewichten berechnet werden k\\\"onnen.","Diese theoretischen Ergebnisse werden in der Praxis angewendet und es werden","auf Grundlage von Daten von OpenStreetMaps sichere Fahrradwege in Stuttgart berechnet.","Dabei zeigt sich, dass eine gute Wahl der Gewichte zu weniger","L\\\"osungen und k\\\"urzeren Rechenzeiten f\\\"uhrt."],"url":"http://arxiv.org/abs/2403.18363v1","category":"math.OC"}
{"created":"2024-03-27 08:31:09","title":"The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations","abstract":"The metric distortion of a randomized social choice function (RSCF) quantifies its worst-case approximation ratio of the optimal social cost when the voters' costs for alternatives are given by distances in a metric space. This notion has recently attracted significant attention as numerous RSCFs that aim to minimize the metric distortion have been suggested. However, such tailored voting rules usually have little appeal other than their low metric distortion. In this paper, we will thus study the metric distortion of well-established RSCFs. In more detail, we first show that C1 maximal lottery rules, a well-known class of RSCFs, have a metric distortion of $4$ and furthermore prove that this is optimal within the class of majoritarian RSCFs (which only depend on the majority relation). As our second contribution, we perform extensive computer experiments on the metric distortion of established RSCFs to obtain insights into their average-case performance. These computer experiments are based on a new linear program for computing the metric distortion of a lottery on a given profile and reveal that some classical RSCFs perform almost as well as the currently best known RSCF with respect to the metric distortion on randomly sampled profiles.","sentences":["The metric distortion of a randomized social choice function (RSCF) quantifies its worst-case approximation ratio of the optimal social cost when the voters' costs for alternatives are given by distances in a metric space.","This notion has recently attracted significant attention as numerous RSCFs that aim to minimize the metric distortion have been suggested.","However, such tailored voting rules usually have little appeal other than their low metric distortion.","In this paper, we will thus study the metric distortion of well-established RSCFs.","In more detail, we first show that C1 maximal lottery rules, a well-known class of RSCFs, have a metric distortion of $4$ and furthermore prove that this is optimal within the class of majoritarian RSCFs (which only depend on the majority relation).","As our second contribution, we perform extensive computer experiments on the metric distortion of established RSCFs to obtain insights into their average-case performance.","These computer experiments are based on a new linear program for computing the metric distortion of a lottery on a given profile and reveal that some classical RSCFs perform almost as well as the currently best known RSCF with respect to the metric distortion on randomly sampled profiles."],"url":"http://arxiv.org/abs/2403.18340v1","category":"cs.GT"}
{"created":"2024-03-27 07:24:11","title":"UAV Corridor Coverage Analysis with Base Station Antenna Uptilt and Strongest Signal Association","abstract":"Unmanned aerial vehicle (UAV) corridors are sky lanes where UAVs fly through safely between their origin and destination. To ensure the successful operation of UAV corridors, beyond visual line of sight (BVLOS) wireless connectivity within the corridor is crucial. One promising solution to support this is the use of cellular-connected UAV (C-UAV) networks, which offer long-range and seamless wireless coverage. However, conventional terrestrial base stations (BSs) that typically employ down-tilted sector antennas to serve ground users are not ideally suited to serve the aerial vehicles positioned above the BSs. In our previous work, we focused on studying the optimal uptilt angle of BS antennas to maximize the wireless coverage probability in UAV corridors. However, the association of BSs with UAVs was restricted to the nearest BS association, which limits the potential coverage benefits. In this paper, we address this limitation by considering the strongest BS signal association in UAV corridors, which enables enhanced coverage within the corridor compared to the nearest BS association. The strongest BS association allows UAVs to connect with the second nearest BSs while also accounting for interference from the third nearest BSs. Closed-form expression analysis and simulation results show that the strongest BSs association in UAV corridors yields a superior coverage probability when compared to the nearest BS association.","sentences":["Unmanned aerial vehicle (UAV) corridors are sky lanes where UAVs fly through safely between their origin and destination.","To ensure the successful operation of UAV corridors, beyond visual line of sight (BVLOS) wireless connectivity within the corridor is crucial.","One promising solution to support this is the use of cellular-connected UAV (C-UAV) networks, which offer long-range and seamless wireless coverage.","However, conventional terrestrial base stations (BSs) that typically employ down-tilted sector antennas to serve ground users are not ideally suited to serve the aerial vehicles positioned above the BSs.","In our previous work, we focused on studying the optimal uptilt angle of BS antennas to maximize the wireless coverage probability in UAV corridors.","However, the association of BSs with UAVs was restricted to the nearest BS association, which limits the potential coverage benefits.","In this paper, we address this limitation by considering the strongest BS signal association in UAV corridors, which enables enhanced coverage within the corridor compared to the nearest BS association.","The strongest BS association allows UAVs to connect with the second nearest BSs while also accounting for interference from the third nearest BSs.","Closed-form expression analysis and simulation results show that the strongest BSs association in UAV corridors yields a superior coverage probability when compared to the nearest BS association."],"url":"http://arxiv.org/abs/2403.18311v1","category":"eess.SP"}
{"created":"2024-03-27 03:50:14","title":"An Execution-time-certified QP Algorithm for $\\ell_1$ penalty-based Soft-constrained MPC","abstract":"Providing an execution time certificate and handling possible infeasibility in closed-loop are two pressing requirements of Model Predictive Control (MPC). To simultaneously meet these two requirements, this paper uses $\\ell_1$ penalty-based soft-constrained MPC formulation and innovatively transforms the resulting non-smooth QP into a box-constrained QP, which is solved by our previously proposed direct and execution-time certified algorithm with only dimension-dependent (data-independent) and exact number of iterations [1]. This approach not only overcomes the limitation of our previously proposed algorithm [1], only applicable to input-constrained MPC, but also enjoys exact recovery feature (exactly recover the same solution when the original problem is feasible) of $\\ell_1$ penalty-based soft-constrained MPC formulation without suffering numerical difficulty of the resulting non-smoothness. Other various real-time QP applications, not limited to MPC, will also benefit from our QP algorithm with execution-time certificate and global feasibility.","sentences":["Providing an execution time certificate and handling possible infeasibility in closed-loop are two pressing requirements of Model Predictive Control (MPC).","To simultaneously meet these two requirements, this paper uses $\\ell_1$ penalty-based soft-constrained MPC formulation and innovatively transforms the resulting non-smooth QP into a box-constrained QP, which is solved by our previously proposed direct and execution-time certified algorithm with only dimension-dependent (data-independent) and exact number of iterations [1].","This approach not only overcomes the limitation of our previously proposed algorithm","[1], only applicable to input-constrained MPC, but also enjoys exact recovery feature (exactly recover the same solution when the original problem is feasible) of $\\ell_1$ penalty-based soft-constrained MPC formulation without suffering numerical difficulty of the resulting non-smoothness.","Other various real-time QP applications, not limited to MPC, will also benefit from our QP algorithm with execution-time certificate and global feasibility."],"url":"http://arxiv.org/abs/2403.18235v1","category":"eess.SY"}
{"created":"2024-03-27 02:51:35","title":"Long-Term Mine Planning with Large Neighbourhood Search","abstract":"We present a Large Neighbourhood Search based approach for solving complex long-term open-pit mine planning problems. An initial feasible solution, generated by a sliding windows heuristic, is improved through repeated solves of a restricted mixed integer program. Each iteration leaves only a subset of the variables in our planning model free to take on new values. We form these subsets through the use of a novel path-based neighbourhood structure, and neighbourhood formation strategies that exploit the structure of the planning model. We show that our method is able to find near-optimal solutions to problems that cannot be solved by an off-the-shelf solver in a reasonable time, or with reasonable computational resources.","sentences":["We present a Large Neighbourhood Search based approach for solving complex long-term open-pit mine planning problems.","An initial feasible solution, generated by a sliding windows heuristic, is improved through repeated solves of a restricted mixed integer program.","Each iteration leaves only a subset of the variables in our planning model free to take on new values.","We form these subsets through the use of a novel path-based neighbourhood structure, and neighbourhood formation strategies that exploit the structure of the planning model.","We show that our method is able to find near-optimal solutions to problems that cannot be solved by an off-the-shelf solver in a reasonable time, or with reasonable computational resources."],"url":"http://arxiv.org/abs/2403.18213v1","category":"math.OC"}
{"created":"2024-03-27 02:21:20","title":"Fault-tolerant properties of scale-free linear protocols for synchronization of homogeneous multi-agent systems","abstract":"Originally, protocols were designed for multi-agent systems (MAS) using information about the network. However, in many cases there is no or only limited information available about the network. Recently, there has been a focus on scale-free synchronization of multi-agent systems (MAS). In this case, the protocol is designed without any prior information about the network. As long as the network contains a directed spanning tree, the scale-free protocol guarantees that the network achieves synchronization.   If there is no directed spanning tree for the network then synchronization cannot be achieved. But what happens when these scale-free protocols are applied to such a network where the directed spanning tree no longer exists? The latter might arise if, for instance, a fault occurs in one of more crucial links. This paper establishes that the network decomposes into a number of basic bicomponents which achieves synchronization among all nodes in this basic bicomponent. On the other hand, nodes which are not part of any basic bicomponent converge to a weighted average of the synchronized trajectories of the basic bicomponents. The weights are independent of the initial conditions and are independent of the designed protocol.","sentences":["Originally, protocols were designed for multi-agent systems (MAS) using information about the network.","However, in many cases there is no or only limited information available about the network.","Recently, there has been a focus on scale-free synchronization of multi-agent systems (MAS).","In this case, the protocol is designed without any prior information about the network.","As long as the network contains a directed spanning tree, the scale-free protocol guarantees that the network achieves synchronization.   ","If there is no directed spanning tree for the network then synchronization cannot be achieved.","But what happens when these scale-free protocols are applied to such a network where the directed spanning tree no longer exists?","The latter might arise if, for instance, a fault occurs in one of more crucial links.","This paper establishes that the network decomposes into a number of basic bicomponents which achieves synchronization among all nodes in this basic bicomponent.","On the other hand, nodes which are not part of any basic bicomponent converge to a weighted average of the synchronized trajectories of the basic bicomponents.","The weights are independent of the initial conditions and are independent of the designed protocol."],"url":"http://arxiv.org/abs/2403.18200v1","category":"eess.SY"}
{"created":"2024-03-27 01:55:57","title":"The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs","abstract":"The increasing polarisation in our societies is a major international concern. Current approaches to defining and detecting polarisation largely rely on finding evidence of bimodality in social networks or voter opinion surveys. It is difficult to detect temporal trends in polarisation, as the results usually fall into a binary of polarised or non-polarised, which cannot robustly show that subsequent increases in bimodality are statistically significant.   Our work is aligned with Baldassari and Gelman's theory that polarisation should be defined as increasing correlation between positions in the ideological field. We also draw from post-structuralist work which argues that polarisation is the process of both the ideological and material layers of society being segregated into two poles, as in cases of apartheid. Thus, in order to measure the polarisation in a society, it would be beneficial to be able to assess social networks directly.   In this paper we use Random Dot Product Graphs to embed social networks in metric spaces. In the case of a social network, the embedded dimensionality corresponds to the number of reasons any two people may form a social connection. A decrease in the optimal dimensionality for the embedding of the network graph, as measured using truncated Singular Value Decomposition of the graph adjacency matrix, indicates increasing polarisation in the network.   We apply this method to two different Twitter networks based on discussions of climate change, and show that our methods agree with other researchers' detection of polarisation in this space. We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities in a network, or the increase in the predominance of one community over the other, are identifiable as polarisation processes.","sentences":["The increasing polarisation in our societies is a major international concern.","Current approaches to defining and detecting polarisation largely rely on finding evidence of bimodality in social networks or voter opinion surveys.","It is difficult to detect temporal trends in polarisation, as the results usually fall into a binary of polarised or non-polarised, which cannot robustly show that subsequent increases in bimodality are statistically significant.   ","Our work is aligned with Baldassari and Gelman's theory that polarisation should be defined as increasing correlation between positions in the ideological field.","We also draw from post-structuralist work which argues that polarisation is the process of both the ideological and material layers of society being segregated into two poles, as in cases of apartheid.","Thus, in order to measure the polarisation in a society, it would be beneficial to be able to assess social networks directly.   ","In this paper we use Random Dot Product Graphs to embed social networks in metric spaces.","In the case of a social network, the embedded dimensionality corresponds to the number of reasons any two people may form a social connection.","A decrease in the optimal dimensionality for the embedding of the network graph, as measured using truncated Singular Value Decomposition of the graph adjacency matrix, indicates increasing polarisation in the network.   ","We apply this method to two different Twitter networks based on discussions of climate change, and show that our methods agree with other researchers' detection of polarisation in this space.","We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities in a network, or the increase in the predominance of one community over the other, are identifiable as polarisation processes."],"url":"http://arxiv.org/abs/2403.18191v1","category":"cs.SI"}
{"created":"2024-03-27 01:24:26","title":"Topology Optimization for the Full-Cell Design of Porous Electrodes in Electrochemical Energy Storage Devices","abstract":"In this paper, we introduce a density-based topology optimization framework to design porous electrodes for maximum energy storage. We simulate the full cell with a model that incorporates electronic potential, ionic potential, and electrolyte concentration. The system consists of three materials, namely pure liquid electrolyte and the porous solids of the anode and cathode, for which we determine the optimal placement. We use separate electronic potentials to model each electrode, which allow interdigitated designs. As the result, a penalization is required to ensure that the anode and cathode do not touch, i.e. causing a short circuit. We compare multiple 2D designs generated for different fixed conditions, e.g. material properties. A 3D design with complex channel and interlocking structure is also created. All optimized designs are far superior to the traditional monolithic electrode design with respect to energy storage metrics. We observe up to 750% increase in energy storage for cases with slow effective ionic diffusion within the porous electrode.","sentences":["In this paper, we introduce a density-based topology optimization framework to design porous electrodes for maximum energy storage.","We simulate the full cell with a model that incorporates electronic potential, ionic potential, and electrolyte concentration.","The system consists of three materials, namely pure liquid electrolyte and the porous solids of the anode and cathode, for which we determine the optimal placement.","We use separate electronic potentials to model each electrode, which allow interdigitated designs.","As the result, a penalization is required to ensure that the anode and cathode do not touch, i.e. causing a short circuit.","We compare multiple 2D designs generated for different fixed conditions, e.g. material properties.","A 3D design with complex channel and interlocking structure is also created.","All optimized designs are far superior to the traditional monolithic electrode design with respect to energy storage metrics.","We observe up to 750% increase in energy storage for cases with slow effective ionic diffusion within the porous electrode."],"url":"http://arxiv.org/abs/2403.18184v1","category":"physics.app-ph"}
{"created":"2024-03-27 00:21:49","title":"Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach","abstract":"The rise of advanced air mobility (AAM) is expected to become a multibillion-dollar industry in the near future. Market-based mechanisms are touted to be an integral part of AAM operations, which comprise heterogeneous operators with private valuations. In this work, we study the problem of designing a mechanism to coordinate the movement of electric vertical take-off and landing (eVTOL) aircraft, operated by multiple operators each having heterogeneous valuations associated with their fleet, between vertiports, while enforcing the arrival, departure, and parking constraints at vertiports. Particularly, we propose an incentive-compatible and individually rational vertiport reservation mechanism that maximizes a social welfare metric, which encapsulates the objective of maximizing the overall valuations of all operators while minimizing the congestion at vertiports. Additionally, we improve the computational tractability of designing the reservation mechanism by proposing a mixed binary linear programming approach that is based on constructing network flow graph corresponding to the underlying problem.","sentences":["The rise of advanced air mobility (AAM) is expected to become a multibillion-dollar industry in the near future.","Market-based mechanisms are touted to be an integral part of AAM operations, which comprise heterogeneous operators with private valuations.","In this work, we study the problem of designing a mechanism to coordinate the movement of electric vertical take-off and landing (eVTOL) aircraft, operated by multiple operators each having heterogeneous valuations associated with their fleet, between vertiports, while enforcing the arrival, departure, and parking constraints at vertiports.","Particularly, we propose an incentive-compatible and individually rational vertiport reservation mechanism that maximizes a social welfare metric, which encapsulates the objective of maximizing the overall valuations of all operators while minimizing the congestion at vertiports.","Additionally, we improve the computational tractability of designing the reservation mechanism by proposing a mixed binary linear programming approach that is based on constructing network flow graph corresponding to the underlying problem."],"url":"http://arxiv.org/abs/2403.18166v1","category":"eess.SY"}
{"created":"2024-03-26 23:47:05","title":"Helical Ribbons: Simple Chiral Sedimentation","abstract":"We investigate the sedimentation of chiral particles in viscous fluid flow. We identify helical ribbons as simple particles with strong translation-rotation coupling whose symmetry ensures that the centers of mass, buoyancy, resistance, and mobility coincide. Experimental measurements of both relevant mobility tensors show excellent agreement with simulations of ribbons made of interacting spheres. We observe quasi-periodic angular dynamics causing complex spatial trajectories. In tilt-spin phase space, orbits are closed due to time-reversal and reflection symmetry. Changing the helical ribbon length reveals a bifurcation at which the stable sedimentation orientations switch.","sentences":["We investigate the sedimentation of chiral particles in viscous fluid flow.","We identify helical ribbons as simple particles with strong translation-rotation coupling whose symmetry ensures that the centers of mass, buoyancy, resistance, and mobility coincide.","Experimental measurements of both relevant mobility tensors show excellent agreement with simulations of ribbons made of interacting spheres.","We observe quasi-periodic angular dynamics causing complex spatial trajectories.","In tilt-spin phase space, orbits are closed due to time-reversal and reflection symmetry.","Changing the helical ribbon length reveals a bifurcation at which the stable sedimentation orientations switch."],"url":"http://arxiv.org/abs/2403.18157v1","category":"cond-mat.soft"}
{"created":"2024-03-26 20:22:16","title":"The Flying Sidekick Traveling Salesman Problem with Multiple Drops: A Simple and Effective Heuristic Approach","abstract":"We study the Flying Sidekick Traveling Salesman Problem with Multiple Drops (FSTSP-MD), a multi-modal last-mile delivery model where a single truck and a single drone cooperatively deliver customer packages. In the FSTSP-MD, the drone can be launched from the truck to deliver multiple packages before it returns to the truck for a new delivery operation. The FSTSP-MD aims to find the synchronized truck and drone delivery routes that minimize the completion time of the delivery process. We develop a simple and effective heuristic approach based on an order-first, split-second scheme. This heuristic combines standard local search and diversification techniques with a novel shortest-path problem that finds FSTSP-MD solutions in polynomial time. We show that our heuristic consistently outperforms state-of-the-art heuristics developed for the FSTSP-MD and the FSTSP (i.e., the single-drop case) through extensive numerical experiments. We also show that the FSTSP-MD substantially reduces completion times compared to a traditional truck-only delivery system. Several managerial insights are described regarding the effects of drone capacity, drone speed, drone flight endurance, and customer distribution.","sentences":["We study the Flying Sidekick Traveling Salesman Problem with Multiple Drops (FSTSP-MD), a multi-modal last-mile delivery model where a single truck and a single drone cooperatively deliver customer packages.","In the FSTSP-MD, the drone can be launched from the truck to deliver multiple packages before it returns to the truck for a new delivery operation.","The FSTSP-MD aims to find the synchronized truck and drone delivery routes that minimize the completion time of the delivery process.","We develop a simple and effective heuristic approach based on an order-first, split-second scheme.","This heuristic combines standard local search and diversification techniques with a novel shortest-path problem that finds FSTSP-MD solutions in polynomial time.","We show that our heuristic consistently outperforms state-of-the-art heuristics developed for the FSTSP-MD and the FSTSP (i.e., the single-drop case) through extensive numerical experiments.","We also show that the FSTSP-MD substantially reduces completion times compared to a traditional truck-only delivery system.","Several managerial insights are described regarding the effects of drone capacity, drone speed, drone flight endurance, and customer distribution."],"url":"http://arxiv.org/abs/2403.18091v1","category":"math.OC"}
{"created":"2024-03-26 20:15:30","title":"ANOCA: AC Network-aware Optimal Curtailment Approach for Dynamic Hosting Capacity","abstract":"With exponential growth in distributed energy resources (DERs) coupled with at-capacity distribution grid infrastructure, prosumers cannot always export all extra power to the grid without violating technical limits. Consequently, a slew of dynamic hosting capacity (DHC) algorithms have emerged for optimal utilization of grid infrastructure while maximizing export from DERs. Most of these DHC algorithms utilize the concept of operating envelopes (OE)}, where the utility gives prosumers technical power export limits, and they are free to export power within these limits. Recent studies have shown that OE-based frameworks have drawbacks, as most develop power export limits based on convex or linear grid models. As OEs must capture extreme operating conditions, both convex and linear models can violate technical limits in practice because they approximate grid physics. However, AC models are unsuitable because they may not be feasible within the whole region of OE. We propose a new two-stage optimization framework for DHC built on three-phase AC models to address the current gaps. In this approach, the prosumers first run a receding horizon multi-period optimization to identify optimal export power setpoints to communicate with the utility. The utility then performs an infeasibility-based optimization to either accept the prosumer's request or dispatch an optimal curtail signal such that overall system technical constraints are not violated. To explore various curtailment strategies, we develop an L1, L2, and Linf norm-based dispatch algorithm with an exact three-phase AC model. We test our framework on a 1420 three-phase node meshed distribution network and show that the proposed algorithm optimally curtails DERs while guaranteeing the AC feasibility of the network.","sentences":["With exponential growth in distributed energy resources (DERs) coupled with at-capacity distribution grid infrastructure, prosumers cannot always export all extra power to the grid without violating technical limits.","Consequently, a slew of dynamic hosting capacity (DHC) algorithms have emerged for optimal utilization of grid infrastructure while maximizing export from DERs.","Most of these DHC algorithms utilize the concept of operating envelopes (OE)}, where the utility gives prosumers technical power export limits, and they are free to export power within these limits.","Recent studies have shown that OE-based frameworks have drawbacks, as most develop power export limits based on convex or linear grid models.","As OEs must capture extreme operating conditions, both convex and linear models can violate technical limits in practice because they approximate grid physics.","However, AC models are unsuitable because they may not be feasible within the whole region of OE.","We propose a new two-stage optimization framework for DHC built on three-phase AC models to address the current gaps.","In this approach, the prosumers first run a receding horizon multi-period optimization to identify optimal export power setpoints to communicate with the utility.","The utility then performs an infeasibility-based optimization to either accept the prosumer's request or dispatch an optimal curtail signal such that overall system technical constraints are not violated.","To explore various curtailment strategies, we develop an L1, L2, and Linf norm-based dispatch algorithm with an exact three-phase AC model.","We test our framework on a 1420 three-phase node meshed distribution network and show that the proposed algorithm optimally curtails DERs while guaranteeing the AC feasibility of the network."],"url":"http://arxiv.org/abs/2403.18085v1","category":"eess.SY"}
{"created":"2024-03-26 19:35:22","title":"Path Integral Control with Rollout Clustering and Dynamic Obstacles","abstract":"Model Predictive Path Integral (MPPI) control has proven to be a powerful tool for the control of uncertain systems (such as systems subject to disturbances and systems with unmodeled dynamics). One important limitation of the baseline MPPI algorithm is that it does not utilize simulated trajectories to their fullest extent. For one, it assumes that the average of all trajectories weighted by their performance index will be a safe trajectory. In this paper, multiple examples are shown where the previous assumption does not hold, and a trajectory clustering technique is presented that reduces the chances of the weighted average crossing in an unsafe region. Secondly, MPPI does not account for dynamic obstacles, so the authors put forward a novel cost function that accounts for dynamic obstacles without adding significant computation time to the overall algorithm. The novel contributions proposed in this paper were evaluated with extensive simulations to demonstrate improvements upon the state-of-the-art MPPI techniques.","sentences":["Model Predictive Path Integral (MPPI) control has proven to be a powerful tool for the control of uncertain systems (such as systems subject to disturbances and systems with unmodeled dynamics).","One important limitation of the baseline MPPI algorithm is that it does not utilize simulated trajectories to their fullest extent.","For one, it assumes that the average of all trajectories weighted by their performance index will be a safe trajectory.","In this paper, multiple examples are shown where the previous assumption does not hold, and a trajectory clustering technique is presented that reduces the chances of the weighted average crossing in an unsafe region.","Secondly, MPPI does not account for dynamic obstacles, so the authors put forward a novel cost function that accounts for dynamic obstacles without adding significant computation time to the overall algorithm.","The novel contributions proposed in this paper were evaluated with extensive simulations to demonstrate improvements upon the state-of-the-art MPPI techniques."],"url":"http://arxiv.org/abs/2403.18066v1","category":"eess.SY"}
{"created":"2024-03-26 19:25:32","title":"The Cordiality Game and the Game Cordiality Number","abstract":"The cordiality game is played on a graph $G$ by two players, Admirable (A) and Impish (I), who take turns selecting \\track{unlabeled} vertices of $G$. Admirable labels the selected vertices by $0$ and Impish by $1$, and the resulting label on any edge is the sum modulo $2$ of the labels of the vertices incident to that edge. The two players have opposite goals: Admirable attempts to minimize the number of edges with different labels as much as possible while Impish attempts to maximize this number. When both Admirable and Impish play their optimal games, we define the \\emph{game cordiality number}, $c_g(G)$, as the absolute difference between the number of edges labeled zero and one. Let $P_n$ be the path on $n$ vertices. We show $c_g(P_n)\\le \\frac{n-3}{3}$ when $n \\equiv 0 \\pmod 3$, $c_g(P_n)\\le \\frac{n-1}{3}$ when $n \\equiv 1 \\pmod 3$, and $c_g(P_n)\\le \\frac{n+1}{3}$ when $n \\equiv 2\\pmod 3$. Furthermore, we show a similar bound, $c_g(T) \\leq \\frac{|T|}{2}$ holds for any tree $T$.","sentences":["The cordiality game is played on a graph $G$ by two players, Admirable (A) and Impish (I), who take turns selecting \\track{unlabeled} vertices of $G$. Admirable labels the selected vertices by $0$ and Impish by $1$, and the resulting label on any edge is the sum modulo $2$ of the labels of the vertices incident to that edge.","The two players have opposite goals: Admirable attempts to minimize the number of edges with different labels as much as possible while Impish attempts to maximize this number.","When both Admirable and Impish play their optimal games, we define the \\emph{game cordiality number}, $c_g(G)$, as the absolute difference between the number of edges labeled zero and one.","Let $P_n$ be the path on $n$ vertices.","We show $c_g(P_n)\\le \\frac{n-3}{3}$ when $n \\equiv 0","\\pmod 3$, $c_g(P_n)\\le \\frac{n-1}{3}$","when $n \\equiv 1 \\pmod 3$, and $c_g(P_n)\\le \\frac{n+1}{3}$ when $n \\equiv 2\\pmod 3$.","Furthermore, we show a similar bound, $c_g(T) \\leq \\frac{|T|}{2}$ holds for any tree $T$."],"url":"http://arxiv.org/abs/2403.18060v1","category":"math.CO"}
{"created":"2024-03-26 16:38:13","title":"A Mixed-Integer Linear Program to create the shifts in a supermarket","abstract":"The shift design and the personnel scheduling problem is known to be a difficult problem. It is a real-world problem which has lots of applications in the organization of companies. Solutions are usually found by dividing the problem in two steps: first the shifts are created, then the employees are assigned to them by respecting a bunch of constraints. The assignment of different tasks increases the complexity, since we have to consider the skills of the single employee necessary to perform any activity. In this paper we present a mixed-integer linear programming formulation which models together the shift creation and the construction of rosters for employees, with the objective of minimizing the amount of uncovered demand. Finally we provide the results for three real-world instances, confirming that this approach is promising.","sentences":["The shift design and the personnel scheduling problem is known to be a difficult problem.","It is a real-world problem which has lots of applications in the organization of companies.","Solutions are usually found by dividing the problem in two steps: first the shifts are created, then the employees are assigned to them by respecting a bunch of constraints.","The assignment of different tasks increases the complexity, since we have to consider the skills of the single employee necessary to perform any activity.","In this paper we present a mixed-integer linear programming formulation which models together the shift creation and the construction of rosters for employees, with the objective of minimizing the amount of uncovered demand.","Finally we provide the results for three real-world instances, confirming that this approach is promising."],"url":"http://arxiv.org/abs/2403.17850v2","category":"math.OC"}
{"created":"2024-03-26 13:50:39","title":"Solution for Point Tracking Task of ICCV 1st Perception Test Challenge 2023","abstract":"This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video. Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction. To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera. To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot. (2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object. Our approach ranked first in the final test with a score of 0.46.","sentences":["This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video.","Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction.","To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera.","To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot.","(2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object.","Our approach ranked first in the final test with a score of 0.46."],"url":"http://arxiv.org/abs/2403.17994v1","category":"cs.CV"}
{"created":"2024-03-27 17:34:05","title":"Constraining primordial non-Gaussianity from the large scale structure two-point and three-point correlation functions","abstract":"Surveys of cosmological large-scale structure (LSS) are sensitive to the presence of local primordial non-Gaussianity (PNG), and may be used to constrain models of inflation. Local PNG, characterized by fNL, the amplitude of the quadratic correction to the potential of a Gaussian random field, is traditionally measured from LSS two-point and three-point clustering via the power spectrum and bi-spectrum. We propose a framework to measure fNL using the configuration space two-point correlation function (2pcf) monopole and three-point correlation function (3pcf) monopole of survey tracers. Our model estimates the effect of the scale-dependent bias induced by the presence of PNG on the 2pcf and 3pcf from the clustering of simulated dark matter halos. We describe how this effect may be scaled to an arbitrary tracer of the cosmological matter density. The 2pcf and 3pcf of this tracer are measured to constrain the value of fNL. Using simulations of luminous red galaxies observed by the Dark Energy Spectroscopic Instrument (DESI), we demonstrate the accuracy and constraining power of our model, and forecast the ability to constrainfNL to a precision of sigma(fNL) = 22 with one year of DESI survey data.","sentences":["Surveys of cosmological large-scale structure (LSS) are sensitive to the presence of local primordial non-Gaussianity (PNG), and may be used to constrain models of inflation.","Local PNG, characterized by fNL, the amplitude of the quadratic correction to the potential of a Gaussian random field, is traditionally measured from LSS two-point and three-point clustering via the power spectrum and bi-spectrum.","We propose a framework to measure fNL using the configuration space two-point correlation function (2pcf) monopole and three-point correlation function (3pcf) monopole of survey tracers.","Our model estimates the effect of the scale-dependent bias induced by the presence of PNG on the 2pcf and 3pcf from the clustering of simulated dark matter halos.","We describe how this effect may be scaled to an arbitrary tracer of the cosmological matter density.","The 2pcf and 3pcf of this tracer are measured to constrain the value of fNL.","Using simulations of luminous red galaxies observed by the Dark Energy Spectroscopic Instrument (DESI), we demonstrate the accuracy and constraining power of our model, and forecast the ability to constrainfNL to a precision of sigma(fNL)","= 22 with one year of DESI survey data."],"url":"http://arxiv.org/abs/2403.18789v1","category":"astro-ph.CO"}
{"created":"2024-03-27 16:51:00","title":"Identifying the electromagnetic counterparts of LISA massive black hole binaries in archival LSST data","abstract":"The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will catalogue the light-curves of up to 100 million quasars. Among these there can be up to approximately 100 ultra-compact massive black hole (MBH) binaries, which 5-15 years later can be detected in gravitational waves (GWs) by the Laser Interferometer Space Antenna (LISA). Here we assume that GWs from a MBH binary have been detected by LISA, and we assess whether or not its electromagnetic (EM) counterpart can be uniquely identified in archival LSST data as a periodic quasar. We use the binary's properties derived from the LISA waveform, such as the past evolution of its orbital frequency, its total mass, distance and sky localization, to predict the redshift, magnitude and historical periodicity of the quasar expected in the archival LSST data. We then use Monte Carlo simulations to compute the false alarm probability, i.e. the number of quasars in the LSST catalogue matching these properties by chance, based on the (extrapolated) quasar luminosity function, the sampling cadence of LSST, and intrinsic ``damped random walk (DRW)\" quasar variability. We perform our analysis on four fiducial LISA binaries, with total masses and redshifts of $(M_{\\rm bin}/{\\rm M_{\\odot}},z) = (3\\times10^5,0.3)$, $(3\\times10^6,0.3)$, $(10^7,0.3)$ and $(10^7,1)$. While DRW noise and aliasing due to LSST's cadence can produce false periodicities by chance, we find that the frequency chirp of the LISA source during the LSST observations washes out these noise peaks and allows the genuine source to stand out in Lomb-Scargle periodograms. We find that all four fiducial binaries yield excellent chances to be uniquely identified, with false alarm probabilities below $10^{-5}$, a week or more before their merger. This then enables deep follow-up EM observations targeting the individual EM counterparts during their inspiral stage.","sentences":["The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will catalogue the light-curves of up to 100 million quasars.","Among these there can be up to approximately 100 ultra-compact massive black hole (MBH) binaries, which 5-15 years later can be detected in gravitational waves (GWs) by the Laser Interferometer Space Antenna (LISA).","Here we assume that GWs from a MBH binary have been detected by LISA, and we assess whether or not its electromagnetic (EM) counterpart can be uniquely identified in archival LSST data as a periodic quasar.","We use the binary's properties derived from the LISA waveform, such as the past evolution of its orbital frequency, its total mass, distance and sky localization, to predict the redshift, magnitude and historical periodicity of the quasar expected in the archival LSST data.","We then use Monte Carlo simulations to compute the false alarm probability, i.e. the number of quasars in the LSST catalogue matching these properties by chance, based on the (extrapolated) quasar luminosity function, the sampling cadence of LSST, and intrinsic ``damped random walk (DRW)\" quasar variability.","We perform our analysis on four fiducial LISA binaries, with total masses and redshifts of $(M_{\\rm bin}/{\\rm M_{\\odot}},z) = (3\\times10^5,0.3)$, $(3\\times10^6,0.3)$, $(10^7,0.3)$ and $(10^7,1)$.","While DRW noise and aliasing due to LSST's cadence can produce false periodicities by chance, we find that the frequency chirp of the LISA source during the LSST observations washes out these noise peaks and allows the genuine source to stand out in Lomb-Scargle periodograms.","We find that all four fiducial binaries yield excellent chances to be uniquely identified, with false alarm probabilities below $10^{-5}$, a week or more before their merger.","This then enables deep follow-up EM observations targeting the individual EM counterparts during their inspiral stage."],"url":"http://arxiv.org/abs/2403.18751v1","category":"astro-ph.HE"}
{"created":"2024-03-27 16:35:11","title":"Perturbative $T$-odd asymmetries in the Drell-Yan process revisited","abstract":"We calculate the perturbative $T$-odd contributions to the lepton angular distribution in the Drell-Yan process. Using collinear factorization, we work at the first order in QCD perturbation theory where these contributions appear, ${\\cal O}(\\alpha_s^2)$, and address both $W^\\pm$ and $\\gamma/Z^0$ boson exchange. A major focus of our calculation is on the regime where the boson's transverse momentum $Q_T$ is much smaller than its mass $Q$. We carefully expand our results up to next-to-next-to-leading power in $Q_T/Q$. Our calculation provides a benchmark for studies of $T$-odd contributions that employ transverse-momentum dependent parton distribution functions. In the neutral-current case we compare our results for the $T$-odd structure functions to available ATLAS data.","sentences":["We calculate the perturbative $T$-odd contributions to the lepton angular distribution in the Drell-Yan process.","Using collinear factorization, we work at the first order in QCD perturbation theory where these contributions appear, ${\\cal O}(\\alpha_s^2)$, and address both $W^\\pm$ and $\\gamma/Z^0$ boson exchange.","A major focus of our calculation is on the regime where the boson's transverse momentum $Q_T$ is much smaller than its mass $Q$. We carefully expand our results up to next-to-next-to-leading power in $Q_T/Q$. Our calculation provides a benchmark for studies of $T$-odd contributions that employ transverse-momentum dependent parton distribution functions.","In the neutral-current case we compare our results for the $T$-odd structure functions to available ATLAS data."],"url":"http://arxiv.org/abs/2403.18741v1","category":"hep-ph"}
{"created":"2024-03-27 16:18:01","title":"Utility of ocean wave parameters for improving predictions of ambient noise","abstract":"This study is concerned with prediction of the \"wind noise\" component of ambient noise (AN) in the ocean. It builds on the seminal paper by Felizardo and Melville (1995), in which the authors quantified the correlation between AN and individual wind/wave parameters. Acoustic data are obtained from hydrophones deployed in the north and northeast Pacific Ocean, and wind/wave parameters are obtained from moored buoys and numerical models. We describe a procedure developed for this study which isolates the correlation of AN with wave parameters, independent of mutual correlation with wind speed (residual correlation). We then describe paired calibration/prediction experiments, whereby multiple wind/wave parameters are used simultaneously to estimate AN. We find that the improvement from inclusion of wave parameters is robust but modest. We interpret the latter outcome as suggesting that wave breaking responds to changes in local winds quickly, relative to, for example, total wave energy, which develops more slowly. This outcome is consistent with prior knowledge of the physics of wave breaking, e.g. Babanin (2011). We discuss this in context of the time/space response of various wave parameters to wind forcing.","sentences":["This study is concerned with prediction of the \"wind noise\" component of ambient noise (AN) in the ocean.","It builds on the seminal paper by Felizardo and Melville (1995), in which the authors quantified the correlation between AN and individual wind/wave parameters.","Acoustic data are obtained from hydrophones deployed in the north and northeast Pacific Ocean, and wind/wave parameters are obtained from moored buoys and numerical models.","We describe a procedure developed for this study which isolates the correlation of AN with wave parameters, independent of mutual correlation with wind speed (residual correlation).","We then describe paired calibration/prediction experiments, whereby multiple wind/wave parameters are used simultaneously to estimate AN.","We find that the improvement from inclusion of wave parameters is robust but modest.","We interpret the latter outcome as suggesting that wave breaking responds to changes in local winds quickly, relative to, for example, total wave energy, which develops more slowly.","This outcome is consistent with prior knowledge of the physics of wave breaking, e.g. Babanin (2011).","We discuss this in context of the time/space response of various wave parameters to wind forcing."],"url":"http://arxiv.org/abs/2403.18728v1","category":"physics.ao-ph"}
{"created":"2024-03-27 15:48:34","title":"Bridging hadronic and vacuum structure by heavy quarkonia","abstract":"We discuss the central and, mostly, spin-dependent potentials in heavy quarkonia $\\bar b b, \\bar c c$, with two goals in mind. The first is phenomenological: using the splitting between the 1S and 2S pairs, as well as the 1P and 2P quartet masses, we obtain very accurate values of all matrix elements of the spin-depepdent potentials. The second is theoretical: using standard wave functions, we compute these matrix elements, from perturbative, nonperturbative and ``string\" contributions. The model for nonperturbative effects is a ``dense instanton liquid model\", in which the QCD vacuum is made of (strongly color correlated) instanton-antiinstanton pairs or \"molecules\". We calcuate their effect via standard Wilson lines, with or without extra powers of gauge fields. We find that this model provides a reasonable description of all central, spin-spin and spin-orbit forces at distances $r= 0-0.7 \\, fm $, relevant for $\\bar b b $ and $\\bar c c$ quarkonia.","sentences":["We discuss the central and, mostly, spin-dependent potentials in heavy quarkonia $\\bar b b, \\bar c c$, with two goals in mind.","The first is phenomenological: using the splitting between the 1S and 2S pairs, as well as the 1P and 2P quartet masses, we obtain very accurate values of all matrix elements of the spin-depepdent potentials.","The second is theoretical: using standard wave functions, we compute these matrix elements, from perturbative, nonperturbative and ``string\" contributions.","The model for nonperturbative effects is a ``dense instanton liquid model\", in which the QCD vacuum is made of (strongly color correlated) instanton-antiinstanton pairs or \"molecules\".","We calcuate their effect via standard Wilson lines, with or without extra powers of gauge fields.","We find that this model provides a reasonable description of all central, spin-spin and spin-orbit forces at distances $r= 0-0.7 \\, fm $, relevant for $\\bar b b $ and $\\bar c c$ quarkonia."],"url":"http://arxiv.org/abs/2403.18700v1","category":"hep-ph"}
{"created":"2024-03-27 15:41:29","title":"Building defect conformal field theory from the Sachdev-Ye-Kitaev interactions","abstract":"The coupling between defects and extended critical degrees of freedom gives rise to the intriguing theory known as defect conformal field theory (CFT). In this work, we introduce a novel family of boundary and interface CFTs by coupling $N$ Majorana chains with SYK$_q$ interactions at the defect. Our analysis reveals that the interaction with $q=2$ constitutes a new marginal defect. Employing a versatile saddle point method, we compute unique entanglement characterizations, including the $g$-function and effective central charge, of the defect CFT. Furthermore, we analytically evaluate the transmission coefficient using CFT techniques. Surprisingly, the transmission coefficient deviates from the universal relation with the effective central charge across the defect at the large $N$ limit, suggesting that our defect CFT extends beyond all known examples of Gaussian defect CFT.","sentences":["The coupling between defects and extended critical degrees of freedom gives rise to the intriguing theory known as defect conformal field theory (CFT).","In this work, we introduce a novel family of boundary and interface CFTs by coupling $N$ Majorana chains with SYK$_q$ interactions at the defect.","Our analysis reveals that the interaction with $q=2$ constitutes a new marginal defect.","Employing a versatile saddle point method, we compute unique entanglement characterizations, including the $g$-function and effective central charge, of the defect CFT.","Furthermore, we analytically evaluate the transmission coefficient using CFT techniques.","Surprisingly, the transmission coefficient deviates from the universal relation with the effective central charge across the defect at the large $N$ limit, suggesting that our defect CFT extends beyond all known examples of Gaussian defect CFT."],"url":"http://arxiv.org/abs/2403.18691v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-27 14:38:53","title":"The effect of cutoff dependence on heavy quark spin symmetry partners","abstract":"Hadron spectroscopy is revealed by observing heavy resonances. Among various explanations of the internal structure of these hadronic states, hadronic molecules play a unique role. For hadronic molecules, which are associated with meson-meson or meson-baryon interactions, the $\\Lambda$ cutoff is a significant factor in determining the composite states' binding energies and overall properties. The cutoff becomes important when it comes to the location of hadronic molecules' masses because it influences the predictions. From this perspective, in the light of cutoff dependency, heavy quark spin partners of near-threshold the $\\chi_{c0}(3915)$, $\\chi_{c1}(3872)$, $P_c(4440)$, and $P_c(4457)$ resonances, which are considered as hadronic molecules, are examined.","sentences":["Hadron spectroscopy is revealed by observing heavy resonances.","Among various explanations of the internal structure of these hadronic states, hadronic molecules play a unique role.","For hadronic molecules, which are associated with meson-meson or meson-baryon interactions, the $\\Lambda$ cutoff is a significant factor in determining the composite states' binding energies and overall properties.","The cutoff becomes important when it comes to the location of hadronic molecules' masses because it influences the predictions.","From this perspective, in the light of cutoff dependency, heavy quark spin partners of near-threshold the $\\chi_{c0}(3915)$, $\\chi_{c1}(3872)$, $P_c(4440)$, and $P_c(4457)$ resonances, which are considered as hadronic molecules, are examined."],"url":"http://arxiv.org/abs/2403.18633v1","category":"hep-ph"}
{"created":"2024-03-27 14:23:31","title":"Influence of interstitial Li on the electronic properties of Li$_{x}$CsPbI$_{3}$ for photovoltaic and battery applications","abstract":"The integrated device of a perovskite solar cell with a Li-ion battery is an innovative solution for decentralized energy storage in smart electronic devices. In this study, we examine the stability of Li ions intercalated in a CsPbI$_3$ perovskite and their effect on the electronic structure of Li$_x$CsPbI$_3$ compounds using first-principles density functional theory. Our simulations demonstrate that the insertion of Li at concentrations up to $x$ = 1 into CsPbI$_3$ is energetically possible. Moreover, we identify that the distortion of the Pb-I octahedra has the strongest impact on the change in the electronic band gap. Specifically, an increase in the amount of intercalated Li causes larger structural distortions, which in turn lead to an increasing band gap as function of the Li content.","sentences":["The integrated device of a perovskite solar cell with a Li-ion battery is an innovative solution for decentralized energy storage in smart electronic devices.","In this study, we examine the stability of Li ions intercalated in a CsPbI$_3$ perovskite and their effect on the electronic structure of Li$_x$CsPbI$_3$ compounds using first-principles density functional theory.","Our simulations demonstrate that the insertion of Li at concentrations up to $x$ = 1 into CsPbI$_3$ is energetically possible.","Moreover, we identify that the distortion of the Pb-I octahedra has the strongest impact on the change in the electronic band gap.","Specifically, an increase in the amount of intercalated Li causes larger structural distortions, which in turn lead to an increasing band gap as function of the Li content."],"url":"http://arxiv.org/abs/2403.18601v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 13:57:49","title":"Logarithmic singularity in the density four-point function of two-dimensional critical percolation in the bulk","abstract":"We provide definitive proof of the logarithmic nature of the percolation conformal field theory in the bulk by showing that the four-point function of the density operator has a logarithmic divergence as two points collide and that the same divergence appears in the operator product expansion (OPE) of two density operators. The right hand side of the OPE contains two operators with the same scaling dimension, one of them multiplied by a term with a logarithmic singularity. Our method involves a probabilistic analysis of the percolation events contributing to the four-point function. It does not require algebraic considerations, nor taking the $Q \\to 1$ limit of the $Q$-state Potts model, and is amenable to a rigorous mathematical formulation. The logarithmic divergence appears as a consequence of scale invariance combined with independence.","sentences":["We provide definitive proof of the logarithmic nature of the percolation conformal field theory in the bulk by showing that the four-point function of the density operator has a logarithmic divergence as two points collide and that the same divergence appears in the operator product expansion (OPE) of two density operators.","The right hand side of the OPE contains two operators with the same scaling dimension, one of them multiplied by a term with a logarithmic singularity.","Our method involves a probabilistic analysis of the percolation events contributing to the four-point function.","It does not require algebraic considerations, nor taking the $Q \\to 1$ limit of the $Q$-state Potts model, and is amenable to a rigorous mathematical formulation.","The logarithmic divergence appears as a consequence of scale invariance combined with independence."],"url":"http://arxiv.org/abs/2403.18576v1","category":"math-ph"}
{"created":"2024-03-27 13:01:58","title":"The SZ effect with anisotropic distributions and high energy electrons","abstract":"Future observations of the Sunyaev-Zeldovich (SZ) effect promise ever improving measurements in terms of both sensitivity and angular resolution. As such, it is increasingly relevant to model `higher-order' contributions to the SZ effect. This work examines the effects of high-energy non-thermal electron distributions and those of anisotropic electron and photon distributions on the SZ signals. Analytic forms of the anisotropic scattering kernels for photons and electrons have been derived and investigated. We present a method for determining the anisotropic contributions through a spherical harmonic decomposition to arbitrary angular multipoles, and discuss the behaviour of these scattering kernels. We then carry out an exploration of various simplistic models of high energy non-thermal electron distributions, and examine their anisotropic behaviour. The kinematic SZ in the relativistic regime is studied using the kernel formulation allowing us to clarifying the role of kinematic corrections to the scattering optical depth. We finally present a release of an updated and refined version of SZpack including a new integrated Python interface and new modules for the calculation of various SZ signals, including those described in this paper.","sentences":["Future observations of the Sunyaev-Zeldovich (SZ) effect promise ever improving measurements in terms of both sensitivity and angular resolution.","As such, it is increasingly relevant to model `higher-order' contributions to the SZ effect.","This work examines the effects of high-energy non-thermal electron distributions and those of anisotropic electron and photon distributions on the SZ signals.","Analytic forms of the anisotropic scattering kernels for photons and electrons have been derived and investigated.","We present a method for determining the anisotropic contributions through a spherical harmonic decomposition to arbitrary angular multipoles, and discuss the behaviour of these scattering kernels.","We then carry out an exploration of various simplistic models of high energy non-thermal electron distributions, and examine their anisotropic behaviour.","The kinematic SZ in the relativistic regime is studied using the kernel formulation allowing us to clarifying the role of kinematic corrections to the scattering optical depth.","We finally present a release of an updated and refined version of SZpack including a new integrated Python interface and new modules for the calculation of various SZ signals, including those described in this paper."],"url":"http://arxiv.org/abs/2403.18530v1","category":"astro-ph.HE"}
{"created":"2024-03-27 12:37:04","title":"CI-sequences and almost complete intersections","abstract":"We study the Hilbert function and the graded Betti numbers of almost complete intersection artinian algebras. We show that that every Hilbert function of a complete intersection artinian algebra is the Hilbert function of an almost complete intersection algebra. In codimension $3$ we focus on almost complete intersection artinian algebras whose Hilbert function coincides with that of a complete intersection defined by $3$ forms of the same degree. We classify all the possible graded Betti numbers of such algebras and we specify what cancellations are allowed in a minimal graded free resolution.","sentences":["We study the Hilbert function and the graded Betti numbers of almost complete intersection artinian algebras.","We show that that every Hilbert function of a complete intersection artinian algebra is the Hilbert function of an almost complete intersection algebra.","In codimension $3$ we focus on almost complete intersection artinian algebras whose Hilbert function coincides with that of a complete intersection defined by $3$ forms of the same degree.","We classify all the possible graded Betti numbers of such algebras and we specify what cancellations are allowed in a minimal graded free resolution."],"url":"http://arxiv.org/abs/2403.18507v1","category":"math.AC"}
{"created":"2024-03-27 12:06:14","title":"Anomalous terahertz photoconductivity caused by the superballistic flow of hydrodynamic electrons in graphene","abstract":"Light incident upon materials can induce changes in their electrical conductivity, a phenomenon referred to as photoresistance. In semiconductors, the photoresistance is negative, as light-induced promotion of electrons across the band gap enhances the number of charge carriers participating in transport. In superconductors, the photoresistance is positive because of the destruction of the superconducting state, whereas in normal metals it is vanishing. Here we report a qualitative deviation from the standard behavior in metallic graphene. We show that Dirac electrons exposed to continuous wave (CW) terahertz (THz) radiation can be thermally decoupled from the lattice by 50~K which activates hydrodynamic electron transport. In this regime, the resistance of graphene constrictions experiences a decrease caused by the THz-driven superballistic flow of correlated electrons. We analyze the dependencies of the negative photoresistance on the carrier density, and the radiation power and show that our superballistic devices operate as sensitive phonon-cooled bolometers and can thus offer a picosecond-scale response time. Beyond their fundamental implications, our findings underscore the practicality of electron hydrodynamics in designing ultra-fast THz sensors and electron thermometers.","sentences":["Light incident upon materials can induce changes in their electrical conductivity, a phenomenon referred to as photoresistance.","In semiconductors, the photoresistance is negative, as light-induced promotion of electrons across the band gap enhances the number of charge carriers participating in transport.","In superconductors, the photoresistance is positive because of the destruction of the superconducting state, whereas in normal metals it is vanishing.","Here we report a qualitative deviation from the standard behavior in metallic graphene.","We show that Dirac electrons exposed to continuous wave (CW) terahertz (THz) radiation can be thermally decoupled from the lattice by 50~K which activates hydrodynamic electron transport.","In this regime, the resistance of graphene constrictions experiences a decrease caused by the THz-driven superballistic flow of correlated electrons.","We analyze the dependencies of the negative photoresistance on the carrier density, and the radiation power and show that our superballistic devices operate as sensitive phonon-cooled bolometers and can thus offer a picosecond-scale response time.","Beyond their fundamental implications, our findings underscore the practicality of electron hydrodynamics in designing ultra-fast THz sensors and electron thermometers."],"url":"http://arxiv.org/abs/2403.18492v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 10:11:50","title":"Electrolyte effects on the alkaline hydrogen evolution reaction: a mean-field approach","abstract":"This paper introduces the combination of an advanced double layer model with electrochemical kinetics to explain electrolyte effects on the alkaline hydrogen evolution reaction. It is known from experimental studies that the alkaline hydrogen evolution current shows a strong dependence on the concentration and identity of cations in the electrolyte, but is independent of pH. To explain these effects, we formulate the faradaic current in terms of the electric potential in the double layer, which is calculated using a mean-field model that takes into account the cation and anion sizes as well as the electric dipole moment of water molecules. We consider that the Volmer step consists of two activated processes: a water reduction sub-step and a sub-step in which a hydroxide ion is transferred from the interface to the electrolyte bulk. Either of these sub-steps may limit the rate. The developed models for these sub-steps qualitatively explain experimental observations, including cation effects, pH-independence, and the trend reversal between gold and platinum electrodes. We also assess the quantitative accuracy of the water reduction-limited current model; we suggest that the predicted functional relationship is valid as long as the hydrogen bonding structure of water near the electrode is sufficiently maintained.","sentences":["This paper introduces the combination of an advanced double layer model with electrochemical kinetics to explain electrolyte effects on the alkaline hydrogen evolution reaction.","It is known from experimental studies that the alkaline hydrogen evolution current shows a strong dependence on the concentration and identity of cations in the electrolyte, but is independent of pH. To explain these effects, we formulate the faradaic current in terms of the electric potential in the double layer, which is calculated using a mean-field model that takes into account the cation and anion sizes as well as the electric dipole moment of water molecules.","We consider that the Volmer step consists of two activated processes: a water reduction sub-step and a sub-step in which a hydroxide ion is transferred from the interface to the electrolyte bulk.","Either of these sub-steps may limit the rate.","The developed models for these sub-steps qualitatively explain experimental observations, including cation effects, pH-independence, and the trend reversal between gold and platinum electrodes.","We also assess the quantitative accuracy of the water reduction-limited current model; we suggest that the predicted functional relationship is valid as long as the hydrogen bonding structure of water near the electrode is sufficiently maintained."],"url":"http://arxiv.org/abs/2403.18418v1","category":"physics.chem-ph"}
{"created":"2024-03-27 09:54:30","title":"The role of hidden-color components in the tetraquark mixing model","abstract":"Multiquarks can have two-hadron components and hidden-color components in their wave functions. The presence of two-hadron components in multiquarks introduces a potential source of confusion, particularly with respect to their resemblance to hadronic molecules. On the other hand, hidden-color components are essential for distinguishing between multiquarks and hadronic molecules. In this work, we study the hidden-color components in the wave functions of the tetraquark mixing model, a model that has been proposed as a suitable framework for describing the properties of two nonets in the $J^P=0^+$ channel: the light nonet [$a_0 (980)$, $K_0^* (700)$, $f_0 (500)$, $f_0 (980)$] and the heavy nonet [$a_0 (1450)$, $K_0^* (1430)$, $f_0 (1370)$, $f_0 (1500)$]. Our analysis reveals a substantial presence of hidden-color components within the tetraquark wave functions. To elucidate the impact of hidden-color components on physical quantities, we conduct computations of the hyperfine masses, $\\langle V_{CS}\\rangle$, for the two nonets, considering scenarios involving only the two-meson components and those incorporating the hidden-color components. We demonstrate that the hidden-color components constitute an important part of the hyperfine masses, such that the mass difference formula, $\\Delta M\\approx \\Delta \\langle V_{CS}\\rangle$, which has been successful for the two nonets, cannot be achieved without the hidden-color contributions. This can provide another evidence supporting the tetraquark nature of the two nonets.","sentences":["Multiquarks can have two-hadron components and hidden-color components in their wave functions.","The presence of two-hadron components in multiquarks introduces a potential source of confusion, particularly with respect to their resemblance to hadronic molecules.","On the other hand, hidden-color components are essential for distinguishing between multiquarks and hadronic molecules.","In this work, we study the hidden-color components in the wave functions of the tetraquark mixing model, a model that has been proposed as a suitable framework for describing the properties of two nonets in the $J^P=0^+$ channel: the light nonet [$a_0 (980)$, $K_0^* (700)$, $f_0 (500)$, $f_0 (980)$] and the heavy nonet [$a_0 (1450)$, $K_0^* (1430)$, $f_0 (1370)$, $f_0 (1500)$].","Our analysis reveals a substantial presence of hidden-color components within the tetraquark wave functions.","To elucidate the impact of hidden-color components on physical quantities, we conduct computations of the hyperfine masses, $\\langle V_{CS}\\rangle$, for the two nonets, considering scenarios involving only the two-meson components and those incorporating the hidden-color components.","We demonstrate that the hidden-color components constitute an important part of the hyperfine masses, such that the mass difference formula, $\\Delta M\\approx \\Delta \\langle V_{CS}\\rangle$, which has been successful for the two nonets, cannot be achieved without the hidden-color contributions.","This can provide another evidence supporting the tetraquark nature of the two nonets."],"url":"http://arxiv.org/abs/2403.18411v1","category":"hep-ph"}
{"created":"2024-03-27 09:29:39","title":"Broad-band ellipsometry study of the anisotropic dielectric response of YAlO3","abstract":"We present a broad band (THz to UV) ellipsometry study of the anisotropic dielectric response of the orthorhombic perovskite YAlO3. The ellipsometric measurements have been performed on YAlO3 crystals with three different surface cuts and for six high symmetry configurations of the crystal axes with respect to the plane of incidence of the photons. The obtained data are presented in terms of the Mueller Matrix elements N, C, and S and their features are analyzed and discussed with respect to the anisotropy of the dielectric response tensor. In particular, in the infrared range we have identified all 25 infrared active phonon modes that have been predicted from theoretical studies. We also discuss a negative refraction effect that naturally occurs in the vicinity of an anisotropic longitudinal-optical phonon. Moreover, we have determined the temperature dependence of the phonon parameters between 10 and 330 K. The dielectric response above the phonon range, from about 0.1 to 6.5 eV, is shown to be featureless and characteristic of an insulator with a large band gap above 6.5 eV and is well described by anisotropic Cauchy model.","sentences":["We present a broad band (THz to UV) ellipsometry study of the anisotropic dielectric response of the orthorhombic perovskite YAlO3.","The ellipsometric measurements have been performed on YAlO3 crystals with three different surface cuts and for six high symmetry configurations of the crystal axes with respect to the plane of incidence of the photons.","The obtained data are presented in terms of the Mueller Matrix elements N, C, and S and their features are analyzed and discussed with respect to the anisotropy of the dielectric response tensor.","In particular, in the infrared range we have identified all 25 infrared active phonon modes that have been predicted from theoretical studies.","We also discuss a negative refraction effect that naturally occurs in the vicinity of an anisotropic longitudinal-optical phonon.","Moreover, we have determined the temperature dependence of the phonon parameters between 10 and 330 K. The dielectric response above the phonon range, from about 0.1 to 6.5 eV, is shown to be featureless and characteristic of an insulator with a large band gap above 6.5 eV and is well described by anisotropic Cauchy model."],"url":"http://arxiv.org/abs/2403.18392v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 09:29:11","title":"Bayesian electron density determination from sparse and noisy single-molecule X-ray scattering images","abstract":"Single molecule X-ray scattering experiments using free electron lasers hold the potential to resolve both single structures and structural ensembles of biomolecules. However, molecular electron density determination has so far not been achieved due to low photon counts, high noise levels and low hit rates. Most analysis approaches therefore focus on large specimen like entire viruses, which scatter substantially more photons per image, such that it becomes possible to determine the molecular orientation for each image. In contrast, for small specimen like proteins, the molecular orientation cannot be determined for each image, and must be considered random and unknown.   Here we developed and tested a rigorous Bayesian approach to overcome these limitations, and also taking into account intensity fluctuations, beam polarization, irregular detector shapes, incoherent scattering and background scattering. We demonstrate using synthetic scattering images that it is possible to determine electron densities of small proteins in this extreme high noise Poisson regime. Tests on published experimental data from the coliphage PR772 achieved the detector-limited resolution of $9\\,\\mathrm{nm}$, using only $0.01\\,\\%$ of the available photons per image.","sentences":["Single molecule X-ray scattering experiments using free electron lasers hold the potential to resolve both single structures and structural ensembles of biomolecules.","However, molecular electron density determination has so far not been achieved due to low photon counts, high noise levels and low hit rates.","Most analysis approaches therefore focus on large specimen like entire viruses, which scatter substantially more photons per image, such that it becomes possible to determine the molecular orientation for each image.","In contrast, for small specimen like proteins, the molecular orientation cannot be determined for each image, and must be considered random and unknown.   ","Here we developed and tested a rigorous Bayesian approach to overcome these limitations, and also taking into account intensity fluctuations, beam polarization, irregular detector shapes, incoherent scattering and background scattering.","We demonstrate using synthetic scattering images that it is possible to determine electron densities of small proteins in this extreme high noise Poisson regime.","Tests on published experimental data from the coliphage PR772 achieved the detector-limited resolution of $9\\,\\mathrm{nm}$, using only $0.01\\,\\%$ of the available photons per image."],"url":"http://arxiv.org/abs/2403.18391v1","category":"physics.comp-ph"}
{"created":"2024-03-27 08:57:42","title":"On the M87 jet structure near the central engine","abstract":"At present, there is no doubt that relativistic jets observed in active galactic nuclei pass from highly magnetized to weakly magnetized stage, which is observed as a break in the dependence on their width $d_{\\rm jet}(z)$ on the distance $z$ to the central engine. In this paper, we discuss the possibility of observing another break, which should be located at shorter distances. The position of this break can be associated with the region of formation of the dense central core near the jet axis which was predicted both analytically and numerically more than a decade ago, but has not yet received sufficient attention. In this case, the observed width should be determined by the dense core, and not by the total transverse size of the jet. The calculations carried out in this paper, which took into account both the transverse electromagnetic structure of the jet and the change in the spectrum of emitting particles along its axis, indeed showed such behaviour. We also found the evidence of the predicted break in the jet expansion profile using stacked 15 GHz VLBA image of M87 radio jet and constrain the light cylinder radius.","sentences":["At present, there is no doubt that relativistic jets observed in active galactic nuclei pass from highly magnetized to weakly magnetized stage, which is observed as a break in the dependence on their width $d_{\\rm jet}(z)$ on the distance $z$ to the central engine.","In this paper, we discuss the possibility of observing another break, which should be located at shorter distances.","The position of this break can be associated with the region of formation of the dense central core near the jet axis which was predicted both analytically and numerically more than a decade ago, but has not yet received sufficient attention.","In this case, the observed width should be determined by the dense core, and not by the total transverse size of the jet.","The calculations carried out in this paper, which took into account both the transverse electromagnetic structure of the jet and the change in the spectrum of emitting particles along its axis, indeed showed such behaviour.","We also found the evidence of the predicted break in the jet expansion profile using stacked 15 GHz VLBA image of M87 radio jet and constrain the light cylinder radius."],"url":"http://arxiv.org/abs/2403.18366v1","category":"astro-ph.GA"}
{"created":"2024-03-27 08:45:33","title":"Magnetic helicity evolution during active region emergence and subsequent flare productivity","abstract":"Aims. Solar active regions (ARs), which are formed by flux emergence, serve as the primary sources of solar eruptions. However, the specific physical mechanism that governs the emergence process and its relationship with flare productivity remains to be thoroughly understood. Methods. We examined 136 emerging ARs, focusing on the evolution of their magnetic helicity and magnetic energy during the emergence phase. Based on the relation between helicity accumulation and magnetic flux evolution, we categorized the samples and investigated their flare productivity. Results. The emerging ARs we studied can be categorized into three types, Type-I, Type-II, and Type-III, and they account for 52.2%, 25%, and 22.8% of the total number in our sample, respectively. Type-I ARs exhibit a synchronous increase in both the magnetic flux and magnetic helicity, while the magnetic helicity in Type-II ARs displays a lag in increasing behind the magnetic flux. Type-III ARs show obvious helicity injections of opposite signs. Significantly, 90% of the flare-productive ARs (flare index > 6) were identified as Type-I ARs, suggesting that this type of AR has a higher potential to become flare productive. In contrast, Type-II and Type-III ARs exhibited a low and moderate likelihood of becoming active, respectively. Our statistical analysis also revealed that Type-I ARs accumulate more magnetic helicity and energy, far beyond what is found in Type-II and Type-III ARs. Moreover, we observed that flare-productive ARs consistently accumulate a significant amount of helicity and energy during their emergence phase. Conclusions. These findings provide valuable insight into the flux emergence phenomena, offering promising possibilities for early-stage predictions of solar eruptions.","sentences":["Aims.","Solar active regions (ARs), which are formed by flux emergence, serve as the primary sources of solar eruptions.","However, the specific physical mechanism that governs the emergence process and its relationship with flare productivity remains to be thoroughly understood.","Methods.","We examined 136 emerging ARs, focusing on the evolution of their magnetic helicity and magnetic energy during the emergence phase.","Based on the relation between helicity accumulation and magnetic flux evolution, we categorized the samples and investigated their flare productivity.","Results.","The emerging ARs we studied can be categorized into three types, Type-I, Type-II, and Type-III, and they account for 52.2%, 25%, and 22.8% of the total number in our sample, respectively.","Type-I ARs exhibit a synchronous increase in both the magnetic flux and magnetic helicity, while the magnetic helicity in Type-II ARs displays a lag in increasing behind the magnetic flux.","Type-III ARs show obvious helicity injections of opposite signs.","Significantly, 90% of the flare-productive ARs (flare index > 6) were identified as Type-I ARs, suggesting that this type of AR has a higher potential to become flare productive.","In contrast, Type-II and Type-III ARs exhibited a low and moderate likelihood of becoming active, respectively.","Our statistical analysis also revealed that Type-I ARs accumulate more magnetic helicity and energy, far beyond what is found in Type-II and Type-III ARs.","Moreover, we observed that flare-productive ARs consistently accumulate a significant amount of helicity and energy during their emergence phase.","Conclusions.","These findings provide valuable insight into the flux emergence phenomena, offering promising possibilities for early-stage predictions of solar eruptions."],"url":"http://arxiv.org/abs/2403.18354v1","category":"astro-ph.SR"}
{"created":"2024-03-27 08:14:36","title":"Quantum gravity of the Heisenberg algebra","abstract":"We consider a simplified model of double scaled SYK (DSSYK) in which the Hamiltonian is the position operator of the Harmonic oscillator. This model captures the high temperature limit of DSSYK but could also be defined as a quantum theory in its own right. We study properties of the emergent geometry including its dynamics in response to inserting matter particles. In particular, we find that the model displays de Sitter-like properties such as that infalling matter reduces the rate of growth of geodesic slices between the two boundaries. The simplicity of the model allows us to compute the full generating functional for correlation functions of the length mode or any number of matter operators. We provide evidence that the effective action of the geodesic length between boundary points is non-local. Furthermore, we use the on-shell solution for the geodesic lengths between any two boundary points to reconstruct an effective bulk metric and reverse engineer the dilaton gravity theory that generates this metric as a solution.","sentences":["We consider a simplified model of double scaled SYK (DSSYK) in which the Hamiltonian is the position operator of the Harmonic oscillator.","This model captures the high temperature limit of DSSYK but could also be defined as a quantum theory in its own right.","We study properties of the emergent geometry including its dynamics in response to inserting matter particles.","In particular, we find that the model displays de Sitter-like properties such as that infalling matter reduces the rate of growth of geodesic slices between the two boundaries.","The simplicity of the model allows us to compute the full generating functional for correlation functions of the length mode or any number of matter operators.","We provide evidence that the effective action of the geodesic length between boundary points is non-local.","Furthermore, we use the on-shell solution for the geodesic lengths between any two boundary points to reconstruct an effective bulk metric and reverse engineer the dilaton gravity theory that generates this metric as a solution."],"url":"http://arxiv.org/abs/2403.18333v1","category":"hep-th"}
{"created":"2024-03-27 08:09:12","title":"Bulk condensation by an active interface","abstract":"We present experiments, supported by mechanically detailed simulations, establishing bulk vapor-liquid condensation of a hard-bead fluid by a tiny population of orientable motile grains that self-assembles into a moving polarized monolayer. In a quasi-1D geometry two such layers, oppositely aligned, immobilize the condensed non-motile component. We account for our observations through a continuum theory with a naturally non-reciprocal Cahn-Hilliard structure, whose predicted trends as a function of packing fraction are consistent with our observations.","sentences":["We present experiments, supported by mechanically detailed simulations, establishing bulk vapor-liquid condensation of a hard-bead fluid by a tiny population of orientable motile grains that self-assembles into a moving polarized monolayer.","In a quasi-1D geometry two such layers, oppositely aligned, immobilize the condensed non-motile component.","We account for our observations through a continuum theory with a naturally non-reciprocal Cahn-Hilliard structure, whose predicted trends as a function of packing fraction are consistent with our observations."],"url":"http://arxiv.org/abs/2403.18329v1","category":"cond-mat.soft"}
{"created":"2024-03-27 07:37:44","title":"Topological Vector Spaces: a non-standard approach with monads and galaxies","abstract":"By generalizing the overspill principle towards directed sets, a new and extensive formalism is developed for monads and galaxies in non-standard enlargements. It is shown that monads and galaxies can be manipulated using order-preserving and order-reversing set-to-set maps, and that set properties associated with these maps can be extended not only to internal sets but to all monads and galaxies. An abstract theory of Intersections of Galaxies is introduced. These concepts are applied to basic topology as well (locally convex) topological vector spaces and their properties. Local properties and completeness can be defined and characterized effortlessly. Duality theory is studied in this framework, allowing in particular to formulate brief and insightful proofs for the theorems of Mackey-Arens and Grothendieck completeness without any technicalities.","sentences":["By generalizing the overspill principle towards directed sets, a new and extensive formalism is developed for monads and galaxies in non-standard enlargements.","It is shown that monads and galaxies can be manipulated using order-preserving and order-reversing set-to-set maps, and that set properties associated with these maps can be extended not only to internal sets but to all monads and galaxies.","An abstract theory of Intersections of Galaxies is introduced.","These concepts are applied to basic topology as well (locally convex) topological vector spaces and their properties.","Local properties and completeness can be defined and characterized effortlessly.","Duality theory is studied in this framework, allowing in particular to formulate brief and insightful proofs for the theorems of Mackey-Arens and Grothendieck completeness without any technicalities."],"url":"http://arxiv.org/abs/2403.18315v1","category":"math.LO"}
{"created":"2024-03-27 07:34:00","title":"Demonstration of Near-Epithermal Neutron Reflective Optics","abstract":"Specular reflection of neutrons on material surfaces has been demonstrated in the energy range of 0.09-0.7 eV. The results suggest that the applicable energy range of reflective neutron optics can be extended to the near-epithermal region by using existing techniques.","sentences":["Specular reflection of neutrons on material surfaces has been demonstrated in the energy range of 0.09-0.7 eV. The results suggest that the applicable energy range of reflective neutron optics can be extended to the near-epithermal region by using existing techniques."],"url":"http://arxiv.org/abs/2403.18313v1","category":"physics.ins-det"}
{"created":"2024-03-27 06:27:44","title":"Infrared properties of carbon stars in our Galaxy","abstract":"In this study, we explore the characteristics of carbon stars within our Galaxy through a comprehensive analysis of observational data spanning visual and infrared (IR) bands. Leveraging datasets from IRAS, ISO, Akari, MSX, 2MASS, WISE, Gaia DR3, AAVSO, and the SIMBAD object database, we conduct a detailed comparison between the observational data and theoretical models. To facilitate this comparison, we introduce various IR two-color diagrams (2CDs), IR color-magnitude diagrams (CMDs), and spectral energy distributions (SEDs). We find that the CMDs, which utilize the latest distance and extinction data from Gaia DR3 for a substantial number of carbon stars, are very useful to distinguish carbon-rich asymptotic giant branch (CAGB) stars from extrinsic carbon stars that are not in the AGB phase. To enhance the accuracy of our analysis, we employ theoretical radiative transfer models for dust shells around CAGB stars. These theoretical dust shell models demonstrate a commendable ability to approximate the observations of CAGB stars across various SEDs, 2CDs, and CMDs. We present the infrared properties of known pulsating variables and explore the infrared variability of the sample stars by analyzing WISE photometric data spanning the last 14 yr. Additionally, we present a novel catalog of CAGB stars, offering enhanced reliability and a wealth of additional information.","sentences":["In this study, we explore the characteristics of carbon stars within our Galaxy through a comprehensive analysis of observational data spanning visual and infrared (IR) bands.","Leveraging datasets from IRAS, ISO, Akari, MSX, 2MASS, WISE, Gaia DR3, AAVSO, and the SIMBAD object database, we conduct a detailed comparison between the observational data and theoretical models.","To facilitate this comparison, we introduce various IR two-color diagrams (2CDs), IR color-magnitude diagrams (CMDs), and spectral energy distributions (SEDs).","We find that the CMDs, which utilize the latest distance and extinction data from Gaia DR3 for a substantial number of carbon stars, are very useful to distinguish carbon-rich asymptotic giant branch (CAGB) stars from extrinsic carbon stars that are not in the AGB phase.","To enhance the accuracy of our analysis, we employ theoretical radiative transfer models for dust shells around CAGB stars.","These theoretical dust shell models demonstrate a commendable ability to approximate the observations of CAGB stars across various SEDs, 2CDs, and CMDs.","We present the infrared properties of known pulsating variables and explore the infrared variability of the sample stars by analyzing WISE photometric data spanning the last 14 yr.","Additionally, we present a novel catalog of CAGB stars, offering enhanced reliability and a wealth of additional information."],"url":"http://arxiv.org/abs/2403.18290v1","category":"astro-ph.SR"}
{"created":"2024-03-27 06:27:22","title":"Identifying the transverse and longitudinal modes of the $K^*$ and $K_{1}$ mesons through their angular dependent decay modes","abstract":"Observing the mass shifts of chiral partners will provide invaluable insight into the role of chiral symmetry breaking in the generation of hadron masses. Because both the $K^*$ and $K_1$ mesons have vacuum widths smaller than 100 MeV, they are ideal candidates for realizing mass shift measurements. On the other hand, the different momentum dependence of the longitudinal and transverse modes smear the peak positions. In this work, we analyze the angular dependence of the two-body decays of both the $K^*$ and $K_1$. It is found that the longitudinal and transverse modes of the $K^*$ can be isolated by observing the pseudoscalar decay in either the forward or perpendicular directions, respectively. For the $K_1$ decaying into a vector meson and a pseudoscalar meson, one can accomplish the same goal by further observing the polarization of the vector meson through its angular dependence on the two pseudoscalar meson decay.","sentences":["Observing the mass shifts of chiral partners will provide invaluable insight into the role of chiral symmetry breaking in the generation of hadron masses.","Because both the $K^*$ and $K_1$ mesons have vacuum widths smaller than 100 MeV, they are ideal candidates for realizing mass shift measurements.","On the other hand, the different momentum dependence of the longitudinal and transverse modes smear the peak positions.","In this work, we analyze the angular dependence of the two-body decays of both the $K^*$ and $K_1$. It is found that the longitudinal and transverse modes of the $K^*$ can be isolated by observing the pseudoscalar decay in either the forward or perpendicular directions, respectively.","For the $K_1$ decaying into a vector meson and a pseudoscalar meson, one can accomplish the same goal by further observing the polarization of the vector meson through its angular dependence on the two pseudoscalar meson decay."],"url":"http://arxiv.org/abs/2403.18288v1","category":"hep-ph"}
{"created":"2024-03-27 05:57:10","title":"Analysis of solution to an elliptic free boundary value problem equipped with a `bad' data","abstract":"We will study a free boundary value problem driven by a source term which is quite {\\it irregular}. In the process, we will establish a monotonicity result, and regularity of the solution.","sentences":["We will study a free boundary value problem driven by a source term which is quite {\\it irregular}.","In the process, we will establish a monotonicity result, and regularity of the solution."],"url":"http://arxiv.org/abs/2403.18273v1","category":"math.AP"}
{"created":"2024-03-27 05:55:25","title":"Recovery of High-energy Low-frequency Quasi-periodic Oscillations from Black Hole X-ray Binary MAXI J1535-571 with a Hilbert-Huang Transform Method","abstract":"We propose a method based on the Hilbert-Huang transform (HHT) to recover the high-energy waveform of low-frequency quasi-periodic oscillations (LFQPOs). Based on the method, we successfully obtain the modulation of the phase-folded light curve above 170 keV using the QPO phase reconstructed at lower energies in MAXI J1535-571 with Insight-HXMT observations. A comprehensive simulation study is conducted to demonstrate that such modulation indeed originates from the QPO. Thus the highest energies turn out to significantly exceed the upper limit of ~100 keV for QPOs reported previously using the Fourier method, marking the first opportunity to study QPO properties above 100 keV in this source. Detailed analyses of these high-energy QPO profiles reveal different QPO properties between the 30-100 keV and 100-200 keV energy ranges: the phase lag remains relatively stable, and the amplitude slightly increases below ~100 keV, whereas above this threshold, soft phase lags and a decrease in amplitude are observed. Given the reports of a hard tail detection in broad spectroscopy, we propose that the newly discovered QPO properties above 100 keV are dominated by the hard tail component, possibly stemming from a relativistic jet. Our findings also indicate a strong correlation between the QPOs originating from the jet and corona, supporting the scenario of jet-corona coupling precssion. We emphasize that our proposed HHT-based method can serve as an efficient manner in expanding the high energy band for studying QPOs, thereby enhancing our understanding of their origin.","sentences":["We propose a method based on the Hilbert-Huang transform (HHT) to recover the high-energy waveform of low-frequency quasi-periodic oscillations (LFQPOs).","Based on the method, we successfully obtain the modulation of the phase-folded light curve above 170 keV using the QPO phase reconstructed at lower energies in MAXI J1535-571 with Insight-HXMT observations.","A comprehensive simulation study is conducted to demonstrate that such modulation indeed originates from the QPO.","Thus the highest energies turn out to significantly exceed the upper limit of ~100 keV for QPOs reported previously using the Fourier method, marking the first opportunity to study QPO properties above 100 keV in this source.","Detailed analyses of these high-energy QPO profiles reveal different QPO properties between the 30-100 keV and 100-200 keV energy ranges: the phase lag remains relatively stable, and the amplitude slightly increases below ~100 keV, whereas above this threshold, soft phase lags and a decrease in amplitude are observed.","Given the reports of a hard tail detection in broad spectroscopy, we propose that the newly discovered QPO properties above 100 keV are dominated by the hard tail component, possibly stemming from a relativistic jet.","Our findings also indicate a strong correlation between the QPOs originating from the jet and corona, supporting the scenario of jet-corona coupling precssion.","We emphasize that our proposed HHT-based method can serve as an efficient manner in expanding the high energy band for studying QPOs, thereby enhancing our understanding of their origin."],"url":"http://arxiv.org/abs/2403.18272v1","category":"astro-ph.HE"}
{"created":"2024-03-27 05:33:18","title":"New Constraints on Exotic Spin-Spin-Velocity-Dependent Interactions with Solid-State Quantum Sensors","abstract":"We report new experimental results on exotic spin-spin-velocity-dependent interactions between electron spins. We designed an elaborate setup that is equipped with two nitrogen-vacancy (NV) ensembles in diamonds. One of the NV ensembles serves as the spin source, while the other functions as the spin sensor. By coherently manipulating the quantum states of two NV ensembles and their relative velocity at the micrometer scale, we are able to scrutinize exotic spin-spin-velocity-dependent interactions at short force ranges. For a T-violating interaction, $V_6$, new limits on the corresponding coupling coefficient, $f_6$, have been established for the force range shorter than 1 cm. For a P,T-violating interaction, $V_{14}$, new constraints on the corresponding coupling coefficient, $f_{14}$, have been obtained for the force range shorter than 1 km.","sentences":["We report new experimental results on exotic spin-spin-velocity-dependent interactions between electron spins.","We designed an elaborate setup that is equipped with two nitrogen-vacancy (NV) ensembles in diamonds.","One of the NV ensembles serves as the spin source, while the other functions as the spin sensor.","By coherently manipulating the quantum states of two NV ensembles and their relative velocity at the micrometer scale, we are able to scrutinize exotic spin-spin-velocity-dependent interactions at short force ranges.","For a T-violating interaction, $V_6$, new limits on the corresponding coupling coefficient, $f_6$, have been established for the force range shorter than 1 cm.","For a P,T-violating interaction, $V_{14}$, new constraints on the corresponding coupling coefficient, $f_{14}$, have been obtained for the force range shorter than 1 km."],"url":"http://arxiv.org/abs/2403.18263v1","category":"hep-ex"}
{"created":"2024-03-27 04:55:01","title":"Statistical inference for multi-regime threshold Ornstein-Uhlenbeck processes","abstract":"In this paper, we investigate the parameter estimation for threshold Ornstein$\\mathit{-}$Uhlenbeck processes. Least squares method is used to obtain continuous-type and discrete-type estimators for the drift parameters based on continuous and discrete observations, respectively. The strong consistency and asymptotic normality of the proposed least squares estimators are studied. We also propose a modified quadratic variation estimator based on the long-time observations for the diffusion parameters and prove its consistency. Our simulation results suggest that the performance of our proposed estimators for the drift parameters may show improvements compared to generalized moment estimators. Additionally, the proposed modified quadratic variation estimator exhibits potential advantages over the usual quadratic variation estimator with relatively small sample sizes. In particular, our method can be applied to the multi-regime cases ($m>2$), while the generalized moment method only deals with the two regime cases ($m=2$). The U.S. treasury rate data is used to illustrate the theoretical results.","sentences":["In this paper, we investigate the parameter estimation for threshold Ornstein$\\mathit{-}$Uhlenbeck processes.","Least squares method is used to obtain continuous-type and discrete-type estimators for the drift parameters based on continuous and discrete observations, respectively.","The strong consistency and asymptotic normality of the proposed least squares estimators are studied.","We also propose a modified quadratic variation estimator based on the long-time observations for the diffusion parameters and prove its consistency.","Our simulation results suggest that the performance of our proposed estimators for the drift parameters may show improvements compared to generalized moment estimators.","Additionally, the proposed modified quadratic variation estimator exhibits potential advantages over the usual quadratic variation estimator with relatively small sample sizes.","In particular, our method can be applied to the multi-regime cases ($m>2$), while the generalized moment method only deals with the two regime cases ($m=2$).","The U.S. treasury rate data is used to illustrate the theoretical results."],"url":"http://arxiv.org/abs/2403.18255v1","category":"math.ST"}
{"created":"2024-03-27 04:21:11","title":"LocalCop: An R package for local likelihood inference for conditional copulas","abstract":"Conditional copulas models allow the dependence structure between multiple response variables to be modelled as a function of covariates. LocalCop (Acar & Lysy, 2024) is an R/C++ package for computationally efficient semiparametric conditional copula modelling using a local likelihood inference framework developed in Acar, Craiu, & Yao (2011), Acar, Craiu, & Yao (2013) and Acar, Czado, & Lysy (2019).","sentences":["Conditional copulas models allow the dependence structure between multiple response variables to be modelled as a function of covariates.","LocalCop (Acar & Lysy, 2024) is an R/C++ package for computationally efficient semiparametric conditional copula modelling using a local likelihood inference framework developed in Acar, Craiu, & Yao (2011), Acar, Craiu, & Yao (2013) and Acar, Czado, & Lysy (2019)."],"url":"http://arxiv.org/abs/2403.18245v1","category":"stat.CO"}
{"created":"2024-03-27 04:20:28","title":"Evidence for conventional superconductivity in Bi$_2$PdPt and prediction of topological superconductivity in disorder-free $\u03b3$-BiPd","abstract":"We present comprehensive investigations into the structural, superconducting, and topological properties of Bi$_2$PdPt. Magnetization and heat capacity measurements performed on polycrystalline Bi$_2$PdPt demonstrate a superconducting transition at $\\approx$ 0.8 K. Moreover, muon spin relaxation/rotation ($\\mu$SR) measurements present evidence for a time reversal symmetry preserving, isotropically gapped superconducting state in Bi$_2$PdPt. We have also performed density-functional theory (DFT) calculations on Bi$_2$PdPt alongside the more general isostructural systems, BiPd$_{x}$Pt$_{1-x}$, of which Bi$_2$PdPt and $\\gamma$-BiPd are special cases for $x=0.5$ and $x=1$ respectively. We have calculated the $Z_2$ topological index from our DFT calculations for a range of substitution fractions, $x$, between $x=0$ and $x=1$ characterizing the topology of the band structure. We find a non-trivial topological state when $x>0.75$ and a trivial topological state when $x<0.75$. Therefore our results indicate that BiPd$_{x}$Pt$_{1-x}$ could be a topological superconductor for $x>0.75$.","sentences":["We present comprehensive investigations into the structural, superconducting, and topological properties of Bi$_2$PdPt.","Magnetization and heat capacity measurements performed on polycrystalline Bi$_2$PdPt demonstrate a superconducting transition at $\\approx$ 0.8 K. Moreover, muon spin relaxation/rotation ($\\mu$SR) measurements present evidence for a time reversal symmetry preserving, isotropically gapped superconducting state in Bi$_2$PdPt.","We have also performed density-functional theory (DFT) calculations on Bi$_2$PdPt alongside the more general isostructural systems, BiPd$_{x}$Pt$_{1-x}$, of which Bi$_2$PdPt and $\\gamma$-BiPd are special cases for $x=0.5$ and $x=1$ respectively.","We have calculated the $Z_2$ topological index from our DFT calculations for a range of substitution fractions, $x$, between $x=0$ and $x=1$ characterizing the topology of the band structure.","We find a non-trivial topological state when $x>0.75$ and a trivial topological state when $x<0.75$.","Therefore our results indicate that BiPd$_{x}$Pt$_{1-x}$ could be a topological superconductor for $x>0.75$."],"url":"http://arxiv.org/abs/2403.18244v1","category":"cond-mat.supr-con"}
{"created":"2024-03-27 02:58:15","title":"Next-Generation Time-Resolved Scanning Probe Microscopy","abstract":"Understanding the nanoscale carrier dynamics induced by light excitation is the key to unlocking futuristic devices and innovative functionalities in advanced materials. Optical pump-probe scanning tunneling microscopy (OPP-STM) has opened a window to these phenomena. However, mastering the combination of ultrafast pulsed lasers with STM requires high expertise and effort. We have shattered this barrier and developed a compact OPP-STM system accessible to all. This system precisely controls laser pulse timing electrically and enables stable laser irradiation on sample surfaces. Furthermore, by applying this technique to atomic force microscopy (AFM), we have captured time-resolved force signals with an exceptionally high signal-to-noise ratio. Originating from the dipole-dipole interactions, these signals provide insights into the carrier dynamics on sample surfaces, which are activated by photo-illumination. These technologies are promising as powerful tools for exploring a wide range of photoinduced phenomena in conductive and insulating materials.","sentences":["Understanding the nanoscale carrier dynamics induced by light excitation is the key to unlocking futuristic devices and innovative functionalities in advanced materials.","Optical pump-probe scanning tunneling microscopy (OPP-STM) has opened a window to these phenomena.","However, mastering the combination of ultrafast pulsed lasers with STM requires high expertise and effort.","We have shattered this barrier and developed a compact OPP-STM system accessible to all.","This system precisely controls laser pulse timing electrically and enables stable laser irradiation on sample surfaces.","Furthermore, by applying this technique to atomic force microscopy (AFM), we have captured time-resolved force signals with an exceptionally high signal-to-noise ratio.","Originating from the dipole-dipole interactions, these signals provide insights into the carrier dynamics on sample surfaces, which are activated by photo-illumination.","These technologies are promising as powerful tools for exploring a wide range of photoinduced phenomena in conductive and insulating materials."],"url":"http://arxiv.org/abs/2403.18215v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 02:52:16","title":"Parity doublet model for baryon octets: ground states saturated by good diquarks and the role of bad diquarks for excited states","abstract":"Parity doublet model is an effective chiral model that includes the chiral variant and invariant masses of baryons. The chiral invariant mass has large impacts on the density dependence of models which can be constrained by neutron star observations. In the previous work, models of two-flavors have been considered up to a few times nuclear saturation density, but in such dense region it is also necessary to consider hyperons. With the chiral invariant masses baryons can stay massive in extreme environments (e.g., neutron stars) where the chiral symmetry restoration takes place. In this work, we generalize the previous $\\mbox{SU(2)}_L \\times \\mbox{SU(2)}_R$ parity models of nucleons to $\\mbox{SU(3)}_L \\times \\mbox{SU(3)}_R$ models of the baryon octet, within the linear realization of the chiral symmetry. The major problem in constructing such models has been too many candidates for the chiral representations of baryons. Motivated by the concepts of diquarks and the mended symmetry, we choose the $(3_L, \\bar{3}_R) + (\\bar{3}_L, 3_R)$, $(3_L, 6_R) + (6_L, 3_R)$ and $(1_L, 8_R) + (8_L, 1_R)$ representations and use quark diagrams to constrain the possible types of Yukawa interactions. The masses of the baryon octets for positive and negative baryons up to the first excitations are successfully reproduced. As expected from the diquark considerations, the ground state baryons are well dominated by $(3_L, \\bar{3}_R) + (\\bar{3}_L, 3_R)$ and $(1_L, 8_R) + (8_L, 1_R)$ representations, while the excited states require $(3_L, 6_R) + (6_L, 3_R)$ representations. Important applications of our model are the chiral restoration for strange quarks at large density and the continuity of diquarks from hadronic to quark matter. We also address the problem of large Yukawa couplings which are enhanced in three-flavor construction.","sentences":["Parity doublet model is an effective chiral model that includes the chiral variant and invariant masses of baryons.","The chiral invariant mass has large impacts on the density dependence of models which can be constrained by neutron star observations.","In the previous work, models of two-flavors have been considered up to a few times nuclear saturation density, but in such dense region it is also necessary to consider hyperons.","With the chiral invariant masses baryons can stay massive in extreme environments (e.g., neutron stars) where the chiral symmetry restoration takes place.","In this work, we generalize the previous $\\mbox{SU(2)}_L \\times \\mbox{SU(2)}_R$ parity models of nucleons to $\\mbox{SU(3)}_L \\times \\mbox{SU(3)}_R$ models of the baryon octet, within the linear realization of the chiral symmetry.","The major problem in constructing such models has been too many candidates for the chiral representations of baryons.","Motivated by the concepts of diquarks and the mended symmetry, we choose the $(3_L, \\bar{3}_R)","+ (\\bar{3}_L, 3_R)$, $(3_L, 6_R) +","(6_L, 3_R)$ and $(1_L, 8_R) +","(8_L, 1_R)$ representations and use quark diagrams to constrain the possible types of Yukawa interactions.","The masses of the baryon octets for positive and negative baryons up to the first excitations are successfully reproduced.","As expected from the diquark considerations, the ground state baryons are well dominated by $(3_L, \\bar{3}_R)","+ (\\bar{3}_L, 3_R)$ and $(1_L, 8_R)","+ (8_L, 1_R)$ representations, while the excited states require $(3_L, 6_R)","+ (6_L, 3_R)$ representations.","Important applications of our model are the chiral restoration for strange quarks at large density and the continuity of diquarks from hadronic to quark matter.","We also address the problem of large Yukawa couplings which are enhanced in three-flavor construction."],"url":"http://arxiv.org/abs/2403.18214v1","category":"hep-ph"}
{"created":"2024-03-27 02:31:08","title":"A Sociotechnical Readiness Level Framework for the Development of Advanced Nuclear Technologies","abstract":"The Technology Readiness Level (TRL) scale was initially developed by NASA in the 1970s and is now widely used in space, nuclear, and other complex technology sectors in the US and beyond. The TRL scale is particularly useful for determining where extrapolation of untested sub-systems or features could produce technical risk, cause expensive redesigns, or act as a roadblock to technology development. In this paper, we propose the development of a sociotechnical readiness level (SRL), premised on the understanding that the successful development and eventual use of a technology requires achieving not only full technological readiness but also anticipating, prioritizing, and addressing societal concerns that may arise during the course of development of a technology. Failures to anticipate and address societal factors in the early stages of technology development have led to high-profile delays and, in some cases, ultimate failures of nuclear technology projects. The sociotechnical readiness scale, which conceptually draws on the design research and science and technology studies scholarship, centers on principles of equity and environmental justice in technology design, and emphasizes the need for social engagement during the process of technology development. Nowhere is such an approach to technology development more vital or needed than for the long-term management of spent nuclear fuel.","sentences":["The Technology Readiness Level (TRL) scale was initially developed by NASA in the 1970s and is now widely used in space, nuclear, and other complex technology sectors in the US and beyond.","The TRL scale is particularly useful for determining where extrapolation of untested sub-systems or features could produce technical risk, cause expensive redesigns, or act as a roadblock to technology development.","In this paper, we propose the development of a sociotechnical readiness level (SRL), premised on the understanding that the successful development and eventual use of a technology requires achieving not only full technological readiness but also anticipating, prioritizing, and addressing societal concerns that may arise during the course of development of a technology.","Failures to anticipate and address societal factors in the early stages of technology development have led to high-profile delays and, in some cases, ultimate failures of nuclear technology projects.","The sociotechnical readiness scale, which conceptually draws on the design research and science and technology studies scholarship, centers on principles of equity and environmental justice in technology design, and emphasizes the need for social engagement during the process of technology development.","Nowhere is such an approach to technology development more vital or needed than for the long-term management of spent nuclear fuel."],"url":"http://arxiv.org/abs/2403.18204v1","category":"physics.soc-ph"}
{"created":"2024-03-27 02:18:43","title":"Hybrid silicon all-optical switching devices integrated with two-dimensional material","abstract":"We propose and demonstrate hybrid all-optical switching devices that combine silicon nanocavities and two-dimensional semiconductor material. By exploiting the refractive index modulation caused by photo-induced carriers in the two-dimensional material instead of the silicon substrate, we overcome the switching performance limitation imposed by the substrate material. Air-mode photonic crystal nanobeam cavities capable of efficient interaction with two-dimensional materials are fabricated, and molybdenum ditelluride, a two-dimensional material with rapid carrier recombination, is transferred onto the cavities. The molybdenum ditelluride flake is excited by an optical pump pulse to shift the resonant wavelength of the cavity for switching operation. We have successfully achieved all-optical switching operations on the time scale of tens of picoseconds while requiring low switching energies of a few hundred femtojoules.","sentences":["We propose and demonstrate hybrid all-optical switching devices that combine silicon nanocavities and two-dimensional semiconductor material.","By exploiting the refractive index modulation caused by photo-induced carriers in the two-dimensional material instead of the silicon substrate, we overcome the switching performance limitation imposed by the substrate material.","Air-mode photonic crystal nanobeam cavities capable of efficient interaction with two-dimensional materials are fabricated, and molybdenum ditelluride, a two-dimensional material with rapid carrier recombination, is transferred onto the cavities.","The molybdenum ditelluride flake is excited by an optical pump pulse to shift the resonant wavelength of the cavity for switching operation.","We have successfully achieved all-optical switching operations on the time scale of tens of picoseconds while requiring low switching energies of a few hundred femtojoules."],"url":"http://arxiv.org/abs/2403.18199v1","category":"physics.optics"}
{"created":"2024-03-27 01:10:31","title":"Growth rate of liquidity provider's wealth in G3Ms","abstract":"Geometric mean market makers (G3Ms), such as Uniswap and Balancer, represent a widely used class of automated market makers (AMMs). These G3Ms are characterized by the following rule: the reserves of the AMM must maintain the same (weighted) geometric mean before and after each trade. This paper investigates the effects of trading fees on liquidity providers' (LP) profitability in a G3M, as well as the adverse selection faced by LPs due to arbitrage activities involving a reference market. Our work expands the model described in previous studies for G3Ms, integrating transaction fees and continuous-time arbitrage into the analysis. Within this context, we analyze G3M dynamics, characterized by stochastic storage processes, and calculate the growth rate of LP wealth. In particular, our results align with and extend the results concerning the constant product market maker, commonly referred to as Uniswap v2.","sentences":["Geometric mean market makers (G3Ms), such as Uniswap and Balancer, represent a widely used class of automated market makers (AMMs).","These G3Ms are characterized by the following rule: the reserves of the AMM must maintain the same (weighted) geometric mean before and after each trade.","This paper investigates the effects of trading fees on liquidity providers' (LP) profitability in a G3M, as well as the adverse selection faced by LPs due to arbitrage activities involving a reference market.","Our work expands the model described in previous studies for G3Ms, integrating transaction fees and continuous-time arbitrage into the analysis.","Within this context, we analyze G3M dynamics, characterized by stochastic storage processes, and calculate the growth rate of LP wealth.","In particular, our results align with and extend the results concerning the constant product market maker, commonly referred to as Uniswap v2."],"url":"http://arxiv.org/abs/2403.18177v1","category":"q-fin.MF"}
{"created":"2024-03-27 01:02:46","title":"Summary of the CKM 2023 Working Group on $V_{ub}$, $V_{cb}$ and semileptonic/leptonic B decays including $\u03c4$","abstract":"This work summarizes recent results, both theoretical and experimental, in $B$-meson leptonic and semileptonic decays and metrology of $V_{ub}$ and $V_{cb}$, which were presented at the CKM 2023 workshop. We place these results in context and discuss future prospects in the field.","sentences":["This work summarizes recent results, both theoretical and experimental, in $B$-meson leptonic and semileptonic decays and metrology of $V_{ub}$ and $V_{cb}$, which were presented at the CKM 2023 workshop.","We place these results in context and discuss future prospects in the field."],"url":"http://arxiv.org/abs/2403.18175v1","category":"hep-ph"}
