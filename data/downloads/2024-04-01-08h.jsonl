{"created":"2024-03-29 17:59:53","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models","abstract":"This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs.","sentences":["This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD).","UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks.","UPD encompasses three distinct settings:","Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD).","To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements.","To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations.","We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs."],"url":"http://arxiv.org/abs/2403.20331v1","category":"cs.CV"}
{"created":"2024-03-29 17:59:06","title":"ReALM: Reference Resolution As Language Modeling","abstract":"Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.","sentences":["Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds.","This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background.","While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized.","This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality.","We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references.","We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it."],"url":"http://arxiv.org/abs/2403.20329v1","category":"cs.CL"}
{"created":"2024-03-29 17:56:40","title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","abstract":"We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.","sentences":["We present Gecko, a compact and versatile text embedding model.","Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever.","Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM.","Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM.","The effectiveness of our approach is demonstrated by the compactness of the Gecko.","On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size.","Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings."],"url":"http://arxiv.org/abs/2403.20327v1","category":"cs.CL"}
{"created":"2024-03-29 17:54:44","title":"Shaving Logs via Large Sieve Inequality: Faster Algorithms for Sparse Convolution and More","abstract":"In sparse convolution-type problems, a common technique is to hash the input integers modulo a random prime $p\\in [Q/2,Q]$ for some parameter $Q$, which reduces the range of the input integers while preserving their additive structure. However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from $[N]$ is $O(\\frac{\\log N}{Q})$ rather than $O(\\frac{1}{Q})$; (2) It is difficult to derandomize the choice of $p$; known derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein STOC'15].   In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory. Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model):   Sparse Nonnegative Convolution: We obtain an $O(t\\log t)$-time Las Vegas algorithm that computes the convolution $A\\star B$ of two nonnegative integer vectors $A,B$, where $t$ is the output sparsity $\\|A\\star B\\|_0$. Moreover, our algorithm terminates in $O(t\\log t)$ time with $1-1/\\mathrm{poly}(t)$ probability.   Text-to-Pattern Hamming Distances: Given a length-$m$ pattern $P$ and a length-$n$ text $T$, we obtain a deterministic $O(n\\sqrt{m\\log \\log m})$-time algorithm that exactly computes the Hamming distance between $P$ and every length-$m$ substring of $T$.   Sparse General Convolution: We also give a Monte Carlo $O(t\\log t)$ time algorithm for sparse convolution with possibly negative input in the restricted case where the length $N$ of the input vectors satisfies $N\\le t^{1.99}$.","sentences":["In sparse convolution-type problems, a common technique is to hash the input integers modulo a random prime $p\\in [Q/2,Q]$ for some parameter $Q$, which reduces the range of the input integers while preserving their additive structure.","However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from $[N]$ is $O(\\frac{\\log N}{Q})$ rather than $O(\\frac{1}{Q})$; (2) It is difficult to derandomize the choice of $p$; known derandomization techniques lead to super-logarithmic overhead","[Chan, Lewenstein STOC'15].   ","In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory.","Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model):   Sparse Nonnegative Convolution: We obtain an $O(t\\log t)$-time Las Vegas algorithm that computes the convolution $A\\star B$ of two nonnegative integer vectors $A,B$, where $t$ is the output sparsity $\\|A\\star B\\|_0$.","Moreover, our algorithm terminates in $O(t\\log t)$ time with $1-1/\\mathrm{poly}(t)$ probability.   ","Text-to-Pattern Hamming Distances: Given a length-$m$ pattern $P$ and a length-$n$ text $T$, we obtain a deterministic $O(n\\sqrt{m\\log \\log m})$-time algorithm that exactly computes the Hamming distance between $P$ and every length-$m$ substring of $T$.   Sparse General Convolution: We also give a Monte Carlo $O(t\\log t)$ time algorithm for sparse convolution with possibly negative input in the restricted case where the length $N$ of the input vectors satisfies $N\\le t^{1.99}$."],"url":"http://arxiv.org/abs/2403.20326v1","category":"cs.DS"}
{"created":"2024-03-29 17:44:25","title":"Structure and Dynamics of Magneto-Inertial, Differentially Rotating Laboratory Plasmas","abstract":"We present a detailed characterization of the structure and evolution of differentially rotating plasmas driven on the MAGPIE pulsed-power generator (1.4 MA peak current, 240 ns rise-time). The experiments were designed to simulate physics relevant to accretion discs and jets on laboratory scales. A cylindrical aluminium wire array Z pinch enclosed by return posts with an overall azimuthal off-set angle was driven to produce ablation plasma flows that propagate inwards in a slightly off-radial trajectory, injecting mass, angular momentum, and confining ram pressure to a rotating plasma column on the axis. However, the plasma is free to expand axially, forming a collimated, differentially rotating axial jet that propagates at $\\approx 100$ km/s. The density profile of the jet corresponds to a dense shell surrounding a low-density core, which is consistent with the centrifugal barrier effect being sustained along the jet's propagation. We show analytically that, as the rotating plasma accretes mass, conservation of mass and momentum implies plasma radial growth scaling as $r \\propto t^{1/3}$. As the characteristic moment of inertia increases, the rotation velocity is predicted to decrease and settle on a characteristic value $\\approx 20$ km/s. We find that both predictions are in agreement with Thomson scattering and optical self-emission imaging measurements.","sentences":["We present a detailed characterization of the structure and evolution of differentially rotating plasmas driven on the MAGPIE pulsed-power generator (1.4 MA peak current, 240 ns rise-time).","The experiments were designed to simulate physics relevant to accretion discs and jets on laboratory scales.","A cylindrical aluminium wire array Z pinch enclosed by return posts with an overall azimuthal off-set angle was driven to produce ablation plasma flows that propagate inwards in a slightly off-radial trajectory, injecting mass, angular momentum, and confining ram pressure to a rotating plasma column on the axis.","However, the plasma is free to expand axially, forming a collimated, differentially rotating axial jet that propagates at $\\approx 100$ km/s. The density profile of the jet corresponds to a dense shell surrounding a low-density core, which is consistent with the centrifugal barrier effect being sustained along the jet's propagation.","We show analytically that, as the rotating plasma accretes mass, conservation of mass and momentum implies plasma radial growth scaling as $r \\propto t^{1/3}$. As the characteristic moment of inertia increases, the rotation velocity is predicted to decrease and settle on a characteristic value $\\approx 20$ km/s. We find that both predictions are in agreement with Thomson scattering and optical self-emission imaging measurements."],"url":"http://arxiv.org/abs/2403.20321v1","category":"physics.plasm-ph"}
{"created":"2024-03-29 17:43:58","title":"MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning","abstract":"Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense prediction tasks. Our extensive experiments on the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream tasks compared to fully fine-tuning the MTL model while reducing the number of trainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal trade-off between the number of trainable parameters and the accuracy of the downstream tasks, outperforming current state-of-the-art parameter-efficient training methods in both accuracy and efficiency. Our code is publicly available.","sentences":["Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning.","Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters.","While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored.","In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models.","MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts.","We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense prediction tasks.","Our extensive experiments on the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream tasks compared to fully fine-tuning the MTL model while reducing the number of trainable parameters by 3.6x.","Furthermore, MTLoRA establishes a Pareto-optimal trade-off between the number of trainable parameters and the accuracy of the downstream tasks, outperforming current state-of-the-art parameter-efficient training methods in both accuracy and efficiency.","Our code is publicly available."],"url":"http://arxiv.org/abs/2403.20320v1","category":"cs.CV"}
{"created":"2024-03-29 17:41:57","title":"SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular 3D Detection of Large Objects","abstract":"Monocular 3D detectors achieve remarkable performance on cars and smaller objects. However, their performance drops on larger objects, leading to fatal accidents. Some attribute the failures to training data scarcity or their receptive field requirements of large objects. In this paper, we highlight this understudied problem of generalization to large objects. We find that modern frontal detectors struggle to generalize to large objects even on nearly balanced datasets. We argue that the cause of failure is the sensitivity of depth regression losses to noise of larger objects. To bridge this gap, we comprehensively investigate regression and dice losses, examining their robustness under varying error levels and object sizes. We mathematically prove that the dice loss leads to superior noise-robustness and model convergence for large objects compared to regression losses for a simplified case. Leveraging our theoretical insights, we propose SeaBird (Segmentation in Bird's View) as the first step towards generalizing to large objects. SeaBird effectively integrates BEV segmentation on foreground objects for 3D detection, with the segmentation head trained with the dice loss. SeaBird achieves SoTA results on the KITTI-360 leaderboard and improves existing detectors on the nuScenes leaderboard, particularly for large objects. Code and models at https://github.com/abhi1kumar/SeaBird","sentences":["Monocular 3D detectors achieve remarkable performance on cars and smaller objects.","However, their performance drops on larger objects, leading to fatal accidents.","Some attribute the failures to training data scarcity or their receptive field requirements of large objects.","In this paper, we highlight this understudied problem of generalization to large objects.","We find that modern frontal detectors struggle to generalize to large objects even on nearly balanced datasets.","We argue that the cause of failure is the sensitivity of depth regression losses to noise of larger objects.","To bridge this gap, we comprehensively investigate regression and dice losses, examining their robustness under varying error levels and object sizes.","We mathematically prove that the dice loss leads to superior noise-robustness and model convergence for large objects compared to regression losses for a simplified case.","Leveraging our theoretical insights, we propose SeaBird (Segmentation in Bird's View) as the first step towards generalizing to large objects.","SeaBird effectively integrates BEV segmentation on foreground objects for 3D detection, with the segmentation head trained with the dice loss.","SeaBird achieves SoTA results on the KITTI-360 leaderboard and improves existing detectors on the nuScenes leaderboard, particularly for large objects.","Code and models at https://github.com/abhi1kumar/SeaBird"],"url":"http://arxiv.org/abs/2403.20318v1","category":"cs.CV"}
{"created":"2024-03-29 17:40:37","title":"Convolutional Prompting meets Language Models for Continual Learning","abstract":"Continual Learning (CL) enables machine learning models to learn from continuously shifting new training data in absence of data from old tasks. Recently, pretrained vision transformers combined with prompt tuning have shown promise for overcoming catastrophic forgetting in CL. These approaches rely on a pool of learnable prompts which can be inefficient in sharing knowledge across tasks leading to inferior performance. In addition, the lack of fine-grained layer specific prompts does not allow these to fully express the strength of the prompts for CL. We address these limitations by proposing ConvPrompt, a novel convolutional prompt creation mechanism that maintains layer-wise shared embeddings, enabling both layer-specific learning and better concept transfer across tasks. The intelligent use of convolution enables us to maintain a low parameter overhead without compromising performance. We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned. Extensive experiments demonstrate the superiority of ConvPrompt and improves SOTA by ~3% with significantly less parameter overhead. We also perform strong ablation over various modules to disentangle the importance of different components.","sentences":["Continual Learning (CL) enables machine learning models to learn from continuously shifting new training data in absence of data from old tasks.","Recently, pretrained vision transformers combined with prompt tuning have shown promise for overcoming catastrophic forgetting in CL.","These approaches rely on a pool of learnable prompts which can be inefficient in sharing knowledge across tasks leading to inferior performance.","In addition, the lack of fine-grained layer specific prompts does not allow these to fully express the strength of the prompts for CL.","We address these limitations by proposing ConvPrompt, a novel convolutional prompt creation mechanism that maintains layer-wise shared embeddings, enabling both layer-specific learning and better concept transfer across tasks.","The intelligent use of convolution enables us to maintain a low parameter overhead without compromising performance.","We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned.","Extensive experiments demonstrate the superiority of ConvPrompt and improves SOTA by ~3% with significantly less parameter overhead.","We also perform strong ablation over various modules to disentangle the importance of different components."],"url":"http://arxiv.org/abs/2403.20317v1","category":"cs.CV"}
{"created":"2024-03-29 17:40:34","title":"Schr\u00f6dinger symmetry: a historical review","abstract":"This paper reviews the history of the conformal extension of Galilean symmetry, now called Schr\\\"odinger symmetry. In the physics literature, its discovery is commonly attributed to Jackiw, Niederer and Hagen (1972). However, Schr\\\"odinger symmetry has a much older ancestry: the associated conserved quantities were known to Jacobi in 1842/43 and its euclidean counterpart was discovered by Sophus Lie in 1881 in his studies of the heat equation. A convenient way to study Schr\\\"odinger symmetry is provided by a non-relativistic Kaluza-Klein-type \"Bargmann\" framework, first proposed by Eisenhart (1929), but then forgotten and re-discovered by Duval {\\it et al.} only in 1984. Representations of Schr\\\"odinger symmetry differ by the value $z=2$ of the dynamical exponent from the value $z=1$ found in representations of relativistic conformal invariance. For generic values of $z$, whole families of new algebras exist, which for $z=2/\\ell$ include the $\\ell$-conformal galilean algebras. We also review the non-relativistic limit of conformal algebras and that this limit leads to the $1$-conformal galilean algebra and not to the Schr\\\"odinger algebra. The latter can be recovered in the Bargmann framework through reduction. A distinctive feature of Galilean and Schr\\\"odinger symmetries are the Bargmann super-selection rules, algebraically related to a central extension. An empirical consequence of this was known as \"mass conservation\" already to Lavoisier. As an illustration of these concepts, some applications to physical ageing in simple model systems are reviewed.","sentences":["This paper reviews the history of the conformal extension of Galilean symmetry, now called Schr\\\"odinger symmetry.","In the physics literature, its discovery is commonly attributed to Jackiw, Niederer and Hagen (1972).","However, Schr\\\"odinger symmetry has a much older ancestry: the associated conserved quantities were known to Jacobi in 1842/43 and its euclidean counterpart was discovered by Sophus Lie in 1881 in his studies of the heat equation.","A convenient way to study Schr\\\"odinger symmetry is provided by a non-relativistic Kaluza-Klein-type \"Bargmann\" framework, first proposed by Eisenhart (1929), but then forgotten and re-discovered by Duval {\\it et al.} only in 1984.","Representations of Schr\\\"odinger symmetry differ by the value $z=2$ of the dynamical exponent from the value $z=1$ found in representations of relativistic conformal invariance.","For generic values of $z$, whole families of new algebras exist, which for $z=2/\\ell$ include the $\\ell$-conformal galilean algebras.","We also review the non-relativistic limit of conformal algebras and that this limit leads to the $1$-conformal galilean algebra and not to the Schr\\\"odinger algebra.","The latter can be recovered in the Bargmann framework through reduction.","A distinctive feature of Galilean and Schr\\\"odinger symmetries are the Bargmann super-selection rules, algebraically related to a central extension.","An empirical consequence of this was known as \"mass conservation\" already to Lavoisier.","As an illustration of these concepts, some applications to physical ageing in simple model systems are reviewed."],"url":"http://arxiv.org/abs/2403.20316v1","category":"hep-th"}
{"created":"2024-03-29 17:35:17","title":"Towards a turnkey approach to unbiased Monte Carlo estimation of smooth functions of expectations","abstract":"Given a smooth function $f$, we develop a general approach to turn Monte   Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$.   Specifically, we develop estimators that are based on randomly truncating   the Taylor series expansion of $f$ and estimating the coefficients of the   truncated series. We derive their properties and propose a strategy to set   their tuning parameters -- which depend on $m$ -- automatically, with a   view to make the whole approach simple to use. We develop our methods for   the specific functions $f(x)=\\log x$ and $f(x)=1/x$, as they arise in   several statistical applications such as maximum likelihood estimation of   latent variable models and Bayesian inference for un-normalised models.   Detailed numerical studies are performed for a range of applications to   determine how competitive and reliable the proposed approach is.","sentences":["Given a smooth function $f$, we develop a general approach to turn Monte   Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$.   ","Specifically, we develop estimators that are based on randomly truncating   the Taylor series expansion of $f$ and estimating the coefficients of the   truncated series.","We derive their properties and propose a strategy to set   their tuning parameters -- which depend on $m$ -- automatically, with a   view to make the whole approach simple to use.","We develop our methods for   the specific functions $f(x)=\\log x$ and $f(x)=1/x$, as they arise in   several statistical applications such as maximum likelihood estimation of   latent variable models and Bayesian inference for un-normalised models.   ","Detailed numerical studies are performed for a range of applications to   determine how competitive and reliable the proposed approach is."],"url":"http://arxiv.org/abs/2403.20313v1","category":"stat.ME"}
{"created":"2024-03-29 17:32:15","title":"Gravitational Waves on Kerr Black Holes I: Reconstruction of Linearized Metric Perturbations","abstract":"The gravitational perturbations of a rotating Kerr black hole are notoriously complicated, even at the linear level. In 1973, Teukolsky showed that their physical degrees of freedom are encoded in two gauge-invariant Weyl curvature scalars that obey a separable wave equation. Determining these scalars is sufficient for many purposes, such as the computation of energy fluxes. However, some applications -- such as second-order perturbation theory -- require the reconstruction of metric perturbations. In principle, this problem was solved long ago, but in practice, the solution has never been worked out explicitly. Here, we do so by writing down the metric perturbation (in either ingoing or outgoing radiation gauge) that corresponds to a given mode of either Weyl scalar. Our formulas make no reference to the Hertz potential (an intermediate quantity that plays no fundamental role) and involve only the radial and angular Kerr modes, but not their derivatives, which can be altogether eliminated using the Teukolsky--Starobinsky identities. We expect these analytic results to prove useful in numerical studies and for extending black hole perturbation theory beyond the linear regime.","sentences":["The gravitational perturbations of a rotating Kerr black hole are notoriously complicated, even at the linear level.","In 1973, Teukolsky showed that their physical degrees of freedom are encoded in two gauge-invariant Weyl curvature scalars that obey a separable wave equation.","Determining these scalars is sufficient for many purposes, such as the computation of energy fluxes.","However, some applications -- such as second-order perturbation theory -- require the reconstruction of metric perturbations.","In principle, this problem was solved long ago, but in practice, the solution has never been worked out explicitly.","Here, we do so by writing down the metric perturbation (in either ingoing or outgoing radiation gauge) that corresponds to a given mode of either Weyl scalar.","Our formulas make no reference to the Hertz potential (an intermediate quantity that plays no fundamental role) and involve only the radial and angular Kerr modes, but not their derivatives, which can be altogether eliminated using the Teukolsky--Starobinsky identities.","We expect these analytic results to prove useful in numerical studies and for extending black hole perturbation theory beyond the linear regime."],"url":"http://arxiv.org/abs/2403.20311v1","category":"gr-qc"}
{"created":"2024-03-29 17:32:02","title":"Predicting the impact of e-commerce indices on international trade in Iran and other selected members of the Organization for Economic Co-operation and Development (OECD) by using the artificial intelligence and P-VAR model","abstract":"This study aims at predicting the impact of e-commerce indicators on international trade of the selected OECD countries and Iran, by using the artificial intelligence approach and P-VAR. According to the nature of export, import, GDP, and ICT functions, and the characteristics of nonlinearity, this analysis is performed by using the MPL neural network. The export, import, GDP, and ICT findings were examined with 99 percent accuracy. Using the P-VAR model in the Eviews software, the initial database and predicted data were applied to estimate the impact of e-commerce on international trade. The findings from analyzing the data show that there is a bilateral correlation between e-commerce which means that ICT and international trade affect each other and the Goodness of fit of the studied model is confirmed.","sentences":["This study aims at predicting the impact of e-commerce indicators on international trade of the selected OECD countries and Iran, by using the artificial intelligence approach and P-VAR.","According to the nature of export, import, GDP, and ICT functions, and the characteristics of nonlinearity, this analysis is performed by using the MPL neural network.","The export, import, GDP, and ICT findings were examined with 99 percent accuracy.","Using the P-VAR model in the Eviews software, the initial database and predicted data were applied to estimate the impact of e-commerce on international trade.","The findings from analyzing the data show that there is a bilateral correlation between e-commerce which means that ICT and international trade affect each other and the Goodness of fit of the studied model is confirmed."],"url":"http://arxiv.org/abs/2403.20310v1","category":"econ.GN"}
{"created":"2024-03-29 17:22:53","title":"ChainNet: Structured Metaphor and Metonymy in WordNet","abstract":"The senses of a word exhibit rich internal structure. In a typical lexicon, this structure is overlooked: a word's senses are encoded as a list without inter-sense relations. We present ChainNet, a lexical resource which for the first time explicitly identifies these structures. ChainNet expresses how senses in the Open English Wordnet are derived from one another: every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected in the case of homonymy. Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy.","sentences":["The senses of a word exhibit rich internal structure.","In a typical lexicon, this structure is overlooked: a word's senses are encoded as a list without inter-sense relations.","We present ChainNet, a lexical resource which for the first time explicitly identifies these structures.","ChainNet expresses how senses in the Open English Wordnet are derived from one another: every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected in the case of homonymy.","Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy."],"url":"http://arxiv.org/abs/2403.20308v1","category":"cs.CL"}
{"created":"2024-03-29 17:22:49","title":"Optimal Communication for Classic Functions in the Coordinator Model and Beyond","abstract":"In the coordinator model of communication with $s$ servers, given an arbitrary non-negative function $f$, we study the problem of approximating the sum $\\sum_{i \\in [n]}f(x_i)$ up to a $1 \\pm \\varepsilon$ factor. Here the vector $x \\in R^n$ is defined to be $x = x(1) + \\cdots + x(s)$, where $x(j) \\ge 0$ denotes the non-negative vector held by the $j$-th server. A special case of the problem is when $f(x) = x^k$ which corresponds to the well-studied problem of $F_k$ moment estimation in the distributed communication model. We introduce a new parameter $c_f[s]$ which captures the communication complexity of approximating $\\sum_{i\\in [n]} f(x_i)$ and for a broad class of functions $f$ which includes $f(x) = x^k$ for $k \\ge 2$ and other robust functions such as the Huber loss function, we give a two round protocol that uses total communication $c_f[s]/\\varepsilon^2$ bits, up to polylogarithmic factors. For this broad class of functions, our result improves upon the communication bounds achieved by Kannan, Vempala, and Woodruff (COLT 2014) and Woodruff and Zhang (STOC 2012), obtaining the optimal communication up to polylogarithmic factors in the minimum number of rounds. We show that our protocol can also be used for approximating higher-order correlations.   Apart from the coordinator model, algorithms for other graph topologies in which each node is a server have been extensively studied. We argue that directly lifting protocols leads to inefficient algorithms. Hence, a natural question is the problems that can be efficiently solved in general graph topologies. We give communication efficient protocols in the so-called personalized CONGEST model for solving linear regression and low rank approximation by designing composable sketches. Our sketch construction may be of independent interest and can implement any importance sampling procedure that has a monotonicity property.","sentences":["In the coordinator model of communication with $s$ servers, given an arbitrary non-negative function $f$, we study the problem of approximating the sum $\\sum_{i \\in","[n]}f(x_i)$ up to a $1 \\pm \\varepsilon$ factor.","Here the vector $x \\in R^n$ is defined to be $x = x(1)","+","\\cdots + x(s)$, where $x(j) \\ge 0$ denotes the non-negative vector held by the $j$-th server.","A special case of the problem is when $f(x) = x^k$ which corresponds to the well-studied problem of $F_k$ moment estimation in the distributed communication model.","We introduce a new parameter $c_f[s]$ which captures the communication complexity of approximating $\\sum_{i\\in [n]} f(x_i)$ and for a broad class of functions $f$ which includes $f(x) = x^k$ for $k \\ge 2$ and other robust functions such as the Huber loss function, we give a two round protocol that uses total communication $c_f[s]/\\varepsilon^2$ bits, up to polylogarithmic factors.","For this broad class of functions, our result improves upon the communication bounds achieved by Kannan, Vempala, and Woodruff (COLT 2014) and Woodruff and Zhang (STOC 2012), obtaining the optimal communication up to polylogarithmic factors in the minimum number of rounds.","We show that our protocol can also be used for approximating higher-order correlations.   ","Apart from the coordinator model, algorithms for other graph topologies in which each node is a server have been extensively studied.","We argue that directly lifting protocols leads to inefficient algorithms.","Hence, a natural question is the problems that can be efficiently solved in general graph topologies.","We give communication efficient protocols in the so-called personalized CONGEST model for solving linear regression and low rank approximation by designing composable sketches.","Our sketch construction may be of independent interest and can implement any importance sampling procedure that has a monotonicity property."],"url":"http://arxiv.org/abs/2403.20307v1","category":"cs.DS"}
{"created":"2024-03-29 17:22:48","title":"Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference","abstract":"With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.","sentences":["With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding.","Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models.","Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models.","In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs.","We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient.","We characterize the impact of these knobs on the latency, throughput, as well as the energy.","By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments."],"url":"http://arxiv.org/abs/2403.20306v1","category":"cs.AI"}
{"created":"2024-03-29 17:20:12","title":"Local Correction of Linear Functions over the Boolean Cube","abstract":"We consider the task of locally correcting, and locally list-correcting, multivariate linear functions over the domain $\\{0,1\\}^n$ over arbitrary fields and more generally Abelian groups. Such functions form error-correcting codes of relative distance $1/2$ and we give local-correction algorithms correcting up to nearly $1/4$-fraction errors making $\\widetilde{\\mathcal{O}}(\\log n)$ queries. This query complexity is optimal up to $\\mathrm{poly}(\\log\\log n)$ factors. We also give local list-correcting algorithms correcting $(1/2 - \\varepsilon)$-fraction errors with $\\widetilde{\\mathcal{O}}_{\\varepsilon}(\\log n)$ queries.   These results may be viewed as natural generalizations of the classical work of Goldreich and Levin whose work addresses the special case where the underlying group is $\\mathbb{Z}_2$. By extending to the case where the underlying group is, say, the reals, we give the first non-trivial locally correctable codes (LCCs) over the reals (with query complexity being sublinear in the dimension (also known as message length)).   The central challenge in constructing the local corrector is constructing ``nearly balanced vectors'' over $\\{-1,1\\}^n$ that span $1^n$ -- we show how to construct $\\mathcal{O}(\\log n)$ vectors that do so, with entries in each vector summing to $\\pm1$. The challenge to the local-list-correction algorithms, given the local corrector, is principally combinatorial, i.e., in proving that the number of linear functions within any Hamming ball of radius $(1/2-\\varepsilon)$ is $\\mathcal{O}_{\\varepsilon}(1)$. Getting this general result covering every Abelian group requires integrating a variety of known methods with some new combinatorial ingredients analyzing the structural properties of codewords that lie within small Hamming balls.","sentences":["We consider the task of locally correcting, and locally list-correcting, multivariate linear functions over the domain $\\{0,1\\}^n$ over arbitrary fields and more generally Abelian groups.","Such functions form error-correcting codes of relative distance $1/2$ and we give local-correction algorithms correcting up to nearly $1/4$-fraction errors making $\\widetilde{\\mathcal{O}}(\\log n)$ queries.","This query complexity is optimal up to $\\mathrm{poly}(\\log\\log n)$ factors.","We also give local list-correcting algorithms correcting $(1/2 - \\varepsilon)$-fraction errors with $\\widetilde{\\mathcal{O}}_{\\varepsilon}(\\log n)$ queries.   ","These results may be viewed as natural generalizations of the classical work of Goldreich and Levin whose work addresses the special case where the underlying group is $\\mathbb{Z}_2$. By extending to the case where the underlying group is, say, the reals, we give the first non-trivial locally correctable codes (LCCs) over the reals (with query complexity being sublinear in the dimension (also known as message length)).   ","The central challenge in constructing the local corrector is constructing ``nearly balanced vectors'' over $\\{-1,1\\}^n$ that span $1^n$ -- we show how to construct $\\mathcal{O}(\\log n)$ vectors that do so, with entries in each vector summing to $\\pm1$. The challenge to the local-list-correction algorithms, given the local corrector, is principally combinatorial, i.e., in proving that the number of linear functions within any Hamming ball of radius $(1/2-\\varepsilon)$ is $\\mathcal{O}_{\\varepsilon}(1)$. Getting this general result covering every Abelian group requires integrating a variety of known methods with some new combinatorial ingredients analyzing the structural properties of codewords that lie within small Hamming balls."],"url":"http://arxiv.org/abs/2403.20305v1","category":"cs.CC"}
{"created":"2024-03-29 17:19:29","title":"Pandigital and penholodigital numbers","abstract":"Pandigital and penholodigital numbers are numbers that contain every digit or nonzero digit respectively. We study properties of pandigital or penholodigital numbers that are also squares or prime.","sentences":["Pandigital and penholodigital numbers are numbers that contain every digit or nonzero digit respectively.","We study properties of pandigital or penholodigital numbers that are also squares or prime."],"url":"http://arxiv.org/abs/2403.20304v1","category":"math.GM"}
{"created":"2024-03-29 17:17:41","title":"A microstructure-sensitive electro-chemo-mechanical phase-field model of pitting and stress corrosion cracking","abstract":"An electro-chemo-mechanical phase-field formulation is developed to simulate pitting and stress corrosion in polycrystalline materials. The formulation incorporates dependencies of mechanical properties and corrosion potential on crystallographic orientation. The model considers the formation and charging dynamics of an electric double layer through a new general boundary condition for the solution potential. The potential of the model is demonstrated by simulating corrosion in polycrystalline materials with various grain morphology distributions. The results show that incorporating the underlying microstructure yields more extensive defects, faster defect kinetics, and irregular pit and crack shapes relative to a microstructurally-insensitive homogeneous material scenario.","sentences":["An electro-chemo-mechanical phase-field formulation is developed to simulate pitting and stress corrosion in polycrystalline materials.","The formulation incorporates dependencies of mechanical properties and corrosion potential on crystallographic orientation.","The model considers the formation and charging dynamics of an electric double layer through a new general boundary condition for the solution potential.","The potential of the model is demonstrated by simulating corrosion in polycrystalline materials with various grain morphology distributions.","The results show that incorporating the underlying microstructure yields more extensive defects, faster defect kinetics, and irregular pit and crack shapes relative to a microstructurally-insensitive homogeneous material scenario."],"url":"http://arxiv.org/abs/2403.20301v1","category":"cs.CE"}
{"created":"2024-03-29 17:16:20","title":"Improving Learnt Local MAPF Policies with Heuristic Search","abstract":"Multi-agent path finding (MAPF) is the problem of finding collision-free paths for a team of agents to reach their goal locations. State-of-the-art classical MAPF solvers typically employ heuristic search to find solutions for hundreds of agents but are typically centralized and can struggle to scale when run with short timeouts. Machine learning (ML) approaches that learn policies for each agent are appealing as these could enable decentralized systems and scale well while maintaining good solution quality. Current ML approaches to MAPF have proposed methods that have started to scratch the surface of this potential. However, state-of-the-art ML approaches produce \"local\" policies that only plan for a single timestep and have poor success rates and scalability. Our main idea is that we can improve a ML local policy by using heuristic search methods on the output probability distribution to resolve deadlocks and enable full horizon planning. We show several model-agnostic ways to use heuristic search with learnt policies that significantly improve the policies' success rates and scalability. To our best knowledge, we demonstrate the first time ML-based MAPF approaches have scaled to high congestion scenarios (e.g. 20% agent density).","sentences":["Multi-agent path finding (MAPF) is the problem of finding collision-free paths for a team of agents to reach their goal locations.","State-of-the-art classical MAPF solvers typically employ heuristic search to find solutions for hundreds of agents but are typically centralized and can struggle to scale when run with short timeouts.","Machine learning (ML) approaches that learn policies for each agent are appealing as these could enable decentralized systems and scale well while maintaining good solution quality.","Current ML approaches to MAPF have proposed methods that have started to scratch the surface of this potential.","However, state-of-the-art ML approaches produce \"local\" policies that only plan for a single timestep and have poor success rates and scalability.","Our main idea is that we can improve a ML local policy by using heuristic search methods on the output probability distribution to resolve deadlocks and enable full horizon planning.","We show several model-agnostic ways to use heuristic search with learnt policies that significantly improve the policies' success rates and scalability.","To our best knowledge, we demonstrate the first time ML-based MAPF approaches have scaled to high congestion scenarios (e.g. 20% agent density)."],"url":"http://arxiv.org/abs/2403.20300v1","category":"cs.MA"}
{"created":"2024-03-29 17:13:33","title":"Balanced Data Placement for GEMV Acceleration with Processing-In-Memory","abstract":"With unprecedented demand for generative AI (GenAI) inference, acceleration of primitives that dominate GenAI such as general matrix-vector multiplication (GEMV) is receiving considerable attention. A challenge with GEMVs is the high memory bandwidth this primitive demands. Multiple memory vendors have proposed commercially viable processing-in-memory (PIM) prototypes that attain bandwidth boost over processor via augmenting memory banks with compute capabilities and broadcasting same command to all banks. While proposed PIM designs stand to accelerate GEMV, we observe in this work that a key impediment to truly harness PIM acceleration is deducing optimal data-placement to place the matrix in memory banks. To this end, we tease out several factors that impact data-placement and propose PIMnast methodology which, like a gymnast, balances these factors to identify data-placements that deliver GEMV acceleration. Across a spectrum of GenAI models, our proposed PIMnast methodology along with additional orchestration knobs we identify delivers up to 6.86$\\times$ speedup for GEMVs (of the available 7$\\times$ roofline speedup) leading to up to 5$\\times$ speedup for per-token latencies.","sentences":["With unprecedented demand for generative AI (GenAI) inference, acceleration of primitives that dominate GenAI such as general matrix-vector multiplication (GEMV) is receiving considerable attention.","A challenge with GEMVs is the high memory bandwidth this primitive demands.","Multiple memory vendors have proposed commercially viable processing-in-memory (PIM) prototypes that attain bandwidth boost over processor via augmenting memory banks with compute capabilities and broadcasting same command to all banks.","While proposed PIM designs stand to accelerate GEMV, we observe in this work that a key impediment to truly harness PIM acceleration is deducing optimal data-placement to place the matrix in memory banks.","To this end, we tease out several factors that impact data-placement and propose PIMnast methodology which, like a gymnast, balances these factors to identify data-placements that deliver GEMV acceleration.","Across a spectrum of GenAI models, our proposed PIMnast methodology along with additional orchestration knobs we identify delivers up to 6.86$\\times$ speedup for GEMVs (of the available 7$\\times$ roofline speedup) leading to up to 5$\\times$ speedup for per-token latencies."],"url":"http://arxiv.org/abs/2403.20297v1","category":"cs.AR"}
{"created":"2024-03-29 17:10:20","title":"Active flow control of a turbulent separation bubble through deep reinforcement learning","abstract":"The control efficacy of classical periodic forcing and deep reinforcement learning (DRL) is assessed for a turbulent separation bubble (TSB) at $Re_\\tau=180$ on the upstream region before separation occurs. The TSB can resemble a separation phenomenon naturally arising in wings, and a successful reduction of the TSB can have practical implications in the reduction of the aviation carbon footprint. We find that the classical zero-net-mas-flux (ZNMF) periodic control is able to reduce the TSB by 15.7%. On the other hand, the DRL-based control achieves 25.3% reduction and provides a smoother control strategy while also being ZNMF. To the best of our knowledge, the current test case is the highest Reynolds-number flow that has been successfully controlled using DRL to this date. In future work, these results will be scaled to well-resolved large-eddy simulation grids. Furthermore, we provide details of our open-source CFD-DRL framework suited for the next generation of exascale computing machines.","sentences":["The control efficacy of classical periodic forcing and deep reinforcement learning (DRL) is assessed for a turbulent separation bubble (TSB) at $Re_\\tau=180$ on the upstream region before separation occurs.","The TSB can resemble a separation phenomenon naturally arising in wings, and a successful reduction of the TSB can have practical implications in the reduction of the aviation carbon footprint.","We find that the classical zero-net-mas-flux (ZNMF) periodic control is able to reduce the TSB by 15.7%.","On the other hand, the DRL-based control achieves 25.3% reduction and provides a smoother control strategy while also being ZNMF.","To the best of our knowledge, the current test case is the highest Reynolds-number flow that has been successfully controlled using DRL to this date.","In future work, these results will be scaled to well-resolved large-eddy simulation grids.","Furthermore, we provide details of our open-source CFD-DRL framework suited for the next generation of exascale computing machines."],"url":"http://arxiv.org/abs/2403.20295v1","category":"physics.flu-dyn"}
{"created":"2024-03-29 17:00:55","title":"Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation","abstract":"Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.","sentences":["Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation.","Effectively generating representations for utterances remains a significant challenge in this task.","Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness.","To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions.","To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions.","Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance.","Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions.","Our code is available at https://github.com/Yu-Fangxu/EACL."],"url":"http://arxiv.org/abs/2403.20289v1","category":"cs.CL"}
{"created":"2024-03-29 16:59:13","title":"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain","abstract":"We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.","sentences":["We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks.","We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios.","We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician.","Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making.","For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice.","Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area."],"url":"http://arxiv.org/abs/2403.20288v1","category":"cs.CL"}
{"created":"2024-03-29 16:58:13","title":"Benchmarking Counterfactual Image Generation","abstract":"Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further evaluate and benchmark existing and future counterfactual image generation methods. Our framework is extendable to additional SCM and other causal methods, generative models, and datasets.","sentences":["Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data.","However, evaluating image generation is a long-standing challenge in itself.","The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths.","In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods.","We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism.","We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm.","Our work is accompanied by a user-friendly Python package which allows to further evaluate and benchmark existing and future counterfactual image generation methods.","Our framework is extendable to additional SCM and other causal methods, generative models, and datasets."],"url":"http://arxiv.org/abs/2403.20287v1","category":"cs.CV"}
{"created":"2024-03-29 16:54:43","title":"On generic singularities of intersections of ellipsoids: the octahedron","abstract":"The goal of this work is to study the smoothings of singular coaxial intersections of ellipsoids (where coaxial includes concentric) with generic singularities, with special attention to the 3-dimensional case.","sentences":["The goal of this work is to study the smoothings of singular coaxial intersections of ellipsoids (where coaxial includes concentric) with generic singularities, with special attention to the 3-dimensional case."],"url":"http://arxiv.org/abs/2403.20286v1","category":"math.GT"}
{"created":"2024-03-29 16:53:11","title":"LayerNorm: A key component in parameter-efficient fine-tuning","abstract":"Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can reach comparable, or in some cases better, performance to full fine-tuning and other parameter-efficient fine-tuning methods. Moreover, we use Fisher information to determine the most critical subset of LayerNorm and demonstrate that many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a small portion of LayerNorm with negligible performance degradation.","sentences":["Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks.","However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive.","One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged.","Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning.","In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning.","We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks.","Then we show that only fine-tuning the LayerNorm can reach comparable, or in some cases better, performance to full fine-tuning and other parameter-efficient fine-tuning methods.","Moreover, we use Fisher information to determine the most critical subset of LayerNorm and demonstrate that many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a small portion of LayerNorm with negligible performance degradation."],"url":"http://arxiv.org/abs/2403.20284v1","category":"cs.CL"}
{"created":"2024-03-29 16:49:40","title":"Sparse multimodal fusion with modal channel attention","abstract":"The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.","sentences":["The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity.","An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA).","Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics.","Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples.","It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks."],"url":"http://arxiv.org/abs/2403.20280v1","category":"cs.LG"}
{"created":"2024-03-29 16:49:24","title":"LUQ: Long-text Uncertainty Quantification for LLMs","abstract":"Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality. We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about. To further improve the factual accuracy of LLM responses, we propose a method called \\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks.","Despite their effectiveness, these models are prone to generate nonfactual content.","Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs.","Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses.","However, real-world applications frequently necessitate much longer responses.","Our study first highlights the limitations of current UQ methods in handling long text generation.","We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text.","Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro).","With \\textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality.","We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about.","To further improve the factual accuracy of LLM responses, we propose a method called \\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty.","The ensembling method greatly improves the response factuality upon the best standalone LLM."],"url":"http://arxiv.org/abs/2403.20279v1","category":"cs.CL"}
{"created":"2024-03-29 16:27:40","title":"CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data","abstract":"Tropical forests are a key component of the global carbon cycle. With plans for upcoming space-borne missions like BIOMASS to monitor forestry, several airborne missions, including TropiSAR and AfriSAR campaigns, have been successfully launched and experimented. Typical Synthetic Aperture Radar Tomography (TomoSAR) methods involve complex models with low accuracy and high computation costs. In recent years, deep learning methods have also gained attention in the TomoSAR framework, showing interesting performance. Recently, a solution based on a fully connected Tomographic Neural Network (TSNN) has demonstrated its effectiveness in accurately estimating forest and ground heights by exploiting the pixel-wise elements of the covariance matrix derived from TomoSAR data. This work instead goes beyond the pixel-wise approach to define a context-aware deep learning-based solution named CATSNet. A convolutional neural network is considered to leverage patch-based information and extract features from a neighborhood rather than focus on a single pixel. The training is conducted by considering TomoSAR data as the input and Light Detection and Ranging (LiDAR) values as the ground truth. The experimental results show striking advantages in both performance and generalization ability by leveraging context information within Multiple Baselines (MB) TomoSAR data across different polarimetric modalities, surpassing existing techniques.","sentences":["Tropical forests are a key component of the global carbon cycle.","With plans for upcoming space-borne missions like BIOMASS to monitor forestry, several airborne missions, including TropiSAR and AfriSAR campaigns, have been successfully launched and experimented.","Typical Synthetic Aperture Radar Tomography (TomoSAR) methods involve complex models with low accuracy and high computation costs.","In recent years, deep learning methods have also gained attention in the TomoSAR framework, showing interesting performance.","Recently, a solution based on a fully connected Tomographic Neural Network (TSNN) has demonstrated its effectiveness in accurately estimating forest and ground heights by exploiting the pixel-wise elements of the covariance matrix derived from TomoSAR data.","This work instead goes beyond the pixel-wise approach to define a context-aware deep learning-based solution named CATSNet.","A convolutional neural network is considered to leverage patch-based information and extract features from a neighborhood rather than focus on a single pixel.","The training is conducted by considering TomoSAR data as the input and Light Detection and Ranging (LiDAR) values as the ground truth.","The experimental results show striking advantages in both performance and generalization ability by leveraging context information within Multiple Baselines (MB) TomoSAR data across different polarimetric modalities, surpassing existing techniques."],"url":"http://arxiv.org/abs/2403.20273v1","category":"cs.CV"}
{"created":"2024-03-29 16:26:20","title":"Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want","abstract":"The interaction between humans and artificial intelligence (AI) is a crucial factor that reflects the effectiveness of multimodal large language models (MLLMs). However, current MLLMs primarily focus on image-level comprehension and limit interaction to textual instructions, thereby constraining their flexibility in usage and depth of response. In this paper, we introduce the Draw-and-Understand project: a new model, a multi-domain dataset, and a challenging benchmark for visual prompting. Specifically, we propose SPHINX-V, a new end-to-end trained Multimodal Large Language Model (MLLM) that connects a vision encoder, a visual prompt encoder and an LLM for various visual prompts (points, bounding boxes, and free-form shape) and language understanding. To advance visual prompting research for MLLMs, we introduce MDVP-Data and MDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M unique image-visual prompt-text instruction-following samples, including natural images, document images, OCR images, mobile screenshots, web screenshots, and multi-panel images. Furthermore, we present MDVP-Bench, a comprehensive and challenging benchmark to assess a model's capability in understanding visual prompting instructions. Our experiments demonstrate SPHINX-V's impressive multimodal interaction capabilities through visual prompting, revealing significant improvements in detailed pixel-level description and question-answering abilities.","sentences":["The interaction between humans and artificial intelligence (AI) is a crucial factor that reflects the effectiveness of multimodal large language models (MLLMs).","However, current MLLMs primarily focus on image-level comprehension and limit interaction to textual instructions, thereby constraining their flexibility in usage and depth of response.","In this paper, we introduce the Draw-and-Understand project: a new model, a multi-domain dataset, and a challenging benchmark for visual prompting.","Specifically, we propose SPHINX-V, a new end-to-end trained Multimodal Large Language Model (MLLM) that connects a vision encoder, a visual prompt encoder and an LLM for various visual prompts (points, bounding boxes, and free-form shape) and language understanding.","To advance visual prompting research for MLLMs, we introduce MDVP-Data and MDVP-Bench.","MDVP-Data features a multi-domain dataset containing 1.6M unique image-visual prompt-text instruction-following samples, including natural images, document images, OCR images, mobile screenshots, web screenshots, and multi-panel images.","Furthermore, we present MDVP-Bench, a comprehensive and challenging benchmark to assess a model's capability in understanding visual prompting instructions.","Our experiments demonstrate SPHINX-V's impressive multimodal interaction capabilities through visual prompting, revealing significant improvements in detailed pixel-level description and question-answering abilities."],"url":"http://arxiv.org/abs/2403.20271v1","category":"cs.CV"}
{"created":"2024-03-29 16:20:07","title":"In Violation of the Prime Directive: Simulating detriments to Delta-Quadrant civilizations from the starship Voyager's impact on planetary rings","abstract":"In the seven years that the starship Voyager spent in the Delta Quadrant, it used many questionable techniques to engage with alien civilizations and ultimately find its way home. From detailed studies of their logs and opening credits, we simulate Voyager's practice of orbiting a planet, to examine the effect on planetary rings. We outline a feasible planetary system and simulate the extent to which its rings would be disrupted. We find that Voyager's orbit could inflate the height of the rings in the vicinity of the spacecraft by a factor of 2, as well as increase the relative speeds of neighboring planetesimals within the rings. This increase in ring thickness has the potential to alter shadows on any moons of this planet, impacting ring-shadow based religions. Additionally, the acceleration of these planetesimals could rival their gravity, bucking any alien inhabitants and their tiny civilizations off of their planetesimal homeworlds. Finally, we posit that due to increased collisions amongst the planetesimals (which may harbor tiny intelligent life) the trajectory of these civilizations may be forever altered, violating the prime directive.","sentences":["In the seven years that the starship Voyager spent in the Delta Quadrant, it used many questionable techniques to engage with alien civilizations and ultimately find its way home.","From detailed studies of their logs and opening credits, we simulate Voyager's practice of orbiting a planet, to examine the effect on planetary rings.","We outline a feasible planetary system and simulate the extent to which its rings would be disrupted.","We find that Voyager's orbit could inflate the height of the rings in the vicinity of the spacecraft by a factor of 2, as well as increase the relative speeds of neighboring planetesimals within the rings.","This increase in ring thickness has the potential to alter shadows on any moons of this planet, impacting ring-shadow based religions.","Additionally, the acceleration of these planetesimals could rival their gravity, bucking any alien inhabitants and their tiny civilizations off of their planetesimal homeworlds.","Finally, we posit that due to increased collisions amongst the planetesimals (which may harbor tiny intelligent life) the trajectory of these civilizations may be forever altered, violating the prime directive."],"url":"http://arxiv.org/abs/2403.20268v1","category":"astro-ph.EP"}
{"created":"2024-03-29 16:18:10","title":"Counterdiabatic, Better, Faster, Stronger: Optimal control for approximate counterdiabatic driving","abstract":"Adiabatic protocols are employed across a variety of quantum technologies, from implementing state preparation and individual operations that are building blocks of larger devices, to higher-level protocols in quantum annealing and adiabatic quantum computation. The main drawback of adiabatic processes, however, is that they require prohibitively long timescales. This generally leads to losses due to decoherence and heating processes. The problem of speeding up system dynamics while retaining the adiabatic condition has garnered a large amount of interest, resulting in a whole host of diverse methods and approaches made for this purpose. This thesis is dedicated to the discovery of new ways to combine optimal control techniques with a universal method from STA: counterdiabatic driving (CD). The CD approach offers perfect suppression of all non-adiabatic effects experienced by a system driven by a time-dependent Hamiltonian regardless of how fast the process occurs. In practice, however, exact CD is difficult to derive often even more difficult to implement. The main result presented in the thesis is thus the development of a new method called counterdiabatic optimized local driving (COLD), which implements optimal control techniques in tandem with \\emph{approximations} of exact CD in a way that maximises suppression of non-adiabatic effects.","sentences":["Adiabatic protocols are employed across a variety of quantum technologies, from implementing state preparation and individual operations that are building blocks of larger devices, to higher-level protocols in quantum annealing and adiabatic quantum computation.","The main drawback of adiabatic processes, however, is that they require prohibitively long timescales.","This generally leads to losses due to decoherence and heating processes.","The problem of speeding up system dynamics while retaining the adiabatic condition has garnered a large amount of interest, resulting in a whole host of diverse methods and approaches made for this purpose.","This thesis is dedicated to the discovery of new ways to combine optimal control techniques with a universal method from STA: counterdiabatic driving (CD).","The CD approach offers perfect suppression of all non-adiabatic effects experienced by a system driven by a time-dependent Hamiltonian regardless of how fast the process occurs.","In practice, however, exact CD is difficult to derive often even more difficult to implement.","The main result presented in the thesis is thus the development of a new method called counterdiabatic optimized local driving (COLD), which implements optimal control techniques in tandem with \\emph{approximations} of exact CD in a way that maximises suppression of non-adiabatic effects."],"url":"http://arxiv.org/abs/2403.20267v1","category":"quant-ph"}
{"created":"2024-03-29 16:16:48","title":"Latxa: An Open Language Model and Evaluation Suite for Basque","abstract":"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses at https://github.com/hitz-zentroa/latxa. Our suite enables reproducible research on methods to build LLMs for low-resource languages.","sentences":["We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters.","Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.","Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations.","In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin.","In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks.","Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses at https://github.com/hitz-zentroa/latxa.","Our suite enables reproducible research on methods to build LLMs for low-resource languages."],"url":"http://arxiv.org/abs/2403.20266v1","category":"cs.CL"}
{"created":"2024-03-29 16:15:56","title":"Rigidity of Euclidean product structure: breakdown for low Sobolev exponents","abstract":"We develop a general toolbox to study $W^{1,p}$ solutions of differential inclusions $\\nabla u \\in K$ for unbounded sets $K$. A key notion is the concept that a subset $K$ of the space $\\mathbb{R}^{d \\times m}$ of $d \\times m$ matrices can be reduced to another set $K'$. We then use this framework to show that the product rigidity for Sobolev maps fails for $p<2$, and also apply our toolbox to simplify several examples from the literature.","sentences":["We develop a general toolbox to study $W^{1,p}$ solutions of differential inclusions $\\nabla u \\in K$ for unbounded sets $K$. A key notion is the concept that a subset $K$ of the space $\\mathbb{R}^{d \\times m}$ of $d \\times m$ matrices can be reduced to another set $K'$. We then use this framework to show that the product rigidity for Sobolev maps fails for $p<2$, and also apply our toolbox to simplify several examples from the literature."],"url":"http://arxiv.org/abs/2403.20265v1","category":"math.AP"}
{"created":"2024-03-29 16:15:31","title":"Infinitesimal calculations in fundamental groups","abstract":"We show that Hopf invariants, defined by evaluation in Harrison cohomology of the commutative cochains of a space, calculate the logarithm map from a fundamental group to its Malcev Lie algebra. They thus present the zeroth Harrison cohomology as a universal dual object to the Malcev Lie algebra. This structural theorem supports explicit calculations in algebraic topology, geometric topology and combinatorial group theory. In particular, we give the first algorithm to determine whether a word raised to some power is a k-fold nested product of commutators, in any group presented by generators and relations.","sentences":["We show that Hopf invariants, defined by evaluation in Harrison cohomology of the commutative cochains of a space, calculate the logarithm map from a fundamental group to its Malcev Lie algebra.","They thus present the zeroth Harrison cohomology as a universal dual object to the Malcev Lie algebra.","This structural theorem supports explicit calculations in algebraic topology, geometric topology and combinatorial group theory.","In particular, we give the first algorithm to determine whether a word raised to some power is a k-fold nested product of commutators, in any group presented by generators and relations."],"url":"http://arxiv.org/abs/2403.20264v1","category":"math.AT"}
{"created":"2024-03-29 16:13:31","title":"ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models","abstract":"Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation. We also provide a thorough analysis of our GPT-4-based evaluation method, encompassing insights from a crowdsourcing study. Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability to differentiate among more than three score levels may be limited.","sentences":["Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents.","While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications.","In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario.","In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data.","Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers.","Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation.","We also provide a thorough analysis of our GPT-4-based evaluation method, encompassing insights from a crowdsourcing study.","Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability to differentiate among more than three score levels may be limited."],"url":"http://arxiv.org/abs/2403.20262v1","category":"cs.CL"}
{"created":"2024-03-29 16:10:34","title":"FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation","abstract":"Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective sampling technique coupled with a confidence model, requiring only minor adjustments to the regression framework of FABind. Experimental results and analysis reveal that FABind+ remarkably outperforms the original FABind, achieves competitive state-of-the-art performance, and delivers insightful modeling strategies. This demonstrates FABind+ represents a substantial step forward in molecular docking and drug discovery. Our code is in https://github.com/QizhiPei/FABind.","sentences":["Molecular docking is a pivotal process in drug discovery.","While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly.","The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency.","Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor.","We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process.","Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities.","In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective sampling technique coupled with a confidence model, requiring only minor adjustments to the regression framework of FABind.","Experimental results and analysis reveal that FABind+ remarkably outperforms the original FABind, achieves competitive state-of-the-art performance, and delivers insightful modeling strategies.","This demonstrates FABind+ represents a substantial step forward in molecular docking and drug discovery.","Our code is in https://github.com/QizhiPei/FABind."],"url":"http://arxiv.org/abs/2403.20261v1","category":"q-bio.BM"}
{"created":"2024-03-29 16:05:07","title":"A Skip-based Algorithm for Weighted Reservoir Random Sampling with Replacement","abstract":"Reservoir sampling techniques can be used to extract a sample from a population of unknown size. Most of attention has been put to sampling without replacement, with only a small number of studies focusing on sampling with replacement. Specifically, to the author's knowledge, no one has explored in detail how to deal with the weighted case in this setting. In this work, we demonstrate that the results shown in [1] can be further generalized using similar techniques to develop a fast skip-based algorithm for weighted reservoir sampling with replacement.","sentences":["Reservoir sampling techniques can be used to extract a sample from a population of unknown size.","Most of attention has been put to sampling without replacement, with only a small number of studies focusing on sampling with replacement.","Specifically, to the author's knowledge, no one has explored in detail how to deal with the weighted case in this setting.","In this work, we demonstrate that the results shown in [1] can be further generalized using similar techniques to develop a fast skip-based algorithm for weighted reservoir sampling with replacement."],"url":"http://arxiv.org/abs/2403.20256v1","category":"cs.DS"}
{"created":"2024-03-29 15:59:11","title":"MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation","abstract":"Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy.","sentences":["Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning.","To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability.","Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis.","Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation.","However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary.","In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings.","To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting.","Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further.","By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy."],"url":"http://arxiv.org/abs/2403.20253v1","category":"cs.CV"}
{"created":"2024-03-29 15:58:46","title":"Using LLMs to Model the Beliefs and Preferences of Targeted Populations","abstract":"We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.","sentences":["We consider the problem of aligning a large language model (LLM) to model the preferences of a human population.","Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical.","Existing work has had mixed success using LLMs to accurately model human behavior in different contexts.","We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs).","We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two.","Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response."],"url":"http://arxiv.org/abs/2403.20252v1","category":"cs.CL"}
{"created":"2024-03-29 15:55:06","title":"Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures","abstract":"This paper deals with optimal policy learning (OPL) with observational data, i.e. data-driven optimal decision-making, in multi-action (or multi-arm) settings, where a finite set of decision options is available. It is organized in three parts, where I discuss respectively: estimation, risk preference, and potential failures. The first part provides a brief review of the key approaches to estimating the reward (or value) function and optimal policy within this context of analysis. Here, I delineate the identification assumptions and statistical properties related to offline optimal policy learning estimators. In the second part, I delve into the analysis of decision risk. This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance. Here, I present an application of the proposed model to real data, illustrating that the average regret of a policy with multi-valued treatment is contingent on the decision-maker's attitude towards risk. The third part of the paper discusses the limitations of optimal data-driven decision-making by highlighting conditions under which decision-making can falter. This aspect is linked to the failure of the two fundamental assumptions essential for identifying the optimal choice: (i) overlapping, and (ii) unconfoundedness. Some conclusions end the paper.","sentences":["This paper deals with optimal policy learning (OPL) with observational data, i.e. data-driven optimal decision-making, in multi-action (or multi-arm) settings, where a finite set of decision options is available.","It is organized in three parts, where I discuss respectively: estimation, risk preference, and potential failures.","The first part provides a brief review of the key approaches to estimating the reward (or value) function and optimal policy within this context of analysis.","Here, I delineate the identification assumptions and statistical properties related to offline optimal policy learning estimators.","In the second part, I delve into the analysis of decision risk.","This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance.","Here, I present an application of the proposed model to real data, illustrating that the average regret of a policy with multi-valued treatment is contingent on the decision-maker's attitude towards risk.","The third part of the paper discusses the limitations of optimal data-driven decision-making by highlighting conditions under which decision-making can falter.","This aspect is linked to the failure of the two fundamental assumptions essential for identifying the optimal choice: (i) overlapping, and (ii) unconfoundedness.","Some conclusions end the paper."],"url":"http://arxiv.org/abs/2403.20250v1","category":"stat.ML"}
{"created":"2024-03-29 15:54:36","title":"Relation Rectification in Diffusion Model","abstract":"Despite their exceptional generative abilities, large text-to-image diffusion models, much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects. This issue, as we uncover through careful analysis, arises from a misaligned text encoder that struggles to interpret specific relationships and differentiate the logical order of associated objects. To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate. To address this, we propose an innovative solution utilizing a Heterogeneous Graph Convolutional Network (HGCN). It models the directional relationships between relation terms and corresponding objects within the input prompts. Specifically, we optimize the HGCN on a pair of prompts with identical relational words but reversed object orders, supplemented by a few reference images. The lightweight HGCN adjusts the text embeddings generated by the text encoder, ensuring the accurate reflection of the textual relation in the embedding space. Crucially, our method retains the parameters of the text encoder and diffusion model, preserving the model's robust performance on unrelated descriptions. We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations. Project page: https://wuyinwei-hah.github.io/rrnet.github.io/.","sentences":["Despite their exceptional generative abilities, large text-to-image diffusion models, much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects.","This issue, as we uncover through careful analysis, arises from a misaligned text encoder that struggles to interpret specific relationships and differentiate the logical order of associated objects.","To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate.","To address this, we propose an innovative solution utilizing a Heterogeneous Graph Convolutional Network (HGCN).","It models the directional relationships between relation terms and corresponding objects within the input prompts.","Specifically, we optimize the HGCN on a pair of prompts with identical relational words but reversed object orders, supplemented by a few reference images.","The lightweight HGCN adjusts the text embeddings generated by the text encoder, ensuring the accurate reflection of the textual relation in the embedding space.","Crucially, our method retains the parameters of the text encoder and diffusion model, preserving the model's robust performance on unrelated descriptions.","We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations.","Project page: https://wuyinwei-hah.github.io/rrnet.github.io/."],"url":"http://arxiv.org/abs/2403.20249v1","category":"cs.CV"}
{"created":"2024-03-29 15:48:37","title":"Gate-tunable quantum acoustoelectric transport in graphene","abstract":"Transport probes the motion of quasiparticles in response to external excitations. Apart from the well-known electric and thermoelectric transport, acoustoelectric transport induced by traveling acoustic waves has been rarely explored. Here, by adopting a hybrid nanodevices integrated with piezoelectric substrates, we establish a simple design of acoustoelectric transport with gate tunability. We fabricate dual-gated acoustoelectric devices based on BN-encapsuled graphene on LiNbO3. Longitudinal and transverse acoustoelectric voltages are generated by launching pulsed surface acoustic wave. The gate dependence of zero-field longitudinal acoustoelectric signal presents strikingly similar profiles as that of Hall resistivity, providing a valid approach for extracting carrier density without magnetic field. In magnetic fields, acoustoelectric quantum oscillations appear due to Landau quantization, which are more robust and pronounced than Shubnikov-de Haas oscillations. Our work demonstrates a feasible acoustoelectric setup with gate tunability, which can be extended to the broad scope of various Van der Waals materials.","sentences":["Transport probes the motion of quasiparticles in response to external excitations.","Apart from the well-known electric and thermoelectric transport, acoustoelectric transport induced by traveling acoustic waves has been rarely explored.","Here, by adopting a hybrid nanodevices integrated with piezoelectric substrates, we establish a simple design of acoustoelectric transport with gate tunability.","We fabricate dual-gated acoustoelectric devices based on BN-encapsuled graphene on LiNbO3.","Longitudinal and transverse acoustoelectric voltages are generated by launching pulsed surface acoustic wave.","The gate dependence of zero-field longitudinal acoustoelectric signal presents strikingly similar profiles as that of Hall resistivity, providing a valid approach for extracting carrier density without magnetic field.","In magnetic fields, acoustoelectric quantum oscillations appear due to Landau quantization, which are more robust and pronounced than Shubnikov-de Haas oscillations.","Our work demonstrates a feasible acoustoelectric setup with gate tunability, which can be extended to the broad scope of various Van der Waals materials."],"url":"http://arxiv.org/abs/2403.20248v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-29 15:44:15","title":"A topology on the poset of quiver mutation classes","abstract":"To better understand mutation-invariant and hereditary properties of quivers (and more generally skew-symmetrizable matrices), we have constructed a topology on the set of all mutation classes of quivers which we call the mutation class topology. This topology is the Alexandrov topology induced by the poset structure on the set of mutation classes of quivers from the partial order of quiver embedding. The closed sets of our topology -- equivalently, the lower sets of the poset -- are in bijective correspondence with mutation-invariant and hereditary properties of quivers. The mutation class space described in this paper is the unique topological space with this property. We show that this space is strictly $T_0$, connected, non-Noetherian, and that every open set is dense. We close by providing open questions from cluster algebra theory in the setting of the mutation class topology and some directions for future research.","sentences":["To better understand mutation-invariant and hereditary properties of quivers (and more generally skew-symmetrizable matrices), we have constructed a topology on the set of all mutation classes of quivers which we call the mutation class topology.","This topology is the Alexandrov topology induced by the poset structure on the set of mutation classes of quivers from the partial order of quiver embedding.","The closed sets of our topology -- equivalently, the lower sets of the poset -- are in bijective correspondence with mutation-invariant and hereditary properties of quivers.","The mutation class space described in this paper is the unique topological space with this property.","We show that this space is strictly $T_0$, connected, non-Noetherian, and that every open set is dense.","We close by providing open questions from cluster algebra theory in the setting of the mutation class topology and some directions for future research."],"url":"http://arxiv.org/abs/2403.20245v1","category":"math.CO"}
{"created":"2024-03-29 15:42:51","title":"Minimizing movements for the generalized power mean curvature flow","abstract":"Motivated by a conjecture of De Giorgi, we consider the Almgren-Taylor-Wang scheme for mean curvature flow, where the volume penalization is replaced by a term of the form \\[ \\int_{E\\Delta F} f\\Big(\\frac{ {\\rm d}_F }{\\tau}\\Big)~dx \\] for $f$ ranging in a large class of strictly increasing continuous functions. In particular, our analysis covers the case \\[ f(r) = r^\\alpha, \\qquad r \\geq 0, \\quad \\alpha>0, \\] considered by De Giorgi. We show that the generalized minimizing movement scheme converges to the geometric evolution equation \\[ f(v) = - \\kappa\\quad \\text{on $\\partial E(t)$,} \\] where $\\{E(t)\\}$ are evolving subsets of $\\mathbb{R}^n,$ $v$ is the normal velocity of $\\partial E(t),$ and $\\kappa$ is the mean curvature of $\\partial E(t)$. We extend our analysis to the anisotropic setting, and in the presence of a driving force. We also show that minimizing movements coincide with the smooth classical solution as long as the latter exists. Finally, we prove that in the absence of forcing, mean convexity and convexity are preserved by the weak flow.","sentences":["Motivated by a conjecture of De Giorgi, we consider the Almgren-Taylor-Wang scheme for mean curvature flow, where the volume penalization is replaced by a term of the form \\[ \\int_{E\\Delta F} f\\Big(\\frac{ {\\rm d}_F }{\\tau}\\Big)~dx \\] for $f$ ranging in a large class of strictly increasing continuous functions.","In particular, our analysis covers the case \\[ f(r) = r^\\alpha, \\qquad r \\geq 0, \\quad \\alpha>0, \\] considered by De Giorgi.","We show that the generalized minimizing movement scheme converges to the geometric evolution equation \\[ f(v) = - \\kappa\\quad \\text{on $\\partial E(t)$,} \\] where $\\{E(t)\\}$ are evolving subsets of $\\mathbb{R}^n,$ $v$ is the normal velocity of $\\partial E(t),$ and $\\kappa$ is the mean curvature of $\\partial E(t)$.","We extend our analysis to the anisotropic setting, and in the presence of a driving force.","We also show that minimizing movements coincide with the smooth classical solution as long as the latter exists.","Finally, we prove that in the absence of forcing, mean convexity and convexity are preserved by the weak flow."],"url":"http://arxiv.org/abs/2403.20244v1","category":"math.AP"}
{"created":"2024-03-29 15:35:41","title":"Nodal Volumes as Differentiable Functionals of Gaussian fields","abstract":"We characterize the absolute continuity of the law and the Malliavin-Sobolev regularity of random nodal volumes associated with smooth Gaussian fields on generic $\\mathcal{C}^2$ manifolds with arbitrary dimension. Our results extend and generalize the seminal contribution by Angst and Poly (2020) about stationary fields on Euclidean spaces and cover, in particular, the case of two-dimensional manifolds, possibly with boundary and corners. The main tools exploited in the proofs include the use of Gaussian measures on Banach spaces, Morse theory, and the characterization of Malliavin-Sobolev spaces in terms of ray absolute continuity. Several examples are analyzed in detail.","sentences":["We characterize the absolute continuity of the law and the Malliavin-Sobolev regularity of random nodal volumes associated with smooth Gaussian fields on generic $\\mathcal{C}^2$ manifolds with arbitrary dimension.","Our results extend and generalize the seminal contribution by Angst and Poly (2020) about stationary fields on Euclidean spaces and cover, in particular, the case of two-dimensional manifolds, possibly with boundary and corners.","The main tools exploited in the proofs include the use of Gaussian measures on Banach spaces, Morse theory, and the characterization of Malliavin-Sobolev spaces in terms of ray absolute continuity.","Several examples are analyzed in detail."],"url":"http://arxiv.org/abs/2403.20243v1","category":"math.PR"}
{"created":"2024-03-29 15:33:21","title":"Deriving Neutron Star Equation of State from AdS/QCD","abstract":"Neutron stars are among the main targets for gravitational wave observatories, however, their equation of state is still not well established. Mainly phenomenological models with many parameters are widely used by far, while theoretical models are not so practical. In arXiv:1902.08477, a theoretical equation of state with only one parameter is derived from Sakai-Sugimoto model, as an application of AdS/QCD, where pointlike instanton case is taken into consideration. When the tidal deformability constraint from gravitational wave event is satisfied, the maximum mass is about 1.7 solar masses. Now we upgrade this model to instanton gas, with one more variable, the instanton width. This is not naively a free parameter, but a function of the chemical potential. Thus we end up with a more complicated and accurate model, but still with only one adjustable parameter. In this case, we find the maximum mass becomes 1.85 solar masses. This is an encouraging and exiting result, as a theoretically derived model.","sentences":["Neutron stars are among the main targets for gravitational wave observatories, however, their equation of state is still not well established.","Mainly phenomenological models with many parameters are widely used by far, while theoretical models are not so practical.","In arXiv:1902.08477, a theoretical equation of state with only one parameter is derived from Sakai-Sugimoto model, as an application of AdS/QCD, where pointlike instanton case is taken into consideration.","When the tidal deformability constraint from gravitational wave event is satisfied, the maximum mass is about 1.7 solar masses.","Now we upgrade this model to instanton gas, with one more variable, the instanton width.","This is not naively a free parameter, but a function of the chemical potential.","Thus we end up with a more complicated and accurate model, but still with only one adjustable parameter.","In this case, we find the maximum mass becomes 1.85 solar masses.","This is an encouraging and exiting result, as a theoretically derived model."],"url":"http://arxiv.org/abs/2403.20240v1","category":"hep-ph"}
{"created":"2024-03-29 15:31:13","title":"An ordinary differential equation for entropic optimal transport and its linearly constrained variants","abstract":"We characterize the solution to the entropically regularized optimal transport problem by a well-posed ordinary differential equation (ODE). Our approach works for discrete marginals and general cost functions, and in addition to two marginal problems, applies to multi-marginal problems and those with additional linear constraints. Solving the ODE gives a new numerical method to solve the optimal transport problem, which has the advantage of yielding the solution for all intermediate values of the ODE parameter (which is equivalent to the usual regularization parameter). We illustrate this method with several numerical simulations. The formulation of the ODE also allows one to compute derivatives of the optimal cost when the ODE parameter is $0$, corresponding to the fully regularized limit problem in which only the entropy is minimized.","sentences":["We characterize the solution to the entropically regularized optimal transport problem by a well-posed ordinary differential equation (ODE).","Our approach works for discrete marginals and general cost functions, and in addition to two marginal problems, applies to multi-marginal problems and those with additional linear constraints.","Solving the ODE gives a new numerical method to solve the optimal transport problem, which has the advantage of yielding the solution for all intermediate values of the ODE parameter (which is equivalent to the usual regularization parameter).","We illustrate this method with several numerical simulations.","The formulation of the ODE also allows one to compute derivatives of the optimal cost when the ODE parameter is $0$, corresponding to the fully regularized limit problem in which only the entropy is minimized."],"url":"http://arxiv.org/abs/2403.20238v1","category":"math.OC"}
{"created":"2024-03-29 15:28:22","title":"Evolving Semantic Communication with Generative Model","abstract":"Recently, learning-based semantic communication (SemCom) has emerged as a promising approach in the upcoming 6G network and researchers have made remarkable efforts in this field. However, existing works have yet to fully explore the advantages of the evolving nature of learning-based systems, where knowledge accumulates during transmission have the potential to enhance system performance. In this paper, we explore an evolving semantic communication system for image transmission, referred to as ESemCom, with the capability to continuously enhance transmission efficiency. The system features a novel channel-aware semantic encoder that utilizes a pre-trained Semantic StyleGAN to extract the channel-correlated latent variables consisting of serval semantic vectors from the input images, which can be directly transmitted over a noisy channel without further channel coding. Moreover, we introduce a semantic caching mechanism that dynamically stores the transmitted semantic vectors in the local caching memory of both the transmitter and receiver. The cached semantic vectors are then exploited to eliminate the need to transmit similar codes in subsequent transmission, thus further reducing communication overhead. Simulation results highlight the evolving performance of the proposed system in terms of transmission efficiency, achieving superior perceptual quality with an average bandwidth compression ratio (BCR) of 1/192 for a sequence of 100 testing images compared to DeepJSCC and Inverse JSCC with the same BCR. Code of this paper is available at \\url{https://github.com/recusant7/GAN_SeCom}.","sentences":["Recently, learning-based semantic communication (SemCom) has emerged as a promising approach in the upcoming 6G network and researchers have made remarkable efforts in this field.","However, existing works have yet to fully explore the advantages of the evolving nature of learning-based systems, where knowledge accumulates during transmission have the potential to enhance system performance.","In this paper, we explore an evolving semantic communication system for image transmission, referred to as ESemCom, with the capability to continuously enhance transmission efficiency.","The system features a novel channel-aware semantic encoder that utilizes a pre-trained Semantic StyleGAN to extract the channel-correlated latent variables consisting of serval semantic vectors from the input images, which can be directly transmitted over a noisy channel without further channel coding.","Moreover, we introduce a semantic caching mechanism that dynamically stores the transmitted semantic vectors in the local caching memory of both the transmitter and receiver.","The cached semantic vectors are then exploited to eliminate the need to transmit similar codes in subsequent transmission, thus further reducing communication overhead.","Simulation results highlight the evolving performance of the proposed system in terms of transmission efficiency, achieving superior perceptual quality with an average bandwidth compression ratio (BCR) of 1/192 for a sequence of 100 testing images compared to DeepJSCC and Inverse JSCC with the same BCR",". Code of this paper is available at \\url{https://github.com/recusant7/GAN_SeCom}."],"url":"http://arxiv.org/abs/2403.20237v1","category":"eess.SP"}
{"created":"2024-03-29 15:23:30","title":"Artificial Neural Networks-based Real-time Classification of ENG Signals for Implanted Nerve Interfaces","abstract":"Neuropathies are gaining higher relevance in clinical settings, as they risk permanently jeopardizing a person's life. To support the recovery of patients, the use of fully implanted devices is emerging as one of the most promising solutions. However, these devices, even if becoming an integral part of a fully complex neural nanonetwork system, pose numerous challenges. In this article, we address one of them, which consists of the classification of motor/sensory stimuli. The task is performed by exploring four different types of artificial neural networks (ANNs) to extract various sensory stimuli from the electroneurographic (ENG) signal measured in the sciatic nerve of rats. Different sizes of the data sets are considered to analyze the feasibility of the investigated ANNs for real-time classification through a comparison of their performance in terms of accuracy, F1-score, and prediction time. The design of the ANNs takes advantage of the modelling of the ENG signal as a multiple-input multiple-output (MIMO) system to describe the measures taken by state-of-the-art implanted nerve interfaces. These are based on the use of multi-contact cuff electrodes to achieve nanoscale spatial discrimination of the nerve activity. The MIMO ENG signal model is another contribution of this paper. Our results show that some ANNs are more suitable for real-time applications, being capable of achieving accuracies over $90\\%$ for signal windows of $100$ and $200\\,$ms with a low enough processing time to be effective for pathology recovery.","sentences":["Neuropathies are gaining higher relevance in clinical settings, as they risk permanently jeopardizing a person's life.","To support the recovery of patients, the use of fully implanted devices is emerging as one of the most promising solutions.","However, these devices, even if becoming an integral part of a fully complex neural nanonetwork system, pose numerous challenges.","In this article, we address one of them, which consists of the classification of motor/sensory stimuli.","The task is performed by exploring four different types of artificial neural networks (ANNs) to extract various sensory stimuli from the electroneurographic (ENG) signal measured in the sciatic nerve of rats.","Different sizes of the data sets are considered to analyze the feasibility of the investigated ANNs for real-time classification through a comparison of their performance in terms of accuracy, F1-score, and prediction time.","The design of the ANNs takes advantage of the modelling of the ENG signal as a multiple-input multiple-output (MIMO) system to describe the measures taken by state-of-the-art implanted nerve interfaces.","These are based on the use of multi-contact cuff electrodes to achieve nanoscale spatial discrimination of the nerve activity.","The MIMO ENG signal model is another contribution of this paper.","Our results show that some ANNs are more suitable for real-time applications, being capable of achieving accuracies over $90\\%$ for signal windows of $100$ and $200\\,$ms with a low enough processing time to be effective for pathology recovery."],"url":"http://arxiv.org/abs/2403.20234v1","category":"cs.AI"}
{"created":"2024-03-29 15:20:34","title":"U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation","abstract":"Concept personalization methods enable large text-to-image models to learn specific subjects (e.g., objects/poses/3D models) and synthesize renditions in new contexts. Given that the image references are highly biased towards visual attributes, state-of-the-art personalization models tend to overfit the whole subject and cannot disentangle visual characteristics in pixel space. In this study, we proposed a more challenging setting, namely fine-grained visual appearance personalization. Different from existing methods, we allow users to provide a sentence describing the desired attributes. A novel decoupled self-augmentation strategy is proposed to generate target-related and non-target samples to learn user-specified visual attributes. These augmented data allow for refining the model's understanding of the target attribute while mitigating the impact of unrelated attributes. At the inference stage, adjustments are conducted on semantic space through the learned target and non-target embeddings to further enhance the disentanglement of target attributes. Extensive experiments on various kinds of visual attributes with SOTA personalization methods show the ability of the proposed method to mimic target visual appearance in novel contexts, thus improving the controllability and flexibility of personalization.","sentences":["Concept personalization methods enable large text-to-image models to learn specific subjects (e.g., objects/poses/3D models) and synthesize renditions in new contexts.","Given that the image references are highly biased towards visual attributes, state-of-the-art personalization models tend to overfit the whole subject and cannot disentangle visual characteristics in pixel space.","In this study, we proposed a more challenging setting, namely fine-grained visual appearance personalization.","Different from existing methods, we allow users to provide a sentence describing the desired attributes.","A novel decoupled self-augmentation strategy is proposed to generate target-related and non-target samples to learn user-specified visual attributes.","These augmented data allow for refining the model's understanding of the target attribute while mitigating the impact of unrelated attributes.","At the inference stage, adjustments are conducted on semantic space through the learned target and non-target embeddings to further enhance the disentanglement of target attributes.","Extensive experiments on various kinds of visual attributes with SOTA personalization methods show the ability of the proposed method to mimic target visual appearance in novel contexts, thus improving the controllability and flexibility of personalization."],"url":"http://arxiv.org/abs/2403.20231v1","category":"cs.CV"}
{"created":"2024-03-29 15:08:37","title":"MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark","abstract":"Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras. This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and anomaly detection. However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations. To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 multi-modal cameras in two different environments - campus and factory - across various time, weather, and season conditions. This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking. We provide baselines and new learning setups on this dataset and set the reference scores for future studies. The datasets, models, and test server will be made publicly available.","sentences":["Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras.","This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and anomaly detection.","However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations.","To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 multi-modal cameras in two different environments - campus and factory - across various time, weather, and season conditions.","This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking.","MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking.","We provide baselines and new learning setups on this dataset and set the reference scores for future studies.","The datasets, models, and test server will be made publicly available."],"url":"http://arxiv.org/abs/2403.20225v1","category":"cs.CV"}
{"created":"2024-03-29 15:07:24","title":"On bi-amalgamated constructions","abstract":"Let $f:A\\longrightarrow B, g:A\\longrightarrow C$ be ring homomorphisms and let $\\mathfrak{b}$ (resp., $\\mathfrak{c}$) be an ideal of $B$ (resp., $C$) satisfying $f^{-1}(\\mathfrak{b})=g^{-1}(\\mathfrak{c})$. Recently Kabbaj, Louartiti and Tamekkante defined and studied the following subring $$A\\bowtie^{f,g}(\\mathfrak{b},\\mathfrak{c}) :=\\{(f(a)+b, g(a)+c)\\mid a\\in A, b\\in\\mathfrak{b}, c\\in \\mathfrak{c} \\}$$ of $B\\times C$, called the bi-amalgamation of $A$ with $(B,C)$ along $(\\mathfrak{b}, \\mathfrak{c})$, with respect to $(f,g)$. This ring construction is a natural generalization of the amalgamated algebras, introduced and studied by D'Anna, Finocchiaro and Fontana. The aim of this paper is to continue the investigation started by Kabbaj, Louartiti and Tamekkante, by providing a deeper insigt on the ideal-theoretic structure of bi-amalgamations.","sentences":["Let $f:A\\longrightarrow B, g:A\\longrightarrow C$ be ring homomorphisms and let $\\mathfrak{b}$ (resp., $\\mathfrak{c}$) be an ideal of $B$ (resp., $C$) satisfying $f^{-1}(\\mathfrak{b})=g^{-1}(\\mathfrak{c})$.","Recently Kabbaj, Louartiti and Tamekkante defined and studied the following subring $$A\\bowtie^{f,g}(\\mathfrak{b},\\mathfrak{c}) :","=\\{(f(a)+b, g(a)+c)\\mid a\\in A, b\\in\\mathfrak{b}, c\\in \\mathfrak{c} \\}$$ of $B\\times C$, called the bi-amalgamation of $A$ with $(B,C)$ along $(\\mathfrak{b}, \\mathfrak{c})$, with respect to $(f,g)$.","This ring construction is a natural generalization of the amalgamated algebras, introduced and studied by D'Anna, Finocchiaro and Fontana.","The aim of this paper is to continue the investigation started by Kabbaj, Louartiti and Tamekkante, by providing a deeper insigt on the ideal-theoretic structure of bi-amalgamations."],"url":"http://arxiv.org/abs/2403.20224v1","category":"math.AC"}
{"created":"2024-03-29 15:07:21","title":"Shallow Cross-Encoders for Low-Latency Retrieval","abstract":"Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstrate significant improvements in shallow and full-scale models in low-latency scenarios. For example, when the latency limit is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches NDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow Cross-Encoders are effective even when used without a GPU (e.g., with CPU inference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms latency), which makes Cross-Encoders practical to run even without specialized hardware acceleration.","sentences":["Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval.","However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window.","However, keeping search latencies low is important for user satisfaction and energy usage.","In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget.","We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks.","Our experiments with TREC Deep Learning passage ranking query sets demonstrate significant improvements in shallow and full-scale models in low-latency scenarios.","For example, when the latency limit is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches NDCG@10 of 0.652, a +51% gain over MonoBERT-Large.","We also show that shallow Cross-Encoders are effective even when used without a GPU (e.g., with CPU inference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms latency), which makes Cross-Encoders practical to run even without specialized hardware acceleration."],"url":"http://arxiv.org/abs/2403.20222v1","category":"cs.IR"}
{"created":"2024-03-29 15:05:57","title":"Graph Neural Aggregation-diffusion with Metastability","abstract":"Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and establishes a connection with classical GNNs. We prove that GRADE achieves competitive performance across various benchmarks and alleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet energy.","sentences":["Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs).","Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied.","However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing.","To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials.","The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters.","In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects.","This nonlinear diffusion in our model generalizes existing diffusion-based models and establishes a connection with classical GNNs.","We prove that GRADE achieves competitive performance across various benchmarks and alleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet energy."],"url":"http://arxiv.org/abs/2403.20221v1","category":"cs.LG"}
{"created":"2024-03-29 14:58:28","title":"Decentralized Multimedia Data Sharing in IoV: A Learning-based Equilibrium of Supply and Demand","abstract":"The Internet of Vehicles (IoV) has great potential to transform transportation systems by enhancing road safety, reducing traffic congestion, and improving user experience through onboard infotainment applications. Decentralized data sharing can improve security, privacy, reliability, and facilitate infotainment data sharing in IoVs. However, decentralized data sharing may not achieve the expected efficiency if there are IoV users who only want to consume the shared data but are not willing to contribute their own data to the community, resulting in incomplete information observed by other vehicles and infrastructure, which can introduce additional transmission latency. Therefore, in this article, by modeling the data sharing ecosystem as a data trading market, we propose a decentralized data-sharing incentive mechanism based on multi-intelligent reinforcement learning to learn the supply-demand balance in markets and minimize transmission latency. Our proposed mechanism takes into account the dynamic nature of IoV markets, which can experience frequent fluctuations in supply and demand. We propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) mechanism coupled with Named Data Networking (NDN) to protect data in IoVs, which adds a layer of security to our proposed solution. Additionally, we design a decentralized market for efficient data sharing in IoVs, where continuous double auctions are adopted. The proposed mechanism based on multi-agent deep reinforcement learning can learn the supply-demand equilibrium in markets, thus improving the efficiency and sustainability of markets. Theoretical analysis and experimental results show that our proposed learning-based incentive mechanism outperforms baselines by 10% in determining the equilibrium of supply and demand while reducing transmission latency by 20%.","sentences":["The Internet of Vehicles (IoV) has great potential to transform transportation systems by enhancing road safety, reducing traffic congestion, and improving user experience through onboard infotainment applications.","Decentralized data sharing can improve security, privacy, reliability, and facilitate infotainment data sharing in IoVs.","However, decentralized data sharing may not achieve the expected efficiency if there are IoV users who only want to consume the shared data but are not willing to contribute their own data to the community, resulting in incomplete information observed by other vehicles and infrastructure, which can introduce additional transmission latency.","Therefore, in this article, by modeling the data sharing ecosystem as a data trading market, we propose a decentralized data-sharing incentive mechanism based on multi-intelligent reinforcement learning to learn the supply-demand balance in markets and minimize transmission latency.","Our proposed mechanism takes into account the dynamic nature of IoV markets, which can experience frequent fluctuations in supply and demand.","We propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) mechanism coupled with Named Data Networking (NDN) to protect data in IoVs, which adds a layer of security to our proposed solution.","Additionally, we design a decentralized market for efficient data sharing in IoVs, where continuous double auctions are adopted.","The proposed mechanism based on multi-agent deep reinforcement learning can learn the supply-demand equilibrium in markets, thus improving the efficiency and sustainability of markets.","Theoretical analysis and experimental results show that our proposed learning-based incentive mechanism outperforms baselines by 10% in determining the equilibrium of supply and demand while reducing transmission latency by 20%."],"url":"http://arxiv.org/abs/2403.20218v1","category":"cs.CR"}
{"created":"2024-03-29 14:55:40","title":"Distributed agency in second language learning and teaching through generative AI","abstract":"Generative AI offers significant opportunities for language learning. Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations. The power and versatility of AI tools are likely to turn them into valuable and constant companions in many peoples lives (akin to smartphones), creating a close connection that goes beyond simple tool use. Ecological theories such as sociomaterialism are helpful in examining the shared agency that develops through close user-AI interactions, as are the perspectives on human-object relations from Indigenous cultures.","sentences":["Generative AI offers significant opportunities for language learning.","Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics.","AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan.","Instructors can use AI to build learning and assessment materials in a variety of media.","AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions.","For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use.","Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations.","The power and versatility of AI tools are likely to turn them into valuable and constant companions in many peoples lives (akin to smartphones), creating a close connection that goes beyond simple tool use.","Ecological theories such as sociomaterialism are helpful in examining the shared agency that develops through close user-AI interactions, as are the perspectives on human-object relations from Indigenous cultures."],"url":"http://arxiv.org/abs/2403.20216v1","category":"cs.CY"}
{"created":"2024-03-29 14:50:43","title":"H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model","abstract":"The generic large Vision-Language Models (VLMs) is rapidly developing, but still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs. Existing Remote Sensing specific Vision Language Models (RSVLMs) still have considerable potential for improvement, primarily owing to the lack of large-scale, high-quality RS vision-language datasets. We constructed HqDC-1.4M, the large scale High quality and Detailed Captions for RS images, containing 1.4 million image-caption pairs, which not only enhance the RSVLM's understanding of RS images but also significantly improve the model's spatial perception abilities, such as localization and counting, thereby increasing the helpfulness of the RSVLM. Moreover, to address the inevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first dataset aimed at enhancing the Self-Awareness capability of RSVLMs. By incorporating a variety of unanswerable questions into typical RS visual question-answering tasks, RSSA effectively improves the truthfulness and reduces the hallucinations of the model's outputs, thereby enhancing the honesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the Helpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved outstanding performance on multiple RS public datasets and is capable of recognizing and refusing to answer the unanswerable questions, effectively mitigating the incorrect generations. We will release the code, data and model weights at https://github.com/opendatalab/H2RSVLM .","sentences":["The generic large Vision-Language Models (VLMs) is rapidly developing, but still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs.","Existing Remote Sensing specific Vision Language Models (RSVLMs) still have considerable potential for improvement, primarily owing to the lack of large-scale, high-quality RS vision-language datasets.","We constructed HqDC-1.4M, the large scale High quality and Detailed Captions for RS images, containing 1.4 million image-caption pairs, which not only enhance the RSVLM's understanding of RS images but also significantly improve the model's spatial perception abilities, such as localization and counting, thereby increasing the helpfulness of the RSVLM.","Moreover, to address the inevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first dataset aimed at enhancing the Self-Awareness capability of RSVLMs.","By incorporating a variety of unanswerable questions into typical RS visual question-answering tasks, RSSA effectively improves the truthfulness and reduces the hallucinations of the model's outputs, thereby enhancing the honesty of the RSVLM.","Based on these datasets, we proposed the H2RSVLM, the Helpful and Honest Remote Sensing Vision Language Model.","H2RSVLM has achieved outstanding performance on multiple RS public datasets and is capable of recognizing and refusing to answer the unanswerable questions, effectively mitigating the incorrect generations.","We will release the code, data and model weights at https://github.com/opendatalab/H2RSVLM ."],"url":"http://arxiv.org/abs/2403.20213v1","category":"cs.CV"}
{"created":"2024-03-29 14:47:54","title":"On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem","abstract":"We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results. Our findings suggest that models trained on harder instances exhibit better generalization capabilities, highlighting the importance of selecting appropriate training instances in solving TSP using Unsupervised Learning.","sentences":["We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP).","We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node.","We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route.","We then apply local search to generate our final predictions.","Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods.","Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP.","Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results.","Our findings suggest that models trained on harder instances exhibit better generalization capabilities, highlighting the importance of selecting appropriate training instances in solving TSP using Unsupervised Learning."],"url":"http://arxiv.org/abs/2403.20212v1","category":"cs.AI"}
{"created":"2024-03-29 14:41:21","title":"Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science","abstract":"In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.","sentences":["In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data.","This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks.","Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data.","This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training.","Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset.","Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios.","Through extensive experiments, our methodology has shown significant improvements over existing benchmarks.","These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence."],"url":"http://arxiv.org/abs/2403.20208v1","category":"cs.LG"}
{"created":"2024-03-29 14:32:41","title":"The Future of Combating Rumors? Retrieval, Discrimination, and Generation","abstract":"Artificial Intelligence Generated Content (AIGC) technology development has facilitated the creation of rumors with misinformation, impacting societal, economic, and political ecosystems, challenging democracy. Current rumor detection efforts fall short by merely labeling potentially misinformation (classification task), inadequately addressing the issue, and it is unrealistic to have authoritative institutions debunk every piece of information on social media. Our proposed comprehensive debunking process not only detects rumors but also provides explanatory generated content to refute the authenticity of the information. The Expert-Citizen Collective Wisdom (ECCW) module we designed aensures high-precision assessment of the credibility of information and the retrieval module is responsible for retrieving relevant knowledge from a Real-time updated debunking database based on information keywords. By using prompt engineering techniques, we feed results and knowledge into a LLM (Large Language Model), achieving satisfactory discrimination and explanatory effects while eliminating the need for fine-tuning, saving computational costs, and contributing to debunking efforts.","sentences":["Artificial Intelligence Generated Content (AIGC) technology development has facilitated the creation of rumors with misinformation, impacting societal, economic, and political ecosystems, challenging democracy.","Current rumor detection efforts fall short by merely labeling potentially misinformation (classification task), inadequately addressing the issue, and it is unrealistic to have authoritative institutions debunk every piece of information on social media.","Our proposed comprehensive debunking process not only detects rumors but also provides explanatory generated content to refute the authenticity of the information.","The Expert-Citizen Collective Wisdom (ECCW) module we designed aensures high-precision assessment of the credibility of information and the retrieval module is responsible for retrieving relevant knowledge from a Real-time updated debunking database based on information keywords.","By using prompt engineering techniques, we feed results and knowledge into a LLM (Large Language Model), achieving satisfactory discrimination and explanatory effects while eliminating the need for fine-tuning, saving computational costs, and contributing to debunking efforts."],"url":"http://arxiv.org/abs/2403.20204v1","category":"cs.AI"}
{"created":"2024-03-29 14:31:36","title":"Voice Signal Processing for Machine Learning. The Case of Speaker Isolation","abstract":"The widespread use of automated voice assistants along with other recent technological developments have increased the demand for applications that process audio signals and human voice in particular. Voice recognition tasks are typically performed using artificial intelligence and machine learning models. Even though end-to-end models exist, properly pre-processing the signal can greatly reduce the complexity of the task and allow it to be solved with a simpler ML model and fewer computational resources. However, ML engineers who work on such tasks might not have a background in signal processing which is an entirely different area of expertise.   The objective of this work is to provide a concise comparative analysis of Fourier and Wavelet transforms that are most commonly used as signal decomposition methods for audio processing tasks. Metrics for evaluating speech intelligibility are also discussed, namely Scale-Invariant Signal-to-Distortion Ratio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-Time Objective Intelligibility (STOI). The level of detail in the exposition is meant to be sufficient for an ML engineer to make informed decisions when choosing, fine-tuning, and evaluating a decomposition method for a specific ML model. The exposition contains mathematical definitions of the relevant concepts accompanied with intuitive non-mathematical explanations in order to make the text more accessible to engineers without deep expertise in signal processing. Formal mathematical definitions and proofs of theorems are intentionally omitted in order to keep the text concise.","sentences":["The widespread use of automated voice assistants along with other recent technological developments have increased the demand for applications that process audio signals and human voice in particular.","Voice recognition tasks are typically performed using artificial intelligence and machine learning models.","Even though end-to-end models exist, properly pre-processing the signal can greatly reduce the complexity of the task and allow it to be solved with a simpler ML model and fewer computational resources.","However, ML engineers who work on such tasks might not have a background in signal processing which is an entirely different area of expertise.   ","The objective of this work is to provide a concise comparative analysis of Fourier and Wavelet transforms that are most commonly used as signal decomposition methods for audio processing tasks.","Metrics for evaluating speech intelligibility are also discussed, namely Scale-Invariant Signal-to-Distortion Ratio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-Time Objective Intelligibility (STOI).","The level of detail in the exposition is meant to be sufficient for an ML engineer to make informed decisions when choosing, fine-tuning, and evaluating a decomposition method for a specific ML model.","The exposition contains mathematical definitions of the relevant concepts accompanied with intuitive non-mathematical explanations in order to make the text more accessible to engineers without deep expertise in signal processing.","Formal mathematical definitions and proofs of theorems are intentionally omitted in order to keep the text concise."],"url":"http://arxiv.org/abs/2403.20202v1","category":"cs.SD"}
{"created":"2024-03-29 14:24:15","title":"NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks","abstract":"Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph.","sentences":["Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions.","The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges.","The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication.","Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9].","We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph."],"url":"http://arxiv.org/abs/2403.20199v1","category":"cs.NI"}
{"created":"2024-03-29 14:19:26","title":"Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization","abstract":"Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling. To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.","sentences":["Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling.","To obtain identifiable solutions, a standard approach is to find minimum-volume solutions.","By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space.","We first prove the identifiability of this maximum-volume dual problem.","Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification.","Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms."],"url":"http://arxiv.org/abs/2403.20197v1","category":"math.NA"}
{"created":"2024-03-29 14:17:30","title":"Enhancing Lithological Mapping with Spatially Constrained Bayesian Network (SCB-Net): An Approach for Field Data-Constrained Predictions with Uncertainty Evaluation","abstract":"Geological maps are an extremely valuable source of information for the Earth sciences. They provide insights into mineral exploration, vulnerability to natural hazards, and many other applications. These maps are created using numerical or conceptual models that use geological observations to extrapolate data. Geostatistical techniques have traditionally been used to generate reliable predictions that take into account the spatial patterns inherent in the data. However, as the number of auxiliary variables increases, these methods become more labor-intensive. Additionally, traditional machine learning methods often struggle with spatially correlated data and extracting valuable non-linear information from geoscientific datasets. To address these limitations, a new architecture called the Spatially Constrained Bayesian Network (SCB-Net) has been developed. The SCB-Net aims to effectively exploit the information from auxiliary variables while producing spatially constrained predictions. It is made up of two parts, the first part focuses on learning underlying patterns in the auxiliary variables while the second part integrates ground-truth data and the learned embeddings from the first part. Moreover, to assess model uncertainty, a technique called Monte Carlo dropout is used as a Bayesian approximation. The SCB-Net has been applied to two selected areas in northern Quebec, Canada, and has demonstrated its potential in generating field-data-constrained lithological maps while allowing assessment of prediction uncertainty for decision-making. This study highlights the promising advancements of deep neural networks in geostatistics, particularly in handling complex spatial feature learning tasks, leading to improved spatial information techniques.","sentences":["Geological maps are an extremely valuable source of information for the Earth sciences.","They provide insights into mineral exploration, vulnerability to natural hazards, and many other applications.","These maps are created using numerical or conceptual models that use geological observations to extrapolate data.","Geostatistical techniques have traditionally been used to generate reliable predictions that take into account the spatial patterns inherent in the data.","However, as the number of auxiliary variables increases, these methods become more labor-intensive.","Additionally, traditional machine learning methods often struggle with spatially correlated data and extracting valuable non-linear information from geoscientific datasets.","To address these limitations, a new architecture called the Spatially Constrained Bayesian Network (SCB-Net) has been developed.","The SCB-Net aims to effectively exploit the information from auxiliary variables while producing spatially constrained predictions.","It is made up of two parts, the first part focuses on learning underlying patterns in the auxiliary variables while the second part integrates ground-truth data and the learned embeddings from the first part.","Moreover, to assess model uncertainty, a technique called Monte Carlo dropout is used as a Bayesian approximation.","The SCB-Net has been applied to two selected areas in northern Quebec, Canada, and has demonstrated its potential in generating field-data-constrained lithological maps while allowing assessment of prediction uncertainty for decision-making.","This study highlights the promising advancements of deep neural networks in geostatistics, particularly in handling complex spatial feature learning tasks, leading to improved spatial information techniques."],"url":"http://arxiv.org/abs/2403.20195v1","category":"cs.CV"}
{"created":"2024-03-29 14:14:22","title":"Motion Inversion for Video Customization","abstract":"In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models. Recognizing the unique challenges posed by video's spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space. Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames. We leverage this understanding to optimize the integration of our motion embeddings. Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.","sentences":["In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models.","Recognizing the unique challenges posed by video's spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video.","These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity.","Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space.","Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames.","We leverage this understanding to optimize the integration of our motion embeddings.","Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments."],"url":"http://arxiv.org/abs/2403.20193v1","category":"cs.CV"}
{"created":"2024-03-29 14:05:40","title":"Distributed Swarm Learning for Edge Internet of Things","abstract":"The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning. With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments. To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner. By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks.","sentences":["The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning.","With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments.","To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner.","By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks."],"url":"http://arxiv.org/abs/2403.20188v1","category":"cs.NI"}
{"created":"2024-03-29 14:04:45","title":"Sketch-to-Architecture: Generative AI-aided Architectural Design","abstract":"Recently, the development of large-scale models has paved the way for various interdisciplinary research, including architecture. By using generative AI, we present a novel workflow that utilizes AI models to generate conceptual floorplans and 3D models from simple sketches, enabling rapid ideation and controlled generation of architectural renderings based on textual descriptions. Our work demonstrates the potential of generative AI in the architectural design process, pointing towards a new direction of computer-aided architectural design. Our project website is available at: https://zrealli.github.io/sketch2arc","sentences":["Recently, the development of large-scale models has paved the way for various interdisciplinary research, including architecture.","By using generative AI, we present a novel workflow that utilizes AI models to generate conceptual floorplans and 3D models from simple sketches, enabling rapid ideation and controlled generation of architectural renderings based on textual descriptions.","Our work demonstrates the potential of generative AI in the architectural design process, pointing towards a new direction of computer-aided architectural design.","Our project website is available at: https://zrealli.github.io/sketch2arc"],"url":"http://arxiv.org/abs/2403.20186v1","category":"cs.CV"}
{"created":"2024-03-29 13:59:34","title":"Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context","abstract":"Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intelligibility and severity scores respectively, using only 95 training samples. It shows that the ASR based Wav2Vec2 model brings the best results and may indicate a strong correlation between ASR and speech quality assessment. We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision.","sentences":["Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation.","However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity.","To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets.","Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments.","This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity.","This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment.","Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intelligibility and severity scores respectively, using only 95 training samples.","It shows that the ASR based Wav2Vec2 model brings the best results and may indicate a strong correlation between ASR and speech quality assessment.","We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision."],"url":"http://arxiv.org/abs/2403.20184v1","category":"eess.AS"}
{"created":"2024-03-29 13:57:46","title":"HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM","abstract":"Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resource constraints encountered in real-time activity recognition scenarios. Our approach involves processing sensor data flow by independently learning each channel and segmenting the data into \"patches\". The marked sensor sequence's position embedding serves as the input token for the bidirectional state space model, ultimately leading to activity categorization through the classification head. Compared to established activity recognition frameworks like Transformer-based models, HARMamba achieves superior performance while also reducing computational and memory overhead. Furthermore, our proposed method has been extensively tested on four public activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating impressive performance in activity recognition tasks.","sentences":["Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing.","While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications.","Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative.","Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design.","Leveraging SSM for activity recognition represents an appealing avenue for exploration.","In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition.","The goal is to address the computational resource constraints encountered in real-time activity recognition scenarios.","Our approach involves processing sensor data flow by independently learning each channel and segmenting the data into \"patches\".","The marked sensor sequence's position embedding serves as the input token for the bidirectional state space model, ultimately leading to activity categorization through the classification head.","Compared to established activity recognition frameworks like Transformer-based models, HARMamba achieves superior performance while also reducing computational and memory overhead.","Furthermore, our proposed method has been extensively tested on four public activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating impressive performance in activity recognition tasks."],"url":"http://arxiv.org/abs/2403.20183v1","category":"cs.CV"}
{"created":"2024-03-29 13:56:54","title":"Quantifying Uncertainty: All We Need is the Bootstrap?","abstract":"Standard errors, confidence intervals, hypothesis tests, and other quantifications of uncertainty are essential to statistical practice. However, they feature a plethora of different methods, mathematical formulas, and concepts. Could we not just replace them all with the general and relatively easy-to-understand non-parametric bootstrap? We contribute to answering this question with a review of related work and a simulation study of one- and two-sided confidence intervals over several different sample sizes, confidence levels, data generating processes, and functionals. Results show that double bootstrap is the overall best method and a viable alternative to typically used approaches in all but the smallest sample sizes.","sentences":["Standard errors, confidence intervals, hypothesis tests, and other quantifications of uncertainty are essential to statistical practice.","However, they feature a plethora of different methods, mathematical formulas, and concepts.","Could we not just replace them all with the general and relatively easy-to-understand non-parametric bootstrap?","We contribute to answering this question with a review of related work and a simulation study of one- and two-sided confidence intervals over several different sample sizes, confidence levels, data generating processes, and functionals.","Results show that double bootstrap is the overall best method and a viable alternative to typically used approaches in all but the smallest sample sizes."],"url":"http://arxiv.org/abs/2403.20182v1","category":"stat.ME"}
{"created":"2024-03-29 13:55:19","title":"Combined spin orientation and phase function of asteroids","abstract":"Large surveys provide numerous non-targeted observations of small bodies (SSOs). The upcoming LSST of the Rubin observatory will be the largest source of SSO photometry in the next decade. With non-coordinated epochs of observation, colors, and therefore taxonomy and composition, can only be computed by comparing absolute magnitudes obtained in each filter by solving the phase function (evolution of brightness of the small body against the solar phase angle). Current models in use in the community (HG, HG12* , HG1G2) however fail to reproduce the long-term photometry of many targets due to the change in aspect angle between apparitions.   We aim at deriving a generic yet simple phase function model accounting for the variable geometry of the SSOs over multiple apparitions.   We propose the sHG1G2 phase function model in which we introduce a term describing the brightness changes due to spin orientation and polar oblateness. We apply this new model to 13,245,908 observations of 122,675 SSOs. These observations were acquired in the g and r filters with the Zwicky Transient Facility. We retrieve them and implement the new sHG1G2 model in Fink, a broker of alerts designed for the LSST.   The sHG1G2 model leads to smaller residuals than other phase function models, providing a better description of the photometry of asteroids. We determine the absolute magnitude H and phase function coefficients (G1, G2) in each filter, the spin orientation (RA_0,DEC_0), and the polar-to-equatorial oblateness R for 95,593 Solar System Objects (SSOs), which constitutes about a tenfold increase in the number of characterised objects compared to current census.   The application of the sHG1G2 model on ZTF alert data using the FINK broker shows that the model is appropriate to extract physical properties of asteroids from multi-band and sparse photometry, such as the forthcoming LSST survey.","sentences":["Large surveys provide numerous non-targeted observations of small bodies (SSOs).","The upcoming LSST of the Rubin observatory will be the largest source of SSO photometry in the next decade.","With non-coordinated epochs of observation, colors, and therefore taxonomy and composition, can only be computed by comparing absolute magnitudes obtained in each filter by solving the phase function (evolution of brightness of the small body against the solar phase angle).","Current models in use in the community (HG, HG12* , HG1G2) however fail to reproduce the long-term photometry of many targets due to the change in aspect angle between apparitions.   ","We aim at deriving a generic yet simple phase function model accounting for the variable geometry of the SSOs over multiple apparitions.   ","We propose the sHG1G2 phase function model in which we introduce a term describing the brightness changes due to spin orientation and polar oblateness.","We apply this new model to 13,245,908 observations of 122,675 SSOs.","These observations were acquired in the g and r filters with the Zwicky Transient Facility.","We retrieve them and implement the new sHG1G2 model in Fink, a broker of alerts designed for the LSST.   ","The sHG1G2 model leads to smaller residuals than other phase function models, providing a better description of the photometry of asteroids.","We determine the absolute magnitude H and phase function coefficients (G1, G2) in each filter, the spin orientation (RA_0,DEC_0), and the polar-to-equatorial oblateness R for 95,593 Solar System Objects (SSOs), which constitutes about a tenfold increase in the number of characterised objects compared to current census.   ","The application of the sHG1G2 model on ZTF alert data using the FINK broker shows that the model is appropriate to extract physical properties of asteroids from multi-band and sparse photometry, such as the forthcoming LSST survey."],"url":"http://arxiv.org/abs/2403.20179v1","category":"astro-ph.EP"}
{"created":"2024-03-29 13:47:47","title":"Artificial consciousness. Some logical and conceptual preliminaries","abstract":"Is artificial consciousness theoretically possible? Is it plausible? If so, is it technically feasible? To make progress on these questions, it is necessary to lay some groundwork clarifying the logical and empirical conditions for artificial consciousness to arise and the meaning of relevant terms involved. Consciousness is a polysemic word: researchers from different fields, including neuroscience, Artificial Intelligence, robotics, and philosophy, among others, sometimes use different terms in order to refer to the same phenomena or the same terms to refer to different phenomena. In fact, if we want to pursue artificial consciousness, a proper definition of the key concepts is required. Here, after some logical and conceptual preliminaries, we argue for the necessity of using dimensions and profiles of consciousness for a balanced discussion about their possible instantiation or realisation in artificial systems. Our primary goal in this paper is to review the main theoretical questions that arise in the domain of artificial consciousness. On the basis of this review, we propose to assess the issue of artificial consciousness within a multidimensional account. The theoretical possibility of artificial consciousness is already presumed within some theoretical frameworks; however, empirical possibility cannot simply be deduced from these frameworks but needs independent empirical validation. We break down the complexity of consciousness by identifying constituents, components, and dimensions, and reflect pragmatically about the general challenges confronting the creation of artificial consciousness. Despite these challenges, we outline a research strategy for showing how \"awareness\" as we propose to understand it could plausibly be realised in artificial systems.","sentences":["Is artificial consciousness theoretically possible?","Is it plausible?","If so, is it technically feasible?","To make progress on these questions, it is necessary to lay some groundwork clarifying the logical and empirical conditions for artificial consciousness to arise and the meaning of relevant terms involved.","Consciousness is a polysemic word: researchers from different fields, including neuroscience, Artificial Intelligence, robotics, and philosophy, among others, sometimes use different terms in order to refer to the same phenomena or the same terms to refer to different phenomena.","In fact, if we want to pursue artificial consciousness, a proper definition of the key concepts is required.","Here, after some logical and conceptual preliminaries, we argue for the necessity of using dimensions and profiles of consciousness for a balanced discussion about their possible instantiation or realisation in artificial systems.","Our primary goal in this paper is to review the main theoretical questions that arise in the domain of artificial consciousness.","On the basis of this review, we propose to assess the issue of artificial consciousness within a multidimensional account.","The theoretical possibility of artificial consciousness is already presumed within some theoretical frameworks; however, empirical possibility cannot simply be deduced from these frameworks but needs independent empirical validation.","We break down the complexity of consciousness by identifying constituents, components, and dimensions, and reflect pragmatically about the general challenges confronting the creation of artificial consciousness.","Despite these challenges, we outline a research strategy for showing how \"awareness\" as we propose to understand it could plausibly be realised in artificial systems."],"url":"http://arxiv.org/abs/2403.20177v1","category":"cs.AI"}
{"created":"2024-03-29 13:41:10","title":"Quadratic optical response of CrSBr controlled by spin-selective interlayer coupling","abstract":"The optical properties of the layered magnet CrSBr are dominated by intralayer excitons: the antiferromagnetic order between the layers makes layer-to-layer charge hopping, and therefore interlayer excitons, spin-forbidden. An external magnetic field, however, continuously drives the magnetic order towards layer-to-layer ferromagnetic, which opens spin-allowed charge-transfer channels between the layers. Here we elaborate how their admixture changes the composition and nature of the excitons, leading to an extension over many layers, and causes a quadratic red-shift with respect to the external magnetic field. We address these effects by ab-initio $GW$-BSE calculations as a function of magnetic field and cast the data into a minimal four-band model to elucidate the interplay between the various interaction and coupling mechanisms. Our findings should be generally valid for antiferromagnetic layered magnets with and without external magnetic fields, and moreover for any couple of layers with different spin directions. Our insights help to systematically address excitons and predict their optical signatures in such systems.","sentences":["The optical properties of the layered magnet CrSBr are dominated by intralayer excitons: the antiferromagnetic order between the layers makes layer-to-layer charge hopping, and therefore interlayer excitons, spin-forbidden.","An external magnetic field, however, continuously drives the magnetic order towards layer-to-layer ferromagnetic, which opens spin-allowed charge-transfer channels between the layers.","Here we elaborate how their admixture changes the composition and nature of the excitons, leading to an extension over many layers, and causes a quadratic red-shift with respect to the external magnetic field.","We address these effects by ab-initio $GW$-BSE calculations as a function of magnetic field and cast the data into a minimal four-band model to elucidate the interplay between the various interaction and coupling mechanisms.","Our findings should be generally valid for antiferromagnetic layered magnets with and without external magnetic fields, and moreover for any couple of layers with different spin directions.","Our insights help to systematically address excitons and predict their optical signatures in such systems."],"url":"http://arxiv.org/abs/2403.20174v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-29 13:36:01","title":"Recovery Sets of Subspaces from a Simplex Code","abstract":"Recovery sets for vectors and subspaces are important in the construction of distributed storage system codes. These concepts are also interesting in their own right. In this paper, we consider the following very basic recovery question: what is the maximum number of possible pairwise disjoint recovery sets for each recovered element? The recovered elements in this work are d-dimensional subspaces of a $k$-dimensional vector space over GF(q). Each server stores one representative for each distinct one-dimensional subspace of the k-dimensional vector space, or equivalently a distinct point of PG(k-1,q). As column vectors, the associated vectors of the stored one-dimensional subspaces form the generator matrix of the $[(q^k -1)/(q-1),k,q^{k-1}]$ simplex code over GF(q). Lower bounds and upper bounds on the maximum number of such recovery sets are provided. It is shown that generally, these bounds are either tight or very close to being tight.","sentences":["Recovery sets for vectors and subspaces are important in the construction of distributed storage system codes.","These concepts are also interesting in their own right.","In this paper, we consider the following very basic recovery question: what is the maximum number of possible pairwise disjoint recovery sets for each recovered element?","The recovered elements in this work are d-dimensional subspaces of a $k$-dimensional vector space over GF(q).","Each server stores one representative for each distinct one-dimensional subspace of the k-dimensional vector space, or equivalently a distinct point of PG(k-1,q).","As column vectors, the associated vectors of the stored one-dimensional subspaces form the generator matrix of the $[(q^k -1)/(q-1),k,q^{k-1}]$ simplex code over GF(q).","Lower bounds and upper bounds on the maximum number of such recovery sets are provided.","It is shown that generally, these bounds are either tight or very close to being tight."],"url":"http://arxiv.org/abs/2403.20170v1","category":"cs.IT"}
{"created":"2024-03-29 13:35:37","title":"Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image Translation","abstract":"Multi-modal brain images from MRI scans are widely used in clinical diagnosis to provide complementary information from different modalities. However, obtaining fully paired multi-modal images in practice is challenging due to various factors, such as time, cost, and artifacts, resulting in modality-missing brain images. To address this problem, unsupervised multi-modal brain image translation has been extensively studied. Existing methods suffer from the problem of brain tumor deformation during translation, as they fail to focus on the tumor areas when translating the whole images. In this paper, we propose an unsupervised tumor-aware distillation teacher-student network called UTAD-Net, which is capable of perceiving and translating tumor areas precisely. Specifically, our model consists of two parts: a teacher network and a student network. The teacher network learns an end-to-end mapping from source to target modality using unpaired images and corresponding tumor masks first. Then, the translation knowledge is distilled into the student network, enabling it to generate more realistic tumor areas and whole images without masks. Experiments show that our model achieves competitive performance on both quantitative and qualitative evaluations of image quality compared with state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the generated images on downstream segmentation tasks. Our code is available at https://github.com/scut-HC/UTAD-Net.","sentences":["Multi-modal brain images from MRI scans are widely used in clinical diagnosis to provide complementary information from different modalities.","However, obtaining fully paired multi-modal images in practice is challenging due to various factors, such as time, cost, and artifacts, resulting in modality-missing brain images.","To address this problem, unsupervised multi-modal brain image translation has been extensively studied.","Existing methods suffer from the problem of brain tumor deformation during translation, as they fail to focus on the tumor areas when translating the whole images.","In this paper, we propose an unsupervised tumor-aware distillation teacher-student network called UTAD-Net, which is capable of perceiving and translating tumor areas precisely.","Specifically, our model consists of two parts: a teacher network and a student network.","The teacher network learns an end-to-end mapping from source to target modality using unpaired images and corresponding tumor masks first.","Then, the translation knowledge is distilled into the student network, enabling it to generate more realistic tumor areas and whole images without masks.","Experiments show that our model achieves competitive performance on both quantitative and qualitative evaluations of image quality compared with state-of-the-art methods.","Furthermore, we demonstrate the effectiveness of the generated images on downstream segmentation tasks.","Our code is available at https://github.com/scut-HC/UTAD-Net."],"url":"http://arxiv.org/abs/2403.20168v1","category":"eess.IV"}
{"created":"2024-03-29 13:29:51","title":"Separation of plane sets by equidistant simple closed curves","abstract":"We prove that if two subsets ${A}$ and ${B}$ of the plane are connected, ${A}$ is bounded, and the Euclidean distance $\\rho({A},{B})$ between ${A}$ and ${B}$ is greater than zero, then for every positive $\\varepsilon<\\rho({A},{B})$, the sets ${A}$ and ${B}$ can be separated by a simple closed curve (also known as a Jordan curve) whose points all lie at distance $\\varepsilon$ from the set ${A}$. We also prove that the $\\varepsilon$-boundary of a connected bounded subset ${A}$ of the plane contains a simple closed curve bounding the domain containing the open $\\varepsilon$-neighbourhood of ${A}$. It is shown that in both statements the connectivity condition can be significantly weakened. We also show that the $\\varepsilon$-boundary of a nonempty bounded subset of the plane contains a simple closed curve. This result complements Morton Brown's statement that the $\\varepsilon$-boundary of a nonempty compact subset of the plane is contained in the union of a finite number of simple closed curves.","sentences":["We prove that if two subsets ${A}$ and ${B}$ of the plane are connected, ${A}$ is bounded, and the Euclidean distance $\\rho({A},{B})$ between ${A}$ and ${B}$ is greater than zero, then for every positive $\\varepsilon<\\rho({A},{B})$, the sets ${A}$ and ${B}$ can be separated by a simple closed curve (also known as a Jordan curve) whose points all lie at distance $\\varepsilon$ from the set ${A}$. We also prove that the $\\varepsilon$-boundary of a connected bounded subset ${A}$ of the plane contains a simple closed curve bounding the domain containing the open $\\varepsilon$-neighbourhood of ${A}$. It is shown that in both statements the connectivity condition can be significantly weakened.","We also show that the $\\varepsilon$-boundary of a nonempty bounded subset of the plane contains a simple closed curve.","This result complements Morton Brown's statement that the $\\varepsilon$-boundary of a nonempty compact subset of the plane is contained in the union of a finite number of simple closed curves."],"url":"http://arxiv.org/abs/2403.20166v1","category":"math.GN"}
{"created":"2024-03-29 13:28:16","title":"A single-bubble source for gravitational waves in a cosmological phase transition","abstract":"We show that quantum fluctuations of an expanding phase transition bubble give rise to gravitational wave (GW) emission, even when considering a single bubble, without bubble collisions or plasma effects. The ratio of GW energy to the total bubble energy reservoir increases with time as $\\propto t$. If the bubble expands for long enough before percolation destroys it, back-reaction due to the GW emission becomes important after $t_{\\rm br}\\sim (16\\pi^5) m_{\\rm pl}^2R_0^3$, where $R_0$ is the bubble nucleation radius and $m_{\\rm pl}$ is the reduced Planck mass. As seen by experiments today, the GW energy spectrum would appear blue. However, simple estimates suggest that the signal falls short of detection by even ambitious future experiments.","sentences":["We show that quantum fluctuations of an expanding phase transition bubble give rise to gravitational wave (GW) emission, even when considering a single bubble, without bubble collisions or plasma effects.","The ratio of GW energy to the total bubble energy reservoir increases with time as $\\propto t$. If the bubble expands for long enough before percolation destroys it, back-reaction due to the GW emission becomes important after $t_{\\rm br}\\sim (16\\pi^5) m_{\\rm pl}^2R_0^3$, where $R_0$ is the bubble nucleation radius and $m_{\\rm pl}$ is the reduced Planck mass.","As seen by experiments today, the GW energy spectrum would appear blue.","However, simple estimates suggest that the signal falls short of detection by even ambitious future experiments."],"url":"http://arxiv.org/abs/2403.20164v1","category":"gr-qc"}
{"created":"2024-03-29 13:25:19","title":"Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning","abstract":"The success of Deep Reinforcement Learning (DRL) is largely attributed to utilizing Artificial Neural Networks (ANNs) as function approximators. Recent advances in neuroscience have unveiled that the human brain achieves efficient reward-based learning, at least by integrating spiking neurons with spatial-temporal dynamics and network topologies with biologically-plausible connectivity patterns. This integration process allows spiking neurons to efficiently combine information across and within layers via nonlinear dendritic trees and lateral interactions. The fusion of these two topologies enhances the network's information-processing ability, crucial for grasping intricate perceptions and guiding decision-making procedures. However, ANNs and brain networks differ significantly. ANNs lack intricate dynamical neurons and only feature inter-layer connections, typically achieved by direct linear summation, without intra-layer connections. This limitation leads to constrained network expressivity. To address this, we propose a novel alternative for function approximator, the Biologically-Plausible Topology improved Spiking Actor Network (BPT-SAN), tailored for efficient decision-making in DRL. The BPT-SAN incorporates spiking neurons with intricate spatial-temporal dynamics and introduces intra-layer connections, enhancing spatial-temporal state representation and facilitating more precise biological simulations. Diverging from the conventional direct linear weighted sum, the BPT-SAN models the local nonlinearities of dendritic trees within the inter-layer connections. For the intra-layer connections, the BPT-SAN introduces lateral interactions between adjacent neurons, integrating them into the membrane potential formula to ensure accurate spike firing.","sentences":["The success of Deep Reinforcement Learning (DRL) is largely attributed to utilizing Artificial Neural Networks (ANNs) as function approximators.","Recent advances in neuroscience have unveiled that the human brain achieves efficient reward-based learning, at least by integrating spiking neurons with spatial-temporal dynamics and network topologies with biologically-plausible connectivity patterns.","This integration process allows spiking neurons to efficiently combine information across and within layers via nonlinear dendritic trees and lateral interactions.","The fusion of these two topologies enhances the network's information-processing ability, crucial for grasping intricate perceptions and guiding decision-making procedures.","However, ANNs and brain networks differ significantly.","ANNs lack intricate dynamical neurons and only feature inter-layer connections, typically achieved by direct linear summation, without intra-layer connections.","This limitation leads to constrained network expressivity.","To address this, we propose a novel alternative for function approximator, the Biologically-Plausible Topology improved Spiking Actor Network (BPT-SAN), tailored for efficient decision-making in DRL.","The BPT-SAN incorporates spiking neurons with intricate spatial-temporal dynamics and introduces intra-layer connections, enhancing spatial-temporal state representation and facilitating more precise biological simulations.","Diverging from the conventional direct linear weighted sum, the BPT-SAN models the local nonlinearities of dendritic trees within the inter-layer connections.","For the intra-layer connections, the BPT-SAN introduces lateral interactions between adjacent neurons, integrating them into the membrane potential formula to ensure accurate spike firing."],"url":"http://arxiv.org/abs/2403.20163v1","category":"cs.NE"}
{"created":"2024-03-29 13:22:03","title":"Existence and Verification of Nash Equilibria in Non-Cooperative Contribution Games with Resource Contention","abstract":"In resource contribution games, a class of non-cooperative games, the players want to obtain a bundle of resources and are endowed with bags of bundles of resources that they can make available into a common for all to enjoy. Available resources can then be used towards their private goals. A player is potentially satisfied with a profile of contributed resources when his bundle could be extracted from the contributed resources. Resource contention occurs when the players who are potentially satisfied, cannot actually all obtain their bundle. The player's preferences are always single-minded (they consider a profile good or they do not) and parsimonious (between two profiles that are equally good, they prefer the profile where they contribute less). What makes a profile of contributed resources good for a player depends on their attitude towards resource contention. We study the problem of deciding whether an outcome is a pure Nash equilibrium for three kinds of players' attitudes towards resource contention: public contention-aversity, private contention-aversity, and contention-tolerance. In particular, we demonstrate that in the general case when the players are contention-averse, then the problem is harder than when they are contention-tolerant. We then identify a natural class of games where, in presence of contention-averse preferences, it becomes tractable, and where there is always a Nash equilibrium.","sentences":["In resource contribution games, a class of non-cooperative games, the players want to obtain a bundle of resources and are endowed with bags of bundles of resources that they can make available into a common for all to enjoy.","Available resources can then be used towards their private goals.","A player is potentially satisfied with a profile of contributed resources when his bundle could be extracted from the contributed resources.","Resource contention occurs when the players who are potentially satisfied, cannot actually all obtain their bundle.","The player's preferences are always single-minded (they consider a profile good or they do not) and parsimonious (between two profiles that are equally good, they prefer the profile where they contribute less).","What makes a profile of contributed resources good for a player depends on their attitude towards resource contention.","We study the problem of deciding whether an outcome is a pure Nash equilibrium for three kinds of players' attitudes towards resource contention: public contention-aversity, private contention-aversity, and contention-tolerance.","In particular, we demonstrate that in the general case when the players are contention-averse, then the problem is harder than when they are contention-tolerant.","We then identify a natural class of games where, in presence of contention-averse preferences, it becomes tractable, and where there is always a Nash equilibrium."],"url":"http://arxiv.org/abs/2403.20161v1","category":"cs.GT"}
{"created":"2024-03-29 13:12:09","title":"ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models","abstract":"In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can ChatGPT detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as BART, ConvBERT, and GPT-2. The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.","sentences":["In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions.","The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection.","Can ChatGPT detect media bias?","This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as BART, ConvBERT, and GPT-2.","The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases."],"url":"http://arxiv.org/abs/2403.20158v1","category":"cs.CL"}
{"created":"2024-03-29 13:05:59","title":"CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening","abstract":"In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, thereby significantly enhancing the overall learning efficiency. We empirically validate our hypothesis and demonstrate the effectiveness of CAESAR in enhancing the learning efficiency of agents, using both a custom-built GridWorld environment and the classical FrozenLake-v1 task, each presenting varying levels of environmental heterogeneity.","sentences":["In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs).","Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance.","However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions.","To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs.","CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism.","By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, thereby significantly enhancing the overall learning efficiency.","We empirically validate our hypothesis and demonstrate the effectiveness of CAESAR in enhancing the learning efficiency of agents, using both a custom-built GridWorld environment and the classical FrozenLake-v1 task, each presenting varying levels of environmental heterogeneity."],"url":"http://arxiv.org/abs/2403.20156v1","category":"cs.LG"}
{"created":"2024-03-29 12:54:53","title":"Energetics of a pulsed quantum battery","abstract":"The challenge of storing energy efficiently and sustainably is highly prominent within modern scientific investigations. Due to the ongoing trend of miniaturization, the design of expressly quantum storage devices is itself a crucial task within current quantum technological research. Here we provide a transparent analytic model of a two-component quantum battery, composed of a charger and an energy holder, which is driven by a short laser pulse. We provide simple expressions for the energy stored in the battery, the maximum amount of work which can be extracted, both the instantaneous and the average powers, and the relevant charging times. This allows us to discuss explicitly the optimal design of the battery in terms of the driving strength of the pulse, the coupling between the charger and the holder, and the inevitable energy loss into the environment. We anticipate that our theory can act as a helpful guide for the nascent experimental work building and characterizing the first generation of truly quantum batteries.","sentences":["The challenge of storing energy efficiently and sustainably is highly prominent within modern scientific investigations.","Due to the ongoing trend of miniaturization, the design of expressly quantum storage devices is itself a crucial task within current quantum technological research.","Here we provide a transparent analytic model of a two-component quantum battery, composed of a charger and an energy holder, which is driven by a short laser pulse.","We provide simple expressions for the energy stored in the battery, the maximum amount of work which can be extracted, both the instantaneous and the average powers, and the relevant charging times.","This allows us to discuss explicitly the optimal design of the battery in terms of the driving strength of the pulse, the coupling between the charger and the holder, and the inevitable energy loss into the environment.","We anticipate that our theory can act as a helpful guide for the nascent experimental work building and characterizing the first generation of truly quantum batteries."],"url":"http://arxiv.org/abs/2403.20155v1","category":"quant-ph"}
{"created":"2024-03-29 12:49:40","title":"Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior","abstract":"Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations.","sentences":["Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames.","However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos.","In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior.","Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio.","Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features.","Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses.","We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations."],"url":"http://arxiv.org/abs/2403.20153v1","category":"cs.CV"}
{"created":"2024-03-29 12:48:33","title":"Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework","abstract":"Investigating solutions of nonlinear equation systems is challenging in a general framework, especially if the equations contain uncertainties about parameters modeled by probability densities. Such random equations, understood as stationary (non-dynamical) equations with parameters as random variables, have a long history and a broad range of applications. In this work, we study nonlinear random equations by combining them with mixture model parameter random variables in order to investigate the combinatorial complexity of such equations and how this can be utilized practically. We derive a general likelihood function and posterior density of approximate best fit solutions while avoiding significant restrictions about the type of nonlinearity or mixture models, and demonstrate their numerically efficient application for the applied researcher. In the results section we are specifically focusing on example simulations of approximate likelihood/posterior solutions for random linear equation systems, nonlinear systems of random conic section equations, as well as applications to portfolio optimization, stochastic control and random matrix theory in order to show the wide applicability of the presented methodology.","sentences":["Investigating solutions of nonlinear equation systems is challenging in a general framework, especially if the equations contain uncertainties about parameters modeled by probability densities.","Such random equations, understood as stationary (non-dynamical) equations with parameters as random variables, have a long history and a broad range of applications.","In this work, we study nonlinear random equations by combining them with mixture model parameter random variables in order to investigate the combinatorial complexity of such equations and how this can be utilized practically.","We derive a general likelihood function and posterior density of approximate best fit solutions while avoiding significant restrictions about the type of nonlinearity or mixture models, and demonstrate their numerically efficient application for the applied researcher.","In the results section we are specifically focusing on example simulations of approximate likelihood/posterior solutions for random linear equation systems, nonlinear systems of random conic section equations, as well as applications to portfolio optimization, stochastic control and random matrix theory in order to show the wide applicability of the presented methodology."],"url":"http://arxiv.org/abs/2403.20152v1","category":"stat.CO"}
{"created":"2024-03-29 12:46:07","title":"A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles","abstract":"Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of automated content generation utilizing AI models. Mobile AIGC services in the Internet of Vehicles (IoV) network have numerous advantages over traditional cloud-based AIGC services, including enhanced network efficiency, better reconfigurability, and stronger data security and privacy. Nonetheless, AIGC service provisioning frequently demands significant resources. Consequently, resource-constrained roadside units (RSUs) face challenges in maintaining a heterogeneous pool of AIGC services and addressing all user service requests without degrading overall performance. Therefore, in this paper, we propose a decentralized incentive mechanism for mobile AIGC service allocation, employing multi-agent deep reinforcement learning to find the balance between the supply of AIGC services on RSUs and user demand for services within the IoV context, optimizing user experience and minimizing transmission latency. Experimental results demonstrate that our approach achieves superior performance compared to other baseline models.","sentences":["Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of automated content generation utilizing AI models.","Mobile AIGC services in the Internet of Vehicles (IoV) network have numerous advantages over traditional cloud-based AIGC services, including enhanced network efficiency, better reconfigurability, and stronger data security and privacy.","Nonetheless, AIGC service provisioning frequently demands significant resources.","Consequently, resource-constrained roadside units (RSUs) face challenges in maintaining a heterogeneous pool of AIGC services and addressing all user service requests without degrading overall performance.","Therefore, in this paper, we propose a decentralized incentive mechanism for mobile AIGC service allocation, employing multi-agent deep reinforcement learning to find the balance between the supply of AIGC services on RSUs and user demand for services within the IoV context, optimizing user experience and minimizing transmission latency.","Experimental results demonstrate that our approach achieves superior performance compared to other baseline models."],"url":"http://arxiv.org/abs/2403.20151v1","category":"cs.AI"}
{"created":"2024-03-29 12:37:57","title":"TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods","abstract":"Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB.","sentences":["Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications.","Not surprisingly, many forecasting methods are being proposed.","To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner.","To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods.","TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines.","To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web.","We also provide a time series characterization to ensure that the selected datasets are comprehensive.","To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods.","To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases.","Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets.","The benchmark code and data are available at https://github.com/decisionintelligence/TFB."],"url":"http://arxiv.org/abs/2403.20150v1","category":"cs.LG"}
{"created":"2024-03-29 12:34:30","title":"A general method to find the spectrum and eigenspaces of the $k$-token of a cycle, and 2-token through continuous fractions","abstract":"The $k$-token graph $F_k(G)$ of a graph $G$ is the graph whose vertices are the $k$-subsets of vertices from $G$, two of which being adjacent whenever their symmetric difference is a pair of adjacent vertices in $G$. In this paper, we propose a general method to find the spectrum and eigenspaces of the $k$-token graph $F_k(C_n)$ of a cycle $C_n$. The method is based on the theory of lift graphs and the recently introduced theory of over-lifts. In the case of $k=2$, we use continuous fractions to derive the spectrum and eigenspaces of the 2-token graph of $C_n$.","sentences":["The $k$-token graph $F_k(G)$ of a graph $G$ is the graph whose vertices are the $k$-subsets of vertices from $G$, two of which being adjacent whenever their symmetric difference is a pair of adjacent vertices in $G$.","In this paper, we propose a general method to find the spectrum and eigenspaces of the $k$-token graph $F_k(C_n)$ of a cycle $C_n$. The method is based on the theory of lift graphs and the recently introduced theory of over-lifts.","In the case of $k=2$, we use continuous fractions to derive the spectrum and eigenspaces of the 2-token graph of $C_n$."],"url":"http://arxiv.org/abs/2403.20148v1","category":"math.CO"}
{"created":"2024-03-29 12:28:34","title":"Unruh effect using Doppler shift method in DSR framework","abstract":"We study the Unruh effect in doubly special relativity (DSR) framework by generalising the Doppler-shift method to DSR. For both the scalar and Dirac particles, we observe a deviation in the power spectrum of Unruh radiation from the standard Bose-Einstein and Fermi-Dirac distributions, respectively, due to the presence of the frame independent length scale of DSR. We further show that this deviation results in the modification of Unruh temperature which then depends non-linearly on the proper acceleration in DSR.","sentences":["We study the Unruh effect in doubly special relativity (DSR) framework by generalising the Doppler-shift method to DSR.","For both the scalar and Dirac particles, we observe a deviation in the power spectrum of Unruh radiation from the standard Bose-Einstein and Fermi-Dirac distributions, respectively, due to the presence of the frame independent length scale of DSR.","We further show that this deviation results in the modification of Unruh temperature which then depends non-linearly on the proper acceleration in DSR."],"url":"http://arxiv.org/abs/2403.20146v1","category":"gr-qc"}
{"created":"2024-03-29 12:25:37","title":"Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries","abstract":"Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators. The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset.","sentences":["Improving mental health support in developing countries is a pressing need.","One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals.","In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations.","We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators.","The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively.","Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset."],"url":"http://arxiv.org/abs/2403.20145v1","category":"cs.CL"}
{"created":"2024-03-29 12:24:50","title":"Echoes from a long time ago: Chewbacca inflation","abstract":"The cosmic microwave background (CMB) radiation offers a unique avenue for exploring the early Universe's dynamics and evolution. In this paper, we delve into the fascinating realm of slow-roll inflation, contextualizing the primordial acoustic perturbations as the resonant echoes akin to the iconic sound of Chewbacca from the Star Wars universe. By extrapolating polynomial potentials for these primordial sounds, we illuminate their role in shaping the inflationary landscape. Leveraging this framework, we calculate the scalar spectral index ($n_s$) and tensor-to-scalar ratio ($r$), providing insights into the underlying physics governing the inflationary epoch. Employing a rigorous chi-square ($\\chi^2$) analysis, we meticulously scrutinize the Planck data combined with that offered by the BICEP/Keck collaboration to identify the Chewbacca sound profile that best aligns with observational constraints. Our findings not only shed light on the intricate interplay between sound and cosmology but also unveil intriguing parallels between the cosmic symphony of the early universe and beloved cultural icons.","sentences":["The cosmic microwave background (CMB) radiation offers a unique avenue for exploring the early Universe's dynamics and evolution.","In this paper, we delve into the fascinating realm of slow-roll inflation, contextualizing the primordial acoustic perturbations as the resonant echoes akin to the iconic sound of Chewbacca from the Star Wars universe.","By extrapolating polynomial potentials for these primordial sounds, we illuminate their role in shaping the inflationary landscape.","Leveraging this framework, we calculate the scalar spectral index ($n_s$) and tensor-to-scalar ratio ($r$), providing insights into the underlying physics governing the inflationary epoch.","Employing a rigorous chi-square ($\\chi^2$) analysis, we meticulously scrutinize the Planck data combined with that offered by the BICEP/Keck collaboration to identify the Chewbacca sound profile that best aligns with observational constraints.","Our findings not only shed light on the intricate interplay between sound and cosmology but also unveil intriguing parallels between the cosmic symphony of the early universe and beloved cultural icons."],"url":"http://arxiv.org/abs/2403.20143v1","category":"astro-ph.CO"}
{"created":"2024-03-29 12:23:58","title":"StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation","abstract":"Most image-to-image translation models postulate that a unique correspondence exists between the semantic classes of the source and target domains. However, this assumption does not always hold in real-world scenarios due to divergent distributions, different class sets, and asymmetrical information representation. As conventional GANs attempt to generate images that match the distribution of the target domain, they may hallucinate spurious instances of classes absent from the source domain, thereby diminishing the usefulness and reliability of translated images. CycleGAN-based methods are also known to hide the mismatched information in the generated images to bypass cycle consistency objectives, a process known as steganography. In response to the challenge of non-bijective image translation, we introduce StegoGAN, a novel model that leverages steganography to prevent spurious features in generated images. Our approach enhances the semantic consistency of the translated images without requiring additional postprocessing or supervision. Our experimental evaluations demonstrate that StegoGAN outperforms existing GAN-based models across various non-bijective image-to-image translation tasks, both qualitatively and quantitatively. Our code and pretrained models are accessible at https://github.com/sian-wusidi/StegoGAN.","sentences":["Most image-to-image translation models postulate that a unique correspondence exists between the semantic classes of the source and target domains.","However, this assumption does not always hold in real-world scenarios due to divergent distributions, different class sets, and asymmetrical information representation.","As conventional GANs attempt to generate images that match the distribution of the target domain, they may hallucinate spurious instances of classes absent from the source domain, thereby diminishing the usefulness and reliability of translated images.","CycleGAN-based methods are also known to hide the mismatched information in the generated images to bypass cycle consistency objectives, a process known as steganography.","In response to the challenge of non-bijective image translation, we introduce StegoGAN, a novel model that leverages steganography to prevent spurious features in generated images.","Our approach enhances the semantic consistency of the translated images without requiring additional postprocessing or supervision.","Our experimental evaluations demonstrate that StegoGAN outperforms existing GAN-based models across various non-bijective image-to-image translation tasks, both qualitatively and quantitatively.","Our code and pretrained models are accessible at https://github.com/sian-wusidi/StegoGAN."],"url":"http://arxiv.org/abs/2403.20142v1","category":"cs.CV"}
{"created":"2024-03-29 12:16:01","title":"Designing Poisson Integrators Through Machine Learning","abstract":"This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry. We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.","sentences":["This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry.","We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units.","Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi).","The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques.","This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data."],"url":"http://arxiv.org/abs/2403.20139v1","category":"math-ph"}
{"created":"2024-03-29 12:15:06","title":"Accurate Block Quantization in LLMs with Outliers","abstract":"The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good quantization accuracy. The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block. In this paper, we focus on the most critical problem of limited KV-cache storage. We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy. We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved. The methodology yields 2x savings in the memory footprint without significant degradation of the model's accuracy. Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency.","sentences":["The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months.","It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement.","The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length.","To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations.","One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor.","These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good quantization accuracy.","The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block.","In this paper, we focus on the most critical problem of limited KV-cache storage.","We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy.","We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved.","The methodology yields 2x savings in the memory footprint without significant degradation of the model's accuracy.","Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency."],"url":"http://arxiv.org/abs/2403.20137v1","category":"cs.AI"}
{"created":"2024-03-29 12:01:31","title":"Differentiated Security Architecture for Secure and Efficient Infotainment Data Communication in IoV Networks","abstract":"This paper aims to provide differentiated security protection for infotainment data communication in Internet-of-Vehicle (IoV) networks. The IoV is a network of vehicles that uses various sensors, software, built-in hardware, and communication technologies to enable information exchange between pedestrians, cars, and urban infrastructure. Negligence on the security of infotainment data communication in IoV networks can unintentionally open an easy access point for social engineering attacks. The attacker can spread false information about traffic conditions, mislead drivers in their directions, and interfere with traffic management. Such attacks can also cause distractions to the driver, which has a potential implication for the safety of driving. The existing literature on IoV communication and network security focuses mainly on generic solutions. In a heterogeneous communication network where different types of communication coexist, we can improve the efficiency of security solutions by considering the different security and efficiency requirements of data communications. Hence, we propose a differentiated security mechanism for protecting infotainment data communication in IoV networks. In particular, we first classify data communication in the IoV network, examine the security focus of each data communication, and then develop a differentiated security architecture to provide security protection on a file-to-file basis. Our architecture leverages Named Data Networking (NDN) so that infotainment files can be efficiently circulated throughout the network where any node can own a copy of the file, thus improving the hit ratio for user file requests. In addition, we propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) scheme for sharing subscription-based infotainment data...","sentences":["This paper aims to provide differentiated security protection for infotainment data communication in Internet-of-Vehicle (IoV) networks.","The IoV is a network of vehicles that uses various sensors, software, built-in hardware, and communication technologies to enable information exchange between pedestrians, cars, and urban infrastructure.","Negligence on the security of infotainment data communication in IoV networks can unintentionally open an easy access point for social engineering attacks.","The attacker can spread false information about traffic conditions, mislead drivers in their directions, and interfere with traffic management.","Such attacks can also cause distractions to the driver, which has a potential implication for the safety of driving.","The existing literature on IoV communication and network security focuses mainly on generic solutions.","In a heterogeneous communication network where different types of communication coexist, we can improve the efficiency of security solutions by considering the different security and efficiency requirements of data communications.","Hence, we propose a differentiated security mechanism for protecting infotainment data communication in IoV networks.","In particular, we first classify data communication in the IoV network, examine the security focus of each data communication, and then develop a differentiated security architecture to provide security protection on a file-to-file basis.","Our architecture leverages Named Data Networking (NDN) so that infotainment files can be efficiently circulated throughout the network where any node can own a copy of the file, thus improving the hit ratio for user file requests.","In addition, we propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) scheme for sharing subscription-based infotainment data..."],"url":"http://arxiv.org/abs/2403.20136v1","category":"cs.CR"}
{"created":"2024-03-29 11:54:13","title":"User Modeling Challenges in Interactive AI Assistant Systems","abstract":"Interactive Artificial Intelligent(AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks. One of the remaining challenges is to understand user's mental states during the task for more personalized guidance. In this work, we analyze users' mental states during task executions and investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance.","sentences":["Interactive Artificial Intelligent(AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks.","One of the remaining challenges is to understand user's mental states during the task for more personalized guidance.","In this work, we analyze users' mental states during task executions and investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance."],"url":"http://arxiv.org/abs/2403.20134v1","category":"cs.CL"}
{"created":"2024-03-29 11:53:28","title":"Regular Games with Imperfect Information Are Not That Regular","abstract":"We consider two-player games with imperfect information and the synthesis of a randomized strategy for one player that ensures the objective is satisfied almost-surely (i.e., with probability 1), regardless of the strategy of the other player. Imperfect information is modeled by an indistinguishability relation %that describing the pairs of histories that the first player cannot distinguish, a generalization of the traditional model with partial observations. The game is regular if it admits a regular function whose kernel commutes with the indistinguishability relation.   The synthesis of pure strategies that ensure all possible outcomes satisfy the objective is possible in regular games, by a generic reduction that holds for all objectives. While the solution for pure strategies extends to randomized strategies in the traditional model with partial observations (which is always regular), we show that a similar reduction does not exist in the more general model. Despite that, we show that in regular games with Buechi objectives the synthesis problem is decidable for randomized strategies that ensure the outcome satisfies the objective almost-surely.","sentences":["We consider two-player games with imperfect information and the synthesis of a randomized strategy for one player that ensures the objective is satisfied almost-surely (i.e., with probability 1), regardless of the strategy of the other player.","Imperfect information is modeled by an indistinguishability relation %that describing the pairs of histories that the first player cannot distinguish, a generalization of the traditional model with partial observations.","The game is regular if it admits a regular function whose kernel commutes with the indistinguishability relation.   ","The synthesis of pure strategies that ensure all possible outcomes satisfy the objective is possible in regular games, by a generic reduction that holds for all objectives.","While the solution for pure strategies extends to randomized strategies in the traditional model with partial observations (which is always regular), we show that a similar reduction does not exist in the more general model.","Despite that, we show that in regular games with Buechi objectives the synthesis problem is decidable for randomized strategies that ensure the outcome satisfies the objective almost-surely."],"url":"http://arxiv.org/abs/2403.20133v1","category":"cs.GT"}
{"created":"2024-03-29 11:46:09","title":"QestOptPOVM: An iterative algorithm to find optimal measurements for quantum parameter estimation","abstract":"Quantum parameter estimation holds significant promise for achieving high precision through the utilization of the most informative measurements. While various lower bounds have been developed to assess the best accuracy for estimates, they are not tight, nor provide a construction of the optimal measurement in general. Thus, determining the explicit forms of optimal measurements has been challenging due to the non-trivial optimization. In this study, we introduce an algorithm, termed QestOptPOVM, designed to directly identify optimal positive operator-valued measure (POVM) using the steepest descent method. Through rigorous testing on several examples for multiple copies of qubit states (up to six copies), we demonstrate the efficiency and accuracy of our proposed algorithm. Moreover, a comparative analysis between numerical results and established lower bounds serves to validate the tightness of the Nagaoka-Hayashi bound in finite-sample quantum metrology for our examples. Concurrently, our algorithm functions as a tool for elucidating the explicit forms of optimal POVMs, thereby enhancing our understanding of quantum parameter estimation methodologies.","sentences":["Quantum parameter estimation holds significant promise for achieving high precision through the utilization of the most informative measurements.","While various lower bounds have been developed to assess the best accuracy for estimates, they are not tight, nor provide a construction of the optimal measurement in general.","Thus, determining the explicit forms of optimal measurements has been challenging due to the non-trivial optimization.","In this study, we introduce an algorithm, termed QestOptPOVM, designed to directly identify optimal positive operator-valued measure (POVM) using the steepest descent method.","Through rigorous testing on several examples for multiple copies of qubit states (up to six copies), we demonstrate the efficiency and accuracy of our proposed algorithm.","Moreover, a comparative analysis between numerical results and established lower bounds serves to validate the tightness of the Nagaoka-Hayashi bound in finite-sample quantum metrology for our examples.","Concurrently, our algorithm functions as a tool for elucidating the explicit forms of optimal POVMs, thereby enhancing our understanding of quantum parameter estimation methodologies."],"url":"http://arxiv.org/abs/2403.20131v1","category":"quant-ph"}
{"created":"2024-03-29 11:44:14","title":"Sound event localization and classification using WASN in Outdoor Environment","abstract":"Deep learning-based sound event localization and classification is an emerging research area within wireless acoustic sensor networks. However, current methods for sound event localization and classification typically rely on a single microphone array, making them susceptible to signal attenuation and environmental noise, which limits their monitoring range. Moreover, methods using multiple microphone arrays often focus solely on source localization, neglecting the aspect of sound event classification. In this paper, we propose a deep learning-based method that employs multiple features and attention mechanisms to estimate the location and class of sound source. We introduce a Soundmap feature to capture spatial information across multiple frequency bands. We also use the Gammatone filter to generate acoustic features more suitable for outdoor environments. Furthermore, we integrate attention mechanisms to learn channel-wise relationships and temporal dependencies within the acoustic features. To evaluate our proposed method, we conduct experiments using simulated datasets with different levels of noise and size of monitoring areas, as well as different arrays and source positions. The experimental results demonstrate the superiority of our proposed method over state-of-the-art methods in both sound event classification and sound source localization tasks. And we provide further analysis to explain the reasons for the observed errors.","sentences":["Deep learning-based sound event localization and classification is an emerging research area within wireless acoustic sensor networks.","However, current methods for sound event localization and classification typically rely on a single microphone array, making them susceptible to signal attenuation and environmental noise, which limits their monitoring range.","Moreover, methods using multiple microphone arrays often focus solely on source localization, neglecting the aspect of sound event classification.","In this paper, we propose a deep learning-based method that employs multiple features and attention mechanisms to estimate the location and class of sound source.","We introduce a Soundmap feature to capture spatial information across multiple frequency bands.","We also use the Gammatone filter to generate acoustic features more suitable for outdoor environments.","Furthermore, we integrate attention mechanisms to learn channel-wise relationships and temporal dependencies within the acoustic features.","To evaluate our proposed method, we conduct experiments using simulated datasets with different levels of noise and size of monitoring areas, as well as different arrays and source positions.","The experimental results demonstrate the superiority of our proposed method over state-of-the-art methods in both sound event classification and sound source localization tasks.","And we provide further analysis to explain the reasons for the observed errors."],"url":"http://arxiv.org/abs/2403.20130v1","category":"cs.SD"}
{"created":"2024-03-29 11:33:34","title":"The Impact of Prompts on Zero-Shot Detection of AI-Generated Text","abstract":"In recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text. We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information. Our experiments reveal the significant influence of prompts on detection accuracy. Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested. Code is available: \\url{https://github.com/kaito25atugich/Detector}.","sentences":["In recent years, there have been significant advancements in the development of Large Language Models (LLMs).","While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns.","To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated.","Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based.","In chat-based applications, users commonly input prompts and utilize the AI-generated texts.","However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts.","It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase.","So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors.","In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text.","We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information.","Our experiments reveal the significant influence of prompts on detection accuracy.","Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested.","Code is available: \\url{https://github.com/kaito25atugich/Detector}."],"url":"http://arxiv.org/abs/2403.20127v1","category":"cs.AI"}
{"created":"2024-03-29 11:31:09","title":"Scanning quantum vortex microscopy reveals thickness-dependent pinning nano-network in superconducting Nb-films","abstract":"The presence of quantum vortices determines the electromagnetic response of superconducting materials and devices. Controlling the vortex motion, their pinning on intrinsic and artificial defects is therefore essential for superconducting electronics. Here we take advantage of the attractive force between a magnetic cantilever of the Magnetic Force Microscope and a single quantum vortex to spatially map the pinning force inside 50-240 nm thick magnetron-sputtered Nb-films, commonly used in advanced superconducting electronics. The revealed pinning nano-network is related to the thickness-dependent granular structure of the films as well as to the characteristic microscopic scales of superconductivity. Our approach is general, and can be directly applied to other type II granular superconducting materials and nanodevices.","sentences":["The presence of quantum vortices determines the electromagnetic response of superconducting materials and devices.","Controlling the vortex motion, their pinning on intrinsic and artificial defects is therefore essential for superconducting electronics.","Here we take advantage of the attractive force between a magnetic cantilever of the Magnetic Force Microscope and a single quantum vortex to spatially map the pinning force inside 50-240 nm thick magnetron-sputtered Nb-films, commonly used in advanced superconducting electronics.","The revealed pinning nano-network is related to the thickness-dependent granular structure of the films as well as to the characteristic microscopic scales of superconductivity.","Our approach is general, and can be directly applied to other type II granular superconducting materials and nanodevices."],"url":"http://arxiv.org/abs/2403.20125v1","category":"cond-mat.supr-con"}
{"created":"2024-03-29 11:23:10","title":"Learning using granularity statistical invariants for classification","abstract":"Learning using statistical invariants (LUSI) is a new learning paradigm, which adopts weak convergence mechanism, and can be applied to a wider range of classification problems. However, the computation cost of invariant matrices in LUSI is high for large-scale datasets during training. To settle this issue, this paper introduces a granularity statistical invariant for LUSI, and develops a new learning paradigm called learning using granularity statistical invariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms, taking a perspective of minimizing expected risk. As far as we know, it is the first time to construct granularity statistical invariants. Compared to LUSI, the introduction of this new statistical invariant brings two advantages. Firstly, it enhances the structural information of the data. Secondly, LUGSI transforms a large invariant matrix into a smaller one by maximizing the distance between classes, achieving feasibility for large-scale datasets classification problems and significantly enhancing the training speed of model operations. Experimental results indicate that LUGSI not only exhibits improved generalization capabilities but also demonstrates faster training speed, particularly for large-scale datasets.","sentences":["Learning using statistical invariants (LUSI) is a new learning paradigm, which adopts weak convergence mechanism, and can be applied to a wider range of classification problems.","However, the computation cost of invariant matrices in LUSI is high for large-scale datasets during training.","To settle this issue, this paper introduces a granularity statistical invariant for LUSI, and develops a new learning paradigm called learning using granularity statistical invariants (LUGSI).","LUGSI employs both strong and weak convergence mechanisms, taking a perspective of minimizing expected risk.","As far as we know, it is the first time to construct granularity statistical invariants.","Compared to LUSI, the introduction of this new statistical invariant brings two advantages.","Firstly, it enhances the structural information of the data.","Secondly, LUGSI transforms a large invariant matrix into a smaller one by maximizing the distance between classes, achieving feasibility for large-scale datasets classification problems and significantly enhancing the training speed of model operations.","Experimental results indicate that LUGSI not only exhibits improved generalization capabilities but also demonstrates faster training speed, particularly for large-scale datasets."],"url":"http://arxiv.org/abs/2403.20122v1","category":"cs.LG"}
{"created":"2024-03-29 10:59:40","title":"LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving","abstract":"Existing Vision-Language models (VLMs) estimate either long-term trajectory waypoints or a set of control actions as a reactive solution for closed-loop planning based on their rich scene comprehension. However, these estimations are coarse and are subjective to their \"world understanding\" which may generate sub-optimal decisions due to perception errors. In this paper, we introduce LeGo-Drive, which aims to address this issue by estimating a goal location based on the given language command as an intermediate representation in an end-to-end setting. The estimated goal might fall in a non-desirable region, like on top of a car for a parking-like command, leading to inadequate planning. Hence, we propose to train the architecture in an end-to-end manner, resulting in iterative refinement of both the goal and the trajectory collectively. We validate the effectiveness of our method through comprehensive experiments conducted in diverse simulated environments. We report significant improvements in standard autonomous driving metrics, with a goal reaching Success Rate of 81%. We further showcase the versatility of LeGo-Drive across different driving scenarios and linguistic inputs, underscoring its potential for practical deployment in autonomous vehicles and intelligent transportation systems.","sentences":["Existing Vision-Language models (VLMs) estimate either long-term trajectory waypoints or a set of control actions as a reactive solution for closed-loop planning based on their rich scene comprehension.","However, these estimations are coarse and are subjective to their \"world understanding\" which may generate sub-optimal decisions due to perception errors.","In this paper, we introduce LeGo-Drive, which aims to address this issue by estimating a goal location based on the given language command as an intermediate representation in an end-to-end setting.","The estimated goal might fall in a non-desirable region, like on top of a car for a parking-like command, leading to inadequate planning.","Hence, we propose to train the architecture in an end-to-end manner, resulting in iterative refinement of both the goal and the trajectory collectively.","We validate the effectiveness of our method through comprehensive experiments conducted in diverse simulated environments.","We report significant improvements in standard autonomous driving metrics, with a goal reaching Success Rate of 81%.","We further showcase the versatility of LeGo-Drive across different driving scenarios and linguistic inputs, underscoring its potential for practical deployment in autonomous vehicles and intelligent transportation systems."],"url":"http://arxiv.org/abs/2403.20116v1","category":"cs.RO"}
{"created":"2024-03-29 10:56:58","title":"High entropy metallic glasses, what does it mean?","abstract":"We performed calorimetric measurements on 30 bulk metallic glasses differing with their mixing entropies DSmix. On this basis, the excess entropies DS and excess enthalpies DH of glasses with respect to their maternal crystalline states are calculated. It is found that the excess entropy DS on the average decreases with increasing mixing entropy DSmix. This means that so-called \"high-entropymetallic glasses\" (i.e. the glasses having high DSmix) actually constitute glasses with low excess entropy DS. We predict that such glasses should have reduced relaxation ability. We also found that the excess enthalpy DH of glass linearly increases with its excess entropy DS, in line with a general thermodynamic estimate.","sentences":["We performed calorimetric measurements on 30 bulk metallic glasses differing with their mixing entropies DSmix.","On this basis, the excess entropies DS and excess enthalpies DH of glasses with respect to their maternal crystalline states are calculated.","It is found that the excess entropy DS on the average decreases with increasing mixing entropy DSmix.","This means that so-called \"high-entropymetallic glasses\" (i.e. the glasses having high DSmix) actually constitute glasses with low excess entropy DS.","We predict that such glasses should have reduced relaxation ability.","We also found that the excess enthalpy DH of glass linearly increases with its excess entropy DS, in line with a general thermodynamic estimate."],"url":"http://arxiv.org/abs/2403.20115v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-29 10:52:18","title":"Static versus dynamically polarizable environments within the many-body $\\bf{GW}$ formalism","abstract":"Continuum or discrete polarizable models for the study of optoelectronic processes in embedded subsystems rely mostly on the restriction of the surrounding electronic dielectric response to its low frequency limit. Such a description hinges on the assumption that the electrons in the surrounding medium react instantaneously to any excitation in the central subsystem, treating thus the environment in the adiabatic limit. Exploiting a recently developed embedded $GW$ formalism, with an environment described at the fully ab initio level, we assess the merits of the adiabatic limit with respect to an environment where the full dynamics of the dielectric response is considered. Further, we show how to properly take the static limit of the environment susceptibility, introducing the so-called Coulomb-hole and screened-exchange contributions to the reaction field. As a first application, we consider a C$_{60}$ molecule at the surface of a C$_{60}$ crystal, namely a case where the dynamics of the embedded and embedding subsystems are similar. The common adiabatic assumption, when properly treated, generates errors below $10\\%$ on the polarization energy associated with frontier energy levels and associated energy gaps. Finally, we consider a water molecule inside a metallic nanotube, the worst case for the environment adiabatic limit. The error on the gap polarization energy remains below $10\\%$, even though the error on the frontier orbitals polarization energies can reach a few tenths of an electronvolt.","sentences":["Continuum or discrete polarizable models for the study of optoelectronic processes in embedded subsystems rely mostly on the restriction of the surrounding electronic dielectric response to its low frequency limit.","Such a description hinges on the assumption that the electrons in the surrounding medium react instantaneously to any excitation in the central subsystem, treating thus the environment in the adiabatic limit.","Exploiting a recently developed embedded $GW$ formalism, with an environment described at the fully ab initio level, we assess the merits of the adiabatic limit with respect to an environment where the full dynamics of the dielectric response is considered.","Further, we show how to properly take the static limit of the environment susceptibility, introducing the so-called Coulomb-hole and screened-exchange contributions to the reaction field.","As a first application, we consider a C$_{60}$ molecule at the surface of a C$_{60}$ crystal, namely a case where the dynamics of the embedded and embedding subsystems are similar.","The common adiabatic assumption, when properly treated, generates errors below $10\\%$ on the polarization energy associated with frontier energy levels and associated energy gaps.","Finally, we consider a water molecule inside a metallic nanotube, the worst case for the environment adiabatic limit.","The error on the gap polarization energy remains below $10\\%$, even though the error on the frontier orbitals polarization energies can reach a few tenths of an electronvolt."],"url":"http://arxiv.org/abs/2403.20114v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-29 10:44:51","title":"Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation","abstract":"Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including penalized LogP, QED, and celecoxib similarity. We believe that Mol-AIR represents a significant advancement in drug discovery, offering a more efficient path to discovering novel therapeutics.","sentences":["Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery.","Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties.","Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties.","To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation.","Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies.","In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including penalized LogP, QED, and celecoxib similarity.","We believe that Mol-AIR represents a significant advancement in drug discovery, offering a more efficient path to discovering novel therapeutics."],"url":"http://arxiv.org/abs/2403.20109v1","category":"cs.LG"}
{"created":"2024-03-29 10:38:25","title":"FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models","abstract":"Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches. We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models. Project page: https://bcorrad.github.io/freesegdiff/","sentences":["Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks.","Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation.","Image generative models are trained on massive datasets that provide them with powerful internal spatial representations.","In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks.","We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations.","To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free.","In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation.","The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively.","The features are clustered and binarized to obtain class agnostic masks for each object.","These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary.","Finally, we add a refinement step that allows to obtain a more precise segmentation mask.","Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets.","In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches.","We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models.","Project page: https://bcorrad.github.io/freesegdiff/"],"url":"http://arxiv.org/abs/2403.20105v1","category":"cs.CV"}
{"created":"2024-03-29 10:33:41","title":"Towards Efficient Aggregation of Storage Flexibilities in Power Grids","abstract":"The increasing penetration of volatile renewables combined with increasing demands poses a challenge to modern power grids. Furthermore, distributed energy resources and flexible devices (electric vehicles, PV generation, ...) are becoming more widespread, making their aggregate usage for ancillary services interesting. However, accurately quantifying the aggregate flexibility of numerous flexible devices is known to be limited by the curse of dimensionality, i.e., it does not scale well computationally. This has led to the development of various approximation algorithms. In this study, we improve upon our previously proposed vertex-based inner approximation, extending it to more general storage devices. We demonstrate the efficacy and accuracy of the proposed method in a case study comparing our approach with an exact centralized control framework, where the flexibility of numerous electric vehicles is combined to reduce the peak load in a residential area.","sentences":["The increasing penetration of volatile renewables combined with increasing demands poses a challenge to modern power grids.","Furthermore, distributed energy resources and flexible devices (electric vehicles, PV generation, ...) are becoming more widespread, making their aggregate usage for ancillary services interesting.","However, accurately quantifying the aggregate flexibility of numerous flexible devices is known to be limited by the curse of dimensionality, i.e., it does not scale well computationally.","This has led to the development of various approximation algorithms.","In this study, we improve upon our previously proposed vertex-based inner approximation, extending it to more general storage devices.","We demonstrate the efficacy and accuracy of the proposed method in a case study comparing our approach with an exact centralized control framework, where the flexibility of numerous electric vehicles is combined to reduce the peak load in a residential area."],"url":"http://arxiv.org/abs/2403.20104v1","category":"math.OC"}
{"created":"2024-03-29 10:32:44","title":"NLP for Counterspeech against Hate: A Survey and How-To Guide","abstract":"In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate. These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence. Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it. In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources. In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic. Finally, we discuss open challenges and future directions of counterspeech research in NLP.","sentences":["In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate.","These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence.","Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it.","In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources.","In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic.","Finally, we discuss open challenges and future directions of counterspeech research in NLP."],"url":"http://arxiv.org/abs/2403.20103v1","category":"cs.CL"}
{"created":"2024-03-29 10:31:38","title":"On AdS$_3$/ICFT$_2$ with a dynamical scalar field located on the brane","abstract":"We exploit holographic duality to study the system of a one-dimensional interface contacting two semi-infinite two-dimensional CFTs. Central to our investigation is the introduction of a dynamical scalar field located on the bulk interface brane which breaks the scaling symmetry of the dual interface field theory, along with its consequential backreaction on the system. We define an interface entropy from holographic entanglement entropy. At zero temperature we construct several illustrative examples and observe that the $g$-theorem is always satisfied. These examples also reveal distinct features of the interface entropy that are intricately linked to the scalar potential profiles. At finite temperature we find that the dynamical scalar field enables the bulk theory to have new configurations which would be infeasible solely with a tension term on the interface brane.","sentences":["We exploit holographic duality to study the system of a one-dimensional interface contacting two semi-infinite two-dimensional CFTs.","Central to our investigation is the introduction of a dynamical scalar field located on the bulk interface brane which breaks the scaling symmetry of the dual interface field theory, along with its consequential backreaction on the system.","We define an interface entropy from holographic entanglement entropy.","At zero temperature we construct several illustrative examples and observe that the $g$-theorem is always satisfied.","These examples also reveal distinct features of the interface entropy that are intricately linked to the scalar potential profiles.","At finite temperature we find that the dynamical scalar field enables the bulk theory to have new configurations which would be infeasible solely with a tension term on the interface brane."],"url":"http://arxiv.org/abs/2403.20102v1","category":"hep-th"}
{"created":"2024-03-29 10:23:18","title":"ITCMA: A Generative Agent Based on a Computational Consciousness Structure","abstract":"Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.","sentences":["Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge.","In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.","This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure.","We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings.","ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment.","Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set.","Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization.","In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings."],"url":"http://arxiv.org/abs/2403.20097v1","category":"cs.AI"}
{"created":"2024-03-29 10:18:21","title":"Controlling the dynamics of atomic correlations via the coupling to a dissipative cavity","abstract":"In this Letter, we report the onset of periodic oscillations of coherences in an interacting bosonic gas coupled to a resonator after a quantum quench. This dynamics extends the collapse and revival features of atomic correlations in optical lattices to a dissipative scenario and exhibits hallmarks of synchronization. The behavior emerges from the interplay of the quantum dissipative nature of the cavity field and the presence of a (approximate) strong symmetry in the dissipative system, providing a general recipe to engineer intriguing quantum dynamics. Additionally, we show that the approximate symmetry can arise dynamically during self-organization and can be employed to obtain long-lived coherences.","sentences":["In this Letter, we report the onset of periodic oscillations of coherences in an interacting bosonic gas coupled to a resonator after a quantum quench.","This dynamics extends the collapse and revival features of atomic correlations in optical lattices to a dissipative scenario and exhibits hallmarks of synchronization.","The behavior emerges from the interplay of the quantum dissipative nature of the cavity field and the presence of a (approximate) strong symmetry in the dissipative system, providing a general recipe to engineer intriguing quantum dynamics.","Additionally, we show that the approximate symmetry can arise dynamically during self-organization and can be employed to obtain long-lived coherences."],"url":"http://arxiv.org/abs/2403.20096v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-29 10:05:29","title":"Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation","abstract":"Images from outdoor scenes may be taken under various weather conditions. It is well studied that weather impacts the performance of computer vision algorithms and needs to be handled properly. However, existing algorithms model weather condition as a discrete status and estimate it using multi-label classification. The fact is that, physically, specifically in meteorology, weather are modeled as a continuous and transitional status. Instead of directly implementing hard classification as existing multi-weather classification methods do, we consider the physical formulation of multi-weather conditions and model the impact of physical-related parameter on learning from the image appearance. In this paper, we start with solid revisit of the physics definition of weather and how it can be described as a continuous machine learning and computer vision task. Namely, we propose to model the weather uncertainty, where the level of probability and co-existence of multiple weather conditions are both considered. A Gaussian mixture model is used to encapsulate the weather uncertainty and a uncertainty-aware multi-weather learning scheme is proposed based on prior-posterior learning. A novel multi-weather co-presence estimation transformer (MeFormer) is proposed. In addition, a new multi-weather co-presence estimation (MePe) dataset, along with 14 fine-grained weather categories and 16,078 samples, is proposed to benchmark both conventional multi-label weather classification task and multi-weather co-presence estimation task. Large scale experiments show that the proposed method achieves state-of-the-art performance and substantial generalization capabilities on both the conventional multi-label weather classification task and the proposed multi-weather co-presence estimation task. Besides, modeling weather uncertainty also benefits adverse-weather semantic segmentation.","sentences":["Images from outdoor scenes may be taken under various weather conditions.","It is well studied that weather impacts the performance of computer vision algorithms and needs to be handled properly.","However, existing algorithms model weather condition as a discrete status and estimate it using multi-label classification.","The fact is that, physically, specifically in meteorology, weather are modeled as a continuous and transitional status.","Instead of directly implementing hard classification as existing multi-weather classification methods do, we consider the physical formulation of multi-weather conditions and model the impact of physical-related parameter on learning from the image appearance.","In this paper, we start with solid revisit of the physics definition of weather and how it can be described as a continuous machine learning and computer vision task.","Namely, we propose to model the weather uncertainty, where the level of probability and co-existence of multiple weather conditions are both considered.","A Gaussian mixture model is used to encapsulate the weather uncertainty and a uncertainty-aware multi-weather learning scheme is proposed based on prior-posterior learning.","A novel multi-weather co-presence estimation transformer (MeFormer) is proposed.","In addition, a new multi-weather co-presence estimation (MePe) dataset, along with 14 fine-grained weather categories and 16,078 samples, is proposed to benchmark both conventional multi-label weather classification task and multi-weather co-presence estimation task.","Large scale experiments show that the proposed method achieves state-of-the-art performance and substantial generalization capabilities on both the conventional multi-label weather classification task and the proposed multi-weather co-presence estimation task.","Besides, modeling weather uncertainty also benefits adverse-weather semantic segmentation."],"url":"http://arxiv.org/abs/2403.20092v1","category":"cs.CV"}
{"created":"2024-03-29 09:54:09","title":"Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness","abstract":"The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years. However, from a legal perspective, particularly from European Union law, many open questions remain. Whereas algorithmic fairness aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed. The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models. Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements.","sentences":["The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years.","However, from a legal perspective, particularly from European Union law, many open questions remain.","Whereas algorithmic fairness aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed.","The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models.","Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements."],"url":"http://arxiv.org/abs/2403.20089v1","category":"cs.AI"}
{"created":"2024-03-29 09:32:45","title":"Phase structure of the de Sitter Spacetime with KR field based on the Lyapunov exponent","abstract":"Since the spontaneously broken of the Lorentz symmetry in the gravity theory with the non-minimally coupling between the Kalb-Ramond (KR) field (that acquires a nonzero vacuum expectation value) and the Einstein gravity, there exists the exactly static and spherically symmetric black holes solutions related with the Lorentz violating parameter. Based on this, we consider the corresponding black hole solution in the de-Sitter (dS) spacetime with the KR field and investigate the thermodynamic properties in the expanded phase space through introducing the interplay entropy between the black hole and cosmological horizons. Especially we analyze the effect of the Lorentz-violating parameter on the thermodynamic properties. Furthermore, the Lyapunov exponent and the shadow of these static and spherically symmetric black holes in this Lorentz-violating gravity theory are also investigated. These study will open a new perspective to probe the thermodynamics of black holes.","sentences":["Since the spontaneously broken of the Lorentz symmetry in the gravity theory with the non-minimally coupling between the Kalb-Ramond (KR) field (that acquires a nonzero vacuum expectation value) and the Einstein gravity, there exists the exactly static and spherically symmetric black holes solutions related with the Lorentz violating parameter.","Based on this, we consider the corresponding black hole solution in the de-Sitter (dS) spacetime with the KR field and investigate the thermodynamic properties in the expanded phase space through introducing the interplay entropy between the black hole and cosmological horizons.","Especially we analyze the effect of the Lorentz-violating parameter on the thermodynamic properties.","Furthermore, the Lyapunov exponent and the shadow of these static and spherically symmetric black holes in this Lorentz-violating gravity theory are also investigated.","These study will open a new perspective to probe the thermodynamics of black holes."],"url":"http://arxiv.org/abs/2403.20083v1","category":"hep-th"}
{"created":"2024-03-29 09:28:39","title":"Phase space analysis of finite and infinite dimensional Fresnel integrals","abstract":"The full characterization of the class of Fresnel integrable functions is an open problem in functional analysis, with significant applications to mathematical physics (Feynman path integrals) and the analysis of the Schr\\\"odinger equation. In finite dimension, we prove the Fresnel integrability of functions in the Sj\\\"ostrand class $M^{\\infty,1}$ - a family of continuous and bounded functions, locally enjoying the mild regularity of the Fourier transform of an integrable function. This result broadly extends the current knowledge on the Fresnel integrability of Fourier transforms of finite complex measures, and relies upon ideas and techniques of Gabor wave packet analysis. We also discuss the problem of designing infinite-dimensional extensions of this result, obtaining the first, non-trivial concrete realization of a general framework of projective functional extensions introduced by Albeverio and Mazzucchi. As an interesting byproduct, we obtain the exact $M^{\\infty,1} \\to L^\\infty$ operator norm of the free Schr\\\"odinger evolution operator.","sentences":["The full characterization of the class of Fresnel integrable functions is an open problem in functional analysis, with significant applications to mathematical physics (Feynman path integrals) and the analysis of the Schr\\\"odinger equation.","In finite dimension, we prove the Fresnel integrability of functions in the Sj\\\"ostrand class $M^{\\infty,1}$ - a family of continuous and bounded functions, locally enjoying the mild regularity of the Fourier transform of an integrable function.","This result broadly extends the current knowledge on the Fresnel integrability of Fourier transforms of finite complex measures, and relies upon ideas and techniques of Gabor wave packet analysis.","We also discuss the problem of designing infinite-dimensional extensions of this result, obtaining the first, non-trivial concrete realization of a general framework of projective functional extensions introduced by Albeverio and Mazzucchi.","As an interesting byproduct, we obtain the exact $M^{\\infty,1} \\to L^\\infty$ operator norm of the free Schr\\\"odinger evolution operator."],"url":"http://arxiv.org/abs/2403.20082v1","category":"math.FA"}
{"created":"2024-03-29 09:19:52","title":"Negative Label Guided OOD Detection with Pretrained Vision-Language Models","abstract":"Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.","sentences":["Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs.","Extensive research has been dedicated to exploring OOD detection in the vision modality.","Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality.","In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases.","We design a novel scheme for the OOD score collaborated with negative labels.","Theoretical analysis helps to understand the mechanism of negative labels.","Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures.","Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts.","The codes are available at https://github.com/tmlr-group/NegLabel."],"url":"http://arxiv.org/abs/2403.20078v1","category":"cs.CV"}
{"created":"2024-03-29 09:18:43","title":"Tannaka-Krein duality for Roelcke-precompact non-archimedean Polish groups","abstract":"Let G be a Roelcke-precompact non-archimedean Polish group, B(G) the algebra of matrix coefficients of G arising from its continuous unitary representations. The Gel'fand spectrum H(G) of the norm closure of B(G) is known as the Hilbert compactification of G. Let A be the dense subalgebra of B(G) generated by indicator maps of open cosets in G. We prove that multiplicative linear functionals on A are automatically continuous, generalizing a result of Krein for finite dimensional representations of topological groups. We deduce two abstract realizations of H(G). One is the space P(MG) of partial isomorphisms with algebraically closed domain of MG, the countable set of open cosets of G seen as a homogeneous first order logical structure. The other is T(G) the Tannaka monoid of G. We also obtain that the natural functor that sends G to the category of its representations is full and faithful.","sentences":["Let G be a Roelcke-precompact non-archimedean Polish group, B(G) the algebra of matrix coefficients of G arising from its continuous unitary representations.","The Gel'fand spectrum H(G) of the norm closure of B(G) is known as the Hilbert compactification of G. Let A be the dense subalgebra of B(G) generated by indicator maps of open cosets in G.","We prove that multiplicative linear functionals on A are automatically continuous, generalizing a result of Krein for finite dimensional representations of topological groups.","We deduce two abstract realizations of H(G).","One is the space P(MG) of partial isomorphisms with algebraically closed domain of MG, the countable set of open cosets of G seen as a homogeneous first order logical structure.","The other is T(G) the Tannaka monoid of G. We also obtain that the natural functor that sends G to the category of its representations is full and faithful."],"url":"http://arxiv.org/abs/2403.20077v1","category":"math.GR"}
{"created":"2024-03-29 09:06:57","title":"Hochschild cohomology of the quadratic monomial algebra ${\\rm N}_m$","abstract":"Let ${\\rm N}_m(R) = \\{ (a_{ij}) \\in {\\rm M}_m(R) \\mid a_{11} = a_{22} = \\cdots = a_{mm} \\mbox{ and } a_{ij} = 0 \\mbox{ for any } i > j \\}$ for a commutative ring $R$. Then ${\\rm N}_m(R)$ is a quadratic monomial algebra over $R$. We calculate ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm M}_m(R)/{\\rm N}_m(R))$ as $R$-modules. We also determine the $R$-algebra structure of the Hochschild cohomology ring ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm N}_m(R))$. For $m \\ge 3$, ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm N}_m(R))$ is an infinitely generated algebra over $R$ and has no Batalin-Vilkovisky algebra structure giving the Gerstenhaber bracket.","sentences":["Let ${\\rm N}_m(R)","= \\{ (a_{ij}) \\in {\\rm M}_m(R)","\\mid a_{11} = a_{22} = \\cdots = a_{mm} \\mbox{ and } a_{ij} = 0","\\mbox{ for any } i >","j \\}$ for a commutative ring $R$. Then ${\\rm N}_m(R)$ is a quadratic monomial algebra over $R$. We calculate ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm M}_m(R)/{\\rm","N}_m(R))$ as $R$-modules.","We also determine the $R$-algebra structure of the Hochschild cohomology ring ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm N}_m(R))$. For $m \\ge 3$, ${\\rm HH}^{\\ast}({\\rm N}_m(R), {\\rm N}_m(R))$ is an infinitely generated algebra over $R$ and has no Batalin-Vilkovisky algebra structure giving the Gerstenhaber bracket."],"url":"http://arxiv.org/abs/2403.20074v1","category":"math.RA"}
{"created":"2024-03-29 09:04:31","title":"A binary version of the Mahler-Popken complexity function","abstract":"The (Mahler-Popken) complexity $\\| n \\|$ of a natural number $n$ is the smallest number of ones that can be used via combinations of multiplication and addition to express $n$, with parentheses arranged in such a way so as to form legal nestings. We generalize $\\| \\cdot \\|$ by defining $\\| n \\|_{m}$ as the smallest number of possibly repeated selections from $\\{ 1, 2, \\ldots, m \\}$ (counting repetitions), for fixed $m \\in \\mathbb{N}$, that can be used to express $n$ with the same operational and bracket symbols as before. There is a close relationship, as we explore, between $\\|\\cdot\\|_{2}$ and lengths of shortest addition chains for a given natural number. This illustrates how remarkable it is that $(\\| n \\|_{2} : n \\in \\mathbb{N} )$ is not currently included in the On-Line Encyclopedia of Integer Sequences and has, apparently, not been studied previously. This, in turn, motivates our exploration of the complexity function $\\| \\cdot\\|_{2}$, in which we prove explicit upper and lower bounds for $\\|\\cdot\\|_{2}$ and describe some problems and further areas of research concerning $\\|\\cdot\\|_{2}$.","sentences":["The (Mahler-Popken) complexity $\\| n \\|$ of a natural number $n$ is the smallest number of ones that can be used via combinations of multiplication and addition to express $n$, with parentheses arranged in such a way so as to form legal nestings.","We generalize $\\| \\cdot \\|$ by defining $\\| n \\|_{m}$ as the smallest number of possibly repeated selections from $\\{ 1, 2, \\ldots, m \\}$ (counting repetitions), for fixed $m \\in \\mathbb{N}$, that can be used to express $n$ with the same operational and bracket symbols as before.","There is a close relationship, as we explore, between $\\|\\cdot\\|_{2}$ and lengths of shortest addition chains for a given natural number.","This illustrates how remarkable it is that $(\\| n \\|_{2} :","n \\in \\mathbb{N} )$ is not currently included in the On-Line Encyclopedia of Integer Sequences and has, apparently, not been studied previously.","This, in turn, motivates our exploration of the complexity function $\\| \\cdot\\|_{2}$, in which we prove explicit upper and lower bounds for $\\|\\cdot\\|_{2}$ and describe some problems and further areas of research concerning $\\|\\cdot\\|_{2}$."],"url":"http://arxiv.org/abs/2403.20073v1","category":"math.NT"}
{"created":"2024-03-29 09:04:24","title":"Helicity in dispersive fluid mechanics","abstract":"By dispersive models of fluid mechanics we are referring to the Euler-Lagrange equations for the constrained Hamilton action functional where the internal energy depends on high order derivatives of unknowns. The mass conservation law is considered as a constraint. The corresponding Euler-Lagrange equations include, in particular, the van der Waals--Korteweg model of capillary fluids, the model of fluids containing small gas bubbles and the model describing long free-surface gravity waves. We obtain new conservation laws generalizing the helicity conservation for classical barotropic fluids.","sentences":["By dispersive models of fluid mechanics we are referring to the Euler-Lagrange equations for the constrained Hamilton action functional where the internal energy depends on high order derivatives of unknowns.","The mass conservation law is considered as a constraint.","The corresponding Euler-Lagrange equations include, in particular, the van der Waals--Korteweg model of capillary fluids, the model of fluids containing small gas bubbles and the model describing long free-surface gravity waves.","We obtain new conservation laws generalizing the helicity conservation for classical barotropic fluids."],"url":"http://arxiv.org/abs/2403.20072v1","category":"math.AP"}
{"created":"2024-03-29 08:59:31","title":"Variational properties of space-periodic standing waves of nonlinear Schr{\u00f6}dinger equations with general nonlinearities","abstract":"Periodic waves are standing wave solutions of nonlinear Schr\\''odinger equations whose profile is periodic in space dimension one. We consider general nonlinearities and provide variational characterizations for the periodic wave profiles. This involves minimizing energy while keeping mass and momentum constant, as well as minimizing the action over the Nehari manifold. These variational approaches are considered both in the periodic and anti-periodic settings, and for focusing and defocusing nonlinearities. In appendix, we study the existence properties of periodic solutions of the triple power nonlinearity.","sentences":["Periodic waves are standing wave solutions of nonlinear Schr\\''odinger equations whose profile is periodic in space dimension one.","We consider general nonlinearities and provide variational characterizations for the periodic wave profiles.","This involves minimizing energy while keeping mass and momentum constant, as well as minimizing the action over the Nehari manifold.","These variational approaches are considered both in the periodic and anti-periodic settings, and for focusing and defocusing nonlinearities.","In appendix, we study the existence properties of periodic solutions of the triple power nonlinearity."],"url":"http://arxiv.org/abs/2403.20068v1","category":"math.AP"}
{"created":"2024-03-29 08:55:34","title":"Three-particle formalism for multiple channels: the $\u03b7\u03c0\u03c0+ K \\overline K \u03c0$ system in isosymmetric QCD","abstract":"We generalize previous three-particle finite-volume formalisms to allow for multiple three-particle channels. For definiteness, we focus on the two-channel $\\eta \\pi \\pi$ and $K \\overline K \\pi$ system in isosymmetric QCD, considering the positive $G$ parity sector of the latter channel, and neglecting the coupling to modes with four or more particles. The formalism we obtain is thus appropriate to study the $b_1(1235)$ and $\\eta(1295)$ resonances. The derivation is made in the generic relativistic field theory approach using the time-ordered perturbation theory method. We study how the resulting quantization condition reduces to that for a single three-particle channel when one drops below the upper ($K\\overline K \\pi$) threshold. We also present parametrizations of the three-particle K matrices that enter into the formalism.","sentences":["We generalize previous three-particle finite-volume formalisms to allow for multiple three-particle channels.","For definiteness, we focus on the two-channel $\\eta \\pi \\pi$ and $K \\overline K \\pi$ system in isosymmetric QCD, considering the positive $G$ parity sector of the latter channel, and neglecting the coupling to modes with four or more particles.","The formalism we obtain is thus appropriate to study the $b_1(1235)$ and $\\eta(1295)$ resonances.","The derivation is made in the generic relativistic field theory approach using the time-ordered perturbation theory method.","We study how the resulting quantization condition reduces to that for a single three-particle channel when one drops below the upper ($K\\overline K \\pi$) threshold.","We also present parametrizations of the three-particle K matrices that enter into the formalism."],"url":"http://arxiv.org/abs/2403.20064v1","category":"hep-ph"}
{"created":"2024-03-29 08:54:53","title":"Non-invertible symmetries act locally by quantum operations","abstract":"Non-invertible symmetries of quantum field theories and many-body systems generalize the concept of symmetries by allowing non-invertible operations in addition to more ordinary invertible ones described by groups. The aim of this paper is to point out that these non-invertible symmetries act on local operators by quantum operations, i.e. completely positive maps between density matrices, which form a natural class of operations containing both unitary evolutions and measurements and play an important role in quantum information theory. This observation will be illustrated by the Kramers--Wannier duality of the one-dimensional quantum Ising chain, which is a prototypical example of non-invertible symmetry operations.","sentences":["Non-invertible symmetries of quantum field theories and many-body systems generalize the concept of symmetries by allowing non-invertible operations in addition to more ordinary invertible ones described by groups.","The aim of this paper is to point out that these non-invertible symmetries act on local operators by quantum operations, i.e. completely positive maps between density matrices, which form a natural class of operations containing both unitary evolutions and measurements and play an important role in quantum information theory.","This observation will be illustrated by the Kramers--Wannier duality of the one-dimensional quantum Ising chain, which is a prototypical example of non-invertible symmetry operations."],"url":"http://arxiv.org/abs/2403.20062v1","category":"hep-th"}
{"created":"2024-03-29 08:53:39","title":"Asymptotic expansion of wave scattering in a periodic 2d-plane","abstract":"We give a counter part of Sommerfeld outging radiation condition for waves propagating in a 2d periodic medium under generical assumptions and provide a uniqueness theorem for outgoing solutions.","sentences":["We give a counter part of Sommerfeld outging radiation condition for waves propagating in a 2d periodic medium under generical assumptions and provide a uniqueness theorem for outgoing solutions."],"url":"http://arxiv.org/abs/2403.20061v1","category":"math.AP"}
{"created":"2024-03-29 08:47:49","title":"Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks","abstract":"Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically (\"mixture of experts\") through learnable weights to learn respective representations from different modalities. Such design will not sacrifice model performance in uni-modal situation. To fully exploit the inherent complex and nonlinear relation among modalities while producing fine-grained representations for uni-modal inference, we subsequently add a modal alignment module to line up a dominant modality (e.g., PET) with representations of auxiliary modalities (MR). We further adopt multimodal reconstruction to promote the quality of learned features. Experiments on precious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis showcase the efficacy of our model toward clinically feasible precision medicine.","sentences":["Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique.","It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion).","Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models.","Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy.","To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model.","It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically (\"mixture of experts\") through learnable weights to learn respective representations from different modalities.","Such design will not sacrifice model performance in uni-modal situation.","To fully exploit the inherent complex and nonlinear relation among modalities while producing fine-grained representations for uni-modal inference, we subsequently add a modal alignment module to line up a dominant modality (e.g., PET) with representations of auxiliary modalities (MR).","We further adopt multimodal reconstruction to promote the quality of learned features.","Experiments on precious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis showcase the efficacy of our model toward clinically feasible precision medicine."],"url":"http://arxiv.org/abs/2403.20058v1","category":"eess.IV"}
{"created":"2024-03-29 08:35:57","title":"On Fractional Kinetic Equations Involving Srivastava Polynomial","abstract":"Kinetic equations hold a very important place in physics and further their fractional generalization enhances the scope of their applicability and significance in describing the continuity of motion in materials. After the development of generalized form of fractional kinetic equations, many researchers proffered several new forms of these equations and found their solutions by different techniques. In this work, we have proposed some novel generalised fractional kinetic equations involving the Srivastava polynomial and, by applying the Laplace transform approach, their solutions are calculated. Further, to study the behaviour of these, numerical and graphical interpretation of the solutions are also provided.","sentences":["Kinetic equations hold a very important place in physics and further their fractional generalization enhances the scope of their applicability and significance in describing the continuity of motion in materials.","After the development of generalized form of fractional kinetic equations, many researchers proffered several new forms of these equations and found their solutions by different techniques.","In this work, we have proposed some novel generalised fractional kinetic equations involving the Srivastava polynomial and, by applying the Laplace transform approach, their solutions are calculated.","Further, to study the behaviour of these, numerical and graphical interpretation of the solutions are also provided."],"url":"http://arxiv.org/abs/2403.20048v1","category":"math-ph"}
{"created":"2024-03-29 08:28:13","title":"Existence and Uniqueness of Rayleigh waves with both normal and tangential boundary conditions","abstract":"Impedance boundary condition are of great interest in linear elasticity as a method to model several non-standard problems. Recently, in the frame of surface wave propagation in an elastic isotropic half-space, Godoy et al. [Wave Motion 49 (2012), 585-594] proposed a general class of impedance boundary conditions that generalize the standard stress-free boundary condition in the non-dispersive regime. A natural question that arises in this context is whether the property of existence and uniqueness of a surface wave, observed in the stress-free case (called Rayleigh wave), holds for full Godoy's impedance boundary conditions. Godoy et al. addressed this question for the tangential case (the tangential stress is proportional to the horizontal displacement times the frequency and the normal stress vanishes). Recently, Giang and Vinh [J Eng Math 130, 13 (2021)] studied the normal case (the tangential stress vanishes and the normal stress is proportional to the normal displacement times the frequency). In this work, we consider an uniparametric family of Godoy's impedance boundary conditions defined by proportional ratios of the same magnitude but opposite sign. We demonstrate the existence and uniqueness of the Rayleigh surface wave for each value of the impedance parameter, showing for the first time that full Godoy's impedance boundary conditions are also capable of explaining surface wave propagation. Numerical examples are presented to illustrate the effect of the impedance parameter in the speed of the surface wave.","sentences":["Impedance boundary condition are of great interest in linear elasticity as a method to model several non-standard problems.","Recently, in the frame of surface wave propagation in an elastic isotropic half-space, Godoy et al.","[Wave Motion 49 (2012), 585-594] proposed a general class of impedance boundary conditions that generalize the standard stress-free boundary condition in the non-dispersive regime.","A natural question that arises in this context is whether the property of existence and uniqueness of a surface wave, observed in the stress-free case (called Rayleigh wave), holds for full Godoy's impedance boundary conditions.","Godoy et al. addressed this question for the tangential case (the tangential stress is proportional to the horizontal displacement times the frequency and the normal stress vanishes).","Recently, Giang and Vinh [J Eng Math 130, 13 (2021)] studied the normal case (the tangential stress vanishes and the normal stress is proportional to the normal displacement times the frequency).","In this work, we consider an uniparametric family of Godoy's impedance boundary conditions defined by proportional ratios of the same magnitude but opposite sign.","We demonstrate the existence and uniqueness of the Rayleigh surface wave for each value of the impedance parameter, showing for the first time that full Godoy's impedance boundary conditions are also capable of explaining surface wave propagation.","Numerical examples are presented to illustrate the effect of the impedance parameter in the speed of the surface wave."],"url":"http://arxiv.org/abs/2403.20042v1","category":"physics.class-ph"}
{"created":"2024-03-29 08:26:53","title":"Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs","abstract":"The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed.","sentences":["The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones.","However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience.","To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference.","Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors.","We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively.","Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed."],"url":"http://arxiv.org/abs/2403.20041v1","category":"cs.CL"}
{"created":"2024-03-29 08:23:44","title":"Variability in Aggregate Personal Income Across Industrial Sectors During COVID-19 Shock: A Time-Series Exploration","abstract":"This study explored the variability in Aggregate Personal Income (PI) across 13 major industrial sectors in the US during the COVID-19 pandemic. Utilizing time-series data from 2010 Q1 to 2019 Q4, we employed Autoregressive Integrated Moving Average (ARIMA) models to establish baseline trends in Personal Income (PI) before the pandemic. We then extended these models to forecast PI values for the subsequent 14 quarters, from 2020 Q1 to 2023 Q2, as if the pandemic had never happened. This forecasted data was compared with the actual PI data collected during the pandemic to quantify its impacts. This approach allowed for the assessment of both immediate and extended effects of COVID-19 on sector-specific PI. Our study highlighted the resilience of PI in sectors like Utilities, Retail, Finance, Real Estate, and Healthcare, with Farming showing an early recovery in PI, despite significant initial setbacks. In contrast, PI in Accommodation and Food Services experienced delayed recovery, contributing significantly to the overall impact variance alongside Farming (53.26\\% and 33.26\\% respectively). Finance and Utilities demonstrated positive deviations, suggesting a lesser impact or potential benefit in early pandemic stages. Meanwhile, sectoral PI in Manufacturing, Wholesale and Education showed moderate recovery, whereas Construction and Government lagged in resilience. The aggregate economic impact, initially negative at -0.027 in 2020 Q1, drastically worsened to -1.42 in Q2, but improved by Q4, reflecting a broader trend of adaptation and resilience across all the sectors during the pandemic.","sentences":["This study explored the variability in Aggregate Personal Income (PI) across 13 major industrial sectors in the US during the COVID-19 pandemic.","Utilizing time-series data from 2010 Q1 to 2019 Q4, we employed Autoregressive Integrated Moving Average (ARIMA) models to establish baseline trends in Personal Income (PI) before the pandemic.","We then extended these models to forecast PI values for the subsequent 14 quarters, from 2020 Q1 to 2023 Q2, as if the pandemic had never happened.","This forecasted data was compared with the actual PI data collected during the pandemic to quantify its impacts.","This approach allowed for the assessment of both immediate and extended effects of COVID-19 on sector-specific PI.","Our study highlighted the resilience of PI in sectors like Utilities, Retail, Finance, Real Estate, and Healthcare, with Farming showing an early recovery in PI, despite significant initial setbacks.","In contrast, PI in Accommodation and Food Services experienced delayed recovery, contributing significantly to the overall impact variance alongside Farming (53.26\\% and 33.26\\% respectively).","Finance and Utilities demonstrated positive deviations, suggesting a lesser impact or potential benefit in early pandemic stages.","Meanwhile, sectoral PI in Manufacturing, Wholesale and Education showed moderate recovery, whereas Construction and Government lagged in resilience.","The aggregate economic impact, initially negative at -0.027 in 2020 Q1, drastically worsened to -1.42 in Q2, but improved by Q4, reflecting a broader trend of adaptation and resilience across all the sectors during the pandemic."],"url":"http://arxiv.org/abs/2403.20039v1","category":"econ.GN"}
{"created":"2024-03-29 08:23:08","title":"Probing solar modulation analytic models with cosmic ray periodic spectra","abstract":"The AMS02 experiment has published the periodic spectra of proton, helium and helium isotopes across the majority of the 24 solar cycle. These precise data exhibit temporal structures that correlate with solar modulation. In this study, we utilize these data to probe three analytic solar modulation models, including the force-field approximation, the convection-diffusion model and the extended force-field approximation with a drift effect. We adopt a method that eliminates the influence of interstellar cosmic ray spectra, and use the Earth-observed spectra at time $t_1$ to predict those at time $t_2$. In order to explore the rigidity-dependence of solar modulation models, we substitute the conventional potential parameter $\\phi$ with a modified parameter $\\phi'=\\frac{R}{ k_2(R)}\\phi$ for our analysis. Combining with the $\\chi^2$ minimization method, the best-fit modulation parameter $\\phi'$ can be evaluated. First, we test the validity of a rigidity-independent $\\phi'$ and find that both the force-field approximation (FFA) and the extended force-field approximation (EFFA) agree well with data near the solar minimum period. However, all models significantly deviate from the data during the solar maximum. Consequently, we assume a constant $\\phi'(t_1)$ at solar minimum and calculate $\\Delta\\phi'=\\phi'(t_2)-\\phi'(t_1)$ for each rigidity bin at time $t_2$. It is found that $\\Delta\\phi'$ generally adheres to a linear-logarithm relationship with rigidity at any given time. By adopting a linear-logarithm formula of $\\Delta\\phi'$, we further discover that both the modified FFA and EFFA can reconcile the observations during solar maxima. This suggests that at solar maximum, the parameter $\\phi'$, which correlates with the diffusion pattern in the heliospheric magnetic fields, exhibits a rigidity dependence.","sentences":["The AMS02 experiment has published the periodic spectra of proton, helium and helium isotopes across the majority of the 24 solar cycle.","These precise data exhibit temporal structures that correlate with solar modulation.","In this study, we utilize these data to probe three analytic solar modulation models, including the force-field approximation, the convection-diffusion model and the extended force-field approximation with a drift effect.","We adopt a method that eliminates the influence of interstellar cosmic ray spectra, and use the Earth-observed spectra at time $t_1$ to predict those at time $t_2$. In order to explore the rigidity-dependence of solar modulation models, we substitute the conventional potential parameter $\\phi$ with a modified parameter $\\phi'=\\frac{R}{ k_2(R)}\\phi$ for our analysis.","Combining with the $\\chi^2$ minimization method, the best-fit modulation parameter $\\phi'$ can be evaluated.","First, we test the validity of a rigidity-independent $\\phi'$ and find that both the force-field approximation (FFA) and the extended force-field approximation (EFFA) agree well with data near the solar minimum period.","However, all models significantly deviate from the data during the solar maximum.","Consequently, we assume a constant $\\phi'(t_1)$ at solar minimum and calculate $\\Delta\\phi'=\\phi'(t_2)-\\phi'(t_1)$ for each rigidity bin at time $t_2$. It is found that $\\Delta\\phi'$ generally adheres to a linear-logarithm relationship with rigidity at any given time.","By adopting a linear-logarithm formula of $\\Delta\\phi'$, we further discover that both the modified FFA and EFFA can reconcile the observations during solar maxima.","This suggests that at solar maximum, the parameter $\\phi'$, which correlates with the diffusion pattern in the heliospheric magnetic fields, exhibits a rigidity dependence."],"url":"http://arxiv.org/abs/2403.20038v1","category":"astro-ph.SR"}
{"created":"2024-03-29 15:33:58","title":"Search for ZZ and ZH production in the $\\mathrm{b\\bar{b}b\\bar{b}}$ final state using proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for ZZ and ZH production in the $\\mathrm{b\\bar{b}b\\bar{b}}$ final state is presented, where H is the standard model (SM) Higgs boson. The search uses an event sample of proton-proton collisions corresponding to an integrated luminosity of 133 fb$^{-1}$ collected at a center-of-mass energy of 13 TeV with the CMS detector at the CERN LHC. The analysis introduces several novel techniques for deriving and validating a multi-dimensional background model based on control samples in data. A multiclass multivariate classifier customized for the $\\mathrm{b\\bar{b}b\\bar{b}}$ final state is developed to derive the background model and extract the signal. The data are found to be consistent, within uncertainties, with the SM predictions. The observed (expected) upper limits at 95\\% confidence level are found to be 3.8 (3.8) and 5.0 (2.9) times the SM prediction for the ZZ and ZH production cross sections, respectively.","sentences":["A search for ZZ and ZH production in the $\\mathrm{b\\bar{b}b\\bar{b}}$ final state is presented, where H is the standard model (SM) Higgs boson.","The search uses an event sample of proton-proton collisions corresponding to an integrated luminosity of 133 fb$^{-1}$ collected at a center-of-mass energy of 13 TeV with the CMS detector at the CERN LHC.","The analysis introduces several novel techniques for deriving and validating a multi-dimensional background model based on control samples in data.","A multiclass multivariate classifier customized for the $\\mathrm{b\\bar{b}b\\bar{b}}$ final state is developed to derive the background model and extract the signal.","The data are found to be consistent, within uncertainties, with the SM predictions.","The observed (expected) upper limits at 95\\% confidence level are found to be 3.8 (3.8) and 5.0 (2.9) times the SM prediction for the ZZ and ZH production cross sections, respectively."],"url":"http://arxiv.org/abs/2403.20241v1","category":"hep-ex"}
{"created":"2024-03-29 14:26:28","title":"Measurement of the production cross section of a Higgs boson with large transverse momentum in its decays to a pair of $\u03c4$ leptons in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A measurement of the production cross section of a Higgs boson with transverse momentum greater than 250 GeV is presented where the Higgs boson decays to a pair of $\\tau$ leptons. It is based on proton-proton collision data collected by the CMS experiment at the CERN LHC at a center-of-mass energy of 13 TeV. The data sample corresponds to an integrated luminosity of 138 fb$^{-1}$. Because of the large transverse momentum of the Higgs boson the $\\tau$ leptons from its decays are boosted and produced spatially close, with their decay products overlapping. Therefore, a dedicated algorithm was developed to reconstruct and identify them. The observed (expected) significance of the measured signal with respect to the standard model background-only hypothesis is 3.5 (2.2) standard deviations. The product of the production cross section and branching fraction is measured to be 1.64$^{+0.68}_{-0.54}$ times the standard model expectation. The fiducial differential production cross section is also measured as functions of the Higgs boson and leading jet transverse momenta. This measurement extends the probed large-transverse-momentum region beyond 600 GeV.","sentences":["A measurement of the production cross section of a Higgs boson with transverse momentum greater than 250 GeV is presented where the Higgs boson decays to a pair of $\\tau$ leptons.","It is based on proton-proton collision data collected by the CMS experiment at the CERN LHC at a center-of-mass energy of 13 TeV.","The data sample corresponds to an integrated luminosity of 138 fb$^{-1}$. Because of the large transverse momentum of the Higgs boson the $\\tau$ leptons from its decays are boosted and produced spatially close, with their decay products overlapping.","Therefore, a dedicated algorithm was developed to reconstruct and identify them.","The observed (expected) significance of the measured signal with respect to the standard model background-only hypothesis is 3.5 (2.2) standard deviations.","The product of the production cross section and branching fraction is measured to be 1.64$^{+0.68}_{-0.54}$ times the standard model expectation.","The fiducial differential production cross section is also measured as functions of the Higgs boson and leading jet transverse momenta.","This measurement extends the probed large-transverse-momentum region beyond 600 GeV."],"url":"http://arxiv.org/abs/2403.20201v1","category":"hep-ex"}
{"created":"2024-03-29 11:37:44","title":"Reflectionless propagation of beams through a stratified medium","abstract":"Reflectionless potentials following the prescription of Kay and Moses allow for total transmission of incoming waves of any kinetic energy. The optical analogue of such potentials occur as dielectric stratified media that can offer null reflectivity and near total transmission over a large range of incidence angles and wavelengths. In a previous work (S. Dutta Gupta and G. S. Agarwal, Opt. Express 15, 9614-9624, 2007), this was demonstrated for linearly polarized plane waves. We extend the earlier work valid for plane waves to structured beams to show near-total transmission of beams across the reflectionless dielectric profile. The analysis is based on the angular spectrum decomposition treating the beam as a collection of plane waves. Gaussian and Laguerre-Gaussian beams are shown to be transmitted through the film with <1% reflection in most scenarios. We also discuss the superlative performance of our proposed profile in preserving the beam shape during transmission comparing these results to a conventional lambda/2 antireflection coating.","sentences":["Reflectionless potentials following the prescription of Kay and Moses allow for total transmission of incoming waves of any kinetic energy.","The optical analogue of such potentials occur as dielectric stratified media that can offer null reflectivity and near total transmission over a large range of incidence angles and wavelengths.","In a previous work (S. Dutta Gupta and G. S. Agarwal, Opt.","Express 15, 9614-9624, 2007), this was demonstrated for linearly polarized plane waves.","We extend the earlier work valid for plane waves to structured beams to show near-total transmission of beams across the reflectionless dielectric profile.","The analysis is based on the angular spectrum decomposition treating the beam as a collection of plane waves.","Gaussian and Laguerre-Gaussian beams are shown to be transmitted through the film with <1% reflection in most scenarios.","We also discuss the superlative performance of our proposed profile in preserving the beam shape during transmission comparing these results to a conventional lambda/2 antireflection coating."],"url":"http://arxiv.org/abs/2403.20129v1","category":"physics.optics"}
{"created":"2024-03-29 17:53:36","title":"Mean-Field Limits for Stochastic Interacting Particles on Digraph Measures","abstract":"Many natural phenomena are effectively described by interacting particle systems, which can be modeled using either deterministic or stochastic differential equations (SDEs). In this study, we specifically investigate particle systems modeled by SDEs, wherein the mean field limit converges to a Vlasov-Fokker-Planck-type equation. Departing from conventional approaches in stochastic analysis, we explore the network connectivity between particles using diagraph measures (DGMs). DGMs are one possible tool to capture sparse, intermediate and dense network/graph interactions in the mean-field thereby going beyond more classical approaches such as graphons. Since the main goal is to capture large classes of mean-field limits, we set up our approach using measure-theoretic arguments and combine them with suitable moment estimates to ensure approximation results for the mean-field.","sentences":["Many natural phenomena are effectively described by interacting particle systems, which can be modeled using either deterministic or stochastic differential equations (SDEs).","In this study, we specifically investigate particle systems modeled by SDEs, wherein the mean field limit converges to a Vlasov-Fokker-Planck-type equation.","Departing from conventional approaches in stochastic analysis, we explore the network connectivity between particles using diagraph measures (DGMs).","DGMs are one possible tool to capture sparse, intermediate and dense network/graph interactions in the mean-field thereby going beyond more classical approaches such as graphons.","Since the main goal is to capture large classes of mean-field limits, we set up our approach using measure-theoretic arguments and combine them with suitable moment estimates to ensure approximation results for the mean-field."],"url":"http://arxiv.org/abs/2403.20325v1","category":"math.AP"}
{"created":"2024-03-29 17:50:28","title":"Towards a Framework for Evaluating Explanations in Automated Fact Verification","abstract":"As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.","sentences":["As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater.","A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions.","In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically.","We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure).","Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures."],"url":"http://arxiv.org/abs/2403.20322v1","category":"cs.CL"}
{"created":"2024-03-29 17:42:31","title":"Towards a Fault-Injection Benchmarking Suite","abstract":"Soft errors in memories and logic circuits are known to disturb program execution. In this context, the research community has been proposing a plethora of fault-tolerance (FT) solutions over the last decades, as well as fault-injection (FI) approaches to test, measure and compare them. However, there is no agreed-upon benchmarking suite for demonstrating FT or FI approaches. As a replacement, authors pick benchmarks from other domains, e.g. embedded systems. This leads to little comparability across publications, and causes behavioral overlap within benchmarks that were not selected for orthogonality in the FT/FI domain.   In this paper, we want to initiate a discussion on what a benchmarking suite for the FT/FI domain should look like, and propose criteria for benchmark selection.","sentences":["Soft errors in memories and logic circuits are known to disturb program execution.","In this context, the research community has been proposing a plethora of fault-tolerance (FT) solutions over the last decades, as well as fault-injection (FI) approaches to test, measure and compare them.","However, there is no agreed-upon benchmarking suite for demonstrating FT or FI approaches.","As a replacement, authors pick benchmarks from other domains, e.g. embedded systems.","This leads to little comparability across publications, and causes behavioral overlap within benchmarks that were not selected for orthogonality in the FT/FI domain.   ","In this paper, we want to initiate a discussion on what a benchmarking suite for the FT/FI domain should look like, and propose criteria for benchmark selection."],"url":"http://arxiv.org/abs/2403.20319v1","category":"cs.SE"}
{"created":"2024-03-29 17:39:41","title":"Resummation phenomenology and PDF determination for precision QCD at the LHC","abstract":"With the ongoing Run 3 of the LHC and its upcoming High-Luminosity upgrade, there is a growing need to study observables with high precision both experimentally and theoretically. To increase precision on the theory side, improvements of fixed-order perturbative predictions, resummation of logarithmic enhancements and accurate determination of proton structure are required. This thesis explores the latter two topics. We discuss high-energy logarithms and their resummation techniques, introducing an extension of the HELL formalism for multi-differential distributions in transverse momentum, rapidity and invariant mass. We apply this framework to heavy-quark pair production at the LHC, studying the kinematics of both a single quark and the final-state pair. An additional discussion is dedicated to a possible extension of the kt-factorisation framework, which underlies high-energy resummation, to capture next-to-leading logarithmic corrections. To test this hypothesis, we delve into the computation of a NLO off-shell coefficient function using Higgs-induced DIS in the infinite top mass limit as a benchmark process and report a partial result. Beside high-energy logarithms, we consider the determination of transverse-momentum distributions from a high mass system with additional QCD radiation and exclusive production cuts. Specifically, we focus on $HW^+$ production with a jet veto and analyse the Higgs transverse momentum spectrum at NNLO, using qt-subtraction. We complement the fixed order study with NNLL resummation of jet-veto logarithms and linear power correction in ptHW using the RadISH formalism. Finally, in the last project pertaining to this thesis we consider the problem of Parton Distribution Function determination. We propose a minimal parametrisation guided by physical arguments and investigate its performance in fitting the HERA dataset with NLO QCD theory predictions.","sentences":["With the ongoing Run 3 of the LHC and its upcoming High-Luminosity upgrade, there is a growing need to study observables with high precision both experimentally and theoretically.","To increase precision on the theory side, improvements of fixed-order perturbative predictions, resummation of logarithmic enhancements and accurate determination of proton structure are required.","This thesis explores the latter two topics.","We discuss high-energy logarithms and their resummation techniques, introducing an extension of the HELL formalism for multi-differential distributions in transverse momentum, rapidity and invariant mass.","We apply this framework to heavy-quark pair production at the LHC, studying the kinematics of both a single quark and the final-state pair.","An additional discussion is dedicated to a possible extension of the kt-factorisation framework, which underlies high-energy resummation, to capture next-to-leading logarithmic corrections.","To test this hypothesis, we delve into the computation of a NLO off-shell coefficient function using Higgs-induced DIS in the infinite top mass limit as a benchmark process and report a partial result.","Beside high-energy logarithms, we consider the determination of transverse-momentum distributions from a high mass system with additional QCD radiation and exclusive production cuts.","Specifically, we focus on $HW^+$ production with a jet veto and analyse","the Higgs transverse momentum spectrum at NNLO, using qt-subtraction.","We complement the fixed order study with NNLL resummation of jet-veto logarithms and linear power correction in ptHW using the RadISH formalism.","Finally, in the last project pertaining to this thesis we consider the problem of Parton Distribution Function determination.","We propose a minimal parametrisation guided by physical arguments and investigate its performance in fitting the HERA dataset with NLO QCD theory predictions."],"url":"http://arxiv.org/abs/2403.20315v1","category":"hep-ph"}
{"created":"2024-03-29 17:35:32","title":"pastamarkers: astrophysical data visualization with pasta-like markers","abstract":"We aim at facilitating the visualization of astrophysical data for several tasks, such as uncovering patterns, presenting results to the community, and facilitating the understanding of complex physical relationships to the public. We present pastamarkers, a customized Python package fully compatible with matplotlib, that contains unique pasta-shaped markers meant to enhance the visualization of astrophysical data. We prove that using different pasta types as markers can improve the clarity of astrophysical plots by reproducing some of the most famous plots in the literature.","sentences":["We aim at facilitating the visualization of astrophysical data for several tasks, such as uncovering patterns, presenting results to the community, and facilitating the understanding of complex physical relationships to the public.","We present pastamarkers, a customized Python package fully compatible with matplotlib, that contains unique pasta-shaped markers meant to enhance the visualization of astrophysical data.","We prove that using different pasta types as markers can improve the clarity of astrophysical plots by reproducing some of the most famous plots in the literature."],"url":"http://arxiv.org/abs/2403.20314v1","category":"astro-ph.IM"}
{"created":"2024-03-29 17:29:58","title":"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds","abstract":"While novel view synthesis (NVS) has made substantial progress in 3D computer vision, it typically requires an initial estimation of camera intrinsics and extrinsics from dense viewpoints. This pre-processing is usually conducted via a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and unreliable, particularly in sparse-view scenarios with insufficient matched features for accurate reconstruction. In this work, we integrate the strengths of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved issues in NVS under unconstrained settings, which encompasses pose-free and sparse view challenges. Our framework, InstantSplat, unifies dense stereo priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview & pose-free images in less than 1 minute. Specifically, InstantSplat comprises a Coarse Geometric Initialization (CGI) module that swiftly establishes a preliminary scene structure and camera parameters across all training views, utilizing globally-aligned 3D point maps derived from a pre-trained dense stereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO) module, which jointly optimizes the 3D Gaussian attributes and the initialized poses with pose regularization. Experiments conducted on the large-scale outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error (ATE) by 80%. These establish InstantSplat as a viable solution for scenarios involving posefree and sparse-view conditions. Project page: instantsplat.github.io.","sentences":["While novel view synthesis (NVS) has made substantial progress in 3D computer vision, it typically requires an initial estimation of camera intrinsics and extrinsics from dense viewpoints.","This pre-processing is usually conducted via a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and unreliable, particularly in sparse-view scenarios with insufficient matched features for accurate reconstruction.","In this work, we integrate the strengths of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved issues in NVS under unconstrained settings, which encompasses pose-free and sparse view challenges.","Our framework, InstantSplat, unifies dense stereo priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview & pose-free images in less than 1 minute.","Specifically, InstantSplat comprises a Coarse Geometric Initialization (CGI) module that swiftly establishes a preliminary scene structure and camera parameters across all training views, utilizing globally-aligned 3D point maps derived from a pre-trained dense stereo pipeline.","This is followed by the Fast 3D-Gaussian Optimization (F-3DGO) module, which jointly optimizes the 3D Gaussian attributes and the initialized poses with pose regularization.","Experiments conducted on the large-scale outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error (ATE) by 80%.","These establish InstantSplat as a viable solution for scenarios involving posefree and sparse-view conditions.","Project page: instantsplat.github.io."],"url":"http://arxiv.org/abs/2403.20309v1","category":"cs.CV"}
{"created":"2024-03-29 17:16:06","title":"Every locally compact group is the outer automorphism group of a II$_1$ factor","abstract":"We prove that every locally compact second countable group $G$ arises as the outer automorphism group Out $M$ of a II$_1$ factor, which was so far only known for totally disconnected groups, compact groups and a few isolated examples. We obtain this result by proving that every locally compact second countable group is a centralizer group, a class of Polish groups that may all be realized as Out $M$ and for which we prove several basic properties and characterizations.","sentences":["We prove that every locally compact second countable group $G$ arises as the outer automorphism group Out $M$ of a II$_1$ factor, which was so far only known for totally disconnected groups, compact groups and a few isolated examples.","We obtain this result by proving that every locally compact second countable group is a centralizer group, a class of Polish groups that may all be realized as Out $M$ and for which we prove several basic properties and characterizations."],"url":"http://arxiv.org/abs/2403.20299v1","category":"math.GR"}
{"created":"2024-03-29 17:15:21","title":"Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement","abstract":"The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical structures. To address this challenge, we propose hierarchy-aware embedding and domain alignment schemes that adjust the scale to extract domain-shareable information without disrupting structural forms. The process involves the initial embedding of review texts in hyperbolic space, followed by feature extraction incorporating degree-based normalization and structure alignment. We conducted extensive experiments to substantiate the efficiency, robustness, and scalability of our proposed model in comparison to state-of-the-art baselines.","sentences":["The issue of data sparsity poses a significant challenge to recommender systems.","In response to this, algorithms that leverage side information such as review texts have been proposed.","Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention.","Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items.","This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships.","We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical structures.","To address this challenge, we propose hierarchy-aware embedding and domain alignment schemes that adjust the scale to extract domain-shareable information without disrupting structural forms.","The process involves the initial embedding of review texts in hyperbolic space, followed by feature extraction incorporating degree-based normalization and structure alignment.","We conducted extensive experiments to substantiate the efficiency, robustness, and scalability of our proposed model in comparison to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.20298v1","category":"cs.IR"}
{"created":"2024-03-29 17:13:18","title":"Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation","abstract":"Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains. However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem. There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users. Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain.   In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users' collaborative information. In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user collaborative information from the source domain. CUT first learns user similarity relationships from the target domain. Then, source-target information transfer is guided by the user similarity, where we design a user transformation layer to learn target-domain user representations and a contrastive loss to supervise the user collaborative information transferred. The results show significant performance improvement of CUT compared with SOTA single and cross-domain methods. Further analysis of the target-domain results illustrates that CUT can effectively alleviate the negative transfer problem.","sentences":["Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains.","However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem.","There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users.","Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain.   ","In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users' collaborative information.","In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user collaborative information from the source domain.","CUT first learns user similarity relationships from the target domain.","Then, source-target information transfer is guided by the user similarity, where we design a user transformation layer to learn target-domain user representations and a contrastive loss to supervise the user collaborative information transferred.","The results show significant performance improvement of CUT compared with SOTA single and cross-domain methods.","Further analysis of the target-domain results illustrates that CUT can effectively alleviate the negative transfer problem."],"url":"http://arxiv.org/abs/2403.20296v1","category":"cs.IR"}
{"created":"2024-03-29 17:09:41","title":"Invertibility of Discrete-Time Linear Systems with Sparse Inputs","abstract":"One of the fundamental problems of interest for discrete-time linear systems is whether its input sequence may be recovered given its output sequence, a.k.a. the left inversion problem. Many conditions on the state space geometry, dynamics, and spectral structure of a system have been used to characterize the well-posedness of this problem, without assumptions on the inputs. However, certain structural assumptions, such as input sparsity, have been shown to translate to practical gains in the performance of inversion algorithms, surpassing classical guarantees. Establishing necessary and sufficient conditions for left invertibility of systems with sparse inputs is therefore a crucial step toward understanding the performance limits of system inversion under structured input assumptions. In this work, we provide the first necessary and sufficient characterizations of left invertibility for linear systems with sparse inputs, echoing classic characterizations for standard linear systems. The key insight in deriving these results is in establishing the existence of two novel geometric invariants unique to the sparse-input setting, the weakly unobservable and strongly reachable subspace arrangements. By means of a concrete example, we demonstrate the utility of these characterizations. We conclude by discussing extensions and applications of this framework to several related problems in sparse control.","sentences":["One of the fundamental problems of interest for discrete-time linear systems is whether its input sequence may be recovered given its output sequence, a.k.a.","the left inversion problem.","Many conditions on the state space geometry, dynamics, and spectral structure of a system have been used to characterize the well-posedness of this problem, without assumptions on the inputs.","However, certain structural assumptions, such as input sparsity, have been shown to translate to practical gains in the performance of inversion algorithms, surpassing classical guarantees.","Establishing necessary and sufficient conditions for left invertibility of systems with sparse inputs is therefore a crucial step toward understanding the performance limits of system inversion under structured input assumptions.","In this work, we provide the first necessary and sufficient characterizations of left invertibility for linear systems with sparse inputs, echoing classic characterizations for standard linear systems.","The key insight in deriving these results is in establishing the existence of two novel geometric invariants unique to the sparse-input setting, the weakly unobservable and strongly reachable subspace arrangements.","By means of a concrete example, we demonstrate the utility of these characterizations.","We conclude by discussing extensions and applications of this framework to several related problems in sparse control."],"url":"http://arxiv.org/abs/2403.20294v1","category":"math.OC"}
{"created":"2024-03-29 17:01:38","title":"Monotone inclusion methods for a class of second-order non-potential mean-field games","abstract":"We propose a monotone splitting algorithm for solving a class of second-order non-potential mean-field games. Following [Achdou, Capuzzo-Dolcetta, \"Mean Field Games: Numerical Methods,\" SINUM (2010)], we introduce a finite-difference scheme and observe that the scheme represents first-order optimality conditions for a primal-dual pair of monotone inclusions. Based on this observation, we prove that the finite-difference system obtains a solution that can be provably recovered by an extension of the celebrated primal-dual hybrid gradient (PDHG) algorithm.","sentences":["We propose a monotone splitting algorithm for solving a class of second-order non-potential mean-field games.","Following [Achdou, Capuzzo-Dolcetta, \"Mean Field Games: Numerical Methods,\" SINUM (2010)], we introduce a finite-difference scheme and observe that the scheme represents first-order optimality conditions for a primal-dual pair of monotone inclusions.","Based on this observation, we prove that the finite-difference system obtains a solution that can be provably recovered by an extension of the celebrated primal-dual hybrid gradient (PDHG) algorithm."],"url":"http://arxiv.org/abs/2403.20290v1","category":"math.OC"}
{"created":"2024-03-29 16:51:58","title":"A New Information Complexity Measure for Multi-pass Streaming with Applications","abstract":"We introduce a new notion of information complexity for multi-pass streaming problems and use it to resolve several important questions in data streams.   In the coin problem, one sees a stream of $n$ i.i.d. uniform bits and one would like to compute the majority with constant advantage. We show that any constant pass algorithm must use $\\Omega(\\log n)$ bits of memory, significantly extending an earlier $\\Omega(\\log n)$ bit lower bound for single-pass algorithms of Braverman-Garg-Woodruff (FOCS, 2020). This also gives the first $\\Omega(\\log n)$ bit lower bound for the problem of approximating a counter up to a constant factor in worst-case turnstile streams for more than one pass.   In the needle problem, one either sees a stream of $n$ i.i.d. uniform samples from a domain $[t]$, or there is a randomly chosen needle $\\alpha \\in[t]$ for which each item independently is chosen to equal $\\alpha$ with probability $p$, and is otherwise uniformly random in $[t]$. The problem of distinguishing these two cases is central to understanding the space complexity of the frequency moment estimation problem in random order streams. We show tight multi-pass space bounds for this problem for every $p < 1/\\sqrt{n \\log^3 n}$, resolving an open question of Lovett and Zhang (FOCS, 2023); even for $1$-pass our bounds are new. To show optimality, we improve both lower and upper bounds from existing results.   Our information complexity framework significantly extends the toolkit for proving multi-pass streaming lower bounds, and we give a wide number of additional streaming applications of our lower bound techniques, including multi-pass lower bounds for $\\ell_p$-norm estimation, $\\ell_p$-point query and heavy hitters, and compressed sensing problems.","sentences":["We introduce a new notion of information complexity for multi-pass streaming problems and use it to resolve several important questions in data streams.   ","In the coin problem, one sees a stream of $n$ i.i.d. uniform bits and one would like to compute the majority with constant advantage.","We show that any constant pass algorithm must use $\\Omega(\\log n)$ bits of memory, significantly extending an earlier $\\Omega(\\log n)$ bit lower bound for single-pass algorithms of Braverman-Garg-Woodruff (FOCS, 2020).","This also gives the first $\\Omega(\\log n)$ bit lower bound for the problem of approximating a counter up to a constant factor in worst-case turnstile streams for more than one pass.   ","In the needle problem, one either sees a stream of $n$ i.i.d. uniform samples from a domain $[t]$, or there is a randomly chosen needle $\\alpha \\in[t]$ for which each item independently is chosen to equal $\\alpha$ with probability $p$, and is otherwise uniformly random in $[t]$. The problem of distinguishing these two cases is central to understanding the space complexity of the frequency moment estimation problem in random order streams.","We show tight multi-pass space bounds for this problem for every $p < 1/\\sqrt{n \\log^3 n}$, resolving an open question of Lovett and Zhang (FOCS, 2023); even for $1$-pass our bounds are new.","To show optimality, we improve both lower and upper bounds from existing results.   ","Our information complexity framework significantly extends the toolkit for proving multi-pass streaming lower bounds, and we give a wide number of additional streaming applications of our lower bound techniques, including multi-pass lower bounds for $\\ell_p$-norm estimation, $\\ell_p$-point query and heavy hitters, and compressed sensing problems."],"url":"http://arxiv.org/abs/2403.20283v1","category":"cs.CC"}
{"created":"2024-03-29 16:49:54","title":"Vulcan: Retreading a Tired Hypothesis with the 2024 Total Solar Eclipse","abstract":"The number of planets in the solar system over the last three centuries has, perhaps surprisingly, been less of a fixed value than one would think it should be. In this paper, we look at the specific case of Vulcan, which was both a planet before Pluto was a planet and discarded from being a planet before Pluto was downgraded. We examine the historical context that led to its discovery in the 19th century, the decades of observations that were taken of it, and its eventual fall from glory. By applying a more modern understanding of astrophysics, we provide multiple mechanisms that may have changed the orbit of Vulcan sufficiently that it would have been outside the footprint of early 20th century searches for it. Finally, we discuss how the April 8, 2024 eclipse provides a renewed opportunity to rediscover this lost planet after more than a century of having been overlooked.","sentences":["The number of planets in the solar system over the last three centuries has, perhaps surprisingly, been less of a fixed value than one would think it should be.","In this paper, we look at the specific case of Vulcan, which was both a planet before Pluto was a planet and discarded from being a planet before Pluto was downgraded.","We examine the historical context that led to its discovery in the 19th century, the decades of observations that were taken of it, and its eventual fall from glory.","By applying a more modern understanding of astrophysics, we provide multiple mechanisms that may have changed the orbit of Vulcan sufficiently that it would have been outside the footprint of early 20th century searches for it.","Finally, we discuss how the April 8, 2024 eclipse provides a renewed opportunity to rediscover this lost planet after more than a century of having been overlooked."],"url":"http://arxiv.org/abs/2403.20281v1","category":"astro-ph.EP"}
{"created":"2024-03-29 16:21:52","title":"Available potential vorticity and the wave-vortex decomposition for arbitrary stratification","abstract":"We consider a rotating non-hydrostatic flow with arbitrary stratification and argue that 1) the appropriate form of potential vorticity (PV) for this system is in terms of isopycnal deviation and 2) the decomposition into energetically orthogonal solutions is fundamentally a PV-inversion.   The new closed-form expression for available potential vorticity (APV) is expressed in terms of isopycnal deviation, following the ideas in Wagner & Young (2015). This form of APV linearizes to quasigeostrophic PV (QGPV) after discarding the nonlinear stretching term and a height nonlinearity, the latter of which is not present in constant stratification. This formulation leads to positive definite definitions of potential enstrophy and total energy expressed in terms of isopycnal deviation, from which the quadratic versions emerge at lowest order. It is exactly these quantities diagonalized by the linear eigenmodes.   Internal-gravity waves, geostrophic motions, inertial oscillations, and a mean density anomaly form the energetically and enstrophically orthogonal constituents of flow. The complete state of the fluid can be represented in terms of these physically realizeable modes and determined from the derived projection operators using the horizontal velocity and density anomaly. The projection of the fluid state onto the non-hydrostatic wave modes, reveals that one must first account for the PV portion of the flow before recovering the wave solutions.   We apply the physical insights of the decomposition to a mesoscale eddy showing how strict adherence to adiabatic rearrangement places strong constraints on the vertical structure of such eddies, including a skew towards stronger cyclonic eddies in the upper water-column. Finally, the expression for APV is shown to reproduce the height nonlinearity of shallow-water PV, a well know feature that breaks the cyclone-anticyclone symmetry in QGPV.","sentences":["We consider a rotating non-hydrostatic flow with arbitrary stratification and argue that 1) the appropriate form of potential vorticity (PV) for this system is in terms of isopycnal deviation and 2) the decomposition into energetically orthogonal solutions is fundamentally a PV-inversion.   ","The new closed-form expression for available potential vorticity (APV) is expressed in terms of isopycnal deviation, following the ideas in Wagner & Young (2015).","This form of APV linearizes to quasigeostrophic PV (QGPV) after discarding the nonlinear stretching term and a height nonlinearity, the latter of which is not present in constant stratification.","This formulation leads to positive definite definitions of potential enstrophy and total energy expressed in terms of isopycnal deviation, from which the quadratic versions emerge at lowest order.","It is exactly these quantities diagonalized by the linear eigenmodes.   ","Internal-gravity waves, geostrophic motions, inertial oscillations, and a mean density anomaly form the energetically and enstrophically orthogonal constituents of flow.","The complete state of the fluid can be represented in terms of these physically realizeable modes and determined from the derived projection operators using the horizontal velocity and density anomaly.","The projection of the fluid state onto the non-hydrostatic wave modes, reveals that one must first account for the PV portion of the flow before recovering the wave solutions.   ","We apply the physical insights of the decomposition to a mesoscale eddy showing how strict adherence to adiabatic rearrangement places strong constraints on the vertical structure of such eddies, including a skew towards stronger cyclonic eddies in the upper water-column.","Finally, the expression for APV is shown to reproduce the height nonlinearity of shallow-water PV, a well know feature that breaks the cyclone-anticyclone symmetry in QGPV."],"url":"http://arxiv.org/abs/2403.20269v1","category":"physics.ao-ph"}
{"created":"2024-03-29 16:07:48","title":"Scaling of variability measures in hierarchical demographic data","abstract":"Demographic heterogeneity is often studied through the geographical lens. Therefore it is considered at a predetermined spatial resolution, which is a suitable choice to understand scalefull phenomena. Spatial autocorrelation indices are well established for this purpose. Yet complex systems are often scale-free, and thus studying the scaling behavior of demographic heterogeneity may provide valuable insights. Furthermore, migration processes are not necessarily influenced by the physical landscape, which is accounted for by the spatial autocorrelation indices. The migration process may be more influenced by the socio-economic landscape, which is better reflected by the hierarchical demographic data. Here we explore the scaling behavior of variability measures in the United Kingdom 2011 census data set. As expected, all of the considered variability measures decrease as the hierarchical scale becomes coarser. Though the non-monotonicity is observed, it can be explained by accounting for the imperfect hierarchical relationships. We show that the scaling behavior of variability measures can be qualitatively understood in terms of Schelling's segregation model and Kawasaki-Ising","sentences":["Demographic heterogeneity is often studied through the geographical lens.","Therefore it is considered at a predetermined spatial resolution, which is a suitable choice to understand scalefull phenomena.","Spatial autocorrelation indices are well established for this purpose.","Yet complex systems are often scale-free, and thus studying the scaling behavior of demographic heterogeneity may provide valuable insights.","Furthermore, migration processes are not necessarily influenced by the physical landscape, which is accounted for by the spatial autocorrelation indices.","The migration process may be more influenced by the socio-economic landscape, which is better reflected by the hierarchical demographic data.","Here we explore the scaling behavior of variability measures in the United Kingdom 2011 census data set.","As expected, all of the considered variability measures decrease as the hierarchical scale becomes coarser.","Though the non-monotonicity is observed, it can be explained by accounting for the imperfect hierarchical relationships.","We show that the scaling behavior of variability measures can be qualitatively understood in terms of Schelling's segregation model and Kawasaki-Ising"],"url":"http://arxiv.org/abs/2403.20259v1","category":"physics.soc-ph"}
{"created":"2024-03-29 16:07:15","title":"Risk-Aware Fixed-Time Stabilization of Stochastic Systems under Measurement Uncertainty","abstract":"This paper addresses the problem of risk-aware fixed-time stabilization of a class of uncertain, output-feedback nonlinear systems modeled via stochastic differential equations. First, novel classes of certificate functions, namely risk-aware fixed-time- and risk-aware path-integral-control Lyapunov functions, are introduced. Then, it is shown how the use of either for control design certifies that a system is both stable in probability and probabilistically fixed-time convergent (for a given probability) to a goal set. That is, the system trajectories probabilistically reach the set within a finite time, independent of the initial condition, despite the additional presence of measurement noise. These methods represent an improvement over the state-of-the-art in stochastic fixed-time stabilization, which presently offers bounds on the settling-time function in expectation only. The theoretical results are verified by an empirical study on an illustrative, stochastic, nonlinear system and the proposed controllers are evaluated against an existing method. Finally, the methods are demonstrated via a simulated fixed-wing aerial robot on a reach-avoid scenario to highlight their ability to certify the probability that a system safely reaches its goal.","sentences":["This paper addresses the problem of risk-aware fixed-time stabilization of a class of uncertain, output-feedback nonlinear systems modeled via stochastic differential equations.","First, novel classes of certificate functions, namely risk-aware fixed-time- and risk-aware path-integral-control Lyapunov functions, are introduced.","Then, it is shown how the use of either for control design certifies that a system is both stable in probability and probabilistically fixed-time convergent (for a given probability) to a goal set.","That is, the system trajectories probabilistically reach the set within a finite time, independent of the initial condition, despite the additional presence of measurement noise.","These methods represent an improvement over the state-of-the-art in stochastic fixed-time stabilization, which presently offers bounds on the settling-time function in expectation only.","The theoretical results are verified by an empirical study on an illustrative, stochastic, nonlinear system and the proposed controllers are evaluated against an existing method.","Finally, the methods are demonstrated via a simulated fixed-wing aerial robot on a reach-avoid scenario to highlight their ability to certify the probability that a system safely reaches its goal."],"url":"http://arxiv.org/abs/2403.20258v1","category":"math.OC"}
{"created":"2024-03-29 15:47:32","title":"Theory of Electron Spin Resonance in Scanning Tunneling Microscopy","abstract":"Electron spin resonance (ESR) spectroscopy in scanning tunneling microscopy (STM) has enabled probing the electronic structure of single magnetic atoms and molecules on surfaces with unprecedented energy resolution, as well as demonstrating coherent manipulation of single spins. Despite this remarkable success, the field could still be greatly advanced by a more quantitative understanding of the ESR-STM physical mechanisms. Here, we present a theory of ESR-STM which quantitatively models not only the ESR signal itself, but also the full background tunneling current, from which the ESR signal is derived. Our theory is based on a combination of Green's function techniques to describe the electron tunneling and a quantum master equation for the dynamics of the spin system along with microwave radiation interacting with both the tunneling current and the spin system. We show that this theory is able to quantitatively reproduce the experimental results for a spin-1/2 system (TiH molecules on MgO) across many orders of magnitude in tunneling current, providing access to the relaxation and decoherence rates that govern the spin dynamics due to intrinsic mechanisms and to the applied bias voltage. More importantly, our work establishes that: (i) sizable ESR signals, which are a measure of microwave-induced changes in the junction magnetoresistance, require surprisingly high tip spin polarizations, (ii) the coupling of the magnetization dynamics to the microwave field gives rise to the asymmetric ESR spectra often observed in this spectroscopy. Additionally, our theory provides very specific predictions for the dependence of the relaxation and decoherence times on the bias voltage and the tip-sample distance. Finally, with the help of electromagnetic simulations, we find that the transitions in our ESR-STM experiments can be driven by the ac magnetic field at the junction.","sentences":["Electron spin resonance (ESR) spectroscopy in scanning tunneling microscopy (STM) has enabled probing the electronic structure of single magnetic atoms and molecules on surfaces with unprecedented energy resolution, as well as demonstrating coherent manipulation of single spins.","Despite this remarkable success, the field could still be greatly advanced by a more quantitative understanding of the ESR-STM physical mechanisms.","Here, we present a theory of ESR-STM which quantitatively models not only the ESR signal itself, but also the full background tunneling current, from which the ESR signal is derived.","Our theory is based on a combination of Green's function techniques to describe the electron tunneling and a quantum master equation for the dynamics of the spin system along with microwave radiation interacting with both the tunneling current and the spin system.","We show that this theory is able to quantitatively reproduce the experimental results for a spin-1/2 system (TiH molecules on MgO) across many orders of magnitude in tunneling current, providing access to the relaxation and decoherence rates that govern the spin dynamics due to intrinsic mechanisms and to the applied bias voltage.","More importantly, our work establishes that: (i) sizable ESR signals, which are a measure of microwave-induced changes in the junction magnetoresistance, require surprisingly high tip spin polarizations, (ii) the coupling of the magnetization dynamics to the microwave field gives rise to the asymmetric ESR spectra often observed in this spectroscopy.","Additionally, our theory provides very specific predictions for the dependence of the relaxation and decoherence times on the bias voltage and the tip-sample distance.","Finally, with the help of electromagnetic simulations, we find that the transitions in our ESR-STM experiments can be driven by the ac magnetic field at the junction."],"url":"http://arxiv.org/abs/2403.20247v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-29 15:45:25","title":"Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids","abstract":"Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.","sentences":["Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability.","When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot.","A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction.","This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots.","This method connects the low-dimension space to the original high-dimensional space.","We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots."],"url":"http://arxiv.org/abs/2403.20246v1","category":"cs.LG"}
{"created":"2024-03-29 15:33:16","title":"A simple EEG-based decision tool for neonatal therapeutic hypothermia in hypoxic-ischemic encephalopathy","abstract":"Indication of therapeutic hypothermia needs an accurate identification of brain injury in the early neonatal period. Here, we aim to provide a simple hypothermia decision-making tool for the term neonates with hypoxic-ischemic encephalopathy (HIE) based on features of conventional electroencephalogram (EEG) taken less than 6 hours from birth. EEG recordings from one hundred full-term babies with HIE were included in the study. Each EEG recording was graded by pediatric neurologists for HIE severity. Amplitude of each EEG segment was analyzed in the slow frequency bands. Temporal fluctuations of spectral power in delta (0.5 - 4 Hz) frequency band was used to characterize each HIE grade. For each grade of abnormality, we estimated level and duration (number of consecutive segments above a given level) probability densities for power of delta oscillations. These 2D representation of EEG dynamics can identify mild HIE group from those of requiring hypothermia. Our discrimination system yielded an accuracy, recall, positive predictive value (precision), negative predictive value, false alarm ratio and F1-score of 98%, 99%, 99%, 0.94%, 0.06 and 99%, respectively. These results provided an accurate discrimination of mild versus moderate or severe HIE, and only one mild case was erroneously detected as relevant for hypothermia. Quantized probability densities of slow spectral features (delta power) from early conventional EEG (withing 6 hours of birth) revealed significant differences in slow spectral dynamics between infants with mild HIE grades and those relevant for hypothermia.","sentences":["Indication of therapeutic hypothermia needs an accurate identification of brain injury in the early neonatal period.","Here, we aim to provide a simple hypothermia decision-making tool for the term neonates with hypoxic-ischemic encephalopathy (HIE) based on features of conventional electroencephalogram (EEG) taken less than 6 hours from birth.","EEG recordings from one hundred full-term babies with HIE were included in the study.","Each EEG recording was graded by pediatric neurologists for HIE severity.","Amplitude of each EEG segment was analyzed in the slow frequency bands.","Temporal fluctuations of spectral power in delta (0.5 - 4 Hz) frequency band was used to characterize each HIE grade.","For each grade of abnormality, we estimated level and duration (number of consecutive segments above a given level) probability densities for power of delta oscillations.","These 2D representation of EEG dynamics can identify mild HIE group from those of requiring hypothermia.","Our discrimination system yielded an accuracy, recall, positive predictive value (precision), negative predictive value, false alarm ratio and F1-score of 98%, 99%, 99%, 0.94%, 0.06 and 99%, respectively.","These results provided an accurate discrimination of mild versus moderate or severe HIE, and only one mild case was erroneously detected as relevant for hypothermia.","Quantized probability densities of slow spectral features (delta power) from early conventional EEG (withing 6 hours of birth) revealed significant differences in slow spectral dynamics between infants with mild HIE grades and those relevant for hypothermia."],"url":"http://arxiv.org/abs/2403.20239v1","category":"q-bio.NC"}
{"created":"2024-03-29 15:20:33","title":"An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT","abstract":"Vision Transformers (ViTs) have achieved significant success in computer vision. However, their intensive computations and massive memory footprint challenge ViTs' deployment on embedded devices, calling for efficient ViTs. Among them, EfficientViT, the state-of-the-art one, features a Convolution-Transformer hybrid architecture, enhancing both accuracy and hardware efficiency. Unfortunately, existing accelerators cannot fully exploit the hardware benefits of EfficientViT due to its unique architecture. In this paper, we propose an FPGA-based accelerator for EfficientViT to advance the hardware efficiency frontier of ViTs. Specifically, we design a reconfigurable architecture to efficiently support various operation types, including lightweight convolutions and attention, boosting hardware utilization. Additionally, we present a time-multiplexed and pipelined dataflow to facilitate both intra- and inter-layer fusions, reducing off-chip data access costs. Experimental results show that our accelerator achieves up to 780.2 GOPS in throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the Xilinx ZCU102 FPGA, which significantly outperforms prior works.","sentences":["Vision Transformers (ViTs) have achieved significant success in computer vision.","However, their intensive computations and massive memory footprint challenge ViTs' deployment on embedded devices, calling for efficient ViTs.","Among them, EfficientViT, the state-of-the-art one, features a Convolution-Transformer hybrid architecture, enhancing both accuracy and hardware efficiency.","Unfortunately, existing accelerators cannot fully exploit the hardware benefits of EfficientViT due to its unique architecture.","In this paper, we propose an FPGA-based accelerator for EfficientViT to advance the hardware efficiency frontier of ViTs.","Specifically, we design a reconfigurable architecture to efficiently support various operation types, including lightweight convolutions and attention, boosting hardware utilization.","Additionally, we present a time-multiplexed and pipelined dataflow to facilitate both intra- and inter-layer fusions, reducing off-chip data access costs.","Experimental results show that our accelerator achieves up to 780.2 GOPS in throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the Xilinx ZCU102 FPGA, which significantly outperforms prior works."],"url":"http://arxiv.org/abs/2403.20230v1","category":"cs.AR"}
{"created":"2024-03-29 14:56:34","title":"Study on a Quantization Condition and the Solvability of Schr\u00f6dinger-type Equations","abstract":"In this thesis, we study a quantization condition in relation to the solvability of Schr\\\"{o}dinger equations. This quantization condition is called the SWKB (supersymmetric Wentzel-Kramers-Brillouin) quantization condition and has been known in the context of supersymmetric quantum mechanics for decades. The main contents of this thesis are recapitulated as follows: the foundation and the application of the SWKB quantization condition. The first half of this thesis aims to understand the fundamental implications of this condition based on extensive case studies. It turns out that the exactness of the SWKB quantization condition indicates the exact solvability of a system via the classical orthogonal polynomials. The SWKB quantization condition provides quantizations of energy, which we call the direct problem of the SWKB. We formulate the inverse problem of the SWKB: the problem of determining the superpotential from a given energy spectrum. The formulation successfully reconstructs all conventional shape-invariant potentials from the given energy spectra. We further construct novel solvable potentials, which are classical-orthogonal-polynomially quasi-exactly solvable, by this formulation. We further demonstrate several explicit solutions of the Schr\\\"{o}dinger equations with the classical-orthogonal-polynomially quasi-exactly solvable potentials, whose family is referred to as a harmonic oscillator with singularity functions in this thesis. In one case, the energy spectra become isospectral, with several additional eigenstates, to the ordinary harmonic oscillator for special choices of a parameter. By virtue of this, we formulate a systematic way of constructing infinitely many potentials that are strictly isospectral to the ordinary harmonic oscillator.","sentences":["In this thesis, we study a quantization condition in relation to the solvability of Schr\\\"{o}dinger equations.","This quantization condition is called the SWKB (supersymmetric Wentzel-Kramers-Brillouin) quantization condition and has been known in the context of supersymmetric quantum mechanics for decades.","The main contents of this thesis are recapitulated as follows: the foundation and the application of the SWKB quantization condition.","The first half of this thesis aims to understand the fundamental implications of this condition based on extensive case studies.","It turns out that the exactness of the SWKB quantization condition indicates the exact solvability of a system via the classical orthogonal polynomials.","The SWKB quantization condition provides quantizations of energy, which we call the direct problem of the SWKB.","We formulate the inverse problem of the SWKB: the problem of determining the superpotential from a given energy spectrum.","The formulation successfully reconstructs all conventional shape-invariant potentials from the given energy spectra.","We further construct novel solvable potentials, which are classical-orthogonal-polynomially quasi-exactly solvable, by this formulation.","We further demonstrate several explicit solutions of the Schr\\\"{o}dinger equations with the classical-orthogonal-polynomially quasi-exactly solvable potentials, whose family is referred to as a harmonic oscillator with singularity functions in this thesis.","In one case, the energy spectra become isospectral, with several additional eigenstates, to the ordinary harmonic oscillator for special choices of a parameter.","By virtue of this, we formulate a systematic way of constructing infinitely many potentials that are strictly isospectral to the ordinary harmonic oscillator."],"url":"http://arxiv.org/abs/2403.20217v1","category":"math-ph"}
{"created":"2024-03-29 14:45:22","title":"Horn maps of semi-parabolic H\u00e9non maps","abstract":"We prove that horn maps associated to quadratic semi-parabolic fixed points of H\\'enon maps, first introduced by Bedford, Smillie, and Ueda, satisfy a weak form of the Ahlfors island property. As a consequence, two natural definitions of their Julia set (the non-normality locus of the family of iterates and the closure of the set of the repelling periodic points) coincide. As another consequence, we also prove that there exist small perturbations of semi-parabolic H\\'enon maps for which the Hausdorff dimension of the forward Julia set $J^+$ is arbitrarily close to 4.","sentences":["We prove that horn maps associated to quadratic semi-parabolic fixed points of H\\'enon maps, first introduced by Bedford, Smillie, and Ueda, satisfy a weak form of the Ahlfors island property.","As a consequence, two natural definitions of their Julia set (the non-normality locus of the family of iterates and the closure of the set of the repelling periodic points) coincide.","As another consequence, we also prove that there exist small perturbations of semi-parabolic H\\'enon maps for which the Hausdorff dimension of the forward Julia set $J^+$ is arbitrarily close to 4."],"url":"http://arxiv.org/abs/2403.20211v1","category":"math.DS"}
{"created":"2024-03-29 14:32:31","title":"Strongly interacting Bose-Fermi mixtures in $4-\u03b5$ dimensions","abstract":"Thermodynamically stable low-temperature phases of the Bose-Fermi mixtures composed of bosons and spinless fermions close to four dimensions are considered. In the regime, where the only boson-fermion two-body interaction is present and tuned to unitary limit, the properties of a system solely depend on the mass and number ratios of constituent atoms. In addition to the phase with the dimers (boson-fermion shallow bound states), we identified one more state of the mixture with the coexistence of fermionic dimers and trimers. The universal physics of these phases, the characteristic feature of is absence of the Bose-Einstein condensate, is discussed.","sentences":["Thermodynamically stable low-temperature phases of the Bose-Fermi mixtures composed of bosons and spinless fermions close to four dimensions are considered.","In the regime, where the only boson-fermion two-body interaction is present and tuned to unitary limit, the properties of a system solely depend on the mass and number ratios of constituent atoms.","In addition to the phase with the dimers (boson-fermion shallow bound states), we identified one more state of the mixture with the coexistence of fermionic dimers and trimers.","The universal physics of these phases, the characteristic feature of is absence of the Bose-Einstein condensate, is discussed."],"url":"http://arxiv.org/abs/2403.20203v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-29 14:23:58","title":"Minimizing End-to-End Latency for Joint Source-Channel Coding Systems","abstract":"While existing studies have highlighted the advantages of deep learning (DL)-based joint source-channel coding (JSCC) schemes in enhancing transmission efficiency, they often overlook the crucial aspect of resource management during the deployment phase. In this paper, we propose an approach to minimize the transmission latency in an uplink JSCC-based system. We first analyze the correlation between end-to-end latency and task performance, based on which the end-to-end delay model for each device is established. Then, we formulate a non-convex optimization problem aiming at minimizing the maximum end-to-end latency across all devices, which is proved to be NP-hard. We then transform the original problem into a more tractable one, from which we derive the closed form solution on the optimal compression ratio, truncation threshold selection policy, and resource allocation strategy. We further introduce a heuristic algorithm with low complexity, leveraging insights from the structure of the optimal solution. Simulation results demonstrate that both the proposed optimal algorithm and the heuristic algorithm significantly reduce end-to-end latency. Notably, the proposed heuristic algorithm achieves nearly the same performance to the optimal solution but with considerably lower computational complexity.","sentences":["While existing studies have highlighted the advantages of deep learning (DL)-based joint source-channel coding (JSCC) schemes in enhancing transmission efficiency, they often overlook the crucial aspect of resource management during the deployment phase.","In this paper, we propose an approach to minimize the transmission latency in an uplink JSCC-based system.","We first analyze the correlation between end-to-end latency and task performance, based on which the end-to-end delay model for each device is established.","Then, we formulate a non-convex optimization problem aiming at minimizing the maximum end-to-end latency across all devices, which is proved to be NP-hard.","We then transform the original problem into a more tractable one, from which we derive the closed form solution on the optimal compression ratio, truncation threshold selection policy, and resource allocation strategy.","We further introduce a heuristic algorithm with low complexity, leveraging insights from the structure of the optimal solution.","Simulation results demonstrate that both the proposed optimal algorithm and the heuristic algorithm significantly reduce end-to-end latency.","Notably, the proposed heuristic algorithm achieves nearly the same performance to the optimal solution but with considerably lower computational complexity."],"url":"http://arxiv.org/abs/2403.20198v1","category":"cs.IT"}
{"created":"2024-03-29 14:09:59","title":"Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data","abstract":"The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it. Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality. The homomorphic evaluation of inference algorithms is practical even for relatively deep Convolution Neural Networks (CNNs). However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition. This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data. Compared to CNNs, WNNs offer better performance with a relatively small accuracy drop. We develop a complete framework for it, including several building blocks that can be of independent interest. Our framework achieves 91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted training (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000 dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after 1 hour. Compared to the state of the art on the HE evaluation of CNN training, Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to 1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even achieved a 0.65% accuracy improvement while being 60 times faster than Glyph. We also provide solutions for small-scale encrypted training. In a single thread on a desktop machine using less than 200MB of memory, we train over 1000 MNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset in just 11 seconds.","sentences":["The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it.","Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality.","The homomorphic evaluation of inference algorithms is practical even for relatively deep Convolution Neural Networks (CNNs).","However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition.","This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data.","Compared to CNNs, WNNs offer better performance with a relatively small accuracy drop.","We develop a complete framework for it, including several building blocks that can be of independent interest.","Our framework achieves 91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted training (multi-threaded), going up to 93.8% in 3.5 hours.","For the HAM10000 dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after 1 hour.","Compared to the state of the art on the HE evaluation of CNN training, Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to 1200 times with an accuracy loss of at most 5.4%.","For HAM10000, we even achieved a 0.65% accuracy improvement while being 60 times faster than Glyph.","We also provide solutions for small-scale encrypted training.","In a single thread on a desktop machine using less than 200MB of memory, we train over 1000 MNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset in just 11 seconds."],"url":"http://arxiv.org/abs/2403.20190v1","category":"cs.CR"}
{"created":"2024-03-29 14:07:28","title":"Field tuning Kitaev systems for spin fractionalization and topological order","abstract":"The honeycomb Kitaev model describes a $Z_2$ spin liquid with topological order and fractionalized excitations consisting of gapped $\\pi$-fluxes and free Majorana fermions. Competing interactions, even when not very strong, are known to destabilize the Kitaev spin liquid. Magnetic fields are a convenient parameter for tuning between different phases of the Kitaev systems, and have even been investigated for potentially counteracting the effects of other destabilizing interactions leading to a revival of the topological phase. Here we review the progress in understanding the effects of magnetic fields on some of the perturbed Kitaev systems, particularly on fractionalization and topological order.","sentences":["The honeycomb Kitaev model describes a $Z_2$ spin liquid with topological order and fractionalized excitations consisting of gapped $\\pi$-fluxes and free Majorana fermions.","Competing interactions, even when not very strong, are known to destabilize the Kitaev spin liquid.","Magnetic fields are a convenient parameter for tuning between different phases of the Kitaev systems, and have even been investigated for potentially counteracting the effects of other destabilizing interactions leading to a revival of the topological phase.","Here we review the progress in understanding the effects of magnetic fields on some of the perturbed Kitaev systems, particularly on fractionalization and topological order."],"url":"http://arxiv.org/abs/2403.20189v1","category":"cond-mat.str-el"}
{"created":"2024-03-29 14:05:24","title":"Estimation of Orbital Parameters from (u,v)-coverage for a Space Radio Interferometer","abstract":"Finding a suitable very long baseline (VLBI) interferometer geometry is a key task in planning observations, especially imaging sessions. The main characteristic of the quality of VLBI imaging data is the (u, v)-coverage. In the case when one or more radio telescopes are located in space, this task becomes more complex. This paper presents a method for recovering the optimal orbital parameters of space radio telescopes for a given desired (u, v)-coverage, which in turn is the inverse task of searching for the optimal geometry and orbital configurations of space-ground and pure space VLBI interferometers.","sentences":["Finding a suitable very long baseline (VLBI) interferometer geometry is a key task in planning observations, especially imaging sessions.","The main characteristic of the quality of VLBI imaging data is the (u, v)-coverage.","In the case when one or more radio telescopes are located in space, this task becomes more complex.","This paper presents a method for recovering the optimal orbital parameters of space radio telescopes for a given desired (u, v)-coverage, which in turn is the inverse task of searching for the optimal geometry and orbital configurations of space-ground and pure space VLBI interferometers."],"url":"http://arxiv.org/abs/2403.20187v1","category":"astro-ph.IM"}
{"created":"2024-03-29 13:56:21","title":"Measuring Taiwanese Mandarin Language Understanding","abstract":"The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts. The findings indicate great headrooms for improvement, and emphasize the goal of TMLU to foster the development of localized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation scripts for the community to promote future research.","sentences":["The evaluation of large language models (LLMs) has drawn substantial attention in the field recently.","This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks.","We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin.","TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels.","In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills.","To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs.","The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts.","The findings indicate great headrooms for improvement, and emphasize the goal of TMLU to foster the development of localized Taiwanese-Mandarin LLMs.","We release the benchmark and evaluation scripts for the community to promote future research."],"url":"http://arxiv.org/abs/2403.20180v1","category":"cs.CL"}
{"created":"2024-03-29 13:54:42","title":"On Classification of compact complex surfaces of class VII","abstract":"Let $S$ be a minimal compact complex surface with Betti numbers $b_1(S)=1$ and $b_2(S)\\ge 1$ i.e. a compact surface in class VII$_0^+$. We show that if there exists a twisted logarithmic 1-form $\\tau\\in H^0(S,\\Omega^1(\\log D)\\otimes \\mathcal L_\\lambda)$, where $D$ is a non zero divisor and $\\mathcal L\\in H^1(S,\\mathbb C^\\star)$, then $S$ is a Kato surface. It is known that $\\lambda$ is in fact real and we show that $\\lambda\\ge 1$ and unique if $S$ is not a Inoue-Hirzebruch surface. Moreover $\\lambda=1$ if and only if $S$ is a Enoki surface. When $\\lambda>1$ these conditions are equivalent to the existence of a negative PSH function $\\hat \\tau$ on the cyclic covering $p:\\hat S\\to S$ of $S$ which is PH outside $\\hat D:=p^{-1}(D)$ with automorphy constant being the same automorphy constant $\\lambda$ for a suitable automorphism of $\\hat S$. With previous results obtained with V.Apostolov it suggests a strategy to prove the GSS conjecture.","sentences":["Let $S$ be a minimal compact complex surface with Betti numbers $b_1(S)=1$ and","$b_2(S)\\ge 1$ i.e. a compact surface in class VII$_0^+$. We show that if there exists a twisted logarithmic 1-form $\\tau\\in H^0(S,\\Omega^1(\\log D)\\otimes \\mathcal L_\\lambda)$, where $D$ is a non zero divisor and $\\mathcal L\\in H^1(S,\\mathbb C^\\star)$, then $S$ is a Kato surface.","It is known that $\\lambda$ is in fact real and we show that $\\lambda\\ge 1$ and unique if $S$ is not a Inoue-Hirzebruch surface.","Moreover $\\lambda=1$ if and only if $S$ is a Enoki surface.","When $\\lambda>1$ these conditions are equivalent to the existence of a negative PSH function $\\hat \\tau$ on the cyclic covering $p:\\hat S\\to S$ of $S$ which is PH outside $\\hat D:=p^{-1}(D)$ with automorphy constant being the same automorphy constant $\\lambda$ for a suitable automorphism of $\\hat S$. With previous results obtained with V.Apostolov it suggests a strategy to prove the GSS conjecture."],"url":"http://arxiv.org/abs/2403.20178v1","category":"math.CV"}
{"created":"2024-03-29 13:41:33","title":"Species Syzygy: Which Animal Has Seen the Most Total Solar Eclipses?","abstract":"A Total Solar Eclipse (TSE) is a shocking and sublime experience. In just a week hundreds of millions of Homo Sapiens will attempt to see the 2024 eclipse as it stretches across the North American continent. However, while Homo Sapiens may be uniquely positioned to understand and predict eclipses, they are not the only species capable of observing them. The precise alignment of the Moon, Earth and Sun all existed well before humans. In the same way we share this planet capable of hosting life, the fantastic astronomical experiences available on it are not exclusive either. We present a framework to calculate the number of Total Solar Eclipses experienced by a species at any point in Earth's history. This includes factoring in the evolution of the Sun-Moon-Earth system, the duration the species is extant, and average population. We normalize over the geographic range by calculating an Astronomical World Eclipse Surface cOverage MEtric (AWESOME) time. To illustrate this framework we look at the case study of the family Limulidae (Horseshoe Crabs) and estimate the number of individuals that have seen an eclipse. We compare it to the number of current Homo Sapiens that view eclipses, and predict if it is possible for another species to take the ''top'' spot before the final total solar eclipse in ~ 380 million years.","sentences":["A Total Solar Eclipse (TSE) is a shocking and sublime experience.","In just a week hundreds of millions of Homo Sapiens will attempt to see the 2024 eclipse as it stretches across the North American continent.","However, while Homo Sapiens may be uniquely positioned to understand and predict eclipses, they are not the only species capable of observing them.","The precise alignment of the Moon, Earth and Sun all existed well before humans.","In the same way we share this planet capable of hosting life, the fantastic astronomical experiences available on it are not exclusive either.","We present a framework to calculate the number of Total Solar Eclipses experienced by a species at any point in Earth's history.","This includes factoring in the evolution of the Sun-Moon-Earth system, the duration the species is extant, and average population.","We normalize over the geographic range by calculating an Astronomical World Eclipse Surface cOverage MEtric (AWESOME) time.","To illustrate this framework we look at the case study of the family Limulidae (Horseshoe Crabs) and estimate the number of individuals that have seen an eclipse.","We compare it to the number of current Homo Sapiens that view eclipses, and predict if it is possible for another species to take the ''top'' spot before the final total solar eclipse in ~ 380 million years."],"url":"http://arxiv.org/abs/2403.20175v1","category":"astro-ph.EP"}
{"created":"2024-03-29 13:40:44","title":"MCNet: A crowd denstity estimation network based on integrating multiscale attention module","abstract":"Aiming at the metro video surveillance system has not been able to effectively solve the metro crowd density estimation problem, a Metro Crowd density estimation Network (called MCNet) is proposed to automatically classify crowd density level of passengers. Firstly, an Integrating Multi-scale Attention (IMA) module is proposed to enhance the ability of the plain classifiers to extract semantic crowd texture features to accommodate to the characteristics of the crowd texture feature. The innovation of the IMA module is to fuse the dilation convolution, multiscale feature extraction and attention mechanism to obtain multi-scale crowd feature activation from a larger receptive field with lower computational cost, and to strengthen the crowds activation state of convolutional features in top layers. Secondly, a novel lightweight crowd texture feature extraction network is proposed, which can directly process video frames and automatically extract texture features for crowd density estimation, while its faster image processing speed and fewer network parameters make it flexible to be deployed on embedded platforms with limited hardware resources. Finally, this paper integrates IMA module and the lightweight crowd texture feature extraction network to construct the MCNet, and validate the feasibility of this network on image classification dataset: Cifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to validate the MCNet whether can be a suitable solution for crowd density estimation in metro video surveillance where there are image processing challenges such as high density, high occlusion, perspective distortion and limited hardware resources.","sentences":["Aiming at the metro video surveillance system has not been able to effectively solve the metro crowd density estimation problem, a Metro Crowd density estimation Network (called MCNet) is proposed to automatically classify crowd density level of passengers.","Firstly, an Integrating Multi-scale Attention (IMA) module is proposed to enhance the ability of the plain classifiers to extract semantic crowd texture features to accommodate to the characteristics of the crowd texture feature.","The innovation of the IMA module is to fuse the dilation convolution, multiscale feature extraction and attention mechanism to obtain multi-scale crowd feature activation from a larger receptive field with lower computational cost, and to strengthen the crowds activation state of convolutional features in top layers.","Secondly, a novel lightweight crowd texture feature extraction network is proposed, which can directly process video frames and automatically extract texture features for crowd density estimation, while its faster image processing speed and fewer network parameters make it flexible to be deployed on embedded platforms with limited hardware resources.","Finally, this paper integrates IMA module and the lightweight crowd texture feature extraction network to construct the MCNet, and validate the feasibility of this network on image classification dataset: Cifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to validate the MCNet whether can be a suitable solution for crowd density estimation in metro video surveillance where there are image processing challenges such as high density, high occlusion, perspective distortion and limited hardware resources."],"url":"http://arxiv.org/abs/2403.20173v1","category":"cs.CV"}
{"created":"2024-03-29 13:35:44","title":"On the smallness of mean oscillations and regularity of weak solutions to regular/degenerate strongly coupled parabolic systems","abstract":"It will be established that the mean oscillation of bounded weak solutions to strongly coupled parabolic systems is small in small balls. If the systems are regular elliptic then their bounded weak solutions are H\\\"older continuous. Further assumptions on the systems will even prove that these solutions exist globally. Weak solutions to degenerate systems of porous media type are also studied.","sentences":["It will be established that the mean oscillation of bounded weak solutions to strongly coupled parabolic systems is small in small balls.","If the systems are regular elliptic then their bounded weak solutions are H\\\"older continuous.","Further assumptions on the systems will even prove that these solutions exist globally.","Weak solutions to degenerate systems of porous media type are also studied."],"url":"http://arxiv.org/abs/2403.20169v1","category":"math.AP"}
{"created":"2024-03-29 13:22:22","title":"Hamiltonian aspects of the kinetic equation for soliton gas","abstract":"We investigate Hamiltonian aspects of the integro-differential kinetic equation for dense soliton gas which results as a thermodynamic limit of the Whitham equations. Under a delta-functional ansatz, the kinetic equation reduces to a non-diagonalisable system of hydrodynamic type whose matrix consists of several $2\\times 2$ Jordan blocks. We demonstrate that the resulting system possesses local Hamiltonian structures of differential-geometric type, for all standard two-soliton interaction kernels (KdV, sinh-Gordon, hard-rod, Lieb-Liniger, DNLS, and separable cases). In the hard-rod case, we show that the continuum limit of these structures provides a local multi-Hamiltonian formulation of the full kinetic equation.","sentences":["We investigate Hamiltonian aspects of the integro-differential kinetic equation for dense soliton gas which results as a thermodynamic limit of the Whitham equations.","Under a delta-functional ansatz, the kinetic equation reduces to a non-diagonalisable system of hydrodynamic type whose matrix consists of several $2\\times 2$ Jordan blocks.","We demonstrate that the resulting system possesses local Hamiltonian structures of differential-geometric type, for all standard two-soliton interaction kernels (KdV, sinh-Gordon, hard-rod, Lieb-Liniger, DNLS, and separable cases).","In the hard-rod case, we show that the continuum limit of these structures provides a local multi-Hamiltonian formulation of the full kinetic equation."],"url":"http://arxiv.org/abs/2403.20162v1","category":"nlin.SI"}
{"created":"2024-03-29 12:34:57","title":"Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets","abstract":"This paper studies the use of conformal prediction (CP), an emerging probabilistic forecasting method, for day-ahead photovoltaic power predictions to enhance participation in electricity markets. First, machine learning models are used to construct point predictions. Thereafter, several variants of CP are implemented to quantify the uncertainty of those predictions by creating CP intervals and cumulative distribution functions. Optimal quantity bids for the electricity market are estimated using several bidding strategies under uncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected utility maximization (EUM). Results show that CP in combination with k-nearest neighbors and/or Mondrian binning outperforms its corresponding linear quantile regressors. Using CP in combination with certain bidding strategies can yield high profit with minimal energy imbalance. In concrete, using conformal predictive systems with k-nearest neighbors and Mondrian binning after random forest regression yields the best profit and imbalance regardless of the decision-making strategy. Combining this uncertainty quantification method with the EUM strategy with conditional value at risk (CVaR) can yield up to 93\\% of the potential profit with minimal energy imbalance.","sentences":["This paper studies the use of conformal prediction (CP), an emerging probabilistic forecasting method, for day-ahead photovoltaic power predictions to enhance participation in electricity markets.","First, machine learning models are used to construct point predictions.","Thereafter, several variants of CP are implemented to quantify the uncertainty of those predictions by creating CP intervals and cumulative distribution functions.","Optimal quantity bids for the electricity market are estimated using several bidding strategies under uncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected utility maximization (EUM).","Results show that CP in combination with k-nearest neighbors and/or Mondrian binning outperforms its corresponding linear quantile regressors.","Using CP in combination with certain bidding strategies can yield high profit with minimal energy imbalance.","In concrete, using conformal predictive systems with k-nearest neighbors and Mondrian binning after random forest regression yields the best profit and imbalance regardless of the decision-making strategy.","Combining this uncertainty quantification method with the EUM strategy with conditional value at risk (CVaR) can yield up to 93\\% of the potential profit with minimal energy imbalance."],"url":"http://arxiv.org/abs/2403.20149v1","category":"cs.LG"}
{"created":"2024-03-29 12:19:57","title":"Development of a full-Scale approach to predict overlay reflective crack","abstract":"Resurfacing a moderately deteriorated Portland cement concrete (PCC) pavement with asphalt concrete (AC) layers is considered an efficient rehabilitation practice. However, reflective cracks may develop shortly after resurfacing because of discontinuities (e.g. joints and cracks) in existing PCC pavement. In this paper, a new accelerated full-scale testing approach was developed to study reflective crack growth in AC overlays. Two hydraulic actuators were used to simulate a moving dual-tire assembly with a loading rate of more than five-thousand-wheel passes per hour. A load cycle consists of three steps, simulating a tire approaching, moving across, and leaving a PCC discontinuity. Experiments were conducted to compare the reflective crack behaviour of two overlay configurations. Both test sections were fully cracked in less than an hour. The initiation and propagation of reflective cracks were explicitly documented using crack detectors in conjunction with a camera. The proposed full-scale testing protocol offers a repeatable and efficient approach to systematically investigate the effects of various overlay configurations, thus enabling the identification of optimal design against reflective cracking.","sentences":["Resurfacing a moderately deteriorated Portland cement concrete (PCC) pavement with asphalt concrete (AC) layers is considered an efficient rehabilitation practice.","However, reflective cracks may develop shortly after resurfacing because of discontinuities (e.g. joints and cracks) in existing PCC pavement.","In this paper, a new accelerated full-scale testing approach was developed to study reflective crack growth in AC overlays.","Two hydraulic actuators were used to simulate a moving dual-tire assembly with a loading rate of more than five-thousand-wheel passes per hour.","A load cycle consists of three steps, simulating a tire approaching, moving across, and leaving a PCC discontinuity.","Experiments were conducted to compare the reflective crack behaviour of two overlay configurations.","Both test sections were fully cracked in less than an hour.","The initiation and propagation of reflective cracks were explicitly documented using crack detectors in conjunction with a camera.","The proposed full-scale testing protocol offers a repeatable and efficient approach to systematically investigate the effects of various overlay configurations, thus enabling the identification of optimal design against reflective cracking."],"url":"http://arxiv.org/abs/2403.20141v1","category":"eess.SY"}
{"created":"2024-03-29 11:56:58","title":"Parallel performance of shared memory parallel spectral deferred corrections","abstract":"We investigate parallel performance of parallel spectral deferred corrections, a numerical approach that provides small-scale parallelism for the numerical solution of initial value problems. The scheme is applied to the shallow water equation and uses an IMEX splitting that integrates fast modes implicitly and slow modes explicitly in order to be efficient. We describe parallel $\\texttt{OpenMP}$-based implementations of parallel SDC in two well established simulation codes: the finite volume based operational ocean model $\\texttt{ICON-O}$ and the spherical harmonics based research code $\\texttt{SWEET}$. The implementations are benchmarked on a single node of the JUSUF ($\\texttt{SWEET}$) and JUWELS ($\\texttt{ICON-O}$) system at J\\\"ulich Supercomputing Centre. We demonstrate a reduction of time-to-solution across a range of accuracies. For $\\texttt{ICON-O}$, we show speedup over the currently used Adams--Bashforth-2 integrator with $\\texttt{OpenMP}$ loop parallelization. For $\\texttt{SWEET}$, we show speedup over serial spectral deferred corrections and a second order implicit-explicit integrator.","sentences":["We investigate parallel performance of parallel spectral deferred corrections, a numerical approach that provides small-scale parallelism for the numerical solution of initial value problems.","The scheme is applied to the shallow water equation and uses an IMEX splitting that integrates fast modes implicitly and slow modes explicitly in order to be efficient.","We describe parallel $\\texttt{OpenMP}$-based implementations of parallel SDC in two well established simulation codes: the finite volume based operational ocean model $\\texttt{ICON-O}$ and the spherical harmonics based research code $\\texttt{SWEET}$.","The implementations are benchmarked on a single node of the JUSUF ($\\texttt{SWEET}$) and JUWELS ($\\texttt{ICON-O}$) system at J\\\"ulich Supercomputing Centre.","We demonstrate a reduction of time-to-solution across a range of accuracies.","For $\\texttt{ICON-O}$, we show speedup over the currently used Adams--Bashforth-2 integrator with $\\texttt{OpenMP}$ loop parallelization.","For $\\texttt{SWEET}$, we show speedup over serial spectral deferred corrections and a second order implicit-explicit integrator."],"url":"http://arxiv.org/abs/2403.20135v1","category":"cs.CE"}
{"created":"2024-03-29 11:37:43","title":"Simple inverse kinematics computation considering joint motion efficiency","abstract":"Inverse kinematics is an important and challenging problem in the operation of industrial manipulators. This study proposes a simple inverse kinematics calculation scheme for an industrial serial manipulator. The proposed technique can calculate appropriate values of the joint variables to realize the desired end-effector position and orientation while considering the motion costs of each joint. Two scalar functions are defined for the joint variables: one is to evaluate the end-effector position and orientation, whereas the other is to evaluate the motion efficiency of the joints. By combining the two scalar functions, the inverse kinematics calculation of the manipulator is formulated as a numerical optimization problem. Furthermore, a simple algorithm for solving the inverse kinematics via the aforementioned optimization is constructed on the basis of the simultaneous perturbation stochastic approximation with a norm-limited update vector (NLSPSA). The proposed scheme considers not only the accuracy of the position and orientation of the end-effector but also the efficiency of the robot movement. Therefore, it yields a practical result of the inverse problem. Moreover, the proposed algorithm is simple and easy to implement owing to the high calculation efficiency of NLSPSA. Finally, the effectiveness of the proposed method is verified through numerical examples using a redundant manipulator.","sentences":["Inverse kinematics is an important and challenging problem in the operation of industrial manipulators.","This study proposes a simple inverse kinematics calculation scheme for an industrial serial manipulator.","The proposed technique can calculate appropriate values of the joint variables to realize the desired end-effector position and orientation while considering the motion costs of each joint.","Two scalar functions are defined for the joint variables: one is to evaluate the end-effector position and orientation, whereas the other is to evaluate the motion efficiency of the joints.","By combining the two scalar functions, the inverse kinematics calculation of the manipulator is formulated as a numerical optimization problem.","Furthermore, a simple algorithm for solving the inverse kinematics via the aforementioned optimization is constructed on the basis of the simultaneous perturbation stochastic approximation with a norm-limited update vector (NLSPSA).","The proposed scheme considers not only the accuracy of the position and orientation of the end-effector but also the efficiency of the robot movement.","Therefore, it yields a practical result of the inverse problem.","Moreover, the proposed algorithm is simple and easy to implement owing to the high calculation efficiency of NLSPSA.","Finally, the effectiveness of the proposed method is verified through numerical examples using a redundant manipulator."],"url":"http://arxiv.org/abs/2403.20128v1","category":"cs.RO"}
{"created":"2024-03-29 11:31:12","title":"ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning","abstract":"Panoptic segmentation, combining semantic and instance segmentation, stands as a cutting-edge computer vision task. Despite recent progress with deep learning models, the dynamic nature of real-world applications necessitates continual learning, where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting). Current continual segmentation methods often rely on distillation strategies like knowledge distillation and pseudo-labeling, which are effective but result in increased training complexity and computational overhead. In this paper, we introduce a novel and efficient method for continual panoptic segmentation based on Visual Prompt Tuning, dubbed ECLIPSE. Our approach involves freezing the base model parameters and fine-tuning only a small set of prompt embeddings, addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters. To mitigate inherent challenges such as error propagation and semantic drift in continual segmentation, we propose logit manipulation to effectively leverage common knowledge across the classes. Experiments on ADE20K continual panoptic segmentation benchmark demonstrate the superiority of ECLIPSE, notably its robustness against catastrophic forgetting and its reasonable plasticity, achieving a new state-of-the-art. The code is available at https://github.com/clovaai/ECLIPSE.","sentences":["Panoptic segmentation, combining semantic and instance segmentation, stands as a cutting-edge computer vision task.","Despite recent progress with deep learning models, the dynamic nature of real-world applications necessitates continual learning, where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting).","Current continual segmentation methods often rely on distillation strategies like knowledge distillation and pseudo-labeling, which are effective but result in increased training complexity and computational overhead.","In this paper, we introduce a novel and efficient method for continual panoptic segmentation based on Visual Prompt Tuning, dubbed ECLIPSE.","Our approach involves freezing the base model parameters and fine-tuning only a small set of prompt embeddings, addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters.","To mitigate inherent challenges such as error propagation and semantic drift in continual segmentation, we propose logit manipulation to effectively leverage common knowledge across the classes.","Experiments on ADE20K continual panoptic segmentation benchmark demonstrate the superiority of ECLIPSE, notably its robustness against catastrophic forgetting and its reasonable plasticity, achieving a new state-of-the-art.","The code is available at https://github.com/clovaai/ECLIPSE."],"url":"http://arxiv.org/abs/2403.20126v1","category":"cs.CV"}
{"created":"2024-03-29 11:27:37","title":"Application of Machine Learning Algorithms in Classifying Postoperative Success in Metabolic Bariatric Surgery: A Comprehensive Study","abstract":"Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for patients living with obesity and related health issues. Accurate classification and prediction of patient outcomes are vital for optimizing treatment strategies. This study presents a novel machine learning approach to classify patients in the context of metabolic bariatric surgery, providing insights into the efficacy of different models and variable types. Methods: Various machine learning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN with RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73 patients. The dataset, comprising psychometric, socioeconomic, and analytical variables, was analyzed to determine the most efficient predictive model. The study also explored the impact of different variable groupings and oversampling techniques. Results: Experimental results indicate average accuracy values as high as 66.7% for the best model. Enhanced versions of KNN and Decision Tree, along with variations of KNN such as RandomOverSampler and SMOTE, yielded the best results. Conclusions: The study unveils a promising avenue for classifying patients in the realm of metabolic bariatric surgery. The results underscore the importance of selecting appropriate variables and employing diverse approaches to achieve optimal performance. The developed system holds potential as a tool to assist healthcare professionals in decision-making, thereby enhancing metabolic bariatric surgery outcomes. These findings lay the groundwork for future collaboration between hospitals and healthcare entities to improve patient care through the utilization of machine learning algorithms. Moreover, the findings suggest room for improvement, potentially achievable with a larger dataset and careful parameter tuning.","sentences":["Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for patients living with obesity and related health issues.","Accurate classification and prediction of patient outcomes are vital for optimizing treatment strategies.","This study presents a novel machine learning approach to classify patients in the context of metabolic bariatric surgery, providing insights into the efficacy of different models and variable types.","Methods: Various machine learning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN with RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73 patients.","The dataset, comprising psychometric, socioeconomic, and analytical variables, was analyzed to determine the most efficient predictive model.","The study also explored the impact of different variable groupings and oversampling techniques.","Results: Experimental results indicate average accuracy values as high as 66.7% for the best model.","Enhanced versions of KNN and Decision Tree, along with variations of KNN such as RandomOverSampler and SMOTE, yielded the best results.","Conclusions: The study unveils a promising avenue for classifying patients in the realm of metabolic bariatric surgery.","The results underscore the importance of selecting appropriate variables and employing diverse approaches to achieve optimal performance.","The developed system holds potential as a tool to assist healthcare professionals in decision-making, thereby enhancing metabolic bariatric surgery outcomes.","These findings lay the groundwork for future collaboration between hospitals and healthcare entities to improve patient care through the utilization of machine learning algorithms.","Moreover, the findings suggest room for improvement, potentially achievable with a larger dataset and careful parameter tuning."],"url":"http://arxiv.org/abs/2403.20124v1","category":"cs.LG"}
{"created":"2024-03-29 11:09:22","title":"Privacy-Preserving Data Aggregation Techniques for Enhanced Efficiency and Security in Wireless Sensor Networks: A Comprehensive Analysis and Evaluation","abstract":"In this paper, we present a multidimensional, highly effective method for aggregating data for wireless sensor networks while maintaining privacy. The suggested system is resistant to data loss and secure against both active and passive privacy compromising attacks, such as the coalition attack from a rogue base station and kidnapped sensor nodes. With regard to cluster size, it achieves consistent communication overhead, which is helpful in large-scale WSNs. Due to its constant size communication overhead, the suggested strategy outperforms the previous privacy-preserving data aggregation scheme not only in terms of privacy preservation but also in terms of communication complexity and energy costs.","sentences":["In this paper, we present a multidimensional, highly effective method for aggregating data for wireless sensor networks while maintaining privacy.","The suggested system is resistant to data loss and secure against both active and passive privacy compromising attacks, such as the coalition attack from a rogue base station and kidnapped sensor nodes.","With regard to cluster size, it achieves consistent communication overhead, which is helpful in large-scale WSNs.","Due to its constant size communication overhead, the suggested strategy outperforms the previous privacy-preserving data aggregation scheme not only in terms of privacy preservation but also in terms of communication complexity and energy costs."],"url":"http://arxiv.org/abs/2403.20120v1","category":"cs.CR"}
{"created":"2024-03-29 10:59:44","title":"Design, Fabrication and Evaluation of a Stretchable High-Density Electromyography Array","abstract":"The adoption of high-density electrode systems for human-machine interfaces in real-life applications has been impeded by practical and technical challenges, including noise interference, motion artifacts and the lack of compact electrode interfaces. To overcome some of these challenges, we introduce a wearable and stretchable electromyography (EMG) array, and present its design, fabrication methodology, characterisation, and comprehensive evaluation. Our proposed solution comprises dry-electrodes on flexible printed circuit board (PCB) substrates, eliminating the need for time-consuming skin preparation. The proposed fabrication method allows the manufacturing of stretchable sleeves, with consistent and standardised coverage across subjects. We thoroughly tested our developed prototype, evaluating its potential for application in both research and real-world environments. The results of our study showed that the developed stretchable array matches or outperforms traditional EMG grids and holds promise in furthering the real-world translation of high-density EMG for human-machine interfaces.","sentences":["The adoption of high-density electrode systems for human-machine interfaces in real-life applications has been impeded by practical and technical challenges, including noise interference, motion artifacts and the lack of compact electrode interfaces.","To overcome some of these challenges, we introduce a wearable and stretchable electromyography (EMG) array, and present its design, fabrication methodology, characterisation, and comprehensive evaluation.","Our proposed solution comprises dry-electrodes on flexible printed circuit board (PCB) substrates, eliminating the need for time-consuming skin preparation.","The proposed fabrication method allows the manufacturing of stretchable sleeves, with consistent and standardised coverage across subjects.","We thoroughly tested our developed prototype, evaluating its potential for application in both research and real-world environments.","The results of our study showed that the developed stretchable array matches or outperforms traditional EMG grids and holds promise in furthering the real-world translation of high-density EMG for human-machine interfaces."],"url":"http://arxiv.org/abs/2403.20117v1","category":"cs.RO"}
{"created":"2024-03-29 10:49:02","title":"Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning","abstract":"This paper explores the application of Human-in-the-Loop (HITL) strategies in training machine learning models in the medical domain. In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be effective.","sentences":["This paper explores the application of Human-in-the-Loop (HITL) strategies in training machine learning models in the medical domain.","In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data.","Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer.","Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results.","The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be effective."],"url":"http://arxiv.org/abs/2403.20112v1","category":"cs.CV"}
{"created":"2024-03-29 10:48:20","title":"Divisibility of Integer Laurent Polynomials, Homoclinic Points, and Lacunary Independence","abstract":"Let $f$, $p$, and $q$ be Laurent polynomials with integer coefficients in one or several variables, and suppose that $f$ divides $p+q$. We establish sufficient conditions to guarantee that $f$ individually divides $p$ and $q$. These conditions involve a bound on coefficients, a separation between the supports of $p$ and $q$, and, surprisingly, a requirement on the complex variety of $f$ called atorality satisfied by many but not all polynomials.   Our proof involves a related dynamical system and the fundamental dynamical notion of homoclinic point. Without the atorality assumption our methods fail, and it is unknown whether our results hold without this assumption.","sentences":["Let $f$, $p$, and $q$ be Laurent polynomials with integer coefficients in one or several variables, and suppose that $f$ divides $p+q$. We establish sufficient conditions to guarantee that $f$ individually divides $p$ and $q$. These conditions involve a bound on coefficients, a separation between the supports of $p$ and $q$, and, surprisingly, a requirement on the complex variety of $f$ called atorality satisfied by many but not all polynomials.   ","Our proof involves a related dynamical system and the fundamental dynamical notion of homoclinic point.","Without the atorality assumption our methods fail, and it is unknown whether our results hold without this assumption."],"url":"http://arxiv.org/abs/2403.20111v1","category":"math.DS"}
{"created":"2024-03-29 10:47:20","title":"Theory of the inverse Faraday effect in dissipative Rashba electron systems: Floquet engineering perspective","abstract":"We theoretically study the inverse Faraday effect (IFE), i.e., photo-induced magnetization, in two-dimensional Rashba spin-orbit coupled electron systems irradiated by a circularly polarized light. Quantum master (GKSL) equation enables us to accurately compute the laser driven dynamics, taking inevitable dissipation effects into account. To find the universal features of laser-driven magnetization and its dynamics, we investigate (i) the nonequilibrium steady state (NESS) driven by a continuous wave (CW) and (ii) ultrafast spin dynamics driven by short laser pulses. In the NESS (i), the laser-induced magnetization and its dependence of several parameters (laser frequency, laser field strength, temperature, dissipation strength, etc.) are shown to be in good agreement with the predictions from Floquet theory for dissipative systems in the high-frequency regime. In the case (ii), we focus on ferromagnetic metal states by introducing an effective magnetic field to the Rashba model as the mean field of electron-electron interaction. We find that a precession of the magnetic moment occurs due to the pulse-driven instantaneous magnetic field and the initial phase of the precession is controlled by changing the sign of light polarization. This is well consistent with the spin dynamics observed in experiments of laser-pulse-driven IFE. We discuss how the pulse-driven dynamics are captured by the Floquet theory. Our results provides a microscopic method to compute ultrafast dynamics in many electron systems irradiated by intense light.","sentences":["We theoretically study the inverse Faraday effect (IFE), i.e., photo-induced magnetization, in two-dimensional Rashba spin-orbit coupled electron systems irradiated by a circularly polarized light.","Quantum master (GKSL) equation enables us to accurately compute the laser driven dynamics, taking inevitable dissipation effects into account.","To find the universal features of laser-driven magnetization and its dynamics, we investigate (i) the nonequilibrium steady state (NESS) driven by a continuous wave (CW) and (ii) ultrafast spin dynamics driven by short laser pulses.","In the NESS (i), the laser-induced magnetization and its dependence of several parameters (laser frequency, laser field strength, temperature, dissipation strength, etc.) are shown to be in good agreement with the predictions from Floquet theory for dissipative systems in the high-frequency regime.","In the case (ii), we focus on ferromagnetic metal states by introducing an effective magnetic field to the Rashba model as the mean field of electron-electron interaction.","We find that a precession of the magnetic moment occurs due to the pulse-driven instantaneous magnetic field and the initial phase of the precession is controlled by changing the sign of light polarization.","This is well consistent with the spin dynamics observed in experiments of laser-pulse-driven IFE.","We discuss how the pulse-driven dynamics are captured by the Floquet theory.","Our results provides a microscopic method to compute ultrafast dynamics in many electron systems irradiated by intense light."],"url":"http://arxiv.org/abs/2403.20110v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-29 10:40:45","title":"Robust Federated Contrastive Recommender System against Model Poisoning Attack","abstract":"Federated Recommender Systems (FedRecs) have garnered increasing attention recently, thanks to their privacy-preserving benefits. However, the decentralized and open characteristics of current FedRecs present two dilemmas. First, the performance of FedRecs is compromised due to highly sparse on-device data for each client. Second, the system's robustness is undermined by the vulnerability to model poisoning attacks launched by malicious users. In this paper, we introduce a novel contrastive learning framework designed to fully leverage the client's sparse data through embedding augmentation, referred to as CL4FedRec. Unlike previous contrastive learning approaches in FedRecs that necessitate clients to share their private parameters, our CL4FedRec aligns with the basic FedRec learning protocol, ensuring compatibility with most existing FedRec implementations. We then evaluate the robustness of FedRecs equipped with CL4FedRec by subjecting it to several state-of-the-art model poisoning attacks. Surprisingly, our observations reveal that contrastive learning tends to exacerbate the vulnerability of FedRecs to these attacks. This is attributed to the enhanced embedding uniformity, making the polluted target item embedding easily proximate to popular items. Based on this insight, we propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by introducing a regularizer to maintain the distance among item embeddings with different popularity levels. Extensive experiments conducted on four commonly used recommendation datasets demonstrate that CL4FedRec significantly enhances both the model's performance and the robustness of FedRecs.","sentences":["Federated Recommender Systems (FedRecs) have garnered increasing attention recently, thanks to their privacy-preserving benefits.","However, the decentralized and open characteristics of current FedRecs present two dilemmas.","First, the performance of FedRecs is compromised due to highly sparse on-device data for each client.","Second, the system's robustness is undermined by the vulnerability to model poisoning attacks launched by malicious users.","In this paper, we introduce a novel contrastive learning framework designed to fully leverage the client's sparse data through embedding augmentation, referred to as CL4FedRec.","Unlike previous contrastive learning approaches in FedRecs that necessitate clients to share their private parameters, our CL4FedRec aligns with the basic FedRec learning protocol, ensuring compatibility with most existing FedRec implementations.","We then evaluate the robustness of FedRecs equipped with CL4FedRec by subjecting it to several state-of-the-art model poisoning attacks.","Surprisingly, our observations reveal that contrastive learning tends to exacerbate the vulnerability of FedRecs to these attacks.","This is attributed to the enhanced embedding uniformity, making the polluted target item embedding easily proximate to popular items.","Based on this insight, we propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by introducing a regularizer to maintain the distance among item embeddings with different popularity levels.","Extensive experiments conducted on four commonly used recommendation datasets demonstrate that CL4FedRec significantly enhances both the model's performance and the robustness of FedRecs."],"url":"http://arxiv.org/abs/2403.20107v1","category":"cs.IR"}
{"created":"2024-03-29 10:40:41","title":"Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring","abstract":"Image deblurring is a process of restoring a high quality image from the corresponding blurred image. Significant progress in this field has been made possible by the emergence of various effective deep learning models, including CNNs and Transformers. However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application. To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features. Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information. The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention. (2) The global block captures long-range dependency features with linear complexity through selective structured state spaces. Nevertheless, we note that the image details are local features of images, we accentuate the local part for restoration by recalibrating the weight when aggregating the two branches for recovery. Experimental results demonstrate that the proposed method outperforms state-of-the-art approaches on widely used benchmarks, highlighting its superior performance.","sentences":["Image deblurring is a process of restoring a high quality image from the corresponding blurred image.","Significant progress in this field has been made possible by the emergence of various effective deep learning models, including CNNs and Transformers.","However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application.","To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features.","Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information.","The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention.","(2) The global block captures long-range dependency features with linear complexity through selective structured state spaces.","Nevertheless, we note that the image details are local features of images, we accentuate the local part for restoration by recalibrating the weight when aggregating the two branches for recovery.","Experimental results demonstrate that the proposed method outperforms state-of-the-art approaches on widely used benchmarks, highlighting its superior performance."],"url":"http://arxiv.org/abs/2403.20106v1","category":"cs.CV"}
{"created":"2024-03-29 10:31:32","title":"RealKIE: Five Novel Datasets for Enterprise Key Information Extraction","abstract":"We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing.   In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to industry-specific problems.   The annotated data and OCR outputs are available to download at https://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines will be available shortly.","sentences":["We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications.","The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts.","Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts.","These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing.   ","In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches.","This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to industry-specific problems.   ","The annotated data and OCR outputs are available to download at https://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines will be available shortly."],"url":"http://arxiv.org/abs/2403.20101v1","category":"cs.CL"}
{"created":"2024-03-29 10:26:28","title":"Flowing Between String Vacua for the Critical Non-Abelian Vortex with Deformation of N=2 Liouville theory","abstract":"It has been shown that non-Abelian solitonic vortex string supported in four-dimensional (4D) N=2 supersymmetric QCD (SQCD) with the U(2) gauge group and $N_f=4$ quark flavors becomes a critical superstring. This string propagates in the ten-dimensional space formed by a product of the flat 4D space and an internal space given by a Calabi-Yau noncompact threefold, namely, the conifold. The spectrum of low lying closed string states was found and interpreted as a spectrum of hadrons in 4D N=2 SQCD. In particular, the lowest string state appears to be a massless BPS baryon associated with the deformation of the complex structure modulus $b$ of the conifold. It was recently shown that the Coulomb branch of the associated string sigma model which opens up at strong coupling can be described by N=2 Liouville theory. Building on these results we switch on quark masses in 4D N=2 SQCD and study the interpolation of the initial U(2) SQCD with $N_f=4$ quarks to the final SQCD with the U(4) gauge group and $N_f=8$ quarks. To find the true string vacuum which arises due to the mass deformation we solve the effective supergravity equations of motion associated with the deformed world sheet Liouville theory. We show that the massless BPS baryon $b$ survives the deformation and that finding of the spectrum of low lying massive hadrons in the final SQCD is linked to the Calogero problem.","sentences":["It has been shown that non-Abelian solitonic vortex string supported in four-dimensional (4D)","N=2 supersymmetric QCD (SQCD) with the U(2) gauge group and $N_f=4$ quark flavors becomes a critical superstring.","This string propagates in the ten-dimensional space formed by a product of the flat 4D space and an internal space given by a Calabi-Yau noncompact threefold, namely, the conifold.","The spectrum of low lying closed string states was found and interpreted as a spectrum of hadrons in 4D N=2 SQCD.","In particular, the lowest string state appears to be a massless BPS baryon associated with the deformation of the complex structure modulus $b$ of the conifold.","It was recently shown that the Coulomb branch of the associated string sigma model which opens up at strong coupling can be described by N=2 Liouville theory.","Building on these results we switch on quark masses in 4D N=2 SQCD and study the interpolation of the initial U(2) SQCD with $N_f=4$ quarks to the final SQCD with the U(4) gauge group and $N_f=8$ quarks.","To find the true string vacuum which arises due to the mass deformation we solve the effective supergravity equations of motion associated with the deformed world sheet Liouville theory.","We show that the massless BPS baryon $b$ survives the deformation and that finding of the spectrum of low lying massive hadrons in the final SQCD is linked to the Calogero problem."],"url":"http://arxiv.org/abs/2403.20099v1","category":"hep-th"}
{"created":"2024-03-29 10:18:19","title":"KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering","abstract":"The recent integration of Graph Neural Networks (GNNs) into recommendation has led to a novel family of Collaborative Filtering (CF) approaches, namely Graph Collaborative Filtering (GCF). Following the same GNNs wave, recommender systems exploiting Knowledge Graphs (KGs) have also been successfully empowered by the GCF rationale to combine the representational power of GNNs with the semantics conveyed by KGs, giving rise to Knowledge-aware Graph Collaborative Filtering (KGCF), which use KGs to mine hidden user intent. Nevertheless, empirical evidence suggests that computing and combining user-level intent might not always be necessary, as simpler approaches can yield comparable or superior results while keeping explicit semantic features. Under this perspective, user historical preferences become essential to refine the KG and retain the most discriminating features, thus leading to concise item representation. Driven by the assumptions above, we propose KGUF, a KGCF model that learns latent representations of semantic features in the KG to better define the item profile. By leveraging user profiles through decision trees, KGUF effectively retains only those features relevant to users. Results on three datasets justify KGUF's rationale, as our approach is able to reach performance comparable or superior to SOTA methods while maintaining a simpler formalization. Link to the repository: https://github.com/sisinflab/KGUF.","sentences":["The recent integration of Graph Neural Networks (GNNs) into recommendation has led to a novel family of Collaborative Filtering (CF) approaches, namely Graph Collaborative Filtering (GCF).","Following the same GNNs wave, recommender systems exploiting Knowledge Graphs (KGs) have also been successfully empowered by the GCF rationale to combine the representational power of GNNs with the semantics conveyed by KGs, giving rise to Knowledge-aware Graph Collaborative Filtering (KGCF), which use KGs to mine hidden user intent.","Nevertheless, empirical evidence suggests that computing and combining user-level intent might not always be necessary, as simpler approaches can yield comparable or superior results while keeping explicit semantic features.","Under this perspective, user historical preferences become essential to refine the KG and retain the most discriminating features, thus leading to concise item representation.","Driven by the assumptions above, we propose KGUF, a KGCF model that learns latent representations of semantic features in the KG to better define the item profile.","By leveraging user profiles through decision trees, KGUF effectively retains only those features relevant to users.","Results on three datasets justify KGUF's rationale, as our approach is able to reach performance comparable or superior to SOTA methods while maintaining a simpler formalization.","Link to the repository: https://github.com/sisinflab/KGUF."],"url":"http://arxiv.org/abs/2403.20095v1","category":"cs.IR"}
{"created":"2024-03-29 10:11:37","title":"Quantum trajectory of the one atom maser","abstract":"The evolution of a quantum system undergoing repeated indirect measurements naturally leads to a Markov chain on the set of states which is called a quantum trajectory. In this paper we consider a specific model of such a quantum trajectory associated to the one-atom maser model. It describes the evolution of one mode of the quantized electromagnetic field in a cavity interacting with two-level atoms. When the system is non-resonant we prove that this Markov chain admits a unique invariant probability measure. We moreover prove convergence in the Wasserstein metric towards this invariant measure. These results rely on a purification theorem: almost surely the state of the system approaches the set of pure states. Compared to similar results in the literature, the system considered here is infinite dimensional. While existence of an invariant measure is a consequence of the compactness of the set of states in finite dimension, in infinite dimension existence of an invariant measure is not free. Furthermore usual purification criterions in finite dimension have no straightforward equivalent in infinite dimension.","sentences":["The evolution of a quantum system undergoing repeated indirect measurements naturally leads to a Markov chain on the set of states which is called a quantum trajectory.","In this paper we consider a specific model of such a quantum trajectory associated to the one-atom maser model.","It describes the evolution of one mode of the quantized electromagnetic field in a cavity interacting with two-level atoms.","When the system is non-resonant we prove that this Markov chain admits a unique invariant probability measure.","We moreover prove convergence in the Wasserstein metric towards this invariant measure.","These results rely on a purification theorem: almost surely the state of the system approaches the set of pure states.","Compared to similar results in the literature, the system considered here is infinite dimensional.","While existence of an invariant measure is a consequence of the compactness of the set of states in finite dimension, in infinite dimension existence of an invariant measure is not free.","Furthermore usual purification criterions in finite dimension have no straightforward equivalent in infinite dimension."],"url":"http://arxiv.org/abs/2403.20094v1","category":"math-ph"}
{"created":"2024-03-29 09:58:58","title":"A Signature Based Approach Towards Global Channel Charting with Ultra Low Complexity","abstract":"Channel charting, an unsupervised learning method that learns a low-dimensional representation from channel information to preserve geometrical property of physical space of user equipments (UEs), has drawn many attentions from both academic and industrial communities, because it can facilitate many downstream tasks, such as indoor localization, UE handover, beam management, and so on. However, many previous works mainly focus on charting that only preserves local geometry and use raw channel information to learn the chart, which do not consider the global geometry and are often computationally intensive and very time-consuming. Therefore, in this paper, a novel signature based approach for global channel charting with ultra low complexity is proposed. By using an iterated-integral based method called signature transform, a compact feature map and a novel distance metric are proposed, which enable channel charting with ultra low complexity and preserving both local and global geometry. We demonstrate the efficacy of our method using synthetic and open-source real-field datasets.","sentences":["Channel charting, an unsupervised learning method that learns a low-dimensional representation from channel information to preserve geometrical property of physical space of user equipments (UEs), has drawn many attentions from both academic and industrial communities, because it can facilitate many downstream tasks, such as indoor localization, UE handover, beam management, and so on.","However, many previous works mainly focus on charting that only preserves local geometry and use raw channel information to learn the chart, which do not consider the global geometry and are often computationally intensive and very time-consuming.","Therefore, in this paper, a novel signature based approach for global channel charting with ultra low complexity is proposed.","By using an iterated-integral based method called signature transform, a compact feature map and a novel distance metric are proposed, which enable channel charting with ultra low complexity and preserving both local and global geometry.","We demonstrate the efficacy of our method using synthetic and open-source real-field datasets."],"url":"http://arxiv.org/abs/2403.20091v1","category":"cs.IT"}
{"created":"2024-03-29 09:52:18","title":"An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models","abstract":"The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \\textit{efficient} method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almost any language. We additionally use our modular approach to quantify negative interference efficiently and categorize languages accordingly. Furthermore, we provide a list of promising transfer-target language configurations that consistently lead to target language performance improvements. Code and data are publicly available: https://github.com/ffaisal93/neg_inf","sentences":["The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established.","However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs.","We propose an \\textit{efficient} method to study transfer language influence in zero-shot performance on another target language.","Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units.","Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages.","We find that no transfer language is beneficial for all target languages.","We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almost any language.","We additionally use our modular approach to quantify negative interference efficiently and categorize languages accordingly.","Furthermore, we provide a list of promising transfer-target language configurations that consistently lead to target language performance improvements.","Code and data are publicly available: https://github.com/ffaisal93/neg_inf"],"url":"http://arxiv.org/abs/2403.20088v1","category":"cs.CL"}
{"created":"2024-03-29 09:34:12","title":"OmniNxt: A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception","abstract":"Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics's capabilities in inspection, reconstruction, and rescue tasks. However, such sensors also elevate system complexity, e.g., hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research. To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception. We design a high-performance flight controller NxtPX4 and a multi-fisheye camera set for OmniNxt. Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy. We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications. All the hardware and software are open-access at https://github.com/HKUST-Aerial-Robotics/OmniNxt, and we provide docker images of each crucial module in the proposed system. Project page: https://hkust-aerial-robotics.github.io/OmniNxt.","sentences":["Adopting omnidirectional Field of View (FoV) cameras in aerial robots vastly improves perception ability, significantly advancing aerial robotics's capabilities in inspection, reconstruction, and rescue tasks.","However, such sensors also elevate system complexity, e.g., hardware design, and corresponding algorithm, which limits researchers from utilizing aerial robots with omnidirectional FoV in their research.","To bridge this gap, we propose OmniNxt, a fully open-source aerial robotics platform with omnidirectional perception.","We design a high-performance flight controller NxtPX4 and a multi-fisheye camera set for OmniNxt.","Meanwhile, the compatible software is carefully devised, which empowers OmniNxt to achieve accurate localization and real-time dense mapping with limited computation resource occupancy.","We conducted extensive real-world experiments to validate the superior performance of OmniNxt in practical applications.","All the hardware and software are open-access at https://github.com/HKUST-Aerial-Robotics/OmniNxt, and we provide docker images of each crucial module in the proposed system.","Project page: https://hkust-aerial-robotics.github.io/OmniNxt."],"url":"http://arxiv.org/abs/2403.20085v1","category":"cs.RO"}
{"created":"2024-03-29 09:17:46","title":"Principle of virtual action in continuum mechanics","abstract":"We present the principle of virtual action as a foundation of continuum mechanics. Used mainly in relativity, the method has a useful application in classical mechanics and places the notion of action as the basic concept of dynamics. The principle is an extension of virtual work to space-time. It extends the efforts made by d'Alembert and Lagrange. Unlike the classical case of equilibrium, the principle of virtual action becomes a postulate for the formulation of models in dynamics. It allows to use a minimal set of clear conjectures and is extended to the case of media with dissipation; it can be used for more complex systems.","sentences":["We present the principle of virtual action as a foundation of continuum mechanics.","Used mainly in relativity, the method has a useful application in classical mechanics and places the notion of action as the basic concept of dynamics.","The principle is an extension of virtual work to space-time.","It extends the efforts made by d'Alembert and Lagrange.","Unlike the classical case of equilibrium, the principle of virtual action becomes a postulate for the formulation of models in dynamics.","It allows to use a minimal set of clear conjectures and is extended to the case of media with dissipation; it can be used for more complex systems."],"url":"http://arxiv.org/abs/2403.20076v1","category":"physics.class-ph"}
{"created":"2024-03-29 09:17:40","title":"Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks","abstract":"In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derived convergence bound reveals the impact of the rounds of local training on the model performance. Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained. Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different graph-based aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios. Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed. Simulation results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes.","sentences":["In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern.","To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative.","Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated.","Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints.","The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets.","To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training.","The derived convergence bound reveals the impact of the rounds of local training on the model performance.","Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained.","Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different graph-based aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios.","Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed.","Simulation results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes."],"url":"http://arxiv.org/abs/2403.20075v1","category":"cs.LG"}
{"created":"2024-03-29 09:03:55","title":"An enthalpy-based model for the physics of ice crystal icing","abstract":"Ice crystal icing (ICI) in aircraft engines is a major threat to flight safety. Due to the complex thermodynamic and phase-change conditions involved in ICI, rigorous modelling of the accretion process remains limited. The present study proposes a novel modelling approach based on the physically-observed mixed-phase nature of the accretion layers. The mathematical model, which is derived from the enthalpy change after accretion (the enthalpy model), is compared to an existing pure-phase layer model (the three-layer model). Scaling laws and asymptotic solutions are developed for both models. The onset of ice accretion, the icing layer thickness, and solid ice fraction within the layer are determined by a set of non-dimensional parameters including the Peclet number, the Stefan number, the Biot number, the Melt Ratio, and the evaporative rate. Thresholds for freezing and non-freezing conditions are developed. The asymptotic solutions presents good agreement with numerical solutions at low Peclet numbers. Both the asymptotic and numerical solutions show that, when compared to the three-layer model, the enthalpy model presents a thicker icing layer and a thicker water layer above the substrate due to mixed-phased features and modified Stefan conditions. Modelling in terms of the enthalpy poses significant advantages in the development of numerical methods to complex three-dimensional geometrical and flow configurations. These results improve understanding of the accretion process and provide a novel, rigorous mathematical framework for accurate modelling of ICI.","sentences":["Ice crystal icing (ICI) in aircraft engines is a major threat to flight safety.","Due to the complex thermodynamic and phase-change conditions involved in ICI, rigorous modelling of the accretion process remains limited.","The present study proposes a novel modelling approach based on the physically-observed mixed-phase nature of the accretion layers.","The mathematical model, which is derived from the enthalpy change after accretion (the enthalpy model), is compared to an existing pure-phase layer model (the three-layer model).","Scaling laws and asymptotic solutions are developed for both models.","The onset of ice accretion, the icing layer thickness, and solid ice fraction within the layer are determined by a set of non-dimensional parameters including the Peclet number, the Stefan number, the Biot number, the Melt Ratio, and the evaporative rate.","Thresholds for freezing and non-freezing conditions are developed.","The asymptotic solutions presents good agreement with numerical solutions at low Peclet numbers.","Both the asymptotic and numerical solutions show that, when compared to the three-layer model, the enthalpy model presents a thicker icing layer and a thicker water layer above the substrate due to mixed-phased features and modified Stefan conditions.","Modelling in terms of the enthalpy poses significant advantages in the development of numerical methods to complex three-dimensional geometrical and flow configurations.","These results improve understanding of the accretion process and provide a novel, rigorous mathematical framework for accurate modelling of ICI."],"url":"http://arxiv.org/abs/2403.20071v1","category":"physics.flu-dyn"}
{"created":"2024-03-29 09:00:59","title":"Efficacy of the Sterile Insect Technique in the presence of inaccessible areas: A study using two-patch models","abstract":"The Sterile Insect Technique (SIT) is one of the sustainable strategies for the control of disease vectors, which consists of releasing sterilized males that will mate with the wild females, resulting in a reduction and, eventually a local elimination, of the wild population. The implementation of the SIT in the field can become problematic when there are inaccessible areas where the release of sterile insects cannot be carried out directly, and the migration of wild insects from these areas to the treated zone may influence the efficacy of this technique. However, we can also take advantage of the movement of sterile individuals to control the wild population in these unreachable places. In this paper, we derive a two-patch model for Aedes mosquitoes where we consider the discrete diffusion between the treated area and the inaccessible zone. We investigate two different release strategies (constant and impulsive periodic releases), and by using the monotonicity of the model, we show that if the number of released sterile males exceeds some threshold, the technique succeeds in driving the whole population in both areas to extinction. This threshold depends on not only the biological parameters of the population but also the diffusion between the two patches.","sentences":["The Sterile Insect Technique (SIT) is one of the sustainable strategies for the control of disease vectors, which consists of releasing sterilized males that will mate with the wild females, resulting in a reduction and, eventually a local elimination, of the wild population.","The implementation of the SIT in the field can become problematic when there are inaccessible areas where the release of sterile insects cannot be carried out directly, and the migration of wild insects from these areas to the treated zone may influence the efficacy of this technique.","However, we can also take advantage of the movement of sterile individuals to control the wild population in these unreachable places.","In this paper, we derive a two-patch model for Aedes mosquitoes where we consider the discrete diffusion between the treated area and the inaccessible zone.","We investigate two different release strategies (constant and impulsive periodic releases), and by using the monotonicity of the model, we show that if the number of released sterile males exceeds some threshold, the technique succeeds in driving the whole population in both areas to extinction.","This threshold depends on not only the biological parameters of the population but also the diffusion between the two patches."],"url":"http://arxiv.org/abs/2403.20069v1","category":"math.DS"}
{"created":"2024-03-29 08:55:24","title":"Dataversifying Natural Sciences: Pioneering a Data Lake Architecture for Curated Data-Centric Experiments in Life \\& Earth Sciences","abstract":"This vision paper introduces a pioneering data lake architecture designed to meet Life \\& Earth sciences' burgeoning data management needs. As the data landscape evolves, the imperative to navigate and maximize scientific opportunities has never been greater. Our vision paper outlines a strategic approach to unify and integrate diverse datasets, aiming to cultivate a collaborative space conducive to scientific discovery.The core of the design and construction of a data lake is the development of formal and semi-automatic tools, enabling the meticulous curation of quantitative and qualitative data from experiments. Our unique ''research-in-the-loop'' methodology ensures that scientists across various disciplines are integrally involved in the curation process, combining automated, mathematical, and manual tasks to address complex problems, from seismic detection to biodiversity studies. By fostering reproducibility and applicability of research, our approach enhances the integrity and impact of scientific experiments. This initiative is set to improve data management practices, strengthening the capacity of Life \\& Earth sciences to solve some of our time's most critical environmental and biological challenges.","sentences":["This vision paper introduces a pioneering data lake architecture designed to meet Life \\& Earth sciences' burgeoning data management needs.","As the data landscape evolves, the imperative to navigate and maximize scientific opportunities has never been greater.","Our vision paper outlines a strategic approach to unify and integrate diverse datasets, aiming to cultivate a collaborative space conducive to scientific discovery.","The core of the design and construction of a data lake is the development of formal and semi-automatic tools, enabling the meticulous curation of quantitative and qualitative data from experiments.","Our unique ''research-in-the-loop'' methodology ensures that scientists across various disciplines are integrally involved in the curation process, combining automated, mathematical, and manual tasks to address complex problems, from seismic detection to biodiversity studies.","By fostering reproducibility and applicability of research, our approach enhances the integrity and impact of scientific experiments.","This initiative is set to improve data management practices, strengthening the capacity of Life \\& Earth sciences to solve some of our time's most critical environmental and biological challenges."],"url":"http://arxiv.org/abs/2403.20063v1","category":"cs.DB"}
{"created":"2024-03-29 08:47:37","title":"Bias versus variance when fitting multi-species molecular lines with a non-LTE radiative transfer model","abstract":"Robust radiative transfer techniques are requisite for efficiently extracting the physical and chemical information from molecular rotational lines.We study several hypotheses that enable robust estimations of the column densities and physical conditions when fitting one or two transitions per molecular species. We study the extent to which simplifying assumptions aimed at reducing the complexity of the problem introduce estimation biases and how to detect them.We focus on the CO and HCO+ isotopologues and analyze maps of a 50 square arcminutes field. We used the RADEX escape probability model to solve the statistical equilibrium equations and compute the emerging line profiles, assuming that all species coexist. Depending on the considered set of species, we also fixed the abundance ratio between some species and explored different values. We proposed a maximum likelihood estimator to infer the physical conditions and considered the effect of both the thermal noise and calibration uncertainty. We analyzed any potential biases induced by model misspecifications by comparing the results on the actual data for several sets of species and confirmed with Monte Carlo simulations. The variance of the estimations and the efficiency of the estimator were studied based on the Cram{\\'e}r-Rao lower bound.Column densities can be estimated with 30% accuracy, while the best estimations of the volume density are found to be within a factor of two. Under the chosen model framework, the peak 12CO(1--0) is useful for constraining the kinetic temperature. The thermal pressure is better and more robustly estimated than the volume density and kinetic temperature separately. Analyzing CO and HCO+ isotopologues and fitting the full line profile are recommended practices with respect to detecting possible biases.Combining a non-local thermodynamic equilibrium model with a rigorous analysis of the accuracy allows us to obtain an efficient estimator and identify where the model is misspecified. We note that other combinations of molecular lines could be studied in the future.","sentences":["Robust radiative transfer techniques are requisite for efficiently extracting the physical and chemical information from molecular rotational lines.","We study several hypotheses that enable robust estimations of the column densities and physical conditions when fitting one or two transitions per molecular species.","We study the extent to which simplifying assumptions aimed at reducing the complexity of the problem introduce estimation biases and how to detect them.","We focus on the CO and HCO+ isotopologues and analyze maps of a 50 square arcminutes field.","We used the RADEX escape probability model to solve the statistical equilibrium equations and compute the emerging line profiles, assuming that all species coexist.","Depending on the considered set of species, we also fixed the abundance ratio between some species and explored different values.","We proposed a maximum likelihood estimator to infer the physical conditions and considered the effect of both the thermal noise and calibration uncertainty.","We analyzed any potential biases induced by model misspecifications by comparing the results on the actual data for several sets of species and confirmed with Monte Carlo simulations.","The variance of the estimations and the efficiency of the estimator were studied based on the Cram{\\'e}r-Rao lower bound.","Column densities can be estimated with 30% accuracy, while the best estimations of the volume density are found to be within a factor of two.","Under the chosen model framework, the peak 12CO(1--0) is useful for constraining the kinetic temperature.","The thermal pressure is better and more robustly estimated than the volume density and kinetic temperature separately.","Analyzing CO and HCO+ isotopologues and fitting the full line profile are recommended practices with respect to detecting possible biases.","Combining a non-local thermodynamic equilibrium model with a rigorous analysis of the accuracy allows us to obtain an efficient estimator and identify where the model is misspecified.","We note that other combinations of molecular lines could be studied in the future."],"url":"http://arxiv.org/abs/2403.20057v1","category":"astro-ph.GA"}
{"created":"2024-03-29 08:45:08","title":"A note on Sarnak processes","abstract":"Basic properties of stationary processes called Sarnak processes are studied. As an application, a combinatorial reformulation of Sarnak's conjecture on M{\\\"o}bius orthogonality is provided.","sentences":["Basic properties of stationary processes called Sarnak processes are studied.","As an application, a combinatorial reformulation of Sarnak's conjecture on M{\\\"o}bius orthogonality is provided."],"url":"http://arxiv.org/abs/2403.20054v1","category":"math.DS"}
{"created":"2024-03-29 08:43:14","title":"Prospects for non-linear memristors as so-far missing core hardware element for transferless data computing and storage","abstract":"We like and need Information and Communications Technologies (ICT) for data processing. This is measureable in the exponential growth of data processed by ICT, e.g. ICT for cryptocurrency mining and search engines. So far, the energy demand for computing technology has increased by a factor of 1.38 every ten years due to the exponentially increasing use of ICT systems as computing devices. The energy consumption of ICT systems is expected to rise from 1500 TWh (8% of global electricity consumption) in 2010 to 5700 TWh (14% of global electricity consumption) in 2030. A large part of this energy is required for the continuous data transfer between the separated memory and processor units which constitute the main components of ICT computing devices in von-Neumann architecture. This at the same time massively slows down the computing power of ICT systems in the von-Neumann architecture. In addition, due to the increasing complexity of AI compute algorithms, since 2010 the AI training compute time demand for computing technology increases tenfold every year, for example in the period from 2010 to 2020 from 1x10^{-6} to 1x10^{+4} Petaflops/Day. It has been theoretically predicted that ICT systems in the neuromorphic computer architecture will circumvent all of this through the use of merged memory and processor units. However, the core hardware element for this has not yet been realized so far. In this work we discuss the prespectives for non-linear resistive switches as the core hardware element for merged memory and processor units in neuromorphic computers.","sentences":["We like and need Information and Communications Technologies (ICT) for data processing.","This is measureable in the exponential growth of data processed by ICT, e.g. ICT for cryptocurrency mining and search engines.","So far, the energy demand for computing technology has increased by a factor of 1.38 every ten years due to the exponentially increasing use of ICT systems as computing devices.","The energy consumption of ICT systems is expected to rise from 1500 TWh (8% of global electricity consumption) in 2010 to 5700 TWh (14% of global electricity consumption) in 2030.","A large part of this energy is required for the continuous data transfer between the separated memory and processor units which constitute the main components of ICT computing devices in von-Neumann architecture.","This at the same time massively slows down the computing power of ICT systems in the von-Neumann architecture.","In addition, due to the increasing complexity of AI compute algorithms, since 2010 the AI training compute time demand for computing technology increases tenfold every year, for example in the period from 2010 to 2020 from 1x10^{-6} to 1x10^{+4} Petaflops/Day.","It has been theoretically predicted that ICT systems in the neuromorphic computer architecture will circumvent all of this through the use of merged memory and processor units.","However, the core hardware element for this has not yet been realized so far.","In this work we discuss the prespectives for non-linear resistive switches as the core hardware element for merged memory and processor units in neuromorphic computers."],"url":"http://arxiv.org/abs/2403.20051v1","category":"cs.ET"}
{"created":"2024-03-29 08:42:40","title":"Hot-LEGO: Architect Microfluidic Cooling Equipped 3DICs with Pre-RTL Thermal Simulation","abstract":"Microfluidic cooling has been recognized as one of the most promising solutions to achieve efficient thermal management for three-dimensional integrated circuits (3DICs). It enables more opportunities to architect 3DICs with different die configurations. It becomes increasingly important to perform thermal analysis in the early design phases to validate the architectural design decisions. This is even more critical for microfluidic cooling equipped 3DICs as the embedded cooling structures greatly influence the performance, power, and reliability of the stacked system. We exploited the existing architectural simulators and developed a Pre-register-transfer-level (Pre-RTL) thermal simulation methodology named Hot-LEGO that integrates these tools with their latest features such as support for microfluidic cooling and 3DIC stacking configurations. This methodology differs from existing ones by looking into the design granularity at a much finer level which enables the exploration of unique architecture combinations across the vertical stack. Though architectural-level simulators are not designed for signoff-calibre, it offers speed and agility which are imperative for early design space exploration. We claim that this ongoing work will speed up the co-design cycle of microfluidic cooling and offer a portable methodology for architects to perform exhaustive search for the optimal microarchitecture solutions in 3DICs.","sentences":["Microfluidic cooling has been recognized as one of the most promising solutions to achieve efficient thermal management for three-dimensional integrated circuits (3DICs).","It enables more opportunities to architect 3DICs with different die configurations.","It becomes increasingly important to perform thermal analysis in the early design phases to validate the architectural design decisions.","This is even more critical for microfluidic cooling equipped 3DICs as the embedded cooling structures greatly influence the performance, power, and reliability of the stacked system.","We exploited the existing architectural simulators and developed a Pre-register-transfer-level (Pre-RTL) thermal simulation methodology named Hot-LEGO that integrates these tools with their latest features such as support for microfluidic cooling and 3DIC stacking configurations.","This methodology differs from existing ones by looking into the design granularity at a much finer level which enables the exploration of unique architecture combinations across the vertical stack.","Though architectural-level simulators are not designed for signoff-calibre, it offers speed and agility which are imperative for early design space exploration.","We claim that this ongoing work will speed up the co-design cycle of microfluidic cooling and offer a portable methodology for architects to perform exhaustive search for the optimal microarchitecture solutions in 3DICs."],"url":"http://arxiv.org/abs/2403.20050v1","category":"cs.AR"}
{"created":"2024-03-29 08:29:32","title":"Blockchain for Energy Market: A Comprehensive Survey","abstract":"The energy market encompasses the behavior of energy supply and trading within a platform system. By utilizing centralized or distributed trading, energy can be effectively managed and distributed across different regions, thereby achieving market equilibrium and satisfying both producers and consumers. However, recent years have presented unprecedented challenges and difficulties for the development of the energy market. These challenges include regional energy imbalances, volatile energy pricing, high computing costs, and issues related to transaction information disclosure. Researchers widely acknowledge that the security features of blockchain technology can enhance the efficiency of energy transactions and establish the fundamental stability and robustness of the energy market. This type of blockchain-enabled energy market is commonly referred to as an energy blockchain. Currently, there is a burgeoning amount of research in this field, encompassing algorithm design, framework construction, and practical application. It is crucial to organize and compare these research efforts to facilitate the further advancement of energy blockchain. This survey aims to comprehensively review the fundamental characteristics of blockchain and energy markets, highlighting the significant advantages of combining the two. Moreover, based on existing research outcomes, we will categorize and compare the current energy market research supported by blockchain in terms of algorithm design, market framework construction, and the policies and practical applications adopted by different countries. Finally, we will address current issues and propose potential future directions for improvement, to provide guidance for the practical implementation of blockchain in the energy market.","sentences":["The energy market encompasses the behavior of energy supply and trading within a platform system.","By utilizing centralized or distributed trading, energy can be effectively managed and distributed across different regions, thereby achieving market equilibrium and satisfying both producers and consumers.","However, recent years have presented unprecedented challenges and difficulties for the development of the energy market.","These challenges include regional energy imbalances, volatile energy pricing, high computing costs, and issues related to transaction information disclosure.","Researchers widely acknowledge that the security features of blockchain technology can enhance the efficiency of energy transactions and establish the fundamental stability and robustness of the energy market.","This type of blockchain-enabled energy market is commonly referred to as an energy blockchain.","Currently, there is a burgeoning amount of research in this field, encompassing algorithm design, framework construction, and practical application.","It is crucial to organize and compare these research efforts to facilitate the further advancement of energy blockchain.","This survey aims to comprehensively review the fundamental characteristics of blockchain and energy markets, highlighting the significant advantages of combining the two.","Moreover, based on existing research outcomes, we will categorize and compare the current energy market research supported by blockchain in terms of algorithm design, market framework construction, and the policies and practical applications adopted by different countries.","Finally, we will address current issues and propose potential future directions for improvement, to provide guidance for the practical implementation of blockchain in the energy market."],"url":"http://arxiv.org/abs/2403.20045v1","category":"cs.NI"}
{"created":"2024-03-29 08:24:47","title":"Rise and fall of a multicomponent droplet in a surrounrdfing fluid: simulation study of a bumpy path","abstract":"The coupling between mass transfer and hydrodynamic phenomena in two-phase flow is not necessarily straightforward due to the different effects that can be encountered. The treatment of such coupling is complex and requires particular efforts, especially in the modelling of the interface between phases. In this paper, we consider the case of a droplet composed of two components (one miscible and one immiscible in water) released in a 2D rectangular domain filled with water. Mass transfer occurs between the miscible element and the surrounding water, which leads to a density inversion that directly affects the droplet trajectory through buoyancy. We perform simulations using a ternary Cahn-Hilliard model (implemented in the \"phase\\_field\" model of the TrioCFD code) to capture such coupled phenomena. The Boussinesq approximation for a multicomponent system is used to define the density law and an analytical chemical potential is proposed for the thermodynamic landscape. The effect of the mobility parameter on the flow is highlighted and the results found are in good agreement with the dynamics described from an experimental study of the open literature.","sentences":["The coupling between mass transfer and hydrodynamic phenomena in two-phase flow is not necessarily straightforward due to the different effects that can be encountered.","The treatment of such coupling is complex and requires particular efforts, especially in the modelling of the interface between phases.","In this paper, we consider the case of a droplet composed of two components (one miscible and one immiscible in water) released in a 2D rectangular domain filled with water.","Mass transfer occurs between the miscible element and the surrounding water, which leads to a density inversion that directly affects the droplet trajectory through buoyancy.","We perform simulations using a ternary Cahn-Hilliard model (implemented in the \"phase\\_field\" model of the TrioCFD code) to capture such coupled phenomena.","The Boussinesq approximation for a multicomponent system is used to define the density law and an analytical chemical potential is proposed for the thermodynamic landscape.","The effect of the mobility parameter on the flow is highlighted and the results found are in good agreement with the dynamics described from an experimental study of the open literature."],"url":"http://arxiv.org/abs/2403.20040v1","category":"physics.flu-dyn"}
{"created":"2024-03-29 17:51:50","title":"Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer","abstract":"Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namely, divergent and convergent. These paradigms evaluate outward and inward effective connections, respectively. Our findings reveal a notable improvement in moving from a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666), marking the first application of the latter in this context. Secondly, we demonstrate the efficacy of the Transformer models in handling heterogeneous electrode placements, increasing the AUROC to 0.730. Lastly, by incorporating inter-trial variability, we further refine the Transformer models, with an AUROC of 0.745, yielding more consistent predictions across patients. These advancements provide a deeper insight into SOZ localisation and represent a significant step in modelling patient-specific intracranial EEG electrode placements in SPES. Future work will explore integrating these models into clinical decision-making processes to bridge the gap between deep learning research and practical healthcare applications.","sentences":["Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures.","For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge.","Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas.","This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses.","We achieve this by introducing Transformer models that incorporate cross-channel attention.","We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   ","Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namely, divergent and convergent.","These paradigms evaluate outward and inward effective connections, respectively.","Our findings reveal a notable improvement in moving from a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666), marking the first application of the latter in this context.","Secondly, we demonstrate the efficacy of the Transformer models in handling heterogeneous electrode placements, increasing the AUROC to 0.730.","Lastly, by incorporating inter-trial variability, we further refine the Transformer models, with an AUROC of 0.745, yielding more consistent predictions across patients.","These advancements provide a deeper insight into SOZ localisation and represent a significant step in modelling patient-specific intracranial EEG electrode placements in SPES.","Future work will explore integrating these models into clinical decision-making processes to bridge the gap between deep learning research and practical healthcare applications."],"url":"http://arxiv.org/abs/2403.20324v1","category":"cs.LG"}
{"created":"2024-03-29 17:04:23","title":"Probing Heavy Charged Higgs Boson Using Multivariate Technique at Gamma-Gamma Collider","abstract":"The current study explores the production of charged Higgs particles through photon-photon collisions within the Two Higgs Doublet Model context, including one-loop-level scattering amplitude of Electroweak and QED radiation. The cross-section has been scanned for plane ($m_{\\phi^{0}}, \\sqrt{s}$) investigating the process of $\\gamma\\gamma \\rightarrow H^{+}H^{-}$. Three particular numerical scenarios low-$m_{H}$, non-alignment, and short-cascade are employed. Hence using $h^{0}$ for low-$m_{H^{0}}$ and $H^{0}$ for non-alignment and short-cascade scenario, the new experimental and theoretical constraints are applied.The decay channels for charged Higgs particles are examined in all the scenarios along with the analysis for cross-sections revealing that at low energy it is consistently higher for all scenarios. However as $\\sqrt{s}$ increases, it reaches a peak value at 1$~$TeV for all benchmark scenarios. The branching ratio of the decay channels indicates that for non-alignment, the mode of decay $W^{\\pm} h^{0}$ takes control %{} when $BR(H^{\\pm} \\rightarrow W^{\\pm} H^{0})$ decreases at larger values of $m_{H^{0}}$.} and for short cascade the prominent decay mode remains $t\\bar{b}$, while in the low-$m_{H}$ the dominant decay channel is of $W^{\\pm} h^{0}$.   In our research, we employ contemporary machine-learning methodologies to investigate the production of high-energy Higgs Bosons within a 3$ $TeV Gamma-Gamma collider. We have used multivariate approaches such as Boosted Decision Trees (BDT), LikelihoodD, and Multilayer Perceptron (MLP) to show the observability of heavy-charged Higgs Bosons versus the most significant Standard Model backgrounds. The purity of the signal efficiency and background rejection are measured for each cut value.","sentences":["The current study explores the production of charged Higgs particles through photon-photon collisions within the Two Higgs Doublet Model context, including one-loop-level scattering amplitude of Electroweak and QED radiation.","The cross-section has been scanned for plane ($m_{\\phi^{0}}, \\sqrt{s}$) investigating the process of $\\gamma\\gamma \\rightarrow H^{+}H^{-}$. Three particular numerical scenarios low-$m_{H}$, non-alignment, and short-cascade are employed.","Hence using $h^{0}$ for low-$m_{H^{0}}$ and $H^{0}$ for non-alignment and short-cascade scenario, the new experimental and theoretical constraints are applied.","The decay channels for charged Higgs particles are examined in all the scenarios along with the analysis for cross-sections revealing that at low energy it is consistently higher for all scenarios.","However as $\\sqrt{s}$ increases, it reaches a peak value at 1$~$TeV for all benchmark scenarios.","The branching ratio of the decay channels indicates that for non-alignment, the mode of decay $W^{\\pm} h^{0}$ takes control %{} when $BR(H^{\\pm} \\rightarrow W^{\\pm} H^{0})$ decreases at larger values of $m_{H^{0}}$.} and for short cascade the prominent decay mode remains $t\\bar{b}$, while in the low-$m_{H}$ the dominant decay channel is of $W^{\\pm} h^{0}$.   In our research, we employ contemporary machine-learning methodologies to investigate the production of high-energy Higgs Bosons within a 3$ $TeV Gamma-Gamma collider.","We have used multivariate approaches such as Boosted Decision Trees (BDT), LikelihoodD, and Multilayer Perceptron (MLP) to show the observability of heavy-charged Higgs Bosons versus the most significant Standard Model backgrounds.","The purity of the signal efficiency and background rejection are measured for each cut value."],"url":"http://arxiv.org/abs/2403.20293v1","category":"hep-ph"}
{"created":"2024-03-29 17:04:12","title":"Further remarks on absorbing Markov decision processes","abstract":"In this note, based on the recent remarkable results of Dufour and Prieto-Rumeau, we deduce that for an absorbing MDP with a given initial state, under a standard compactness-continuity condition, the space of occupation measures has the same convergent sequences, when it is endowed with the weak topology and with the weak-strong topology. We provided two examples demonstrating that imposed condition cannot be replaced with its popular alternative, and the above assertion does not hold for the space of marginals of occupation measures on the state space. Moreover, the examples also clarify some results in the previous literature.","sentences":["In this note, based on the recent remarkable results of Dufour and Prieto-Rumeau, we deduce that for an absorbing MDP with a given initial state, under a standard compactness-continuity condition, the space of occupation measures has the same convergent sequences, when it is endowed with the weak topology and with the weak-strong topology.","We provided two examples demonstrating that imposed condition cannot be replaced with its popular alternative, and the above assertion does not hold for the space of marginals of occupation measures on the state space.","Moreover, the examples also clarify some results in the previous literature."],"url":"http://arxiv.org/abs/2403.20292v1","category":"math.OC"}
{"created":"2024-03-29 17:04:03","title":"A catalogue of asteroseismically calibrated ages for APOGEE DR17. The predictions of a CatBoost machine learning model based on the [Mg/Ce] chemical clock and other stellar parameters","abstract":"Context. Understanding the Milky Way's formation and evolution across cosmic epochs necessitates precise stellar age determination across all Galactic components. Recent advancements in asteroseismology, spectroscopy, stellar modelling, and machine learning, coupled with all-sky surveys, now offer highly reliable stellar age estimates. Aims. This study aims to furnish accurate age assessments for the Main Red Star Sample within the APOGEE DR17 catalogue. Leveraging asteroseismic age constraints, we employ machine learning to achieve this goal. Methods. We explore optimal non-asteroseismic stellar parameters, including T$_{eff}$, L, [CI/N], [Mg/Ce], [$\\alpha$/Fe], U(LSR) velocity, and 'Z' vertical height from the Galactic plane, to predict ages via categorical gradient boost decision trees. Training merges samples from the TESS Southern Continuous Viewing Zone and Second APOKASC catalogue to mitigate data shifts, enhancing prediction reliability. Validation employs an independent dataset from the K2 Galactic Archaeology Program. Results. Our model yields a median fractional age error of 20.8%, with a prediction variance of 4.77%. Median fractional errors for stars older than 3 Gyr range from 7% to 23%, from 1 to 3 Gyr range from 26% to 28%, and for stars younger than 1 Gyr, it's 43%. Applicable to 125,445 stars in the APOGEE DR17 Main Red Star Sample, our analysis confirms previous findings on the young Galactic disc's flaring and reveals an age gradient among the youngest Galactic plane stars. Additionally, we identify two groups of metal-poor ([Fe/H] < -1 dex) young stars (Age < 2 Gyr) exhibiting similar chemical abundances and halo kinematics, likely remnants of the predicted third gas infall episode (~2.7 Gyr ago).","sentences":["Context.","Understanding the Milky Way's formation and evolution across cosmic epochs necessitates precise stellar age determination across all Galactic components.","Recent advancements in asteroseismology, spectroscopy, stellar modelling, and machine learning, coupled with all-sky surveys, now offer highly reliable stellar age estimates.","Aims.","This study aims to furnish accurate age assessments for the Main Red Star Sample within the APOGEE DR17 catalogue.","Leveraging asteroseismic age constraints, we employ machine learning to achieve this goal.","Methods.","We explore optimal non-asteroseismic stellar parameters, including T$_{eff}$, L, [CI/N], [Mg/Ce], [$\\alpha$/Fe], U(LSR) velocity, and 'Z' vertical height from the Galactic plane, to predict ages via categorical gradient boost decision trees.","Training merges samples from the TESS Southern Continuous Viewing Zone and Second APOKASC catalogue to mitigate data shifts, enhancing prediction reliability.","Validation employs an independent dataset from the K2 Galactic Archaeology Program.","Results.","Our model yields a median fractional age error of 20.8%, with a prediction variance of 4.77%.","Median fractional errors for stars older than 3 Gyr range from 7% to 23%, from 1 to 3 Gyr range from 26% to 28%, and for stars younger than 1 Gyr, it's 43%.","Applicable to 125,445 stars in the APOGEE DR17 Main Red Star Sample, our analysis confirms previous findings on the young Galactic disc's flaring and reveals an age gradient among the youngest Galactic plane stars.","Additionally, we identify two groups of metal-poor ([Fe/H] < -1 dex) young stars (Age < 2 Gyr) exhibiting similar chemical abundances and halo kinematics, likely remnants of the predicted third gas infall episode (~2.7 Gyr ago)."],"url":"http://arxiv.org/abs/2403.20291v1","category":"astro-ph.GA"}
{"created":"2024-03-29 16:48:59","title":"A realistic theory of $\\mathrm{E_{6}}$ unification through novel intermediate symmetries","abstract":"We propose a non-supersymmetric $\\mathrm{E}_{6}$ GUT with the scalar sector consisting of $\\mathbf{650}\\oplus \\mathbf{351'} \\oplus \\mathbf{27}$. Making use of the first representation for the initial symmetry breaking to an intermediate stage, and the latter two representations for second-stage breaking to the Standard Model and a realistic Yukawa sector, this theory represents the minimal $\\mathrm{E}_{6}$ GUT that proceeds through one of the intermediate stages that are novel compared to $\\mathrm{SU(5)}$ or $\\mathrm{SO}(10)$ GUT: trinification $\\mathrm{SU}(3)_C\\times \\mathrm{SU}(3)_L\\times \\mathrm{SU}(3)_R$, $\\mathrm{SU}(6)\\times \\mathrm{SU}(2)$ and flipped $\\mathrm{SO}(10)\\times\\mathrm{U}(1)$. We analyze these possibilities under the choice of vacuum that preserves a $\\mathbb{Z}_{2}$ ``spinorial parity'', which disentangles the chiral and vector-like fermions of $\\mathrm{E}_{6}$ and provides a dark matter candidate in the form of a (scalar) inert doublet. Three cases are shown to consistently unify under the extended survival hypothesis (with minimal fine-tuning): trinification symmetry $\\mathrm{SU}(3)_C\\times \\mathrm{SU}(3)_L\\times \\mathrm{SU}(3)_R$ with either $LR$ or $CR$ parity, and $\\mathrm{SU}(6)_{CR}\\times\\mathrm{SU}(2)_L$. Although the successful cases give a large range for proton lifetime estimates, all of them include regions consistent with current experimental bounds and within reach of forthcoming experiments. The scenario investigated in this paper essentially represents the unique (potentially) viable choice in the class of $\\mathrm{E}_{6}$ GUTs proceeding through a novel-symmetry intermediate stage, since non-minimal alternatives seem to be intrinsically non-perturbative.","sentences":["We propose a non-supersymmetric $\\mathrm{E}_{6}$ GUT with the scalar sector consisting of $\\mathbf{650}\\oplus \\mathbf{351'} \\oplus \\mathbf{27}$. Making use of the first representation for the initial symmetry breaking to an intermediate stage, and the latter two representations for second-stage breaking to the Standard Model and a realistic Yukawa sector, this theory represents the minimal $\\mathrm{E}_{6}$ GUT that proceeds through one of the intermediate stages that are novel compared to $\\mathrm{SU(5)}$ or $\\mathrm{SO}(10)$ GUT: trinification $\\mathrm{SU}(3)_C\\times \\mathrm{SU}(3)_L\\times \\mathrm{SU}(3)_R$, $\\mathrm{SU}(6)\\times \\mathrm{SU}(2)$ and flipped $\\mathrm{SO}(10)\\times\\mathrm{U}(1)$. We analyze these possibilities under the choice of vacuum that preserves a $\\mathbb{Z}_{2}$ ``spinorial parity'', which disentangles the chiral and vector-like fermions of $\\mathrm{E}_{6}$ and provides a dark matter candidate in the form of a (scalar) inert doublet.","Three cases are shown to consistently unify under the extended survival hypothesis (with minimal fine-tuning): trinification symmetry $\\mathrm{SU}(3)_C\\times \\mathrm{SU}(3)_L\\times \\mathrm{SU}(3)_R$ with either $LR$ or $CR$ parity, and $\\mathrm{SU}(6)_{CR}\\times\\mathrm{SU}(2)_L$. Although the successful cases give a large range for proton lifetime estimates, all of them include regions consistent with current experimental bounds and within reach of forthcoming experiments.","The scenario investigated in this paper essentially represents the unique (potentially) viable choice in the class of $\\mathrm{E}_{6}$ GUTs proceeding through a novel-symmetry intermediate stage, since non-minimal alternatives seem to be intrinsically non-perturbative."],"url":"http://arxiv.org/abs/2403.20278v1","category":"hep-ph"}
{"created":"2024-03-29 16:14:44","title":"Probing Dark Matter Particles from Evaporating Primordial Black Holes via Electron Scattering in the CDEX-10 Experiment","abstract":"Dark matter (DM) is a major constituent of the Universe. However, no definite evidence of DM particles (denoted as ``$\\chi$\") has been found in DM direct detection (DD) experiments to date. There is a novel concept that detecting $\\chi$ from evaporating primordial black holes (PBHs). We search for $\\chi$ emitted from PBHs by investigating their interaction with target electrons. The examined PBH masses range from 1$\\times$10$^{15}$ to 7$\\times$10$^{16}$ g under the current limits of PBH abundance $f_{PBH}$. Using 205.4 kg$\\cdot$day data obtained from the CDEX-10 experiment conducted in the China Jinping Underground Laboratory, we exclude the $\\chi$--electron ($\\chi$--$e$) elastic-scattering cross section $\\sigma_{\\chi e} \\sim 5\\times10^{-29}$ cm$^2$ for $\\chi$ with a mass $m_{\\chi}\\lesssim$ 0.1 keV from our results. If ($m_{\\chi}$, $\\sigma_{\\chi e}$) can be determined in the future, DD experiments are expected to impose strong constraints on $f_{PBH}$ for large $M_{PBH}$s.","sentences":["Dark matter (DM) is a major constituent of the Universe.","However, no definite evidence of DM particles (denoted as ``$\\chi$\") has been found in DM direct detection (DD) experiments to date.","There is a novel concept that detecting $\\chi$ from evaporating primordial black holes (PBHs).","We search for $\\chi$ emitted from PBHs by investigating their interaction with target electrons.","The examined PBH masses range from 1$\\times$10$^{15}$ to 7$\\times$10$^{16}$ g under the current limits of PBH abundance $f_{PBH}$. Using 205.4 kg$\\cdot$day data obtained from the CDEX-10 experiment conducted in the China Jinping Underground Laboratory, we exclude the $\\chi$--electron ($\\chi$--$e$) elastic-scattering cross section $\\sigma_{\\chi e} \\sim 5\\times10^{-29}$ cm$^2$ for $\\chi$ with a mass $m_{\\chi}\\lesssim$ 0.1 keV from our results.","If ($m_{\\chi}$, $\\sigma_{\\chi e}$) can be determined in the future, DD experiments are expected to impose strong constraints on $f_{PBH}$ for large $M_{PBH}$s."],"url":"http://arxiv.org/abs/2403.20263v1","category":"hep-ex"}
{"created":"2024-03-29 16:08:59","title":"Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges","abstract":"Deep learning models have achieved high performance in medical applications, however, their adoption in clinical practice is hindered due to their black-box nature. Self-explainable models, like prototype-based models, can be especially beneficial as they are interpretable by design. However, if the learnt prototypes are of low quality then the prototype-based models are as good as black-box. Having high quality prototypes is a pre-requisite for a truly interpretable model. In this work, we propose a prototype evaluation framework for coherence (PEF-C) for quantitatively evaluating the quality of the prototypes based on domain knowledge. We show the use of PEF-C in the context of breast cancer prediction using mammography. Existing works on prototype-based models on breast cancer prediction using mammography have focused on improving the classification performance of prototype-based models compared to black-box models and have evaluated prototype quality through anecdotal evidence. We are the first to go beyond anecdotal evidence and evaluate the quality of the mammography prototypes systematically using our PEF-C. Specifically, we apply three state-of-the-art prototype-based models, ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer prediction and evaluate these models w.r.t. i) classification performance, and ii) quality of the prototypes, on three public datasets. Our results show that prototype-based models are competitive with black-box models in terms of classification performance, and achieve a higher score in detecting ROIs. However, the quality of the prototypes are not yet sufficient and can be improved in aspects of relevance, purity and learning a variety of prototypes. We call the XAI community to systematically evaluate the quality of the prototypes to check their true usability in high stake decisions and improve such models further.","sentences":["Deep learning models have achieved high performance in medical applications, however, their adoption in clinical practice is hindered due to their black-box nature.","Self-explainable models, like prototype-based models, can be especially beneficial as they are interpretable by design.","However, if the learnt prototypes are of low quality then the prototype-based models are as good as black-box.","Having high quality prototypes is a pre-requisite for a truly interpretable model.","In this work, we propose a prototype evaluation framework for coherence (PEF-C) for quantitatively evaluating the quality of the prototypes based on domain knowledge.","We show the use of PEF-C in the context of breast cancer prediction using mammography.","Existing works on prototype-based models on breast cancer prediction using mammography have focused on improving the classification performance of prototype-based models compared to black-box models and have evaluated prototype quality through anecdotal evidence.","We are the first to go beyond anecdotal evidence and evaluate the quality of the mammography prototypes systematically using our PEF-C. Specifically, we apply three state-of-the-art prototype-based models, ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer prediction and evaluate these models w.r.t.","i) classification performance, and ii) quality of the prototypes, on three public datasets.","Our results show that prototype-based models are competitive with black-box models in terms of classification performance, and achieve a higher score in detecting ROIs.","However, the quality of the prototypes are not yet sufficient and can be improved in aspects of relevance, purity and learning a variety of prototypes.","We call the XAI community to systematically evaluate the quality of the prototypes to check their true usability in high stake decisions and improve such models further."],"url":"http://arxiv.org/abs/2403.20260v1","category":"cs.CV"}
{"created":"2024-03-29 15:20:57","title":"Lattices in rigid analytic representations","abstract":"For a profinite group $G$ and a rigid analytic space $X$, we study when an $\\mathcal O_X(X)$-linear representation $V$ of $G$ admits a lattice, i.e. an $\\mathcal O_{\\mathcal X}(\\mathcal X)$-linear model for a formal model $\\mathcal X$ of $X$ in the sense of Berthelot. We give a positive answer, under mild assumptions, when $X$ is a \"wide open\" space. Via such a result, we are able to describe explicit subdomains of $X$ over which $V$ is constant after reduction modulo a power of $p$. As an application, we prove some explicit results on the reduction of sheaves of crystalline and semistable representations modulo powers of $p$. We also give an application to the pseudorepresentation carried by the Coleman--Mazur eigencurve.","sentences":["For a profinite group $G$ and a rigid analytic space $X$, we study when an $\\mathcal O_X(X)$-linear representation $V$ of $G$ admits a lattice, i.e. an $\\mathcal O_{\\mathcal X}(\\mathcal X)$-linear model for a formal model $\\mathcal X$ of $X$ in the sense of Berthelot.","We give a positive answer, under mild assumptions, when $X$ is a \"wide open\" space.","Via such a result, we are able to describe explicit subdomains of $X$ over which $V$ is constant after reduction modulo a power of $p$. As an application, we prove some explicit results on the reduction of sheaves of crystalline and semistable representations modulo powers of $p$. We also give an application to the pseudorepresentation carried by the Coleman--Mazur eigencurve."],"url":"http://arxiv.org/abs/2403.20232v1","category":"math.NT"}
{"created":"2024-03-29 15:16:36","title":"Confinement and magnetic field effect on chiral ferroelectric nematic in Grandjean-Cano wedge cells","abstract":"We explore the structure and magnetic field response of edge dislocations in Grandjean-Cano wedge cells filled with chiral mixtures of the ferroelectric nematic mesogen DIO. Upon cooling, the ordering changes from paraelectric in the cholesteric phase N* to antiferroelectric in the smectic SmZ_A* and to ferroelectric in the cholesteric N_F*. Dislocations of the Burgers vector b equal the helicoidal pitch P are stable in all three phases, while dislocations with b=P/2 exist only in the N* and SmZ_A*. The b=P/2 dislocations split into pairs of {\\tau}(-1/2) {\\lambda}(+1/2) disclinations, while the thick dislocations b=P are pairs of nonsingular {\\lambda}(-1/2) {\\lambda}(+1/2) disclinations. The polar order makes the {\\tau}(-1/2) disclinations unstable in the N_F* phase, as they should be connected to singular walls in the polarization field. We propose a model of transformation of the composite {\\tau}(-1/2) line-wall defect into a nonsingular {\\lambda}(-1/2) disclination, which is paired up with a {\\lambda}(+1/2) line to form a b=P dislocation. The SmZ_A* behavior in the in-plane magnetic field is different from that of the N_F* and N*: the dislocations show no zigzag instability, and the pitch remains unchanged in the magnetic fields up to 1 T. The behavior is associated with the finite compressibility of smectic layers.","sentences":["We explore the structure and magnetic field response of edge dislocations in Grandjean-Cano wedge cells filled with chiral mixtures of the ferroelectric nematic mesogen DIO.","Upon cooling, the ordering changes from paraelectric in the cholesteric phase N* to antiferroelectric in the smectic SmZ_A* and to ferroelectric in the cholesteric N_F*.","Dislocations of the Burgers vector b equal the helicoidal pitch P are stable in all three phases, while dislocations with b=P/2 exist only in the N* and SmZ_A*.","The b=P/2 dislocations split into pairs of {\\tau}(-1/2) {\\lambda}(+1/2) disclinations, while the thick dislocations b=P are pairs of nonsingular {\\lambda}(-1/2) {\\lambda}(+1/2) disclinations.","The polar order makes the {\\tau}(-1/2) disclinations unstable in the N_F* phase, as they should be connected to singular walls in the polarization field.","We propose a model of transformation of the composite {\\tau}(-1/2) line-wall defect into a nonsingular {\\lambda}(-1/2) disclination, which is paired up with a {\\lambda}(+1/2) line to form a b=P dislocation.","The SmZ_A* behavior in the in-plane magnetic field is different from that of the N_F* and N*: the dislocations show no zigzag instability, and the pitch remains unchanged in the magnetic fields","up to 1 T. The behavior is associated with the finite compressibility of smectic layers."],"url":"http://arxiv.org/abs/2403.20229v1","category":"cond-mat.soft"}
{"created":"2024-03-29 15:11:53","title":"Relationships between Global and Local Monotonicity of Operators","abstract":"The paper is devoted to establishing relationships between global and local monotonicity, as well as their maximality versions, for single-valued and set-valued mappings between finite-dimensional and infinite-dimensional spaces. We first show that for single-valued operators with convex domains in locally convex topological spaces, their continuity ensures that their global monotonicity agrees with the local one around any point of the graph. This also holds for set-valued mappings defined on the real line under a certain connectedness condition. The situation is different for set-valued operators in multidimensional spaces as demonstrated by an example of locally monotone operator on the plane that is not globally monotone. Finally, we invoke coderivative criteria from variational analysis to characterize both global and local maximal monotonicity of set-valued operators in Hilbert spaces to verify the equivalence between these monotonicity properties under the closed-graph and global hypomonotonicity assumptions.","sentences":["The paper is devoted to establishing relationships between global and local monotonicity, as well as their maximality versions, for single-valued and set-valued mappings between finite-dimensional and infinite-dimensional spaces.","We first show that for single-valued operators with convex domains in locally convex topological spaces, their continuity ensures that their global monotonicity agrees with the local one around any point of the graph.","This also holds for set-valued mappings defined on the real line under a certain connectedness condition.","The situation is different for set-valued operators in multidimensional spaces as demonstrated by an example of locally monotone operator on the plane that is not globally monotone.","Finally, we invoke coderivative criteria from variational analysis to characterize both global and local maximal monotonicity of set-valued operators in Hilbert spaces to verify the equivalence between these monotonicity properties under the closed-graph and global hypomonotonicity assumptions."],"url":"http://arxiv.org/abs/2403.20227v1","category":"math.FA"}
{"created":"2024-03-29 14:18:26","title":"Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks","abstract":"Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In this paper, we introduce a fully automatic approach to address the challenges. Specifically, we extend the label-anchored contrastive learning method introduced by Zhang et al. (2022b) to learn label embeddings during a classification task. These embeddings are then utilized to map discourse relations from different frameworks. We show experimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et al., 2018).","sentences":["Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints.","Despite surface differences, these frameworks share basic understandings of discourse relations.","The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks.","Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks.","However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed.","Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel.","In this paper, we introduce a fully automatic approach to address the challenges.","Specifically, we extend the label-anchored contrastive learning method introduced by Zhang et al. (2022b) to learn label embeddings during a classification task.","These embeddings are then utilized to map discourse relations from different frameworks.","We show experimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et al., 2018)."],"url":"http://arxiv.org/abs/2403.20196v1","category":"cs.CL"}
{"created":"2024-03-29 13:30:11","title":"Analytic holonomicity of real C$^{\\mathrm{exp}}$-class distributions","abstract":"We introduce a notion of distributions on $\\mathbb{R}^n$, called distributions of C$^{{\\mathrm{exp}}}$-class, based on wavelet transforms of distributions and the theory from [6] about C$^{{\\mathrm{exp}}}$-class functions. We prove that the framework of C$^{{\\mathrm{exp}}}$-class distributions is closed under natural operations, like push-forward, pull-back, derivation and antiderivation, and, in the tempered case, Fourier transforms. Our main result is the (real analytic) holonomicity of all distributions of C$^{{\\mathrm{exp}}}$-class.","sentences":["We introduce a notion of distributions on $\\mathbb{R}^n$, called distributions of C$^{{\\mathrm{exp}}}$-class, based on wavelet transforms of distributions and the theory from [6] about C$^{{\\mathrm{exp}}}$-class functions.","We prove that the framework of C$^{{\\mathrm{exp}}}$-class distributions is closed under natural operations, like push-forward, pull-back, derivation and antiderivation, and, in the tempered case, Fourier transforms.","Our main result is the (real analytic) holonomicity of all distributions of C$^{{\\mathrm{exp}}}$-class."],"url":"http://arxiv.org/abs/2403.20167v1","category":"math.AG"}
{"created":"2024-03-29 13:09:23","title":"A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation","abstract":"Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.","sentences":["Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations.","This paper studies the role of subword segmentation in cross-lingual transfer.","We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies.","Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning.","Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness.","Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling."],"url":"http://arxiv.org/abs/2403.20157v1","category":"cs.CL"}
{"created":"2024-03-29 12:25:02","title":"FOOD I: A New Division Scheme For The Stelliferous Era","abstract":"In recent years the James Webb Space Telescope has enabled the frontier of observational galaxy formation to push to ever higher redshift, deep within cosmic dawn. However, what is high-redshift, and when was cosmic dawn? While widely used, these terms (as well as many other confusing terms) are not consistently defined in the literature; this both hampers effective communication but also impedes our ability to precisely characterize and understand the phenomena under investigation. In this article we seek to address this issue of utmost importance. We begin by definitively defining terms such as ``high-redshift'', ``cosmic dawn'', etc. However, despite the rigorous definitions for them we present, both the adjective-based redshift and diurnal marker (time-of-day) division schemes suffer from issues including not being sufficiently granular, angering cosmologists, being arbitrary, and having a geocentric bias. To overcome these we introduce the \\textit{redshiFt epOchs fOr everyboDy} (FOOD) framework, a revolutionary new division scheme based on eating occasions, i.e. meals.","sentences":["In recent years the James Webb Space Telescope has enabled the frontier of observational galaxy formation to push to ever higher redshift, deep within cosmic dawn.","However, what is high-redshift, and when was cosmic dawn?","While widely used, these terms (as well as many other confusing terms) are not consistently defined in the literature; this both hampers effective communication but also impedes our ability to precisely characterize and understand the phenomena under investigation.","In this article we seek to address this issue of utmost importance.","We begin by definitively defining terms such as ``high-redshift'', ``cosmic dawn'', etc.","However, despite the rigorous definitions for them we present, both the adjective-based redshift and diurnal marker (time-of-day) division schemes suffer from issues including not being sufficiently granular, angering cosmologists, being arbitrary, and having a geocentric bias.","To overcome these we introduce the \\textit{redshiFt epOchs fOr everyboDy} (FOOD) framework, a revolutionary new division scheme based on eating occasions, i.e. meals."],"url":"http://arxiv.org/abs/2403.20144v1","category":"astro-ph.GA"}
{"created":"2024-03-29 11:49:42","title":"A formal specification of the jq language","abstract":"jq is a widely used tool that provides a programming language to manipulate JSON data. However, the jq language is currently only specified by its implementation, making it difficult to reason about its behaviour. To this end, we provide a formal syntax and denotational semantics for a large subset of the jq language. Our most significant contribution is to provide a new way to interpret updates that allows for more predictable and performant execution.","sentences":["jq is a widely used tool that provides a programming language to manipulate JSON data.","However, the jq language is currently only specified by its implementation, making it difficult to reason about its behaviour.","To this end, we provide a formal syntax and denotational semantics for a large subset of the jq language.","Our most significant contribution is to provide a new way to interpret updates that allows for more predictable and performant execution."],"url":"http://arxiv.org/abs/2403.20132v1","category":"cs.LO"}
{"created":"2024-03-29 11:15:30","title":"Vortex glass transition and thermal creep in niobium films","abstract":"The evolution of the vortex glass (VG) phase transition and vortex creep with decreasing film thickness is studied in ultrathin, polycrystalline niobium films, with thickness in the range 7.4 nm to 44 nm, using current-voltage characteristics measurements in perpendicular magnetic field. Standard methods, including scaling laws, allow to identify VG transition in the thickest film, while in thinner films creep produces large uncertainty in the putative VG transition temperature and scaling exponents. Using strong pinning theory we perform analysis of the creep, and extract the dependence of the activation energy for vortex pinning on temperature, magnetic field, and film thickness. This analysis provides more information on vortex dynamics than the standard evaluation of critical current density. The results reveal two distinct regimes of pinning, which we propose to identify with $\\delta l$ or $\\delta T_c$-types of pinning (due to spacial fluctuation of mean free path $l$ or spacial fluctuation of superconducting transition temperature $T_c$, respectively). In the thickest film $\\delta l$ pinning is observed, but with the decrease of film thickness the second pinning regime appears, and becomes dominant in the thinnest film. We link these pinning regimes with the structural disorder due to grain boundaries, which produce charge carrier scattering in the thickest film, but with decreasing film thickness gradually evolve into amorphous inclusions, producing fluctuations in the $T_c$.","sentences":["The evolution of the vortex glass (VG) phase transition and vortex creep with decreasing film thickness is studied in ultrathin, polycrystalline niobium films, with thickness in the range 7.4 nm to 44 nm, using current-voltage characteristics measurements in perpendicular magnetic field.","Standard methods, including scaling laws, allow to identify VG transition in the thickest film, while in thinner films creep produces large uncertainty in the putative VG transition temperature and scaling exponents.","Using strong pinning theory we perform analysis of the creep, and extract the dependence of the activation energy for vortex pinning on temperature, magnetic field, and film thickness.","This analysis provides more information on vortex dynamics than the standard evaluation of critical current density.","The results reveal two distinct regimes of pinning, which we propose to identify with $\\delta l$ or $\\delta T_c$-types of pinning (due to spacial fluctuation of mean free path $l$ or spacial fluctuation of superconducting transition temperature $T_c$, respectively).","In the thickest film $\\delta l$ pinning is observed, but with the decrease of film thickness the second pinning regime appears, and becomes dominant in the thinnest film.","We link these pinning regimes with the structural disorder due to grain boundaries, which produce charge carrier scattering in the thickest film, but with decreasing film thickness gradually evolve into amorphous inclusions, producing fluctuations in the $T_c$."],"url":"http://arxiv.org/abs/2403.20121v1","category":"cond-mat.supr-con"}
{"created":"2024-03-29 11:02:13","title":"Balmer Decrement Anomalies in Galaxies at z ~ 6 Found by JWST Observations: Density-Bounded Nebulae or Excited H I Clouds?","abstract":"We investigate the physical origins of the Balmer decrement anomalies in GS-NDG-9422 (Cameron et al. 2023) and RXCJ2248-ID (Topping et al. 2024) galaxies at $z\\sim 6$ whose $\\mathrm{H}\\alpha/\\mathrm{H}\\beta$ values are significantly smaller than $2.7$, the latter of which also shows anomalous $\\mathrm{H}\\gamma/\\mathrm{H}\\beta$ and $\\mathrm{H}\\delta/\\mathrm{H}\\beta$ values beyond the errors. Because the anomalous Balmer decrements are not reproduced under the Case B recombination, we explore the nebulae with the optical depths smaller and larger than the Case B recombination by physical modeling. We find two cases quantitatively explaining the anomalies; 1) density-bounded nebulae that are opaque only up to around Ly$\\gamma$-Ly8 transitions and 2) ionization-bounded nebulae partly/fully surrounded by optically-thick excited H{\\sc i} clouds. The case of 1) produces more H$\\beta$ photons via Ly$\\gamma$ absorption in the nebulae, requiring fine tuning in optical depth values, while this case helps ionizing photon escape for cosmic reionization. The case of 2) needs the optically-thick excited H{\\sc i} clouds with $N_2\\simeq 10^{12}-10^{13}$ $\\mathrm{cm^{-2}}$, where $N_2$ is the column density of the hydrogen atom with the principal quantum number of $n=2$. Interestingly, the high $N_2$ values qualitatively agree with the recent claims for GS-NDG-9422 with the strong nebular continuum requiring a number of $2s$-state electrons and for RXCJ2248-ID with the dense ionized regions likely coexisting with the optically-thick clouds. While the physical origin of the optically-thick excited H{\\sc i} clouds is unclear, these results may suggest gas clouds with excessive collisional excitation caused by an amount of accretion and supernovae in the high-$z$ galaxies.","sentences":["We investigate the physical origins of the Balmer decrement anomalies in GS-NDG-9422 (Cameron et al. 2023) and RXCJ2248-ID (Topping et al. 2024) galaxies at $z\\sim 6$ whose $\\mathrm{H}\\alpha/\\mathrm{H}\\beta$ values are significantly smaller than $2.7$, the latter of which also shows anomalous $\\mathrm{H}\\gamma/\\mathrm{H}\\beta$ and $\\mathrm{H}\\delta/\\mathrm{H}\\beta$ values beyond the errors.","Because the anomalous Balmer decrements are not reproduced under the Case B recombination, we explore the nebulae with the optical depths smaller and larger than the Case B recombination by physical modeling.","We find two cases quantitatively explaining the anomalies; 1) density-bounded nebulae that are opaque only up to around Ly$\\gamma$-Ly8 transitions and 2) ionization-bounded nebulae partly/fully surrounded by optically-thick excited H{\\sc i} clouds.","The case of 1) produces more H$\\beta$ photons via Ly$\\gamma$ absorption in the nebulae, requiring fine tuning in optical depth values, while this case helps ionizing photon escape for cosmic reionization.","The case of 2) needs the optically-thick excited H{\\sc i} clouds with $N_2\\simeq 10^{12}-10^{13}$ $\\mathrm{cm^{-2}}$, where $N_2$ is the column density of the hydrogen atom with the principal quantum number of $n=2$. Interestingly, the high $N_2$ values qualitatively agree with the recent claims for GS-NDG-9422 with the strong nebular continuum requiring a number of $2s$-state electrons and for RXCJ2248-ID with the dense ionized regions likely coexisting with the optically-thick clouds.","While the physical origin of the optically-thick excited H{\\sc i} clouds is unclear, these results may suggest gas clouds with excessive collisional excitation caused by an amount of accretion and supernovae in the high-$z$ galaxies."],"url":"http://arxiv.org/abs/2403.20118v1","category":"astro-ph.GA"}
{"created":"2024-03-29 10:09:21","title":"Neural Network-based model of galaxy power spectrum: Fast full-shape galaxy power spectrum analysis","abstract":"We present a Neural Network based emulator for the galaxy redshift-space power spectrum that enables several orders of magnitude acceleration in the galaxy clustering parameter inference, while preserving 3$\\sigma$ accuracy better than 0.5\\% up to $k_{\\mathrm{max}}$=0.25$h^{-1}Mpc$ within $\\Lambda$CDM and around 0.5\\% $w_0$-$w_a$CDM. Our surrogate model only emulates the galaxy bias-invariant terms of 1-loop perturbation theory predictions, these terms are then combined analytically with galaxy bias terms, counter-terms and stochastic terms in order to obtain the non-linear redshift space galaxy power spectrum. This allows us to avoid any galaxy bias prescription in the training of the emulator, which makes it more flexible. Moreover, we include the redshift $z \\in [0,1.4]$ in the training which further avoids the need for re-training the emulator. We showcase the performance of the emulator in recovering the cosmological parameters of $\\Lambda$CDM by analysing the suite of 25 AbacusSummit simulations that mimic the DESI Luminous Red Galaxies at $z=0.5$ and $z=0.8$, together as the Emission Line Galaxies at $z=0.8$. We obtain similar performance in all cases, demonstrating the reliability of the emulator for any galaxy sample at any redshift in $0 < z < 1.4$","sentences":["We present a Neural Network based emulator for the galaxy redshift-space power spectrum that enables several orders of magnitude acceleration in the galaxy clustering parameter inference, while preserving 3$\\sigma$ accuracy better than 0.5\\% up to $k_{\\mathrm{max}}$=0.25$h^{-1}Mpc$ within $\\Lambda$CDM and around 0.5\\% $w_0$-$w_a$CDM.","Our surrogate model only emulates the galaxy bias-invariant terms of 1-loop perturbation theory predictions, these terms are then combined analytically with galaxy bias terms, counter-terms and stochastic terms in order to obtain the non-linear redshift space galaxy power spectrum.","This allows us to avoid any galaxy bias prescription in the training of the emulator, which makes it more flexible.","Moreover, we include the redshift $z \\in","[0,1.4]$ in the training which further avoids the need for re-training the emulator.","We showcase the performance of the emulator in recovering the cosmological parameters of $\\Lambda$CDM by analysing the suite of 25 AbacusSummit simulations that mimic the DESI Luminous Red Galaxies at $z=0.5$ and $z=0.8$, together as the Emission Line Galaxies at $z=0.8$. We obtain similar performance in all cases, demonstrating the reliability of the emulator for any galaxy sample at any redshift in $0 < z < 1.4$"],"url":"http://arxiv.org/abs/2403.20093v1","category":"astro-ph.CO"}
{"created":"2024-03-29 09:52:05","title":"Unobserving the Moon: the spurious possibility of orbital decoupling due to solar neutrino Arago spot","abstract":"The Arago spot is an intensity maximum at the center of a shadow created by constructive interference of diffracted waves around a spherical object. While the study of diffraction patterns usually concerns visible light, de Broglie's wave nature of matter makes diffraction theory applicable for particles, such as neutrinos, as well. During a solar eclipse, some of the neutrinos emitted by the Sun are diffracted by the Moon, resulting in a diffraction pattern that can be observed on Earth. In this paper we consider the theoretically emerging solar neutrino Arago spot as a means to measure the location of the Moon with high accuracy and consider its implication on the orbit of the Moon given Heisenberg's uncertainty principle. Our results indicate that the Moon is not at immediate risk of orbital decoupling due to the observation of a solar neutrino Arago spot.","sentences":["The Arago spot is an intensity maximum at the center of a shadow created by constructive interference of diffracted waves around a spherical object.","While the study of diffraction patterns usually concerns visible light, de Broglie's wave nature of matter makes diffraction theory applicable for particles, such as neutrinos, as well.","During a solar eclipse, some of the neutrinos emitted by the Sun are diffracted by the Moon, resulting in a diffraction pattern that can be observed on Earth.","In this paper we consider the theoretically emerging solar neutrino Arago spot as a means to measure the location of the Moon with high accuracy and consider its implication on the orbit of the Moon given Heisenberg's uncertainty principle.","Our results indicate that the Moon is not at immediate risk of orbital decoupling due to the observation of a solar neutrino Arago spot."],"url":"http://arxiv.org/abs/2403.20087v1","category":"quant-ph"}
{"created":"2024-03-29 09:01:00","title":"Splitting maps in Type I Ricci flows","abstract":"We study the existence and small scale behaviour of almost splitting maps along a Ricci flow satisfying Type I curvature bounds. These are special solutions of the heat equation that serve as parabolic analogues of harmonic almost splitting maps, which have proven to be an indespensable tool in the study of the structure of the singular set of non-collapsed Ricci limit spaces.   In this paper, motivated by the recent work of Cheeger-Jiang-Naber in the Ricci limit setting, we construct sharp splitting maps on Ricci flows that are almost selfsimilar, and then investigate their small scale behaviour. We show that, modulo linear transformations, an almost splitting map at a large scale remains a splitting map even at smaller scales, provided that the Ricci flow remains sufficiently self-similar. Allowing these linear transformations means that a priori an almost splitting map might degenerate at small scales. However, we show that under an additional summability hypothesis such degeneration doesn't occur.","sentences":["We study the existence and small scale behaviour of almost splitting maps along a Ricci flow satisfying Type I curvature bounds.","These are special solutions of the heat equation that serve as parabolic analogues of harmonic almost splitting maps, which have proven to be an indespensable tool in the study of the structure of the singular set of non-collapsed Ricci limit spaces.   ","In this paper, motivated by the recent work of Cheeger-Jiang-Naber in the Ricci limit setting, we construct sharp splitting maps on Ricci flows that are almost selfsimilar, and then investigate their small scale behaviour.","We show that, modulo linear transformations, an almost splitting map at a large scale remains a splitting map even at smaller scales, provided that the Ricci flow remains sufficiently self-similar.","Allowing these linear transformations means that a priori an almost splitting map might degenerate at small scales.","However, we show that under an additional summability hypothesis such degeneration doesn't occur."],"url":"http://arxiv.org/abs/2403.20070v1","category":"math.DG"}
{"created":"2024-03-29 08:47:15","title":"Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets","abstract":"Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages.","sentences":["Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language.","These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER).","This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set.","We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship.","We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations.","Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks.","If a source and target language have more entities in common, the transfer ability is stronger.","Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL.","Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages."],"url":"http://arxiv.org/abs/2403.20056v1","category":"cs.CL"}
{"created":"2024-03-29 08:30:34","title":"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning","abstract":"Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\textsc{CoTErrorSet} will be published soon on \\texttt{Anonymity Link}.","sentences":["Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting.","While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition.","Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning?","}","This study investigates this problem from both the prompting and model-tuning perspectives.","We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes.","To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology.","We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions.","Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references.","We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome.","\\textsc{CoTErrorSet} will be published soon on \\texttt{Anonymity Link}."],"url":"http://arxiv.org/abs/2403.20046v1","category":"cs.CL"}
{"created":"2024-03-29 08:29:30","title":"Compton scattering in Bandos-Lechner-Sorokin-Townsend nonlinear electrodynamics","abstract":"The nonlinear electrodynamics proposed by Bandos, Lechner, Sorokin and Townsend is a remarkable theory that unifies Maxwell, Bialynicki-Birula and ModMax theories, which are known theories invariant under conformal transformations and electromagnetic duality transformations. In the Bandos-Lechner-Sorokin-Townsend nonlinear electrodynamics, we calculate the energy flux density, dispersion relations, refractive indices, phase and group velocities of plane waves as well as the changes of the photon wavelength in the Compton scattering process in the presence of a constant uniform electromagnetic background. Our results are useful for testing and constraining this new theory of nonlinear electrodynamics.","sentences":["The nonlinear electrodynamics proposed by Bandos, Lechner, Sorokin and Townsend is a remarkable theory that unifies Maxwell, Bialynicki-Birula and ModMax theories, which are known theories invariant under conformal transformations and electromagnetic duality transformations.","In the Bandos-Lechner-Sorokin-Townsend nonlinear electrodynamics, we calculate the energy flux density, dispersion relations, refractive indices, phase and group velocities of plane waves as well as the changes of the photon wavelength in the Compton scattering process in the presence of a constant uniform electromagnetic background.","Our results are useful for testing and constraining this new theory of nonlinear electrodynamics."],"url":"http://arxiv.org/abs/2403.20044v1","category":"hep-ph"}
{"created":"2024-03-29 13:16:05","title":"HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes","abstract":"Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed.","sentences":["Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles.","Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements.","3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping.","However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes.","To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes.","To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties.","Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction.","To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes.","Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed."],"url":"http://arxiv.org/abs/2403.20159v1","category":"cs.CV"}
{"created":"2024-03-29 09:22:44","title":"Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter","abstract":"Compression of large and performant vision foundation models (VFMs) into arbitrary bit-wise operations (BitOPs) allows their deployment on various hardware. We propose to fine-tune a VFM to a mixed-precision quantized supernet. The supernet-based neural architecture search (NAS) can be adopted for this purpose, which trains a supernet, and then subnets within arbitrary hardware budgets can be extracted. However, existing methods face difficulties in optimizing the mixed-precision search space and incurring large memory costs during training. To tackle these challenges, first, we study the effective search space design for fine-tuning a VFM by comparing different operators (such as resolution, feature size, width, depth, and bit-widths) in terms of performance and BitOPs reduction. Second, we propose memory-efficient supernet training using a low-rank adapter (LoRA) and a progressive training strategy. The proposed method is evaluated for the recently proposed VFM, Segment Anything Model, fine-tuned on segmentation tasks. The searched model yields about a 95% reduction in BitOPs without incurring performance degradation.","sentences":["Compression of large and performant vision foundation models (VFMs) into arbitrary bit-wise operations (BitOPs) allows their deployment on various hardware.","We propose to fine-tune a VFM to a mixed-precision quantized supernet.","The supernet-based neural architecture search (NAS) can be adopted for this purpose, which trains a supernet, and then subnets within arbitrary hardware budgets can be extracted.","However, existing methods face difficulties in optimizing the mixed-precision search space and incurring large memory costs during training.","To tackle these challenges, first, we study the effective search space design for fine-tuning a VFM by comparing different operators (such as resolution, feature size, width, depth, and bit-widths) in terms of performance and BitOPs reduction.","Second, we propose memory-efficient supernet training using a low-rank adapter (LoRA) and a progressive training strategy.","The proposed method is evaluated for the recently proposed VFM, Segment Anything Model, fine-tuned on segmentation tasks.","The searched model yields about a 95% reduction in BitOPs without incurring performance degradation."],"url":"http://arxiv.org/abs/2403.20080v1","category":"cs.CV"}
{"created":"2024-03-29 15:22:03","title":"Functional Bilevel Optimization for Machine Learning","abstract":"In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.","sentences":["In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space.","These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function.","The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function.","We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures."],"url":"http://arxiv.org/abs/2403.20233v1","category":"stat.ML"}
{"created":"2024-03-29 13:56:23","title":"Computational Shape Derivatives in Heat Conduction: An Optimization Approach for Enhanced Thermal Performance","abstract":"We analyze an optimization problem of the conductivity in a composite material arising in a heat conduction energy storage problem. The model is described by the heat equation that specifies the heat exchange between two types of materials with different conductive properties with Dirichlet-Neumann boundary conditions on the external part of the domain, and on the interface characterized by the resisting coefficient between the highly conductive material and the less conductive material. The main purpose of the paper is to compute a shape gradient of an optimization functional in order to accurately determine the optimal location of the conductive material using a classical shape optimization strategy. We also present some numerical experiments to illustrate the efficiency of the proposed method.","sentences":["We analyze an optimization problem of the conductivity in a composite material arising in a heat conduction energy storage problem.","The model is described by the heat equation that specifies the heat exchange between two types of materials with different conductive properties with Dirichlet-Neumann boundary conditions on the external part of the domain, and on the interface characterized by the resisting coefficient between the highly conductive material and the less conductive material.","The main purpose of the paper is to compute a shape gradient of an optimization functional in order to accurately determine the optimal location of the conductive material using a classical shape optimization strategy.","We also present some numerical experiments to illustrate the efficiency of the proposed method."],"url":"http://arxiv.org/abs/2403.20181v1","category":"math.OC"}
{"created":"2024-03-29 13:46:27","title":"Energy solutions of the Cauchy-Dirichlet problem for fractional nonlinear diffusion equations","abstract":"The present paper is concerned with the Cauchy-Dirichlet problem for fractional (and non-fractional) nonlinear diffusion equations posed in bounded domains. Main results consist of well-posedness in an energy class with no sign restriction and convergence of such (possibly sign-changing) energy solutions to asymptotic profiles after a proper rescaling. They will be proved in a variational scheme only, without any use of semigroup theories nor classical quasilinear parabolic theories. Proofs are self-contained and performed in a totally unified fashion for both fractional and non-fractional cases as well as for both porous medium and fast diffusion cases.","sentences":["The present paper is concerned with the Cauchy-Dirichlet problem for fractional (and non-fractional) nonlinear diffusion equations posed in bounded domains.","Main results consist of well-posedness in an energy class with no sign restriction and convergence of such (possibly sign-changing) energy solutions to asymptotic profiles after a proper rescaling.","They will be proved in a variational scheme only, without any use of semigroup theories nor classical quasilinear parabolic theories.","Proofs are self-contained and performed in a totally unified fashion for both fractional and non-fractional cases as well as for both porous medium and fast diffusion cases."],"url":"http://arxiv.org/abs/2403.20176v1","category":"math.AP"}
{"created":"2024-03-29 12:15:25","title":"Na Vacancy Driven Phase Transformation and Fast Ion Conduction in W-doped Na$_3$SbS$_4$ from Machine Learning Force Fields","abstract":"Solid-state sodium batteries require effective electrolytes that conduct at room temperature. The Na$_3$SbS$_4$ (Pn = P, Sb; Ch = S, Se) family have been studied for their high Na ion conductivity. The population of Na vacancies, which mediate ion diffusion in these materials, can be enhanced through aliovalent doping on the pnictogen site. To probe the microscopic role of extrinsic doping, and its impact on diffusion and phase stability, we trained a machine learning force field for Na$_{3-x}$W$_{x}$Sb$_{1-x}$S$_4$ based on an equivariant graph neural network. Analysis of large-scale molecular dynamics trajectories shows that an increased Na vacancy population stabilises the global cubic phase at lower temperatures with enhanced Na ion diffusion, and that the explicit role of the substitutional W dopants is limited. In the global cubic phase we observe large and long-lived deviations of atoms from the averaged symmetry, echoing recent experimental suggestions. Evidence of correlated Na ion diffusion is also presented that underpins the suggested superionic nature of these materials.","sentences":["Solid-state sodium batteries require effective electrolytes that conduct at room temperature.","The Na$_3$SbS$_4$ (Pn = P, Sb; Ch = S, Se) family have been studied for their high Na ion conductivity.","The population of Na vacancies, which mediate ion diffusion in these materials, can be enhanced through aliovalent doping on the pnictogen site.","To probe the microscopic role of extrinsic doping, and its impact on diffusion and phase stability, we trained a machine learning force field for Na$_{3-x}$W$_{x}$Sb$_{1-x}$S$_4$ based on an equivariant graph neural network.","Analysis of large-scale molecular dynamics trajectories shows that an increased Na vacancy population stabilises the global cubic phase at lower temperatures with enhanced Na ion diffusion, and that the explicit role of the substitutional W dopants is limited.","In the global cubic phase we observe large and long-lived deviations of atoms from the averaged symmetry, echoing recent experimental suggestions.","Evidence of correlated Na ion diffusion is also presented that underpins the suggested superionic nature of these materials."],"url":"http://arxiv.org/abs/2403.20138v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-29 09:20:29","title":"SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior","abstract":"Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.","sentences":["Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation.","The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).","Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints.","This issue stems from the sparse training views captured by a fixed camera on a moving vehicle.","To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data.","Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information.","Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training.","Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views."],"url":"http://arxiv.org/abs/2403.20079v1","category":"cs.CV"}
{"created":"2024-03-29 08:48:04","title":"Optimal s-boxes against alternative operations","abstract":"Civino et al. have characterised diffusion layers that expose an SPN to vulnerability from differential cryptanalysis when employing alternative operations coming from groups isomorphic to the translation group on the message space. In this study, we present a classification of diffusion layers that exhibit linearity in parallel alternative operations for ciphers with 4-bit s-boxes, enabling the possibility of an alternative differential attack simultaneously targeting all the s-boxes within the block. Furthermore, we investigate the differential behaviour with respect to alternative operations for all classes of optimal 4-bit s-boxes, as defined by Leander and Poschmann (2007). Our examination reveals that certain classes contain weak permutations w.r.t. alternative differential attacks, and we leverage these vulnerabilities to execute a series of experiments.","sentences":["Civino et al. have characterised diffusion layers that expose an SPN to vulnerability from differential cryptanalysis when employing alternative operations coming from groups isomorphic to the translation group on the message space.","In this study, we present a classification of diffusion layers that exhibit linearity in parallel alternative operations for ciphers with 4-bit s-boxes, enabling the possibility of an alternative differential attack simultaneously targeting all the s-boxes within the block.","Furthermore, we investigate the differential behaviour with respect to alternative operations for all classes of optimal 4-bit s-boxes, as defined by Leander and Poschmann (2007).","Our examination reveals that certain classes contain weak permutations w.r.t. alternative differential attacks, and we leverage these vulnerabilities to execute a series of experiments."],"url":"http://arxiv.org/abs/2403.20059v1","category":"cs.CR"}
{"created":"2024-03-29 08:43:46","title":"The Peculiar Destiny of Sentiment de Monsieur Leibnitz (May 1705 -- March 1706)","abstract":"During the querelle des infiniment petits, Leibniz wrote several texts to justify using Differential calculus among Parisian savants. However, only three were published. Among these publications, ''Sentiment de Monsieur Leibnitz'' had a peculiar destiny. Although we are aware of the manuscript (Gotha FB A 448--449, Bl. 41--42), it is only recently that we have identified a copy of its impression in the British Library catalogue. This copy was printed in 1706 together with writings by other mathematicians united in the defence of the new calculus -- Joseph Saurin, Jacob Hermann and the Bernoulli brothers. Recently published epistolary exchanges indicate that Jean-Paul Bignon, at the time director of the Royal Academy of Sciences, in order to calm down the institution, had prohibited this publication and confiscated the prints.This article examine the epistemological and institutional issues at stake in ''Sentiment de Monsieur Leibnitz''.","sentences":["During the querelle des infiniment petits, Leibniz wrote several texts to justify using Differential calculus among Parisian savants.","However, only three were published.","Among these publications, ''Sentiment de Monsieur Leibnitz'' had a peculiar destiny.","Although we are aware of the manuscript (Gotha FB A 448--449, Bl. 41--42), it is only recently that we have identified a copy of its impression in the British Library catalogue.","This copy was printed in 1706 together with writings by other mathematicians united in the defence of the new calculus -- Joseph Saurin, Jacob Hermann and the Bernoulli brothers.","Recently published epistolary exchanges indicate that Jean-Paul Bignon, at the time director of the Royal Academy of Sciences, in order to calm down the institution, had prohibited this publication and confiscated the prints.","This article examine the epistemological and institutional issues at stake in ''Sentiment de Monsieur Leibnitz''."],"url":"http://arxiv.org/abs/2403.20052v1","category":"math.HO"}
{"created":"2024-03-29 08:33:05","title":"Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World","abstract":"Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to additional OOD data. Theoretical insights demonstrate how our method reduces model confidence when faced with OOD samples. Empirical experiments across multiple datasets, model architectures, and sparsity levels validate the effectiveness of our method, with improvements of up to \\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and calibration. This research enhances the understanding and readiness of sparse DNNs for deployment in resource-limited applications. Our code is available on: \\url{https://github.com/StevenBoys/MOON}.","sentences":["Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications.","However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data.","This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability.","The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge.","To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to additional OOD data.","Theoretical insights demonstrate how our method reduces model confidence when faced with OOD samples.","Empirical experiments across multiple datasets, model architectures, and sparsity levels validate the effectiveness of our method, with improvements of up to \\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and calibration.","This research enhances the understanding and readiness of sparse DNNs for deployment in resource-limited applications.","Our code is available on: \\url{https://github.com/StevenBoys/MOON}."],"url":"http://arxiv.org/abs/2403.20047v1","category":"cs.LG"}
{"created":"2024-03-29 17:59:05","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","abstract":"Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, demonstrating the robot's ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door. Project website: https://zhengmaohe.github.io/leg-manip","sentences":["Quadruped robots are progressively being integrated into human environments.","Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited.","While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs).","Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs.","We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner.","By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC.","Our approach is validated through simulations and real-world experiments, demonstrating the robot's ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door.","Project website: https://zhengmaohe.github.io/leg-manip"],"url":"http://arxiv.org/abs/2403.20328v1","category":"cs.RO"}
{"created":"2024-03-29 17:33:42","title":"Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via Negations","abstract":"Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word \"not\" in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.","sentences":["Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning.","An important aspect of reasoning in logic and language is negations.","This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word \"not\" in a given prompt.","To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions.","Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations.","This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets.","Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text.","Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility."],"url":"http://arxiv.org/abs/2403.20312v1","category":"cs.CV"}
{"created":"2024-03-29 15:26:44","title":"Long-Tailed Anomaly Detection with Learnable Class Names","abstract":"Anomaly detection (AD) aims to identify defective images and localize their defects (if any). Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without anomaly supervision; and be robust to the long-tailed distributions of real-world applications. To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation. We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names. LTAD combines AD by reconstruction and semantic AD modules. AD by reconstruction is implemented with a transformer-based reconstruction module. Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained foundation model. These modules are learned over two phases. Phase 1 learns the pseudo-class names and a variational autoencoder (VAE) for feature synthesis that augments the training data to combat long-tails. Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD. Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance. The long-tailed dataset split is available at https://zenodo.org/records/10854201 .","sentences":["Anomaly detection (AD) aims to identify defective images and localize their defects (if any).","Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without anomaly supervision; and be robust to the long-tailed distributions of real-world applications.","To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation.","We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names.","LTAD combines AD by reconstruction and semantic AD modules.","AD by reconstruction is implemented with a transformer-based reconstruction module.","Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained foundation model.","These modules are learned over two phases.","Phase 1 learns the pseudo-class names and a variational autoencoder (VAE) for feature synthesis that augments the training data to combat long-tails.","Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD.","Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance.","The long-tailed dataset split is available at https://zenodo.org/records/10854201 ."],"url":"http://arxiv.org/abs/2403.20236v1","category":"cs.CV"}
{"created":"2024-03-29 14:24:49","title":"High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile","abstract":"High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data. We propose to investigate high-dimensional regression models for independent but non-identically distributed data. To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate. Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile. In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator. For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero. We also exhibit variance profiles for which the shape of this predictive risk differs from double descent. The proofs of our results are based on tools from random matrix theory in the presence of a variance profile that have not been considered so far to study regression models. Numerical experiments are provided to show the accuracy of the aforementioned deterministic equivalents on the computation of the predictive risk of ridge regression. We also investigate the similarities and differences that exist with the standard setting of independent and identically distributed data.","sentences":["High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data.","We propose to investigate high-dimensional regression models for independent but non-identically distributed data.","To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate.","Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile.","In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator.","For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero.","We also exhibit variance profiles for which the shape of this predictive risk differs from double descent.","The proofs of our results are based on tools from random matrix theory in the presence of a variance profile that have not been considered so far to study regression models.","Numerical experiments are provided to show the accuracy of the aforementioned deterministic equivalents on the computation of the predictive risk of ridge regression.","We also investigate the similarities and differences that exist with the standard setting of independent and identically distributed data."],"url":"http://arxiv.org/abs/2403.20200v1","category":"math.ST"}
{"created":"2024-03-29 09:46:14","title":"Selective Attention-based Modulation for Continual Learning","abstract":"We present SAM, a biologically-plausible selective attention-driven modulation approach to enhance classification models in a continual learning setting. Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial attention biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks. Experimental results confirm that SAM effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings. Moreover, we show that attention-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods. Code is available at: https://github.com/perceivelab/SAM.","sentences":["We present SAM, a biologically-plausible selective attention-driven modulation approach to enhance classification models in a continual learning setting.","Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial attention biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks.","Experimental results confirm that SAM effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings.","Moreover, we show that attention-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods.","Code is available at: https://github.com/perceivelab/SAM."],"url":"http://arxiv.org/abs/2403.20086v1","category":"cs.CV"}
{"created":"2024-03-29 08:46:57","title":"Reinforcement learning for graph theory, II. Small Ramsey numbers","abstract":"We describe here how the recent Wagner's approach for applying reinforcement learning to construct examples in graph theory can be used in the search for critical graphs for small Ramsey numbers. We illustrate this application by providing lower bounds for the small Ramsey numbers $R(K_{2,5}, K_{3,5})$, $R(B_3, B_6)$ and $R(B_4, B_5)$ and by improving the lower known bound for $R(W_5, W_7)$.","sentences":["We describe here how the recent Wagner's approach for applying reinforcement learning to construct examples in graph theory can be used in the search for critical graphs for small Ramsey numbers.","We illustrate this application by providing lower bounds for the small Ramsey numbers $R(K_{2,5}, K_{3,5})$, $R(B_3, B_6)$ and $R(B_4, B_5)$ and by improving the lower known bound for $R(W_5, W_7)$."],"url":"http://arxiv.org/abs/2403.20055v1","category":"math.CO"}
{"created":"2024-03-29 16:29:07","title":"Spherical Particle in Nematic Liquid Crystal with a Magnetic Field and Planar Anchoring","abstract":"We study minimizers of the Landau-de Gennes energy in $\\mathbb{R}^3\\setminus B_1(0)$ with external magnetic field in the large particle limit. We impose strong tangential anchoring and uniaxiality of the $Q-$tensor on the boundary. We derive a lower bound for the energy in terms of the boundary condition and show in the extreme cases that the longitudinal director field is energy minimizing, indicating the presence of two half-point defects, so called boojums, at two opposite points of the sphere. Using a recovery sequence, we show that the energy bound is optimal.","sentences":["We study minimizers of the Landau-de Gennes energy in $\\mathbb{R}^3\\setminus B_1(0)$ with external magnetic field in the large particle limit.","We impose strong tangential anchoring and uniaxiality of the $Q-$tensor on the boundary.","We derive a lower bound for the energy in terms of the boundary condition and show in the extreme cases that the longitudinal director field is energy minimizing, indicating the presence of two half-point defects, so called boojums, at two opposite points of the sphere.","Using a recovery sequence, we show that the energy bound is optimal."],"url":"http://arxiv.org/abs/2403.20274v1","category":"math.AP"}
{"created":"2024-03-29 15:57:38","title":"Latent Embedding Clustering for Occlusion Robust Head Pose Estimation","abstract":"Head pose estimation has become a crucial area of research in computer vision given its usefulness in a wide range of applications, including robotics, surveillance, or driver attention monitoring. One of the most difficult challenges in this field is managing head occlusions that frequently take place in real-world scenarios. In this paper, we propose a novel and efficient framework that is robust in real world head occlusion scenarios. In particular, we propose an unsupervised latent embedding clustering with regression and classification components for each pose angle. The model optimizes latent feature representations for occluded and non-occluded images through a clustering term while improving fine-grained angle predictions. Experimental evaluation on in-the-wild head pose benchmark datasets reveal competitive performance in comparison to state-of-the-art methodologies with the advantage of having a significant data reduction. We observe a substantial improvement in occluded head pose estimation. Also, an ablation study is conducted to ascertain the impact of the clustering term within our proposed framework.","sentences":["Head pose estimation has become a crucial area of research in computer vision given its usefulness in a wide range of applications, including robotics, surveillance, or driver attention monitoring.","One of the most difficult challenges in this field is managing head occlusions that frequently take place in real-world scenarios.","In this paper, we propose a novel and efficient framework that is robust in real world head occlusion scenarios.","In particular, we propose an unsupervised latent embedding clustering with regression and classification components for each pose angle.","The model optimizes latent feature representations for occluded and non-occluded images through a clustering term while improving fine-grained angle predictions.","Experimental evaluation on in-the-wild head pose benchmark datasets reveal competitive performance in comparison to state-of-the-art methodologies with the advantage of having a significant data reduction.","We observe a substantial improvement in occluded head pose estimation.","Also, an ablation study is conducted to ascertain the impact of the clustering term within our proposed framework."],"url":"http://arxiv.org/abs/2403.20251v1","category":"cs.CV"}
{"created":"2024-03-29 15:14:16","title":"Cooperative Sensing and Communication for ISAC Networks: Performance Analysis and Optimization","abstract":"In this work, we study integrated sensing and communication (ISAC) networks intending to effectively balance sensing and communication (S&C) performance at the network level. Through the simultaneous utilization of multi-point (CoMP) coordinated joint transmission and distributed multiple-input multiple-output (MIMO) radar techniques, we propose a cooperative networked ISAC scheme to enhance both S&C services. Then, the tool of stochastic geometry is exploited to capture the S&C performance, which allows us to illuminate key cooperative dependencies in the ISAC network. Remarkably, the derived expression of the Cramer-Rao lower bound (CRLB) of the localization accuracy unveils a significant finding: Deploying $N$ ISAC transceivers yields an enhanced sensing performance across the entire network, in accordance with the $\\ln^2N$ scaling law. Simulation results demonstrate that compared to the time-sharing scheme, the proposed cooperative ISAC scheme can effectively improve the average data rate and reduce the CRLB.","sentences":["In this work, we study integrated sensing and communication (ISAC) networks intending to effectively balance sensing and communication (S&C) performance at the network level.","Through the simultaneous utilization of multi-point (CoMP) coordinated joint transmission and distributed multiple-input multiple-output (MIMO) radar techniques, we propose a cooperative networked ISAC scheme to enhance both S&C services.","Then, the tool of stochastic geometry is exploited to capture the S&C performance, which allows us to illuminate key cooperative dependencies in the ISAC network.","Remarkably, the derived expression of the Cramer-Rao lower bound (CRLB) of the localization accuracy unveils a significant finding: Deploying $N$ ISAC transceivers yields an enhanced sensing performance across the entire network, in accordance with the $\\ln^2N$ scaling law.","Simulation results demonstrate that compared to the time-sharing scheme, the proposed cooperative ISAC scheme can effectively improve the average data rate and reduce the CRLB."],"url":"http://arxiv.org/abs/2403.20228v1","category":"cs.IT"}
{"created":"2024-03-29 14:40:59","title":"Scaled Brownian motion with random anomalous diffusion exponent","abstract":"The scaled Brownian motion (SBM) is regarded as one of the paradigmatic random processes, featuring the anomalous diffusion property characterized by the diffusion exponent. It is a Gaussian, self-similar process with independent increments, which has found applications across various fields, from turbulence and stochastic hydrology to biophysics. In our paper, inspired by recent single particle tracking biological experiments, we introduce a process termed the scaled Brownian motion with random exponent (SBMRE), which preserves SBM characteristics at the level of individual trajectories, albeit with randomly varying anomalous diffusion exponents across the trajectories. We discuss the main probabilistic properties of SBMRE, including its probability density function (pdf), and the q-th absolute moment. Additionally, we present the expected value of the time-averaged mean squared displacement (TAMSD) and the ergodicity breaking parameter. Furthermore, we analyze the pdf of the first hitting time in a semi-infinite domain, the martingale property of SBMRE, and its stochastic exponential. As special cases, we consider two distributions of the anomalous diffusion exponent, namely the two-point and beta distributions, and discuss the asymptotics of the presented characteristics in such cases. Theoretical results for SBMRE are validated through numerical simulations and compared with the corresponding characteristics for SBM.","sentences":["The scaled Brownian motion (SBM) is regarded as one of the paradigmatic random processes, featuring the anomalous diffusion property characterized by the diffusion exponent.","It is a Gaussian, self-similar process with independent increments, which has found applications across various fields, from turbulence and stochastic hydrology to biophysics.","In our paper, inspired by recent single particle tracking biological experiments, we introduce a process termed the scaled Brownian motion with random exponent (SBMRE), which preserves SBM characteristics at the level of individual trajectories, albeit with randomly varying anomalous diffusion exponents across the trajectories.","We discuss the main probabilistic properties of SBMRE, including its probability density function (pdf), and the q-th absolute moment.","Additionally, we present the expected value of the time-averaged mean squared displacement (TAMSD) and the ergodicity breaking parameter.","Furthermore, we analyze the pdf of the first hitting time in a semi-infinite domain, the martingale property of SBMRE, and its stochastic exponential.","As special cases, we consider two distributions of the anomalous diffusion exponent, namely the two-point and beta distributions, and discuss the asymptotics of the presented characteristics in such cases.","Theoretical results for SBMRE are validated through numerical simulations and compared with the corresponding characteristics for SBM."],"url":"http://arxiv.org/abs/2403.20206v1","category":"math.PR"}
{"created":"2024-03-29 14:37:44","title":"Stochastic Approximation Proximal Subgradient Method for Stochastic Convex-Concave Minimax Optimization","abstract":"This paper presents a stochastic approximation proximal subgradient (SAPS) method for stochastic convex-concave minimax optimization. By accessing unbiased and variance bounded approximate subgradients, we show that this algorithm exhibits ${\\rm O}(N^{-1/2})$ expected convergence rate of the minimax optimality measure if the parameters in the algorithm are properly chosen, where $N$ denotes the number of iterations. Moreover, we show that the algorithm has ${\\rm O}(\\log(N)N^{-1/2})$ minimax optimality measure bound with high probability. Further we study a specific stochastic convex-concave minimax optimization problems arising from stochastic convex conic optimization problems, which the the bounded subgradient condition is fail. To overcome the lack of the bounded subgradient conditions in convex-concave minimax problems, we propose a linearized stochastic approximation augmented Lagrange (LSAAL) method and prove that this algorithm exhibits ${\\rm O}(N^{-1/2})$ expected convergence rate for the minimax optimality measure and ${\\rm O}(\\log^2(N)N^{-1/2})$ minimax optimality measure bound with high probability as well. Preliminary numerical results demonstrate the effect of the SAPS and LSAAL methods.","sentences":["This paper presents a stochastic approximation proximal subgradient (SAPS) method for stochastic convex-concave minimax optimization.","By accessing unbiased and variance bounded approximate subgradients, we show that this algorithm exhibits ${\\rm O}(N^{-1/2})$ expected convergence rate of the minimax optimality measure if the parameters in the algorithm are properly chosen, where $N$ denotes the number of iterations.","Moreover, we show that the algorithm has ${\\rm O}(\\log(N)N^{-1/2})$ minimax optimality measure bound with high probability.","Further we study a specific stochastic convex-concave minimax optimization problems arising from stochastic convex conic optimization problems, which the the bounded subgradient condition is fail.","To overcome the lack of the bounded subgradient conditions in convex-concave minimax problems, we propose a linearized stochastic approximation augmented Lagrange (LSAAL) method and prove that this algorithm exhibits ${\\rm O}(N^{-1/2})$ expected convergence rate for the minimax optimality measure and ${\\rm O}(\\log^2(N)N^{-1/2})$ minimax optimality measure bound with high probability as well.","Preliminary numerical results demonstrate the effect of the SAPS and LSAAL methods."],"url":"http://arxiv.org/abs/2403.20205v1","category":"math.OC"}
{"created":"2024-03-29 10:51:35","title":"The minimal control time for the exact controllability by internal controls of 1D linear hyperbolic balance laws","abstract":"In this article we study the internal controllability of 1D linear hyperbolic balance laws when the number of controls is equal to the number of state variables. The controls are supported in space in an arbitrary open subset. Our main result is a complete characterization of the minimal control time for the exact controllability property.","sentences":["In this article we study the internal controllability of 1D linear hyperbolic balance laws when the number of controls is equal to the number of state variables.","The controls are supported in space in an arbitrary open subset.","Our main result is a complete characterization of the minimal control time for the exact controllability property."],"url":"http://arxiv.org/abs/2403.20113v1","category":"math.OC"}
{"created":"2024-03-29 09:54:37","title":"Non-Exponential Reverberation Modeling Using Dark Velvet Noise","abstract":"Previous research on late-reverberation modeling has mainly focused on exponentially decaying room impulse responses, whereas methods for accurately modeling non-exponential reverberation remain challenging. This paper extends the previously proposed basic dark-velvet-noise reverberation algorithm and proposes a parametrization scheme for modeling late reverberation with arbitrary temporal energy decay. Each pulse in the velvet-noise sequence is routed to a single dictionary filter that is selected from a set of filters based on weighted probabilities. The probabilities control the spectral evolution of the late-reverberation model and are optimized to fit a target impulse response via non-negative least-squares optimization. In this way, the frequency-dependent energy decay of a target late-reverberation impulse response can be fitted with mean and maximum T60 errors of 4% and 8%, respectively, requiring about 50% less coloration filters than a previously proposed filtered velvet-noise algorithm. Furthermore, the extended dark-velvet-noise reverberation algorithm allows the modeled impulse response to be gated, the frequency-dependent reverberation time to be modified, and the model's spectral evolution and broadband decay to be decoupled. The proposed method is suitable for the parametric late-reverberation synthesis of various acoustic environments, especially spaces that exhibit a non-exponential energy decay, motivating its use in musical audio and virtual reality.","sentences":["Previous research on late-reverberation modeling has mainly focused on exponentially decaying room impulse responses, whereas methods for accurately modeling non-exponential reverberation remain challenging.","This paper extends the previously proposed basic dark-velvet-noise reverberation algorithm and proposes a parametrization scheme for modeling late reverberation with arbitrary temporal energy decay.","Each pulse in the velvet-noise sequence is routed to a single dictionary filter that is selected from a set of filters based on weighted probabilities.","The probabilities control the spectral evolution of the late-reverberation model and are optimized to fit a target impulse response via non-negative least-squares optimization.","In this way, the frequency-dependent energy decay of a target late-reverberation impulse response can be fitted with mean and maximum T60 errors of 4% and 8%, respectively, requiring about 50% less coloration filters than a previously proposed filtered velvet-noise algorithm.","Furthermore, the extended dark-velvet-noise reverberation algorithm allows the modeled impulse response to be gated, the frequency-dependent reverberation time to be modified, and the model's spectral evolution and broadband decay to be decoupled.","The proposed method is suitable for the parametric late-reverberation synthesis of various acoustic environments, especially spaces that exhibit a non-exponential energy decay, motivating its use in musical audio and virtual reality."],"url":"http://arxiv.org/abs/2403.20090v1","category":"eess.AS"}
