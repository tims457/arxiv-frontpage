{"created":"2024-06-10 17:59:59","title":"IllumiNeRF: 3D Relighting without Inverse Rendering","abstract":"Existing methods for relightable view synthesis -- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination -- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images. Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive. In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting. We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks. Please see our project page at https://illuminerf.github.io/.","sentences":["Existing methods for relightable view synthesis -- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination -- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images.","Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive.","In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting.","We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks.","Please see our project page at https://illuminerf.github.io/."],"url":"http://arxiv.org/abs/2406.06527v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:55","title":"GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation","abstract":"3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation. However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2. In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass. Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation. 2) Spatial-aware Gaussian Attribute Decoder: We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points. Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS).","sentences":["3D city generation with NeRF-based methods shows promising generation results but is computationally inefficient.","Recently 3D Gaussian Splatting (3D-GS) has emerged as a highly efficient alternative for object-level 3D generation.","However, adapting 3D-GS from finite-scale 3D objects and humans to infinite-scale 3D cities is non-trivial.","Unbounded 3D city generation entails significant storage overhead (out-of-memory issues), arising from the need to expand points to billions, often demanding hundreds of Gigabytes of VRAM for a city scene spanning 10km^2.","In this paper, we propose GaussianCity, a generative Gaussian Splatting framework dedicated to efficiently synthesizing unbounded 3D cities with a single feed-forward pass.","Our key insights are two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a highly compact intermediate representation, ensuring that the growth in VRAM usage for unbounded scenes remains constant, thus enabling unbounded city generation.","2) Spatial-aware Gaussian Attribute Decoder:","We present spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which leverages Point Serializer to integrate the structural and contextual characteristics of BEV points.","Extensive experiments demonstrate that GaussianCity achieves state-of-the-art results in both drone-view and street-view 3D city generation.","Notably, compared to CityDreamer, GaussianCity exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18 FPS)."],"url":"http://arxiv.org/abs/2406.06526v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:52","title":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","abstract":"We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.","sentences":["We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain.","It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly.","We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality.","The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark.","(2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment.","(4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup.","We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models."],"url":"http://arxiv.org/abs/2406.06525v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:46","title":"NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing","abstract":"We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results at https://koi953215.github.io/NaRCan_page/.","sentences":["We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video.","Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model's ability to handle complex video dynamics.","By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods.","Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times.","Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences.","See our project page for video results at https://koi953215.github.io/NaRCan_page/."],"url":"http://arxiv.org/abs/2406.06523v1","category":"cs.CV"}
{"created":"2024-06-10 17:59:46","title":"Gas Fees on the Ethereum Blockchain: From Foundations to Derivatives Valuations","abstract":"The gas fee, paid for inclusion in the blockchain, is analyzed in two parts. First, we consider how effort in terms of resources required to process and store a transaction turns into a gas limit, which, through a fee, comprised of the base and priority fee in the current version of Ethereum, is converted into the cost paid by the user. We hew closely to the Ethereum protocol to simplify the analysis and to constrain the design choices when considering multidimensional gas. Second, we assume that the gas price is given deus ex machina by a fractional Ornstein-Uhlenbeck process and evaluate various derivatives. These contracts can, for example, mitigate gas cost volatility. The ability to price and trade forwards besides the existing spot inclusion into the blockchain could be beneficial.","sentences":["The gas fee, paid for inclusion in the blockchain, is analyzed in two parts.","First, we consider how effort in terms of resources required to process and store a transaction turns into a gas limit, which, through a fee, comprised of the base and priority fee in the current version of Ethereum, is converted into the cost paid by the user.","We hew closely to the Ethereum protocol to simplify the analysis and to constrain the design choices when considering multidimensional gas.","Second, we assume that the gas price is given deus ex machina by a fractional Ornstein-Uhlenbeck process and evaluate various derivatives.","These contracts can, for example, mitigate gas cost volatility.","The ability to price and trade forwards besides the existing spot inclusion into the blockchain could be beneficial."],"url":"http://arxiv.org/abs/2406.06524v1","category":"q-fin.PM"}
{"created":"2024-06-10 17:59:01","title":"PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction","abstract":"Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due to its high-quality rendering, and ultra-fast training and rendering speed. However, due to the unstructured and irregular nature of Gaussian point clouds, it is difficult to guarantee geometric reconstruction accuracy and multi-view consistency simply by relying on image reconstruction loss. Although many studies on surface reconstruction based on 3DGS have emerged recently, the quality of their meshes is generally unsatisfactory. To address this problem, we propose a fast planar-based Gaussian splatting reconstruction representation (PGSR) to achieve high-fidelity surface reconstruction while ensuring high-quality rendering. Specifically, we first introduce an unbiased depth rendering method, which directly renders the distance from the camera origin to the Gaussian plane and the corresponding normal map based on the Gaussian distribution of the point cloud, and divides the two to obtain the unbiased depth. We then introduce single-view geometric, multi-view photometric, and geometric regularization to preserve global geometric accuracy. We also propose a camera exposure compensation model to cope with scenes with large illumination variations. Experiments on indoor and outdoor scenes show that our method achieves fast training and rendering while maintaining high-fidelity rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based methods.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due to its high-quality rendering, and ultra-fast training and rendering speed.","However, due to the unstructured and irregular nature of Gaussian point clouds, it is difficult to guarantee geometric reconstruction accuracy and multi-view consistency simply by relying on image reconstruction loss.","Although many studies on surface reconstruction based on 3DGS have emerged recently, the quality of their meshes is generally unsatisfactory.","To address this problem, we propose a fast planar-based Gaussian splatting reconstruction representation (PGSR) to achieve high-fidelity surface reconstruction while ensuring high-quality rendering.","Specifically, we first introduce an unbiased depth rendering method, which directly renders the distance from the camera origin to the Gaussian plane and the corresponding normal map based on the Gaussian distribution of the point cloud, and divides the two to obtain the unbiased depth.","We then introduce single-view geometric, multi-view photometric, and geometric regularization to preserve global geometric accuracy.","We also propose a camera exposure compensation model to cope with scenes with large illumination variations.","Experiments on indoor and outdoor scenes show that our method achieves fast training and rendering while maintaining high-fidelity rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based methods."],"url":"http://arxiv.org/abs/2406.06521v1","category":"cs.CV"}
{"created":"2024-06-10 17:58:48","title":"Decentralized Personalized Federated Learning","abstract":"This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning. We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively. Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency. Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead. We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning. Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL. DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios.","sentences":["This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning.","We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively.","Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency.","Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead.","We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning.","Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL.","DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios."],"url":"http://arxiv.org/abs/2406.06520v1","category":"cs.LG"}
{"created":"2024-06-10 17:58:29","title":"UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor","abstract":"Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.","sentences":["Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems.","Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious.","A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts.","Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper.","Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems.","Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies.","UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field.","UMBRELA is available at https://github.com/castorini/umbrela."],"url":"http://arxiv.org/abs/2406.06519v1","category":"cs.IR"}
{"created":"2024-06-10 17:53:01","title":"Merlin: A Vision Language Foundation Model for 3D Computed Tomography","abstract":"Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs). However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens). We evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU.","sentences":["Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen.","Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies.","Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs).","However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision.","We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens).","We evaluate Merlin on 6 task types and 752 individual tasks.","The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs).","We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator).","Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines.","We derive data scaling laws to empirically assess training data needs for requisite downstream task performance.","Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU."],"url":"http://arxiv.org/abs/2406.06512v1","category":"cs.CV"}
{"created":"2024-06-10 17:47:14","title":"Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer","abstract":"Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing? Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space. In this work, we explore the attention mechanism of pre-trained motion diffusion models. We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer. Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively. Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing. Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones. MoMo harnesses motion inversion, extending its application to both real and generated motions. Experimental results show the advantage of our approach over the current art. In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training. Our webpage is at https://monkeyseedocg.github.io.","sentences":["Given the remarkable results of motion synthesis with diffusion models, a natural question arises: how can we effectively leverage these models for motion editing?","Existing diffusion-based motion editing methods overlook the profound potential of the prior embedded within the weights of pre-trained models, which enables manipulating the latent feature space; hence, they primarily center on handling the motion space.","In this work, we explore the attention mechanism of pre-trained motion diffusion models.","We uncover the roles and interactions of attention elements in capturing and representing intricate human motion patterns, and carefully integrate these elements to transfer a leader motion to a follower one while maintaining the nuanced characteristics of the follower, resulting in zero-shot motion transfer.","Editing features associated with selected motions allows us to confront a challenge observed in prior motion diffusion approaches, which use general directives (e.g., text, music) for editing, ultimately failing to convey subtle nuances effectively.","Our work is inspired by how a monkey closely imitates what it sees while maintaining its unique motion patterns; hence we call it Monkey See, Monkey Do, and dub it MoMo.","Employing our technique enables accomplishing tasks such as synthesizing out-of-distribution motions, style transfer, and spatial editing.","Furthermore, diffusion inversion is seldom employed for motions; as a result, editing efforts focus on generated motions, limiting the editability of real ones.","MoMo harnesses motion inversion, extending its application to both real and generated motions.","Experimental results show the advantage of our approach over the current art.","In particular, unlike methods tailored for specific applications through training, our approach is applied at inference time, requiring no training.","Our webpage is at https://monkeyseedocg.github.io."],"url":"http://arxiv.org/abs/2406.06508v1","category":"cs.CV"}
{"created":"2024-06-10 17:43:13","title":"Equivariant Neural Tangent Kernels","abstract":"Equivariant neural networks have in recent years become an important technique for guiding architecture selection for neural networks with many applications in domains ranging from medical image analysis to quantum chemistry. In particular, as the most general linear equivariant layers with respect to the regular representation, group convolutions have been highly impactful in numerous applications. Although equivariant architectures have been studied extensively, much less is known about the training dynamics of equivariant neural networks. Concurrently, neural tangent kernels (NTKs) have emerged as a powerful tool to analytically understand the training dynamics of wide neural networks. In this work, we combine these two fields for the first time by giving explicit expressions for NTKs of group convolutional neural networks. In numerical experiments, we demonstrate superior performance for equivariant NTKs over non-equivariant NTKs on a classification task for medical images.","sentences":["Equivariant neural networks have in recent years become an important technique for guiding architecture selection for neural networks with many applications in domains ranging from medical image analysis to quantum chemistry.","In particular, as the most general linear equivariant layers with respect to the regular representation, group convolutions have been highly impactful in numerous applications.","Although equivariant architectures have been studied extensively, much less is known about the training dynamics of equivariant neural networks.","Concurrently, neural tangent kernels (NTKs) have emerged as a powerful tool to analytically understand the training dynamics of wide neural networks.","In this work, we combine these two fields for the first time by giving explicit expressions for NTKs of group convolutional neural networks.","In numerical experiments, we demonstrate superior performance for equivariant NTKs over non-equivariant NTKs on a classification task for medical images."],"url":"http://arxiv.org/abs/2406.06504v1","category":"cs.LG"}
{"created":"2024-06-10 17:34:58","title":"New bounds on a generalization of Tuza's conjecture","abstract":"For a $k$-uniform hypergraph $H$, let $\\nu^{(m)}(H)$ denote the maximum size of a set $S$ of edges of $H$ whose pairwise intersection has size less than $m$. Let $\\tau^{(m)}(H)$ denote the minimum size of a set $S$ of $m$-sets of $V(H)$ such that every edge of $H$ contains some $m$-set from $S$. A conjecture by Aharoni and Zerbib, which generalizes a conjecture of Tuza on the size of minimum edge covers of triangles of a graph, states that for a $k$-uniform hypergraph $H$, $\\tau^{(k - 1)}(H)/\\nu^{(k - 1)}(H) \\leq \\left \\lceil \\frac{k + 1}{2} \\right \\rceil$. In this paper, we show that this generalization of Tuza's conjecture holds when $\\nu^{(k - 1)}(H) \\leq 3$. As a corollary, we obtain a graph class which satisfies Tuza's conjecture. We also prove various bounds on $\\tau^{(m)}(H)/\\nu^{(m)}(H)$ for other values of $m$ as well as some bounds on the fractional analogues of these numbers.","sentences":["For a $k$-uniform hypergraph $H$, let $\\nu^{(m)}(H)$ denote the maximum size of a set $S$ of edges of $H$ whose pairwise intersection has size less than $m$. Let $\\tau^{(m)}(H)$ denote the minimum size of a set $S$ of $m$-sets of $V(H)$ such that every edge of $H$ contains some $m$-set from $S$. A conjecture by Aharoni and Zerbib, which generalizes a conjecture of Tuza on the size of minimum edge covers of triangles of a graph, states that for a $k$-uniform hypergraph $H$, $\\tau^{(k - 1)}(H)/\\nu^{(k - 1)}(H) \\leq \\left \\lceil \\frac{k + 1}{2} \\right \\rceil$. In this paper, we show that this generalization of Tuza's conjecture holds when $\\nu^{(k - 1)}(H) \\leq 3$.","As a corollary, we obtain a graph class which satisfies Tuza's conjecture.","We also prove various bounds on $\\tau^{(m)}(H)/\\nu^{(m)}(H)$ for other values of $m$ as well as some bounds on the fractional analogues of these numbers."],"url":"http://arxiv.org/abs/2406.06501v1","category":"math.CO"}
{"created":"2024-06-10 17:34:44","title":"Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation","abstract":"In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments. While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance. Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies. These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice. Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies. OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank. Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments. Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights.","sentences":["In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments.","While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance.","Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies.","These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice.","Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies.","OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank.","Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments.","Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights."],"url":"http://arxiv.org/abs/2406.06500v1","category":"cs.AI"}
{"created":"2024-06-10 17:34:24","title":"NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative","abstract":"Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models' ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal-temporal narrative. Extensive experiments demonstrate that CEN is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning. For project details, visit https://narrativebridge.github.io/.","sentences":["Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents.","This lack of narrative restricts models' ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content.","To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal-temporal narrative.","Extensive experiments demonstrate that CEN is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively.","The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning.","For project details, visit https://narrativebridge.github.io/."],"url":"http://arxiv.org/abs/2406.06499v1","category":"cs.CV"}
{"created":"2024-06-10 17:33:27","title":"Rephasing spectral diffusion in time-bin spin-spin entanglement protocols","abstract":"Generating high fidelity spin-spin entanglement is an essential task of quantum repeater networks for the distribution of quantum information across long distances. Solid-state based spin-photon interfaces are promising candidates to realize nodes of a quantum network, but are often limited by spectral diffusion of the optical transition, which results in phase errors on the entangled states. Here, we introduce a method to correct phase errors from quasi-static frequency fluctuations after the entangled state is generated, by shelving the emitters in the excited state to refocus the unknown phase. For quasi-static frequency fluctuations, the fidelity is determined only by the lifetime of the excited state used for shelving, making it particularly suitable for systems with a long-lived shelving state with correlated spectral diffusion. Such a shelving state may be found in Kramers doublet systems such as rare-earth emitters and color centers in Si or SiC interfaced with nanophotonic cavities with a strongly frequency-dependent Purcell enhancement. The protocol can be used to generate high-fidelity entangled spin pairs without reducing the rate of entanglement generation.","sentences":["Generating high fidelity spin-spin entanglement is an essential task of quantum repeater networks for the distribution of quantum information across long distances.","Solid-state based spin-photon interfaces are promising candidates to realize nodes of a quantum network, but are often limited by spectral diffusion of the optical transition, which results in phase errors on the entangled states.","Here, we introduce a method to correct phase errors from quasi-static frequency fluctuations after the entangled state is generated, by shelving the emitters in the excited state to refocus the unknown phase.","For quasi-static frequency fluctuations, the fidelity is determined only by the lifetime of the excited state used for shelving, making it particularly suitable for systems with a long-lived shelving state with correlated spectral diffusion.","Such a shelving state may be found in Kramers doublet systems such as rare-earth emitters and color centers in Si or SiC interfaced with nanophotonic cavities with a strongly frequency-dependent Purcell enhancement.","The protocol can be used to generate high-fidelity entangled spin pairs without reducing the rate of entanglement generation."],"url":"http://arxiv.org/abs/2406.06497v1","category":"quant-ph"}
{"created":"2024-06-10 17:31:36","title":"Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation","abstract":"Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.","sentences":["Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm.","Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations.","We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation.","Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics.","Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy."],"url":"http://arxiv.org/abs/2406.06496v1","category":"cs.LG"}
{"created":"2024-06-10 17:30:17","title":"Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits","abstract":"Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs.","sentences":["Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs).","PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs.","They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   ","So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale.","In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training.","In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs."],"url":"http://arxiv.org/abs/2406.06494v1","category":"cs.LG"}
{"created":"2024-06-10 17:29:31","title":"Probing the Heights and Depths of Y Dwarf Atmospheres: A Retrieval Analysis of the JWST Spectral Energy Distribution of WISE J035934.06$-$540154.6","abstract":"We present an atmospheric retrieval analysis of the Y0 brown dwarf WISE J035934.06$-$540154.6 using the low-resolution 0.96--12 $\\mu$m JWST spectrum presented in \\citet{Beiler_2023}. We obtain volume number mixing ratios of the major gas-phase absorbers (H$_2$O, CH$_4$, CO, CO$_2$, PH$_3$, and H$_2$S) that are 3--5$\\times$ more precise than previous work that used HST spectra. We also find an order-of-magnitude improvement in the precision of the retrieved thermal profile, a direct result of the broad wavelength coverage of the JWST data. We used the retrieved thermal profile and surface gravity to generate a grid of chemical forward models with varying metallicity, (C/O)$_\\textrm{atm}$, and strengths of vertical mixing as encapsulated by the eddy diffusion coefficient $K_\\textrm{zz}$. Comparison of the retrieved abundances with this grid of models suggests that the deep atmosphere of WISE 0359$-$54 shows signs of vigorous vertical mixing with $K_\\textrm{zz}=10^9$ [cm$^{2}$ s$^{-1}$]. To test the sensitivity of these results to our 5-knot spline thermal profile model, we performed a second retrieval using the \\citet{Madhusudhan_2009} thermal profile model. While the results of the two retrievals generally agree well, we do find differences between the retrieved values of mass and volume number mixing ratio of H$_2$S with fractional differences of the median values of $-$0.64 and $-$0.10, respectively. In addition, the 5-knot thermal profile is consistently warmer at pressure between 1 and 70 bar. Nevertheless, our results underscore the power that the broad-wavelength infrared spectra obtainable with the James Webb Space Telescope have to characterize the atmospheres of cool brown dwarfs.","sentences":["We present an atmospheric retrieval analysis of the Y0 brown dwarf WISE J035934.06$-$540154.6 using the low-resolution 0.96--12 $\\mu$m JWST spectrum presented in \\citet{Beiler_2023}.","We obtain volume number mixing ratios of the major gas-phase absorbers (H$_2$O, CH$_4$, CO, CO$_2$, PH$_3$, and H$_2$S) that are 3--5$\\times$ more precise than previous work that used HST spectra.","We also find an order-of-magnitude improvement in the precision of the retrieved thermal profile, a direct result of the broad wavelength coverage of the JWST data.","We used the retrieved thermal profile and surface gravity to generate a grid of chemical forward models with varying metallicity, (C/O)$_\\textrm{atm}$, and strengths of vertical mixing as encapsulated by the eddy diffusion coefficient $K_\\textrm{zz}$. Comparison of the retrieved abundances with this grid of models suggests that the deep atmosphere of WISE 0359$-$54 shows signs of vigorous vertical mixing with $K_\\textrm{zz}=10^9$ [cm$^{2}$ s$^{-1}$].","To test the sensitivity of these results to our 5-knot spline thermal profile model, we performed a second retrieval using the \\citet{Madhusudhan_2009} thermal profile model.","While the results of the two retrievals generally agree well, we do find differences between the retrieved values of mass and volume number mixing ratio of H$_2$S with fractional differences of the median values of $-$0.64 and $-$0.10, respectively.","In addition, the 5-knot thermal profile is consistently warmer at pressure between 1 and 70 bar.","Nevertheless, our results underscore the power that the broad-wavelength infrared spectra obtainable with the James Webb Space Telescope have to characterize the atmospheres of cool brown dwarfs."],"url":"http://arxiv.org/abs/2406.06493v1","category":"astro-ph.SR"}
{"created":"2024-06-10 17:29:08","title":"Macroscopic quantum superpositions in superconducting circuits","abstract":"A possible route to test whether macroscopic systems can acquire quantum features using superconducting circuits is here presented. It is shown that under general assumptions a classical test current pulse of fixed energy and adjustable length acquires quantum features after interacting with the quantum vacuum of the photon field. Further, it is shown that the mere existence of vacuum fluctuations can lead to the breakdown of energy and momentum conservation, and as the length of the pulse grows with respect to the characteristic size of the quantum system, the test pulse undergoes quantum-to-classical transition. This model differs from previous ones for its simplicity and points towards a new way of creating correlated systems suitable for quantum-based technology.","sentences":["A possible route to test whether macroscopic systems can acquire quantum features using superconducting circuits is here presented.","It is shown that under general assumptions a classical test current pulse of fixed energy and adjustable length acquires quantum features after interacting with the quantum vacuum of the photon field.","Further, it is shown that the mere existence of vacuum fluctuations can lead to the breakdown of energy and momentum conservation, and as the length of the pulse grows with respect to the characteristic size of the quantum system, the test pulse undergoes quantum-to-classical transition.","This model differs from previous ones for its simplicity and points towards a new way of creating correlated systems suitable for quantum-based technology."],"url":"http://arxiv.org/abs/2406.06492v1","category":"quant-ph"}
{"created":"2024-06-10 17:27:43","title":"How much longer do you have to drive than the crow has to fly?","abstract":"When we travel by car from one location to another, our route is constrained by the road network. The resulting network distance is generally longer than the geodetic distance, i.e. the distance as the crow flies, between the two locations. We report a systematic relation between the statistical properties of these two distances. In empirical analyses for large motorway networks in various countries and areas, we work out distributions of network and geodetic distances and identify a surprisingly robust scaling property between them. A simple consequence is that we typically have to drive $1.3\\pm0.1$ times longer than the crow flies. Moreover, we show that this scaling is not present in standard random networks; rather, it requires a certain non-randomness, namely adjacency. We develop a set of rules to build a realistic motorway network, also consistent with the scaling properties found empirically. We hypothesize that the scaling reflects, in a rather universal fashion, a compromise between two societal needs: high efficiency and accessibility on the one hand, and limitation of costs and other burdens on the other.","sentences":["When we travel by car from one location to another, our route is constrained by the road network.","The resulting network distance is generally longer than the geodetic distance, i.e. the distance as the crow flies, between the two locations.","We report a systematic relation between the statistical properties of these two distances.","In empirical analyses for large motorway networks in various countries and areas, we work out distributions of network and geodetic distances and identify a surprisingly robust scaling property between them.","A simple consequence is that we typically have to drive $1.3\\pm0.1$ times longer than the crow flies.","Moreover, we show that this scaling is not present in standard random networks; rather, it requires a certain non-randomness, namely adjacency.","We develop a set of rules to build a realistic motorway network, also consistent with the scaling properties found empirically.","We hypothesize that the scaling reflects, in a rather universal fashion, a compromise between two societal needs: high efficiency and accessibility on the one hand, and limitation of costs and other burdens on the other."],"url":"http://arxiv.org/abs/2406.06490v1","category":"physics.soc-ph"}
{"created":"2024-06-10 17:27:12","title":"Probing out-of-distribution generalization in machine learning for materials","abstract":"Scientific machine learning (ML) endeavors to develop generalizable models with broad applicability. However, the assessment of generalizability is often based on heuristics. Here, we demonstrate in the materials science setting that heuristics based evaluations lead to substantially biased conclusions of ML generalizability and benefits of neural scaling. We evaluate generalization performance in over 700 out-of-distribution tasks that features new chemistry or structural symmetry not present in the training data. Surprisingly, good performance is found in most tasks and across various ML models including simple boosted trees. Analysis of the materials representation space reveals that most tasks contain test data that lie in regions well covered by training data, while poorly-performing tasks contain mainly test data outside the training domain. For the latter case, increasing training set size or training time has marginal or even adverse effects on the generalization performance, contrary to what the neural scaling paradigm assumes. Our findings show that most heuristically-defined out-of-distribution tests are not genuinely difficult and evaluate only the ability to interpolate. Evaluating on such tasks rather than the truly challenging ones can lead to an overestimation of generalizability and benefits of scaling.","sentences":["Scientific machine learning (ML) endeavors to develop generalizable models with broad applicability.","However, the assessment of generalizability is often based on heuristics.","Here, we demonstrate in the materials science setting that heuristics based evaluations lead to substantially biased conclusions of ML generalizability and benefits of neural scaling.","We evaluate generalization performance in over 700 out-of-distribution tasks that features new chemistry or structural symmetry not present in the training data.","Surprisingly, good performance is found in most tasks and across various ML models including simple boosted trees.","Analysis of the materials representation space reveals that most tasks contain test data that lie in regions well covered by training data, while poorly-performing tasks contain mainly test data outside the training domain.","For the latter case, increasing training set size or training time has marginal or even adverse effects on the generalization performance, contrary to what the neural scaling paradigm assumes.","Our findings show that most heuristically-defined out-of-distribution tests are not genuinely difficult and evaluate only the ability to interpolate.","Evaluating on such tasks rather than the truly challenging ones can lead to an overestimation of generalizability and benefits of scaling."],"url":"http://arxiv.org/abs/2406.06489v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 17:26:39","title":"When is Multicalibration Post-Processing Necessary?","abstract":"Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts.","sentences":["Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates.","Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income).","We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs.","Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly.","More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts."],"url":"http://arxiv.org/abs/2406.06487v1","category":"cs.LG"}
{"created":"2024-06-10 17:25:46","title":"Continuum Attention for Neural Operators","abstract":"Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.","sentences":["Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning.","Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems.","Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators.","Motivated by this, we study transformers in the function space setting.","We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator.","The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result.","The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures.","For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators.","Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators."],"url":"http://arxiv.org/abs/2406.06486v1","category":"cs.LG"}
{"created":"2024-06-10 17:24:44","title":"Can Language Models Serve as Text-Based World Simulators?","abstract":"Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.","sentences":["Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand.","Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding?","Our goal is to answer this question in the context of text-based simulators.","Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks.","We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators.","We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations.","This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear."],"url":"http://arxiv.org/abs/2406.06485v1","category":"cs.CL"}
{"created":"2024-06-10 17:22:09","title":"Quantum Equilibrium Propagation for efficient training of quantum systems based on Onsager reciprocity","abstract":"The widespread adoption of machine learning and artificial intelligence in all branches of science and technology has created a need for energy-efficient, alternative hardware platforms. While such neuromorphic approaches have been proposed and realised for a wide range of platforms, physically extracting the gradients required for training remains challenging as generic approaches only exist in certain cases. Equilibrium propagation (EP) is such a procedure that has been introduced and applied to classical energy-based models which relax to an equilibrium. Here, we show a direct connection between EP and Onsager reciprocity and exploit this to derive a quantum version of EP. This can be used to optimize loss functions that depend on the expectation values of observables of an arbitrary quantum system. Specifically, we illustrate this new concept with supervised and unsupervised learning examples in which the input or the solvable task is of quantum mechanical nature, e.g., the recognition of quantum many-body ground states, quantum phase exploration, sensing and phase boundary exploration. We propose that in the future quantum EP may be used to solve tasks such as quantum phase discovery with a quantum simulator even for Hamiltonians which are numerically hard to simulate or even partially unknown. Our scheme is relevant for a variety of quantum simulation platforms such as ion chains, superconducting qubit arrays, neutral atom Rydberg tweezer arrays and strongly interacting atoms in optical lattices.","sentences":["The widespread adoption of machine learning and artificial intelligence in all branches of science and technology has created a need for energy-efficient, alternative hardware platforms.","While such neuromorphic approaches have been proposed and realised for a wide range of platforms, physically extracting the gradients required for training remains challenging as generic approaches only exist in certain cases.","Equilibrium propagation (EP) is such a procedure that has been introduced and applied to classical energy-based models which relax to an equilibrium.","Here, we show a direct connection between EP and Onsager reciprocity and exploit this to derive a quantum version of EP.","This can be used to optimize loss functions that depend on the expectation values of observables of an arbitrary quantum system.","Specifically, we illustrate this new concept with supervised and unsupervised learning examples in which the input or the solvable task is of quantum mechanical nature, e.g., the recognition of quantum many-body ground states, quantum phase exploration, sensing and phase boundary exploration.","We propose that in the future quantum EP may be used to solve tasks such as quantum phase discovery with a quantum simulator even for Hamiltonians which are numerically hard to simulate or even partially unknown.","Our scheme is relevant for a variety of quantum simulation platforms such as ion chains, superconducting qubit arrays, neutral atom Rydberg tweezer arrays and strongly interacting atoms in optical lattices."],"url":"http://arxiv.org/abs/2406.06482v1","category":"quant-ph"}
{"created":"2024-06-10 17:22:01","title":"Nodewise Loreg: Nodewise $L_0$-penalized Regression for High-dimensional Sparse Precision Matrix Estimation","abstract":"We propose Nodewise Loreg, a nodewise $L_0$-penalized regression method for estimating high-dimensional sparse precision matrices. We establish its asymptotic properties, including convergence rates, support recovery, and asymptotic normality under high-dimensional sub-Gaussian settings. Notably, the Nodewise Loreg estimator is asymptotically unbiased and normally distributed, eliminating the need for debiasing required by Nodewise Lasso. We also develop a desparsified version of Nodewise Loreg, similar to the desparsified Nodewise Lasso estimator. The asymptotic variances of the undesparsified Nodewise Loreg estimator are upper bounded by those of both desparsified Nodewise Loreg and Lasso estimators for Gaussian data, potentially offering more powerful statistical inference. Extensive simulations show that the undesparsified Nodewise Loreg estimator generally outperforms the two desparsified estimators in asymptotic normal behavior. Moreover, Nodewise Loreg surpasses Nodewise Lasso, CLIME, and GLasso in most simulations in terms of matrix norm losses, support recovery, and timing performance. Application to a breast cancer gene expression dataset further demonstrates Nodewise Loreg's superiority over the three $L_1$-norm based methods.","sentences":["We propose Nodewise Loreg, a nodewise $L_0$-penalized regression method for estimating high-dimensional sparse precision matrices.","We establish its asymptotic properties, including convergence rates, support recovery, and asymptotic normality under high-dimensional sub-Gaussian settings.","Notably, the Nodewise Loreg estimator is asymptotically unbiased and normally distributed, eliminating the need for debiasing required by Nodewise Lasso.","We also develop a desparsified version of Nodewise Loreg, similar to the desparsified Nodewise Lasso estimator.","The asymptotic variances of the undesparsified Nodewise Loreg estimator are upper bounded by those of both desparsified Nodewise Loreg and Lasso estimators for Gaussian data, potentially offering more powerful statistical inference.","Extensive simulations show that the undesparsified Nodewise Loreg estimator generally outperforms the two desparsified estimators in asymptotic normal behavior.","Moreover, Nodewise Loreg surpasses Nodewise Lasso, CLIME, and GLasso in most simulations in terms of matrix norm losses, support recovery, and timing performance.","Application to a breast cancer gene expression dataset further demonstrates Nodewise Loreg's superiority over the three $L_1$-norm based methods."],"url":"http://arxiv.org/abs/2406.06481v1","category":"math.ST"}
{"created":"2024-06-10 17:16:59","title":"Survey for Landing Generative AI in Social and E-commerce Recsys -- the Industry Perspectives","abstract":"Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys). Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication. Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys. As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research.","sentences":["Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys).","Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication.","Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys.","As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research."],"url":"http://arxiv.org/abs/2406.06475v1","category":"cs.IR"}
{"created":"2024-06-10 17:16:49","title":"Towards a Personal Health Large Language Model","abstract":"In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.","sentences":["In health, most large language model (LLM) research has focused on clinical tasks.","However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring.","Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data.","We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes.","For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness.","Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights.","We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations.","PH-LLM achieved 79% on sleep and 88% on fitness, exceeding average scores from a sample of human experts.","Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models.","Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM."],"url":"http://arxiv.org/abs/2406.06474v1","category":"cs.AI"}
{"created":"2024-06-10 17:14:53","title":"DiffAudit: Auditing Privacy Practices of Online Services for Children and Adolescents","abstract":"Children's and adolescents' online data privacy are regulated by laws such as the Children's Online Privacy Protection Act (COPPA) and the California Consumer Privacy Act (CCPA). Online services that are directed towards general audiences (i.e., including children, adolescents, and adults) must comply with these laws. In this paper, first, we present DiffAudit, a platform-agnostic privacy auditing methodology for general audience services. DiffAudit performs differential analysis of network traffic data flows to compare data processing practices (i) between child, adolescent, and adult users and (ii) before and after consent is given and user age is disclosed. We also present a data type classification method that utilizes GPT-4 and our data type ontology based on COPPA and CCPA, allowing us to identify considerably more data types than prior work. Second, we apply DiffAudit to a set of popular general audience mobile and web services and observe a rich set of behaviors extracted from over 440K outgoing requests, containing 3,968 unique data types we extracted and classified. We reveal problematic data processing practices prior to consent and age disclosure, lack of differentiation between age-specific data flows, inconsistent privacy policy disclosures, and sharing of linkable data with third parties, including advertising and tracking services.","sentences":["Children's and adolescents' online data privacy are regulated by laws such as the Children's Online Privacy Protection Act (COPPA) and the California Consumer Privacy Act (CCPA).","Online services that are directed towards general audiences (i.e., including children, adolescents, and adults) must comply with these laws.","In this paper, first, we present DiffAudit, a platform-agnostic privacy auditing methodology for general audience services.","DiffAudit performs differential analysis of network traffic data flows to compare data processing practices (i) between child, adolescent, and adult users and (ii) before and after consent is given and user age is disclosed.","We also present a data type classification method that utilizes GPT-4 and our data type ontology based on COPPA and CCPA, allowing us to identify considerably more data types than prior work.","Second, we apply DiffAudit to a set of popular general audience mobile and web services and observe a rich set of behaviors extracted from over 440K outgoing requests, containing 3,968 unique data types we extracted and classified.","We reveal problematic data processing practices prior to consent and age disclosure, lack of differentiation between age-specific data flows, inconsistent privacy policy disclosures, and sharing of linkable data with third parties, including advertising and tracking services."],"url":"http://arxiv.org/abs/2406.06473v1","category":"cs.CR"}
{"created":"2024-06-10 17:14:19","title":"Multi-Amplifier Sensing Charge-coupled Devices for Next Generation Spectroscopy","abstract":"We present characterization results and performance of a prototype Multiple-Amplifier Sensing (MAS) silicon charge-coupled device (CCD) sensor with 16 channels potentially suitable for faint object astronomical spectroscopy and low-signal, photon-limited imaging. The MAS CCD is designed to reach sub-electron readout noise by repeatedly measuring charge through a line of amplifiers during the serial transfer shifts. Using synchronized readout electronics based on the DESI CCD controller, we report a read noise of 1.03 e- rms/pix at a speed of 26 $\\mu$s/pix with a single-sample readout scheme where charge in a pixel is measured only once for each output stage. At these operating parameters, we find the amplifier-to-amplifier charge transfer efficiency (ACTE) to be $>0.9995$ at low counts for all amplifiers but one for which the ACTE is 0.997. This charge transfer efficiency falls above 50,000 electrons for the read-noise optimized voltage configuration we chose for the serial clocks and gates. The amplifier linearity across a broad dynamic range from $\\sim$300--35,000 e- was also measured to be $\\pm 2.5\\%$. We describe key operating parameters to optimize on these characteristics and describe the specific applications for which the MAS CCD may be a suitable detector candidate.","sentences":["We present characterization results and performance of a prototype Multiple-Amplifier Sensing (MAS) silicon charge-coupled device (CCD) sensor with 16 channels potentially suitable for faint object astronomical spectroscopy and low-signal, photon-limited imaging.","The MAS CCD is designed to reach sub-electron readout noise by repeatedly measuring charge through a line of amplifiers during the serial transfer shifts.","Using synchronized readout electronics based on the DESI CCD controller, we report a read noise of 1.03 e-","rms/pix at a speed of 26 $\\mu$s/pix with a single-sample readout scheme where charge in a pixel is measured only once for each output stage.","At these operating parameters, we find the amplifier-to-amplifier charge transfer efficiency (ACTE) to be $>0.9995$ at low counts for all amplifiers but one for which the ACTE is 0.997.","This charge transfer efficiency falls above 50,000 electrons for the read-noise optimized voltage configuration we chose for the serial clocks and gates.","The amplifier linearity across a broad dynamic range from $\\sim$300--35,000 e- was also measured to be $\\pm 2.5\\%$.","We describe key operating parameters to optimize on these characteristics and describe the specific applications for which the MAS CCD may be a suitable detector candidate."],"url":"http://arxiv.org/abs/2406.06472v1","category":"astro-ph.IM"}
{"created":"2024-06-10 17:10:41","title":"It\u00f4's Formula for the Rearranged Stochastic Heat Equation","abstract":"The purpose of this short note is to prove a convenient version of It\\^o's formula for the Rearranged Stochastic Heat Equation (RSHE) introduced by the two authors in a previous contribution. This equation is a penalised version of the standard Stochastic Heat Equation (SHE) on the circle subject to a coloured noise, whose solution is constrained to stay within the set of symmetric quantile functions by means of a reflection term. Here, we identity the generator of the solution when it is acting on functions defined on the space ${\\mathcal P}_2({\\mathbb R})$ (of one-dimensional probability measures with a finite second moment) that are assumed to be smooth in Lions' sense. In particular, we prove that the reflection term in the RSHE is orthogonal to the Lions (or Wasserstein) derivative of smooth functions defined on ${\\mathcal P}_2({\\mathbb R})$. The proof relies on non-trivial bounds for the gradient of the solution to the RSHE.","sentences":["The purpose of this short note is to prove a convenient version of It\\^o's formula for the Rearranged Stochastic Heat Equation (RSHE) introduced by the two authors in a previous contribution.","This equation is a penalised version of the standard Stochastic Heat Equation (SHE) on the circle subject to a coloured noise, whose solution is constrained to stay within the set of symmetric quantile functions by means of a reflection term.","Here, we identity the generator of the solution when it is acting on functions defined on the space ${\\mathcal P}_2({\\mathbb R})$ (of one-dimensional probability measures with a finite second moment) that are assumed to be smooth in Lions' sense.","In particular, we prove that the reflection term in the RSHE is orthogonal to the Lions (or Wasserstein) derivative of smooth functions defined on ${\\mathcal P}_2({\\mathbb R})$.","The proof relies on non-trivial bounds for the gradient of the solution to the RSHE."],"url":"http://arxiv.org/abs/2406.06471v1","category":"math.PR"}
{"created":"2024-06-10 17:09:38","title":"GKAN: Graph Kolmogorov-Arnold Networks","abstract":"We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks. Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure. We present two different ways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable functions are applied to input features after aggregation and architecture 2 -- where the learnable functions are applied to input features before aggregation. We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora). We find that architecture generally performs better. We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model. For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66. We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN.","sentences":["We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data.","By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks.","Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure.","We present two different ways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable functions are applied to input features after aggregation and architecture 2 -- where the learnable functions are applied to input features before aggregation.","We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora).","We find that architecture generally performs better.","We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model.","For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66.","We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN."],"url":"http://arxiv.org/abs/2406.06470v1","category":"cs.LG"}
{"created":"2024-06-10 17:07:25","title":"Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning","abstract":"Language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems. Our code and models are available at https://github.com/agent-husky/Husky-v1.","sentences":["Language agents perform complex tasks by using tools to execute each step precisely.","However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering.","We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning.","Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state.","We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions.","Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets.","Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning.","Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems.","Our code and models are available at https://github.com/agent-husky/Husky-v1."],"url":"http://arxiv.org/abs/2406.06469v1","category":"cs.AI"}
{"created":"2024-06-10 17:06:35","title":"Randomized Binary and Tree Search under Pressure","abstract":"We study a generalized binary search problem on the line and general trees. On the line (e.g., a sorted array), binary search finds a target node in $O(\\log n)$ queries in the worst case, where $n$ is the number of nodes. In situations with limited budget or time, we might only be able to perform a few queries, possibly sub-logarithmic many. In this case, it is impossible to guarantee that the target will be found regardless of its position. Our main result is the construction of a randomized strategy that maximizes the minimum (over the target position) probability of finding the target. Such a strategy provides a natural solution where there is no apriori (stochastic) information of the target's position. As with regular binary search, we can find and run the strategy in $O(\\log n)$ time (and using only $O(\\log n)$ random bits). Our construction is obtained by reinterpreting the problem as a two-player (\\textit{seeker} and \\textit{hider}) zero-sum game and exploiting an underlying number theoretical structure.   Furthermore, we generalize the setting to study a search game on trees. In this case, a query returns the edge's endpoint closest to the target. Again, when the number of queries is bounded by some given $k$, we quantify a \\emph{the-less-queries-the-better} approach by defining a seeker's profit $p$ depending on the number of queries needed to locate the hider. For the linear programming formulation of the corresponding zero-sum game, we show that computing the best response for the hider (i.e., the separation problem of the underlying dual LP) can be done in time $O(n^2 2^{2k})$, where $n$ is the size of the tree. This result allows to compute a Nash equilibrium in polynomial time whenever $k=O(\\log n)$. In contrast, computing the best response for the hider is NP-hard.","sentences":["We study a generalized binary search problem on the line and general trees.","On the line (e.g., a sorted array), binary search finds a target node in $O(\\log n)$ queries in the worst case, where $n$ is the number of nodes.","In situations with limited budget or time, we might only be able to perform a few queries, possibly sub-logarithmic many.","In this case, it is impossible to guarantee that the target will be found regardless of its position.","Our main result is the construction of a randomized strategy that maximizes the minimum (over the target position) probability of finding the target.","Such a strategy provides a natural solution where there is no apriori (stochastic) information of the target's position.","As with regular binary search, we can find and run the strategy in $O(\\log n)$ time (and using only $O(\\log n)$ random bits).","Our construction is obtained by reinterpreting the problem as a two-player (\\textit{seeker} and \\textit{hider}) zero-sum game and exploiting an underlying number theoretical structure.   ","Furthermore, we generalize the setting to study a search game on trees.","In this case, a query returns the edge's endpoint closest to the target.","Again, when the number of queries is bounded by some given $k$, we quantify a \\emph{the-less-queries-the-better} approach by defining a seeker's profit $p$ depending on the number of queries needed to locate the hider.","For the linear programming formulation of the corresponding zero-sum game, we show that computing the best response for the hider (i.e., the separation problem of the underlying dual LP) can be done in time $O(n^2 2^{2k})$, where $n$ is the size of the tree.","This result allows to compute a Nash equilibrium in polynomial time whenever $k=O(\\log n)$. In contrast, computing the best response for the hider is NP-hard."],"url":"http://arxiv.org/abs/2406.06468v1","category":"cs.DS"}
{"created":"2024-06-10 17:05:12","title":"How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad","abstract":"Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target. As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains. Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks.","sentences":["Can Transformers predict new syllogisms by composing established ones?","More generally, what type of targets can be learned by such models from scratch?","Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective.","This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target.","As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently.","In particular, syllogisms cannot be composed on long chains.","Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks."],"url":"http://arxiv.org/abs/2406.06467v1","category":"cs.LG"}
{"created":"2024-06-10 17:02:08","title":"AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction","abstract":"Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation. Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task. However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets. We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control. Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task. To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions. More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction. Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs. Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains. More examples can be found at our website https://chenhsing.github.io/AID.","sentences":["Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation.","Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task.","However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets.","We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control.","Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task.","To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions.","More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction.","Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs.","Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101.","Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains.","More examples can be found at our website https://chenhsing.github.io/AID."],"url":"http://arxiv.org/abs/2406.06465v1","category":"cs.CV"}
{"created":"2024-06-10 17:00:54","title":"Transforming Wearable Data into Health Insights using Large Language Model Agents","abstract":"Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data. The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale. Yet, the application of LLM agents in analyzing personal health is still largely untapped. In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables. We curate two benchmark question-answering datasets of over 4000 health insights questions. Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84% of factual numerical questions and more than 83% of crowd-sourced open-ended questions. This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights.","sentences":["Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data.","The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale.","Yet, the application of LLM agents in analyzing personal health is still largely untapped.","In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables.","We curate two benchmark question-answering datasets of over 4000 health insights questions.","Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84% of factual numerical questions and more than 83% of crowd-sourced open-ended questions.","This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights."],"url":"http://arxiv.org/abs/2406.06464v1","category":"cs.AI"}
{"created":"2024-06-10 16:58:48","title":"VCR: Visual Caption Restoration","abstract":"We introduce Visual Caption Restoration (VCR), a novel vision-language task that challenges models to accurately restore partially obscured texts using pixel-level hints within images. This task stems from the observation that text embedded in images is intrinsically different from common visual elements and natural language due to the need to align the modalities of vision, text, and text embedded in images. While numerous works have integrated text embedded in images into visual question-answering tasks, approaches to these tasks generally rely on optical character recognition or masked language modeling, thus reducing the task to mainly text-based processing. However, text-based processing becomes ineffective in VCR as accurate text restoration depends on the combined information from provided images, context, and subtle cues from the tiny exposed areas of masked texts. We develop a pipeline to generate synthetic images for the VCR task using image-caption pairs, with adjustable caption visibility to control the task difficulty. With this pipeline, we construct a dataset for VCR called VCR-Wiki using images with captions from Wikipedia, comprising 2.11M English and 346K Chinese entities in both easy and hard split variants. Our results reveal that current vision language models significantly lag behind human performance in the VCR task, and merely fine-tuning the models on our dataset does not lead to notable improvements. We release VCR-Wiki and the data construction code to facilitate future research.","sentences":["We introduce Visual Caption Restoration (VCR), a novel vision-language task that challenges models to accurately restore partially obscured texts using pixel-level hints within images.","This task stems from the observation that text embedded in images is intrinsically different from common visual elements and natural language due to the need to align the modalities of vision, text, and text embedded in images.","While numerous works have integrated text embedded in images into visual question-answering tasks, approaches to these tasks generally rely on optical character recognition or masked language modeling, thus reducing the task to mainly text-based processing.","However, text-based processing becomes ineffective in VCR as accurate text restoration depends on the combined information from provided images, context, and subtle cues from the tiny exposed areas of masked texts.","We develop a pipeline to generate synthetic images for the VCR task using image-caption pairs, with adjustable caption visibility to control the task difficulty.","With this pipeline, we construct a dataset for VCR called VCR-Wiki using images with captions from Wikipedia, comprising 2.11M English and 346K Chinese entities in both easy and hard split variants.","Our results reveal that current vision language models significantly lag behind human performance in the VCR task, and merely fine-tuning the models on our dataset does not lead to notable improvements.","We release VCR-Wiki and the data construction code to facilitate future research."],"url":"http://arxiv.org/abs/2406.06462v1","category":"cs.CV"}
{"created":"2024-06-10 16:54:51","title":"Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots","abstract":"In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp.","sentences":["In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions.","Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error.","Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success.","For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback.","Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location.","We assessed our approach through a series of experiments in both simulated and real-world environments.","The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp."],"url":"http://arxiv.org/abs/2406.06460v1","category":"cs.RO"}
{"created":"2024-06-10 16:46:22","title":"Evaluating the Retrieval Component in LLM-Based Question Answering Systems","abstract":"Question answering systems (QA) utilizing Large Language Models (LLMs) heavily depend on the retrieval component to provide them with domain-specific information and reduce the risk of generating inaccurate responses or hallucinations. Although the evaluation of retrievers dates back to the early research in Information Retrieval, assessing their performance within LLM-based chatbots remains a challenge.   This study proposes a straightforward baseline for evaluating retrievers in Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate that this evaluation framework provides a better image of how the retriever performs and is more aligned with the overall performance of the QA system. Although conventional metrics such as precision, recall, and F1 score may not fully capture LLMs' capabilities - as they can yield accurate responses despite imperfect retrievers - our method considers LLMs' strengths to ignore irrelevant contexts, as well as potential errors and hallucinations in their responses.","sentences":["Question answering systems (QA) utilizing Large Language Models (LLMs) heavily depend on the retrieval component to provide them with domain-specific information and reduce the risk of generating inaccurate responses or hallucinations.","Although the evaluation of retrievers dates back to the early research in Information Retrieval, assessing their performance within LLM-based chatbots remains a challenge.   ","This study proposes a straightforward baseline for evaluating retrievers in Retrieval-Augmented Generation (RAG)-based chatbots.","Our findings demonstrate that this evaluation framework provides a better image of how the retriever performs and is more aligned with the overall performance of the QA system.","Although conventional metrics such as precision, recall, and F1 score may not fully capture LLMs' capabilities - as they can yield accurate responses despite imperfect retrievers - our method considers LLMs' strengths to ignore irrelevant contexts, as well as potential errors and hallucinations in their responses."],"url":"http://arxiv.org/abs/2406.06458v1","category":"cs.CL"}
{"created":"2024-06-10 16:46:13","title":"Improved convergence rates for the multiobjective Frank-Wolfe method","abstract":"This paper analyzes the convergence rates of the {\\it Frank-Wolfe } method for solving convex constrained multiobjective optimization. We establish improved convergence rates under different assumptions on the objective function, the feasible set, and the localization of the limit point of the sequence generated by the method. In terms of the objective function values, we firstly show that if the objective function is strongly convex and the limit point of the sequence generated by the method lies in the relative interior of the feasible set, then the algorithm achieves a linear convergence rate. Next, we focus on a special class of problems where the feasible constraint set is $(\\alpha,q)$-uniformly convex for some $\\alpha >0$ and $q \\geq 2$, including, in particular, \\(\\ell_p\\)-balls for all $p>1$. In this context, we prove that the method attains: (i) a rate of $\\mathcal{O}(1/k^\\frac{q}{q-1})$ when the objective function is strongly convex; and (ii) a linear rate (if $q=2$) or a rate of $\\mathcal{O}(1/k^{\\frac{q}{q-2}})$ (if $q>2$) under an additional assumption, which always holds if the feasible set does not contain an unconstrained weak Pareto point. We also discuss enhanced convergence rates for the algorithm in terms of an optimality measure. Finally, we provide some simple examples to illustrate the convergence rates and the set of assumptions.","sentences":["This paper analyzes the convergence rates of the {\\it Frank-Wolfe } method for solving convex constrained multiobjective optimization.","We establish improved convergence rates under different assumptions on the objective function, the feasible set, and the localization of the limit point of the sequence generated by the method.","In terms of the objective function values, we firstly show that if the objective function is strongly convex and the limit point of the sequence generated by the method lies in the relative interior of the feasible set, then the algorithm achieves a linear convergence rate.","Next, we focus on a special class of problems where the feasible constraint set is $(\\alpha,q)$-uniformly convex for some $\\alpha >0$ and $q \\geq 2$, including, in particular, \\(\\ell_p\\)-balls for all $p>1$. In this context, we prove that the method attains: (i) a rate of $\\mathcal{O}(1/k^\\frac{q}{q-1})$ when the objective function is strongly convex; and (ii) a linear rate (if $q=2$) or a rate of $\\mathcal{O}(1/k^{\\frac{q}{q-2}})$ (if $q>2$) under an additional assumption, which always holds if the feasible set does not contain an unconstrained weak Pareto point.","We also discuss enhanced convergence rates for the algorithm in terms of an optimality measure.","Finally, we provide some simple examples to illustrate the convergence rates and the set of assumptions."],"url":"http://arxiv.org/abs/2406.06457v1","category":"math.OC"}
{"created":"2024-06-10 16:44:48","title":"A Large Language Model Pipeline for Breast Cancer Oncology","abstract":"Large language models (LLMs) have demonstrated potential in the innovation of many disciplines. However, how they can best be developed for oncology remains underdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical dataset and clinical guidelines text corpus for two important cancer treatment factors, adjuvant radiation therapy and chemotherapy, using a novel Langchain prompt engineering pipeline. A high accuracy (0.85+) was achieved in the classification of adjuvant radiation therapy and chemotherapy for breast cancer patients. Furthermore, a confidence interval was formed from observational data on the quality of treatment from human oncologists to estimate the proportion of scenarios in which the model must outperform the original oncologist in its treatment prediction to be a better solution overall as 8.2% to 13.3%. Due to indeterminacy in the outcomes of cancer treatment decisions, future investigation, potentially a clinical trial, would be required to determine if this threshold was met by the models. Nevertheless, with 85% of U.S. cancer patients receiving treatment at local community facilities, these kinds of models could play an important part in expanding access to quality care with outcomes that lie, at minimum, close to a human oncologist.","sentences":["Large language models (LLMs) have demonstrated potential in the innovation of many disciplines.","However, how they can best be developed for oncology remains underdeveloped.","State-of-the-art OpenAI models were fine-tuned on a clinical dataset and clinical guidelines text corpus for two important cancer treatment factors, adjuvant radiation therapy and chemotherapy, using a novel Langchain prompt engineering pipeline.","A high accuracy (0.85+) was achieved in the classification of adjuvant radiation therapy and chemotherapy for breast cancer patients.","Furthermore, a confidence interval was formed from observational data on the quality of treatment from human oncologists to estimate the proportion of scenarios in which the model must outperform the original oncologist in its treatment prediction to be a better solution overall as 8.2% to 13.3%.","Due to indeterminacy in the outcomes of cancer treatment decisions, future investigation, potentially a clinical trial, would be required to determine if this threshold was met by the models.","Nevertheless, with 85% of U.S. cancer patients receiving treatment at local community facilities, these kinds of models could play an important part in expanding access to quality care with outcomes that lie, at minimum, close to a human oncologist."],"url":"http://arxiv.org/abs/2406.06455v1","category":"cs.AI"}
{"created":"2024-06-10 16:40:14","title":"Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course","abstract":"The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education. However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities. Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage. We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course. Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage. Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course.","sentences":["The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education.","However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities.","Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage.","We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course.","Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage.","Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course."],"url":"http://arxiv.org/abs/2406.06451v1","category":"cs.HC"}
{"created":"2024-06-10 16:39:39","title":"Cometh: A continuous-time discrete-state graph diffusion model","abstract":"Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, integrating graph data into a continuous-time diffusion model framework. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets.","sentences":["Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain.","Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality.","Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, integrating graph data into a continuous-time diffusion model framework.","Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets."],"url":"http://arxiv.org/abs/2406.06449v1","category":"cs.LG"}
{"created":"2024-06-10 16:36:02","title":"Deep Generative Modeling Reshapes Compression and Transmission: From Efficiency to Resiliency","abstract":"Information theory and machine learning are inextricably linked and have even been referred to as \"two sides of the same coin\". One particularly elegant connection is the essential equivalence between probabilistic generative modeling and data compression or transmission. In this article, we reveal the dual-functionality of deep generative models that reshapes both data compression for efficiency and transmission error concealment for resiliency. We present how the contextual predictive capabilities of powerful generative models can be well positioned to be strong compressors and estimators. In this sense, we advocate for viewing the deep generative modeling problem through the lens of end-to-end communications, and evaluate the compression and error restoration capabilities of foundation generative models. We show that the kernel of many large generative models is powerful predictor that can capture complex relationships among semantic latent variables, and the communication viewpoints provide novel insights into semantic feature tokenization, contextual learning, and usage of deep generative models. In summary, our article highlights the essential connections of generative AI to source and channel coding techniques, and motivates researchers to make further explorations in this emerging topic.","sentences":["Information theory and machine learning are inextricably linked and have even been referred to as \"two sides of the same coin\".","One particularly elegant connection is the essential equivalence between probabilistic generative modeling and data compression or transmission.","In this article, we reveal the dual-functionality of deep generative models that reshapes both data compression for efficiency and transmission error concealment for resiliency.","We present how the contextual predictive capabilities of powerful generative models can be well positioned to be strong compressors and estimators.","In this sense, we advocate for viewing the deep generative modeling problem through the lens of end-to-end communications, and evaluate the compression and error restoration capabilities of foundation generative models.","We show that the kernel of many large generative models is powerful predictor that can capture complex relationships among semantic latent variables, and the communication viewpoints provide novel insights into semantic feature tokenization, contextual learning, and usage of deep generative models.","In summary, our article highlights the essential connections of generative AI to source and channel coding techniques, and motivates researchers to make further explorations in this emerging topic."],"url":"http://arxiv.org/abs/2406.06446v1","category":"cs.IT"}
{"created":"2024-06-10 16:35:52","title":"Parallel Quantum Local Search via Evolutionary Mechanism","abstract":"We propose an innovative Parallel Quantum Local Search (PQLS) methodology that leverages the capabilities of small-scale quantum computers to efficiently address complex combinatorial optimization problems. Traditional Quantum Local Search (QLS) methods face limitations due to the sequential nature of solving sub-problems, which arises from dependencies between their solutions. Our approach transcends this constraint by simultaneously executing multiple QLS pathways and aggregating their most effective outcomes at certain intervals to establish a ``generation''. Each subsequent generation commences with the optimal solution from its predecessor, thereby significantly accelerating the convergence towards an optimal solution. Our findings demonstrate the profound impact of parallel quantum computing in enhancing the resolution of Ising problems, which are synonymous with combinatorial optimization challenges.","sentences":["We propose an innovative Parallel Quantum Local Search (PQLS) methodology that leverages the capabilities of small-scale quantum computers to efficiently address complex combinatorial optimization problems.","Traditional Quantum Local Search (QLS) methods face limitations due to the sequential nature of solving sub-problems, which arises from dependencies between their solutions.","Our approach transcends this constraint by simultaneously executing multiple QLS pathways and aggregating their most effective outcomes at certain intervals to establish a ``generation''.","Each subsequent generation commences with the optimal solution from its predecessor, thereby significantly accelerating the convergence towards an optimal solution.","Our findings demonstrate the profound impact of parallel quantum computing in enhancing the resolution of Ising problems, which are synonymous with combinatorial optimization challenges."],"url":"http://arxiv.org/abs/2406.06445v1","category":"quant-ph"}
{"created":"2024-06-10 16:34:30","title":"Interpretability of Language Models via Task Spaces","abstract":"The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.","sentences":["The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes.","In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities.","To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena.","Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'.","To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD).","We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure.","Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks.","The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."],"url":"http://arxiv.org/abs/2406.06441v1","category":"cs.CL"}
{"created":"2024-06-10 16:33:23","title":"Messengers: Breaking Echo Chambers in Collective Opinion Dynamics with Homophily","abstract":"Collective estimation manifests computational intelligence emerging from inter-individual local interactions, e.g., by aggregating opinions from neighbors to estimate a quantity. Use cases of collective estimation may include directed motion in physical space, such that agents, for example, have to collectively explore a distributed feature, and collectively agree on a numerical value. In doing so, collectives face several challenges in achieving precise estimations. These challenges exhibit complex behaviors, particularly when the interaction network and opinion of agents evolve simultaneously. We take homophilic networks as an example, where disproportionate interaction with like-minded neighbors leads to the emergence of echo chambers, preventing collective consensus. Our simulation results confirm that, besides a lack of exposure to attitude-challenging opinions, seeking reaffirming information entraps agents in echo chambers. We propose a generic novel approach based on a Dichotomous Markov Process (DMP) where stubborn agents (called Messengers) connect the disconnected clusters by physically transporting their opinions to other clusters to inform and direct the other agents. We show that diverse collective behaviors arise from the DMP and study a continuum between task specialization with no switching (full-time Messengers), generalization with slow task switching (part-time Messengers), and rapid task switching (short-time Messengers) and its impact on system performance. Our results show that stubborn agents can, in various ways, break the echo chambers and promote consensus in collective opinion.","sentences":["Collective estimation manifests computational intelligence emerging from inter-individual local interactions, e.g., by aggregating opinions from neighbors to estimate a quantity.","Use cases of collective estimation may include directed motion in physical space, such that agents, for example, have to collectively explore a distributed feature, and collectively agree on a numerical value.","In doing so, collectives face several challenges in achieving precise estimations.","These challenges exhibit complex behaviors, particularly when the interaction network and opinion of agents evolve simultaneously.","We take homophilic networks as an example, where disproportionate interaction with like-minded neighbors leads to the emergence of echo chambers, preventing collective consensus.","Our simulation results confirm that, besides a lack of exposure to attitude-challenging opinions, seeking reaffirming information entraps agents in echo chambers.","We propose a generic novel approach based on a Dichotomous Markov Process (DMP) where stubborn agents (called Messengers) connect the disconnected clusters by physically transporting their opinions to other clusters to inform and direct the other agents.","We show that diverse collective behaviors arise from the DMP and study a continuum between task specialization with no switching (full-time Messengers), generalization with slow task switching (part-time Messengers), and rapid task switching (short-time Messengers) and its impact on system performance.","Our results show that stubborn agents can, in various ways, break the echo chambers and promote consensus in collective opinion."],"url":"http://arxiv.org/abs/2406.06440v1","category":"cs.SI"}
{"created":"2024-06-10 16:31:34","title":"Multimodal Contextualized Semantic Parsing from Speech","abstract":"We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.","sentences":["We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts.","SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication.","We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration.","We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.","These innovations aim to improve multimodal information processing and integration.","Both the VG-SPICE dataset and the AViD-SP model are publicly available."],"url":"http://arxiv.org/abs/2406.06438v1","category":"cs.CL"}
{"created":"2024-06-10 16:29:38","title":"Solitary waves and kinks in FPU lattices with soft-hard-soft trilinear interactions","abstract":"We consider a version of the classical Hamiltonian Fermi-Pasta-Ulam (FPU) problem with a trilinear force-strain relation of soft-hard-soft type that is in general non-symmetric. In addition to the classical spatially localized solitary waves, such hardening-softening model also exhibits supersonic kinks and finite-amplitude, spatially delocalized flat-top solitary waves that acquire the structure of a kink-antikink bundle when their velocity approaches the kink limit. Exploiting the fact that traveling waves are periodic modulo shift by a lattice spacing, we compute these solutions as fixed points of the corresponding nonlinear map and investigate how their properties depend on the parameter measuring the asymmetry of the problem. In a particularly interesting case when one of the soft regimes has zero elastic modulus, we obtain explicit solutions for sufficiently slow solitary waves. In contrast to conventional delocalization in the sonic limit, these compact structures mounted on a constant background become localized at the lattice scale as their velocity tends to zero. Numerical simulations of Riemann-type initial value problem in this degenerate model show the emergence of Whitham shocks that involve periodic trains of solitary waves. We investigate stability of the obtained solutions using direct numerical simulations and Floquet analysis. We also obtain explicit solutions for a quasicontinuum model that captures some important features of the discrete problem.","sentences":["We consider a version of the classical Hamiltonian Fermi-Pasta-Ulam (FPU) problem with a trilinear force-strain relation of soft-hard-soft type that is in general non-symmetric.","In addition to the classical spatially localized solitary waves, such hardening-softening model also exhibits supersonic kinks and finite-amplitude, spatially delocalized flat-top solitary waves that acquire the structure of a kink-antikink bundle when their velocity approaches the kink limit.","Exploiting the fact that traveling waves are periodic modulo shift by a lattice spacing, we compute these solutions as fixed points of the corresponding nonlinear map and investigate how their properties depend on the parameter measuring the asymmetry of the problem.","In a particularly interesting case when one of the soft regimes has zero elastic modulus, we obtain explicit solutions for sufficiently slow solitary waves.","In contrast to conventional delocalization in the sonic limit, these compact structures mounted on a constant background become localized at the lattice scale as their velocity tends to zero.","Numerical simulations of Riemann-type initial value problem in this degenerate model show the emergence of Whitham shocks that involve periodic trains of solitary waves.","We investigate stability of the obtained solutions using direct numerical simulations and Floquet analysis.","We also obtain explicit solutions for a quasicontinuum model that captures some important features of the discrete problem."],"url":"http://arxiv.org/abs/2406.06437v1","category":"nlin.PS"}
{"created":"2024-06-10 16:25:23","title":"Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain","abstract":"In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize an individual's decision. We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs). This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert. We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails. Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting. Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2. Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance. Our results provide new research directions in the use of LLMs as alignable decision-makers. The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm.","sentences":["In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer.","Such decisions may be guided by different attributes that can be used to characterize an individual's decision.","We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs).","This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert.","We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails.","Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting.","Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2.","Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance.","Our results provide new research directions in the use of LLMs as alignable decision-makers.","The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm."],"url":"http://arxiv.org/abs/2406.06435v1","category":"cs.CL"}
{"created":"2024-06-10 16:24:35","title":"DISCO: An End-to-End Bandit Framework for Personalised Discount Allocation","abstract":"Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce. Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment. Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS.com. DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control. Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy. Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\"). These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network. These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base. These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity. Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint. Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems.","sentences":["Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce.","Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment.","Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS.com.","DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control.","Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy.","Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\").","These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network.","These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base.","These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity.","Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint.","Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems."],"url":"http://arxiv.org/abs/2406.06433v1","category":"cs.LG"}
{"created":"2024-06-10 16:24:07","title":"SYM3D: Learning Symmetric Triplanes for Better 3D-Awareness of GANs","abstract":"Despite the growing success of 3D-aware GANs, which can be trained on 2D images to generate high-quality 3D assets, they still rely on multi-view images with camera annotations to synthesize sufficient details from all viewing directions. However, the scarce availability of calibrated multi-view image datasets, especially in comparison to single-view images, has limited the potential of 3D GANs. Moreover, while bypassing camera pose annotations with a camera distribution constraint reduces dependence on exact camera parameters, it still struggles to generate a consistent orientation of 3D assets. To this end, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent reflectional symmetry structure found in natural and man-made objects, alongside a proposed view-aware spatial attention mechanism in learning the 3D representation. We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and Airplanes) and real-world datasets (ABO-Chair), demonstrating its superior performance in capturing detailed geometry and texture, even when trained on only single-view images. Finally, we demonstrate the effectiveness of incorporating symmetry regularization in helping reduce artifacts in the modeling of 3D assets in the text-to-3D task.","sentences":["Despite the growing success of 3D-aware GANs, which can be trained on 2D images to generate high-quality 3D assets, they still rely on multi-view images with camera annotations to synthesize sufficient details from all viewing directions.","However, the scarce availability of calibrated multi-view image datasets, especially in comparison to single-view images, has limited the potential of 3D GANs.","Moreover, while bypassing camera pose annotations with a camera distribution constraint reduces dependence on exact camera parameters, it still struggles to generate a consistent orientation of 3D assets.","To this end, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent reflectional symmetry structure found in natural and man-made objects, alongside a proposed view-aware spatial attention mechanism in learning the 3D representation.","We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and Airplanes) and real-world datasets (ABO-Chair), demonstrating its superior performance in capturing detailed geometry and texture, even when trained on only single-view images.","Finally, we demonstrate the effectiveness of incorporating symmetry regularization in helping reduce artifacts in the modeling of 3D assets in the text-to-3D task."],"url":"http://arxiv.org/abs/2406.06432v1","category":"cs.CV"}
{"created":"2024-06-10 16:14:45","title":"Margin-aware Preference Optimization for Aligning Diffusion Models without Reference","abstract":"Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability. However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model. In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy. Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO). MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences. For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch. Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods. Our code, models, and datasets are publicly available via https://mapo-t2i.github.io","sentences":["Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability.","However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model.","In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy.","Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO).","MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences.","For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch.","Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods.","Our code, models, and datasets are publicly available via https://mapo-t2i.github.io"],"url":"http://arxiv.org/abs/2406.06424v1","category":"cs.CV"}
{"created":"2024-06-10 16:09:16","title":"Explainable Graph Neural Networks Under Fire","abstract":"Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs. In an attempt to tackle this, many GNN explanation methods have emerged. Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications. Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes. In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations. That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations. This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs. To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings. Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness.","sentences":["Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs.","In an attempt to tackle this, many GNN explanation methods have emerged.","Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications.","Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes.","In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations.","That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations.","This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs.","To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings.","Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness."],"url":"http://arxiv.org/abs/2406.06417v1","category":"cs.LG"}
{"created":"2024-06-10 16:08:47","title":"Discovering classical spin liquids by topological search of high symmetry nets","abstract":"Spin liquids are a paradigmatic example of a non-trivial state of matter, and the search for new spin liquids is a key direction of physics, chemistry, and materials science. Geometric frustration -- where the geometry of the net that the spins occupy precludes the generation of a simple ordered state -- is a particularly fruitful way to generate these intrinsically disordered states. A particular focus has been on a handful of high symmetry nets. There are, however, many three-dimensional nets, each of which has the potential to form unique states. In this paper, we investigate the high symmetry nets -- those which are both node- and edge-transitive -- for the simplest possible interaction sets: nearest-neighbour couplings of antiferromagnetic Heisenberg and Ising spins. While the well-known crs (pyrochlore) net is the only nearest-neighbour Heisenberg antiferromagnet which does not order, we identify two new frustrated nets (lcx and thp) that possess finite temperature Heisenberg spin-liquid states with strongly suppressed magnetic ordering and non-collinear ground states. With Ising spins, we identify four new classical spin liquids that do not order down to $T/J = 10^{-2}$. We highlight materials that contain these high symmetry nets, and which could, if substituted with appropriate magnetic ions, potentially host these unusual states. Our systematic survey will guide searches for novel magnetic phases.","sentences":["Spin liquids are a paradigmatic example of a non-trivial state of matter, and the search for new spin liquids is a key direction of physics, chemistry, and materials science.","Geometric frustration -- where the geometry of the net that the spins occupy precludes the generation of a simple ordered state -- is a particularly fruitful way to generate these intrinsically disordered states.","A particular focus has been on a handful of high symmetry nets.","There are, however, many three-dimensional nets, each of which has the potential to form unique states.","In this paper, we investigate the high symmetry nets -- those which are both node- and edge-transitive -- for the simplest possible interaction sets: nearest-neighbour couplings of antiferromagnetic Heisenberg and Ising spins.","While the well-known crs (pyrochlore) net is the only nearest-neighbour Heisenberg antiferromagnet which does not order, we identify two new frustrated nets (lcx and thp) that possess finite temperature Heisenberg spin-liquid states with strongly suppressed magnetic ordering and non-collinear ground states.","With Ising spins, we identify four new classical spin liquids that do not order down to $T/J = 10^{-2}$. We highlight materials that contain these high symmetry nets, and which could, if substituted with appropriate magnetic ions, potentially host these unusual states.","Our systematic survey will guide searches for novel magnetic phases."],"url":"http://arxiv.org/abs/2406.06416v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 16:08:31","title":"Quadratic dispersion relations in gapless frustration-free systems","abstract":"Recent case-by-case studies revealed that the dispersion of low energy excitations in gapless frustration-free Hamiltonians is often quadratic or softer. In this work, we argue that this is actually a general property of such systems. By combining a previous study by Bravyi and Gosset and the min-max principle, we prove this hypothesis for models with local Hilbert spaces of dimension two that contains only nearest-neighbor interactions on cubic lattice. This may be understood as a no-go theorem realizing gapless phases with linearly dispersive excitations in frustration-free Hamiltonians. We also provide examples of frustration-free Hamiltonians in which the plane-wave state of a single spin flip does not constitute low energy excitations.","sentences":["Recent case-by-case studies revealed that the dispersion of low energy excitations in gapless frustration-free Hamiltonians is often quadratic or softer.","In this work, we argue that this is actually a general property of such systems.","By combining a previous study by Bravyi and Gosset and the min-max principle, we prove this hypothesis for models with local Hilbert spaces of dimension two that contains only nearest-neighbor interactions on cubic lattice.","This may be understood as a no-go theorem realizing gapless phases with linearly dispersive excitations in frustration-free Hamiltonians.","We also provide examples of frustration-free Hamiltonians in which the plane-wave state of a single spin flip does not constitute low energy excitations."],"url":"http://arxiv.org/abs/2406.06414v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 16:02:59","title":"On the structure of the value function of optimal exit time problems","abstract":"In this paper, we study an optimal exit time problem with general running and terminal costs and a target $\\mathcal{S}\\subset\\mathbb{R}^d$ having an inner ball property for a nonlinear control system that satisfies mild controllability assumptions. In particular, Petrov's condition at the boundary of $\\mathcal{S}$ is not required and the value function $V$ may fail to be locally Lipschitz. In such a weakened set-up, we first establish a representation formula for proximal (horizontal) supergradients of $V$ by using transported proximal normal vectors. This allows us to obtain an external sphere condition for the hypograph of $V$ which yields several regularity properties. In particular, $V$ is almost everywhere twice differentiable and the Hausdorff dimension of its singularities is not greater than $d-1/2$. Furthermore, besides optimality conditions for trajectories of the optimal control problem, we extend the analysis to propagation of singularities and differentiability properties of the value function. An upper bound for the Hausdorff measure of the singular set is also studied, which implies that $V$ is a function of special bounded variation.","sentences":["In this paper, we study an optimal exit time problem with general running and terminal costs and a target $\\mathcal{S}\\subset\\mathbb{R}^d$ having an inner ball property for a nonlinear control system that satisfies mild controllability assumptions.","In particular, Petrov's condition at the boundary of $\\mathcal{S}$ is not required and the value function $V$ may fail to be locally Lipschitz.","In such a weakened set-up, we first establish a representation formula for proximal (horizontal) supergradients of $V$ by using transported proximal normal vectors.","This allows us to obtain an external sphere condition for the hypograph of $V$ which yields several regularity properties.","In particular, $V$ is almost everywhere twice differentiable and the Hausdorff dimension of its singularities is not greater than $d-1/2$. Furthermore, besides optimality conditions for trajectories of the optimal control problem, we extend the analysis to propagation of singularities and differentiability properties of the value function.","An upper bound for the Hausdorff measure of the singular set is also studied, which implies that $V$ is a function of special bounded variation."],"url":"http://arxiv.org/abs/2406.06409v1","category":"math.OC"}
{"created":"2024-06-10 15:58:42","title":"Controlling Emotion in Text-to-Speech with Natural Language Prompts","abstract":"In recent years, prompting has quickly become one of the standard ways of steering the outputs of generative machine learning models, due to its intuitive use of natural language. In this work, we propose a system conditioned on embeddings derived from an emotionally rich text that serves as prompt. Thereby, a joint representation of speaker and prompt embeddings is integrated at several points within a transformer-based architecture. Our approach is trained on merged emotional speech and text datasets and varies prompts in each training iteration to increase the generalization capabilities of the model. Objective and subjective evaluation results demonstrate the ability of the conditioned synthesis system to accurately transfer the emotions present in a prompt to speech. At the same time, precise tractability of speaker identities as well as overall high speech quality and intelligibility are maintained.","sentences":["In recent years, prompting has quickly become one of the standard ways of steering the outputs of generative machine learning models, due to its intuitive use of natural language.","In this work, we propose a system conditioned on embeddings derived from an emotionally rich text that serves as prompt.","Thereby, a joint representation of speaker and prompt embeddings is integrated at several points within a transformer-based architecture.","Our approach is trained on merged emotional speech and text datasets and varies prompts in each training iteration to increase the generalization capabilities of the model.","Objective and subjective evaluation results demonstrate the ability of the conditioned synthesis system to accurately transfer the emotions present in a prompt to speech.","At the same time, precise tractability of speaker identities as well as overall high speech quality and intelligibility are maintained."],"url":"http://arxiv.org/abs/2406.06406v1","category":"cs.CL"}
{"created":"2024-06-10 15:57:56","title":"Relevant information in TDD experiment reporting","abstract":"Experiments are a commonly used method of research in software engineering (SE). Researchers report their experiments following detailed guidelines. However, researchers do not, in the field of test-driven development (TDD) at least, specify how they operationalized the response variables and the measurement process. This article has three aims: (i) identify the response variable operationalization components in TDD experiments that study external quality; (ii) study their influence on the experimental results;(ii) determine if the experiment reports describe the measurement process components that have an impact on the results. Sequential mixed method. The first part of the research adopts a quantitative approach applying a statistical an\\'alisis (SA) of the impact of the operationalization components on the experimental results. The second part follows on with a qualitative approach applying a systematic mapping study (SMS). The test suites, intervention types and measurers have an influence on the measurements and results of the SA of TDD experiments in SE. The test suites have a major impact on both the measurements and the results of the experiments. The intervention type has less impact on the results than on the measurements. While the measurers have an impact on the measurements, this is not transferred to the experimental results. On the other hand, the results of our SMS confirm that TDD experiments do not usually report either the test suites, the test case generation method, or the details of how external quality was measured. A measurement protocol should be used to assure that the measurements made by different measurers are similar. It is necessary to report the test cases, the experimental task and the intervention type in order to be able to reproduce the measurements and SA, as well as to replicate experiments and build dependable families of experiments.","sentences":["Experiments are a commonly used method of research in software engineering (SE).","Researchers report their experiments following detailed guidelines.","However, researchers do not, in the field of test-driven development (TDD) at least, specify how they operationalized the response variables and the measurement process.","This article has three aims: (i) identify the response variable operationalization components in TDD experiments that study external quality; (ii) study their influence on the experimental results;(ii) determine if the experiment reports describe the measurement process components that have an impact on the results.","Sequential mixed method.","The first part of the research adopts a quantitative approach applying a statistical an\\'alisis (SA) of the impact of the operationalization components on the experimental results.","The second part follows on with a qualitative approach applying a systematic mapping study (SMS).","The test suites, intervention types and measurers have an influence on the measurements and results of the SA of TDD experiments in SE.","The test suites have a major impact on both the measurements and the results of the experiments.","The intervention type has less impact on the results than on the measurements.","While the measurers have an impact on the measurements, this is not transferred to the experimental results.","On the other hand, the results of our SMS confirm that TDD experiments do not usually report either the test suites, the test case generation method, or the details of how external quality was measured.","A measurement protocol should be used to assure that the measurements made by different measurers are similar.","It is necessary to report the test cases, the experimental task and the intervention type in order to be able to reproduce the measurements and SA, as well as to replicate experiments and build dependable families of experiments."],"url":"http://arxiv.org/abs/2406.06405v1","category":"cs.SE"}
{"created":"2024-06-10 15:56:52","title":"Meta Learning Text-to-Speech Synthesis in over 7000 Languages","abstract":"In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development. By leveraging a novel integration of massively multilingual pretraining and meta learning to approximate language representations, our approach enables zero-shot speech synthesis in languages without any available data. We validate our system's performance through objective measures and human evaluation across a diverse linguistic landscape. By releasing our code and models publicly, we aim to empower communities with limited linguistic resources and foster further innovation in the field of speech technology.","sentences":["In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development.","By leveraging a novel integration of massively multilingual pretraining and meta learning to approximate language representations, our approach enables zero-shot speech synthesis in languages without any available data.","We validate our system's performance through objective measures and human evaluation across a diverse linguistic landscape.","By releasing our code and models publicly, we aim to empower communities with limited linguistic resources and foster further innovation in the field of speech technology."],"url":"http://arxiv.org/abs/2406.06403v1","category":"cs.CL"}
{"created":"2024-06-10 15:53:50","title":"An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics","abstract":"The integration of Large Language Models (LLMs) in social robotics presents a unique set of ethical challenges and social impacts. This research is set out to identify ethical considerations that arise in the design and development of these two technologies in combination. Using LLMs for social robotics may provide benefits, such as enabling natural language open-domain dialogues. However, the intersection of these two technologies also gives rise to ethical concerns related to misinformation, non-verbal cues, emotional disruption, and biases. The robot's physical social embodiment adds complexity, as ethical hazards associated with LLM-based Social AI, such as hallucinations and misinformation, can be exacerbated due to the effects of physical embodiment on social perception and communication. To address these challenges, this study employs an empirical design justice-based methodology, focusing on identifying socio-technical ethical considerations through a qualitative co-design and interaction study. The purpose of the study is to identify ethical considerations relevant to the process of co-design of, and interaction with a humanoid social robot as the interface of a LLM, and to evaluate how a design justice methodology can be used in the context of designing LLMs-based social robotics. The findings reveal a mapping of ethical considerations arising in four conceptual dimensions: interaction, co-design, terms of service and relationship and evaluates how a design justice approach can be used empirically in the intersection of LLMs and social robotics.","sentences":["The integration of Large Language Models (LLMs) in social robotics presents a unique set of ethical challenges and social impacts.","This research is set out to identify ethical considerations that arise in the design and development of these two technologies in combination.","Using LLMs for social robotics may provide benefits, such as enabling natural language open-domain dialogues.","However, the intersection of these two technologies also gives rise to ethical concerns related to misinformation, non-verbal cues, emotional disruption, and biases.","The robot's physical social embodiment adds complexity, as ethical hazards associated with LLM-based Social AI, such as hallucinations and misinformation, can be exacerbated due to the effects of physical embodiment on social perception and communication.","To address these challenges, this study employs an empirical design justice-based methodology, focusing on identifying socio-technical ethical considerations through a qualitative co-design and interaction study.","The purpose of the study is to identify ethical considerations relevant to the process of co-design of, and interaction with a humanoid social robot as the interface of a LLM, and to evaluate how a design justice methodology can be used in the context of designing LLMs-based social robotics.","The findings reveal a mapping of ethical considerations arising in four conceptual dimensions: interaction, co-design, terms of service and relationship and evaluates how a design justice approach can be used empirically in the intersection of LLMs and social robotics."],"url":"http://arxiv.org/abs/2406.06400v1","category":"cs.RO"}
{"created":"2024-06-10 15:52:49","title":"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue","abstract":"We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.","sentences":["We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue.","Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain).","However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics.","In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types.","We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.","We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type.","We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge.","We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols.","Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue.","Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics."],"url":"http://arxiv.org/abs/2406.06399v1","category":"cs.CL"}
{"created":"2024-06-10 15:50:45","title":"Contrastive learning of T cell receptor representations","abstract":"Computational prediction of the interaction of T cell receptors (TCRs) and their ligands is a grand challenge in immunology. Despite advances in high-throughput assays, specificity-labelled TCR data remains sparse. In other domains, the pre-training of language models on unlabelled data has been successfully used to address data bottlenecks. However, it is unclear how to best pre-train protein language models for TCR specificity prediction. Here we introduce a TCR language model called SCEPTR (Simple Contrastive Embedding of the Primary sequence of T cell Receptors), capable of data-efficient transfer learning. Through our model, we introduce a novel pre-training strategy combining autocontrastive learning and masked-language modelling, which enables SCEPTR to achieve its state-of-the-art performance. In contrast, existing protein language models and a variant of SCEPTR pre-trained without autocontrastive learning are outperformed by sequence alignment-based methods. We anticipate that contrastive learning will be a useful paradigm to decode the rules of TCR specificity.","sentences":["Computational prediction of the interaction of T cell receptors (TCRs) and their ligands is a grand challenge in immunology.","Despite advances in high-throughput assays, specificity-labelled TCR data remains sparse.","In other domains, the pre-training of language models on unlabelled data has been successfully used to address data bottlenecks.","However, it is unclear how to best pre-train protein language models for TCR specificity prediction.","Here we introduce a TCR language model called SCEPTR (Simple Contrastive Embedding of the Primary sequence of T cell Receptors), capable of data-efficient transfer learning.","Through our model, we introduce a novel pre-training strategy combining autocontrastive learning and masked-language modelling, which enables SCEPTR to achieve its state-of-the-art performance.","In contrast, existing protein language models and a variant of SCEPTR pre-trained without autocontrastive learning are outperformed by sequence alignment-based methods.","We anticipate that contrastive learning will be a useful paradigm to decode the rules of TCR specificity."],"url":"http://arxiv.org/abs/2406.06397v1","category":"q-bio.BM"}
{"created":"2024-06-10 15:49:02","title":"Lightwave-controlled relativistic plasma mirrors","abstract":"We report on attosecond-scale control of high-harmonic and electron emission from plasma mirrors driven by relativistic-intensity near-single-cycle lightwaves at kHz repetition rate. By controlling the waveform of the intense light transient, we reproducibly form a sub-cycle temporal intensity gate at the plasma mirror surface, leading to the observation of extreme ultraviolet spectral continua, characteristic of isolated attosecond pulse generation. We also observe the correlated emission of a waveform-dependent relativistic electron beam, paving the way towards fully lightwave-controlled dynamics of relativistic plasma mirrors.","sentences":["We report on attosecond-scale control of high-harmonic and electron emission from plasma mirrors driven by relativistic-intensity near-single-cycle lightwaves at kHz repetition rate.","By controlling the waveform of the intense light transient, we reproducibly form a sub-cycle temporal intensity gate at the plasma mirror surface, leading to the observation of extreme ultraviolet spectral continua, characteristic of isolated attosecond pulse generation.","We also observe the correlated emission of a waveform-dependent relativistic electron beam, paving the way towards fully lightwave-controlled dynamics of relativistic plasma mirrors."],"url":"http://arxiv.org/abs/2406.06396v1","category":"physics.plasm-ph"}
{"created":"2024-06-10 15:48:08","title":"A Gigabit, DMA-enhanced Open-Source Ethernet Controller for Mixed-Criticality Systems","abstract":"The ongoing revolution in application domains targeting autonomous navigation, first and foremost automotive \"zonalization\", has increased the importance of certain off-chip communication interfaces, particularly Ethernet. The latter will play an essential role in next-generation vehicle architectures as the backbone connecting simultaneously and instantaneously the zonal/domain controllers. There is thereby an incumbent need to introduce a performant Ethernet controller in the open-source HW community, to be used as a proxy for architectural explorations and prototyping of mixed-criticality systems (MCSs). Driven by this trend, in this work, we propose a fully open-source, DMA-enhanced, technology-agnostic Gigabit Ethernet architecture that overcomes the limitations of existing open-source architectures, such as Lowrisc's Ethernet, often tied to FPGA implementation, performance-bound by sub-optimal design choices such as large memory buffers, and in general not mature enough to bridge the gap between academia and industry. Besides the area advantage, the proposed design increases packet transmission speed up to almost 3x compared to Lowrisc's and is validated through implementation and FPGA prototyping into two open-source, heterogeneous MCSs.","sentences":["The ongoing revolution in application domains targeting autonomous navigation, first and foremost automotive \"zonalization\", has increased the importance of certain off-chip communication interfaces, particularly Ethernet.","The latter will play an essential role in next-generation vehicle architectures as the backbone connecting simultaneously and instantaneously the zonal/domain controllers.","There is thereby an incumbent need to introduce a performant Ethernet controller in the open-source HW community, to be used as a proxy for architectural explorations and prototyping of mixed-criticality systems (MCSs).","Driven by this trend, in this work, we propose a fully open-source, DMA-enhanced, technology-agnostic Gigabit Ethernet architecture that overcomes the limitations of existing open-source architectures, such as Lowrisc's Ethernet, often tied to FPGA implementation, performance-bound by sub-optimal design choices such as large memory buffers, and in general not mature enough to bridge the gap between academia and industry.","Besides the area advantage, the proposed design increases packet transmission speed up to almost 3x compared to Lowrisc's and is validated through implementation and FPGA prototyping into two open-source, heterogeneous MCSs."],"url":"http://arxiv.org/abs/2406.06394v1","category":"cs.AR"}
{"created":"2024-06-10 15:45:38","title":"Reconstructing the genealogy of LIGO-Virgo black holes","abstract":"We propose a Bayesian inference framework to predict the merger history of LIGO-Virgo binary black holes, whose binary components may have undergone hierarchical mergers in the past. The framework relies on numerical relativity predictions for the mass, spin, and kick velocity of the remnant black holes. This proposed framework computes the masses, spins, and kicks imparted to the remnant of the parent binaries, given the initial masses and spin magnitudes of the binary constituents. We validate our approach by performing an \"injection study\" based on a constructed sequence of hierarchically-formed binaries. Noise is added to the final binary in the sequence, and the parameters of the 'parent' and 'grandparent' binaries in the merger chain are then reconstructed. This method is then applied to three GWTC-3 events: GW190521, GW200220_061928, GW190426_190642. These events were selected because at least one of the binary companions lies in the putative pair-instability supernova mass gap, in which stellar processes alone cannot produce black holes. Hierarchical mergers offer a natural explanation for the formation of black holes in the pair-instability mass-gap. We use the backward evolution framework to predict the parameters of the parents of the primary companion of these three binaries. Our results indicate that at least one component of these three observed binaries was formed through a prior binary black hole merger. This approach can be readily applied to future high-mass gravitational wave events to predict their formation history under the hierarchical merger assumption.","sentences":["We propose a Bayesian inference framework to predict the merger history of LIGO-Virgo binary black holes, whose binary components may have undergone hierarchical mergers in the past.","The framework relies on numerical relativity predictions for the mass, spin, and kick velocity of the remnant black holes.","This proposed framework computes the masses, spins, and kicks imparted to the remnant of the parent binaries, given the initial masses and spin magnitudes of the binary constituents.","We validate our approach by performing an \"injection study\" based on a constructed sequence of hierarchically-formed binaries.","Noise is added to the final binary in the sequence, and the parameters of the 'parent' and 'grandparent' binaries in the merger chain are then reconstructed.","This method is then applied to three GWTC-3 events: GW190521, GW200220_061928, GW190426_190642.","These events were selected because at least one of the binary companions lies in the putative pair-instability supernova mass gap, in which stellar processes alone cannot produce black holes.","Hierarchical mergers offer a natural explanation for the formation of black holes in the pair-instability mass-gap.","We use the backward evolution framework to predict the parameters of the parents of the primary companion of these three binaries.","Our results indicate that at least one component of these three observed binaries was formed through a prior binary black hole merger.","This approach can be readily applied to future high-mass gravitational wave events to predict their formation history under the hierarchical merger assumption."],"url":"http://arxiv.org/abs/2406.06390v1","category":"astro-ph.HE"}
{"created":"2024-06-10 15:45:15","title":"Implications of DES 5YR SNe Dataset for $\u039b$CDM","abstract":"Dark Energy Survey five-year supernovae data (DES 5YR SNe) in conjunction with Planck CMB and Dark Energy Spectroscopic Instrument (DESI) BAO data has detected a strong dynamical dark energy deviation from the $\\Lambda$CDM model. Here, we shift the focus of DES data to the pressureless matter sector in the $\\Lambda$CDM model by studying the matter density parameter $\\Omega_m$. Employing frequentist profile likelihoods, we demonstrate that $\\Omega_m$ increases with effective redshift in the DES dataset at up to $2.5 \\sigma$ level. At the highest redshifts, one encounters negative dark energy densities $\\Omega_m > 1$. Our findings corroborate earlier observations in Pantheon and Pantheon+ datasets with an independent SNe dataset with a higher effective redshift. In an appendix, we confirm that curvature $\\Omega_k$ decreases with effective redshift disfavouring a flat Universe in higher redshift DES SNe at $> 3 \\sigma$. Our choice of $\\Omega_k$ prior leads to an underestimation of the tension with a flat Universe.","sentences":["Dark Energy Survey five-year supernovae data (DES 5YR SNe) in conjunction with Planck CMB and Dark Energy Spectroscopic Instrument (DESI)","BAO data has detected a strong dynamical dark energy deviation from the $\\Lambda$CDM model.","Here, we shift the focus of DES data to the pressureless matter sector in the $\\Lambda$CDM model by studying the matter density parameter $\\Omega_m$. Employing frequentist profile likelihoods, we demonstrate that $\\Omega_m$ increases with effective redshift in the DES dataset at up to $2.5 \\sigma$ level.","At the highest redshifts, one encounters negative dark energy densities $\\Omega_m > 1$.","Our findings corroborate earlier observations in Pantheon and Pantheon+ datasets with an independent SNe dataset with a higher effective redshift.","In an appendix, we confirm that curvature $\\Omega_k$ decreases with effective redshift disfavouring a flat Universe in higher redshift DES SNe at $> 3 \\sigma$. Our choice of $\\Omega_k$ prior leads to an underestimation of the tension with a flat Universe."],"url":"http://arxiv.org/abs/2406.06389v1","category":"astro-ph.CO"}
{"created":"2024-06-10 15:44:41","title":"FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for Classification of Mass Margins in Digital Mammography","abstract":"Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis. In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models. Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography. However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant. This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification. Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes.","sentences":["Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis.","In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models.","Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography.","However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant.","This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification.","Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes."],"url":"http://arxiv.org/abs/2406.06386v1","category":"cs.CV"}
{"created":"2024-06-10 15:44:22","title":"Low-Rank Quantization-Aware Training for LLMs","abstract":"Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.","sentences":["Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands.","Quantization is one of the most effective ways to make them more compute and memory efficient.","Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs.","Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs.","LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing.","Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques.","We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks.","Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage.","Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory."],"url":"http://arxiv.org/abs/2406.06385v1","category":"cs.LG"}
{"created":"2024-06-10 15:43:56","title":"Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled Representations","abstract":"Diabetic Retinopathy (DR), induced by diabetes, poses a significant risk of visual impairment. Accurate and effective grading of DR aids in the treatment of this condition. Yet existing models experience notable performance degradation on unseen domains due to domain shifts. Previous methods address this issue by simulating domain style through simple visual transformation and mitigating domain noise via learning robust representations. However, domain shifts encompass more than image styles. They overlook biases caused by implicit factors such as ethnicity, age, and diagnostic criteria. In our work, we propose a novel framework where representations of paired data from different domains are decoupled into semantic features and domain noise. The resulting augmented representation comprises original retinal semantics and domain noise from other domains, aiming to generate enhanced representations aligned with real-world clinical needs, incorporating rich information from diverse domains. Subsequently, to improve the robustness of the decoupled representations, class and domain prototypes are employed to interpolate the disentangled representations while data-aware weights are designed to focus on rare classes and domains. Finally, we devise a robust pixel-level semantic alignment loss to align retinal semantics decoupled from features, maintaining a balance between intra-class diversity and dense class features. Experimental results on multiple benchmarks demonstrate the effectiveness of our method on unseen domains. The code implementations are accessible on https://github.com/richard-peng-xia/DECO.","sentences":["Diabetic Retinopathy (DR), induced by diabetes, poses a significant risk of visual impairment.","Accurate and effective grading of DR aids in the treatment of this condition.","Yet existing models experience notable performance degradation on unseen domains due to domain shifts.","Previous methods address this issue by simulating domain style through simple visual transformation and mitigating domain noise via learning robust representations.","However, domain shifts encompass more than image styles.","They overlook biases caused by implicit factors such as ethnicity, age, and diagnostic criteria.","In our work, we propose a novel framework where representations of paired data from different domains are decoupled into semantic features and domain noise.","The resulting augmented representation comprises original retinal semantics and domain noise from other domains, aiming to generate enhanced representations aligned with real-world clinical needs, incorporating rich information from diverse domains.","Subsequently, to improve the robustness of the decoupled representations, class and domain prototypes are employed to interpolate the disentangled representations while data-aware weights are designed to focus on rare classes and domains.","Finally, we devise a robust pixel-level semantic alignment loss to align retinal semantics decoupled from features, maintaining a balance between intra-class diversity and dense class features.","Experimental results on multiple benchmarks demonstrate the effectiveness of our method on unseen domains.","The code implementations are accessible on https://github.com/richard-peng-xia/DECO."],"url":"http://arxiv.org/abs/2406.06384v1","category":"cs.CV"}
{"created":"2024-06-10 15:42:03","title":"Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization","abstract":"Aligning large language models with human preferences has emerged as a critical focus in language modeling research. Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory. The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts. We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively. This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities. Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment. Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment. Our code is available at https://github.com/yigu1008/Diffusion-RPO","sentences":["Aligning large language models with human preferences has emerged as a critical focus in language modeling research.","Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory.","The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts.","We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively.","This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities.","Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment.","Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment.","Our code is available at https://github.com/yigu1008/Diffusion-RPO"],"url":"http://arxiv.org/abs/2406.06382v1","category":"cs.CV"}
{"created":"2024-06-10 15:40:23","title":"FinVerse: An Autonomous Agent System for Versatile Financial Analysis","abstract":"With the significant advancements in cognitive intelligence driven by LLMs, autonomous agent systems have attracted extensive attention. Despite this growing interest, the development of stable and efficient agent systems poses substantial practical challenges. In this paper, we introduce FinVerse, a meticulously crafted agent system designed for a broad range of financial topics. FinVerse integrates over 600 financial APIs, enabling access to more accurate and extensive financial information compared to generalist agents. To enhance financial information processing capabilities, FinVerse is equipped with an embedded code interpreter, enabling the execution of complex data analysis tasks with precision and efficiency. Our work includes an empirical comparison of several LLMs in driving FinVerse. Specifically, we propose our own scheme for training LLMs using SFT to optimize LLM performance within FinVerse. Recognizing the scarcity of specialized datasets to build LLMs for agents, we have constructed a dataset and plan to make it open-source, providing a valuable resource for peer application developers. The demo video has been released on YouTube at https://www.youtube.com/watch?v=sk8L9_Wv7J4","sentences":["With the significant advancements in cognitive intelligence driven by LLMs, autonomous agent systems have attracted extensive attention.","Despite this growing interest, the development of stable and efficient agent systems poses substantial practical challenges.","In this paper, we introduce FinVerse, a meticulously crafted agent system designed for a broad range of financial topics.","FinVerse integrates over 600 financial APIs, enabling access to more accurate and extensive financial information compared to generalist agents.","To enhance financial information processing capabilities, FinVerse is equipped with an embedded code interpreter, enabling the execution of complex data analysis tasks with precision and efficiency.","Our work includes an empirical comparison of several LLMs in driving FinVerse.","Specifically, we propose our own scheme for training LLMs using SFT to optimize LLM performance within FinVerse.","Recognizing the scarcity of specialized datasets to build LLMs for agents, we have constructed a dataset and plan to make it open-source, providing a valuable resource for peer application developers.","The demo video has been released on YouTube at https://www.youtube.com/watch?v=sk8L9_Wv7J4"],"url":"http://arxiv.org/abs/2406.06379v1","category":"cs.CE"}
{"created":"2024-06-10 15:39:32","title":"Background resilient quantitative phase microscopy using entangled photons","abstract":"In this work, we introduce a quantum-based quantitative phase microscopy technique using a phase gradient approach that is inherently background resistant and does not rely on interferometry or scanning. Here, a transparent sample is illuminated by both photons of a position-momentum entangled pair with one photon setup for position measurement in the near-field (NF) of the sample and its partner for momentum measurement in the far-field (FF). By virtue of the spatial correlation property inherent to the entanglement, both the position and momentum information of the photons can thus be obtained simultaneously. The phase profile of the sample is then deduced through a phase gradient measurement obtained by measuring the centroid shift of the photons' in the FF momentum plane for each NF position. We show that the technique, while achieving an imaging resolution of 2.76\\,$\\mu$m, is phase accurate to at least $\\lambda/30$ and phase sensitive to $\\lambda/100$ at a wavelength of 810\\,nm. In addition, through the temporal correlation between the photon pairs, our technique shows resilience to strong dynamic background lights, which can prove difficult to account for in classical phase imaging techniques. We believe this work marks a significant advancement in the capabilities of quantum phase microscopy and quantum imaging in general, it showcases imaging and phase resolutions approaching those attainable with classical phase microscopes. This advancement brings quantum imaging closer to practical real-world applications, heralding new possibilities in the field.","sentences":["In this work, we introduce a quantum-based quantitative phase microscopy technique using a phase gradient approach that is inherently background resistant and does not rely on interferometry or scanning.","Here, a transparent sample is illuminated by both photons of a position-momentum entangled pair with one photon setup for position measurement in the near-field (NF) of the sample and its partner for momentum measurement in the far-field (FF).","By virtue of the spatial correlation property inherent to the entanglement, both the position and momentum information of the photons can thus be obtained simultaneously.","The phase profile of the sample is then deduced through a phase gradient measurement obtained by measuring the centroid shift of the photons' in the FF momentum plane for each NF position.","We show that the technique, while achieving an imaging resolution of 2.76\\,$\\mu$m, is phase accurate to at least $\\lambda/30$ and phase sensitive to $\\lambda/100$ at a wavelength of 810\\,nm.","In addition, through the temporal correlation between the photon pairs, our technique shows resilience to strong dynamic background lights, which can prove difficult to account for in classical phase imaging techniques.","We believe this work marks a significant advancement in the capabilities of quantum phase microscopy and quantum imaging in general, it showcases imaging and phase resolutions approaching those attainable with classical phase microscopes.","This advancement brings quantum imaging closer to practical real-world applications, heralding new possibilities in the field."],"url":"http://arxiv.org/abs/2406.06377v1","category":"quant-ph"}
{"created":"2024-06-10 15:37:46","title":"MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing","abstract":"In cross-modal music processing, translation between visual, auditory, and semantic content opens up new possibilities as well as challenges. The construction of such a transformative scheme depends upon a benchmark corpus with a comprehensive data infrastructure. In particular, the assembly of a large-scale cross-modal dataset presents major challenges. In this paper, we present the MOSA (Music mOtion with Semantic Annotation) dataset, which contains high quality 3-D motion capture data, aligned audio recordings, and note-by-note semantic annotations of pitch, beat, phrase, dynamic, articulation, and harmony for 742 professional music performances by 23 professional musicians, comprising more than 30 hours and 570 K notes of data. To our knowledge, this is the largest cross-modal music dataset with note-level annotations to date. To demonstrate the usage of the MOSA dataset, we present several innovative cross-modal music information retrieval (MIR) and musical content generation tasks, including the detection of beats, downbeats, phrase, and expressive contents from audio, video and motion data, and the generation of musicians' body motion from given music audio. The dataset and codes are available alongside this publication (https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset).","sentences":["In cross-modal music processing, translation between visual, auditory, and semantic content opens up new possibilities as well as challenges.","The construction of such a transformative scheme depends upon a benchmark corpus with a comprehensive data infrastructure.","In particular, the assembly of a large-scale cross-modal dataset presents major challenges.","In this paper, we present the MOSA (Music mOtion with Semantic Annotation) dataset, which contains high quality 3-D motion capture data, aligned audio recordings, and note-by-note semantic annotations of pitch, beat, phrase, dynamic, articulation, and harmony for 742 professional music performances by 23 professional musicians, comprising more than 30 hours and 570 K notes of data.","To our knowledge, this is the largest cross-modal music dataset with note-level annotations to date.","To demonstrate the usage of the MOSA dataset, we present several innovative cross-modal music information retrieval (MIR) and musical content generation tasks, including the detection of beats, downbeats, phrase, and expressive contents from audio, video and motion data, and the generation of musicians' body motion from given music audio.","The dataset and codes are available alongside this publication (https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset)."],"url":"http://arxiv.org/abs/2406.06375v1","category":"cs.SD"}
{"created":"2024-06-10 15:35:36","title":"Entanglement and steering in quantum batteries","abstract":"The advantage of quantum batteries is that quantum resources can be used to improve charging efficiency. The quantum resources that are known to be available are: quantum entanglement and quantum coherence. In this paper, we introduce quantum steering as a new quantum resource into batteries for the first time. We analyze the relationship between quantum steering, quantum entanglement, energy storage, and extractable work by considering two models: Field-quantum battery and Cavity-Heisenberg quantum battery. We find that in the steerable range, the quantum steering of different qubits has a maximum or minimum value, which corresponds to the energy storage of the battery, and the extractable work has a maximum value. The occurrence of the minimum value of quantum entanglement is always accompanied by the occurrence of the maximum value of parameters such as energy storage. Ultimately, we analyzed the reasons for these results using the purity of the system. And found a relatively general conclusion: when the purity is at the maximum, important parameters such as the energy storage of the battery are also at the maximum.","sentences":["The advantage of quantum batteries is that quantum resources can be used to improve charging efficiency.","The quantum resources that are known to be available are: quantum entanglement and quantum coherence.","In this paper, we introduce quantum steering as a new quantum resource into batteries for the first time.","We analyze the relationship between quantum steering, quantum entanglement, energy storage, and extractable work by considering two models: Field-quantum battery and Cavity-Heisenberg quantum battery.","We find that in the steerable range, the quantum steering of different qubits has a maximum or minimum value, which corresponds to the energy storage of the battery, and the extractable work has a maximum value.","The occurrence of the minimum value of quantum entanglement is always accompanied by the occurrence of the maximum value of parameters such as energy storage.","Ultimately, we analyzed the reasons for these results using the purity of the system.","And found a relatively general conclusion: when the purity is at the maximum, important parameters such as the energy storage of the battery are also at the maximum."],"url":"http://arxiv.org/abs/2406.06373v1","category":"quant-ph"}
{"created":"2024-06-10 15:34:23","title":"Improving Deep Learning-based Automatic Cranial Defect Reconstruction by Heavy Data Augmentation: From Image Registration to Latent Diffusion Models","abstract":"Modeling and manufacturing of personalized cranial implants are important research areas that may decrease the waiting time for patients suffering from cranial damage. The modeling of personalized implants may be partially automated by the use of deep learning-based methods. However, this task suffers from difficulties with generalizability into data from previously unseen distributions that make it difficult to use the research outcomes in real clinical settings. Due to difficulties with acquiring ground-truth annotations, different techniques to improve the heterogeneity of datasets used for training the deep networks have to be considered and introduced. In this work, we present a large-scale study of several augmentation techniques, varying from classical geometric transformations, image registration, variational autoencoders, and generative adversarial networks, to the most recent advances in latent diffusion models. We show that the use of heavy data augmentation significantly increases both the quantitative and qualitative outcomes, resulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96 for the SkullFix datasets. Moreover, we show that the synthetically augmented network successfully reconstructs real clinical defects. The work is a considerable contribution to the field of artificial intelligence in the automatic modeling of personalized cranial implants.","sentences":["Modeling and manufacturing of personalized cranial implants are important research areas that may decrease the waiting time for patients suffering from cranial damage.","The modeling of personalized implants may be partially automated by the use of deep learning-based methods.","However, this task suffers from difficulties with generalizability into data from previously unseen distributions that make it difficult to use the research outcomes in real clinical settings.","Due to difficulties with acquiring ground-truth annotations, different techniques to improve the heterogeneity of datasets used for training the deep networks have to be considered and introduced.","In this work, we present a large-scale study of several augmentation techniques, varying from classical geometric transformations, image registration, variational autoencoders, and generative adversarial networks, to the most recent advances in latent diffusion models.","We show that the use of heavy data augmentation significantly increases both the quantitative and qualitative outcomes, resulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96 for the SkullFix datasets.","Moreover, we show that the synthetically augmented network successfully reconstructs real clinical defects.","The work is a considerable contribution to the field of artificial intelligence in the automatic modeling of personalized cranial implants."],"url":"http://arxiv.org/abs/2406.06372v1","category":"cs.CV"}
{"created":"2024-06-10 15:32:42","title":"mHuBERT-147: A Compact Multilingual HuBERT Model","abstract":"We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment over the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations and with only 95M parameters, mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min/1h leaderboards respectively, with SOTA scores for all LID tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings suggest that mHuBERT-147 is a promising model for multilingual speech processing tasks, offering an unprecedented balance between high performance and parameter efficiency.","sentences":["We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data.","To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment over the original method.","We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity.","After 3 training iterations and with only 95M parameters, mHuBERT-147 outperforms larger models trained on substantially more data.","We rank second and first on the ML-SUPERB 10min/1h leaderboards respectively, with SOTA scores for all LID tasks.","Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours).","Our findings suggest that mHuBERT-147 is a promising model for multilingual speech processing tasks, offering an unprecedented balance between high performance and parameter efficiency."],"url":"http://arxiv.org/abs/2406.06371v1","category":"cs.CL"}
{"created":"2024-06-10 15:32:16","title":"UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving","abstract":"Dealing with atypical traffic scenarios remains a challenging task in autonomous driving. However, most anomaly detection approaches cannot be trained on raw sensor data but require exposure to outlier data and powerful semantic segmentation models trained in a supervised fashion. This limits the representation of normality to labeled data, which does not scale well. In this work, we revisit unsupervised anomaly detection and present UMAD, leveraging generative world models and unsupervised image segmentation. Our method outperforms state-of-the-art unsupervised anomaly detection.","sentences":["Dealing with atypical traffic scenarios remains a challenging task in autonomous driving.","However, most anomaly detection approaches cannot be trained on raw sensor data but require exposure to outlier data and powerful semantic segmentation models trained in a supervised fashion.","This limits the representation of normality to labeled data, which does not scale well.","In this work, we revisit unsupervised anomaly detection and present UMAD, leveraging generative world models and unsupervised image segmentation.","Our method outperforms state-of-the-art unsupervised anomaly detection."],"url":"http://arxiv.org/abs/2406.06370v1","category":"cs.CV"}
{"created":"2024-06-10 15:26:48","title":"MVGamba: Unify 3D Content Generation as State Space Sequence Modeling","abstract":"Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (\\eg, Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\times$ of the model size.","sentences":["Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors.","Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency.","However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures.","We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (\\eg, Transformers).","To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM).","Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity.","With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts.","Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\times$ of the model size."],"url":"http://arxiv.org/abs/2406.06367v1","category":"cs.CV"}
{"created":"2024-06-10 15:23:41","title":"A curious symmetric decomposition of the (des, exc)-Eulerian polynomials","abstract":"One of the most central result in combinatorics says that the descent statistic and the excedance statistic are equidistribued over the symmetric group. As a continuation of the work of Shareshian-Wachs (Adv. Math., 225(6) (2010), 2921--2966), we provide a curious $t$-symmetric decomposition for the generating polynomial of the joint distribution of the descent and excedance statistics over the symmetric group.","sentences":["One of the most central result in combinatorics says that the descent statistic and the excedance statistic are equidistribued over the symmetric group.","As a continuation of the work of Shareshian-Wachs (Adv. Math., 225(6) (2010), 2921--2966), we provide a curious $t$-symmetric decomposition for the generating polynomial of the joint distribution of the descent and excedance statistics over the symmetric group."],"url":"http://arxiv.org/abs/2406.06365v1","category":"math.CO"}
{"created":"2024-06-10 15:22:41","title":"Automating Food Drop: The Power of Two Choices for Dynamic and Fair Food Allocation","abstract":"Food waste and food insecurity are two closely related pressing global issues. Food rescue organizations worldwide run programs aimed at addressing the two problems. In this paper, we partner with a non-profit organization in the state of Indiana that leads \\emph{Food Drop}, a program that is designed to redirect rejected truckloads of food away from landfills and into food banks. The truckload to food bank matching decisions are currently made by an employee of our partner organization. In addition to this being a very time-consuming task, as perhaps expected from human-based matching decisions, the allocations are often skewed: a small percentage of the possible recipients receives the majority of donations. Our goal in this partnership is to completely automate Food Drop. In doing so, we need a matching algorithm for making real-time decisions that strikes a balance between ensuring fairness for the food banks that receive the food and optimizing efficiency for the truck drivers. In this paper, we describe the theoretical guarantees and experiments that dictated our choice of algorithm in the platform we built and deployed for our partner organization. Our work also makes contributions to the literature on load balancing and balls-into-bins games, that might be of independent interest. Specifically, we study the allocation of $m$ weighted balls into $n$ weighted bins, where each ball has two non-uniformly sampled random bin choices, and prove upper bounds, that hold with high probability, on the maximum load of any bin.","sentences":["Food waste and food insecurity are two closely related pressing global issues.","Food rescue organizations worldwide run programs aimed at addressing the two problems.","In this paper, we partner with a non-profit organization in the state of Indiana that leads \\emph{Food Drop}, a program that is designed to redirect rejected truckloads of food away from landfills and into food banks.","The truckload to food bank matching decisions are currently made by an employee of our partner organization.","In addition to this being a very time-consuming task, as perhaps expected from human-based matching decisions, the allocations are often skewed: a small percentage of the possible recipients receives the majority of donations.","Our goal in this partnership is to completely automate Food Drop.","In doing so, we need a matching algorithm for making real-time decisions that strikes a balance between ensuring fairness for the food banks that receive the food and optimizing efficiency for the truck drivers.","In this paper, we describe the theoretical guarantees and experiments that dictated our choice of algorithm in the platform we built and deployed for our partner organization.","Our work also makes contributions to the literature on load balancing and balls-into-bins games, that might be of independent interest.","Specifically, we study the allocation of $m$ weighted balls into $n$ weighted bins, where each ball has two non-uniformly sampled random bin choices, and prove upper bounds, that hold with high probability, on the maximum load of any bin."],"url":"http://arxiv.org/abs/2406.06363v1","category":"cs.GT"}
{"created":"2024-06-10 15:19:09","title":"MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows","abstract":"Scientific innovation relies on detailed workflows, which include critical steps such as analyzing literature, generating ideas, validating these ideas, interpreting results, and inspiring follow-up research. However, scientific publications that document these workflows are extensive and unstructured. This makes it difficult for both human researchers and AI systems to effectively navigate and explore the space of scientific innovation. To address this issue, we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization of Scientific Workflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications -- context, key idea, method, outcome, and projected impact -- which correspond to five key steps in the research workflow. These structured summaries facilitate a variety of downstream tasks and analyses. The quality of the LLM-extracted summaries is validated by comparing them with human annotations. We demonstrate the utility of MASSW through multiple novel machine-learning tasks that can be benchmarked using this new dataset, which make various types of predictions and recommendations along the scientific workflow. MASSW holds significant potential for researchers to create and benchmark new AI methods for optimizing scientific workflows and fostering scientific innovation in the field. Our dataset is openly available at \\url{https://github.com/xingjian-zhang/massw}.","sentences":["Scientific innovation relies on detailed workflows, which include critical steps such as analyzing literature, generating ideas, validating these ideas, interpreting results, and inspiring follow-up research.","However, scientific publications that document these workflows are extensive and unstructured.","This makes it difficult for both human researchers and AI systems to effectively navigate and explore the space of scientific innovation.","To address this issue, we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization of Scientific Workflows.","MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years.","Using Large Language Models (LLMs), we automatically extract five core aspects from these publications -- context, key idea, method, outcome, and projected impact -- which correspond to five key steps in the research workflow.","These structured summaries facilitate a variety of downstream tasks and analyses.","The quality of the LLM-extracted summaries is validated by comparing them with human annotations.","We demonstrate the utility of MASSW through multiple novel machine-learning tasks that can be benchmarked using this new dataset, which make various types of predictions and recommendations along the scientific workflow.","MASSW holds significant potential for researchers to create and benchmark new AI methods for optimizing scientific workflows and fostering scientific innovation in the field.","Our dataset is openly available at \\url{https://github.com/xingjian-zhang/massw}."],"url":"http://arxiv.org/abs/2406.06357v1","category":"cs.CL"}
{"created":"2024-06-10 15:18:14","title":"Re.Dis.Cover Place with Generative AI: Exploring the Experience and Design of City Wandering with Image-to-Image AI","abstract":"The HCI field has demonstrated a growing interest in leveraging emerging technologies to enrich urban experiences. However, insufficient studies investigate the experience and design space of AI image technology (AIGT) applications for playful urban interaction, despite its widespread adoption. To explore this gap, we conducted an exploratory study involving four participants who wandered and photographed within Eindhoven Centre and interacted with an image-to-image AI. Preliminary findings present their observations, the effect of their familiarity with places, and how AIGT becomes an explorer's tool or co-speculator. We then highlight AIGT's capability of supporting playfulness, reimaginations, and rediscoveries of places through defamiliarizing and familiarizing cityscapes. Additionally, we propose the metaphor AIGT as a 'tourist' to discuss its opportunities for engaging explorations and risks of stereotyping places. Collectively, our research provides initial empirical insights and design considerations, inspiring future HCI endeavors for creating urban play with generative AI.","sentences":["The HCI field has demonstrated a growing interest in leveraging emerging technologies to enrich urban experiences.","However, insufficient studies investigate the experience and design space of AI image technology (AIGT) applications for playful urban interaction, despite its widespread adoption.","To explore this gap, we conducted an exploratory study involving four participants who wandered and photographed within Eindhoven Centre and interacted with an image-to-image AI.","Preliminary findings present their observations, the effect of their familiarity with places, and how AIGT becomes an explorer's tool or co-speculator.","We then highlight AIGT's capability of supporting playfulness, reimaginations, and rediscoveries of places through defamiliarizing and familiarizing cityscapes.","Additionally, we propose the metaphor AIGT as a 'tourist' to discuss its opportunities for engaging explorations and risks of stereotyping places.","Collectively, our research provides initial empirical insights and design considerations, inspiring future HCI endeavors for creating urban play with generative AI."],"url":"http://arxiv.org/abs/2406.06356v1","category":"cs.HC"}
{"created":"2024-06-10 15:14:33","title":"On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions","abstract":"We investigate the out-of-domain generalization of random feature (RF) models and Transformers. We first prove that in the `generalization on the unseen (GOTU)' setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023). We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case. We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators. This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place. For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized.","sentences":["We investigate the out-of-domain generalization of random feature (RF) models and Transformers.","We first prove that in the `generalization on the unseen (GOTU)' setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023).","We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case.","We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators.","This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place.","For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized."],"url":"http://arxiv.org/abs/2406.06354v1","category":"cs.LG"}
{"created":"2024-06-10 15:13:51","title":"Latent Directions: A Simple Pathway to Bias Mitigation in Generative AI","abstract":"Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society. The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions. Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process. Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases. Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments. Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected. Thus, we provide a tool to empower developers to select their desired concepts to mitigate. The project page with code is available online.","sentences":["Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society.","The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions.","Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process.","Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases.","Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments.","Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected.","Thus, we provide a tool to empower developers to select their desired concepts to mitigate.","The project page with code is available online."],"url":"http://arxiv.org/abs/2406.06352v1","category":"cs.CV"}
{"created":"2024-06-10 15:12:53","title":"Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks","abstract":"We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs). We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations). We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons. The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees. This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\\tanh$ activation function. Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation. Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses.","sentences":["We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs).","We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations).","We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons.","The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees.","This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\\tanh$ activation function.","Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation.","Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses."],"url":"http://arxiv.org/abs/2406.06350v1","category":"math.NA"}
{"created":"2024-06-10 15:09:58","title":"ARMA Processes with Discrete-Continuous Excitation: Compressibility Beyond Sparsity","abstract":"R\\'enyi Information Dimension (RID) plays a central role in quantifying the compressibility of random variables with singularities in their distribution, encompassing and extending beyond the class of sparse sources. The RID, from a high perspective, presents the average number of bits that is needed for coding the i.i.d. samples of a random variable with high precision. There are two main extensions of the RID for stochastic processes: information dimension rate (IDR) and block information dimension (BID). In addition, a more recent approach towards the compressibility of stochastic processes revolves around the concept of $\\epsilon$-achievable compression rates, which treat a random process as the limiting point of finite-dimensional random vectors and apply the compressed sensing tools on these random variables. While there is limited knowledge about the interplay of the the BID, the IDR, and $\\epsilon$-achievable compression rates, the value of IDR and BID themselves are known only for very specific types of processes, namely i.i.d. sequences (i.e., discrete-domain white noise) and moving-average (MA) processes. This paper investigates the IDR and BID of discrete-time Auto-Regressive Moving-Average (ARMA) processes in general, and their relations with $\\epsilon$-achievable compression rates when the excitation noise has a discrete-continuous measure. To elaborate, this paper shows that the RID and $\\epsilon$-achievable compression rates of this type of processes are equal to that of their excitation noise. In other words, the samples of such ARMA processes can be compressed as much as their sparse excitation noise, although the samples themselves are by no means sparse. The results of this paper can be used to evaluate the compressibility of various types of locally correlated data with finite- or infinite-memory as they are often modelled via ARMA processes.","sentences":["R\\'enyi Information Dimension (RID) plays a central role in quantifying the compressibility of random variables with singularities in their distribution, encompassing and extending beyond the class of sparse sources.","The RID, from a high perspective, presents the average number of bits that is needed for coding the i.i.d. samples of a random variable with high precision.","There are two main extensions of the RID for stochastic processes: information dimension rate (IDR) and block information dimension (BID).","In addition, a more recent approach towards the compressibility of stochastic processes revolves around the concept of $\\epsilon$-achievable compression rates, which treat a random process as the limiting point of finite-dimensional random vectors and apply the compressed sensing tools on these random variables.","While there is limited knowledge about the interplay of the the BID, the IDR, and $\\epsilon$-achievable compression rates, the value of IDR and BID themselves are known only for very specific types of processes, namely i.i.d. sequences (i.e., discrete-domain white noise) and moving-average (MA) processes.","This paper investigates the IDR and BID of discrete-time Auto-Regressive Moving-Average (ARMA) processes in general, and their relations with $\\epsilon$-achievable compression rates when the excitation noise has a discrete-continuous measure.","To elaborate, this paper shows that the RID and $\\epsilon$-achievable compression rates of this type of processes are equal to that of their excitation noise.","In other words, the samples of such ARMA processes can be compressed as much as their sparse excitation noise, although the samples themselves are by no means sparse.","The results of this paper can be used to evaluate the compressibility of various types of locally correlated data with finite- or infinite-memory as they are often modelled via ARMA processes."],"url":"http://arxiv.org/abs/2406.06349v1","category":"cs.IT"}
{"created":"2024-06-10 15:08:14","title":"Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning","abstract":"The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way -- without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure -- a set of learned or existing candidate hypotheses -- to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.","sentences":["The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses.","Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way -- without necessarily tailoring to a specific domain.","Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data.","For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap.","In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees.","We leverage the idea of a superstructure -- a set of learned or existing candidate hypotheses -- to partition the search space.","We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph.","We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables.","This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces."],"url":"http://arxiv.org/abs/2406.06348v1","category":"cs.LG"}
{"created":"2024-06-10 15:06:02","title":"Dynamical Mean-Field Theory of Complex Systems on Sparse Directed Networks","abstract":"Although real-world complex systems typically interact through sparse and heterogeneous networks, analytic solutions of their dynamics are limited to models with all-to-all interactions. Here, we solve the dynamics of a broad range of nonlinear models of complex systems on sparse directed networks with a random structure. By generalizing dynamical mean-field theory to sparse systems, we derive an exact equation for the path-probability describing the effective dynamics of a single degree of freedom. Our general solution applies to key models in the study of neural networks, ecosystems, epidemic spreading, and synchronization. Using the population dynamics algorithm, we solve the path-probability equation to determine the phase diagram of a seminal neural network model in the sparse regime, showing that this model undergoes a transition from a fixed-point phase to chaos as a function of the network topology.","sentences":["Although real-world complex systems typically interact through sparse and heterogeneous networks, analytic solutions of their dynamics are limited to models with all-to-all interactions.","Here, we solve the dynamics of a broad range of nonlinear models of complex systems on sparse directed networks with a random structure.","By generalizing dynamical mean-field theory to sparse systems, we derive an exact equation for the path-probability describing the effective dynamics of a single degree of freedom.","Our general solution applies to key models in the study of neural networks, ecosystems, epidemic spreading, and synchronization.","Using the population dynamics algorithm, we solve the path-probability equation to determine the phase diagram of a seminal neural network model in the sparse regime, showing that this model undergoes a transition from a fixed-point phase to chaos as a function of the network topology."],"url":"http://arxiv.org/abs/2406.06346v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-10 15:05:36","title":"Accurate Prediction of Core Level Binding Energies from Ground-State Density Functional Calculations: The Importance of Localization and Screening","abstract":"A new method for predicting core level binding energies (CLBEs) is developed by both localizing the core-level states and describing the screening effect. CLBEs contain important information about the electronic structure, elemental chemistry, and chemical environment of molecules and materials. Theoretical study of CLBEs can provide insights for analyzing and interpreting the experimental results obtained from the X-ray photoelectron spectroscopy, in which the overlapping of signals is very common. The localization of core-level holes is important for the theoretical calculation of CLBEs. Predicting CLBEs from commonly used density functional approximations (DFAs) is challenging, because conventional DFAs often produce delocalized core-level states, especially when degenerate core-level states exist. In this work, we combine the localization procedure from the localized orbital scaling correction method and the curvature matrix generalized from the exact second-order correction method that contains the screening effect, and the resulting approach can accurately predict CLBEs from ground-state density functional calculations.","sentences":["A new method for predicting core level binding energies (CLBEs) is developed by both localizing the core-level states and describing the screening effect.","CLBEs contain important information about the electronic structure, elemental chemistry, and chemical environment of molecules and materials.","Theoretical study of CLBEs can provide insights for analyzing and interpreting the experimental results obtained from the X-ray photoelectron spectroscopy, in which the overlapping of signals is very common.","The localization of core-level holes is important for the theoretical calculation of CLBEs.","Predicting CLBEs from commonly used density functional approximations (DFAs) is challenging, because conventional DFAs often produce delocalized core-level states, especially when degenerate core-level states exist.","In this work, we combine the localization procedure from the localized orbital scaling correction method and the curvature matrix generalized from the exact second-order correction method that contains the screening effect, and the resulting approach can accurately predict CLBEs from ground-state density functional calculations."],"url":"http://arxiv.org/abs/2406.06345v1","category":"physics.chem-ph"}
{"created":"2024-06-10 15:03:12","title":"Thin Film Reconfigurable Intelligent Surface for Harmonic Beam Steering","abstract":"This letter explores the development and implementation of a novel thin film 1-by-4 reconfigurable intelligent surface (RIS) designed for future communication and sensing scenarios. Utilizing cost-effective inkjet printing methods and additive manufacturing, our approach significantly simplifies the RIS construction process and reduces production costs. The RIS, fabricated on a flexible and lightweight polyethylene terephthalate (PET) substrate, integrates antennas, switching circuitry, and a microcontroller unit (MCU). This setup enables individual and simultaneous control of each RIS element, manipulating the captured carrier signal by steering its dominant harmonics toward multiple desired directions. Measurement results of the beam steering show the manufactured RIS has the potential to enable RIS-aided communication and sensing applications.","sentences":["This letter explores the development and implementation of a novel thin film 1-by-4 reconfigurable intelligent surface (RIS) designed for future communication and sensing scenarios.","Utilizing cost-effective inkjet printing methods and additive manufacturing, our approach significantly simplifies the RIS construction process and reduces production costs.","The RIS, fabricated on a flexible and lightweight polyethylene terephthalate (PET) substrate, integrates antennas, switching circuitry, and a microcontroller unit (MCU).","This setup enables individual and simultaneous control of each RIS element, manipulating the captured carrier signal by steering its dominant harmonics toward multiple desired directions.","Measurement results of the beam steering show the manufactured RIS has the potential to enable RIS-aided communication and sensing applications."],"url":"http://arxiv.org/abs/2406.06343v1","category":"eess.SP"}
{"created":"2024-06-10 15:02:30","title":"A Guide to Stochastic Optimisation for Large-Scale Inverse Problems","abstract":"Stochastic optimisation algorithms are the de facto standard for machine learning with large amounts of data. Handling only a subset of available data in each optimisation step dramatically reduces the per-iteration computational costs, while still ensuring significant progress towards the solution. Driven by the need to solve large-scale optimisation problems as efficiently as possible, the last decade has witnessed an explosion of research in this area. Leveraging the parallels between machine learning and inverse problems has allowed harnessing the power of this research wave for solving inverse problems. In this survey, we provide a comprehensive account of the state-of-the-art in stochastic optimisation from the viewpoint of inverse problems. We present algorithms with diverse modalities of problem randomisation and discuss the roles of variance reduction, acceleration, higher-order methods, and other algorithmic modifications, and compare theoretical results with practical behaviour. We focus on the potential and the challenges for stochastic optimisation that are unique to inverse imaging problems and are not commonly encountered in machine learning. We conclude the survey with illustrative examples from imaging problems to examine the advantages and disadvantages that this new generation of algorithms bring to the field of inverse problems.","sentences":["Stochastic optimisation algorithms are the de facto standard for machine learning with large amounts of data.","Handling only a subset of available data in each optimisation step dramatically reduces the per-iteration computational costs, while still ensuring significant progress towards the solution.","Driven by the need to solve large-scale optimisation problems as efficiently as possible, the last decade has witnessed an explosion of research in this area.","Leveraging the parallels between machine learning and inverse problems has allowed harnessing the power of this research wave for solving inverse problems.","In this survey, we provide a comprehensive account of the state-of-the-art in stochastic optimisation from the viewpoint of inverse problems.","We present algorithms with diverse modalities of problem randomisation and discuss the roles of variance reduction, acceleration, higher-order methods, and other algorithmic modifications, and compare theoretical results with practical behaviour.","We focus on the potential and the challenges for stochastic optimisation that are unique to inverse imaging problems and are not commonly encountered in machine learning.","We conclude the survey with illustrative examples from imaging problems to examine the advantages and disadvantages that this new generation of algorithms bring to the field of inverse problems."],"url":"http://arxiv.org/abs/2406.06342v1","category":"math.NA"}
{"created":"2024-06-10 15:01:46","title":"Predicting Heart Activity from Speech using Data-driven and Knowledge-based features","abstract":"Accurately predicting heart activity and other biological signals is crucial for diagnosis and monitoring. Given that speech is an outcome of multiple physiological systems, a significant body of work studied the acoustic correlates of heart activity. Recently, self-supervised models have excelled in speech-related tasks compared to traditional acoustic methods. However, the robustness of data-driven representations in predicting heart activity remained unexplored. In this study, we demonstrate that self-supervised speech models outperform acoustic features in predicting heart activity parameters. We also emphasize the impact of individual variability on model generalizability. These findings underscore the value of data-driven representations in such tasks and the need for more speech-based physiological data to mitigate speaker-related challenges.","sentences":["Accurately predicting heart activity and other biological signals is crucial for diagnosis and monitoring.","Given that speech is an outcome of multiple physiological systems, a significant body of work studied the acoustic correlates of heart activity.","Recently, self-supervised models have excelled in speech-related tasks compared to traditional acoustic methods.","However, the robustness of data-driven representations in predicting heart activity remained unexplored.","In this study, we demonstrate that self-supervised speech models outperform acoustic features in predicting heart activity parameters.","We also emphasize the impact of individual variability on model generalizability.","These findings underscore the value of data-driven representations in such tasks and the need for more speech-based physiological data to mitigate speaker-related challenges."],"url":"http://arxiv.org/abs/2406.06341v1","category":"cs.SD"}
{"created":"2024-06-10 15:01:03","title":"Optimisation of federated learning settings under statistical heterogeneity variations","abstract":"Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator. However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation. In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets. We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID. Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics. On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets","sentences":["Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator.","However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data.","Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation.","In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets.","We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID.","Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics.","On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets"],"url":"http://arxiv.org/abs/2406.06340v1","category":"cs.LG"}
{"created":"2024-06-10 14:58:46","title":"The Lattice Problem for Models of $\\mathsf{PA}$","abstract":"The lattice problem for models of Peano Arithmetic ($\\mathsf{PA}$) is to determine which lattices can be represented as lattices of elementary submodels of a model of $\\mathsf{PA}$, or, in greater generality, for a given model $\\mathcal{M}$, which lattices can be represented as interstructure lattices of elementary submodels $\\mathcal{K}$ of an elementary extension $\\mathcal{N}$ such that $\\mathcal{M} \\preccurlyeq \\mathcal{K} \\preccurlyeq \\mathcal{N}$. The problem has been studied for the last 60 years and the results and their proofs show an interesting interplay between the model theory of PA, Ramsey style combinatorics, lattice representation theory, and some results in elementary number theory. We present a survey of the most important results together with a detailed analysis of some special cases to explain and motivate a technique of CPP-representations developed by James Schmerl for constructing elementary extensions with prescribed interstructure lattices. The last section is devoted to a discussion of lesser-known results about lattices of elementary submodels of countable recursively saturated models of PA.","sentences":["The lattice problem for models of Peano Arithmetic ($\\mathsf{PA}$) is to determine which lattices can be represented as lattices of elementary submodels of a model of $\\mathsf{PA}$, or, in greater generality, for a given model $\\mathcal{M}$, which lattices can be represented as interstructure lattices of elementary submodels $\\mathcal{K}$ of an elementary extension $\\mathcal{N}$ such that $\\mathcal{M} \\preccurlyeq \\mathcal{K} \\preccurlyeq \\mathcal{N}$.","The problem has been studied for the last 60 years and the results and their proofs show an interesting interplay between the model theory of PA, Ramsey style combinatorics, lattice representation theory, and some results in elementary number theory.","We present a survey of the most important results together with a detailed analysis of some special cases to explain and motivate a technique of CPP-representations developed by James Schmerl for constructing elementary extensions with prescribed interstructure lattices.","The last section is devoted to a discussion of lesser-known results about lattices of elementary submodels of countable recursively saturated models of PA."],"url":"http://arxiv.org/abs/2406.06338v1","category":"math.LO"}
{"created":"2024-06-10 14:57:35","title":"System- and Sample-agnostic Isotropic 3D Microscopy by Weakly Physics-informed, Domain-shift-resistant Axial Deblurring","abstract":"Three-dimensional (3D) subcellular imaging is essential for biomedical research, but the diffraction limit of optical microscopy compromises axial resolution, hindering accurate 3D structural analysis. This challenge is particularly pronounced in label-free imaging of thick, heterogeneous tissues, where assumptions about data distribution (e.g. sparsity, label-specific distribution, and lateral-axial similarity) and system priors (e.g. independent and identically distributed (i.i.d.) noise and linear shift-invariant (LSI) point-spread functions (PSFs)) are often invalid. Here, we introduce SSAI-3D, a weakly physics-informed, domain-shift-resistant framework for robust isotropic 3D imaging. SSAI-3D enables robust axial deblurring by generating a PSF-flexible, noise-resilient, sample-informed training dataset and sparsely fine-tuning a large pre-trained blind deblurring network. SSAI-3D was applied to label-free nonlinear imaging of living organoids, freshly excised human endometrium tissue, and mouse whisker pads, and further validated in publicly available ground-truth-paired experimental datasets of 3D heterogeneous biological tissues with unknown blurring and noise across different microscopy systems.","sentences":["Three-dimensional (3D) subcellular imaging is essential for biomedical research, but the diffraction limit of optical microscopy compromises axial resolution, hindering accurate 3D structural analysis.","This challenge is particularly pronounced in label-free imaging of thick, heterogeneous tissues, where assumptions about data distribution (e.g. sparsity, label-specific distribution, and lateral-axial similarity) and system priors (e.g. independent and identically distributed (i.i.d.) noise and linear shift-invariant (LSI) point-spread functions (PSFs)) are often invalid.","Here, we introduce SSAI-3D, a weakly physics-informed, domain-shift-resistant framework for robust isotropic 3D imaging.","SSAI-3D enables robust axial deblurring by generating a PSF-flexible, noise-resilient, sample-informed training dataset and sparsely fine-tuning a large pre-trained blind deblurring network.","SSAI-3D was applied to label-free nonlinear imaging of living organoids, freshly excised human endometrium tissue, and mouse whisker pads, and further validated in publicly available ground-truth-paired experimental datasets of 3D heterogeneous biological tissues with unknown blurring and noise across different microscopy systems."],"url":"http://arxiv.org/abs/2406.06337v1","category":"physics.optics"}
{"created":"2024-06-10 14:52:20","title":"Feasibility of accelerating homogeneous catalyst discovery with fault-tolerant quantum computers","abstract":"The industrial manufacturing of chemicals consumes a significant amount of energy and raw materials. In principle, the development of new catalysts could greatly improve the efficiency of chemical production. However, the discovery of viable catalysts can be exceedingly challenging because it is difficult to know the efficacy of a candidate without experimentally synthesizing and characterizing it. This study explores the feasibility of using fault-tolerant quantum computers to accelerate the discovery of homogeneous catalysts for nitrogen fixation, an industrially important chemical process. It introduces a set of ground-state energy estimation problems representative of calculations needed for the discovery of homogeneous catalysts and analyzes them on three dimensions: economic utility, classical hardness, and quantum resource requirements. For the highest utility problem considered, two steps of a catalytic cycle for the generation of cyanate anion from dinitrogen, the economic utility of running these computations is estimated to be $200,000, and the required runtime for double-factorized phase estimation on a fault-tolerant superconducting device is estimated under conservative assumptions to be 139,000 QPU-hours. The computational cost of an equivalent DMRG calculation is estimated to be about 400,000 CPU-hours. These results suggest that, with continued development, it will be feasible for fault-tolerant quantum computers to accelerate the discovery of homogeneous catalysts.","sentences":["The industrial manufacturing of chemicals consumes a significant amount of energy and raw materials.","In principle, the development of new catalysts could greatly improve the efficiency of chemical production.","However, the discovery of viable catalysts can be exceedingly challenging because it is difficult to know the efficacy of a candidate without experimentally synthesizing and characterizing it.","This study explores the feasibility of using fault-tolerant quantum computers to accelerate the discovery of homogeneous catalysts for nitrogen fixation, an industrially important chemical process.","It introduces a set of ground-state energy estimation problems representative of calculations needed for the discovery of homogeneous catalysts and analyzes them on three dimensions: economic utility, classical hardness, and quantum resource requirements.","For the highest utility problem considered, two steps of a catalytic cycle for the generation of cyanate anion from dinitrogen, the economic utility of running these computations is estimated to be $200,000, and the required runtime for double-factorized phase estimation on a fault-tolerant superconducting device is estimated under conservative assumptions to be 139,000 QPU-hours.","The computational cost of an equivalent DMRG calculation is estimated to be about 400,000 CPU-hours.","These results suggest that, with continued development, it will be feasible for fault-tolerant quantum computers to accelerate the discovery of homogeneous catalysts."],"url":"http://arxiv.org/abs/2406.06335v1","category":"quant-ph"}
{"created":"2024-06-10 14:47:04","title":"MedExQA: Medical Question Answering Benchmark with Multiple Explanations","abstract":"This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models' (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs' ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding. Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model.","sentences":["This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models' (LLMs) understanding of medical knowledge through explanations.","By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs' ability to generate nuanced medical explanations.","Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding.","Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs.","To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B).","The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain.","We will share our benchmark datasets and the trained model."],"url":"http://arxiv.org/abs/2406.06331v1","category":"cs.CL"}
{"created":"2024-06-10 14:46:29","title":"Theory of phonon sidebands in the absorption spectra of moir\u00e9 exciton-polaritons","abstract":"Excitons in twisted bilayers of transition metal dichalcogenides have strongly modified dispersion relations due to the formation of periodic moir\\'e potentials. The strong coupling to a light field in an optical cavity leads to the appearance of moir\\'e polaritons. In this paper, we derive a theoretical model for the linear absorption spectrum of the coupled moir\\'e polariton-phonon system based on the time-convolutionless (TCL) approach. Results obtained by numerically solving the TCL equation are compared to those obtained in the Markovian limit and from a perturbative treatment of non-Markovian corrections. A key quantity for the interpretation of the findings is the generalized phonon spectral density. We discuss the phonon impact on the spectrum for realistic moir\\'e exciton dispersions by varying twist angle and temperature.   Key features introduced by the coupling to phonons are broadenings and energy shifts of the upper and lower polariton peak and the appearance of phonon sidebands between them. We analyze these features with respect to the role of Markovian and non-Markovian effects and find that they strongly depend on the twist angle. We can distinguish between the regimes of large, small, and intermediate twist angles. In the latter phonon effects are particularly pronounced due to dominating phonon transitions into regions which are characterized by van Hove singularities in the density of states.","sentences":["Excitons in twisted bilayers of transition metal dichalcogenides have strongly modified dispersion relations due to the formation of periodic moir\\'e potentials.","The strong coupling to a light field in an optical cavity leads to the appearance of moir\\'e polaritons.","In this paper, we derive a theoretical model for the linear absorption spectrum of the coupled moir\\'e polariton-phonon system based on the time-convolutionless (TCL) approach.","Results obtained by numerically solving the TCL equation are compared to those obtained in the Markovian limit and from a perturbative treatment of non-Markovian corrections.","A key quantity for the interpretation of the findings is the generalized phonon spectral density.","We discuss the phonon impact on the spectrum for realistic moir\\'e exciton dispersions by varying twist angle and temperature.   ","Key features introduced by the coupling to phonons are broadenings and energy shifts of the upper and lower polariton peak and the appearance of phonon sidebands between them.","We analyze these features with respect to the role of Markovian and non-Markovian effects and find that they strongly depend on the twist angle.","We can distinguish between the regimes of large, small, and intermediate twist angles.","In the latter phonon effects are particularly pronounced due to dominating phonon transitions into regions which are characterized by van Hove singularities in the density of states."],"url":"http://arxiv.org/abs/2406.06330v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 14:43:43","title":"Exploring the generation and annihilation of three dimensional nulls through MHD simulations in initially chaotic magnetic field devoid of nulls","abstract":"Three-dimensional (3D) magnetic nulls are abundant in the solar atmosphere, as been firmly established through contemporary observations. They are established to be important magnetic structures in, for example, jets and circular ribbon flares. While simulations and extrapolations support this, the mechanisms behind 3D null generation remain an open question. Recent magnetohydrodynamics (MHD) simulations propose that magnetic reconnection is responsible for both generating and annihilating 3D nulls, a novel concept. However, these simulations began with initial magnetic fields already supporting pre-existing nulls, raising the question of whether magnetic reconnection can create nulls in fields initially devoid of them. Previously, this question was briefly explored in a simulation with an initial chaotic magnetic field. However, the study failed to precisely identify locations, topological degrees, and natures (spiral or radial) of nulls, and it approximated magnetic reconnection without fully tracking field line in time. In this paper these findings are revisited in light of recent advancements and tools used to locate and trace nulls, along with the tracing of field lines, through which the concept of generation/annihilation of 3D nulls from chaotic fields is established in a precise manner.","sentences":["Three-dimensional (3D) magnetic nulls are abundant in the solar atmosphere, as been firmly established through contemporary observations.","They are established to be important magnetic structures in, for example, jets and circular ribbon flares.","While simulations and extrapolations support this, the mechanisms behind 3D null generation remain an open question.","Recent magnetohydrodynamics (MHD) simulations propose that magnetic reconnection is responsible for both generating and annihilating 3D nulls, a novel concept.","However, these simulations began with initial magnetic fields already supporting pre-existing nulls, raising the question of whether magnetic reconnection can create nulls in fields initially devoid of them.","Previously, this question was briefly explored in a simulation with an initial chaotic magnetic field.","However, the study failed to precisely identify locations, topological degrees, and natures (spiral or radial) of nulls, and it approximated magnetic reconnection without fully tracking field line in time.","In this paper these findings are revisited in light of recent advancements and tools used to locate and trace nulls, along with the tracing of field lines, through which the concept of generation/annihilation of 3D nulls from chaotic fields is established in a precise manner."],"url":"http://arxiv.org/abs/2406.06328v1","category":"astro-ph.SR"}
{"created":"2024-06-10 14:41:14","title":"Navigating Unknowns: Deep Learning Robustness for Gravitational Wave Signal Reconstruction","abstract":"We present a rapid and reliable deep learning-based method for gravitational wave signal reconstruction from elusive, generic binary black hole mergers in LIGO data. We demonstrate that our model, \\texttt{AWaRe}, effectively recovers gravitational waves with parameters it has not encountered during training. This includes features like higher black hole masses, additional harmonics, eccentricity, and varied waveform systematics, which introduce complex modulations in the waveform's amplitudes and phases. The accurate reconstructions of these unseen signal characteristics demonstrates \\texttt{AWaRe}'s ability to handle complex features in the waveforms. By directly incorporating waveform reconstruction uncertainty estimation into the \\texttt{AWaRe} framework, we show that for real gravitational wave events, the uncertainties in \\texttt{AWaRe}'s reconstructions align closely with those achieved by benchmark algorithms like BayesWave and coherent WaveBurst. The robustness of our model to real gravitational wave events and its ability to extrapolate to unseen data open new avenues for investigations in various aspects of gravitational wave astrophysics and data analysis, including tests of General Relativity and the enhancement of current gravitational wave search methodologies.","sentences":["We present a rapid and reliable deep learning-based method for gravitational wave signal reconstruction from elusive, generic binary black hole mergers in LIGO data.","We demonstrate that our model, \\texttt{AWaRe}, effectively recovers gravitational waves with parameters it has not encountered during training.","This includes features like higher black hole masses, additional harmonics, eccentricity, and varied waveform systematics, which introduce complex modulations in the waveform's amplitudes and phases.","The accurate reconstructions of these unseen signal characteristics demonstrates \\texttt{AWaRe}'s ability to handle complex features in the waveforms.","By directly incorporating waveform reconstruction uncertainty estimation into the \\texttt{AWaRe} framework, we show that for real gravitational wave events, the uncertainties in \\texttt{AWaRe}'s reconstructions align closely with those achieved by benchmark algorithms like BayesWave and coherent WaveBurst.","The robustness of our model to real gravitational wave events and its ability to extrapolate to unseen data open new avenues for investigations in various aspects of gravitational wave astrophysics and data analysis, including tests of General Relativity and the enhancement of current gravitational wave search methodologies."],"url":"http://arxiv.org/abs/2406.06324v1","category":"gr-qc"}
{"created":"2024-06-10 14:33:02","title":"Tx-LLM: A Large Language Model for Therapeutics","abstract":"Developing therapeutics is a lengthy and expensive process that requires the satisfaction of many different criteria, and AI models capable of expediting the process would be invaluable. However, the majority of current AI approaches address only a narrowly defined set of tasks, often circumscribed within a particular domain. To bridge this gap, we introduce Tx-LLM, a generalist large language model (LLM) fine-tuned from PaLM-2 which encodes knowledge about diverse therapeutic modalities. Tx-LLM is trained using a collection of 709 datasets that target 66 tasks spanning various stages of the drug discovery pipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide variety of chemical or biological entities(small molecules, proteins, nucleic acids, cell lines, diseases) interleaved with free-text, allowing it to predict a broad range of associated properties, achieving competitive with state-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on 22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text such as cell line names or disease names, likely due to context learned during pretraining. We observe evidence of positive transfer between tasks with diverse drug types (e.g.,tasks involving small molecules and tasks involving proteins), and we study the impact of model size, domain finetuning, and prompting strategies on performance. We believe Tx-LLM represents an important step towards LLMs encoding biochemical knowledge and could have a future role as an end-to-end tool across the drug discovery development pipeline.","sentences":["Developing therapeutics is a lengthy and expensive process that requires the satisfaction of many different criteria, and AI models capable of expediting the process would be invaluable.","However, the majority of current AI approaches address only a narrowly defined set of tasks, often circumscribed within a particular domain.","To bridge this gap, we introduce Tx-LLM, a generalist large language model (LLM) fine-tuned from PaLM-2 which encodes knowledge about diverse therapeutic modalities.","Tx-LLM is trained using a collection of 709 datasets that target 66 tasks spanning various stages of the drug discovery pipeline.","Using a single set of weights, Tx-LLM simultaneously processes a wide variety of chemical or biological entities(small molecules, proteins, nucleic acids, cell lines, diseases) interleaved with free-text, allowing it to predict a broad range of associated properties, achieving competitive with state-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on 22.","Among these, Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text such as cell line names or disease names, likely due to context learned during pretraining.","We observe evidence of positive transfer between tasks with diverse drug types (e.g.,tasks involving small molecules and tasks involving proteins), and we study the impact of model size, domain finetuning, and prompting strategies on performance.","We believe Tx-LLM represents an important step towards LLMs encoding biochemical knowledge and could have a future role as an end-to-end tool across the drug discovery development pipeline."],"url":"http://arxiv.org/abs/2406.06316v1","category":"cs.CL"}
{"created":"2024-06-10 14:26:09","title":"Unsupervised Improved MVDR Beamforming for Sound Enhancement","abstract":"Neural networks have recently become the dominant approach to sound separation. Their good performance relies on large datasets of isolated recordings. For speech and music, isolated single channel data are readily available; however the same does not hold in the multi-channel case, and with most other sound classes. Multi-channel methods have the potential to outperform single channel approaches as they can exploit both spatial and spectral features, but the lack of training data remains a challenge. We propose unsupervised improved minimum variation distortionless response (UIMVDR), which enables multi-channel separation to leverage in-the-wild single-channel data through unsupervised training and beamforming. Results show that UIMVDR generalizes well and improves separation performance compared to supervised models, particularly in cases with limited supervised data. By using data available online, it also reduces the effort required to gather data for multi-channel approaches.","sentences":["Neural networks have recently become the dominant approach to sound separation.","Their good performance relies on large datasets of isolated recordings.","For speech and music, isolated single channel data are readily available; however the same does not hold in the multi-channel case, and with most other sound classes.","Multi-channel methods have the potential to outperform single channel approaches as they can exploit both spatial and spectral features, but the lack of training data remains a challenge.","We propose unsupervised improved minimum variation distortionless response (UIMVDR), which enables multi-channel separation to leverage in-the-wild single-channel data through unsupervised training and beamforming.","Results show that UIMVDR generalizes well and improves separation performance compared to supervised models, particularly in cases with limited supervised data.","By using data available online, it also reduces the effort required to gather data for multi-channel approaches."],"url":"http://arxiv.org/abs/2406.06310v1","category":"cs.SD"}
{"created":"2024-06-10 14:25:11","title":"Is Value Functions Estimation with Classification Plug-and-play for Offline Reinforcement Learning?","abstract":"In deep Reinforcement Learning (RL), value functions are typically approximated using deep neural networks and trained via mean squared error regression objectives to fit the true value functions. Recent research has proposed an alternative approach, utilizing the cross-entropy classification objective, which has demonstrated improved performance and scalability of RL algorithms. However, existing study have not extensively benchmarked the effects of this replacement across various domains, as the primary objective was to demonstrate the efficacy of the concept across a broad spectrum of tasks, without delving into in-depth analysis. Our work seeks to empirically investigate the impact of such a replacement in an offline RL setup and analyze the effects of different aspects on performance. Through large-scale experiments conducted across a diverse range of tasks using different algorithms, we aim to gain deeper insights into the implications of this approach. Our results reveal that incorporating this change can lead to superior performance over state-of-the-art solutions for some algorithms in certain tasks, while maintaining comparable performance levels in other tasks, however for other algorithms this modification might lead to the dramatic performance drop. This findings are crucial for further application of classification approach in research and practical tasks.","sentences":["In deep Reinforcement Learning (RL), value functions are typically approximated using deep neural networks and trained via mean squared error regression objectives to fit the true value functions.","Recent research has proposed an alternative approach, utilizing the cross-entropy classification objective, which has demonstrated improved performance and scalability of RL algorithms.","However, existing study have not extensively benchmarked the effects of this replacement across various domains, as the primary objective was to demonstrate the efficacy of the concept across a broad spectrum of tasks, without delving into in-depth analysis.","Our work seeks to empirically investigate the impact of such a replacement in an offline RL setup and analyze the effects of different aspects on performance.","Through large-scale experiments conducted across a diverse range of tasks using different algorithms, we aim to gain deeper insights into the implications of this approach.","Our results reveal that incorporating this change can lead to superior performance over state-of-the-art solutions for some algorithms in certain tasks, while maintaining comparable performance levels in other tasks, however for other algorithms this modification might lead to the dramatic performance drop.","This findings are crucial for further application of classification approach in research and practical tasks."],"url":"http://arxiv.org/abs/2406.06309v1","category":"cs.LG"}
{"created":"2024-06-10 14:23:25","title":"Building Continuous Quantum-Classical Bayesian Neural Networks for a Classical Clinical Dataset","abstract":"In this work, we are introducing a Quantum-Classical Bayesian Neural Network (QCBNN) that is capable to perform uncertainty-aware classification of classical medical dataset. This model is a symbiosis of a classical Convolutional NN that performs ultra-sound image processing and a quantum circuit that generates its stochastic weights, within a Bayesian learning framework. To test the utility of this idea for the possible future deployment in the medical sector we track multiple behavioral metrics that capture both predictive performance as well as model's uncertainty. It is our ambition to create a hybrid model that is capable to classify samples in a more uncertainty aware fashion, which will advance the trustworthiness of these models and thus bring us step closer to utilizing them in the industry. We test multiple setups for quantum circuit for this task, and our best architectures display bigger uncertainty gap between correctly and incorrectly identified samples than its classical benchmark at an expense of a slight drop in predictive performance. The innovation of this paper is two-fold: (1) combining of different approaches that allow the stochastic weights from the quantum circuit to be continues thus allowing the model to classify application-driven dataset; (2) studying architectural features of quantum circuit that make-or-break these models, which pave the way into further investigation of more informed architectural designs.","sentences":["In this work, we are introducing a Quantum-Classical Bayesian Neural Network (QCBNN) that is capable to perform uncertainty-aware classification of classical medical dataset.","This model is a symbiosis of a classical Convolutional NN that performs ultra-sound image processing and a quantum circuit that generates its stochastic weights, within a Bayesian learning framework.","To test the utility of this idea for the possible future deployment in the medical sector we track multiple behavioral metrics that capture both predictive performance as well as model's uncertainty.","It is our ambition to create a hybrid model that is capable to classify samples in a more uncertainty aware fashion, which will advance the trustworthiness of these models and thus bring us step closer to utilizing them in the industry.","We test multiple setups for quantum circuit for this task, and our best architectures display bigger uncertainty gap between correctly and incorrectly identified samples than its classical benchmark at an expense of a slight drop in predictive performance.","The innovation of this paper is two-fold: (1) combining of different approaches that allow the stochastic weights from the quantum circuit to be continues thus allowing the model to classify application-driven dataset; (2) studying architectural features of quantum circuit that make-or-break these models, which pave the way into further investigation of more informed architectural designs."],"url":"http://arxiv.org/abs/2406.06307v1","category":"quant-ph"}
{"created":"2024-06-10 14:19:09","title":"Evolution of the autoresonant plasma wave excitation in two-dimensional particle-in-cell simulations","abstract":"The generation of an autoresonantly phase-locked high amplitude plasma waves to the chirped beat frequency of two driving lasers is studied in two dimensions using particle-in-cell simulations. The two-dimensional plasma and laser parameters correspond to those that optimized the plasma wave amplitude in one-dimensional simulations. Near the start of autoresonant locking, the two-dimensional simulations appear similar to one-dimensional particle-in-cell results [Luo et al., Phys. Rev. Res. 6, 013338 (2024)] with plasma wave amplitudes above the Rosenbluth-Liu limit. Later, just below wave-breaking, the two-dimensional simulation exhibits a Weibel-like instability and eventually laser beam filamentation. These limit the coherence of the plasma oscillation after the peak plasma wave field is obtained. In spite of the reduction of spatial coherence of the accelerating density structure, the acceleration of self-injected electrons in the case studied remains at $70\\%$ to $80\\%$ of that observed in one dimension. Other effects such as plasma wave bowing are discussed.","sentences":["The generation of an autoresonantly phase-locked high amplitude plasma waves to the chirped beat frequency of two driving lasers is studied in two dimensions using particle-in-cell simulations.","The two-dimensional plasma and laser parameters correspond to those that optimized the plasma wave amplitude in one-dimensional simulations.","Near the start of autoresonant locking, the two-dimensional simulations appear similar to one-dimensional particle-in-cell results","[Luo et al., Phys. Rev. Res. 6, 013338 (2024)] with plasma wave amplitudes above the Rosenbluth-Liu limit.","Later, just below wave-breaking, the two-dimensional simulation exhibits a Weibel-like instability and eventually laser beam filamentation.","These limit the coherence of the plasma oscillation after the peak plasma wave field is obtained.","In spite of the reduction of spatial coherence of the accelerating density structure, the acceleration of self-injected electrons in the case studied remains at $70\\%$ to $80\\%$ of that observed in one dimension.","Other effects such as plasma wave bowing are discussed."],"url":"http://arxiv.org/abs/2406.06303v1","category":"physics.plasm-ph"}
{"created":"2024-06-10 14:18:56","title":"Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks","abstract":"The recent release of GPT-4o has garnered widespread attention due to its powerful general capabilities. While its impressive performance is widely acknowledged, its safety aspects have not been sufficiently explored. Given the potential societal impact of risky content generated by advanced generative AI such as GPT-4o, it is crucial to rigorously evaluate its safety. In response to this question, this paper for the first time conducts a rigorous evaluation of GPT-4o against jailbreak attacks. Specifically, this paper adopts a series of multi-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks encompassing three modalities (\\ie, text, speech, and image), which involves the optimization of over 4,000 initial text queries and the analysis and statistical evaluation of nearly 8,000+ response on GPT-4o. Our extensive experiments reveal several novel observations: (1) In contrast to the previous version (such as GPT-4V), GPT-4o has enhanced safety in the context of text modality jailbreak; (2) The newly introduced audio modality opens up new attack vectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal jailbreak attack methods are largely ineffective against GPT-4o and GPT-4V. These findings provide critical insights into the safety implications of GPT-4o and underscore the need for robust alignment guardrails in large models. Our code is available at \\url{https://github.com/NY1024/Jailbreak_GPT4o}.","sentences":["The recent release of GPT-4o has garnered widespread attention due to its powerful general capabilities.","While its impressive performance is widely acknowledged, its safety aspects have not been sufficiently explored.","Given the potential societal impact of risky content generated by advanced generative AI such as GPT-4o, it is crucial to rigorously evaluate its safety.","In response to this question, this paper for the first time conducts a rigorous evaluation of GPT-4o against jailbreak attacks.","Specifically, this paper adopts a series of multi-modal and uni-modal jailbreak attacks on 4 commonly used benchmarks encompassing three modalities (\\ie, text, speech, and image), which involves the optimization of over 4,000 initial text queries and the analysis and statistical evaluation of nearly 8,000+ response on GPT-4o.","Our extensive experiments reveal several novel observations: (1) In contrast to the previous version (such as GPT-4V), GPT-4o has enhanced safety in the context of text modality jailbreak; (2) The newly introduced audio modality opens up new attack vectors for jailbreak attacks on GPT-4o; (3) Existing black-box multimodal jailbreak attack methods are largely ineffective against GPT-4o and GPT-4V. These findings provide critical insights into the safety implications of GPT-4o and underscore the need for robust alignment guardrails in large models.","Our code is available at \\url{https://github.com/NY1024/Jailbreak_GPT4o}."],"url":"http://arxiv.org/abs/2406.06302v1","category":"cs.CR"}
{"created":"2024-06-10 14:16:28","title":"Zero-Shot Audio Captioning Using Soft and Hard Prompts","abstract":"In traditional audio captioning methods, a model is usually trained in a fully supervised manner using a human-annotated dataset containing audio-text pairs and then evaluated on the test sets from the same dataset. Such methods have two limitations. First, these methods are often data-hungry and require time-consuming and expensive human annotations to obtain audio-text pairs. Second, these models often suffer from performance degradation in cross-domain scenarios, i.e., when the input audio comes from a different domain than the training set, which, however, has received little attention. We propose an effective audio captioning method based on the contrastive language-audio pre-training (CLAP) model to address these issues. Our proposed method requires only textual data for training, enabling the model to generate text from the textual feature in the cross-modal semantic space.In the inference stage, the model generates the descriptive text for the given audio from the audio feature by leveraging the audio-text alignment from CLAP.We devise two strategies to mitigate the discrepancy between text and audio embeddings: a mixed-augmentation-based soft prompt and a retrieval-based acoustic-aware hard prompt. These approaches are designed to enhance the generalization performance of our proposed model, facilitating the model to generate captions more robustly and accurately. Extensive experiments on AudioCaps and Clotho benchmarks show the effectiveness of our proposed method, which outperforms other zero-shot audio captioning approaches for in-domain scenarios and outperforms the compared methods for cross-domain scenarios, underscoring the generalization ability of our method.","sentences":["In traditional audio captioning methods, a model is usually trained in a fully supervised manner using a human-annotated dataset containing audio-text pairs and then evaluated on the test sets from the same dataset.","Such methods have two limitations.","First, these methods are often data-hungry and require time-consuming and expensive human annotations to obtain audio-text pairs.","Second, these models often suffer from performance degradation in cross-domain scenarios, i.e., when the input audio comes from a different domain than the training set, which, however, has received little attention.","We propose an effective audio captioning method based on the contrastive language-audio pre-training (CLAP) model to address these issues.","Our proposed method requires only textual data for training, enabling the model to generate text from the textual feature in the cross-modal semantic space.","In the inference stage, the model generates the descriptive text for the given audio from the audio feature by leveraging the audio-text alignment from CLAP.We devise two strategies to mitigate the discrepancy between text and audio embeddings: a mixed-augmentation-based soft prompt and a retrieval-based acoustic-aware hard prompt.","These approaches are designed to enhance the generalization performance of our proposed model, facilitating the model to generate captions more robustly and accurately.","Extensive experiments on AudioCaps and Clotho benchmarks show the effectiveness of our proposed method, which outperforms other zero-shot audio captioning approaches for in-domain scenarios and outperforms the compared methods for cross-domain scenarios, underscoring the generalization ability of our method."],"url":"http://arxiv.org/abs/2406.06295v1","category":"cs.SD"}
{"created":"2024-06-10 14:15:47","title":"Exact formulae for ranks of partitions","abstract":"In 2009, Bringmann arXiv:0708.0691 [math.NT] used the circle method to prove an asymptotic formula for the Fourier coefficients of rank generating functions. In this paper, we prove that Bringmann's formula, when summing up to infinity and in the case of prime modulus, gives a Rademacher-type exact formula involving sums of vector-valued Kloosterman sums. As a corollary, in an upcoming paper, we will provide a new proof of Dyson's conjectures by showing that the certain Kloosterman sums vanish.","sentences":["In 2009, Bringmann arXiv:0708.0691","[math.NT] used the circle method to prove an asymptotic formula for the Fourier coefficients of rank generating functions.","In this paper, we prove that Bringmann's formula, when summing up to infinity and in the case of prime modulus, gives a Rademacher-type exact formula involving sums of vector-valued Kloosterman sums.","As a corollary, in an upcoming paper, we will provide a new proof of Dyson's conjectures by showing that the certain Kloosterman sums vanish."],"url":"http://arxiv.org/abs/2406.06294v1","category":"math.NT"}
{"created":"2024-06-10 14:13:29","title":"Modeling inclusive electron-nucleus scattering with Bayesian artificial neural networks","abstract":"We introduce a Bayesian protocol based on artificial neural networks that is suitable for modeling inclusive electron-nucleus scattering on a variety of nuclear targets with quantified uncertainties. Unlike previous applications in the field, which directly parameterize the cross sections, our approach employs artificial neural networks to represent the longitudinal and transverse response functions. In contrast to cross sections, which depend on the incoming energy, scattering angle, and energy transfer, the response functions are determined solely by the energy and momentum transfer to the system, allowing the angular component to be treated analytically. We assess the accuracy and predictive power of our framework against the extensive data in the quasielastic inclusive electron-scattering database. Additionally, we present novel extractions of the longitudinal and transverse response functions and compare them with previous experimental analysis and nuclear ab-initio calculations.","sentences":["We introduce a Bayesian protocol based on artificial neural networks that is suitable for modeling inclusive electron-nucleus scattering on a variety of nuclear targets with quantified uncertainties.","Unlike previous applications in the field, which directly parameterize the cross sections, our approach employs artificial neural networks to represent the longitudinal and transverse response functions.","In contrast to cross sections, which depend on the incoming energy, scattering angle, and energy transfer, the response functions are determined solely by the energy and momentum transfer to the system, allowing the angular component to be treated analytically.","We assess the accuracy and predictive power of our framework against the extensive data in the quasielastic inclusive electron-scattering database.","Additionally, we present novel extractions of the longitudinal and transverse response functions and compare them with previous experimental analysis and nuclear ab-initio calculations."],"url":"http://arxiv.org/abs/2406.06292v1","category":"nucl-th"}
{"created":"2024-06-10 14:11:34","title":"Ladder mice","abstract":"Assume ZF + AD + $V=L(\\mathbb{R})$. We prove some \"mouse set\" theorems, for definability over $J_\\alpha(\\mathbb{R})$ where $[\\alpha,\\alpha]$ is a projective-like gap (of $L(\\mathbb{R})$) and $\\alpha$ is either a successor ordinal or has countable cofinality, but $\\alpha\\neq\\beta+1$ where $\\beta$ ends a strong gap. For such ordinals $\\alpha$ and integers $n\\geq 1$, we show that there is a mouse $M$ with $\\mathbb{R}\\cap M=\\mathrm{OD}_{\\alpha n}$.   The proof involves an analysis of ladder mice and their generalizations to $J_\\alpha(\\mathbb{R})$. This analysis is related to earlier work of Rudominer, Woodin and Steel on ladder mice. However, it also yields a new proof of the mouse set theorem even at the least point where ladder mice arise -- one which avoids the stationary tower. The analysis also yields a corresponding \"anti-correctness\" result on a cone, generalizing facts familiar in the projective hierarchy; for example, that $(\\Pi^1_3)^V\\upharpoonright M_1$ truth is $(\\Sigma^1_3)^{M_1}$-definable and $(\\Sigma^1_3)^{M_1}$ truth is $(\\Pi^1_3)^V\\upharpoonright M_1$-definable.   We also define and study versions of ladder mice on a cone at the end of weak gap, and at the successor of the end of a strong gap, and an anti-correctness result on a cone there.","sentences":["Assume ZF + AD + $V=L(\\mathbb{R})$. We prove some \"mouse set\" theorems, for definability over $J_\\alpha(\\mathbb{R})$ where $[\\alpha,\\alpha]$ is a projective-like gap (of $L(\\mathbb{R})$) and $\\alpha$ is either a successor ordinal or has countable cofinality, but $\\alpha\\neq\\beta+1$ where $\\beta$ ends a strong gap.","For such ordinals $\\alpha$ and integers $n\\geq 1$, we show that there is a mouse $M$ with $\\mathbb{R}\\cap M=\\mathrm{OD}_{\\alpha n}$.   The proof involves an analysis of ladder mice and their generalizations to $J_\\alpha(\\mathbb{R})$. This analysis is related to earlier work of Rudominer, Woodin and Steel on ladder mice.","However, it also yields a new proof of the mouse set theorem even at the least point where ladder mice arise -- one which avoids the stationary tower.","The analysis also yields a corresponding \"anti-correctness\" result on a cone, generalizing facts familiar in the projective hierarchy; for example, that $(\\Pi^1_3)^V\\upharpoonright M_1$ truth is $(\\Sigma^1_3)^{M_1}$-definable and $(\\Sigma^1_3)^{M_1}$ truth is $(\\Pi^1_3)^V\\upharpoonright M_1$-definable.   ","We also define and study versions of ladder mice on a cone at the end of weak gap, and at the successor of the end of a strong gap, and an anti-correctness result on a cone there."],"url":"http://arxiv.org/abs/2406.06289v1","category":"math.LO"}
{"created":"2024-06-10 14:01:21","title":"PowerInfer-2: Fast Large Language Model Inference on a Smartphone","abstract":"This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.","sentences":["This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity.","The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations.","Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference.","Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations.","The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks.","Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone.","For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM.","For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2."],"url":"http://arxiv.org/abs/2406.06282v1","category":"cs.LG"}
{"created":"2024-06-10 13:53:58","title":"Relativistic and wide-angle corrections to galaxy power spectra","abstract":"Galaxy surveys contain information on the largest scales via wide-angle and relativistic contributions. By combining two different galaxy populations, we can suppress the strong cosmic variance on ultra-large scales and thus enhance the detectability of the signals. The relativistic Doppler and Sachs-Wolfe effects are of a similar magnitude to the leading wide-angle corrections, so that it is important to treat them together, especially since they can partially cancel. The power spectra depend on the choice of line of sight for each galaxy pair and we present results for a general line of sight. Then we estimate the detection significance of the auto- and cross-power spectra for a variety of cases. We use two futuristic galaxy samples based on a `beyond-DESI' survey and a SKA Phase 2 survey, covering 15,000\\,deg$^2$ up to $z=1$. We find a detection significance for the total relativistic wide-angle effects that ranges from $\\sim 5\\sigma$ to $>15\\sigma$, depending on the line-of-sight configuration.","sentences":["Galaxy surveys contain information on the largest scales via wide-angle and relativistic contributions.","By combining two different galaxy populations, we can suppress the strong cosmic variance on ultra-large scales and thus enhance the detectability of the signals.","The relativistic Doppler and Sachs-Wolfe effects are of a similar magnitude to the leading wide-angle corrections, so that it is important to treat them together, especially since they can partially cancel.","The power spectra depend on the choice of line of sight for each galaxy pair and we present results for a general line of sight.","Then we estimate the detection significance of the auto- and cross-power spectra for a variety of cases.","We use two futuristic galaxy samples based on a `beyond-DESI' survey and a SKA Phase 2 survey, covering 15,000\\,deg$^2$ up to $z=1$. We find a detection significance for the total relativistic wide-angle effects that ranges from $\\sim 5\\sigma$ to $>15\\sigma$, depending on the line-of-sight configuration."],"url":"http://arxiv.org/abs/2406.06274v1","category":"astro-ph.CO"}
{"created":"2024-06-10 13:52:45","title":"Improving the long-term stability of new-generation perovskite-based TCO using binary and ternary oxides capping layers","abstract":"We report the impact of capping layers on vanadate based transparent conductive oxides (TCOs) to prolong the thermal stability with a minimal loss of resistivity during heat treatment in ambient environment. In the present study, various protecting layers (amorphous Al2O3, LaAlO3 (LAO), TiO2 grown in base pressure and TiO2 deposited under oxygen partial pressure) are grown in-situ on polycrystalline perovskite SrVO3 (SVO) thin films using Pulsed Laser Deposition (PLD). The results show that amorphous LaAlO3 is the most promising protection layer among the oxide layers, to preserve both electrical and optical properties of perovskite SVO films from natural as well as artificial aging. Our present approach for a capping layer on SVO may address the long-term stability issues of correlated TCOs and would open an opportunity for the future oxide electronics applications.","sentences":["We report the impact of capping layers on vanadate based transparent conductive oxides (TCOs) to prolong the thermal stability with a minimal loss of resistivity during heat treatment in ambient environment.","In the present study, various protecting layers (amorphous Al2O3, LaAlO3 (LAO), TiO2 grown in base pressure and TiO2 deposited under oxygen partial pressure) are grown in-situ on polycrystalline perovskite SrVO3 (SVO) thin films using Pulsed Laser Deposition (PLD).","The results show that amorphous LaAlO3 is the most promising protection layer among the oxide layers, to preserve both electrical and optical properties of perovskite SVO films from natural as well as artificial aging.","Our present approach for a capping layer on SVO may address the long-term stability issues of correlated TCOs and would open an opportunity for the future oxide electronics applications."],"url":"http://arxiv.org/abs/2406.06271v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 13:52:37","title":"Recursive algorithm for generating high-temperature expansions for spin systems and the chiral non-linear susceptibility","abstract":"We show that the high-temperature expansion of the free energy and arbitrary connected correlation functions of quantum spin systems can be recursively obtained from the exact renormalization group flow equation for the generating functional of connected spin correlation functions derived by Krieg and Kopietz [Phys. Rev. B 99, 060403(R) (2019)]. Our recursive algorithm can be explicitly written down in closed form including all combinatorial factors. We use our method to estimate critical temperatures of Heisenberg magnets from low-order truncations of the inverse spin susceptibility in the static limit. We also calculate the connected correlation function involving three different spin components (chiral non-linear susceptibility) of quantum Heisenberg magnets up to second order in the exchange couplings.","sentences":["We show that the high-temperature expansion of the free energy and arbitrary connected correlation functions of quantum spin systems can be recursively obtained from the exact renormalization group flow equation for the generating functional of connected spin correlation functions derived by Krieg and Kopietz [Phys.","Rev. B 99, 060403(R) (2019)].","Our recursive algorithm can be explicitly written down in closed form including all combinatorial factors.","We use our method to estimate critical temperatures of Heisenberg magnets from low-order truncations of the inverse spin susceptibility in the static limit.","We also calculate the connected correlation function involving three different spin components (chiral non-linear susceptibility) of quantum Heisenberg magnets up to second order in the exchange couplings."],"url":"http://arxiv.org/abs/2406.06270v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 13:50:42","title":"On antiferromagnetic regimes in the Ashkin-Teller model","abstract":"The Ashkin-Teller model can be represented by a pair of Ising spin configurations with coupling constants $J$ and $J'$ for each, and $U$ for their product. We study this representation on the square lattice and confirm the presence of a mixed antiferromagnetic phase in the isotropic case ($J=J'$) when $-U>0$ is sufficiently large and $J=J'>0$ is sufficiently small, by means of a graphical representation. We then construct a coupling with the eight-vertex model and show that, in analogy to the first result, the corresponding height function is localised, although with antiferromagnetically ordered heights on one class of the graph.   Using the OSSS inequality, we proceed to establish a subcritical sharpness statement along suitable curves in the anisotropic ($J\\neq J'$) phase diagram when $U<0$, circumventing the difficulty of the lack of general monotonicity properties in the parameters. We then address the isotropic case and provide indications of monotonicity.","sentences":["The Ashkin-Teller model can be represented by a pair of Ising spin configurations with coupling constants $J$ and $J'$ for each, and $U$ for their product.","We study this representation on the square lattice and confirm the presence of a mixed antiferromagnetic phase in the isotropic case ($J=J'$) when $-U>0$ is sufficiently large and $J=J'>0$ is sufficiently small, by means of a graphical representation.","We then construct a coupling with the eight-vertex model and show that, in analogy to the first result, the corresponding height function is localised, although with antiferromagnetically ordered heights on one class of the graph.   ","Using the OSSS inequality, we proceed to establish a subcritical sharpness statement along suitable curves in the anisotropic ($J\\neq J'$) phase diagram when $U<0$, circumventing the difficulty of the lack of general monotonicity properties in the parameters.","We then address the isotropic case and provide indications of monotonicity."],"url":"http://arxiv.org/abs/2406.06266v1","category":"math.PR"}
{"created":"2024-06-10 13:44:07","title":"Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum Learning","abstract":"Structural modularity is a pervasive feature of biological neural networks, which have been linked to several functional and computational advantages. Yet, the use of modular architectures in artificial neural networks has been relatively limited despite early successes. Here, we explore the performance and functional dynamics of a modular network trained on a memory task via an iterative growth curriculum. We find that for a given classical, non-modular recurrent neural network (RNN), an equivalent modular network will perform better across multiple metrics, including training time, generalizability, and robustness to some perturbations. We further examine how different aspects of a modular network's connectivity contribute to its computational capability. We then demonstrate that the inductive bias introduced by the modular topology is strong enough for the network to perform well even when the connectivity within modules is fixed and only the connections between modules are trained. Our findings suggest that gradual modular growth of RNNs could provide advantages for learning increasingly complex tasks on evolutionary timescales, and help build more scalable and compressible artificial networks.","sentences":["Structural modularity is a pervasive feature of biological neural networks, which have been linked to several functional and computational advantages.","Yet, the use of modular architectures in artificial neural networks has been relatively limited despite early successes.","Here, we explore the performance and functional dynamics of a modular network trained on a memory task via an iterative growth curriculum.","We find that for a given classical, non-modular recurrent neural network (RNN), an equivalent modular network will perform better across multiple metrics, including training time, generalizability, and robustness to some perturbations.","We further examine how different aspects of a modular network's connectivity contribute to its computational capability.","We then demonstrate that the inductive bias introduced by the modular topology is strong enough for the network to perform well even when the connectivity within modules is fixed and only the connections between modules are trained.","Our findings suggest that gradual modular growth of RNNs could provide advantages for learning increasingly complex tasks on evolutionary timescales, and help build more scalable and compressible artificial networks."],"url":"http://arxiv.org/abs/2406.06262v1","category":"cs.NE"}
{"created":"2024-06-10 13:43:07","title":"What All the PHUZZ Is About: A Coverage-guided Fuzzer for Finding Vulnerabilities in PHP Web Applications","abstract":"Coverage-guided fuzz testing has received significant attention from the research community, with a strong focus on binary applications, greatly disregarding other targets, such as web applications. The importance of the World Wide Web in everyone's life cannot be overstated, and to this day, many web applications are developed in PHP. In this work, we address the challenges of applying coverage-guided fuzzing to PHP web applications and introduce PHUZZ, a modular fuzzing framework for PHP web applications. PHUZZ uses novel approaches to detect more client-side and server-side vulnerability classes than state-of-the-art related work, including SQL injections, remote command injections, insecure deserialization, path traversal, external entity injection, cross-site scripting, and open redirection. We evaluate PHUZZ on a diverse set of artificial and real-world web applications with known and unknown vulnerabilities, and compare it against a variety of state-of-the-art fuzzers. In order to show PHUZZ' effectiveness, we fuzz over 1,000 API endpoints of the 115 most popular WordPress plugins, resulting in over 20 security issues and 2 new CVE-IDs. Finally, we make the framework publicly available to motivate and encourage further research on web application fuzz testing.","sentences":["Coverage-guided fuzz testing has received significant attention from the research community, with a strong focus on binary applications, greatly disregarding other targets, such as web applications.","The importance of the World Wide Web in everyone's life cannot be overstated, and to this day, many web applications are developed in PHP.","In this work, we address the challenges of applying coverage-guided fuzzing to PHP web applications and introduce PHUZZ, a modular fuzzing framework for PHP web applications.","PHUZZ uses novel approaches to detect more client-side and server-side vulnerability classes than state-of-the-art related work, including SQL injections, remote command injections, insecure deserialization, path traversal, external entity injection, cross-site scripting, and open redirection.","We evaluate PHUZZ on a diverse set of artificial and real-world web applications with known and unknown vulnerabilities, and compare it against a variety of state-of-the-art fuzzers.","In order to show PHUZZ' effectiveness, we fuzz over 1,000 API endpoints of the 115 most popular WordPress plugins, resulting in over 20 security issues and 2 new CVE-IDs.","Finally, we make the framework publicly available to motivate and encourage further research on web application fuzz testing."],"url":"http://arxiv.org/abs/2406.06261v1","category":"cs.CR"}
{"created":"2024-06-10 13:42:24","title":"The $n$-Queens Problem in Higher Dimensions","abstract":"How many mutually non-attacking queens can be placed on a d-dimensional chessboard of size n? The n-queens problem in higher dimensions is a generalization of the well-known n-queens problem. We provide a comprehensive overview of theoretical results, bounds, solution methods, and the interconnectivity of the problem within topics of discrete optimization and combinatorics. We present an integer programming formulation of the n-queens problem in higher dimensions and several strengthenings through additional valid inequalities. Compared to recent benchmarks, we achieve a speedup in computational time between 15-70x over all instances of the integer programs. Our computational results prove optimality of certificates for several large instances. Breaking additional, previously unsolved instances with the proposed methods is likely possible. On the primal side, we further discuss heuristic approaches to constructing solutions that turn out to be optimal when compared to the IP. We conclude with preliminary results on the number and density of the solutions.","sentences":["How many mutually non-attacking queens can be placed on a d-dimensional chessboard of size n?","The n-queens problem in higher dimensions is a generalization of the well-known n-queens problem.","We provide a comprehensive overview of theoretical results, bounds, solution methods, and the interconnectivity of the problem within topics of discrete optimization and combinatorics.","We present an integer programming formulation of the n-queens problem in higher dimensions and several strengthenings through additional valid inequalities.","Compared to recent benchmarks, we achieve a speedup in computational time between 15-70x over all instances of the integer programs.","Our computational results prove optimality of certificates for several large instances.","Breaking additional, previously unsolved instances with the proposed methods is likely possible.","On the primal side, we further discuss heuristic approaches to constructing solutions that turn out to be optimal when compared to the IP.","We conclude with preliminary results on the number and density of the solutions."],"url":"http://arxiv.org/abs/2406.06260v1","category":"math.OC"}
{"created":"2024-06-10 13:41:10","title":"Tuning-Free Visual Customization via View Iterative Self-Attention Control","abstract":"Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities. While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications. In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge. Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model. Initially, we obtain the initial noise for both the reference and target images through DDIM inversion. Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism. Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image. This approach results in consistent and harmonious editing with only one reference image in a few denoising steps. Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains. Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes.","sentences":["Fine-Tuning Diffusion Models enable a wide range of personalized generation and editing applications on diverse visual modalities.","While Low-Rank Adaptation (LoRA) accelerates the fine-tuning process, it still requires multiple reference images and time-consuming training, which constrains its scalability for large-scale and real-time applications.","In this paper, we propose \\textit{View Iterative Self-Attention Control (VisCtrl)} to tackle this challenge.","Specifically, VisCtrl is a training-free method that injects the appearance and structure of a user-specified subject into another subject in the target image, unlike previous approaches that require fine-tuning the model.","Initially, we obtain the initial noise for both the reference and target images through DDIM inversion.","Then, during the denoising phase, features from the reference image are injected into the target image via the self-attention mechanism.","Notably, by iteratively performing this feature injection process, we ensure that the reference image features are gradually integrated into the target image.","This approach results in consistent and harmonious editing with only one reference image in a few denoising steps.","Moreover, benefiting from our plug-and-play architecture design and the proposed Feature Gradual Sampling strategy for multi-view editing, our method can be easily extended to edit in complex visual domains.","Extensive experiments show the efficacy of VisCtrl across a spectrum of tasks, including personalized editing of images, videos, and 3D scenes."],"url":"http://arxiv.org/abs/2406.06258v1","category":"cs.CV"}
{"created":"2024-06-10 13:31:18","title":"Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning","abstract":"As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.","sentences":["As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations.","In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module.","To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches.","Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality.","Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter.","Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups."],"url":"http://arxiv.org/abs/2406.06251v1","category":"eess.AS"}
{"created":"2024-06-10 13:23:00","title":"Data-Efficient Learning with Neural Programs","abstract":"Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.","sentences":["Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM.","We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite.","When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components.","We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components.","For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature.","Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks.","For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner."],"url":"http://arxiv.org/abs/2406.06246v1","category":"cs.LG"}
{"created":"2024-06-10 13:14:43","title":"Tjurina spectrum and Hertling conjecture","abstract":"We present a proof of a conjecture of Q. Shi, Y. Wang and H. Zuo claiming that the maximal spectral number of a hypersurface isolated singularity does not belong to the Tjurina spectrum. This follows from the self-duality of the Jacobian ring, which is compatible with the action of $f$ and also with the $V$-filtration. We also provide a sufficient condition for the generalized Hertling conjecture on the variance of Tjurina spectrum to fail, and calculate some examples using some codes in Singular.","sentences":["We present a proof of a conjecture of Q. Shi, Y. Wang and H. Zuo claiming that the maximal spectral number of a hypersurface isolated singularity does not belong to the Tjurina spectrum.","This follows from the self-duality of the Jacobian ring, which is compatible with the action of $f$ and also with the $V$-filtration.","We also provide a sufficient condition for the generalized Hertling conjecture on the variance of Tjurina spectrum to fail, and calculate some examples using some codes in Singular."],"url":"http://arxiv.org/abs/2406.06242v1","category":"math.AG"}
{"created":"2024-06-10 13:08:31","title":"I-MPN: Inductive Message Passing Network for Effective and Efficient Human-in-the-Loop Annotation of Mobile Eye Tracking Data","abstract":"Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design. Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights. However, manual analysis of these recordings is time-intensive. In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings. Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions. This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment. Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback. Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation.","sentences":["Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design.","Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights.","However, manual analysis of these recordings is time-intensive.","In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings.","Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions.","This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment.","Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback.","Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation."],"url":"http://arxiv.org/abs/2406.06239v1","category":"cs.CV"}
{"created":"2024-06-10 13:06:28","title":"UnSupDLA: Towards Unsupervised Document Layout Analysis","abstract":"Document layout analysis is a key area in document research, involving techniques like text mining and visual analysis. Despite various methods developed to tackle layout analysis, a critical but frequently overlooked problem is the scarcity of labeled data needed for analyses. With the rise of internet use, an overwhelming number of documents are now available online, making the process of accurately labeling them for research purposes increasingly challenging and labor-intensive. Moreover, the diversity of documents online presents a unique set of challenges in maintaining the quality and consistency of these labels, further complicating document layout analysis in the digital era. To address this, we employ a vision-based approach for analyzing document layouts designed to train a network without labels. Instead, we focus on pre-training, initially generating simple object masks from the unlabeled document images. These masks are then used to train a detector, enhancing object detection and segmentation performance. The model's effectiveness is further amplified through several unsupervised training iterations, continuously refining its performance. This approach significantly advances document layout analysis, particularly precision and efficiency, without labels.","sentences":["Document layout analysis is a key area in document research, involving techniques like text mining and visual analysis.","Despite various methods developed to tackle layout analysis, a critical but frequently overlooked problem is the scarcity of labeled data needed for analyses.","With the rise of internet use, an overwhelming number of documents are now available online, making the process of accurately labeling them for research purposes increasingly challenging and labor-intensive.","Moreover, the diversity of documents online presents a unique set of challenges in maintaining the quality and consistency of these labels, further complicating document layout analysis in the digital era.","To address this, we employ a vision-based approach for analyzing document layouts designed to train a network without labels.","Instead, we focus on pre-training, initially generating simple object masks from the unlabeled document images.","These masks are then used to train a detector, enhancing object detection and segmentation performance.","The model's effectiveness is further amplified through several unsupervised training iterations, continuously refining its performance.","This approach significantly advances document layout analysis, particularly precision and efficiency, without labels."],"url":"http://arxiv.org/abs/2406.06236v1","category":"cs.CV"}
{"created":"2024-06-10 13:06:13","title":"Adaptive combinations of tail-risk forecasts","abstract":"In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES). However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures. Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS). In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models. Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications.","sentences":["In order to meet the increasingly stringent global standards of banking management and regulation, several methods have been proposed in the literature for forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected Shortfall (ES).","However, regardless of the approach used, there are several sources of uncertainty, including model specifications, data-related issues and the estimation procedure, which can significantly affect the accuracy of VaR and ES measures.","Aiming to mitigate the influence of these sources of uncertainty and improve the predictive performance of individual models, we propose novel forecast combination strategies based on the Model Confidence Set (MCS).","In particular, consistent joint VaR and ES loss functions within the MCS framework are used to adaptively combine forecasts generated by a wide range of parametric, semi-parametric, and non-parametric models.","Our results reveal that the proposed combined predictors provide a suitable alternative for forecasting risk measures, passing the usual backtests, entering the set of superior models of the MCS, and usually exhibiting lower standard deviations than other model specifications."],"url":"http://arxiv.org/abs/2406.06235v1","category":"q-fin.RM"}
{"created":"2024-06-10 13:06:09","title":"Quantum thermodynamics with coherence: Covariant Gibbs-preserving operation is characterized by the free energy","abstract":"The resource theory with covariant Gibbs-preserving operations, also called enhanced thermal operations, is investigated. We prove that with the help of a correlated catalyst, the state convertibility for any coherent state is fully characterized by the free energy defined with the quantum relative entropy. We can extend this result to general resource theories in the form that imposing the covariant condition to a general resource theory does not change the state convertibility as long as the initial state is coherent and distillable. This means that the additional constraint from the law of energy conservation is irrelevant in the correlated-catalytic framework.","sentences":["The resource theory with covariant Gibbs-preserving operations, also called enhanced thermal operations, is investigated.","We prove that with the help of a correlated catalyst, the state convertibility for any coherent state is fully characterized by the free energy defined with the quantum relative entropy.","We can extend this result to general resource theories in the form that imposing the covariant condition to a general resource theory does not change the state convertibility as long as the initial state is coherent and distillable.","This means that the additional constraint from the law of energy conservation is irrelevant in the correlated-catalytic framework."],"url":"http://arxiv.org/abs/2406.06234v1","category":"quant-ph"}
{"created":"2024-06-10 13:00:22","title":"UEMM-Air: A Synthetic Multi-modal Dataset for Unmanned Aerial Vehicle Object Detection","abstract":"The development of multi-modal object detection for Unmanned Aerial Vehicles (UAVs) typically relies on a large amount of pixel-aligned multi-modal image data. However, existing datasets face challenges such as limited modalities, high construction costs, and imprecise annotations. To this end, we propose a synthetic multi-modal UAV-based object detection dataset, UEMM-Air. Specially, we simulate various UAV flight scenarios and object types using the Unreal Engine (UE). Then we design the UAV's flight logic to automatically collect data from different scenarios, perspectives, and altitudes. Finally, we propose a novel heuristic automatic annotation algorithm to generate accurate object detection labels. In total, our UEMM-Air consists of 20k pairs of images with 5 modalities and precise annotations. Moreover, we conduct numerous experiments and establish new benchmark results on our dataset. We found that models pre-trained on UEMM-Air exhibit better performance on downstream tasks compared to other similar datasets. The dataset is publicly available (https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal UAV object detection models.","sentences":["The development of multi-modal object detection for Unmanned Aerial Vehicles (UAVs) typically relies on a large amount of pixel-aligned multi-modal image data.","However, existing datasets face challenges such as limited modalities, high construction costs, and imprecise annotations.","To this end, we propose a synthetic multi-modal UAV-based object detection dataset, UEMM-Air.","Specially, we simulate various UAV flight scenarios and object types using the Unreal Engine (UE).","Then we design the UAV's flight logic to automatically collect data from different scenarios, perspectives, and altitudes.","Finally, we propose a novel heuristic automatic annotation algorithm to generate accurate object detection labels.","In total, our UEMM-Air consists of 20k pairs of images with 5 modalities and precise annotations.","Moreover, we conduct numerous experiments and establish new benchmark results on our dataset.","We found that models pre-trained on UEMM-Air exhibit better performance on downstream tasks compared to other similar datasets.","The dataset is publicly available (https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal UAV object detection models."],"url":"http://arxiv.org/abs/2406.06230v1","category":"cs.CV"}
{"created":"2024-06-10 12:59:13","title":"Global $H^2$-solutions for the generalized derivative NLS on $\\mathbb{T}$","abstract":"We prove global existence of $H^2$ solutions to the Cauchy problem for the generalized derivative nonlinear Schr\\\"{o}dinger equation on the 1-d torus. This answers an open problem posed by Ambrose and Simpson (2015). The key is the extraction of the terms that cause the problem in energy estimates and the construction of suitable energies so as to cancel the problematic terms out by effectively using integration by parts and the equation.","sentences":["We prove global existence of $H^2$ solutions to the Cauchy problem for the generalized derivative nonlinear Schr\\\"{o}dinger equation on the 1-d torus.","This answers an open problem posed by Ambrose and Simpson (2015).","The key is the extraction of the terms that cause the problem in energy estimates and the construction of suitable energies so as to cancel the problematic terms out by effectively using integration by parts and the equation."],"url":"http://arxiv.org/abs/2406.06229v1","category":"math.AP"}
{"created":"2024-06-10 12:53:13","title":"PAC-Bayes Analysis for Recalibration in Classification","abstract":"Nonparametric estimation with binning is widely employed in the calibration error evaluation and the recalibration of machine learning models. Recently, theoretical analyses of the bias induced by this estimation approach have been actively pursued; however, the understanding of the generalization of the calibration error to unknown data remains limited. In addition, although many recalibration algorithms have been proposed, their generalization performance lacks theoretical guarantees. To address this problem, we conduct a generalization analysis of the calibration error under the probably approximately correct (PAC) Bayes framework. This approach enables us to derive a first optimizable upper bound for the generalization error in the calibration context. We then propose a generalization-aware recalibration algorithm based on our generalization theory. Numerical experiments show that our algorithm improves the Gaussian-process-based recalibration performance on various benchmark datasets and models.","sentences":["Nonparametric estimation with binning is widely employed in the calibration error evaluation and the recalibration of machine learning models.","Recently, theoretical analyses of the bias induced by this estimation approach have been actively pursued; however, the understanding of the generalization of the calibration error to unknown data remains limited.","In addition, although many recalibration algorithms have been proposed, their generalization performance lacks theoretical guarantees.","To address this problem, we conduct a generalization analysis of the calibration error under the probably approximately correct (PAC) Bayes framework.","This approach enables us to derive a first optimizable upper bound for the generalization error in the calibration context.","We then propose a generalization-aware recalibration algorithm based on our generalization theory.","Numerical experiments show that our algorithm improves the Gaussian-process-based recalibration performance on various benchmark datasets and models."],"url":"http://arxiv.org/abs/2406.06227v1","category":"cs.LG"}
{"created":"2024-06-10 12:37:21","title":"Remote Implementation of Hidden or Partially Unknown Quantum Operators using Optimal Resources: A Generalized View","abstract":"Two protocols are proposed for two closely linked but different variants of remote implementation of quantum operators of specific forms. The first protocol is designed for the remote implementation of the single qubit hidden quantum operator, whereas the second one is designed for the remote implementation of the partially unknown single qubit quantum operator. In both cases two-qubit maximally entangled state, which is entangled in the spatial degree of freedom is used. The quantum resources used here are optimal and easy to realize and maintain in comparison to the multi-partite or multi-mode entangled states used in earlier works. The proposed protocols are also generalized to their controlled, bidirectional, cyclic, controlled cyclic, and controlled bidirectional versions and it is shown that either Bell state alone or products of Bell states will be sufficient to perform these tasks with some additional classical communications in the controlled cases only. This is in sharp contrast to the earlier proposals that require large entangled states. In addition, it's noted that remote implementation of hidden or partially unknown operators involving multiple controllers and/or multiple players who jointly apply the desired operator(s) would require quantum channels more complex than the Bell states and their products. Explicit forms of such quantum channels are also provided.","sentences":["Two protocols are proposed for two closely linked but different variants of remote implementation of quantum operators of specific forms.","The first protocol is designed for the remote implementation of the single qubit hidden quantum operator, whereas the second one is designed for the remote implementation of the partially unknown single qubit quantum operator.","In both cases two-qubit maximally entangled state, which is entangled in the spatial degree of freedom is used.","The quantum resources used here are optimal and easy to realize and maintain in comparison to the multi-partite or multi-mode entangled states used in earlier works.","The proposed protocols are also generalized to their controlled, bidirectional, cyclic, controlled cyclic, and controlled bidirectional versions and it is shown that either Bell state alone or products of Bell states will be sufficient to perform these tasks with some additional classical communications in the controlled cases only.","This is in sharp contrast to the earlier proposals that require large entangled states.","In addition, it's noted that remote implementation of hidden or partially unknown operators involving multiple controllers and/or multiple players who jointly apply the desired operator(s) would require quantum channels more complex than the Bell states and their products.","Explicit forms of such quantum channels are also provided."],"url":"http://arxiv.org/abs/2406.06223v1","category":"quant-ph"}
{"created":"2024-06-10 12:35:10","title":"Shear thickening in suspensions of particles with dynamic brush layers","abstract":"Control of frictional interactions among liquid-suspended particles has led to tunable, strikingly non-Newtonian rheology via the formation of strong flow constraints as particles come into close proximity under shear. Typically, these frictional interactions have been in the form of physical contact, controllable via particle shape and surface roughness. We investigate a different route, where molecular bridging between nearby particle surfaces generates a controllable \"sticky\" friction. This is achieved with surface-functionalized colloidal particles capable of forming dynamic covalent bonds with telechelic polymers that comprise the suspending fluid. At low shear stress this results in particles coated with a uniform polymer brush layer. Beyond an onset stress the telechelic polymers become capable of bridging and generate shear thickening. Over the size range investigated, we find that the dynamic brush layer leads to dependence of the onset stress on particle diameter that closely follows a power law with exponent -1.76. In the shear thickening regime, we observe an enhanced dilation in measurements of the first normal stress difference and reduction in the extrapolated volume fraction required for jamming, both consistent with an effective particle friction that increases with decreasing particle diameter. These results are discussed in light of predictions for suspensions of hard spheres and of polymer-grafted particles.","sentences":["Control of frictional interactions among liquid-suspended particles has led to tunable, strikingly non-Newtonian rheology via the formation of strong flow constraints as particles come into close proximity under shear.","Typically, these frictional interactions have been in the form of physical contact, controllable via particle shape and surface roughness.","We investigate a different route, where molecular bridging between nearby particle surfaces generates a controllable \"sticky\" friction.","This is achieved with surface-functionalized colloidal particles capable of forming dynamic covalent bonds with telechelic polymers that comprise the suspending fluid.","At low shear stress this results in particles coated with a uniform polymer brush layer.","Beyond an onset stress the telechelic polymers become capable of bridging and generate shear thickening.","Over the size range investigated, we find that the dynamic brush layer leads to dependence of the onset stress on particle diameter that closely follows a power law with exponent -1.76.","In the shear thickening regime, we observe an enhanced dilation in measurements of the first normal stress difference and reduction in the extrapolated volume fraction required for jamming, both consistent with an effective particle friction that increases with decreasing particle diameter.","These results are discussed in light of predictions for suspensions of hard spheres and of polymer-grafted particles."],"url":"http://arxiv.org/abs/2406.06222v1","category":"cond-mat.soft"}
{"created":"2024-06-10 12:34:38","title":"Label-Looping: Highly Efficient Decoding for Transducers","abstract":"This paper introduces a highly efficient greedy decoding algorithm for Transducer inference. We propose a novel data structure using CUDA tensors to represent partial hypotheses in a batch that supports parallelized hypothesis manipulations. During decoding, our algorithm maximizes GPU parallelism by adopting a nested-loop design, where the inner loop consumes all blank predictions, while non-blank predictions are handled in the outer loop. Our algorithm is general-purpose and can work with both conventional Transducers and Token-and-Duration Transducers. Experiments show that the label-looping algorithm can bring a speedup up to 2.0X compared to conventional batched decoding algorithms when using batch size 32, and can be combined with other compiler or GPU call-related techniques to bring more speedup. We will open-source our implementation to benefit the research community.","sentences":["This paper introduces a highly efficient greedy decoding algorithm for Transducer inference.","We propose a novel data structure using CUDA tensors to represent partial hypotheses in a batch that supports parallelized hypothesis manipulations.","During decoding, our algorithm maximizes GPU parallelism by adopting a nested-loop design, where the inner loop consumes all blank predictions, while non-blank predictions are handled in the outer loop.","Our algorithm is general-purpose and can work with both conventional Transducers and Token-and-Duration Transducers.","Experiments show that the label-looping algorithm can bring a speedup up to 2.0X compared to conventional batched decoding algorithms when using batch size 32, and can be combined with other compiler or GPU call-related techniques to bring more speedup.","We will open-source our implementation to benefit the research community."],"url":"http://arxiv.org/abs/2406.06220v1","category":"eess.AS"}
{"created":"2024-06-10 12:33:47","title":"Data Augmentation in Earth Observation: A Diffusion Model Approach","abstract":"The scarcity of high-quality Earth Observation (EO) imagery poses a significant challenge, despite its critical role in enabling precise analysis and informed decision-making across various sectors. This scarcity is primarily due to atmospheric conditions, seasonal variations, and limited geographical coverage, which complicates the application of Artificial Intelligence (AI) in EO. Data augmentation, a widely used technique in AI that involves generating additional data mainly through parameterized image transformations, has been employed to increase the volume and diversity of data. However, this method often falls short in generating sufficient diversity across key semantic axes, adversely affecting the accuracy of EO applications. To address this issue, we propose a novel four-stage approach aimed at improving the diversity of augmented data by integrating diffusion models. Our approach employs meta-prompts for instruction generation, harnesses general-purpose vision-language models for generating rich captions, fine-tunes an Earth Observation diffusion model, and iteratively augments data. We conducted extensive experiments using four different data augmentation techniques, and our approach consistently demonstrated improvements, outperforming the established augmentation methods, revealing its effectiveness in generating semantically rich and diverse EO images.","sentences":["The scarcity of high-quality Earth Observation (EO) imagery poses a significant challenge, despite its critical role in enabling precise analysis and informed decision-making across various sectors.","This scarcity is primarily due to atmospheric conditions, seasonal variations, and limited geographical coverage, which complicates the application of Artificial Intelligence (AI) in EO.","Data augmentation, a widely used technique in AI that involves generating additional data mainly through parameterized image transformations, has been employed to increase the volume and diversity of data.","However, this method often falls short in generating sufficient diversity across key semantic axes, adversely affecting the accuracy of EO applications.","To address this issue, we propose a novel four-stage approach aimed at improving the diversity of augmented data by integrating diffusion models.","Our approach employs meta-prompts for instruction generation, harnesses general-purpose vision-language models for generating rich captions, fine-tunes an Earth Observation diffusion model, and iteratively augments data.","We conducted extensive experiments using four different data augmentation techniques, and our approach consistently demonstrated improvements, outperforming the established augmentation methods, revealing its effectiveness in generating semantically rich and diverse EO images."],"url":"http://arxiv.org/abs/2406.06218v1","category":"cs.CV"}
{"created":"2024-06-10 12:30:08","title":"ZTF SN Ia DR2: Evidence of Changing Dust Distributions With Redshift Using Type Ia Supernovae","abstract":"Type Ia supernova (SNIa) are excellent probes of local distance, and the increasing sample sizes of SNIa have driven an increased need to study the associated systematic uncertainties and improve the standardisation methods in preparation for the next generation of cosmological surveys into the dark energy equation-of-state $w$. We aim to probe the potential change in the SNIa standardisation parameter $c$ with redshift and the host-galaxy of the supernova. Improving the standardisation of SNIa brightnesses will require accounting for the relationship between the host and the SNIa, and potential shifts in the SNIa standardisation parameters with redshift will cause biases in the recovered cosmology. Here, we assemble a volume-limited sample of ~3000 likely SNIa across a redshift range of $z = 0.015$ to $z = 0.36$. This sample is fitted with changing mass and redshift bins to determine the relationship between intrinsic properties of SNe Ia and their redshift and host galaxy parameters. We then investigate the colour-luminosity parameter $\\beta$ as a further test of the SNIa standardisation process. We find that the changing colour distribution of SNe Ia with redshift is driven by dust at a confidence of $>4\\sigma$. Additionally, we show a strong correlation between the host galaxy mass and the colour-luminosity coefficient $\\beta$ ($> 4\\sigma$), even when accounting for the quantity of dust in a host galaxy.","sentences":["Type Ia supernova (SNIa) are excellent probes of local distance, and the increasing sample sizes of SNIa have driven an increased need to study the associated systematic uncertainties and improve the standardisation methods in preparation for the next generation of cosmological surveys into the dark energy equation-of-state $w$. We aim to probe the potential change in the SNIa standardisation parameter $c$ with redshift and the host-galaxy of the supernova.","Improving the standardisation of SNIa brightnesses will require accounting for the relationship between the host and the SNIa, and potential shifts in the SNIa standardisation parameters with redshift will cause biases in the recovered cosmology.","Here, we assemble a volume-limited sample of ~3000 likely SNIa across a redshift range of $z = 0.015$ to $z = 0.36$.","This sample is fitted with changing mass and redshift bins to determine the relationship between intrinsic properties of SNe Ia and their redshift and host galaxy parameters.","We then investigate the colour-luminosity parameter $\\beta$ as a further test of the SNIa standardisation process.","We find that the changing colour distribution of SNe Ia with redshift is driven by dust at a confidence of $>4\\sigma$. Additionally, we show a strong correlation between the host galaxy mass and the colour-luminosity coefficient $\\beta$ ($> 4\\sigma$), even when accounting for the quantity of dust in a host galaxy."],"url":"http://arxiv.org/abs/2406.06215v1","category":"astro-ph.CO"}
{"created":"2024-06-10 12:25:13","title":"A Statistical Theory of Regularization-Based Continual Learning","abstract":"We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance. We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously. Next, we consider a family of generalized $\\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases. As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm. Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity. Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator. In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality. A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\\ell_2$-regularization in continual learning, which may be of independent interest. Finally, we conduct experiments to complement our theory.","sentences":["We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance.","We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously.","Next, we consider a family of generalized $\\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases.","As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm.","Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity.","Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator.","In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality.","A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\\ell_2$-regularization in continual learning, which may be of independent interest.","Finally, we conduct experiments to complement our theory."],"url":"http://arxiv.org/abs/2406.06213v1","category":"cs.LG"}
{"created":"2024-06-10 12:22:06","title":"iMotion-LLM: Motion Prediction Instruction Tuning","abstract":"We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with trajectory prediction, tailored to guide interactive multi-agent scenarios. Different from conventional motion prediction approaches, iMotion-LLM capitalizes on textual instructions as key inputs for generating contextually relevant trajectories.By enriching the real-world driving scenarios in the Waymo Open Dataset with textual motion instructions, we created InstructWaymo. Leveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to translate scene features into the LLM input space. iMotion-LLM offers significant advantages over conventional motion prediction models. First, it can generate trajectories that align with the provided instructions if it is a feasible direction. Second, when given an infeasible direction, it can reject the instruction, thereby enhancing safety. These findings act as milestones in empowering autonomous navigation systems to interpret and predict the dynamics of multi-agent environments, laying the groundwork for future advancements in this field.","sentences":["We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with trajectory prediction, tailored to guide interactive multi-agent scenarios.","Different from conventional motion prediction approaches, iMotion-LLM capitalizes on textual instructions as key inputs for generating contextually relevant trajectories.","By enriching the real-world driving scenarios in the Waymo Open Dataset with textual motion instructions, we created InstructWaymo.","Leveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to translate scene features into the LLM input space.","iMotion-LLM offers significant advantages over conventional motion prediction models.","First, it can generate trajectories that align with the provided instructions if it is a feasible direction.","Second, when given an infeasible direction, it can reject the instruction, thereby enhancing safety.","These findings act as milestones in empowering autonomous navigation systems to interpret and predict the dynamics of multi-agent environments, laying the groundwork for future advancements in this field."],"url":"http://arxiv.org/abs/2406.06211v1","category":"cs.CV"}
{"created":"2024-06-10 12:17:46","title":"Quantum Architecture Search: A Survey","abstract":"Quantum computing has made significant progress in recent years, attracting immense interest not only in research laboratories but also in various industries. However, the application of quantum computing to solve real-world problems is still hampered by a number of challenges, including hardware limitations and a relatively under-explored landscape of quantum algorithms, especially when compared to the extensive development of classical computing. The design of quantum circuits, in particular parameterized quantum circuits (PQCs), which contain learnable parameters optimized by classical methods, is a non-trivial and time-consuming task requiring expert knowledge. As a result, research on the automated generation of PQCs, known as quantum architecture search (QAS), has gained considerable interest. QAS focuses on the use of machine learning and optimization-driven techniques to generate PQCs tailored to specific problems and characteristics of quantum hardware. In this paper, we provide an overview of QAS methods by examining relevant research studies in the field. We discuss main challenges in designing and performing an automated search for an optimal PQC, and survey ways to address them to ease future research.","sentences":["Quantum computing has made significant progress in recent years, attracting immense interest not only in research laboratories but also in various industries.","However, the application of quantum computing to solve real-world problems is still hampered by a number of challenges, including hardware limitations and a relatively under-explored landscape of quantum algorithms, especially when compared to the extensive development of classical computing.","The design of quantum circuits, in particular parameterized quantum circuits (PQCs), which contain learnable parameters optimized by classical methods, is a non-trivial and time-consuming task requiring expert knowledge.","As a result, research on the automated generation of PQCs, known as quantum architecture search (QAS), has gained considerable interest.","QAS focuses on the use of machine learning and optimization-driven techniques to generate PQCs tailored to specific problems and characteristics of quantum hardware.","In this paper, we provide an overview of QAS methods by examining relevant research studies in the field.","We discuss main challenges in designing and performing an automated search for an optimal PQC, and survey ways to address them to ease future research."],"url":"http://arxiv.org/abs/2406.06210v1","category":"quant-ph"}
{"created":"2024-06-10 12:14:05","title":"Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning","abstract":"Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models. \\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.","sentences":["Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data.","However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data.","To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data.","While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications.","In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks.","Our analysis showcases a tale of two cities.","On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models.","Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks.","On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security.","We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems.","\\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process.","Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models.","\\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms.","Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges."],"url":"http://arxiv.org/abs/2406.06207v1","category":"cs.LG"}
{"created":"2024-06-10 11:53:29","title":"2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension Method for Multimodal Moment Retrieval","abstract":"Moment retrieval aims to locate the most relevant moment in an untrimmed video based on a given natural language query. Existing solutions can be roughly categorized into moment-based and clip-based methods. The former often involves heavy computations, while the latter, due to overlooking coarse-grained information, typically underperforms compared to moment-based models. Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine Reading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address the issue of imprecise localization in clip-based methods while maintaining lower computational complexity than moment-based methods. Specifically, we introduce an AV-Encoder to capture coarse-grained information at moment and video levels. Additionally, a 2D pointer encoder module is introduced to further enhance boundary detection for target moment. Extensive experiments on the HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing baseline models.","sentences":["Moment retrieval aims to locate the most relevant moment in an untrimmed video based on a given natural language query.","Existing solutions can be roughly categorized into moment-based and clip-based methods.","The former often involves heavy computations, while the latter, due to overlooking coarse-grained information, typically underperforms compared to moment-based models.","Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine Reading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address the issue of imprecise localization in clip-based methods while maintaining lower computational complexity than moment-based methods.","Specifically, we introduce an AV-Encoder to capture coarse-grained information at moment and video levels.","Additionally, a 2D pointer encoder module is introduced to further enhance boundary detection for target moment.","Extensive experiments on the HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing baseline models."],"url":"http://arxiv.org/abs/2406.06201v1","category":"cs.CV"}
{"created":"2024-06-10 11:52:25","title":"Implications for Governance in Public Perceptions of Societal-scale AI Risks","abstract":"Amid growing concerns over AI's societal risks--ranging from civilizational collapse to misinformation and systemic bias--this study explores the perceptions of AI experts and the general US registered voters on the likelihood and impact of 18 specific AI risks, alongside their policy preferences for managing these risks. While both groups favor international oversight over national or corporate governance, our survey reveals a discrepancy: voters perceive AI risks as both more likely and more impactful than experts, and also advocate for slower AI development. Specifically, our findings indicate that policy interventions may best assuage collective concerns if they attempt to more carefully balance mitigation efforts across all classes of societal-scale risks, effectively nullifying the near-vs-long-term debate over AI risks. More broadly, our results will serve not only to enable more substantive policy discussions for preventing and mitigating AI risks, but also to underscore the challenge of consensus building for effective policy implementation.","sentences":["Amid growing concerns over AI's societal risks--ranging from civilizational collapse to misinformation and systemic bias--this study explores the perceptions of AI experts and the general US registered voters on the likelihood and impact of 18 specific AI risks, alongside their policy preferences for managing these risks.","While both groups favor international oversight over national or corporate governance, our survey reveals a discrepancy: voters perceive AI risks as both more likely and more impactful than experts, and also advocate for slower AI development.","Specifically, our findings indicate that policy interventions may best assuage collective concerns if they attempt to more carefully balance mitigation efforts across all classes of societal-scale risks, effectively nullifying the near-vs-long-term debate over AI risks.","More broadly, our results will serve not only to enable more substantive policy discussions for preventing and mitigating AI risks, but also to underscore the challenge of consensus building for effective policy implementation."],"url":"http://arxiv.org/abs/2406.06199v1","category":"cs.CY"}
{"created":"2024-06-10 11:50:38","title":"Learning effective Hamiltonians for adaptive time-evolution quantum algorithms","abstract":"Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization. Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions. However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled. Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration. Our key observation is that deviations from the target generator remain bounded on all simulation times. This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system. Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms.","sentences":["Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization.","Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions.","However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled.","Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration.","Our key observation is that deviations from the target generator remain bounded on all simulation times.","This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system.","Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms."],"url":"http://arxiv.org/abs/2406.06198v1","category":"quant-ph"}
{"created":"2024-06-10 11:50:29","title":"LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages","abstract":"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 35.3% accuracy, 21.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.","sentences":["In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models.","Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions.","The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty.","We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation.","Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems.","On harder problems, even the top model only achieved 35.3% accuracy, 21.7% improvement over the no-context baseline.","Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores.","These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models."],"url":"http://arxiv.org/abs/2406.06196v1","category":"cs.CL"}
{"created":"2024-06-10 11:44:54","title":"R\u00e9nyi entanglement entropy of spin chain with Generative Neural Networks","abstract":"We describe a method to estimate R\\'enyi entanglement entropy of a spin system, which is based on the replica trick and generative neural networks with explicit probability estimation. It can be extended to any spin system or lattice field theory. We demonstrate our method on a one-dimensional quantum Ising spin chain. As the generative model, we use a hierarchy of autoregressive networks, allowing us to simulate up to 32 spins. We calculate the second R\\'enyi entropy and its derivative and cross-check our results with the numerical evaluation of entropy and results available in the literature.","sentences":["We describe a method to estimate R\\'enyi entanglement entropy of a spin system, which is based on the replica trick and generative neural networks with explicit probability estimation.","It can be extended to any spin system or lattice field theory.","We demonstrate our method on a one-dimensional quantum Ising spin chain.","As the generative model, we use a hierarchy of autoregressive networks, allowing us to simulate up to 32 spins.","We calculate the second R\\'enyi entropy and its derivative and cross-check our results with the numerical evaluation of entropy and results available in the literature."],"url":"http://arxiv.org/abs/2406.06193v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-10 11:44:15","title":"AI Cat Narrator: Designing an AI Tool for Exploring the Shared World and Social Connection with a Cat","abstract":"As technology continues to advance, the interaction between humans and cats is becoming more diverse. Our research introduces a new tool called the AI Cat Narrator, which offers a unique perspective on the shared lives of humans and cats. We combined the method of ethnography with fictional storytelling, using a defamiliarization strategy to merge real-world data seen through the eyes of cats with excerpts from cat literature. This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative. Our findings indicate that using defamiliarized data for training purposes significantly contributes to the development of characters that are both more empathetic and individualized. The contributions of our study are twofold: 1) proposing an innovative approach to prompting a reevaluation of living alongside cats; 2) establishing a collaborative, exploratory tool developed by humans, cats, and AI together.","sentences":["As technology continues to advance, the interaction between humans and cats is becoming more diverse.","Our research introduces a new tool called the AI Cat Narrator, which offers a unique perspective on the shared lives of humans and cats.","We combined the method of ethnography with fictional storytelling, using a defamiliarization strategy to merge real-world data seen through the eyes of cats with excerpts from cat literature.","This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative.","Our findings indicate that using defamiliarized data for training purposes significantly contributes to the development of characters that are both more empathetic and individualized.","The contributions of our study are twofold: 1) proposing an innovative approach to prompting a reevaluation of living alongside cats; 2) establishing a collaborative, exploratory tool developed by humans, cats, and AI together."],"url":"http://arxiv.org/abs/2406.06192v1","category":"cs.HC"}
{"created":"2024-06-10 11:31:04","title":"A Survey on Machine Unlearning: Techniques and New Emerged Privacy Risks","abstract":"The explosive growth of machine learning has made it a critical infrastructure in the era of artificial intelligence. The extensive use of data poses a significant threat to individual privacy. Various countries have implemented corresponding laws, such as GDPR, to protect individuals' data privacy and the right to be forgotten. This has made machine unlearning a research hotspot in the field of privacy protection in recent years, with the aim of efficiently removing the contribution and impact of individual data from trained models. The research in academia on machine unlearning has continuously enriched its theoretical foundation, and many methods have been proposed, targeting different data removal requests in various application scenarios. However, recently researchers have found potential privacy leakages of various of machine unlearning approaches, making the privacy preservation on machine unlearning area a critical topic. This paper provides an overview and analysis of the existing research on machine unlearning, aiming to present the current vulnerabilities of machine unlearning approaches. We analyze privacy risks in various aspects, including definitions, implementation methods, and real-world applications. Compared to existing reviews, we analyze the new challenges posed by the latest malicious attack techniques on machine unlearning from the perspective of privacy threats. We hope that this survey can provide an initial but comprehensive discussion on this new emerging area.","sentences":["The explosive growth of machine learning has made it a critical infrastructure in the era of artificial intelligence.","The extensive use of data poses a significant threat to individual privacy.","Various countries have implemented corresponding laws, such as GDPR, to protect individuals' data privacy and the right to be forgotten.","This has made machine unlearning a research hotspot in the field of privacy protection in recent years, with the aim of efficiently removing the contribution and impact of individual data from trained models.","The research in academia on machine unlearning has continuously enriched its theoretical foundation, and many methods have been proposed, targeting different data removal requests in various application scenarios.","However, recently researchers have found potential privacy leakages of various of machine unlearning approaches, making the privacy preservation on machine unlearning area a critical topic.","This paper provides an overview and analysis of the existing research on machine unlearning, aiming to present the current vulnerabilities of machine unlearning approaches.","We analyze privacy risks in various aspects, including definitions, implementation methods, and real-world applications.","Compared to existing reviews, we analyze the new challenges posed by the latest malicious attack techniques on machine unlearning from the perspective of privacy threats.","We hope that this survey can provide an initial but comprehensive discussion on this new emerging area."],"url":"http://arxiv.org/abs/2406.06186v1","category":"cs.CR"}
{"created":"2024-06-10 11:28:29","title":"EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation","abstract":"We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online.","sentences":["We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data.","The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech.","We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics.","In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred.","We introduce a blind test set that allows for automatic online evaluation of uploaded data.","Dataset download links and automatic evaluation server can be found online."],"url":"http://arxiv.org/abs/2406.06185v1","category":"eess.AS"}
{"created":"2024-06-10 11:28:25","title":"Deep Multi-Objective Reinforcement Learning for Utility-Based Infrastructural Maintenance Optimization","abstract":"In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent Actor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL) method for infrastructural maintenance optimization, an area traditionally dominated by single-objective reinforcement learning (RL) approaches. Previous single-objective RL methods combine multiple objectives, such as probability of collapse and cost, into a singular reward signal through reward-shaping. In contrast, MO-DCMAC can optimize a policy for multiple objectives directly, even when the utility function is non-linear. We evaluated MO-DCMAC using two utility functions, which use probability of collapse and cost as input. The first utility function is the Threshold utility, in which MO-DCMAC should minimize cost so that the probability of collapse is never above the threshold. The second is based on the Failure Mode, Effects, and Criticality Analysis (FMECA) methodology used by asset managers to asses maintenance plans. We evaluated MO-DCMAC, with both utility functions, in multiple maintenance environments, including ones based on a case study of the historical quay walls of Amsterdam. The performance of MO-DCMAC was compared against multiple rule-based policies based on heuristics currently used for constructing maintenance plans. Our results demonstrate that MO-DCMAC outperforms traditional rule-based policies across various environments and utility functions.","sentences":["In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent Actor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL) method for infrastructural maintenance optimization, an area traditionally dominated by single-objective reinforcement learning (RL) approaches.","Previous single-objective RL methods combine multiple objectives, such as probability of collapse and cost, into a singular reward signal through reward-shaping.","In contrast, MO-DCMAC can optimize a policy for multiple objectives directly, even when the utility function is non-linear.","We evaluated MO-DCMAC using two utility functions, which use probability of collapse and cost as input.","The first utility function is the Threshold utility, in which MO-DCMAC should minimize cost so that the probability of collapse is never above the threshold.","The second is based on the Failure Mode, Effects, and Criticality Analysis (FMECA) methodology used by asset managers to asses maintenance plans.","We evaluated MO-DCMAC, with both utility functions, in multiple maintenance environments, including ones based on a case study of the historical quay walls of Amsterdam.","The performance of MO-DCMAC was compared against multiple rule-based policies based on heuristics currently used for constructing maintenance plans.","Our results demonstrate that MO-DCMAC outperforms traditional rule-based policies across various environments and utility functions."],"url":"http://arxiv.org/abs/2406.06184v1","category":"cs.AI"}
{"created":"2024-06-10 11:26:01","title":"Cyclicity of the shift operator through Bezout identities","abstract":"In this paper, we study the cyclicity of the shift operator $S$ acting on a Banach space $\\X$ of analytic functions on the open unit disc $\\D$. We develop a general framework where a method based on a corona theorem can be used to show that if $f,g\\in\\X$ satisfy $|g(z)|\\leq |f(z)|$, for every $z\\in\\D$, and if $g$ is cyclic, then $f$ is cyclic. We also give sufficient conditions for cyclicity in this context. This enable us to recapture some recent results obtained in de Branges-Rovnayk spaces, in Besov--Dirichlet spaces and in weighted Dirichlet type spaces.","sentences":["In this paper, we study the cyclicity of the shift operator $S$ acting on a Banach space $\\X$ of analytic functions on the open unit disc $\\D$. We develop a general framework where a method based on a corona theorem can be used to show that if $f,g\\in\\X$ satisfy $|g(z)|\\leq |f(z)|$, for every $z\\in\\D$, and if $g$ is cyclic, then $f$ is cyclic.","We also give sufficient conditions for cyclicity in this context.","This enable us to recapture some recent results obtained in de Branges-Rovnayk spaces, in Besov--Dirichlet spaces and in weighted Dirichlet type spaces."],"url":"http://arxiv.org/abs/2406.06182v1","category":"math.CV"}
{"created":"2024-06-10 11:22:04","title":"Operator-Valued Twisted Araki-Woods Algebras","abstract":"We introduce operator-valued twisted Araki-Woods algebras. These are operator-valued versions of a class of second quantization algebras that includes $q$-Gaussian and $q$-Araki-Woods algebras and also generalize Shlyakhtenko's von Neumann algebras generated by operator-valued semicircular variables. We develop a disintegration theory that reduces the isomorphism type of operator-valued twisted Araki-Woods algebras over type I factors to the scalar-valued case. Moreover, these algebras come with a natural weight, and we characterize its modular theory. We also give sufficient criteria that guarantee that factoriality of these algebras.","sentences":["We introduce operator-valued twisted Araki-Woods algebras.","These are operator-valued versions of a class of second quantization algebras that includes $q$-Gaussian and $q$-Araki-Woods algebras and also generalize Shlyakhtenko's von Neumann algebras generated by operator-valued semicircular variables.","We develop a disintegration theory that reduces the isomorphism type of operator-valued twisted Araki-Woods algebras over type I factors to the scalar-valued case.","Moreover, these algebras come with a natural weight, and we characterize its modular theory.","We also give sufficient criteria that guarantee that factoriality of these algebras."],"url":"http://arxiv.org/abs/2406.06179v1","category":"math.OA"}
{"created":"2024-06-10 11:18:25","title":"Ratchet current and scaling properties in a nontwist mapping","abstract":"We investigate the transport of particles in the chaotic component of phase space for a two-dimensional, area-preserving nontwist map. The survival probability for particles within the chaotic sea is described by an exponential decay for regions in phase space predominantly chaotic and it is scaling invariant in this case. Alternatively, when considering mixed chaotic and regular regions, there is a deviation from the exponential decay, characterized by a power law tail for long times, a signature of the stickiness effect. Furthermore, due to the asymmetry of the chaotic component of phase space with respect to the line $I = 0$, there is an unbalanced stickiness which generates a ratchet current in phase space. Finally, we perform a phenomenological description of the diffusion of chaotic particles by identifying three scaling hypotheses, and obtaining the critical exponents via extensive numerical simulations.","sentences":["We investigate the transport of particles in the chaotic component of phase space for a two-dimensional, area-preserving nontwist map.","The survival probability for particles within the chaotic sea is described by an exponential decay for regions in phase space predominantly chaotic and it is scaling invariant in this case.","Alternatively, when considering mixed chaotic and regular regions, there is a deviation from the exponential decay, characterized by a power law tail for long times, a signature of the stickiness effect.","Furthermore, due to the asymmetry of the chaotic component of phase space with respect to the line $I = 0$, there is an unbalanced stickiness which generates a ratchet current in phase space.","Finally, we perform a phenomenological description of the diffusion of chaotic particles by identifying three scaling hypotheses, and obtaining the critical exponents via extensive numerical simulations."],"url":"http://arxiv.org/abs/2406.06175v1","category":"nlin.CD"}
{"created":"2024-06-10 11:14:50","title":"Apparent non-variable stars from the Kepler mission","abstract":"The analysis of non-variable stars is generally neglected in the literature. However, such objects are needed for many calibration processes and for testing pulsational models. The photometric time series of the Kepler satellite mission still stand as the most accurate data available today and are excellently suited to the search for non-variable stars. We analysed all long-cadence light curves for stars not reported as a variable so far from the Kepler satellite mission. Using the known characteristics and flaws of these data sets, we defined three different frequency ranges where we searched for non-variability. We used the Lomb-Scargle periodogram and the false-alarm probability (FAP) to analyse the cleaned data sets of 138 451 light curves. We then used log FAP > -2 to define a star as \"non-variable\" in the ranges below 0.1 c/d, 0.1 to 2.0 c/d, and 2.0 to 25.0 c/d, respectively. Furthermore, we also calculated the standard deviation of the mean light curve to obtain another parameter. In total, we found 14 154 stars that fulfil the set","sentences":["The analysis of non-variable stars is generally neglected in the literature.","However, such objects are needed for many calibration processes and for testing pulsational models.","The photometric time series of the Kepler satellite mission still stand as the most accurate data available today and are excellently suited to the search for non-variable stars.","We analysed all long-cadence light curves for stars not reported as a variable so far from the Kepler satellite mission.","Using the known characteristics and flaws of these data sets, we defined three different frequency ranges where we searched for non-variability.","We used the Lomb-Scargle periodogram and the false-alarm probability (FAP) to analyse the cleaned data sets of 138 451 light curves.","We then used log FAP >","-2 to define a star as \"non-variable\" in the ranges below 0.1 c/d, 0.1 to 2.0 c/d, and 2.0 to 25.0 c/d, respectively.","Furthermore, we also calculated the standard deviation of the mean light curve to obtain another parameter.","In total, we found 14 154 stars that fulfil the set"],"url":"http://arxiv.org/abs/2406.06174v1","category":"astro-ph.SR"}
{"created":"2024-06-10 11:01:09","title":"Exploring the rich geometrical information in cosmic drift signals with covariant cosmography","abstract":"Real-time measurements are becoming feasible in cosmology, where the next generation of telescopes will detect the temporal change of redshifts and sky positions of individual sources with a precision that will allow a direct detection of the cosmic expansion rate. These detections of cosmic drifts of redshifts and positions are likely to become cornerstones in modern cosmology, where one has otherwise relied on the indirect inference of cosmic expansion by estimation of the slope of the fitted distance-redshift relation. Because of their ability to directly detect the cosmic time-evolution, real-time measurements are powerful as model-independent probes. We develop a cosmographic framework for analysing cosmological redshift drift and position drift signals without knowledge of the space-time geometry. The framework can be applied to analyse data from surveys such as the Gaia observatory, the Square Kilometer Array (SKA), and the Extremely Large Telescope (ELT). The drift effects are distorted by the regional kinematics and tidal effects in the cosmic neighbourhood of the observer, giving rise to non-trivial corrections to the well known Friedmann-Lema\\^{\\i}tre-Robertson-Walker (FLRW) results. We discuss how one may concretely implement the framework in the statistical analysis of real-time data, along with assumptions and limitations that come with such an analysis. We also discuss the geometrical information that can ideally be extracted from ideal high-resolution data of cosmic drifts in combination with distance-redshift data.","sentences":["Real-time measurements are becoming feasible in cosmology, where the next generation of telescopes will detect the temporal change of redshifts and sky positions of individual sources with a precision that will allow a direct detection of the cosmic expansion rate.","These detections of cosmic drifts of redshifts and positions are likely to become cornerstones in modern cosmology, where one has otherwise relied on the indirect inference of cosmic expansion by estimation of the slope of the fitted distance-redshift relation.","Because of their ability to directly detect the cosmic time-evolution, real-time measurements are powerful as model-independent probes.","We develop a cosmographic framework for analysing cosmological redshift drift and position drift signals without knowledge of the space-time geometry.","The framework can be applied to analyse data from surveys such as the Gaia observatory, the Square Kilometer Array (SKA), and the Extremely Large Telescope (ELT).","The drift effects are distorted by the regional kinematics and tidal effects in the cosmic neighbourhood of the observer, giving rise to non-trivial corrections to the well known Friedmann-Lema\\^{\\i}tre-Robertson-Walker (FLRW) results.","We discuss how one may concretely implement the framework in the statistical analysis of real-time data, along with assumptions and limitations that come with such an analysis.","We also discuss the geometrical information that can ideally be extracted from ideal high-resolution data of cosmic drifts in combination with distance-redshift data."],"url":"http://arxiv.org/abs/2406.06167v1","category":"astro-ph.CO"}
{"created":"2024-06-10 11:00:42","title":"Faraday moments of the Southern Twenty-centimeter All-sky Polarization Survey (STAPS)","abstract":"Faraday tomography of broadband radio polarization surveys enables us to study magnetic fields and their interaction with the interstellar medium (ISM). Such surveys include the Global Magneto-Ionic Medium Survey (GMIMS), which covers the northern and southern hemispheres at $\\sim$ 300-1800 MHz.   In this work, we used the GMIMS High Band South (1328-1768 MHz), also named the Southern Twenty-centimeter All-sky Polarization Survey (STAPS), which observes the southern sky at a resolution of 18$\\arcmin$.   To extract the key parameters of the magnetized ISM from STAPS, we computed the Faraday moments of the tomographic data cubes. These moments include the total polarized intensity, the mean Faraday depth weighted by the polarized intensity, the weighted dispersion of the Faraday spectrum, and its skewness. We compared the Faraday moments to those calculated over the same frequency range in the northern sky (using the Dominion Radio Astrophysical Observatory, DRAO), in a strip of $360\\degr \\times 30\\degr$ that overlaps with STAPS coverage.   We find that the total polarized intensity is generally dominated by diffuse emission that decreases at longitudes of $l \\leq 300\\degr$. The Faraday moments reveal a variety of polarization structures. Low-intensity regions at high latitudes usually have a single Faraday depth component. Due to its insufficiently large frequency coverage, STAPS cannot detect Faraday thick structures. Comparing the Faraday depths from STAPS to total rotation measures from extragalactic sources suggests that STAPS frequencies are high enough that the intervening ISM causes depolarization to background emission at intermediate and high Galactic latitudes. Where they overlap, the STAPS and DRAO surveys exhibit broad correspondence but differ in polarized intensity by a factor of $\\sim$1.8.","sentences":["Faraday tomography of broadband radio polarization surveys enables us to study magnetic fields and their interaction with the interstellar medium (ISM).","Such surveys include the Global Magneto-Ionic Medium Survey (GMIMS), which covers the northern and southern hemispheres at $\\sim$ 300-1800 MHz.   ","In this work, we used the GMIMS High Band South (1328-1768 MHz), also named the Southern Twenty-centimeter All-sky Polarization Survey (STAPS), which observes the southern sky at a resolution of 18$\\arcmin$.   To extract the key parameters of the magnetized ISM from STAPS, we computed the Faraday moments of the tomographic data cubes.","These moments include the total polarized intensity, the mean Faraday depth weighted by the polarized intensity, the weighted dispersion of the Faraday spectrum, and its skewness.","We compared the Faraday moments to those calculated over the same frequency range in the northern sky (using the Dominion Radio Astrophysical Observatory, DRAO), in a strip of $360\\degr \\times 30\\degr$ that overlaps with STAPS coverage.   ","We find that the total polarized intensity is generally dominated by diffuse emission that decreases at longitudes of $l \\leq 300\\degr$.","The Faraday moments reveal a variety of polarization structures.","Low-intensity regions at high latitudes usually have a single Faraday depth component.","Due to its insufficiently large frequency coverage, STAPS cannot detect Faraday thick structures.","Comparing the Faraday depths from STAPS to total rotation measures from extragalactic sources suggests that STAPS frequencies are high enough that the intervening ISM causes depolarization to background emission at intermediate and high Galactic latitudes.","Where they overlap, the STAPS and DRAO surveys exhibit broad correspondence but differ in polarized intensity by a factor of $\\sim$1.8."],"url":"http://arxiv.org/abs/2406.06166v1","category":"astro-ph.GA"}
{"created":"2024-06-10 11:00:26","title":"Generalized Nested Latent Variable Models for Lossy Coding applied to Wind Turbine Scenarios","abstract":"Rate-distortion optimization through neural networks has accomplished competitive results in compression efficiency and image quality. This learning-based approach seeks to minimize the compromise between compression rate and reconstructed image quality by automatically extracting and retaining crucial information, while discarding less critical details. A successful technique consists in introducing a deep hyperprior that operates within a 2-level nested latent variable model, enhancing compression by capturing complex data dependencies. This paper extends this concept by designing a generalized L-level nested generative model with a Markov chain structure. We demonstrate as L increases that a trainable prior is detrimental and explore a common dimensionality along the distinct latent variables to boost compression performance. As this structured framework can represent autoregressive coders, we outperform the hyperprior model and achieve state-of-the-art performance while reducing substantially the computational cost. Our experimental evaluation is performed on wind turbine scenarios to study its application on visual inspections","sentences":["Rate-distortion optimization through neural networks has accomplished competitive results in compression efficiency and image quality.","This learning-based approach seeks to minimize the compromise between compression rate and reconstructed image quality by automatically extracting and retaining crucial information, while discarding less critical details.","A successful technique consists in introducing a deep hyperprior that operates within a 2-level nested latent variable model, enhancing compression by capturing complex data dependencies.","This paper extends this concept by designing a generalized L-level nested generative model with a Markov chain structure.","We demonstrate as L increases that a trainable prior is detrimental and explore a common dimensionality along the distinct latent variables to boost compression performance.","As this structured framework can represent autoregressive coders, we outperform the hyperprior model and achieve state-of-the-art performance while reducing substantially the computational cost.","Our experimental evaluation is performed on wind turbine scenarios to study its application on visual inspections"],"url":"http://arxiv.org/abs/2406.06165v1","category":"cs.CV"}
{"created":"2024-06-10 10:56:59","title":"Time to Separate from StackOverflow and Match with ChatGPT for Encryption","abstract":"Cryptography is known as a challenging topic for developers. We studied StackOverflow posts to identify the problems that developers encounter when using Java Cryptography Architecture (JCA) for symmetric encryption. We investigated security risks that are disseminated in these posts, and we examined whether ChatGPT helps avoid cryptography issues. We found that developers frequently struggle with key and IV generations, as well as padding. Security is a top concern among developers, but security issues are pervasive in code snippets. ChatGPT can effectively aid developers when they engage with it properly. Nevertheless, it does not substitute human expertise, and developers should remain alert.","sentences":["Cryptography is known as a challenging topic for developers.","We studied StackOverflow posts to identify the problems that developers encounter when using Java Cryptography Architecture (JCA) for symmetric encryption.","We investigated security risks that are disseminated in these posts, and we examined whether ChatGPT helps avoid cryptography issues.","We found that developers frequently struggle with key and IV generations, as well as padding.","Security is a top concern among developers, but security issues are pervasive in code snippets.","ChatGPT can effectively aid developers when they engage with it properly.","Nevertheless, it does not substitute human expertise, and developers should remain alert."],"url":"http://arxiv.org/abs/2406.06164v1","category":"cs.CR"}
{"created":"2024-06-10 10:53:18","title":"Long-Range Quantum Tunneling via Matter Wave","abstract":"Quantum tunneling refers to a phenomenon that a microscopic object can pass through a potential barrier even it does not have enough energy to overcome the barrier. It has led to many modern applications and nanotechnologies. A general belief is that quantum tunneling, as a manifestation of the wave-particle duality, occurs only when the width of the barrier is comparable to or smaller than the de Broglie's wavelength of the object. Here, via studying the tunneling of an ultracold atom among $N$ far-separated trapping potentials in a state-selective optical lattice, we discover a mechanism to realize a long-range quantum tunneling. It is found that, by the mediation role of the propagating matter wave emitted from the excited-state atom, a coherent tunneling of the tightly confined atom to the remote trapping potentials can occur as long as bound states are present in the energy spectrum of the total system formed by the atom and its matter wave. Breaking through the generally believed distance constraint of quantum tunneling, our result opens another avenue to realize quantum tunneling and gives a guideline to develop tunneling devices.","sentences":["Quantum tunneling refers to a phenomenon that a microscopic object can pass through a potential barrier even it does not have enough energy to overcome the barrier.","It has led to many modern applications and nanotechnologies.","A general belief is that quantum tunneling, as a manifestation of the wave-particle duality, occurs only when the width of the barrier is comparable to or smaller than the de Broglie's wavelength of the object.","Here, via studying the tunneling of an ultracold atom among $N$ far-separated trapping potentials in a state-selective optical lattice, we discover a mechanism to realize a long-range quantum tunneling.","It is found that, by the mediation role of the propagating matter wave emitted from the excited-state atom, a coherent tunneling of the tightly confined atom to the remote trapping potentials can occur as long as bound states are present in the energy spectrum of the total system formed by the atom and its matter wave.","Breaking through the generally believed distance constraint of quantum tunneling, our result opens another avenue to realize quantum tunneling and gives a guideline to develop tunneling devices."],"url":"http://arxiv.org/abs/2406.06162v1","category":"quant-ph"}
{"created":"2024-06-10 10:46:44","title":"The Effect of Training Dataset Size on Discriminative and Diffusion-Based Speech Enhancement Systems","abstract":"The performance of deep neural network-based speech enhancement systems typically increases with the training dataset size. However, studies that investigated the effect of training dataset size on speech enhancement performance did not consider recent approaches, such as diffusion-based generative models. Diffusion models are typically trained with massive datasets for image generation tasks, but whether this is also required for speech enhancement is unknown. Moreover, studies that investigated the effect of training dataset size did not control for the data diversity. It is thus unclear whether the performance improvement was due to the increased dataset size or diversity. Therefore, we systematically investigate the effect of training dataset size on the performance of popular state-of-the-art discriminative and diffusion-based speech enhancement systems. We control for the data diversity by using a fixed set of speech utterances, noise segments and binaural room impulse responses to generate datasets of different sizes. We find that the diffusion-based systems do not benefit from increasing the training dataset size as much as the discriminative systems. They perform the best relative to the discriminative systems with datasets of 10 h or less, but they are outperformed by the discriminative systems with datasets of 100 h or more.","sentences":["The performance of deep neural network-based speech enhancement systems typically increases with the training dataset size.","However, studies that investigated the effect of training dataset size on speech enhancement performance did not consider recent approaches, such as diffusion-based generative models.","Diffusion models are typically trained with massive datasets for image generation tasks, but whether this is also required for speech enhancement is unknown.","Moreover, studies that investigated the effect of training dataset size did not control for the data diversity.","It is thus unclear whether the performance improvement was due to the increased dataset size or diversity.","Therefore, we systematically investigate the effect of training dataset size on the performance of popular state-of-the-art discriminative and diffusion-based speech enhancement systems.","We control for the data diversity by using a fixed set of speech utterances, noise segments and binaural room impulse responses to generate datasets of different sizes.","We find that the diffusion-based systems do not benefit from increasing the training dataset size as much as the discriminative systems.","They perform the best relative to the discriminative systems with datasets of 10 h or less, but they are outperformed by the discriminative systems with datasets of 100 h or more."],"url":"http://arxiv.org/abs/2406.06160v1","category":"eess.AS"}
{"created":"2024-06-10 10:42:37","title":"Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning","abstract":"While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs with balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.","sentences":["While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime.","In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning.","Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space.","We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions.","In linear networks, rapid feature learning only occurs with balanced initializations, where all layers learn at similar speeds.","While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning.","Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic.","Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning."],"url":"http://arxiv.org/abs/2406.06158v1","category":"cs.LG"}
{"created":"2024-06-10 10:41:22","title":"Model predictive control for tracking using artificial references: Fundamentals, recent results and practical implementation","abstract":"This paper provides a comprehensive tutorial on a family of Model Predictive Control (MPC) formulations, known as MPC for tracking, which are characterized by including an artificial reference as part of the decision variables in the optimization problem. These formulations have several benefits with respect to the classical MPC formulations, including guaranteed recursive feasibility under online reference changes, as well as asymptotic stability and an increased domain of attraction. This tutorial paper introduces the concept of using an artificial reference in MPC, presenting the benefits and theoretical guarantees obtained by its use. We then provide a survey of the main advances and extensions of the original linear MPC for tracking, including its non-linear extension. Additionally, we discuss its application to learning-based MPC, and discuss optimization aspects related to its implementation.","sentences":["This paper provides a comprehensive tutorial on a family of Model Predictive Control (MPC) formulations, known as MPC for tracking, which are characterized by including an artificial reference as part of the decision variables in the optimization problem.","These formulations have several benefits with respect to the classical MPC formulations, including guaranteed recursive feasibility under online reference changes, as well as asymptotic stability and an increased domain of attraction.","This tutorial paper introduces the concept of using an artificial reference in MPC, presenting the benefits and theoretical guarantees obtained by its use.","We then provide a survey of the main advances and extensions of the original linear MPC for tracking, including its non-linear extension.","Additionally, we discuss its application to learning-based MPC, and discuss optimization aspects related to its implementation."],"url":"http://arxiv.org/abs/2406.06157v1","category":"eess.SY"}
{"created":"2024-06-10 10:39:28","title":"Stronger, Faster, and Cheaper Log Parsing with LLMs","abstract":"Log parsing, the process of converting raw log messages into structured formats, is an important initial step for automated analysis of logs of large-scale software systems. Traditional log parsers often rely on heuristics or handcrafted features, which may not generalize well across diverse log sources or require extensive model tuning. Recently, some log parsers have utilized powerful generative capabilities of large language models (LLMs). However, they heavily rely on demonstration examples, resulting in substantial overhead in LLM invocations. To address these issues, we propose LogBatcher, a cost-effective LLM-based log parser that requires no training process or labeled data. To leverage latent characteristics of log data and reduce the overhead, we divide logs into several partitions through clustering. Then we perform a cache matching process to match logs with previously parsed log templates. Finally, we provide LLMs with better prompt context specialized for log parsing by batching a group of logs from each partition. We have conducted experiments on 16 public log datasets and the results show that LogBatcher is effective and efficient for log parsing.","sentences":["Log parsing, the process of converting raw log messages into structured formats, is an important initial step for automated analysis of logs of large-scale software systems.","Traditional log parsers often rely on heuristics or handcrafted features, which may not generalize well across diverse log sources or require extensive model tuning.","Recently, some log parsers have utilized powerful generative capabilities of large language models (LLMs).","However, they heavily rely on demonstration examples, resulting in substantial overhead in LLM invocations.","To address these issues, we propose LogBatcher, a cost-effective LLM-based log parser that requires no training process or labeled data.","To leverage latent characteristics of log data and reduce the overhead, we divide logs into several partitions through clustering.","Then we perform a cache matching process to match logs with previously parsed log templates.","Finally, we provide LLMs with better prompt context specialized for log parsing by batching a group of logs from each partition.","We have conducted experiments on 16 public log datasets and the results show that LogBatcher is effective and efficient for log parsing."],"url":"http://arxiv.org/abs/2406.06156v1","category":"cs.SE"}
{"created":"2024-06-10 10:37:38","title":"Asymptotic Approximation of Fading Mode in Neurooscillator Dynamics","abstract":"We consider a system consisting of two delay differential equations with a large parameter, modeling the association of a pair of neurooscillators. The unknown functions describe the changes in the normalized membrane potentials of neurons over time, with the large parameter characterizing the speed of electrical processes. The first equation is separated from the system and represents a generalized Hutchinson equation. This equation, as known, possesses periodic solutions with high peaks over the period. The second equation is also based on the generalized Hutchinson equation, but with an additional term, linking it to an oscillator satisfying the first equation. For the second equation, it is possible to asymptotically construct the so-called fading neuron mode, which is as follows: for any natural number $n$, one can adjust the parameters of the problem in such a way that the solution is asymptotically close to a periodic function with high peaks over $n$ periods, and then, after a transient process represented by decreasing peaks, becomes asymptotically small.","sentences":["We consider a system consisting of two delay differential equations with a large parameter, modeling the association of a pair of neurooscillators.","The unknown functions describe the changes in the normalized membrane potentials of neurons over time, with the large parameter characterizing the speed of electrical processes.","The first equation is separated from the system and represents a generalized Hutchinson equation.","This equation, as known, possesses periodic solutions with high peaks over the period.","The second equation is also based on the generalized Hutchinson equation, but with an additional term, linking it to an oscillator satisfying the first equation.","For the second equation, it is possible to asymptotically construct the so-called fading neuron mode, which is as follows: for any natural number $n$, one can adjust the parameters of the problem in such a way that the solution is asymptotically close to a periodic function with high peaks over $n$ periods, and then, after a transient process represented by decreasing peaks, becomes asymptotically small."],"url":"http://arxiv.org/abs/2406.06155v1","category":"nlin.SI"}
{"created":"2024-06-10 10:24:11","title":"Computationally Efficient Machine-Learning-Based Online Battery State of Health Estimation","abstract":"A key function of battery management systems (BMS) in e-mobility applications is estimating the battery state of health (SoH) with high accuracy. This is typically achieved in commercial BMS using model-based methods. There has been considerable research in developing data-driven methods for improving the accuracy of SoH estimation. The data-driven methods are diverse and use different machine-learning (ML) or artificial intelligence (AI) based techniques. Complex AI/ML techniques are difficult to implement in low-cost microcontrollers used in BMS due to the extensive use of non-linear functions and large matrix operations. This paper proposes a computationally efficient and data-lightweight SoH estimation technique. Online impedance at four discrete frequencies is evaluated to derive the features of a linear regression problem. The proposed solution avoids complex mathematical operations and it is well-suited for online implementation in a commercial BMS. The accuracy of this method is validated on two experimental datasets and is shown to have a mean absolute error (MAE) of less than 2% across diverse training and testing data.","sentences":["A key function of battery management systems (BMS) in e-mobility applications is estimating the battery state of health (SoH) with high accuracy.","This is typically achieved in commercial BMS using model-based methods.","There has been considerable research in developing data-driven methods for improving the accuracy of SoH estimation.","The data-driven methods are diverse and use different machine-learning (ML) or artificial intelligence (AI) based techniques.","Complex AI/ML techniques are difficult to implement in low-cost microcontrollers used in BMS due to the extensive use of non-linear functions and large matrix operations.","This paper proposes a computationally efficient and data-lightweight SoH estimation technique.","Online impedance at four discrete frequencies is evaluated to derive the features of a linear regression problem.","The proposed solution avoids complex mathematical operations and it is well-suited for online implementation in a commercial BMS.","The accuracy of this method is validated on two experimental datasets and is shown to have a mean absolute error (MAE) of less than 2% across diverse training and testing data."],"url":"http://arxiv.org/abs/2406.06151v1","category":"eess.SY"}
{"created":"2024-06-10 10:15:32","title":"Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations","abstract":"A Marked Temporal Point Process (MTPP) is a stochastic process whose realization is a set of event-time data. MTPP is often used to understand complex dynamics of asynchronous temporal events such as money transaction, social media, healthcare, etc. Recent studies have utilized deep neural networks to capture complex temporal dependencies of events and generate embedding that aptly represent the observed events. While most previous studies focus on the inter-event dependencies and their representations, how individual events influence the overall dynamics over time has been under-explored. In this regime, we propose a Decoupled MTPP framework that disentangles characterization of a stochastic process into a set of evolving influences from different events. Our approach employs Neural Ordinary Differential Equations (Neural ODEs) to learn flexible continuous dynamics of these influences while simultaneously addressing multiple inference problems, such as density estimation and survival rate computation. We emphasize the significance of disentangling the influences by comparing our framework with state-of-the-art methods on real-life datasets, and provide analysis on the model behavior for potential applications.","sentences":["A Marked Temporal Point Process (MTPP) is a stochastic process whose realization is a set of event-time data.","MTPP is often used to understand complex dynamics of asynchronous temporal events such as money transaction, social media, healthcare, etc.","Recent studies have utilized deep neural networks to capture complex temporal dependencies of events and generate embedding that aptly represent the observed events.","While most previous studies focus on the inter-event dependencies and their representations, how individual events influence the overall dynamics over time has been under-explored.","In this regime, we propose a Decoupled MTPP framework that disentangles characterization of a stochastic process into a set of evolving influences from different events.","Our approach employs Neural Ordinary Differential Equations (Neural ODEs) to learn flexible continuous dynamics of these influences while simultaneously addressing multiple inference problems, such as density estimation and survival rate computation.","We emphasize the significance of disentangling the influences by comparing our framework with state-of-the-art methods on real-life datasets, and provide analysis on the model behavior for potential applications."],"url":"http://arxiv.org/abs/2406.06149v1","category":"cs.LG"}
{"created":"2024-06-10 10:13:12","title":"Nanoscale Transmitters Employing Cooperative Transmembrane Transport Proteins for Molecular Communication","abstract":"This paper introduces a novel optically controllable molecular communication (MC) transmitter (TX) design, which is based on a vesicular nanodevice (ND) functionalized for the release of signaling molecules via transmembrane proteins. Due to its optical-to-chemical conversion capability, the ND can be used as an externally controllable TX for several MC applications such as bit transmission and targeted drug delivery. The proposed TX design comprises two cooperating modules, an energizing module and a release module, and depending on the specific choices for the modules allows for the release of different types of signaling molecules. After setting up a general system model for the proposed TX design, we conduct a detailed mathematical analysis of a specific realization. In particular, we derive an exact analytical and an approximate closed-form solution for the concentration of the released signaling molecules and validate our results by comparison with a numerical solution. Moreover, we consider the impact of a buffering medium, which is typically present in experimental and application environments, in both our analytical and numerical analyses to evaluate the feasibility of our proposed TX design for practical chemical implementation. The proposed analytical and closed-form models facilitate system parameter optimization, which can accelerate the experimental development cycle of the proposed ND architecture in the future.","sentences":["This paper introduces a novel optically controllable molecular communication (MC) transmitter (TX) design, which is based on a vesicular nanodevice (ND) functionalized for the release of signaling molecules via transmembrane proteins.","Due to its optical-to-chemical conversion capability, the ND can be used as an externally controllable TX for several MC applications such as bit transmission and targeted drug delivery.","The proposed TX design comprises two cooperating modules, an energizing module and a release module, and depending on the specific choices for the modules allows for the release of different types of signaling molecules.","After setting up a general system model for the proposed TX design, we conduct a detailed mathematical analysis of a specific realization.","In particular, we derive an exact analytical and an approximate closed-form solution for the concentration of the released signaling molecules and validate our results by comparison with a numerical solution.","Moreover, we consider the impact of a buffering medium, which is typically present in experimental and application environments, in both our analytical and numerical analyses to evaluate the feasibility of our proposed TX design for practical chemical implementation.","The proposed analytical and closed-form models facilitate system parameter optimization, which can accelerate the experimental development cycle of the proposed ND architecture in the future."],"url":"http://arxiv.org/abs/2406.06147v1","category":"cs.ET"}
{"created":"2024-06-10 10:04:21","title":"Mastering truss structure optimization with tree search","abstract":"This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures. Our approach accommodates intermediate construction stages characteristic of progressive construction settings. We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning. These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs. We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for identifying the global optimum. Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality. While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications.","sentences":["This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures.","Our approach accommodates intermediate construction stages characteristic of progressive construction settings.","We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning.","These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs.","We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for identifying the global optimum.","Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality.","While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications."],"url":"http://arxiv.org/abs/2406.06145v1","category":"cs.CE"}
{"created":"2024-06-10 10:03:16","title":"Language Models Resist Alignment","abstract":"Large language models (LLMs) may exhibit undesirable behaviors. Recent efforts have focused on aligning these models to prevent harmful generation. Despite these efforts, studies have shown that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Do alignment fine-tuning have robust effects on models, or are merely superficial? In this work, we answer this question through both theoretical and empirical means. Empirically, we demonstrate the elasticity of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Using compression theory, we formally derive that such fine-tuning process \\textit{disproportionately} undermines alignment compared to pre-training, potentially by orders of magnitude. We conduct experimental validations to confirm the presence of elasticity across models of varying types and sizes. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. We further reveal that elasticity positively correlates with increased model size and the expansion of pre-training data. Our discovery signifies the importance of taming the inherent elasticity of LLMs, thereby overcoming the resistance of LLMs to alignment finetuning.","sentences":["Large language models (LLMs) may exhibit undesirable behaviors.","Recent efforts have focused on aligning these models to prevent harmful generation.","Despite these efforts, studies have shown that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally.","Do alignment fine-tuning have robust effects on models, or are merely superficial?","In this work, we answer this question through both theoretical and empirical means.","Empirically, we demonstrate the elasticity of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning.","Using compression theory, we formally derive that such fine-tuning process \\textit{disproportionately} undermines alignment compared to pre-training, potentially by orders of magnitude.","We conduct experimental validations to confirm the presence of elasticity across models of varying types and sizes.","Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly.","We further reveal that elasticity positively correlates with increased model size and the expansion of pre-training data.","Our discovery signifies the importance of taming the inherent elasticity of LLMs, thereby overcoming the resistance of LLMs to alignment finetuning."],"url":"http://arxiv.org/abs/2406.06144v1","category":"cs.CL"}
{"created":"2024-06-10 10:01:19","title":"The Integrated Information Theory needs Attention","abstract":"The Integrated Information Theory (IIT) might be our current best bet at a scientific explanation of phenomenal consciousness. IIT focuses on the distinctively subjective and phenomenological aspects of conscious experience. Currently, it offers the fundaments of a formal account, but future developments shall explain the qualitative structures of every possible conscious experience. But this ambitious project is hindered by one fundamental limitation. IIT fails to acknowledge the crucial roles of attention in generating phenomenally conscious experience and shaping its contents. Here, we argue that IIT urgently needs an account of attention. Without this account, IIT cannot explain important informational differences between different kinds of experiences. Furthermore, though some IIT proponents celebratedly endorse a double dissociation between consciousness and attention, close analysis reveals that such as dissociation is in fact incompatible with IIT. Notably, the issues we raise for IIT will likely arise for many internalist theories of conscious contents in philosophy, especially theories with primitivist inclinations. Our arguments also extend to the recently popularized structuralist approaches. Overall, our discussion highlights how considerations about attention are indispensable for scientific as well as philosophical theorizing about conscious experience.","sentences":["The Integrated Information Theory (IIT) might be our current best bet at a scientific explanation of phenomenal consciousness.","IIT focuses on the distinctively subjective and phenomenological aspects of conscious experience.","Currently, it offers the fundaments of a formal account, but future developments shall explain the qualitative structures of every possible conscious experience.","But this ambitious project is hindered by one fundamental limitation.","IIT fails to acknowledge the crucial roles of attention in generating phenomenally conscious experience and shaping its contents.","Here, we argue that IIT urgently needs an account of attention.","Without this account, IIT cannot explain important informational differences between different kinds of experiences.","Furthermore, though some IIT proponents celebratedly endorse a double dissociation between consciousness and attention, close analysis reveals that such as dissociation is in fact incompatible with IIT.","Notably, the issues we raise for IIT will likely arise for many internalist theories of conscious contents in philosophy, especially theories with primitivist inclinations.","Our arguments also extend to the recently popularized structuralist approaches.","Overall, our discussion highlights how considerations about attention are indispensable for scientific as well as philosophical theorizing about conscious experience."],"url":"http://arxiv.org/abs/2406.06143v1","category":"q-bio.NC"}
{"created":"2024-06-10 10:00:37","title":"Phonon Drag Effect in Nernst and Thermal Hall Effects: General Theory and Application to Dilute Metal SrTiO$_{3-\u03b4}$","abstract":"In magnetic fields, thermal gradient-induced effects such as the Nernst and thermal Hall effects are significantly influenced by phonon drag, which works in conjunction with the Lorentz force on electrons. We introduce a method to calculate Nernst and thermal Hall conductivities influenced by phonon drag using linear response theory to treat the magnetic field as a first-order perturbation. Our formula is general enough to apply to any system as long as the Green's functions of electrons and phonons are given. We apply the obtained general theory to the recent experiments of dilute metal SrTiO$_{3-\\delta}$, known for strong Nernst and thermal Hall effects due to phonon drag. We find good agreement even quantitatively. This is notable as all model parameters are derived from experimental data without adjustable parameters.","sentences":["In magnetic fields, thermal gradient-induced effects such as the Nernst and thermal Hall effects are significantly influenced by phonon drag, which works in conjunction with the Lorentz force on electrons.","We introduce a method to calculate Nernst and thermal Hall conductivities influenced by phonon drag using linear response theory to treat the magnetic field as a first-order perturbation.","Our formula is general enough to apply to any system as long as the Green's functions of electrons and phonons are given.","We apply the obtained general theory to the recent experiments of dilute metal SrTiO$_{3-\\delta}$, known for strong Nernst and thermal Hall effects due to phonon drag.","We find good agreement even quantitatively.","This is notable as all model parameters are derived from experimental data without adjustable parameters."],"url":"http://arxiv.org/abs/2406.06142v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 09:53:54","title":"Can I understand what I create? Self-Knowledge Evaluation of Large Language Models","abstract":"Large language models (LLMs) have achieved remarkable progress in linguistic tasks, necessitating robust evaluation frameworks to understand their capabilities and limitations. Inspired by Feynman's principle of understanding through creation, we introduce a self-knowledge evaluation framework that is easy to implement, evaluating models on their ability to comprehend and respond to self-generated questions. Our findings, based on testing multiple models across diverse tasks, reveal significant gaps in the model's self-knowledge ability. Further analysis indicates these gaps may be due to misalignment with human attention mechanisms. Additionally, fine-tuning on self-generated math task may enhance the model's math performance, highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs.","sentences":["Large language models (LLMs) have achieved remarkable progress in linguistic tasks, necessitating robust evaluation frameworks to understand their capabilities and limitations.","Inspired by Feynman's principle of understanding through creation, we introduce a self-knowledge evaluation framework that is easy to implement, evaluating models on their ability to comprehend and respond to self-generated questions.","Our findings, based on testing multiple models across diverse tasks, reveal significant gaps in the model's self-knowledge ability.","Further analysis indicates these gaps may be due to misalignment with human attention mechanisms.","Additionally, fine-tuning on self-generated math task may enhance the model's math performance, highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs."],"url":"http://arxiv.org/abs/2406.06140v1","category":"cs.CL"}
{"created":"2024-06-10 09:52:25","title":"Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse Step using Brownian Bridge","abstract":"Diffusion-based speech enhancement has shown promising results, but can suffer from a slower inference time. Initializing the diffusion process with the enhanced audio generated by a regression-based model can be used to reduce the computational steps required. However, these approaches often necessitate a regression model, further increasing the system's complexity. We propose Thunder, a unified regression-diffusion model that utilizes the Brownian bridge process which can allow the model to act in both modes. The regression mode can be accessed by setting the diffusion time step closed to 1. However, the standard score-based diffusion modeling does not perform well in this setup due to gradient instability. To mitigate this problem, we modify the diffusion model to predict the clean speech instead of the score function, achieving competitive performance with a more compact model size and fewer reverse steps.","sentences":["Diffusion-based speech enhancement has shown promising results, but can suffer from a slower inference time.","Initializing the diffusion process with the enhanced audio generated by a regression-based model can be used to reduce the computational steps required.","However, these approaches often necessitate a regression model, further increasing the system's complexity.","We propose Thunder, a unified regression-diffusion model that utilizes the Brownian bridge process which can allow the model to act in both modes.","The regression mode can be accessed by setting the diffusion time step closed to 1.","However, the standard score-based diffusion modeling does not perform well in this setup due to gradient instability.","To mitigate this problem, we modify the diffusion model to predict the clean speech instead of the score function, achieving competitive performance with a more compact model size and fewer reverse steps."],"url":"http://arxiv.org/abs/2406.06139v1","category":"cs.SD"}
{"created":"2024-06-10 09:51:01","title":"Power-law correlation in the homogeneous disordered state of anisotropically self-propelled systems","abstract":"Self-propelled particles display unique collective phenomena, due to the intrinsic coupling of density and polarity. For instance, the giant number fluctuation appears in the orientationally ordered state, and the motility-induced phase separation appears in systems with repulsion. Effects of strong noise typically lead to a homogeneous disordered state, in which the coupling of density and polarity can still play a significant role. Here, we study universal properties of the homogeneous disordered state in two-dimensional systems with uniaxially anisotropic self-propulsion. Using hydrodynamic arguments, we propose that the density correlation and polarity correlation generically exhibit power-law decay with distinct exponents (-2 and -4, respectively) through the coupling of density and polarity. Simulations of self-propelled lattice gas models indeed show the predicted power-law correlations, regardless of whether the interaction type is repulsion or alignment. Further, by mapping the model to a two-component boson system and employing non-Hermitian perturbation theory, we obtain the analytical expression for the structure factors, the Fourier transform of the correlation functions. This reveals that even the first order of the interaction strength induces the power-law correlations.","sentences":["Self-propelled particles display unique collective phenomena, due to the intrinsic coupling of density and polarity.","For instance, the giant number fluctuation appears in the orientationally ordered state, and the motility-induced phase separation appears in systems with repulsion.","Effects of strong noise typically lead to a homogeneous disordered state, in which the coupling of density and polarity can still play a significant role.","Here, we study universal properties of the homogeneous disordered state in two-dimensional systems with uniaxially anisotropic self-propulsion.","Using hydrodynamic arguments, we propose that the density correlation and polarity correlation generically exhibit power-law decay with distinct exponents (-2 and -4, respectively) through the coupling of density and polarity.","Simulations of self-propelled lattice gas models indeed show the predicted power-law correlations, regardless of whether the interaction type is repulsion or alignment.","Further, by mapping the model to a two-component boson system and employing non-Hermitian perturbation theory, we obtain the analytical expression for the structure factors, the Fourier transform of the correlation functions.","This reveals that even the first order of the interaction strength induces the power-law correlations."],"url":"http://arxiv.org/abs/2406.06138v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-10 09:50:53","title":"Matrix norm shrinkage estimators and priors","abstract":"We develop a class of minimax estimators for a normal mean matrix under the Frobenius loss, which generalizes the James--Stein and Efron--Morris estimators. It shrinks the Schatten norm towards zero and works well for low-rank matrices. We also propose a class of superharmonic priors based on the Schatten norm, which generalizes Stein's prior and the singular value shrinkage prior. The generalized Bayes estimators and Bayesian predictive densities with respect to these priors are minimax. We examine the performance of the proposed estimators and priors in simulation.","sentences":["We develop a class of minimax estimators for a normal mean matrix under the Frobenius loss, which generalizes the James--Stein and Efron--Morris estimators.","It shrinks the Schatten norm towards zero and works well for low-rank matrices.","We also propose a class of superharmonic priors based on the Schatten norm, which generalizes Stein's prior and the singular value shrinkage prior.","The generalized Bayes estimators and Bayesian predictive densities with respect to these priors are minimax.","We examine the performance of the proposed estimators and priors in simulation."],"url":"http://arxiv.org/abs/2406.06137v1","category":"math.ST"}
{"created":"2024-06-10 09:48:13","title":"A Comparative Survey of Vision Transformers for Feature Extraction in Texture Analysis","abstract":"Texture, a significant visual attribute in images, has been extensively investigated across various image recognition applications. Convolutional Neural Networks (CNNs), which have been successful in many computer vision tasks, are currently among the best texture analysis approaches. On the other hand, Vision Transformers (ViTs) have been surpassing the performance of CNNs on tasks such as object recognition, causing a paradigm shift in the field. However, ViTs have so far not been scrutinized for texture recognition, hindering a proper appreciation of their potential in this specific setting. For this reason, this work explores various pre-trained ViT architectures when transferred to tasks that rely on textures. We review 21 different ViT variants and perform an extensive evaluation and comparison with CNNs and hand-engineered models on several tasks, such as assessing robustness to changes in texture rotation, scale, and illumination, and distinguishing color textures, material textures, and texture attributes. The goal is to understand the potential and differences among these models when directly applied to texture recognition, using pre-trained ViTs primarily for feature extraction and employing linear classifiers for evaluation. We also evaluate their efficiency, which is one of the main drawbacks in contrast to other methods. Our results show that ViTs generally outperform both CNNs and hand-engineered models, especially when using stronger pre-training and tasks involving in-the-wild textures (images from the internet). We highlight the following promising models: ViT-B with DINO pre-training, BeiTv2, and the Swin architecture, as well as the EfficientFormer as a low-cost alternative. In terms of efficiency, although having a higher number of GFLOPs and parameters, ViT-B and BeiT(v2) can achieve a lower feature extraction time on GPUs compared to ResNet50.","sentences":["Texture, a significant visual attribute in images, has been extensively investigated across various image recognition applications.","Convolutional Neural Networks (CNNs), which have been successful in many computer vision tasks, are currently among the best texture analysis approaches.","On the other hand, Vision Transformers (ViTs) have been surpassing the performance of CNNs on tasks such as object recognition, causing a paradigm shift in the field.","However, ViTs have so far not been scrutinized for texture recognition, hindering a proper appreciation of their potential in this specific setting.","For this reason, this work explores various pre-trained ViT architectures when transferred to tasks that rely on textures.","We review 21 different ViT variants and perform an extensive evaluation and comparison with CNNs and hand-engineered models on several tasks, such as assessing robustness to changes in texture rotation, scale, and illumination, and distinguishing color textures, material textures, and texture attributes.","The goal is to understand the potential and differences among these models when directly applied to texture recognition, using pre-trained ViTs primarily for feature extraction and employing linear classifiers for evaluation.","We also evaluate their efficiency, which is one of the main drawbacks in contrast to other methods.","Our results show that ViTs generally outperform both CNNs and hand-engineered models, especially when using stronger pre-training and tasks involving in-the-wild textures (images from the internet).","We highlight the following promising models: ViT-B with DINO pre-training, BeiTv2, and the Swin architecture, as well as the EfficientFormer as a low-cost alternative.","In terms of efficiency, although having a higher number of GFLOPs and parameters, ViT-B and BeiT(v2) can achieve a lower feature extraction time on GPUs compared to ResNet50."],"url":"http://arxiv.org/abs/2406.06136v1","category":"cs.CV"}
{"created":"2024-06-10 09:45:38","title":"DiffInject: Revisiting Debias via Synthetic Data Generation using Diffusion-based Style Injection","abstract":"Dataset bias is a significant challenge in machine learning, where specific attributes, such as texture or color of the images are unintentionally learned resulting in detrimental performance. To address this, previous efforts have focused on debiasing models either by developing novel debiasing algorithms or by generating synthetic data to mitigate the prevalent dataset biases. However, generative approaches to date have largely relied on using bias-specific samples from the dataset, which are typically too scarce. In this work, we propose, DiffInject, a straightforward yet powerful method to augment synthetic bias-conflict samples using a pretrained diffusion model. This approach significantly advances the use of diffusion models for debiasing purposes by manipulating the latent space. Our framework does not require any explicit knowledge of the bias types or labelling, making it a fully unsupervised setting for debiasing. Our methodology demonstrates substantial result in effectively reducing dataset bias.","sentences":["Dataset bias is a significant challenge in machine learning, where specific attributes, such as texture or color of the images are unintentionally learned resulting in detrimental performance.","To address this, previous efforts have focused on debiasing models either by developing novel debiasing algorithms or by generating synthetic data to mitigate the prevalent dataset biases.","However, generative approaches to date have largely relied on using bias-specific samples from the dataset, which are typically too scarce.","In this work, we propose, DiffInject, a straightforward yet powerful method to augment synthetic bias-conflict samples using a pretrained diffusion model.","This approach significantly advances the use of diffusion models for debiasing purposes by manipulating the latent space.","Our framework does not require any explicit knowledge of the bias types or labelling, making it a fully unsupervised setting for debiasing.","Our methodology demonstrates substantial result in effectively reducing dataset bias."],"url":"http://arxiv.org/abs/2406.06134v1","category":"cs.CV"}
{"created":"2024-06-10 09:41:33","title":"Instantaneous optical singularities and duality-protected dark directions","abstract":"Electromagnetic waves are described by not only polarization ellipses but also cyclically rotating vectors tracing out them. The corresponding fields are respectively directionless steady line fields and directional instantaneous vector fields. Here we study the seminal topic of electromagnetic scattering from the perspective of instantaneous vector fields and uncover how the global topology of the momentum sphere regulates local distributions of tangent scattered fields. Structurally-stable generic singularities of vector fields move cyclically along lines of linear polarizations and at any instant their index sum has to be the Euler characteristic $\\chi=2$. This contrasts sharply with steady line fields, of which generic singularities constrained by the Euler characteristic locate on points of circular polarizations. From such unique perspective of instantaneous singularities, we discovered that for circularly-polarized waves scattered by electromagnetic duality-symmetric particles, since linearly-polarized scatterings are prohibited by helicity conservation, there must exist at least one dark direction along which the scattering is strictly zero. Two such dark directions can be tuned to overlap, along which the scattering would remain zero for arbitrary incident polarizations. We have essentially revealed that \\textit{polarizations underdescribe vectorial electromagnetic waves and the instantaneous perspective is indispensable}. The complementarity we discover provides broader and deeper insights into not only electromagnetism, but also other branches of wave physics where singularities are generic and ubiquitous.","sentences":["Electromagnetic waves are described by not only polarization ellipses but also cyclically rotating vectors tracing out them.","The corresponding fields are respectively directionless steady line fields and directional instantaneous vector fields.","Here we study the seminal topic of electromagnetic scattering from the perspective of instantaneous vector fields and uncover how the global topology of the momentum sphere regulates local distributions of tangent scattered fields.","Structurally-stable generic singularities of vector fields move cyclically along lines of linear polarizations and at any instant their index sum has to be the Euler characteristic $\\chi=2$. This contrasts sharply with steady line fields, of which generic singularities constrained by the Euler characteristic locate on points of circular polarizations.","From such unique perspective of instantaneous singularities, we discovered that for circularly-polarized waves scattered by electromagnetic duality-symmetric particles, since linearly-polarized scatterings are prohibited by helicity conservation, there must exist at least one dark direction along which the scattering is strictly zero.","Two such dark directions can be tuned to overlap, along which the scattering would remain zero for arbitrary incident polarizations.","We have essentially revealed that \\textit{polarizations underdescribe vectorial electromagnetic waves and the instantaneous perspective is indispensable}.","The complementarity we discover provides broader and deeper insights into not only electromagnetism, but also other branches of wave physics where singularities are generic and ubiquitous."],"url":"http://arxiv.org/abs/2406.06132v1","category":"physics.optics"}
{"created":"2024-06-10 09:39:19","title":"Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into German","abstract":"The translation of gender-neutral person-referring terms (e.g., the students) is often non-trivial. Translating from English into German poses an interesting case -- in German, person-referring nouns are usually gender-specific, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used. This solution, however, reduces the visibility of other genders, such as women and non-binary people. To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems). However, gender-fair German is currently barely supported in machine translation (MT), requiring post-editing or manual translations. We address this research gap by studying gender-fair language in English-to-German MT. Concretely, we enrich a community-created gender-fair language dictionary and sample multi-sentence test instances from encyclopedic text and parliamentary speeches. Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural MT models for translating words in isolation and natural contexts across two domains. Our findings show that most systems produce mainly masculine forms and rarely gender-neutral variants, highlighting the need for future research. We release code and data at https://github.com/g8a9/building-bridges-gender-fair-german-mt.","sentences":["The translation of gender-neutral person-referring terms (e.g., the students) is often non-trivial.","Translating from English into German poses an interesting case -- in German, person-referring nouns are usually gender-specific, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used.","This solution, however, reduces the visibility of other genders, such as women and non-binary people.","To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems).","However, gender-fair German is currently barely supported in machine translation (MT), requiring post-editing or manual translations.","We address this research gap by studying gender-fair language in English-to-German MT.","Concretely, we enrich a community-created gender-fair language dictionary and sample multi-sentence test instances from encyclopedic text and parliamentary speeches.","Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural MT models for translating words in isolation and natural contexts across two domains.","Our findings show that most systems produce mainly masculine forms and rarely gender-neutral variants, highlighting the need for future research.","We release code and data at https://github.com/g8a9/building-bridges-gender-fair-german-mt."],"url":"http://arxiv.org/abs/2406.06131v1","category":"cs.CL"}
{"created":"2024-06-10 09:38:38","title":"Nonlinear Model Predictive Control of Tiltrotor Quadrotors with Feasible Control Allocation","abstract":"This paper presents a new flight control framework for tilt-rotor multirotor uncrewed aerial vehicles (MRUAVs). Tiltrotor designs offer full actuation but introduce complexity in control allocation due to actuator redundancy. We propose a new approach where the allocator is tightly coupled with the controller, ensuring that the control signals generated by the controller are feasible within the vehicle actuation space. We leverage nonlinear model predictive control (NMPC) to implement the above framework, providing feasible control signals and optimizing performance. This unified control structure simultaneously manages both position and attitude, which eliminates the need for cascaded position and attitude control loops. Extensive numerical experiments demonstrate that our approach significantly outperforms conventional techniques that are based on linear quadratic regulator (LQR) and sliding mode control (SMC), especially in high-acceleration trajectories and disturbance rejection scenarios, making the proposed approach a viable option for enhanced control precision and robustness, particularly in challenging missions.","sentences":["This paper presents a new flight control framework for tilt-rotor multirotor uncrewed aerial vehicles (MRUAVs).","Tiltrotor designs offer full actuation but introduce complexity in control allocation due to actuator redundancy.","We propose a new approach where the allocator is tightly coupled with the controller, ensuring that the control signals generated by the controller are feasible within the vehicle actuation space.","We leverage nonlinear model predictive control (NMPC) to implement the above framework, providing feasible control signals and optimizing performance.","This unified control structure simultaneously manages both position and attitude, which eliminates the need for cascaded position and attitude control loops.","Extensive numerical experiments demonstrate that our approach significantly outperforms conventional techniques that are based on linear quadratic regulator (LQR) and sliding mode control (SMC), especially in high-acceleration trajectories and disturbance rejection scenarios, making the proposed approach a viable option for enhanced control precision and robustness, particularly in challenging missions."],"url":"http://arxiv.org/abs/2406.06130v1","category":"cs.RO"}
{"created":"2024-06-10 09:37:51","title":"Federated Machine Reasoning for Resource Provisioning in 6G O-RAN","abstract":"O-RAN specifications reshape RANs with function disaggregation and open interfaces, driven by RAN Intelligent Controllers. This enables data-driven management through AI/ML but poses trust challenges due to human operators' limited understanding of AI/ML decision-making. Balancing resource provisioning and avoiding overprovisioning and underprovisioning is critical, especially among the multiple virtualized base station(vBS) instances. Thus, we propose a novel Federated Machine Reasoning (FLMR) framework, a neurosymbolic method for federated reasoning, learning, and querying. FLMR optimizes CPU demand prediction based on contextual information and vBS configuration using local monitoring data from virtual base stations (vBS) on a shared O-Cloud platform.This optimization is critical, as insufficient computing resources can result in synchronization loss and significantly reduce network throughput. In the telecom domain, particularly in the virtual Radio Access Network (vRAN) sector, predicting and managing the CPU load of vBSs poses a significant challenge for network operators. Our proposed FLMR framework ensures transparency and human understanding in AI/ML decisions and addresses the evolving demands of the 6G O-RAN landscape, where reliability and performance are paramount. Furthermore, we performed a comparative analysis using \\textit{DeepCog} as the baseline method. The outcomes highlight how our proposed approach outperforms the baseline and strikes a better balance between resource overprovisioning and underprovisioning. Our method notably lowers both provisioning relative to the baseline by a factor of 6.","sentences":["O-RAN specifications reshape RANs with function disaggregation and open interfaces, driven by RAN Intelligent Controllers.","This enables data-driven management through AI/ML but poses trust challenges due to human operators' limited understanding of AI/ML decision-making.","Balancing resource provisioning and avoiding overprovisioning and underprovisioning is critical, especially among the multiple virtualized base station(vBS) instances.","Thus, we propose a novel Federated Machine Reasoning (FLMR) framework, a neurosymbolic method for federated reasoning, learning, and querying.","FLMR optimizes CPU demand prediction based on contextual information and vBS configuration using local monitoring data from virtual base stations (vBS) on a shared O-Cloud platform.","This optimization is critical, as insufficient computing resources can result in synchronization loss and significantly reduce network throughput.","In the telecom domain, particularly in the virtual Radio Access Network (vRAN) sector, predicting and managing the CPU load of vBSs poses a significant challenge for network operators.","Our proposed FLMR framework ensures transparency and human understanding in AI/ML decisions and addresses the evolving demands of the 6G O-RAN landscape, where reliability and performance are paramount.","Furthermore, we performed a comparative analysis using \\textit{DeepCog} as the baseline method.","The outcomes highlight how our proposed approach outperforms the baseline and strikes a better balance between resource overprovisioning and underprovisioning.","Our method notably lowers both provisioning relative to the baseline by a factor of 6."],"url":"http://arxiv.org/abs/2406.06128v1","category":"cs.IT"}
{"created":"2024-06-10 09:36:05","title":"Comparing Data Augmentation Methods for End-to-End Task-Oriented Dialog Systems","abstract":"Creating effective and reliable task-oriented dialog systems (ToDSs) is challenging, not only because of the complex structure of these systems, but also due to the scarcity of training data, especially when several modules need to be trained separately, each one with its own input/output training examples. Data augmentation (DA), whereby synthetic training examples are added to the training data, has been successful in other NLP systems, but has not been explored as extensively in ToDSs. We empirically evaluate the effectiveness of DA methods in an end-to-end ToDS setting, where a single system is trained to handle all processing stages, from user inputs to system outputs. We experiment with two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET). We consider three types of DA methods (word-level, sentence-level, dialog-level), comparing eight DA methods that have shown promising results in ToDSs and other NLP systems. We show that all DA methods considered are beneficial, and we highlight the best ones, also providing advice to practitioners. We also introduce a more challenging few-shot cross-domain ToDS setting, reaching similar conclusions.","sentences":["Creating effective and reliable task-oriented dialog systems (ToDSs) is challenging, not only because of the complex structure of these systems, but also due to the scarcity of training data, especially when several modules need to be trained separately, each one with its own input/output training examples.","Data augmentation (DA), whereby synthetic training examples are added to the training data, has been successful in other NLP systems, but has not been explored as extensively in ToDSs.","We empirically evaluate the effectiveness of DA methods in an end-to-end ToDS setting, where a single system is trained to handle all processing stages, from user inputs to system outputs.","We experiment with two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET).","We consider three types of DA methods (word-level, sentence-level, dialog-level), comparing eight DA methods that have shown promising results in ToDSs and other NLP systems.","We show that all DA methods considered are beneficial, and we highlight the best ones, also providing advice to practitioners.","We also introduce a more challenging few-shot cross-domain ToDS setting, reaching similar conclusions."],"url":"http://arxiv.org/abs/2406.06127v1","category":"cs.CL"}
{"created":"2024-06-10 09:32:37","title":"Verifiable Generation with Subsentence-Level Fine-Grained Citations","abstract":"Verifiable generation requires large language models (LLMs) to cite source documents supporting their outputs, thereby improve output transparency and trustworthiness. Yet, previous work mainly targets the generation of sentence-level citations, lacking specificity about which parts of a sentence are backed by the cited sources. This work studies verifiable generation with subsentence-level fine-grained citations for more precise location of generated content supported by the cited sources. We first present a dataset, SCiFi, comprising 10K Wikipedia paragraphs with subsentence-level citations. Each paragraph is paired with a set of candidate source documents for citation and a query that triggers the generation of the paragraph content. On SCiFi, we evaluate the performance of state-of-the-art LLMs and strategies for processing long documents designed for these models. Our experiment results reveals key factors that could enhance the quality of citations, including the expansion of the source documents' context accessible to the models and the implementation of specialized model tuning.","sentences":["Verifiable generation requires large language models (LLMs) to cite source documents supporting their outputs, thereby improve output transparency and trustworthiness.","Yet, previous work mainly targets the generation of sentence-level citations, lacking specificity about which parts of a sentence are backed by the cited sources.","This work studies verifiable generation with subsentence-level fine-grained citations for more precise location of generated content supported by the cited sources.","We first present a dataset, SCiFi, comprising 10K Wikipedia paragraphs with subsentence-level citations.","Each paragraph is paired with a set of candidate source documents for citation and a query that triggers the generation of the paragraph content.","On SCiFi, we evaluate the performance of state-of-the-art LLMs and strategies for processing long documents designed for these models.","Our experiment results reveals key factors that could enhance the quality of citations, including the expansion of the source documents' context accessible to the models and the implementation of specialized model tuning."],"url":"http://arxiv.org/abs/2406.06125v1","category":"cs.CL"}
{"created":"2024-06-10 09:29:08","title":"Enhancing Long-Term Memory using Hierarchical Aggregate Tree for Retrieval Augmented Generation","abstract":"Large language models have limited context capacity, hindering reasoning over long conversations. We propose the Hierarchical Aggregate Tree memory structure to recursively aggregate relevant dialogue context through conditional tree traversals. HAT encapsulates information from children nodes, enabling broad coverage with depth control. We formulate finding best context as optimal tree traversal. Experiments show HAT improves dialog coherence and summary quality over baseline contexts, demonstrating the techniques effectiveness for multi turn reasoning without exponential parameter growth. This memory augmentation enables more consistent, grounded longform conversations from LLMs","sentences":["Large language models have limited context capacity, hindering reasoning over long conversations.","We propose the Hierarchical Aggregate Tree memory structure to recursively aggregate relevant dialogue context through conditional tree traversals.","HAT encapsulates information from children nodes, enabling broad coverage with depth control.","We formulate finding best context as optimal tree traversal.","Experiments show HAT improves dialog coherence and summary quality over baseline contexts, demonstrating the techniques effectiveness for multi turn reasoning without exponential parameter growth.","This memory augmentation enables more consistent, grounded longform conversations from LLMs"],"url":"http://arxiv.org/abs/2406.06124v1","category":"cs.CL"}
{"created":"2024-06-10 09:16:27","title":"W-Net: One-Shot Arbitrary-Style Chinese Character Generation with Deep Neural Networks","abstract":"Due to the huge category number, the sophisticated combinations of various strokes and radicals, and the free writing or printing styles, generating Chinese characters with diverse styles is always considered as a difficult task. In this paper, an efficient and generalized deep framework, namely, the W-Net, is introduced for the one-shot arbitrary-style Chinese character generation task. Specifically, given a single character (one-shot) with a specific style (e.g., a printed font or hand-writing style), the proposed W-Net model is capable of learning and generating any arbitrary characters sharing the style similar to the given single character. Such appealing property was rarely seen in the literature. We have compared the proposed W-Net framework to many other competitive methods. Experimental results showed the proposed method is significantly superior in the one-shot setting.","sentences":["Due to the huge category number, the sophisticated combinations of various strokes and radicals, and the free writing or printing styles, generating Chinese characters with diverse styles is always considered as a difficult task.","In this paper, an efficient and generalized deep framework, namely, the W-Net, is introduced for the one-shot arbitrary-style Chinese character generation task.","Specifically, given a single character (one-shot) with a specific style (e.g., a printed font or hand-writing style), the proposed W-Net model is capable of learning and generating any arbitrary characters sharing the style similar to the given single character.","Such appealing property was rarely seen in the literature.","We have compared the proposed W-Net framework to many other competitive methods.","Experimental results showed the proposed method is significantly superior in the one-shot setting."],"url":"http://arxiv.org/abs/2406.06122v1","category":"cs.CV"}
{"created":"2024-06-10 09:00:59","title":"The rational rank of the support of generalized power series solutions of differential and $q$-difference equations","abstract":"Given a differential or $q$-difference equation $P$ of order $n$, we prove that the set of exponents of a generalized power series solution has its rational rank bounded by the rational rank of the support of $P$ plus $n$. We also prove that when the support of the solution has maximum rational rank, it is convergent. Using the Newton polygon technique, we show also that in the maximum rational rank case, an initial segment can always be completed to a true solution. The techniques are the same for the differential and the $q$-difference case.","sentences":["Given a differential or $q$-difference equation $P$ of order $n$, we prove that the set of exponents of a generalized power series solution has its rational rank bounded by the rational rank of the support of $P$ plus $n$. We also prove that when the support of the solution has maximum rational rank, it is convergent.","Using the Newton polygon technique, we show also that in the maximum rational rank case, an initial segment can always be completed to a true solution.","The techniques are the same for the differential and the $q$-difference case."],"url":"http://arxiv.org/abs/2406.06115v1","category":"math.CA"}
{"created":"2024-06-10 08:51:04","title":"JenGAN: Stacked Shifted Filters in GAN-Based Speech Synthesis","abstract":"Non-autoregressive GAN-based neural vocoders are widely used due to their fast inference speed and high perceptual quality. However, they often suffer from audible artifacts such as tonal artifacts in their generated results. Therefore, we propose JenGAN, a new training strategy that involves stacking shifted low-pass filters to ensure the shift-equivariant property. This method helps prevent aliasing and reduce artifacts while preserving the model structure used during inference. In our experimental evaluation, JenGAN consistently enhances the performance of vocoder models, yielding significantly superior scores across the majority of evaluation metrics.","sentences":["Non-autoregressive GAN-based neural vocoders are widely used due to their fast inference speed and high perceptual quality.","However, they often suffer from audible artifacts such as tonal artifacts in their generated results.","Therefore, we propose JenGAN, a new training strategy that involves stacking shifted low-pass filters to ensure the shift-equivariant property.","This method helps prevent aliasing and reduce artifacts while preserving the model structure used during inference.","In our experimental evaluation, JenGAN consistently enhances the performance of vocoder models, yielding significantly superior scores across the majority of evaluation metrics."],"url":"http://arxiv.org/abs/2406.06111v1","category":"eess.AS"}
{"created":"2024-06-10 08:50:59","title":"Recurrent Context Compression: Efficiently Expanding the Context Window of LLM","abstract":"To extend the context length of Transformer-based large language models (LLMs) and improve comprehension capabilities, we often face limitations due to computational resources and bounded memory storage capacity. This work introduces a method called Recurrent Context Compression (RCC), designed to efficiently expand the context window length of LLMs within constrained storage space. We also investigate the issue of poor model responses when both instructions and context are compressed in downstream tasks, and propose an instruction reconstruction method to mitigate this problem. We validated the effectiveness of our approach on multiple tasks, achieving a compression rate of up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and nearly 100\\% accuracy on a passkey retrieval task with a sequence length of 1M. Finally, our method demonstrated competitive performance in long-text question-answering tasks compared to non-compressed methods, while significantly saving storage resources in long-text inference tasks. Our code, models, and demo are available at https://github.com/WUHU-G/RCC_Transformer","sentences":["To extend the context length of Transformer-based large language models (LLMs) and improve comprehension capabilities, we often face limitations due to computational resources and bounded memory storage capacity.","This work introduces a method called Recurrent Context Compression (RCC), designed to efficiently expand the context window length of LLMs within constrained storage space.","We also investigate the issue of poor model responses when both instructions and context are compressed in downstream tasks, and propose an instruction reconstruction method to mitigate this problem.","We validated the effectiveness of our approach on multiple tasks, achieving a compression rate of up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and nearly 100\\% accuracy on a passkey retrieval task with a sequence length of 1M. Finally, our method demonstrated competitive performance in long-text question-answering tasks compared to non-compressed methods, while significantly saving storage resources in long-text inference tasks.","Our code, models, and demo are available at https://github.com/WUHU-G/RCC_Transformer"],"url":"http://arxiv.org/abs/2406.06110v1","category":"cs.CL"}
{"created":"2024-06-10 08:50:50","title":"Multi-Generational Black Hole Population Analysis with an Astrophysically Informed Mass Function","abstract":"We analyze the population statistics of black holes in the LIGO/Virgo/KAGRA GWTC-3 catalog using a parametric mass function derived from simulations of massive stars experiencing pulsational pair-instability supernovae (PPISN). Our formalism enables us to separate the black hole mass function into sub-populations corresponding to mergers between objects formed via different astrophysical pathways, allowing us to infer the properties of black holes formed from stellar collapse and black holes formed via prior mergers separately. Applying this formalism, we find that this model fits the data better than the powerlaw+peak model with Bayes factor $ 9.7\\pm0.1$. We measure the location of the lower edge of the upper black hole mass gap to be $M_{\\rm BHMG}=84.05_{-12.88}^{+17.19}{\\rm M}_{\\odot}$, providing evidence that the $35{\\rm M}_{\\odot}$ Gaussian peak detected in the data using other models is not associated with the PPISN pile-up predicted to precede this gap. Incorporating spin, we find that the normalized spins of stellar remnant black holes are close to zero while those of higher generation black holes tend to larger values. All of these results are in accordance with the predictions of stellar structure theory and black hole merger scenarios. Finally, we combine our mass function with the spectral siren method for measuring the Hubble constant to find $H_0=36.19_{-10.91}^{17.50}$ km/s/Mpc and discuss potential explanations of this low value. Our results demonstrate how astrophysically-informed mass functions can facilitate the interpretation of gravitational wave catalog data to provide information about black hole formation and cosmology. Future data releases will improve the precision of our measurements.","sentences":["We analyze the population statistics of black holes in the LIGO/Virgo/KAGRA GWTC-3 catalog using a parametric mass function derived from simulations of massive stars experiencing pulsational pair-instability supernovae (PPISN).","Our formalism enables us to separate the black hole mass function into sub-populations corresponding to mergers between objects formed via different astrophysical pathways, allowing us to infer the properties of black holes formed from stellar collapse and black holes formed via prior mergers separately.","Applying this formalism, we find that this model fits the data better than the powerlaw+peak model with Bayes factor $ 9.7\\pm0.1$. We measure the location of the lower edge of the upper black hole mass gap to be $M_{\\rm BHMG}=84.05_{-12.88}^{+17.19}{\\rm M}_{\\odot}$, providing evidence that the $35{\\rm M}_{\\odot}$ Gaussian peak detected in the data using other models is not associated with the PPISN pile-up predicted to precede this gap.","Incorporating spin, we find that the normalized spins of stellar remnant black holes are close to zero while those of higher generation black holes tend to larger values.","All of these results are in accordance with the predictions of stellar structure theory and black hole merger scenarios.","Finally, we combine our mass function with the spectral siren method for measuring the Hubble constant to find $H_0=36.19_{-10.91}^{17.50}$ km/s/Mpc and discuss potential explanations of this low value.","Our results demonstrate how astrophysically-informed mass functions can facilitate the interpretation of gravitational wave catalog data to provide information about black hole formation and cosmology.","Future data releases will improve the precision of our measurements."],"url":"http://arxiv.org/abs/2406.06109v1","category":"astro-ph.HE"}
{"created":"2024-06-10 08:46:49","title":"EXPIL: Explanatory Predicate Invention for Learning in Games","abstract":"Reinforcement learning (RL) has proven to be a powerful tool for training agents that excel in various games. However, the black-box nature of neural network models often hinders our ability to understand the reasoning behind the agent's actions. Recent research has attempted to address this issue by using the guidance of pretrained neural agents to encode logic-based policies, allowing for interpretable decisions. A drawback of such approaches is the requirement of large amounts of predefined background knowledge in the form of predicates, limiting its applicability and scalability. In this work, we propose a novel approach, Explanatory Predicate Invention for Learning in Games (EXPIL), that identifies and extracts predicates from a pretrained neural agent, later used in the logic-based agents, reducing the dependency on predefined background knowledge. Our experimental evaluation on various games demonstrate the effectiveness of EXPIL in achieving explainable behavior in logic agents while requiring less background knowledge.","sentences":["Reinforcement learning (RL) has proven to be a powerful tool for training agents that excel in various games.","However, the black-box nature of neural network models often hinders our ability to understand the reasoning behind the agent's actions.","Recent research has attempted to address this issue by using the guidance of pretrained neural agents to encode logic-based policies, allowing for interpretable decisions.","A drawback of such approaches is the requirement of large amounts of predefined background knowledge in the form of predicates, limiting its applicability and scalability.","In this work, we propose a novel approach, Explanatory Predicate Invention for Learning in Games (EXPIL), that identifies and extracts predicates from a pretrained neural agent, later used in the logic-based agents, reducing the dependency on predefined background knowledge.","Our experimental evaluation on various games demonstrate the effectiveness of EXPIL in achieving explainable behavior in logic agents while requiring less background knowledge."],"url":"http://arxiv.org/abs/2406.06107v1","category":"cs.AI"}
{"created":"2024-06-10 08:42:48","title":"Testably Learning Polynomial Threshold Functions","abstract":"Rubinfeld & Vasilyan recently introduced the framework of testable learning as an extension of the classical agnostic model. It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a tester. The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts. We focus on the setting where the tester has to accept standard Gaussian data. There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model. In this work, we ask whether there is a price to pay for testably learning more complex concept classes. In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces. We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\\varepsilon > 0$ in time $n^{\\mathrm{poly}(1/\\varepsilon)}$. This qualitatively matches the best known guarantees in the agnostic model. Our results build on a connection between testable learning and fooling. In particular, we show that distributions that approximately match at least $\\mathrm{poly}(1/\\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\\varepsilon$). As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs.","sentences":["Rubinfeld & Vasilyan recently introduced the framework of testable learning as an extension of the classical agnostic model.","It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a tester.","The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts.","We focus on the setting where the tester has to accept standard Gaussian data.","There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model.","In this work, we ask whether there is a price to pay for testably learning more complex concept classes.","In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces.","We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\\varepsilon > 0$ in time $n^{\\mathrm{poly}(1/\\varepsilon)}$. This qualitatively matches the best known guarantees in the agnostic model.","Our results build on a connection between testable learning and fooling.","In particular, we show that distributions that approximately match at least $\\mathrm{poly}(1/\\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\\varepsilon$).","As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs."],"url":"http://arxiv.org/abs/2406.06106v1","category":"cs.LG"}
{"created":"2024-06-10 08:40:11","title":"The Evolution of Applications, Hardware Design, and Channel Modeling for Terahertz (THz) Band Communications and Sensing: Ready for 6G?","abstract":"For decades, the terahertz (THz) frequency band had been primarily explored in the context of radar, imaging, and spectroscopy, where multi-gigahertz (GHz) and even THz-wide channels and the properties of terahertz photons offered attractive target accuracy, resolution, and classification capabilities. Meanwhile, the exploitation of the terahertz band for wireless communication had originally been limited due to several reasons, including (i) no immediate need for such high data rates available via terahertz bands and (ii) challenges in designing sufficiently high power terahertz systems at reasonable cost and efficiency, leading to what was often referred to as \"the terahertz gap\". This roadmap paper first reviews the evolution of the hardware design approaches for terahertz systems, including electronic, photonic, and plasmonic approaches, and the understanding of the terahertz channel itself, in diverse scenarios, ranging from common indoors and outdoors scenarios to intra-body and outer-space environments. The article then summarizes the lessons learned during this multi-decade process and the cutting-edge state-of-the-art findings, including novel methods to quantify power efficiency, which will become more important in making design choices. Finally, the manuscript presents the authors' perspective and insights on how the evolution of terahertz systems design will continue toward enabling efficient terahertz communications and sensing solutions as an integral part of next-generation wireless systems.","sentences":["For decades, the terahertz (THz) frequency band had been primarily explored in the context of radar, imaging, and spectroscopy, where multi-gigahertz (GHz) and even THz-wide channels and the properties of terahertz photons offered attractive target accuracy, resolution, and classification capabilities.","Meanwhile, the exploitation of the terahertz band for wireless communication had originally been limited due to several reasons, including (i) no immediate need for such high data rates available via terahertz bands and (ii) challenges in designing sufficiently high power terahertz systems at reasonable cost and efficiency, leading to what was often referred to as \"the terahertz gap\".","This roadmap paper first reviews the evolution of the hardware design approaches for terahertz systems, including electronic, photonic, and plasmonic approaches, and the understanding of the terahertz channel itself, in diverse scenarios, ranging from common indoors and outdoors scenarios to intra-body and outer-space environments.","The article then summarizes the lessons learned during this multi-decade process and the cutting-edge state-of-the-art findings, including novel methods to quantify power efficiency, which will become more important in making design choices.","Finally, the manuscript presents the authors' perspective and insights on how the evolution of terahertz systems design will continue toward enabling efficient terahertz communications and sensing solutions as an integral part of next-generation wireless systems."],"url":"http://arxiv.org/abs/2406.06105v1","category":"cs.NI"}
{"created":"2024-06-10 08:36:55","title":"Adaptive Control in Assistive Application -- A Study Evaluating Shared Control by Users with Limited Upper Limb Mobility","abstract":"Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments. This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments. It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting. Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data. The results reveal consistently high success rates in trial completions, irrespective of the input device used. Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system. Notably, each participant effectively managed the given task within a short time frame.","sentences":["Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments.","This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments.","It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting.","Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data.","The results reveal consistently high success rates in trial completions, irrespective of the input device used.","Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system.","Notably, each participant effectively managed the given task within a short time frame."],"url":"http://arxiv.org/abs/2406.06103v1","category":"cs.HC"}
{"created":"2024-06-10 08:35:01","title":"On the Consistency of Kernel Methods with Dependent Observations","abstract":"The consistency of a learning method is usually established under the assumption that the observations are a realization of an independent and identically distributed (i.i.d.) or mixing process. Yet, kernel methods such as support vector machines (SVMs), Gaussian processes, or conditional kernel mean embeddings (CKMEs) all give excellent performance under sampling schemes that are obviously non-i.i.d., such as when data comes from a dynamical system. We propose the new notion of empirical weak convergence (EWC) as a general assumption explaining such phenomena for kernel methods. It assumes the existence of a random asymptotic data distribution and is a strict weakening of previous assumptions in the field. Our main results then establish consistency of SVMs, kernel mean embeddings, and general Hilbert-space valued empirical expectations with EWC data. Our analysis holds for both finite- and infinite-dimensional outputs, as we extend classical results of statistical learning to the latter case. In particular, it is also applicable to CKMEs. Overall, our results open new classes of processes to statistical learning and can serve as a foundation for a theory of learning beyond i.i.d. and mixing.","sentences":["The consistency of a learning method is usually established under the assumption that the observations are a realization of an independent and identically distributed (i.i.d.) or mixing process.","Yet, kernel methods such as support vector machines (SVMs), Gaussian processes, or conditional kernel mean embeddings (CKMEs) all give excellent performance under sampling schemes that are obviously non-i.i.d., such as when data comes from a dynamical system.","We propose the new notion of empirical weak convergence (EWC) as a general assumption explaining such phenomena for kernel methods.","It assumes the existence of a random asymptotic data distribution and is a strict weakening of previous assumptions in the field.","Our main results then establish consistency of SVMs, kernel mean embeddings, and general Hilbert-space valued empirical expectations with EWC data.","Our analysis holds for both finite- and infinite-dimensional outputs, as we extend classical results of statistical learning to the latter case.","In particular, it is also applicable to CKMEs.","Overall, our results open new classes of processes to statistical learning and can serve as a foundation for a theory of learning beyond i.i.d. and mixing."],"url":"http://arxiv.org/abs/2406.06101v1","category":"cs.LG"}
{"created":"2024-06-10 08:34:13","title":"Sequential Binary Classification for Intrusion Detection in Software Defined Networks","abstract":"Software-Defined Networks (SDN) are the standard architecture for network deployment. Intrusion Detection Systems (IDS) are a pivotal part of this technology as networks become more vulnerable to new and sophisticated attacks. Machine Learning (ML)-based IDS are increasingly seen as the most effective approach to handle this issue. However, IDS datasets suffer from high class imbalance, which impacts the performance of standard ML models. We propose Sequential Binary Classification (SBC) - an algorithm for multi-class classification to address this issue. SBC is a hierarchical cascade of base classifiers, each of which can be modelled on any general binary classifier. Extensive experiments are reported on benchmark datasets that evaluate the performance of SBC under different scenarios.","sentences":["Software-Defined Networks (SDN) are the standard architecture for network deployment.","Intrusion Detection Systems (IDS) are a pivotal part of this technology as networks become more vulnerable to new and sophisticated attacks.","Machine Learning (ML)-based IDS are increasingly seen as the most effective approach to handle this issue.","However, IDS datasets suffer from high class imbalance, which impacts the performance of standard ML models.","We propose Sequential Binary Classification (SBC) - an algorithm for multi-class classification to address this issue.","SBC is a hierarchical cascade of base classifiers, each of which can be modelled on any general binary classifier.","Extensive experiments are reported on benchmark datasets that evaluate the performance of SBC under different scenarios."],"url":"http://arxiv.org/abs/2406.06099v1","category":"cs.CR"}
{"created":"2024-06-10 08:27:58","title":"StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection","abstract":"Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.","sentences":["Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream.","Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams.","This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints.","Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST.","To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST.","Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research."],"url":"http://arxiv.org/abs/2406.06097v1","category":"cs.SD"}
{"created":"2024-06-10 08:23:49","title":"Weights on homogeneous coherent configurations","abstract":"D. G. Higman generalized a coherent configuration and defined a weight. In this article, we will modify the definition and investigate weights on coherent configurations. If our weights are on a thin homogeneous coherent configuration, that is essentially a finite group, then there is a natural correspondence between the set of equivalence classes of weights and $2$-cohomology group of the group. We also give a construction of weights as a generalization of Higman's method using monomial representations of finite groups.","sentences":["D. G. Higman generalized a coherent configuration and defined a weight.","In this article, we will modify the definition and investigate weights on coherent configurations.","If our weights are on a thin homogeneous coherent configuration, that is essentially a finite group, then there is a natural correspondence between the set of equivalence classes of weights and $2$-cohomology group of the group.","We also give a construction of weights as a generalization of Higman's method using monomial representations of finite groups."],"url":"http://arxiv.org/abs/2406.06093v1","category":"math.CO"}
{"created":"2024-06-10 08:20:24","title":"Cosmological coupling of local gravitational systems","abstract":"We investigate the cosmological coupling of spherical, local astrophysical systems. We derive a general formula quantifying the cosmological coupling of the Misner-Sharp mass of these objects. We show that, in the weak-field limit, the cosmological coupling is only allowed if there are pressure anisotropies. We also apply our results to galaxies, modelling them with the Navarro-Frenk-White and Einasto profiles. We show that the galactic mass can be coupled to the cosmological dynamics and examine its dependence on the scale factor of the universe.","sentences":["We investigate the cosmological coupling of spherical, local astrophysical systems.","We derive a general formula quantifying the cosmological coupling of the Misner-Sharp mass of these objects.","We show that, in the weak-field limit, the cosmological coupling is only allowed if there are pressure anisotropies.","We also apply our results to galaxies, modelling them with the Navarro-Frenk-White and Einasto profiles.","We show that the galactic mass can be coupled to the cosmological dynamics and examine its dependence on the scale factor of the universe."],"url":"http://arxiv.org/abs/2406.06091v1","category":"gr-qc"}
{"created":"2024-06-10 08:18:55","title":"Texture Re-scalable Universal Adversarial Perturbation","abstract":"Universal adversarial perturbation (UAP), also known as image-agnostic perturbation, is a fixed perturbation map that can fool the classifier with high probabilities on arbitrary images, making it more practical for attacking deep models in the real world. Previous UAP methods generate a scale-fixed and texture-fixed perturbation map for all images, which ignores the multi-scale objects in images and usually results in a low fooling ratio. Since the widely used convolution neural networks tend to classify objects according to semantic information stored in local textures, it seems a reasonable and intuitive way to improve the UAP from the perspective of utilizing local contents effectively. In this work, we find that the fooling ratios significantly increase when we add a constraint to encourage a small-scale UAP map and repeat it vertically and horizontally to fill the whole image domain. To this end, we propose texture scale-constrained UAP (TSC-UAP), a simple yet effective UAP enhancement method that automatically generates UAPs with category-specific local textures that can fool deep models more easily. Through a low-cost operation that restricts the texture scale, TSC-UAP achieves a considerable improvement in the fooling ratio and attack transferability for both data-dependent and data-free UAP methods. Experiments conducted on two state-of-the-art UAP methods, eight popular CNN models and four classical datasets show the remarkable performance of TSC-UAP.","sentences":["Universal adversarial perturbation (UAP), also known as image-agnostic perturbation, is a fixed perturbation map that can fool the classifier with high probabilities on arbitrary images, making it more practical for attacking deep models in the real world.","Previous UAP methods generate a scale-fixed and texture-fixed perturbation map for all images, which ignores the multi-scale objects in images and usually results in a low fooling ratio.","Since the widely used convolution neural networks tend to classify objects according to semantic information stored in local textures, it seems a reasonable and intuitive way to improve the UAP from the perspective of utilizing local contents effectively.","In this work, we find that the fooling ratios significantly increase when we add a constraint to encourage a small-scale UAP map and repeat it vertically and horizontally to fill the whole image domain.","To this end, we propose texture scale-constrained UAP (TSC-UAP), a simple yet effective UAP enhancement method that automatically generates UAPs with category-specific local textures that can fool deep models more easily.","Through a low-cost operation that restricts the texture scale, TSC-UAP achieves a considerable improvement in the fooling ratio and attack transferability for both data-dependent and data-free UAP methods.","Experiments conducted on two state-of-the-art UAP methods, eight popular CNN models and four classical datasets show the remarkable performance of TSC-UAP."],"url":"http://arxiv.org/abs/2406.06089v1","category":"cs.CV"}
{"created":"2024-06-10 08:18:07","title":"GAIA: Rethinking Action Quality Assessment for AI-Generated Videos","abstract":"Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods correlate poorly with human opinions, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.","sentences":["Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV).","Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs.","To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs.","Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions.","We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods.","Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods correlate poorly with human opinions, indicating a sizable gap between current models and human action perception patterns in AIGVs.","Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs."],"url":"http://arxiv.org/abs/2406.06087v1","category":"cs.CV"}
{"created":"2024-06-10 08:06:21","title":"Towards NNPDFpol2.0","abstract":"We review the recent efforts in the NNPDF Collaboration towards a new global extraction of polarized parton distributions functions (pPDF). Polarized PDFs are highly relevant for the interpretation of current and future polarized high-energy experiments, including the upcoming Electron-Ion Collider (EIC). We present a recent study of the role played by heavy quark effects in polarized DIS, where we apply the FONLL general-mass variable-flavour-number scheme for the first time in a polarized setup and demonstrate the significant impact of charm mass corrections, specifically for the polarized gluon distribution. We show preliminary results (DIS-only) of this new pPDF release, NNPDFpol2.0, based on the NNPDF4.0 fitting machinery and the associated new theory prediction pipeline.","sentences":["We review the recent efforts in the NNPDF Collaboration towards a new global extraction of polarized parton distributions functions (pPDF).","Polarized PDFs are highly relevant for the interpretation of current and future polarized high-energy experiments, including the upcoming Electron-Ion Collider (EIC).","We present a recent study of the role played by heavy quark effects in polarized DIS, where we apply the FONLL general-mass variable-flavour-number scheme for the first time in a polarized setup and demonstrate the significant impact of charm mass corrections, specifically for the polarized gluon distribution.","We show preliminary results (DIS-only) of this new pPDF release, NNPDFpol2.0, based on the NNPDF4.0 fitting machinery and the associated new theory prediction pipeline."],"url":"http://arxiv.org/abs/2406.06083v1","category":"hep-ph"}
{"created":"2024-06-10 16:39:21","title":"How is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-physiological Signals","abstract":"Vertical take-off and landing (VTOL) aircraft do not require a prolonged runway, thus allowing them to land almost anywhere. In recent years, their flexibility has made them popular in development, research, and operation. When compared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique challenges as they combine many maneuvers from both types of aircraft. Pilot workload is a critical factor for safe and efficient operation of VTOLs. In this work, we conduct a user study to collect multimodal data from 28 pilots while they perform a variety of VTOL flight tasks. We analyze and interpolate behavioral patterns related to their performance and perceived workload. Finally, we build machine learning models to estimate their workload from the collected data. Our results are promising, suggesting that quantitative and accurate VTOL pilot workload monitoring is viable. Such assistive tools would help the research field understand VTOL operations and serve as a stepping stone for the industry to ensure VTOL safe operations and further remote operations.","sentences":["Vertical take-off and landing (VTOL) aircraft do not require a prolonged runway, thus allowing them to land almost anywhere.","In recent years, their flexibility has made them popular in development, research, and operation.","When compared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique challenges as they combine many maneuvers from both types of aircraft.","Pilot workload is a critical factor for safe and efficient operation of VTOLs.","In this work, we conduct a user study to collect multimodal data from 28 pilots while they perform a variety of VTOL flight tasks.","We analyze and interpolate behavioral patterns related to their performance and perceived workload.","Finally, we build machine learning models to estimate their workload from the collected data.","Our results are promising, suggesting that quantitative and accurate VTOL pilot workload monitoring is viable.","Such assistive tools would help the research field understand VTOL operations and serve as a stepping stone for the industry to ensure VTOL safe operations and further remote operations."],"url":"http://arxiv.org/abs/2406.06448v1","category":"cs.HC"}
{"created":"2024-06-10 16:12:32","title":"An Improved Empirical Fisher Approximation for Natural Gradient Descent","abstract":"Approximate Natural Gradient Descent (NGD) methods are an important family of optimisers for deep learning models, which use approximate Fisher information matrices to pre-condition gradients during training. The empirical Fisher (EF) method approximates the Fisher information matrix empirically by reusing the per-sample gradients collected during back-propagation. Despite its ease of implementation, the EF approximation has its theoretical and practical limitations. This paper first investigates the inversely-scaled projection issue of EF, which is shown to be a major cause of the poor empirical approximation quality. An improved empirical Fisher (iEF) method, motivated as a generalised NGD method from a loss reduction perspective, is proposed to address this issue, meanwhile retaining the practical convenience of EF. The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100). Optimisation experiments show that applying exact iEF as an optimiser provides strong convergence and generalisation. It achieves the best test performance and the lowest training loss for majority of the tasks, even when compared with well-tuned AdamW/Adafactor baselines. Additionally, under a novel empirical evaluation framework, the proposed iEF method shows consistently better approximation quality to the exact Natural Gradient updates than both EF and the more expensive sampled Fisher (SF). Further investigation also shows that the superior approximation quality of iEF is robust to damping across tasks and training stages. Improving existing approximate NGD optimisers with iEF is expected to lead to better convergence ability and stronger robustness to choice of damping.","sentences":["Approximate Natural Gradient Descent (NGD) methods are an important family of optimisers for deep learning models, which use approximate Fisher information matrices to pre-condition gradients during training.","The empirical Fisher (EF) method approximates the Fisher information matrix empirically by reusing the per-sample gradients collected during back-propagation.","Despite its ease of implementation, the EF approximation has its theoretical and practical limitations.","This paper first investigates the inversely-scaled projection issue of EF, which is shown to be a major cause of the poor empirical approximation quality.","An improved empirical Fisher (iEF) method, motivated as a generalised NGD method from a loss reduction perspective, is proposed to address this issue, meanwhile retaining the practical convenience of EF.","The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100).","Optimisation experiments show that applying exact iEF as an optimiser provides strong convergence and generalisation.","It achieves the best test performance and the lowest training loss for majority of the tasks, even when compared with well-tuned AdamW/Adafactor baselines.","Additionally, under a novel empirical evaluation framework, the proposed iEF method shows consistently better approximation quality to the exact Natural Gradient updates than both EF and the more expensive sampled Fisher (SF).","Further investigation also shows that the superior approximation quality of iEF is robust to damping across tasks and training stages.","Improving existing approximate NGD optimisers with iEF is expected to lead to better convergence ability and stronger robustness to choice of damping."],"url":"http://arxiv.org/abs/2406.06420v1","category":"cs.LG"}
{"created":"2024-06-10 15:57:33","title":"A LoRa-based Energy-efficient Sensing System for Urban Data Collection","abstract":"Nowadays, cities provide much more than shopping opportunities or working spaces. Individual locations such as parks and squares are used as meeting points and local recreation areas by many people. To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public. These utilization trends can be derived using public data collection. The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis. Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy. This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating. To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN). The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months. By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run. The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution. All data collected during the field trial is publicly available as open data.","sentences":["Nowadays, cities provide much more than shopping opportunities or working spaces.","Individual locations such as parks and squares are used as meeting points and local recreation areas by many people.","To ensure that they remain attractive in the future, the design of such squares must be regularly adapted to the needs of the public.","These utilization trends can be derived using public data collection.","The more diverse and rich the data sets are, the easier it is to optimize public space design through data analysis.","Traditional data collection methods such as questionnaires, observations, or videos are either labor intensive or cannot guarantee to preserve the individual's privacy.","This work presents a privacy-preserving, low-power, and low-cost smart sensing system that is capable of anonymously collecting data about public space utilization by analyzing the occupancy distribution of public seating.","To support future urban planning the sensor nodes are capable of monitoring environmental noise, chair utilization, and their position, temperature, and humidity and provide them over a city-wide Long Range Wide Area Network (LoRaWAN).","The final sensing system's robust operation is proven in a trial run at two public squares in a city with 16 sensor nodes over a duration of two months.","By consuming 33.65 mWh per day with all subsystems enabled, including sitting detection based on a continuous acceleration measurement operating on a robust and simple threshold algorithm, the custom-designed sensor node achieves continuous monitoring during the 2-month trial run.","The evaluation of the experimental results clearly shows how the two locations are used, which confirms the practicability of the proposed solution.","All data collected during the field trial is publicly available as open data."],"url":"http://arxiv.org/abs/2406.06404v1","category":"eess.SY"}
{"created":"2024-06-10 14:32:06","title":"Topological transition in cyanobacteria: from motion to structure","abstract":"Many active systems are capable of forming intriguing patterns at scales significantly larger than the size of their individual constituents. We recently introduced a model for gliding filamentous cyanobacteria, which form a reticulate pattern as an early-stage biofilm. Our observations reveal that the filaments' bodies follow the track of the head, and are subject to curvature fluctuations, defining a new class of active matter: active spaghetti. By identifying the dominant aspects of the filament's dynamics, our model achieves great computational efficiency, without coarse-graining past the scale of an individual filament. Here, we explore large-scale collective effects and rich dynamics of cyanobacteria colonies, while still retaining information about the individual constituents' dynamics and their interactions. We characterize the system's topological transition from an isotropic distribution to a state of large-scale reticulate patterns by quantifying both dynamical and structural observables. Although it is not a periodic structure, in the steady state the reticulate pattern possesses a well-defined length scale determined by the filaments' Peclet number, which controls the relative importance of activity and curvature fluctuations.","sentences":["Many active systems are capable of forming intriguing patterns at scales significantly larger than the size of their individual constituents.","We recently introduced a model for gliding filamentous cyanobacteria, which form a reticulate pattern as an early-stage biofilm.","Our observations reveal that the filaments' bodies follow the track of the head, and are subject to curvature fluctuations, defining a new class of active matter: active spaghetti.","By identifying the dominant aspects of the filament's dynamics, our model achieves great computational efficiency, without coarse-graining past the scale of an individual filament.","Here, we explore large-scale collective effects and rich dynamics of cyanobacteria colonies, while still retaining information about the individual constituents' dynamics and their interactions.","We characterize the system's topological transition from an isotropic distribution to a state of large-scale reticulate patterns by quantifying both dynamical and structural observables.","Although it is not a periodic structure, in the steady state the reticulate pattern possesses a well-defined length scale determined by the filaments' Peclet number, which controls the relative importance of activity and curvature fluctuations."],"url":"http://arxiv.org/abs/2406.06314v1","category":"cond-mat.soft"}
{"created":"2024-06-10 14:18:16","title":"Quantum Geometric Tensor and Critical Metrology in the Anisotropic Dicke Model","abstract":"We investigate the quantum phase transition in the anisotropic Dicke model through an examination of the quantum geometric tensor of the ground state. In this analysis, two distinct classical limits exhibit their unique anisotropic characteristics. The classical spin limit demonstrates a preference for the rotating-wave coupling, whereas the classical oscillator limit exhibits symmetry in the coupling strength of the bias. The anisotropic features of the classical spin limit persist at finite scales. Furthermore, we observe that the interplay among the anisotropic ratio, spin length, and frequency ratio can collectively enhance the critical behaviors. This critical enhancement without trade-off between these factors provides a flexible method for quantum precision measurement.","sentences":["We investigate the quantum phase transition in the anisotropic Dicke model through an examination of the quantum geometric tensor of the ground state.","In this analysis, two distinct classical limits exhibit their unique anisotropic characteristics.","The classical spin limit demonstrates a preference for the rotating-wave coupling, whereas the classical oscillator limit exhibits symmetry in the coupling strength of the bias.","The anisotropic features of the classical spin limit persist at finite scales.","Furthermore, we observe that the interplay among the anisotropic ratio, spin length, and frequency ratio can collectively enhance the critical behaviors.","This critical enhancement without trade-off between these factors provides a flexible method for quantum precision measurement."],"url":"http://arxiv.org/abs/2406.06301v1","category":"quant-ph"}
{"created":"2024-06-10 14:17:34","title":"Performance Test Methodology for Atmosphere-Breathing Electric Propulsion Intakes in an Atomic Oxygen Facility","abstract":"The testing of atmosphere-breathing electric propulsion intakes is an important step in the development of functional propulsion systems which provide sustained drag compensation in very low Earth orbits. To make satellite operations more sustainable, it is necessary to develop new materials which withstand erosion, long-lasting propulsion systems to overcome drag, and tools that allow for ground-based testing. Among the tools to enable these innovations is the Rarefied Orbital Aerodynamics Research facility at the University of Manchester. Here, a description of the facility is provided together with two different methodologies for testing sub-scaled intake designs for atmosphere-breathing electric propulsion systems. The first methodology is based on measurements of the pressure difference between the two extremities of the intake, while the second uses a gas sensor to measure the collection efficiency of the intake. Direct Simulation Monte Carlo models have been used to assess the viability of the proposed testing methodologies. The results of this analysis indicate that either methodology or a combination of both can provide suitable measurements to assess the performance of future intake designs.","sentences":["The testing of atmosphere-breathing electric propulsion intakes is an important step in the development of functional propulsion systems which provide sustained drag compensation in very low Earth orbits.","To make satellite operations more sustainable, it is necessary to develop new materials which withstand erosion, long-lasting propulsion systems to overcome drag, and tools that allow for ground-based testing.","Among the tools to enable these innovations is the Rarefied Orbital Aerodynamics Research facility at the University of Manchester.","Here, a description of the facility is provided together with two different methodologies for testing sub-scaled intake designs for atmosphere-breathing electric propulsion systems.","The first methodology is based on measurements of the pressure difference between the two extremities of the intake, while the second uses a gas sensor to measure the collection efficiency of the intake.","Direct Simulation Monte Carlo models have been used to assess the viability of the proposed testing methodologies.","The results of this analysis indicate that either methodology or a combination of both can provide suitable measurements to assess the performance of future intake designs."],"url":"http://arxiv.org/abs/2406.06299v1","category":"physics.space-ph"}
{"created":"2024-06-10 13:56:39","title":"Measurement of the branching fractions of $\\bar{B}\\to D^{(*)} K^- K^{(*)0}_{(S)}$ and $\\bar{B}\\to D^{(*)}D_s^{-}$ decays at Belle II","abstract":"We present measurements of the branching fractions of eight $\\overline B{}^0\\to D^{(*)+} K^- K^{(*)0}_{(S)}$, $B^{-}\\to D^{(*)0} K^- K^{(*)0}_{(S)}$ decay channels. The results are based on data from SuperKEKB electron-positron collisions at the $\\Upsilon(4S)$ resonance collected with the Belle II detector, corresponding to an integrated luminosity of $362~\\text{fb}^{-1}$. The event yields are extracted from fits to the distributions of the difference between expected and observed $B$ meson energy, and are efficiency-corrected as a function of $m(K^-K^{(*)0}_{(S)})$ and $m(D^{(*)}K^{(*)0}_{(S)})$ in order to avoid dependence on the decay model. These results include the first observation of $\\overline B{}^0\\to D^+K^-K_S^0$, $B^-\\to D^{*0}K^-K_S^0$, and $\\overline B{}^0\\to D^{*+}K^-K_S^0$ decays and a significant improvement in the precision of the other channels compared to previous measurements. The helicity-angle distributions and the invariant mass distributions of the $K^- K^{(*)0}_{(S)}$ systems are compatible with quasi-two-body decays via a resonant transition with spin-parity $J^P=1^-$ for the $K^-K_S^0$ systems and $J^P= 1^+$ for the $K^-K^{*0}$ systems. We also present measurements of the branching fractions of four $\\overline B{}^0\\to D^{(*)+} D_s^-$, $B^{-}\\to D^{(*)0} D_s^- $ decay channels with a precision compatible to the current world averages.","sentences":["We present measurements of the branching fractions of eight $\\overline B{}^0\\to D^{(*)+} K^- K^{(*)0}_{(S)}$, $B^{-}\\to D^{(*)0} K^- K^{(*)0}_{(S)}$ decay channels.","The results are based on data from SuperKEKB electron-positron collisions at the $\\Upsilon(4S)$ resonance collected with the Belle II detector, corresponding to an integrated luminosity of $362~\\text{fb}^{-1}$. The event yields are extracted from fits to the distributions of the difference between expected and observed $B$ meson energy, and are efficiency-corrected as a function of $m(K^-K^{(*)0}_{(S)})$ and $m(D^{(*)}K^{(*)0}_{(S)})$ in order to avoid dependence on the decay model.","These results include the first observation of $\\overline B{}^0\\to D^+K^-K_S^0$, $B^-\\to D^{*0}K^-K_S^0$, and $\\overline B{}^0\\to D^{*+}K^-K_S^0$ decays and a significant improvement in the precision of the other channels compared to previous measurements.","The helicity-angle distributions and the invariant mass distributions of the $K^- K^{(*)0}_{(S)}$ systems are compatible with quasi-two-body decays via a resonant transition with spin-parity $J^P=1^-$ for the $K^-K_S^0$ systems and $J^P= 1^+$ for the $K^-K^{*0}$ systems.","We also present measurements of the branching fractions of four $\\overline B{}^0\\to D^{(*)+} D_s^-$, $B^{-}\\to D^{(*)0} D_s^- $ decay channels with a precision compatible to the current world averages."],"url":"http://arxiv.org/abs/2406.06277v1","category":"hep-ex"}
{"created":"2024-06-10 13:34:23","title":"Understanding Students' Acceptance of ChatGPT as a Translation Tool: A UTAUT Model Analysis","abstract":"The potential of ChatGPT to transform the education landscape is drawing increasing attention. With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training. The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation. Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships. An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students. Respondents were divided into two groups based on their ChatGPT use experience. Data were analyzed using structural equation modeling. A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups. Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users. Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors. These results are discussed with the different primary purposes of translation and non-translation students' translation practices. The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs.","sentences":["The potential of ChatGPT to transform the education landscape is drawing increasing attention.","With its translation-related capabilities being tested and examined, ChatGPT presents both opportunities and challenges for translation training.","The effective integration of ChatGPT into translation training necessitates an understanding of students' reactions to and acceptance of ChatGPT-assisted translation.","Against this backdrop, this study draws on the Unified Theory of Acceptance and Use of Technology (UTAUT) to examine the potential determinants of students' adoption of ChatGPT for translation and investigates the moderating effects of use experience and translation training on those relationships.","An online survey targeting university students in Hong Kong collected 308 valid responses, including 148 from translation students and 160 from non-translation students.","Respondents were divided into two groups based on their ChatGPT use experience.","Data were analyzed using structural equation modeling.","A multigroup analysis revealed different structural relationships between the influencing factors of students' intention to use ChatGPT across groups.","Notably, less-experienced users' behavioral intention to use ChatGPT for translation was more strongly correlated with social influence compared with experienced users.","Non-translation students' use intention was more strongly driven by facilitating conditions compared to translation majors.","These results are discussed with the different primary purposes of translation and non-translation students' translation practices.","The findings of this study contribute to the growing body of research on AI-powered translation training and provide insights for the ongoing adaptation of translation training programs."],"url":"http://arxiv.org/abs/2406.06254v1","category":"cs.HC"}
{"created":"2024-06-10 13:21:05","title":"A Lora-Based and Maintenance-Free Cattle Monitoring System for Alpine Pastures and Remote Locations","abstract":"The advent of the Internet of Things (IoT) is boosting the proliferation of sensors and smart devices in industry and daily life. Continuous monitoring IoT systems are also finding applications in agriculture, particularly in the realm of smart farming. The adoption of wearable sensors to record the activity of livestock has garnered increasing interest. Such a device enables farmers to locate, monitor, and constantly assess the health status of their cattle more efficiently and effectively, even in challenging terrain and remote locations. This work presents a maintenance-free and robust smart sensing system that is capable of tracking cattle in remote locations and collecting activity parameters, such as the individual's grazing- and resting time. To support the paradigm of smart farming, the cattle tracker is capable of monitoring the cow's activity by analyzing data from an accelerometer, magnetometer, temperature sensor, and Global Navigation Satellite System (GNSS) module, providing them over Long Range Wide Area Network (LoRaWAN) to a backend server. By consuming 511.9 J per day with all subsystems enabled and a data transmission every 15 minutes, the custom-designed sensor node achieves a battery lifetime of 4 months. When exploiting the integrated solar energy harvesting subsystem, this can be even increased by 40% to up to 6 months. The final sensing system's robust operation is proven in a trial run with two cows on a pasture for over three days. Evaluations of the experimental results clearly show behavior patterns, which confirms the practicability of the proposed solution.","sentences":["The advent of the Internet of Things (IoT) is boosting the proliferation of sensors and smart devices in industry and daily life.","Continuous monitoring IoT systems are also finding applications in agriculture, particularly in the realm of smart farming.","The adoption of wearable sensors to record the activity of livestock has garnered increasing interest.","Such a device enables farmers to locate, monitor, and constantly assess the health status of their cattle more efficiently and effectively, even in challenging terrain and remote locations.","This work presents a maintenance-free and robust smart sensing system that is capable of tracking cattle in remote locations and collecting activity parameters, such as the individual's grazing- and resting time.","To support the paradigm of smart farming, the cattle tracker is capable of monitoring the cow's activity by analyzing data from an accelerometer, magnetometer, temperature sensor, and Global Navigation Satellite System (GNSS) module, providing them over Long Range Wide Area Network (LoRaWAN) to a backend server.","By consuming 511.9 J per day with all subsystems enabled and a data transmission every 15 minutes, the custom-designed sensor node achieves a battery lifetime of 4 months.","When exploiting the integrated solar energy harvesting subsystem, this can be even increased by 40% to up to 6 months.","The final sensing system's robust operation is proven in a trial run with two cows on a pasture for over three days.","Evaluations of the experimental results clearly show behavior patterns, which confirms the practicability of the proposed solution."],"url":"http://arxiv.org/abs/2406.06245v1","category":"eess.SY"}
{"created":"2024-06-10 12:16:28","title":"Quantifying the effect of speech pathology on automatic and human speaker verification","abstract":"This study investigates how surgical intervention for speech pathology (specifically, as a result of oral cancer surgery) impacts the performance of an automatic speaker verification (ASV) system. Using two recently collected Dutch datasets with parallel pre and post-surgery audio from the same speaker, NKI-OC-VC and SPOKE, we assess the extent to which speech pathology influences ASV performance, and whether objective/subjective measures of speech severity are correlated with the performance. Finally, we carry out a perceptual study to compare judgements of ASV and human listeners. Our findings reveal that pathological speech negatively affects ASV performance, and the severity of the speech is negatively correlated with the performance. There is a moderate agreement in perceptual and objective scores of speaker similarity and severity, however, we could not clearly establish in the perceptual study, whether the same phenomenon also exists in human perception.","sentences":["This study investigates how surgical intervention for speech pathology (specifically, as a result of oral cancer surgery) impacts the performance of an automatic speaker verification (ASV) system.","Using two recently collected Dutch datasets with parallel pre and post-surgery audio from the same speaker, NKI-OC-VC and SPOKE, we assess the extent to which speech pathology influences ASV performance, and whether objective/subjective measures of speech severity are correlated with the performance.","Finally, we carry out a perceptual study to compare judgements of ASV and human listeners.","Our findings reveal that pathological speech negatively affects ASV performance, and the severity of the speech is negatively correlated with the performance.","There is a moderate agreement in perceptual and objective scores of speaker similarity and severity, however, we could not clearly establish in the perceptual study, whether the same phenomenon also exists in human perception."],"url":"http://arxiv.org/abs/2406.06208v1","category":"cs.SD"}
{"created":"2024-06-10 11:21:04","title":"Cognitive control and mental workload in multitasking","abstract":"This study examines the relationship between mental workload and the cognitive control implemented in multitasking activity. A MATB-II experiment was conducted to simulate different conditions of multitasking demand, and to collect the behavioral and physiological activities of 17 participants. The results show that implementation of different modes of cognitive control can be detected with physiological indicators, and that cognitive control could be seen as a moderator of the effect of mental stress (task demand) upon mental strain (physiological responses).","sentences":["This study examines the relationship between mental workload and the cognitive control implemented in multitasking activity.","A MATB-II experiment was conducted to simulate different conditions of multitasking demand, and to collect the behavioral and physiological activities of 17 participants.","The results show that implementation of different modes of cognitive control can be detected with physiological indicators, and that cognitive control could be seen as a moderator of the effect of mental stress (task demand) upon mental strain (physiological responses)."],"url":"http://arxiv.org/abs/2406.06178v1","category":"cs.HC"}
{"created":"2024-06-10 09:08:56","title":"Strong and weak $CP$ tests in sequential decays of polarized $\u03a3^0$ hyperons","abstract":"The $J/\\psi, \\psi(3686) \\to \\Sigma^0 \\bar{\\Sigma}^{0}$ processes and subsequent decays are studied using the world's largest $J/\\psi$ and $\\psi(3686)$ data samples collected with the BESIII detector. The strong-$CP$ symmetry is tested in the decays of the $\\Sigma^0$ hyperons for the first time by measuring the decay parameters, $\\alpha_{\\Sigma^0} = -0.0017 \\pm 0.0021 \\pm 0.0018$ and $\\bar{\\alpha}_{\\Sigma^0} = 0.0021 \\pm 0.0020 \\pm 0.0022$. The weak-$CP$ test is performed in the subsequent decays of their daughter particles $\\Lambda$ and $\\bar{\\Lambda}$. Also for the first time, the transverse polarizations of the $\\Sigma^0$ hyperons in $J/\\psi$ and $\\psi(3686)$ decays are observed with opposite directions, and the ratios between the S-wave and D-wave contributions of the $J/\\psi, \\psi(3686) \\to \\Sigma^0 \\bar{\\Sigma}^{0}$ decays are obtained. These results are crucial to understand the decay dynamics of the charmonium states and the production mechanism of the $\\Sigma^0-\\bar{\\Sigma}^0$ pairs.","sentences":["The $J/\\psi, \\psi(3686) \\to \\Sigma^0 \\bar{\\Sigma}^{0}$ processes and subsequent decays are studied using the world's largest $J/\\psi$ and $\\psi(3686)$ data samples collected with the BESIII detector.","The strong-$CP$ symmetry is tested in the decays of the $\\Sigma^0$ hyperons for the first time by measuring the decay parameters, $\\alpha_{\\Sigma^0} = -0.0017 \\pm 0.0021 \\pm 0.0018$ and $\\bar{\\alpha}_{\\Sigma^0} = 0.0021 \\pm 0.0020 \\pm 0.0022$.","The weak-$CP$ test is performed in the subsequent decays of their daughter particles $\\Lambda$ and $\\bar{\\Lambda}$. Also for the first time, the transverse polarizations of the $\\Sigma^0$ hyperons in $J/\\psi$ and $\\psi(3686)$ decays are observed with opposite directions, and the ratios between the S-wave and D-wave contributions of the $J/\\psi, \\psi(3686) \\to \\Sigma^0 \\bar{\\Sigma}^{0}$ decays are obtained.","These results are crucial to understand the decay dynamics of the charmonium states and the production mechanism of the $\\Sigma^0-\\bar{\\Sigma}^0$ pairs."],"url":"http://arxiv.org/abs/2406.06118v1","category":"hep-ex"}
{"created":"2024-06-10 09:05:14","title":"Nuclear Reactor Data Excludes Cosmological Triangle in Search for Axion-Like Particles","abstract":"We report new constraints on axion-like particle (ALP) using data corresponding to a sodium iodine target exposure of 3063\\,kg$\\cdot$days from the neutrino elastic scattering observation with NaI (NEON) experiment. A 16.7\\,kg of thallium-doped sodium iodide target was located 23.7 meters from a 2.8\\,GW thermal power nuclear reactor. We searched for ALPs produced by high-flux photons by comparing the energy spectra of data collected during reactor-on (1596\\,kg$\\cdot$days exposure) and reactor-off (1467\\,kg$\\cdot$days exposure) periods. No signal consistent with ALP interaction was identified, allowing us to set exclusion limits at the 95\\% confidence level. Our limits cover previously unexplored regions for both photon couplings (${g_{a\\gamma}}$) and electron couplings (${g_{ae}}$) for axion masses around 1\\,MeV/c$^2$. Notably, the NEON data excludes the unconstrained region identified by laboratory-based searches for photon couplings within the ``cosmological triangle'' for the first time. The observed 95\\% confidence level limits reach as low as ${g_{a\\gamma}}$ of 4.33$\\times$ 10$^{-8}$\\,GeV$^{-1}$ and ${g_{ae}}$ of 1.10$\\times$ 10$^{-9}$ for axion masses of 1.7\\,MeV/c$^2$ and 1.0\\,MeV/c$^2$, respectively.","sentences":["We report new constraints on axion-like particle (ALP) using data corresponding to a sodium iodine target exposure of 3063\\,kg$\\cdot$days from the neutrino elastic scattering observation with NaI (NEON) experiment.","A 16.7\\,kg of thallium-doped sodium iodide target was located 23.7 meters from a 2.8\\,GW thermal power nuclear reactor.","We searched for ALPs produced by high-flux photons by comparing the energy spectra of data collected during reactor-on (1596\\,kg$\\cdot$days exposure) and reactor-off (1467\\,kg$\\cdot$days exposure) periods.","No signal consistent with ALP interaction was identified, allowing us to set exclusion limits at the 95\\% confidence level.","Our limits cover previously unexplored regions for both photon couplings (${g_{a\\gamma}}$) and electron couplings (${g_{ae}}$) for axion masses around 1\\,MeV/c$^2$. Notably, the NEON data excludes the unconstrained region identified by laboratory-based searches for photon couplings within the ``cosmological triangle'' for the first time.","The observed 95\\% confidence level limits reach as low as ${g_{a\\gamma}}$ of 4.33$\\times$ 10$^{-8}$\\,GeV$^{-1}$ and ${g_{ae}}$ of 1.10$\\times$ 10$^{-9}$ for axion masses of 1.7\\,MeV/c$^2$ and 1.0\\,MeV/c$^2$, respectively."],"url":"http://arxiv.org/abs/2406.06117v1","category":"hep-ex"}
{"created":"2024-06-10 08:23:16","title":"Sim-To-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery","abstract":"Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making. Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost. A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.   In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation. We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery. After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images. Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images. This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.","sentences":["Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making.","Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost.","A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.   ","In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation.","We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery.","After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images.","Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images.","This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics."],"url":"http://arxiv.org/abs/2406.06092v1","category":"cs.RO"}
{"created":"2024-06-10 07:50:45","title":"A characterization of uniquely representable two-directional orthogonal ray graphs","abstract":"In this paper, we provide a characterization of uniquely representable two-directional orthogonal ray graphs, which are defined as the intersection graphs of rightward and downward rays. The collection of these rays is called a representation of the graph. Two-directional orthogonal ray graphs are equivalent to several well-studied classes of graphs, including complements of circular-arc graphs with clique cover number two. Normalized representations of two-directional orthogonal ray graphs, where the positions of certain rays are determined by neighborhood containment relations, can be obtained from the normalized representations of circular-arc graphs. However, the normalized representations are not necessarily unique, even when considering only the relative positions of the rays. Recent studies indicate that two-directional orthogonal ray graphs share similar characterizations to interval graphs. Hanlon (1982) and Fishburn (1985) characterized uniquely representable interval graphs by introducing the notion of a buried subgraph. Following their characterization, we define buried subgraphs of two-directional orthogonal ray graphs and prove that their absence is a necessary and sufficient condition for a graph to be uniquely representable.","sentences":["In this paper, we provide a characterization of uniquely representable two-directional orthogonal ray graphs, which are defined as the intersection graphs of rightward and downward rays.","The collection of these rays is called a representation of the graph.","Two-directional orthogonal ray graphs are equivalent to several well-studied classes of graphs, including complements of circular-arc graphs with clique cover number two.","Normalized representations of two-directional orthogonal ray graphs, where the positions of certain rays are determined by neighborhood containment relations, can be obtained from the normalized representations of circular-arc graphs.","However, the normalized representations are not necessarily unique, even when considering only the relative positions of the rays.","Recent studies indicate that two-directional orthogonal ray graphs share similar characterizations to interval graphs.","Hanlon (1982) and Fishburn (1985) characterized uniquely representable interval graphs by introducing the notion of a buried subgraph.","Following their characterization, we define buried subgraphs of two-directional orthogonal ray graphs and prove that their absence is a necessary and sufficient condition for a graph to be uniquely representable."],"url":"http://arxiv.org/abs/2406.06077v1","category":"math.CO"}
{"created":"2024-06-10 07:18:41","title":"ProcessPainter: Learn Painting Process from Sequence Data","abstract":"The painting process of artists is inherently stepwise and varies significantly among different painters and styles. Generating detailed, step-by-step painting processes is essential for art education and research, yet remains largely underexplored. Traditional stroke-based rendering methods break down images into sequences of brushstrokes, yet they fall short of replicating the authentic processes of artists, with limitations confined to basic brushstroke modifications. Text-to-image models utilizing diffusion processes generate images through iterative denoising, also diverge substantially from artists' painting process. To address these challenges, we introduce ProcessPainter, a text-to-video model that is initially pre-trained on synthetic data and subsequently fine-tuned with a select set of artists' painting sequences using the LoRA model. This approach successfully generates painting processes from text prompts for the first time. Furthermore, we introduce an Artwork Replication Network capable of accepting arbitrary-frame input, which facilitates the controlled generation of painting processes, decomposing images into painting sequences, and completing semi-finished artworks. This paper offers new perspectives and tools for advancing art education and image generation technology.","sentences":["The painting process of artists is inherently stepwise and varies significantly among different painters and styles.","Generating detailed, step-by-step painting processes is essential for art education and research, yet remains largely underexplored.","Traditional stroke-based rendering methods break down images into sequences of brushstrokes, yet they fall short of replicating the authentic processes of artists, with limitations confined to basic brushstroke modifications.","Text-to-image models utilizing diffusion processes generate images through iterative denoising, also diverge substantially from artists' painting process.","To address these challenges, we introduce ProcessPainter, a text-to-video model that is initially pre-trained on synthetic data and subsequently fine-tuned with a select set of artists' painting sequences using the LoRA model.","This approach successfully generates painting processes from text prompts for the first time.","Furthermore, we introduce an Artwork Replication Network capable of accepting arbitrary-frame input, which facilitates the controlled generation of painting processes, decomposing images into painting sequences, and completing semi-finished artworks.","This paper offers new perspectives and tools for advancing art education and image generation technology."],"url":"http://arxiv.org/abs/2406.06062v1","category":"cs.CV"}
{"created":"2024-06-10 07:18:24","title":"Greedy SLIM: A SLIM-Based Approach For Preference Elicitation","abstract":"Preference elicitation is an active learning approach to tackle the cold-start problem of recommender systems. Roughly speaking, new users are asked to rate some carefully selected items in order to compute appropriate recommendations for them. To the best of our knowledge, we are the first to propose a method for preference elicitation that is based on SLIM , a state-of-the-art technique for top-N recommendation. Our approach mainly consists of a new training technique for SLIM, which we call Greedy SLIM. This technique iteratively selects items for the training in order to minimize the SLIM loss greedily. We conduct offline experiments as well as a user study to assess the performance of this new method. The results are remarkable, especially with respect to the user study. We conclude that Greedy SLIM seems to be more suitable for preference elicitation than widely used methods based on latent factor models.","sentences":["Preference elicitation is an active learning approach to tackle the cold-start problem of recommender systems.","Roughly speaking, new users are asked to rate some carefully selected items in order to compute appropriate recommendations for them.","To the best of our knowledge, we are the first to propose a method for preference elicitation that is based on SLIM , a state-of-the-art technique for top-N recommendation.","Our approach mainly consists of a new training technique for SLIM, which we call Greedy SLIM.","This technique iteratively selects items for the training in order to minimize the SLIM loss greedily.","We conduct offline experiments as well as a user study to assess the performance of this new method.","The results are remarkable, especially with respect to the user study.","We conclude that Greedy SLIM seems to be more suitable for preference elicitation than widely used methods based on latent factor models."],"url":"http://arxiv.org/abs/2406.06061v1","category":"cs.IR"}
{"created":"2024-06-10 06:39:37","title":"On the Utility of Accounting for Human Beliefs about AI Behavior in Human-AI Collaboration","abstract":"To enable effective human-AI collaboration, merely optimizing AI performance while ignoring humans is not sufficient. Recent research has demonstrated that designing AI agents to account for human behavior leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior is static, irrespective of AI behavior. In reality, humans may adjust their action plans based on their observations of AI behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider the beliefs of its human partner, i.e., what the human partner thinks the AI agent is doing, and design its action plan to facilitate easier collaboration with its human partner. Specifically, we developed a model of human beliefs that accounts for how humans reason about the behavior of their AI partners. Based on this belief model, we then developed an AI agent that considers both human behavior and human beliefs in devising its strategy for working with humans. Through extensive real-world human-subject experiments, we demonstrated that our belief model more accurately predicts humans' beliefs about AI behavior. Moreover, we showed that our design of AI agents that accounts for human beliefs enhances performance in human-AI collaboration.","sentences":["To enable effective human-AI collaboration, merely optimizing AI performance while ignoring humans is not sufficient.","Recent research has demonstrated that designing AI agents to account for human behavior leads to improved performance in human-AI collaboration.","However, a limitation of most existing approaches is their assumption that human behavior is static, irrespective of AI behavior.","In reality, humans may adjust their action plans based on their observations of AI behavior.","In this paper, we address this limitation by enabling a collaborative AI agent to consider the beliefs of its human partner, i.e., what the human partner thinks the AI agent is doing, and design its action plan to facilitate easier collaboration with its human partner.","Specifically, we developed a model of human beliefs that accounts for how humans reason about the behavior of their AI partners.","Based on this belief model, we then developed an AI agent that considers both human behavior and human beliefs in devising its strategy for working with humans.","Through extensive real-world human-subject experiments, we demonstrated that our belief model more accurately predicts humans' beliefs about AI behavior.","Moreover, we showed that our design of AI agents that accounts for human beliefs enhances performance in human-AI collaboration."],"url":"http://arxiv.org/abs/2406.06051v1","category":"cs.AI"}
{"created":"2024-06-10 06:29:00","title":"Robust Latent Representation Tuning for Image-text Classification","abstract":"Large models have demonstrated exceptional generalization capabilities in computer vision and natural language processing. Recent efforts have focused on enhancing these models with multimodal processing abilities. However, addressing the challenges posed by scenarios where one modality is absent remains a significant hurdle. In response to this issue, we propose a robust latent representation tuning method for large models. Specifically, our approach introduces a modality latent translation module to maximize the correlation between modalities. Following this, a newly designed fusion module is employed to facilitate information interaction between the modalities. In this framework, not only are common semantics refined during training, but the method also yields robust representations in the absence of one modality. Importantly, our method maintains the frozen state of the image and text foundation models to preserve their abilities acquired through large-scale pretraining. We conduct experiments on several public datasets, and the results underscore the effectiveness of our proposed method.","sentences":["Large models have demonstrated exceptional generalization capabilities in computer vision and natural language processing.","Recent efforts have focused on enhancing these models with multimodal processing abilities.","However, addressing the challenges posed by scenarios where one modality is absent remains a significant hurdle.","In response to this issue, we propose a robust latent representation tuning method for large models.","Specifically, our approach introduces a modality latent translation module to maximize the correlation between modalities.","Following this, a newly designed fusion module is employed to facilitate information interaction between the modalities.","In this framework, not only are common semantics refined during training, but the method also yields robust representations in the absence of one modality.","Importantly, our method maintains the frozen state of the image and text foundation models to preserve their abilities acquired through large-scale pretraining.","We conduct experiments on several public datasets, and the results underscore the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2406.06048v1","category":"cs.CV"}
{"created":"2024-06-10 06:27:42","title":"MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models","abstract":"Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we fine-tune a small data influence model to approximate oracle data preference signals collected by locally probing the pretraining model and to select data accordingly for the next pretraining stage. Experiments on Pythia and the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks in both zero- and few-shot settings. It doubles the gains achieved by recent data selection approaches that leverage larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analysis validates the ever-changing data preferences of pretraining models and the effectiveness of our data influence models to capture them. Our code is open-sourced at https://github.com/cxcscmu/MATES.","sentences":["Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora.","Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining.","In this paper, we introduce model-aware data selection with data influence models (MATES), where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress.","Specifically, we fine-tune a small data influence model to approximate oracle data preference signals collected by locally probing the pretraining model and to select data accordingly for the next pretraining stage.","Experiments on Pythia and the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks in both zero-","and few-shot settings.","It doubles the gains achieved by recent data selection approaches that leverage larger reference models and reduces the total FLOPs required to reach certain performances by half.","Further analysis validates the ever-changing data preferences of pretraining models and the effectiveness of our data influence models to capture them.","Our code is open-sourced at https://github.com/cxcscmu/MATES."],"url":"http://arxiv.org/abs/2406.06046v1","category":"cs.CL"}
{"created":"2024-06-10 06:26:03","title":"Synthesizing Efficient Data with Diffusion Models for Person Re-Identification Pre-Training","abstract":"Existing person re-identification (Re-ID) methods principally deploy the ImageNet-1K dataset for model initialization, which inevitably results in sub-optimal situations due to the large domain gap. One of the key challenges is that building large-scale person Re-ID datasets is time-consuming. Some previous efforts address this problem by collecting person images from the internet e.g., LUPerson, but it struggles to learn from unlabeled, uncontrollable, and noisy data. In this paper, we present a novel paradigm Diffusion-ReID to efficiently augment and generate diverse images based on known identities without requiring any cost of data collection and annotation. Technically, this paradigm unfolds in two stages: generation and filtering. During the generation stage, we propose Language Prompts Enhancement (LPE) to ensure the ID consistency between the input image sequence and the generated images. In the diffusion process, we propose a Diversity Injection (DI) module to increase attribute diversity. In order to make the generated data have higher quality, we apply a Re-ID confidence threshold filter to further remove the low-quality images. Benefiting from our proposed paradigm, we first create a new large-scale person Re-ID dataset Diff-Person, which consists of over 777K images from 5,183 identities. Next, we build a stronger person Re-ID backbone pre-trained on our Diff-Person. Extensive experiments are conducted on four person Re-ID benchmarks in six widely used settings. Compared with other pre-training and self-supervised competitors, our approach shows significant superiority.","sentences":["Existing person re-identification (Re-ID) methods principally deploy the ImageNet-1K dataset for model initialization, which inevitably results in sub-optimal situations due to the large domain gap.","One of the key challenges is that building large-scale person Re-ID datasets is time-consuming.","Some previous efforts address this problem by collecting person images from the internet e.g., LUPerson, but it struggles to learn from unlabeled, uncontrollable, and noisy data.","In this paper, we present a novel paradigm Diffusion-ReID to efficiently augment and generate diverse images based on known identities without requiring any cost of data collection and annotation.","Technically, this paradigm unfolds in two stages: generation and filtering.","During the generation stage, we propose Language Prompts Enhancement (LPE) to ensure the ID consistency between the input image sequence and the generated images.","In the diffusion process, we propose a Diversity Injection (DI) module to increase attribute diversity.","In order to make the generated data have higher quality, we apply a Re-ID confidence threshold filter to further remove the low-quality images.","Benefiting from our proposed paradigm, we first create a new large-scale person Re-ID dataset Diff-Person, which consists of over 777K images from 5,183 identities.","Next, we build a stronger person Re-ID backbone pre-trained on our Diff-Person.","Extensive experiments are conducted on four person Re-ID benchmarks in six widely used settings.","Compared with other pre-training and self-supervised competitors, our approach shows significant superiority."],"url":"http://arxiv.org/abs/2406.06045v1","category":"cs.CV"}
{"created":"2024-06-10 06:06:38","title":"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning","abstract":"Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.","sentences":["Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL).","However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups.","To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions.","Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments.","In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones.","We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB."],"url":"http://arxiv.org/abs/2406.06037v1","category":"cs.LG"}
{"created":"2024-06-10 04:36:21","title":"Neuro-TransUNet: Segmentation of stroke lesion in MRI using transformers","abstract":"Accurate segmentation of the stroke lesions using magnetic resonance imaging (MRI) is associated with difficulties due to the complicated anatomy of the brain and the different properties of the lesions. This study introduces the Neuro-TransUNet framework, which synergizes the U-Net's spatial feature extraction with SwinUNETR's global contextual processing ability, further enhanced by advanced feature fusion and segmentation synthesis techniques. The comprehensive data pre-processing pipeline improves the framework's efficiency, which involves resampling, bias correction, and data standardization, enhancing data quality and consistency. Ablation studies confirm the significant impact of the advanced integration of U-Net with SwinUNETR and data pre-processing pipelines on performance and demonstrate the model's effectiveness. The proposed Neuro-TransUNet model, trained with the ATLAS v2.0 \\emph{training} dataset, outperforms existing deep learning algorithms and establishes a new benchmark in stroke lesion segmentation.","sentences":["Accurate segmentation of the stroke lesions using magnetic resonance imaging (MRI) is associated with difficulties due to the complicated anatomy of the brain and the different properties of the lesions.","This study introduces the Neuro-TransUNet framework, which synergizes the U-Net's spatial feature extraction with SwinUNETR's global contextual processing ability, further enhanced by advanced feature fusion and segmentation synthesis techniques.","The comprehensive data pre-processing pipeline improves the framework's efficiency, which involves resampling, bias correction, and data standardization, enhancing data quality and consistency.","Ablation studies confirm the significant impact of the advanced integration of U-Net with SwinUNETR and data pre-processing pipelines on performance and demonstrate the model's effectiveness.","The proposed Neuro-TransUNet model, trained with the ATLAS v2.0 \\emph{training} dataset, outperforms existing deep learning algorithms and establishes a new benchmark in stroke lesion segmentation."],"url":"http://arxiv.org/abs/2406.06017v1","category":"eess.IV"}
{"created":"2024-06-10 04:10:18","title":"The Impact of AI on Academic Research and Publishing","abstract":"Generative artificial intelligence (AI) technologies like ChatGPT, have significantly impacted academic writing and publishing through their ability to generate content at levels comparable to or surpassing human writers. Through a review of recent interdisciplinary literature, this paper examines ethical considerations surrounding the integration of AI into academia, focusing on the potential for this technology to be used for scholarly misconduct and necessary oversight when using it for writing, editing, and reviewing of scholarly papers. The findings highlight the need for collaborative approaches to AI usage among publishers, editors, reviewers, and authors to ensure that this technology is used ethically and productively.","sentences":["Generative artificial intelligence (AI) technologies like ChatGPT, have significantly impacted academic writing and publishing through their ability to generate content at levels comparable to or surpassing human writers.","Through a review of recent interdisciplinary literature, this paper examines ethical considerations surrounding the integration of AI into academia, focusing on the potential for this technology to be used for scholarly misconduct and necessary oversight when using it for writing, editing, and reviewing of scholarly papers.","The findings highlight the need for collaborative approaches to AI usage among publishers, editors, reviewers, and authors to ensure that this technology is used ethically and productively."],"url":"http://arxiv.org/abs/2406.06009v1","category":"cs.DL"}
{"created":"2024-06-10 04:07:09","title":"CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models","abstract":"Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.","sentences":["Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare.","However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment.","In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain.","We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness.","CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions.","Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups.","Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness.","We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES."],"url":"http://arxiv.org/abs/2406.06007v1","category":"cs.LG"}
{"created":"2024-06-10 03:57:39","title":"FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model","abstract":"Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce score smoothing to align as closely as possible with human judgment and to be robust to user-defined grading criteria. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: https://github.com/Yebin46/FLEUR.","sentences":["Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions.","However, these methods do not provide an explanation for the assigned score.","Moreover, reference captions are expensive to acquire.","In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics.","By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score.","We introduce score smoothing to align as closely as possible with human judgment and to be robust to user-defined grading criteria.","FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics.","Our source code and results are publicly available at: https://github.com/Yebin46/FLEUR."],"url":"http://arxiv.org/abs/2406.06004v1","category":"cs.CV"}
{"created":"2024-06-10 03:38:35","title":"fSEAD: a Composable FPGA-based Streaming Ensemble Anomaly Detection Library","abstract":"Machine learning ensembles combine multiple base models to produce a more accurate output. They can be applied to a range of machine learning problems, including anomaly detection. In this paper, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD). To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors. Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream. Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance. Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported. Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at run-time to create an ensemble that maximizes the use of FPGA resources and accuracy. Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at run-time to adapt to changing environmental conditions. We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speed-ups ranging from $3\\times$ to $8\\times$.","sentences":["Machine learning ensembles combine multiple base models to produce a more accurate output.","They can be applied to a range of machine learning problems, including anomaly detection.","In this paper, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD).","To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors.","Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream.","Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance.","Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported.","Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at run-time to create an ensemble that maximizes the use of FPGA resources and accuracy.","Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at run-time to adapt to changing environmental conditions.","We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speed-ups ranging from $3\\times$ to $8\\times$."],"url":"http://arxiv.org/abs/2406.05999v1","category":"cs.AR"}
{"created":"2024-06-10 03:29:23","title":"A Dual-View Approach to Classifying Radiology Reports by Co-Training","abstract":"Radiology report analysis provides valuable information that can aid with public health initiatives, and has been attracting increasing attention from the research community. In this work, we present a novel insight that the structure of a radiology report (namely, the Findings and Impression sections) offers different views of a radiology scan. Based on this intuition, we further propose a co-training approach, where two machine learning models are built upon the Findings and Impression sections, respectively, and use each other's information to boost performance with massive unlabeled data in a semi-supervised manner. We conducted experiments in a public health surveillance study, and results show that our co-training approach is able to improve performance using the dual views and surpass competing supervised and semi-supervised methods.","sentences":["Radiology report analysis provides valuable information that can aid with public health initiatives, and has been attracting increasing attention from the research community.","In this work, we present a novel insight that the structure of a radiology report (namely, the Findings and Impression sections) offers different views of a radiology scan.","Based on this intuition, we further propose a co-training approach, where two machine learning models are built upon the Findings and Impression sections, respectively, and use each other's information to boost performance with massive unlabeled data in a semi-supervised manner.","We conducted experiments in a public health surveillance study, and results show that our co-training approach is able to improve performance using the dual views and surpass competing supervised and semi-supervised methods."],"url":"http://arxiv.org/abs/2406.05995v1","category":"cs.CL"}
{"created":"2024-06-10 02:51:16","title":"Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook","abstract":"Mental health constitutes a complex and pervasive global challenge, affecting millions of lives and often leading to severe consequences. In this paper, we conduct a thorough survey to explore the intersection of data science, artificial intelligence, and mental healthcare, focusing on the recent developments of mental disorder detection through online social media (OSM). A significant portion of the population actively engages in OSM platforms, creating a vast repository of personal data that holds immense potential for mental health analytics. The paper navigates through traditional diagnostic methods, state-of-the-art data- and AI-driven research studies, and the emergence of explainable AI (XAI) models for mental healthcare. We review state-of-the-art machine learning methods, particularly those based on modern deep learning, while emphasising the need for explainability in healthcare AI models. The experimental design section provides insights into prevalent practices, including available datasets and evaluation approaches. We also identify key issues and challenges in the field and propose promising future research directions. As mental health decisions demand transparency, interpretability, and ethical considerations, this paper contributes to the ongoing discourse on advancing XAI in mental healthcare through social media. The comprehensive overview presented here aims to guide researchers, practitioners, and policymakers in developing the area of mental disorder detection.","sentences":["Mental health constitutes a complex and pervasive global challenge, affecting millions of lives and often leading to severe consequences.","In this paper, we conduct a thorough survey to explore the intersection of data science, artificial intelligence, and mental healthcare, focusing on the recent developments of mental disorder detection through online social media (OSM).","A significant portion of the population actively engages in OSM platforms, creating a vast repository of personal data that holds immense potential for mental health analytics.","The paper navigates through traditional diagnostic methods, state-of-the-art data- and AI-driven research studies, and the emergence of explainable AI (XAI) models for mental healthcare.","We review state-of-the-art machine learning methods, particularly those based on modern deep learning, while emphasising the need for explainability in healthcare AI models.","The experimental design section provides insights into prevalent practices, including available datasets and evaluation approaches.","We also identify key issues and challenges in the field and propose promising future research directions.","As mental health decisions demand transparency, interpretability, and ethical considerations, this paper contributes to the ongoing discourse on advancing XAI in mental healthcare through social media.","The comprehensive overview presented here aims to guide researchers, practitioners, and policymakers in developing the area of mental disorder detection."],"url":"http://arxiv.org/abs/2406.05984v1","category":"cs.LG"}
{"created":"2024-06-10 02:50:33","title":"Artificial Intelligence for Neuro MRI Acquisition: A Review","abstract":"Magnetic resonance imaging (MRI) has significantly benefited from the resurgence of artificial intelligence (AI). By leveraging AI's capabilities in large-scale optimization and pattern recognition, innovative methods are transforming the MRI acquisition workflow, including planning, sequence design, and correction of acquisition artifacts. These emerging algorithms demonstrate substantial potential in enhancing the efficiency and throughput of acquisition steps. This review discusses several pivotal AI-based methods in neuro MRI acquisition, focusing on their technological advances, impact on clinical practice, and potential risks.","sentences":["Magnetic resonance imaging (MRI) has significantly benefited from the resurgence of artificial intelligence (AI).","By leveraging AI's capabilities in large-scale optimization and pattern recognition, innovative methods are transforming the MRI acquisition workflow, including planning, sequence design, and correction of acquisition artifacts.","These emerging algorithms demonstrate substantial potential in enhancing the efficiency and throughput of acquisition steps.","This review discusses several pivotal AI-based methods in neuro MRI acquisition, focusing on their technological advances, impact on clinical practice, and potential risks."],"url":"http://arxiv.org/abs/2406.05982v1","category":"eess.IV"}
{"created":"2024-06-10 02:47:55","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization","abstract":"Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.","sentences":["Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks.","Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM.","However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs.","To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM.","Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors.","The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices.","To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors.","Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency.","Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs.","Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM."],"url":"http://arxiv.org/abs/2406.05981v1","category":"cs.LG"}
{"created":"2024-06-10 02:14:19","title":"Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context","abstract":"When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics, to evaluate the decision-making behaviors of LLMs. Through a multiple-choice-list experiment, we estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities. However, there are significant variations in the degree to which these behaviors are expressed across different LLMs. We also explore their behavior when embedded with socio-demographic features, uncovering significant disparities. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for developing standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments.","sentences":["When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion.","Given the widespread use of large language models (LLMs) in decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases.","Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood.","This paper proposes a framework, grounded in behavioral economics, to evaluate the decision-making behaviors of LLMs.","Through a multiple-choice-list experiment, we estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro.","Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities.","However, there are significant variations in the degree to which these behaviors are expressed across different LLMs.","We also explore their behavior when embedded with socio-demographic features, uncovering significant disparities.","For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices.","These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios.","Therefore, this study advocates for developing standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments."],"url":"http://arxiv.org/abs/2406.05972v1","category":"cs.AI"}
{"created":"2024-06-10 01:59:00","title":"CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark","abstract":"Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.","sentences":["Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data.","However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric.","While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages.","More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation.","To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process.","As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions.","We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models.","This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field."],"url":"http://arxiv.org/abs/2406.05967v1","category":"cs.CV"}
{"created":"2024-06-10 01:47:52","title":"MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing Voice Synthesis via Classifier-free Diffusion Guidance","abstract":"In this paper, we propose MakeSinger, a semi-supervised training method for singing voice synthesis (SVS) via classifier-free diffusion guidance. The challenge in SVS lies in the costly process of gathering aligned sets of text, pitch, and audio data. MakeSinger enables the training of the diffusion-based SVS model from any speech and singing voice data regardless of its labeling, thereby enhancing the quality of generated voices with large amount of unlabeled data. At inference, our novel dual guiding mechanism gives text and pitch guidance on the reverse diffusion step by estimating the score of masked input. Experimental results show that the model trained in a semi-supervised manner outperforms other baselines trained only on the labeled data in terms of pronunciation, pitch accuracy and overall quality. Furthermore, we demonstrate that by adding Text-to-Speech (TTS) data in training, the model can synthesize the singing voices of TTS speakers even without their singing voices.","sentences":["In this paper, we propose MakeSinger, a semi-supervised training method for singing voice synthesis (SVS) via classifier-free diffusion guidance.","The challenge in SVS lies in the costly process of gathering aligned sets of text, pitch, and audio data.","MakeSinger enables the training of the diffusion-based SVS model from any speech and singing voice data regardless of its labeling, thereby enhancing the quality of generated voices with large amount of unlabeled data.","At inference, our novel dual guiding mechanism gives text and pitch guidance on the reverse diffusion step by estimating the score of masked input.","Experimental results show that the model trained in a semi-supervised manner outperforms other baselines trained only on the labeled data in terms of pronunciation, pitch accuracy and overall quality.","Furthermore, we demonstrate that by adding Text-to-Speech (TTS) data in training, the model can synthesize the singing voices of TTS speakers even without their singing voices."],"url":"http://arxiv.org/abs/2406.05965v1","category":"eess.AS"}
{"created":"2024-06-10 01:45:55","title":"Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024","abstract":"In this paper, the solution of HYU MLLAB KT Team to the Multimodal Algorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyond conventional visual question-answering problems, the SMART-101 challenge aims to achieve human-level multimodal understanding by tackling complex visio-linguistic puzzles designed for children in the 6-8 age group. To solve this problem, we suggest two main ideas. First, to utilize the reasoning ability of a large-scale language model (LLM), the given visual cues (images) are grounded in the text modality. For this purpose, we generate highly detailed text captions that describe the context of the image and use these captions as input for the LLM. Second, due to the nature of puzzle images, which often contain various geometric visual patterns, we utilize an object detection algorithm to ensure these patterns are not overlooked in the captioning process. We employed the SAM algorithm, which can detect various-size objects, to capture the visual features of these geometric patterns and used this information as input for the LLM. Under the puzzle split configuration, we achieved an option selection accuracy Oacc of 29.5 on the test set and a weighted option selection accuracy (WOSA) of 27.1 on the challenge set.","sentences":["In this paper, the solution of HYU MLLAB KT Team to the Multimodal Algorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented.","Beyond conventional visual question-answering problems, the SMART-101 challenge aims to achieve human-level multimodal understanding by tackling complex visio-linguistic puzzles designed for children in the 6-8 age group.","To solve this problem, we suggest two main ideas.","First, to utilize the reasoning ability of a large-scale language model (LLM), the given visual cues (images) are grounded in the text modality.","For this purpose, we generate highly detailed text captions that describe the context of the image and use these captions as input for the LLM.","Second, due to the nature of puzzle images, which often contain various geometric visual patterns, we utilize an object detection algorithm to ensure these patterns are not overlooked in the captioning process.","We employed the SAM algorithm, which can detect various-size objects, to capture the visual features of these geometric patterns and used this information as input for the LLM.","Under the puzzle split configuration, we achieved an option selection accuracy Oacc of 29.5 on the test set and a weighted option selection accuracy (WOSA) of 27.1 on the challenge set."],"url":"http://arxiv.org/abs/2406.05963v1","category":"cs.CV"}
{"created":"2024-06-10 01:35:42","title":"Photoinduced non-reciprocal magnetism","abstract":"Out of equilibrium, the action-reaction symmetry of the interactions is often broken, leading to the emergence of various collective phenomena with no equilibrium counterparts. Although ubiquitous in classical active systems, implementing such non-reciprocal interactions in solid-state systems has remained challenging, as the known quantum schemes require precise control over the system on a single-site level. Here, we propose a novel dissipation-engineering protocol to induce non-reciprocal interactions in solid-state platforms with light, which we expect to be achievable with state-of-the-art experimental techniques. Focusing on magnetic metals for concreteness, we show microscopically that a light injection that introduces the decay channel to a virtually excited state gives rise to non-reciprocal interactions between localized spins. One can even realize a situation where spin A tries to align with spin B but the B tries the opposite, resulting in a chase-and-runaway dynamics. Applying our scheme to layered ferromagnets, we show that a non-reciprocal phase transition from a static to a many-body time-dependent chiral phase emerges. Our work paves the way to bring solid-state systems to the realm of non-reciprocal science, providing yet another possibility to control quantum matter with light.","sentences":["Out of equilibrium, the action-reaction symmetry of the interactions is often broken, leading to the emergence of various collective phenomena with no equilibrium counterparts.","Although ubiquitous in classical active systems, implementing such non-reciprocal interactions in solid-state systems has remained challenging, as the known quantum schemes require precise control over the system on a single-site level.","Here, we propose a novel dissipation-engineering protocol to induce non-reciprocal interactions in solid-state platforms with light, which we expect to be achievable with state-of-the-art experimental techniques.","Focusing on magnetic metals for concreteness, we show microscopically that a light injection that introduces the decay channel to a virtually excited state gives rise to non-reciprocal interactions between localized spins.","One can even realize a situation where spin A tries to align with spin B but the B tries the opposite, resulting in a chase-and-runaway dynamics.","Applying our scheme to layered ferromagnets, we show that a non-reciprocal phase transition from a static to a many-body time-dependent chiral phase emerges.","Our work paves the way to bring solid-state systems to the realm of non-reciprocal science, providing yet another possibility to control quantum matter with light."],"url":"http://arxiv.org/abs/2406.05957v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 01:21:31","title":"Aligning Large Language Models with Representation Editing: A Control Perspective","abstract":"Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods.","sentences":["Aligning large language models (LLMs) with human objectives is crucial for real-world applications.","However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources.","Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities.","To address these challenges, we propose aligning LLMs through representation editing.","The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system.","To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system.","We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time.","Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods."],"url":"http://arxiv.org/abs/2406.05954v1","category":"cs.AI"}
{"created":"2024-06-10 00:53:25","title":"Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models","abstract":"Backdoor attacks present significant threats to Large Language Models (LLMs), particularly with the rise of third-party services that offer API integration and prompt engineering. Untrustworthy third parties can plant backdoors into LLMs and pose risks to users by embedding malicious instructions into user queries. The backdoor-compromised LLM will generate malicious output when and input is embedded with a specific trigger predetermined by an attacker. Traditional defense strategies, which primarily involve model parameter fine-tuning and gradient calculation, are inadequate for LLMs due to their extensive computational and clean data requirements. In this paper, we propose a novel solution, Chain-of-Scrutiny (CoS), to address these challenges. Backdoor attacks fundamentally create a shortcut from the trigger to the target output, thus lack reasoning support. Accordingly, CoS guides the LLMs to generate detailed reasoning steps for the input, then scrutinizes the reasoning process to ensure consistency with the final answer. Any inconsistency may indicate an attack. CoS only requires black-box access to LLM, offering a practical defense, particularly for API-accessible LLMs. It is user-friendly, enabling users to conduct the defense themselves. Driven by natural language, the entire defense process is transparent to users. We validate the effectiveness of CoS through extensive experiments across various tasks and LLMs. Additionally, experiments results shows CoS proves more beneficial for more powerful LLMs.","sentences":["Backdoor attacks present significant threats to Large Language Models (LLMs), particularly with the rise of third-party services that offer API integration and prompt engineering.","Untrustworthy third parties can plant backdoors into LLMs and pose risks to users by embedding malicious instructions into user queries.","The backdoor-compromised LLM will generate malicious output when and input is embedded with a specific trigger predetermined by an attacker.","Traditional defense strategies, which primarily involve model parameter fine-tuning and gradient calculation, are inadequate for LLMs due to their extensive computational and clean data requirements.","In this paper, we propose a novel solution, Chain-of-Scrutiny (CoS), to address these challenges.","Backdoor attacks fundamentally create a shortcut from the trigger to the target output, thus lack reasoning support.","Accordingly, CoS guides the LLMs to generate detailed reasoning steps for the input, then scrutinizes the reasoning process to ensure consistency with the final answer.","Any inconsistency may indicate an attack.","CoS only requires black-box access to LLM, offering a practical defense, particularly for API-accessible LLMs.","It is user-friendly, enabling users to conduct the defense themselves.","Driven by natural language, the entire defense process is transparent to users.","We validate the effectiveness of CoS through extensive experiments across various tasks and LLMs.","Additionally, experiments results shows CoS proves more beneficial for more powerful LLMs."],"url":"http://arxiv.org/abs/2406.05948v1","category":"cs.CR"}
{"created":"2024-06-10 00:35:23","title":"Safety Alignment Should Be Made More Than Just a Few Tokens Deep","abstract":"The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.","sentences":["The safety alignment of current Large Language Models (LLMs) is vulnerable.","Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models.","We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens.","We refer to this issue as shallow safety alignment.","In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue.","We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks.","Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities.","For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits.","Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens.","Overall, we advocate that future safety alignment should be made more than just a few tokens deep."],"url":"http://arxiv.org/abs/2406.05946v1","category":"cs.CR"}
{"created":"2024-06-10 00:35:23","title":"Accent Conversion with Articulatory Representations","abstract":"Conversion of non-native accented speech to native (American) English has a wide range of applications such as improving intelligibility of non-native speech. Previous work on this domain has used phonetic posteriograms as the target speech representation to train an acoustic model which is then used to extract a compact representation of input speech for accent conversion. In this work, we introduce the idea of using an effective articulatory speech representation, extracted from an acoustic-to-articulatory speech inversion system, to improve the acoustic model used in accent conversion. The idea to incorporate articulatory representations originates from their ability to well characterize accents in speech. To incorporate articulatory representations with conventional phonetic posteriograms, a multi-task learning based acoustic model is proposed. Objective and subjective evaluations show that the use of articulatory representations can improve the effectiveness of accent conversion.","sentences":["Conversion of non-native accented speech to native (American) English has a wide range of applications such as improving intelligibility of non-native speech.","Previous work on this domain has used phonetic posteriograms as the target speech representation to train an acoustic model which is then used to extract a compact representation of input speech for accent conversion.","In this work, we introduce the idea of using an effective articulatory speech representation, extracted from an acoustic-to-articulatory speech inversion system, to improve the acoustic model used in accent conversion.","The idea to incorporate articulatory representations originates from their ability to well characterize accents in speech.","To incorporate articulatory representations with conventional phonetic posteriograms, a multi-task learning based acoustic model is proposed.","Objective and subjective evaluations show that the use of articulatory representations can improve the effectiveness of accent conversion."],"url":"http://arxiv.org/abs/2406.05947v1","category":"eess.AS"}
{"created":"2024-06-10 00:34:23","title":"Machine Unlearning for Uplink Interference Cancellation","abstract":"Machine unlearning (MUL) is introduced as a means to achieve interference cancellation within artificial intelligence (AI)-enabled wireless systems. It is observed that interference cancellation with MUL demonstrates $30\\%$ improvement in a classification task accuracy in the presence of a corrupted AI model. Accordingly, the necessity for instantaneous channel state information for existing interference source is eliminated and a corrupted latent space with interference noise is cleansed with MUL algorithm, achieving this without the necessity for either retraining or dataset cleansing. A Membership Interference Attack (MIA) served as a benchmark for assessing the efficacy of MUL in mitigating interference within a neural network model. The advantage of the MUL algorithm was determined by evaluating both the probability of interference and the quantity of samples requiring retraining. In a simple signal-to-noise ratio classification task, the comprehensive improvement across various test cases in terms of accuracy demonstrates that MUL exhibits extensive capabilities and limitations, particularly in native AI applications.","sentences":["Machine unlearning (MUL) is introduced as a means to achieve interference cancellation within artificial intelligence (AI)-enabled wireless systems.","It is observed that interference cancellation with MUL demonstrates $30\\%$ improvement in a classification task accuracy in the presence of a corrupted AI model.","Accordingly, the necessity for instantaneous channel state information for existing interference source is eliminated and a corrupted latent space with interference noise is cleansed with MUL algorithm, achieving this without the necessity for either retraining or dataset cleansing.","A Membership Interference Attack (MIA) served as a benchmark for assessing the efficacy of MUL in mitigating interference within a neural network model.","The advantage of the MUL algorithm was determined by evaluating both the probability of interference and the quantity of samples requiring retraining.","In a simple signal-to-noise ratio classification task, the comprehensive improvement across various test cases in terms of accuracy demonstrates that MUL exhibits extensive capabilities and limitations, particularly in native AI applications."],"url":"http://arxiv.org/abs/2406.05945v1","category":"eess.SP"}
{"created":"2024-06-10 00:13:35","title":"SETC: A Vulnerability Telemetry Collection Framework","abstract":"As emerging software vulnerabilities continuously threaten enterprises and Internet services, there is a critical need for improved security research capabilities. This paper introduces the Security Exploit Telemetry Collection (SETC) framework - an automated framework to generate reproducible vulnerability exploit data at scale for robust defensive security research. SETC deploys configurable environments to execute and record rich telemetry of vulnerability exploits within isolated containers. Exploits, vulnerable services, monitoring tools, and logging pipelines are defined via modular JSON configurations and deployed on demand. Compared to current manual processes, SETC enables automated, customizable, and repeatable vulnerability testing to produce diverse security telemetry. This research enables scalable exploit data generation to drive innovations in threat modeling, detection methods, analysis techniques, and remediation strategies. The capabilities of the framework are demonstrated through an example scenario. By addressing key barriers in security data generation, SETC represents a valuable platform to support impactful vulnerability and defensive security research.","sentences":["As emerging software vulnerabilities continuously threaten enterprises and Internet services, there is a critical need for improved security research capabilities.","This paper introduces the Security Exploit Telemetry Collection (SETC) framework - an automated framework to generate reproducible vulnerability exploit data at scale for robust defensive security research.","SETC deploys configurable environments to execute and record rich telemetry of vulnerability exploits within isolated containers.","Exploits, vulnerable services, monitoring tools, and logging pipelines are defined via modular JSON configurations and deployed on demand.","Compared to current manual processes, SETC enables automated, customizable, and repeatable vulnerability testing to produce diverse security telemetry.","This research enables scalable exploit data generation to drive innovations in threat modeling, detection methods, analysis techniques, and remediation strategies.","The capabilities of the framework are demonstrated through an example scenario.","By addressing key barriers in security data generation, SETC represents a valuable platform to support impactful vulnerability and defensive security research."],"url":"http://arxiv.org/abs/2406.05942v1","category":"cs.CR"}
{"created":"2024-06-09 21:58:32","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","abstract":"Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation. For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval. Furthermore, the persona module conducts dynamic persona modeling for both users and agents. The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses. The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks. The code is released at https://github.com/leolee99/LD-Agent.","sentences":["Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs).","Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots.","Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses.","Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving.","In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation.","For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval.","Furthermore, the persona module conducts dynamic persona modeling for both users and agents.","The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses.","The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks.","The code is released at https://github.com/leolee99/LD-Agent."],"url":"http://arxiv.org/abs/2406.05925v1","category":"cs.CL"}
{"created":"2024-06-09 21:12:15","title":"Why Don't Prompt-Based Fairness Metrics Correlate?","abstract":"The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases. In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics. CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively. Our code is available at https://github.com/chandar-lab/CAIRO.","sentences":["The widespread use of large language models has brought up essential questions about the potential biases these models might learn.","This led to the development of several metrics aimed at evaluating and mitigating these biases.","In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts.","Then, we outline six relevant reasons why such a low correlation is observed across existing metrics.","Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics.","CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics.","We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively.","Our code is available at https://github.com/chandar-lab/CAIRO."],"url":"http://arxiv.org/abs/2406.05918v1","category":"cs.CL"}
{"created":"2024-06-09 20:54:58","title":"BD-SAT: High-resolution Land Use Land Cover Dataset & Benchmark Results for Developing Division: Dhaka, BD","abstract":"Land Use Land Cover (LULC) analysis on satellite images using deep learning-based methods is significantly helpful in understanding the geography, socio-economic conditions, poverty levels, and urban sprawl in developing countries. Recent works involve segmentation with LULC classes such as farmland, built-up areas, forests, meadows, water bodies, etc. Training deep learning methods on satellite images requires large sets of images annotated with LULC classes. However, annotated data for developing countries are scarce due to a lack of funding, absence of dedicated residential/industrial/economic zones, a large population, and diverse building materials. BD-SAT provides a high-resolution dataset that includes pixel-by-pixel LULC annotations for Dhaka metropolitan city and surrounding rural/urban areas. Using a strict and standardized procedure, the ground truth is created using Bing satellite imagery with a ground spatial distance of 2.22 meters per pixel. A three-stage, well-defined annotation process has been followed with support from GIS experts to ensure the reliability of the annotations. We performed several experiments to establish benchmark results. The results show that the annotated BD-SAT is sufficient to train large deep learning models with adequate accuracy for five major LULC classes: forest, farmland, built-up areas, water bodies, and meadows.","sentences":["Land Use Land Cover (LULC) analysis on satellite images using deep learning-based methods is significantly helpful in understanding the geography, socio-economic conditions, poverty levels, and urban sprawl in developing countries.","Recent works involve segmentation with LULC classes such as farmland, built-up areas, forests, meadows, water bodies, etc.","Training deep learning methods on satellite images requires large sets of images annotated with LULC classes.","However, annotated data for developing countries are scarce due to a lack of funding, absence of dedicated residential/industrial/economic zones, a large population, and diverse building materials.","BD-SAT provides a high-resolution dataset that includes pixel-by-pixel LULC annotations for Dhaka metropolitan city and surrounding rural/urban areas.","Using a strict and standardized procedure, the ground truth is created using Bing satellite imagery with a ground spatial distance of 2.22 meters per pixel.","A three-stage, well-defined annotation process has been followed with support from GIS experts to ensure the reliability of the annotations.","We performed several experiments to establish benchmark results.","The results show that the annotated BD-SAT is sufficient to train large deep learning models with adequate accuracy for five major LULC classes: forest, farmland, built-up areas, water bodies, and meadows."],"url":"http://arxiv.org/abs/2406.05912v1","category":"cs.CV"}
{"created":"2024-06-09 20:18:58","title":"TTM-RE: Memory-Augmented Document-Level Relation Extraction","abstract":"Document-level relation extraction aims to categorize the association between any two entities within a document. We find that previous methods for document-level relation extraction are ineffective in exploiting the full potential of large amounts of training data with varied noise levels. For example, in the ReDocRED benchmark dataset, state-of-the-art methods trained on the large-scale, lower-quality, distantly supervised training data generally do not perform better than those trained solely on the smaller, high-quality, human-annotated training data. To unlock the full potential of large-scale noisy training data for document-level relation extraction, we propose TTM-RE, a novel approach that integrates a trainable memory module, known as the Token Turing Machine, with a noisy-robust loss function that accounts for the positive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark dataset for document-level relation extraction, reveal that TTM-RE achieves state-of-the-art performance (with an absolute F1 score improvement of over 3%). Ablation studies further illustrate the superiority of TTM-RE in other domains (the ChemDisGene dataset in the biomedical domain) and under highly unlabeled settings.","sentences":["Document-level relation extraction aims to categorize the association between any two entities within a document.","We find that previous methods for document-level relation extraction are ineffective in exploiting the full potential of large amounts of training data with varied noise levels.","For example, in the ReDocRED benchmark dataset, state-of-the-art methods trained on the large-scale, lower-quality, distantly supervised training data generally do not perform better than those trained solely on the smaller, high-quality, human-annotated training data.","To unlock the full potential of large-scale noisy training data for document-level relation extraction, we propose TTM-RE, a novel approach that integrates a trainable memory module, known as the Token Turing Machine, with a noisy-robust loss function that accounts for the positive-unlabeled setting.","Extensive experiments on ReDocRED, a benchmark dataset for document-level relation extraction, reveal that TTM-RE achieves state-of-the-art performance (with an absolute F1 score improvement of over 3%).","Ablation studies further illustrate the superiority of TTM-RE in other domains (the ChemDisGene dataset in the biomedical domain) and under highly unlabeled settings."],"url":"http://arxiv.org/abs/2406.05906v1","category":"cs.CL"}
{"created":"2024-06-09 19:42:25","title":"Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback","abstract":"There is a growing body of work on learning from human feedback to align various aspects of machine learning systems with human values and preferences. We consider the setting of fairness in content moderation, in which human feedback is used to determine how two comments -- referencing different sensitive attribute groups -- should be treated in comparison to one another. With a novel dataset collected from Prolific and MTurk, we find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators. We also demonstrate that demographics mentioned in text have a strong influence on how users perceive individual fairness in moderation. Further, we find that differences also exist in downstream classifiers trained to predict human preferences. Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation.","sentences":["There is a growing body of work on learning from human feedback to align various aspects of machine learning systems with human values and preferences.","We consider the setting of fairness in content moderation, in which human feedback is used to determine how two comments -- referencing different sensitive attribute groups -- should be treated in comparison to one another.","With a novel dataset collected from Prolific and MTurk, we find significant gaps in fairness preferences depending on the race, age, political stance, educational level, and LGBTQ+ identity of annotators.","We also demonstrate that demographics mentioned in text have a strong influence on how users perceive individual fairness in moderation.","Further, we find that differences also exist in downstream classifiers trained to predict human preferences.","Finally, we observe that an ensemble, giving equal weight to classifiers trained on annotations from different demographics, performs better for different demographic intersections; compared to a single classifier that gives equal weight to each annotation."],"url":"http://arxiv.org/abs/2406.05902v1","category":"cs.LG"}
{"created":"2024-06-09 19:35:20","title":"Async Learned User Embeddings for Ads Delivery Optimization","abstract":"User representation is crucial for recommendation systems as it helps to deliver personalized recommendations by capturing user preferences and behaviors in low-dimensional vectors. High-quality user embeddings can capture subtle preferences, enable precise similarity calculations, and adapt to changing preferences over time to maintain relevance. The effectiveness of recommendation systems depends significantly on the quality of user embedding. We propose to asynchronously learn high fidelity user embeddings for billions of users each day from sequence based multimodal user activities in Meta platforms through a Transformer-like large scale feature learning module. The async learned user representations embeddings (ALURE) are further converted to user similarity graphs through graph learning and then combined with user realtime activities to retrieval highly related ads candidates for the entire ads delivery system. Our method shows significant gains in both offline and online experiments.","sentences":["User representation is crucial for recommendation systems as it helps to deliver personalized recommendations by capturing user preferences and behaviors in low-dimensional vectors.","High-quality user embeddings can capture subtle preferences, enable precise similarity calculations, and adapt to changing preferences over time to maintain relevance.","The effectiveness of recommendation systems depends significantly on the quality of user embedding.","We propose to asynchronously learn high fidelity user embeddings for billions of users each day from sequence based multimodal user activities in Meta platforms through a Transformer-like large scale feature learning module.","The async learned user representations embeddings (ALURE) are further converted to user similarity graphs through graph learning and then combined with user realtime activities to retrieval highly related ads candidates for the entire ads delivery system.","Our method shows significant gains in both offline and online experiments."],"url":"http://arxiv.org/abs/2406.05898v1","category":"cs.IR"}
{"created":"2024-06-09 18:59:08","title":"Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach","abstract":"Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.","sentences":["Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible.","This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning.","Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples.","In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network.","The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers.","Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric.","Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models."],"url":"http://arxiv.org/abs/2406.05887v1","category":"cs.LG"}
{"created":"2024-06-09 18:42:27","title":"Revisiting institutional punishment in the $N$-person prisoner's dilemma","abstract":"The conflict between individual and collective interests makes fostering cooperation in human societies a challenging task, requiring drastic measures such as the establishment of sanctioning institutions. These institutions are costly because they have to be maintained regardless of the presence or absence of offenders. Here, we propose realistic improvements to the standard $N$-person prisoner's dilemma formulation with institutional punishment by eliminating overpunishment, requiring a minimum number of contributors to establish the sanctioning institution, and sharing the cost among them once this minimum number is reached. In addition, we focus on large groups or communities for which sanctioning institutions are ubiquitous. Using the replicator equation framework for an infinite population, we find that by sufficiently fining players who fail to contribute either to the public good or to the sanctioning institution, a population of contributors immune to invasion by these free riders can be established, provided that the contributors are sufficiently numerous. In a finite population, we use finite-size scaling to show that, for some parameter settings, demographic noise helps to fixate the strategy that contributes to the public good but not to the sanctioning institution even for infinitely large populations when, somewhat counterintuitively, its proportion in the initial population vanishes with a small power of the population size.","sentences":["The conflict between individual and collective interests makes fostering cooperation in human societies a challenging task, requiring drastic measures such as the establishment of sanctioning institutions.","These institutions are costly because they have to be maintained regardless of the presence or absence of offenders.","Here, we propose realistic improvements to the standard $N$-person prisoner's dilemma formulation with institutional punishment by eliminating overpunishment, requiring a minimum number of contributors to establish the sanctioning institution, and sharing the cost among them once this minimum number is reached.","In addition, we focus on large groups or communities for which sanctioning institutions are ubiquitous.","Using the replicator equation framework for an infinite population, we find that by sufficiently fining players who fail to contribute either to the public good or to the sanctioning institution, a population of contributors immune to invasion by these free riders can be established, provided that the contributors are sufficiently numerous.","In a finite population, we use finite-size scaling to show that, for some parameter settings, demographic noise helps to fixate the strategy that contributes to the public good but not to the sanctioning institution even for infinitely large populations when, somewhat counterintuitively, its proportion in the initial population vanishes with a small power of the population size."],"url":"http://arxiv.org/abs/2406.05884v1","category":"physics.soc-ph"}
{"created":"2024-06-09 18:11:05","title":"Conserving Human Creativity with Evolutionary Generative Algorithms: A Case Study in Music Generation","abstract":"This study explores the application of evolutionary generative algorithms in music production to preserve and enhance human creativity. By integrating human feedback into Differential Evolution algorithms, we produced six songs that were submitted to international record labels, all of which received contract offers. In addition to testing the commercial viability of these methods, this paper examines the long-term implications of content generation using traditional machine learning methods compared with evolutionary algorithms. Specifically, as current generative techniques continue to scale, the potential for computer-generated content to outpace human creation becomes likely. This trend poses a risk of exhausting the pool of human-created training data, potentially forcing generative machine learning models to increasingly depend on their random input functions for generating novel content. In contrast to a future of content generation guided by aimless random functions, our approach allows for individualized creative exploration, ensuring that computer-assisted content generation methods are human-centric and culturally relevant through time.","sentences":["This study explores the application of evolutionary generative algorithms in music production to preserve and enhance human creativity.","By integrating human feedback into Differential Evolution algorithms, we produced six songs that were submitted to international record labels, all of which received contract offers.","In addition to testing the commercial viability of these methods, this paper examines the long-term implications of content generation using traditional machine learning methods compared with evolutionary algorithms.","Specifically, as current generative techniques continue to scale, the potential for computer-generated content to outpace human creation becomes likely.","This trend poses a risk of exhausting the pool of human-created training data, potentially forcing generative machine learning models to increasingly depend on their random input functions for generating novel content.","In contrast to a future of content generation guided by aimless random functions, our approach allows for individualized creative exploration, ensuring that computer-assisted content generation methods are human-centric and culturally relevant through time."],"url":"http://arxiv.org/abs/2406.05873v1","category":"cs.NE"}
{"created":"2024-06-09 18:07:47","title":"STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models","abstract":"Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.","sentences":["Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents.","Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills.","In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment.","These games let the agent hone their skills on a predefined set of tasks.","We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision.","Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can.","These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL."],"url":"http://arxiv.org/abs/2406.05872v1","category":"cs.LG"}
{"created":"2024-06-09 17:44:17","title":"Procrastination Is All You Need: Exponent Indexed Accumulators for Floating Point, Posits and Logarithmic Numbers","abstract":"This paper discusses a simple and effective method for the summation of long sequences of floating point numbers. The method comprises two phases: an accumulation phase where the mantissas of the floating point numbers are added to accumulators indexed by the exponents and a reconstruction phase where the actual summation result is finalised. Various architectural details are given for both FPGAs and ASICs including fusing the operation with a multiplier, creating efficient MACs. Some results are presented for FPGAs, including a tensor core capable of multiplying and accumulating two 4x4 matrices of bfloat16 values every clock cycle using ~6,400 LUTs + 64 DSP48 in AMD FPGAs at 700+ MHz. The method is then extended to posits and logarithmic numbers.","sentences":["This paper discusses a simple and effective method for the summation of long sequences of floating point numbers.","The method comprises two phases: an accumulation phase where the mantissas of the floating point numbers are added to accumulators indexed by the exponents and a reconstruction phase where the actual summation result is finalised.","Various architectural details are given for both FPGAs and ASICs including fusing the operation with a multiplier, creating efficient MACs.","Some results are presented for FPGAs, including a tensor core capable of multiplying and accumulating two 4x4 matrices of bfloat16 values every clock cycle using ~6,400 LUTs + 64 DSP48 in AMD FPGAs at 700+ MHz.","The method is then extended to posits and logarithmic numbers."],"url":"http://arxiv.org/abs/2406.05866v1","category":"cs.CV"}
{"created":"2024-06-09 17:25:47","title":"II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models","abstract":"The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model's higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/m-a-p/II-Bench.","sentences":["The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks.","In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs.","However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs.","To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model's higher-order perception of images.","Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings.","Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench.","The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%.","Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details.","Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts.","This observation underscores a notable deficiency in their inherent understanding of image sentiment.","We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI).","II-Bench is publicly available at https://huggingface.co/datasets/m-a-p/II-Bench."],"url":"http://arxiv.org/abs/2406.05862v1","category":"cs.CL"}
{"created":"2024-06-09 16:58:19","title":"Self-Distilled Disentangled Learning for Counterfactual Prediction","abstract":"The advancements in disentangled representation learning significantly enhance the accuracy of counterfactual predictions by granting precise control over instrumental variables, confounders, and adjustable variables. An appealing method for achieving the independent separation of these factors is mutual information minimization, a task that presents challenges in numerous machine learning scenarios, especially within high-dimensional spaces. To circumvent this challenge, we propose the Self-Distilled Disentanglement framework, referred to as $SD^2$. Grounded in information theory, it ensures theoretically sound independent disentangled representations without intricate mutual information estimator designs for high-dimensional representations. Our comprehensive experiments, conducted on both synthetic and real-world datasets, confirms the effectiveness of our approach in facilitating counterfactual inference in the presence of both observed and unobserved confounders.","sentences":["The advancements in disentangled representation learning significantly enhance the accuracy of counterfactual predictions by granting precise control over instrumental variables, confounders, and adjustable variables.","An appealing method for achieving the independent separation of these factors is mutual information minimization, a task that presents challenges in numerous machine learning scenarios, especially within high-dimensional spaces.","To circumvent this challenge, we propose the Self-Distilled Disentanglement framework, referred to as $SD^2$. Grounded in information theory, it ensures theoretically sound independent disentangled representations without intricate mutual information estimator designs for high-dimensional representations.","Our comprehensive experiments, conducted on both synthetic and real-world datasets, confirms the effectiveness of our approach in facilitating counterfactual inference in the presence of both observed and unobserved confounders."],"url":"http://arxiv.org/abs/2406.05855v1","category":"cs.LG"}
{"created":"2024-06-09 16:56:01","title":"Can market volumes reveal traders' rationality and a new risk premium?","abstract":"An empirical analysis, suggested by optimal Merton dynamics, reveals some unexpected features of asset volumes. These features are connected to traders' belief and risk aversion. This paper proposes a trading strategy model in the optimal Merton framework that is representative of the collective behavior of heterogeneous rational traders. This model allows for the estimation of the average risk aversion of traders acting on a specific risky asset, while revealing the existence of a price of risk closely related to market price of risk and volume rate. The empirical analysis, conducted on real data, confirms the validity of the proposed model.","sentences":["An empirical analysis, suggested by optimal Merton dynamics, reveals some unexpected features of asset volumes.","These features are connected to traders' belief and risk aversion.","This paper proposes a trading strategy model in the optimal Merton framework that is representative of the collective behavior of heterogeneous rational traders.","This model allows for the estimation of the average risk aversion of traders acting on a specific risky asset, while revealing the existence of a price of risk closely related to market price of risk and volume rate.","The empirical analysis, conducted on real data, confirms the validity of the proposed model."],"url":"http://arxiv.org/abs/2406.05854v1","category":"q-fin.TR"}
{"created":"2024-06-09 16:10:01","title":"The Concept of Statistical Evidence: Historical Roots and Current Developments","abstract":"One can argue that one of the main roles of the subject of statistics is to characterize what the evidence in collected data says about questions of scientific interest. There are two broad questions that we will refer to as the estimation question and the hypothesis assessment question. For estimation, the evidence in the data should determine a particular value of an object of interest together with a measure of the accuracy of the estimate, while for hypothesis assessment, the evidence in the data should provide evidence in favor of or against some hypothesized value of the object of interest together with a measure of the strength of the evidence. This will be referred to as the evidential approach to statistical reasoning which can be contrasted with the behavioristic or decision-theoretic approach where the notion of loss is introduced and the goal is to minimize expected losses. While the two approaches often lead to similar outcomes, this is not always the case and it is commonly argued that the evidential approach is more suited to scientific applications. This paper traces the history of the evidential approach and summarizes current developments.","sentences":["One can argue that one of the main roles of the subject of statistics is to characterize what the evidence in collected data says about questions of scientific interest.","There are two broad questions that we will refer to as the estimation question and the hypothesis assessment question.","For estimation, the evidence in the data should determine a particular value of an object of interest together with a measure of the accuracy of the estimate, while for hypothesis assessment, the evidence in the data should provide evidence in favor of or against some hypothesized value of the object of interest together with a measure of the strength of the evidence.","This will be referred to as the evidential approach to statistical reasoning which can be contrasted with the behavioristic or decision-theoretic approach where the notion of loss is introduced and the goal is to minimize expected losses.","While the two approaches often lead to similar outcomes, this is not always the case and it is commonly argued that the evidential approach is more suited to scientific applications.","This paper traces the history of the evidential approach and summarizes current developments."],"url":"http://arxiv.org/abs/2406.05843v1","category":"math.ST"}
{"created":"2024-06-09 16:00:00","title":"MaLa-ASR: Multimedia-Assisted LLM-Based ASR","abstract":"As more and more information-rich data like video become available, utilizing multi-modal auxiliary information to enhance audio tasks has sparked widespread research interest. The recent surge in research on LLM-based audio models provides fresh perspectives for tackling audio tasks. Given that LLM can flexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model that can integrate textual keywords extracted from presentation slides to improve recognition of conference content. MaLa-ASR yields average WERs of 9.4% and 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a significant relative WER drop of 27.9% and 44.7% over the baseline model reported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in speech tasks and the capability to integrate auxiliary information conveniently. By adding keywords to the input prompt, the biased word error rate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on this dataset.","sentences":["As more and more information-rich data like video become available, utilizing multi-modal auxiliary information to enhance audio tasks has sparked widespread research interest.","The recent surge in research on LLM-based audio models provides fresh perspectives for tackling audio tasks.","Given that LLM can flexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model that can integrate textual keywords extracted from presentation slides to improve recognition of conference content.","MaLa-ASR yields average WERs of 9.4% and 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a significant relative WER drop of 27.9% and 44.7% over the baseline model reported in SlideSpeech.","MaLa-ASR underscores LLM's strong performance in speech tasks and the capability to integrate auxiliary information conveniently.","By adding keywords to the input prompt, the biased word error rate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on this dataset."],"url":"http://arxiv.org/abs/2406.05839v1","category":"eess.AS"}
{"created":"2024-06-09 15:56:35","title":"Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic Segmentation","abstract":"In this report, we present our solution for the semantic segmentation in adverse weather, in UG2+ Challenge at CVPR 2024. To achieve robust and accurate segmentation results across various weather conditions, we initialize the InternImage-H backbone with pre-trained weights from the large-scale joint dataset and enhance it with the state-of-the-art Upernet segmentation method. Specifically, we utilize offline and online data augmentation approaches to extend the train set, which helps us to further improve the performance of the segmenter. As a result, our proposed solution demonstrates advanced performance on the test set and achieves 3rd position in this challenge.","sentences":["In this report, we present our solution for the semantic segmentation in adverse weather, in UG2+ Challenge at CVPR 2024.","To achieve robust and accurate segmentation results across various weather conditions, we initialize the InternImage-H backbone with pre-trained weights from the large-scale joint dataset and enhance it with the state-of-the-art Upernet segmentation method.","Specifically, we utilize offline and online data augmentation approaches to extend the train set, which helps us to further improve the performance of the segmenter.","As a result, our proposed solution demonstrates advanced performance on the test set and achieves 3rd position in this challenge."],"url":"http://arxiv.org/abs/2406.05837v1","category":"cs.CV"}
{"created":"2024-06-09 15:56:21","title":"Spatial resolution improvements with finer-pitch GEMs","abstract":"Gas Electron Multipliers (GEMs) are used in many particle physics experiments, employing their 'standard' configuration with amplification holes of 140 um pitch in a hexagonal pattern. However, the collection of the charge cloud from the primary ionisation electrons from the drift region of the detector into the GEM holes affects the position information from the initial interacting particle. In this paper, the results from studies with a triple-GEM detector with an X-Y-strip readout anode are presented. It is demonstrated that GEMs with a finer hole pitch of here 90 um improve the detector's spatial resolution. Within these studies, also the impact of the front-end electronics on the spatial resolution was investigated, which is briefly discussed in the paper.","sentences":["Gas Electron Multipliers (GEMs) are used in many particle physics experiments, employing their 'standard' configuration with amplification holes of 140 um pitch in a hexagonal pattern.","However, the collection of the charge cloud from the primary ionisation electrons from the drift region of the detector into the GEM holes affects the position information from the initial interacting particle.","In this paper, the results from studies with a triple-GEM detector with an X-Y-strip readout anode are presented.","It is demonstrated that GEMs with a finer hole pitch of here 90 um improve the detector's spatial resolution.","Within these studies, also the impact of the front-end electronics on the spatial resolution was investigated, which is briefly discussed in the paper."],"url":"http://arxiv.org/abs/2406.05836v1","category":"physics.ins-det"}
{"created":"2024-06-09 15:35:49","title":"Multi-Stain Multi-Level Convolutional Network for Multi-Tissue Breast Cancer Image Segmentation","abstract":"Digital pathology and microscopy image analysis are widely employed in the segmentation of digitally scanned IHC slides, primarily to identify cancer and pinpoint regions of interest (ROI) indicative of tumor presence. However, current ROI segmentation models are either stain-specific or suffer from the issues of stain and scanner variance due to different staining protocols or modalities across multiple labs. Also, tissues like Ductal Carcinoma in Situ (DCIS), acini, etc. are often classified as Tumors due to their structural similarities and color compositions. In this paper, we proposed a novel convolutional neural network (CNN) based Multi-class Tissue Segmentation model for histopathology whole-slide Breast slides which classify tumors and segments other tissue regions such as Ducts, acini, DCIS, Squamous epithelium, Blood Vessels, Necrosis, etc. as a separate class. Our unique pixel-aligned non-linear merge across spatial resolutions empowers models with both local and global fields of view for accurate detection of various classes. Our proposed model is also able to separate bad regions such as folds, artifacts, blurry regions, bubbles, etc. from tissue regions using multi-level context from different resolutions of WSI. Multi-phase iterative training with context-aware augmentation and increasing noise was used to efficiently train a multi-stain generic model with partial and noisy annotations from 513 slides. Our training pipeline used 12 million patches generated using context-aware augmentations which made our model stain and scanner invariant across data sources. To extrapolate stain and scanner invariance, our model was evaluated on 23000 patches which were for a completely new stain (Hematoxylin and Eosin) from a completely new scanner (Motic) from a different lab. The mean IOU was 0.72 which is on par with model performance on other data sources and scanners.","sentences":["Digital pathology and microscopy image analysis are widely employed in the segmentation of digitally scanned IHC slides, primarily to identify cancer and pinpoint regions of interest (ROI) indicative of tumor presence.","However, current ROI segmentation models are either stain-specific or suffer from the issues of stain and scanner variance due to different staining protocols or modalities across multiple labs.","Also, tissues like Ductal Carcinoma in Situ (DCIS), acini, etc. are often classified as Tumors due to their structural similarities and color compositions.","In this paper, we proposed a novel convolutional neural network (CNN) based Multi-class Tissue Segmentation model for histopathology whole-slide Breast slides which classify tumors and segments other tissue regions such as Ducts, acini, DCIS, Squamous epithelium, Blood Vessels, Necrosis, etc. as a separate class.","Our unique pixel-aligned non-linear merge across spatial resolutions empowers models with both local and global fields of view for accurate detection of various classes.","Our proposed model is also able to separate bad regions such as folds, artifacts, blurry regions, bubbles, etc. from tissue regions using multi-level context from different resolutions of WSI.","Multi-phase iterative training with context-aware augmentation and increasing noise was used to efficiently train a multi-stain generic model with partial and noisy annotations from 513 slides.","Our training pipeline used 12 million patches generated using context-aware augmentations which made our model stain and scanner invariant across data sources.","To extrapolate stain and scanner invariance, our model was evaluated on 23000 patches which were for a completely new stain (Hematoxylin and Eosin) from a completely new scanner (Motic) from a different lab.","The mean IOU was 0.72 which is on par with model performance on other data sources and scanners."],"url":"http://arxiv.org/abs/2406.05828v1","category":"cs.CV"}
{"created":"2024-06-09 15:35:05","title":"Measurement of the integrated luminosity of the data collected at 3.773 GeV by BESIII from 2021 to 2024","abstract":"We present a measurement of the integrated luminosity of $e^+e^-$ collision data collected with the BESIII detector at the BEPCII collider at a center-of-mass energy of $E_{\\rm cm} = 3.773$~GeV. The integrated luminosities of the data sets taken from December 2021 to June 2022, from November 2022 to June 2023, and from October 2023 to February 2024 are determined to be $4.995 \\pm 0.019$~fb$^{-1}$, $8.157 \\pm 0.031$~fb$^{-1}$, and $4.191 \\pm 0.016$~fb$^{-1}$, respectively, by analyzing large angle Bhabha scattering events. The uncertainties are dominated by systematic effects and the statistical uncertainties are negligible. Our results provide essential input for future analyses and precision measurements.","sentences":["We present a measurement of the integrated luminosity of $e^+e^-$ collision data collected with the BESIII detector at the BEPCII collider at a center-of-mass energy of $E_{\\rm cm} = 3.773$~GeV.","The integrated luminosities of the data sets taken from December 2021 to June 2022, from November 2022 to June 2023, and from October 2023 to February 2024 are determined to be $4.995 \\pm 0.019$~fb$^{-1}$, $8.157 \\pm 0.031$~fb$^{-1}$, and $4.191 \\pm 0.016$~fb$^{-1}$, respectively, by analyzing large angle Bhabha scattering events.","The uncertainties are dominated by systematic effects and the statistical uncertainties are negligible.","Our results provide essential input for future analyses and precision measurements."],"url":"http://arxiv.org/abs/2406.05827v1","category":"hep-ex"}
{"created":"2024-06-09 15:31:00","title":"PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection","abstract":"Deep neural networks are susceptible to backdoor attacks, where adversaries manipulate model predictions by inserting malicious samples into the training data. Currently, there is still a lack of direct filtering methods for identifying suspicious training data to unveil potential backdoor samples. In this paper, we propose a novel method, Prediction Shift Backdoor Detection (PSBD), leveraging an uncertainty-based approach requiring minimal unlabeled clean validation data. PSBD is motivated by an intriguing Prediction Shift (PS) phenomenon, where poisoned models' predictions on clean data often shift away from true labels towards certain other labels with dropout applied during inference, while backdoor samples exhibit less PS. We hypothesize PS results from neuron bias effect, making neurons favor features of certain classes. PSBD identifies backdoor training samples by computing the Prediction Shift Uncertainty (PSU), the variance in probability values when dropout layers are toggled on and off during model inference. Extensive experiments have been conducted to verify the effectiveness and efficiency of PSBD, which achieves state-of-the-art results among mainstream detection methods.","sentences":["Deep neural networks are susceptible to backdoor attacks, where adversaries manipulate model predictions by inserting malicious samples into the training data.","Currently, there is still a lack of direct filtering methods for identifying suspicious training data to unveil potential backdoor samples.","In this paper, we propose a novel method, Prediction Shift Backdoor Detection (PSBD), leveraging an uncertainty-based approach requiring minimal unlabeled clean validation data.","PSBD is motivated by an intriguing Prediction Shift (PS) phenomenon, where poisoned models' predictions on clean data often shift away from true labels towards certain other labels with dropout applied during inference, while backdoor samples exhibit less PS.","We hypothesize PS results from neuron bias effect, making neurons favor features of certain classes.","PSBD identifies backdoor training samples by computing the Prediction Shift Uncertainty (PSU), the variance in probability values when dropout layers are toggled on and off during model inference.","Extensive experiments have been conducted to verify the effectiveness and efficiency of PSBD, which achieves state-of-the-art results among mainstream detection methods."],"url":"http://arxiv.org/abs/2406.05826v1","category":"cs.LG"}
{"created":"2024-06-09 15:08:00","title":"Attention as a Hypernetwork","abstract":"Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training but whose compositions have not. What mechanisms underlie this ability for compositional generalization? By reformulating multi-head attention as a hypernetwork, we reveal that a low-dimensional latent code specifies key-query specific operations. We find empirically that this latent code is highly structured, capturing information about the subtasks performed by the network. Using the framework of attention as a hypernetwork we further propose a simple modification of multi-head linear attention that strengthens the ability for compositional generalization on a range of abstract reasoning tasks. In particular, we introduce a symbolic version of the Raven Progressive Matrices human intelligence test on which we demonstrate how scaling model size and data enables compositional generalization and gives rise to a functionally structured latent code in the transformer.","sentences":["Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training but whose compositions have not.","What mechanisms underlie this ability for compositional generalization?","By reformulating multi-head attention as a hypernetwork, we reveal that a low-dimensional latent code specifies key-query specific operations.","We find empirically that this latent code is highly structured, capturing information about the subtasks performed by the network.","Using the framework of attention as a hypernetwork we further propose a simple modification of multi-head linear attention that strengthens the ability for compositional generalization on a range of abstract reasoning tasks.","In particular, we introduce a symbolic version of the Raven Progressive Matrices human intelligence test on which we demonstrate how scaling model size and data enables compositional generalization and gives rise to a functionally structured latent code in the transformer."],"url":"http://arxiv.org/abs/2406.05816v1","category":"cs.LG"}
{"created":"2024-06-09 15:00:28","title":"Unified Text-to-Image Generation and Retrieval","abstract":"How humans can efficiently and effectively acquire images has always been a perennial question. A typical solution is text-to-image retrieval from an existing database given the text query; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in text-to-image generation have made it possible to produce fancy and diverse visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval and propose a unified framework in the context of Multimodal Large Language Models (MLLMs). Specifically, we first explore the intrinsic discriminative abilities of MLLMs and introduce a generative retrieval method to perform retrieval in a training-free manner. Subsequently, we unify generation and retrieval in an autoregressive generation way and propose an autonomous decision module to choose the best-matched one between generated and retrieved images as the response to the text query. Additionally, we construct a benchmark called TIGeR-Bench, including creative and knowledge-intensive domains, to standardize the evaluation of unified text-to-image generation and retrieval. Extensive experimental results on TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO, demonstrate the superiority and effectiveness of our proposed method.","sentences":["How humans can efficiently and effectively acquire images has always been a perennial question.","A typical solution is text-to-image retrieval from an existing database given the text query; however, the limited database typically lacks creativity.","By contrast, recent breakthroughs in text-to-image generation have made it possible to produce fancy and diverse visual content, but it faces challenges in synthesizing knowledge-intensive images.","In this work, we rethink the relationship between text-to-image generation and retrieval and propose a unified framework in the context of Multimodal Large Language Models (MLLMs).","Specifically, we first explore the intrinsic discriminative abilities of MLLMs and introduce a generative retrieval method to perform retrieval in a training-free manner.","Subsequently, we unify generation and retrieval in an autoregressive generation way and propose an autonomous decision module to choose the best-matched one between generated and retrieved images as the response to the text query.","Additionally, we construct a benchmark called TIGeR-Bench, including creative and knowledge-intensive domains, to standardize the evaluation of unified text-to-image generation and retrieval.","Extensive experimental results on TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO, demonstrate the superiority and effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2406.05814v1","category":"cs.CV"}
{"created":"2024-06-09 14:54:22","title":"Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models","abstract":"Large language models have gained tremendous popularity in domains such as e-commerce, finance, healthcare, and education. Fine-tuning is a common approach to customize an LLM on a domain-specific dataset for a desired downstream task. In this paper, we present a valuable resource for fine-tuning LLMs developed for the Spanish language to perform a variety of tasks such as classification, masked language modeling, clustering, and others. Our resource is a collection of handwritten notary records from the seventeenth century obtained from the National Archives of Argentina. This collection contains a combination of original images and transcribed text (and metadata) of 160+ pages that were handwritten by two notaries, namely, Estenban Agreda de Vergara and Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical evaluation, we demonstrate that our collection can be used to fine-tune Spanish LLMs for tasks such as classification and masked language modeling, and can outperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource will be an invaluable resource for historical text analysis and is publicly available on GitHub.","sentences":["Large language models have gained tremendous popularity in domains such as e-commerce, finance, healthcare, and education.","Fine-tuning is a common approach to customize an LLM on a domain-specific dataset for a desired downstream task.","In this paper, we present a valuable resource for fine-tuning LLMs developed for the Spanish language to perform a variety of tasks such as classification, masked language modeling, clustering, and others.","Our resource is a collection of handwritten notary records from the seventeenth century obtained from the National Archives of Argentina.","This collection contains a combination of original images and transcribed text (and metadata) of 160+ pages that were handwritten by two notaries, namely, Estenban Agreda de Vergara and Nicolas de Valdivia y Brisuela nearly 400 years ago.","Through empirical evaluation, we demonstrate that our collection can be used to fine-tune Spanish LLMs for tasks such as classification and masked language modeling, and can outperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o.","Our resource will be an invaluable resource for historical text analysis and is publicly available on GitHub."],"url":"http://arxiv.org/abs/2406.05812v1","category":"cs.CL"}
{"created":"2024-06-09 14:50:14","title":"The Paradox of Collective Certainty in Science","abstract":"We explore a paradox of collective action and certainty in science wherein the more scientists research together, the less that work contributes to the value of their collective certainty. When scientists address similar problems and share data, methods, and collaborators, their understanding of and trust in their colleagues' research rises, a quality required for scientific advance. This increases the positive reinforcement scientists receive for shared beliefs as they become more dependent on their colleagues' knowledge, interests, and findings. This collective action increases the potential for scientists to reside in epistemic ''bubbles'' that limit their capacity to make new discoveries or have their discoveries generalize. In short, as scientists grow closer, their experience of scientific validity rises as the likelihood of genuine replication falls, creating a trade-off between certainty and truth.","sentences":["We explore a paradox of collective action and certainty in science wherein the more scientists research together, the less that work contributes to the value of their collective certainty.","When scientists address similar problems and share data, methods, and collaborators, their understanding of and trust in their colleagues' research rises, a quality required for scientific advance.","This increases the positive reinforcement scientists receive for shared beliefs as they become more dependent on their colleagues' knowledge, interests, and findings.","This collective action increases the potential for scientists to reside in epistemic ''bubbles'' that limit their capacity to make new discoveries or have their discoveries generalize.","In short, as scientists grow closer, their experience of scientific validity rises as the likelihood of genuine replication falls, creating a trade-off between certainty and truth."],"url":"http://arxiv.org/abs/2406.05809v1","category":"physics.soc-ph"}
{"created":"2024-06-09 14:43:06","title":"Toward identifiability of total effects in summary causal graphs with latent confounders: an extension of the front-door criterion","abstract":"Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine if it is possible to identify these effects from observational data. Identifying total effects in fully specified non-temporal causal graphs has garnered considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of hidden confounding and when no variable set is sufficient for adjustment, contributing to the ongoing effort to understand and estimate causal effects from observational data using summary causal graphs.","sentences":["Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations.","As an alternative, researchers often rely on causal graphs to determine if it is possible to identify these effects from observational data.","Identifying total effects in fully specified non-temporal causal graphs has garnered considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment.","However, specifying a complete causal graph is challenging in many domains.","Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time.","This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles.","In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of hidden confounding and when no variable set is sufficient for adjustment, contributing to the ongoing effort to understand and estimate causal effects from observational data using summary causal graphs."],"url":"http://arxiv.org/abs/2406.05805v1","category":"stat.ME"}
{"created":"2024-06-09 14:42:55","title":"A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components","abstract":"Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated agentic workflows, offering improvements over traditional single-path, Chain-of-Thought (CoT) prompting techniques. This survey summarize the common workflows, with the particular focus on LLM-Profiled Components (LMPCs) and ignorance of non-LLM components. The reason behind such exploration is to facilitate a clearer understanding of LLM roles and see how reusabile of the LMPCs.","sentences":["Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated agentic workflows, offering improvements over traditional single-path, Chain-of-Thought (CoT) prompting techniques.","This survey summarize the common workflows, with the particular focus on LLM-Profiled Components (LMPCs) and ignorance of non-LLM components.","The reason behind such exploration is to facilitate a clearer understanding of LLM roles and see how reusabile of the LMPCs."],"url":"http://arxiv.org/abs/2406.05804v1","category":"cs.AI"}
{"created":"2024-06-09 14:33:38","title":"SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention","abstract":"In the domain of large foundation models, the Segment Anything Model (SAM) has gained notable recognition for its exceptional performance in image segmentation. However, tackling the video camouflage object detection (VCOD) task presents a unique challenge. Camouflaged objects typically blend into the background, making them difficult to distinguish in still images. Additionally, ensuring temporal consistency in this context is a challenging problem. As a result, SAM encounters limitations and falls short when applied to the VCOD task. To overcome these challenges, we propose a new method called the SAM Propagation Module (SAM-PM). Our propagation module enforces temporal consistency within SAM by employing spatio-temporal cross-attention mechanisms. Moreover, we exclusively train the propagation module while keeping the SAM network weights frozen, allowing us to integrate task-specific insights with the vast knowledge accumulated by the large model. Our method effectively incorporates temporal consistency and domain-specific expertise into the segmentation network with an addition of less than 1% of SAM's parameters. Extensive experimentation reveals a substantial performance improvement in the VCOD benchmark when compared to the most recent state-of-the-art techniques. Code and pre-trained weights are open-sourced at https://github.com/SpiderNitt/SAM-PM","sentences":["In the domain of large foundation models, the Segment Anything Model (SAM) has gained notable recognition for its exceptional performance in image segmentation.","However, tackling the video camouflage object detection (VCOD) task presents a unique challenge.","Camouflaged objects typically blend into the background, making them difficult to distinguish in still images.","Additionally, ensuring temporal consistency in this context is a challenging problem.","As a result, SAM encounters limitations and falls short when applied to the VCOD task.","To overcome these challenges, we propose a new method called the SAM Propagation Module (SAM-PM).","Our propagation module enforces temporal consistency within SAM by employing spatio-temporal cross-attention mechanisms.","Moreover, we exclusively train the propagation module while keeping the SAM network weights frozen, allowing us to integrate task-specific insights with the vast knowledge accumulated by the large model.","Our method effectively incorporates temporal consistency and domain-specific expertise into the segmentation network with an addition of less than 1% of SAM's parameters.","Extensive experimentation reveals a substantial performance improvement in the VCOD benchmark when compared to the most recent state-of-the-art techniques.","Code and pre-trained weights are open-sourced at https://github.com/SpiderNitt/SAM-PM"],"url":"http://arxiv.org/abs/2406.05802v1","category":"cs.CV"}
{"created":"2024-06-09 14:26:01","title":"Double-RIS-Assisted Orbital Angular Momentum Near-Field Secure Communications","abstract":"To satisfy the various demands of growing devices and services, emerging high-frequency-based technologies promote near-field wireless communications. Therefore, near-field physical layer security has attracted much attention to facilitate the wireless information security against illegitimate eavesdropping. However, highly correlated channels between legitimate transceivers and eavesdroppers of existing multiple-input multiple-output (MIMO) based near-field secure technologies along with the low degrees of freedom significantly limit the enhancement of security results in wireless communications. To significantly increase the secrecy rates of near-field wireless communications, in this paper we propose the double-reconfigurable-intelligent-surface (RIS) assisted orbital angular momentum (OAM) secure scheme, where RISs with few reflecting elements are easily deployed to reconstruct the direct links blocked by obstacles between the legitimate transceivers, mitigate the inter-mode interference caused by the misalignment of legitimate transceivers, and adjust the OAM beams direction to interfere with eavesdroppers. Meanwhile, due to the unique orthogonality among OAM modes, the OAM-based joint index modulation and artificial noise scheme is proposed to weaken the information acquisition by eavesdroppers while increasing the achievable rate with the low cost of legitimate communications. To maximize the secrecy rate of our proposed scheme, we develop the Riemannian manifold conjugate gradient (RMCG)-based alternative optimization (AO) algorithm to jointly optimize the transmit power allocation of OAM modes and phase shifts of double RISs. Numerical results show that our proposed double-RIS-assisted OAM near-field secure scheme outperforms the existing works in terms of the secrecy rate and the eavesdropper's bit error rate.","sentences":["To satisfy the various demands of growing devices and services, emerging high-frequency-based technologies promote near-field wireless communications.","Therefore, near-field physical layer security has attracted much attention to facilitate the wireless information security against illegitimate eavesdropping.","However, highly correlated channels between legitimate transceivers and eavesdroppers of existing multiple-input multiple-output (MIMO) based near-field secure technologies along with the low degrees of freedom significantly limit the enhancement of security results in wireless communications.","To significantly increase the secrecy rates of near-field wireless communications, in this paper we propose the double-reconfigurable-intelligent-surface (RIS) assisted orbital angular momentum (OAM) secure scheme, where RISs with few reflecting elements are easily deployed to reconstruct the direct links blocked by obstacles between the legitimate transceivers, mitigate the inter-mode interference caused by the misalignment of legitimate transceivers, and adjust the OAM beams direction to interfere with eavesdroppers.","Meanwhile, due to the unique orthogonality among OAM modes, the OAM-based joint index modulation and artificial noise scheme is proposed to weaken the information acquisition by eavesdroppers while increasing the achievable rate with the low cost of legitimate communications.","To maximize the secrecy rate of our proposed scheme, we develop the Riemannian manifold conjugate gradient (RMCG)-based alternative optimization (AO) algorithm to jointly optimize the transmit power allocation of OAM modes and phase shifts of double RISs.","Numerical results show that our proposed double-RIS-assisted OAM near-field secure scheme outperforms the existing works in terms of the secrecy rate and the eavesdropper's bit error rate."],"url":"http://arxiv.org/abs/2406.05799v1","category":"eess.SP"}
{"created":"2024-06-09 14:25:09","title":"Hidden Holes: topological aspects of language models","abstract":"We explore the topology of representation manifolds arising in autoregressive neural language models trained on raw text data. In order to study their properties, we introduce tools from computational algebraic topology, which we use as a basis for a measure of topological complexity, that we call perforation.   Using this measure, we study the evolution of topological structure in GPT based large language models across depth and time during training. We then compare these to gated recurrent models, and show that the latter exhibit more topological complexity, with a distinct pattern of changes common to all natural languages but absent from synthetically generated data. The paper presents a detailed analysis of the representation manifolds derived by these models based on studying the shapes of vector clouds induced by them as they are conditioned on sentences from corpora of natural language text.   The methods developed in this paper are novel in the field and based on mathematical apparatus that might be unfamiliar to the target audience. To help with that we introduce the minimum necessary theory, and provide additional visualizations in the appendices.   The main contribution of the paper is a striking observation about the topological structure of the transformer as compared to LSTM based neural architectures. It suggests that further research into mathematical properties of these neural networks is necessary to understand the operation of large transformer language models. We hope this work inspires further explorations in this direction within the NLP community.","sentences":["We explore the topology of representation manifolds arising in autoregressive neural language models trained on raw text data.","In order to study their properties, we introduce tools from computational algebraic topology, which we use as a basis for a measure of topological complexity, that we call perforation.   ","Using this measure, we study the evolution of topological structure in GPT based large language models across depth and time during training.","We then compare these to gated recurrent models, and show that the latter exhibit more topological complexity, with a distinct pattern of changes common to all natural languages but absent from synthetically generated data.","The paper presents a detailed analysis of the representation manifolds derived by these models based on studying the shapes of vector clouds induced by them as they are conditioned on sentences from corpora of natural language text.   ","The methods developed in this paper are novel in the field and based on mathematical apparatus that might be unfamiliar to the target audience.","To help with that we introduce the minimum necessary theory, and provide additional visualizations in the appendices.   ","The main contribution of the paper is a striking observation about the topological structure of the transformer as compared to LSTM based neural architectures.","It suggests that further research into mathematical properties of these neural networks is necessary to understand the operation of large transformer language models.","We hope this work inspires further explorations in this direction within the NLP community."],"url":"http://arxiv.org/abs/2406.05798v1","category":"cs.CL"}
{"created":"2024-06-09 14:20:55","title":"3D-MolT5: Towards Unified 3D Molecule-Text Modeling with 3D Molecular Tokenization","abstract":"The integration of molecule and language has garnered increasing attention in molecular science. Recent advancements in Language Models (LMs) have demonstrated potential for the comprehensive modeling of molecule and language. However, existing works exhibit notable limitations. Most existing works overlook the modeling of 3D information, which is crucial for understanding molecular structures and also functions. While some attempts have been made to leverage external structure encoding modules to inject the 3D molecular information into LMs, there exist obvious difficulties that hinder the integration of molecular structure and language text, such as modality alignment and separate tuning. To bridge this gap, we propose 3D-MolT5, a unified framework designed to model both 1D molecular sequence and 3D molecular structure. The key innovation lies in our methodology for mapping fine-grained 3D substructure representations (based on 3D molecular fingerprints) to a specialized 3D token vocabulary for 3D-MolT5. This 3D structure token vocabulary enables the seamless combination of 1D sequence and 3D structure representations in a tokenized format, allowing 3D-MolT5 to encode molecular sequence (SELFIES), molecular structure, and text sequences within a unified architecture. Alongside, we further introduce 1D and 3D joint pre-training to enhance the model's comprehension of these diverse modalities in a joint representation space and better generalize to various tasks for our foundation model. Through instruction tuning on multiple downstream datasets, our proposed 3D-MolT5 shows superior performance than existing methods in molecular property prediction, molecule captioning, and text-based molecule generation tasks. Our code will be available on GitHub soon.","sentences":["The integration of molecule and language has garnered increasing attention in molecular science.","Recent advancements in Language Models (LMs) have demonstrated potential for the comprehensive modeling of molecule and language.","However, existing works exhibit notable limitations.","Most existing works overlook the modeling of 3D information, which is crucial for understanding molecular structures and also functions.","While some attempts have been made to leverage external structure encoding modules to inject the 3D molecular information into LMs, there exist obvious difficulties that hinder the integration of molecular structure and language text, such as modality alignment and separate tuning.","To bridge this gap, we propose 3D-MolT5, a unified framework designed to model both 1D molecular sequence and 3D molecular structure.","The key innovation lies in our methodology for mapping fine-grained 3D substructure representations (based on 3D molecular fingerprints) to a specialized 3D token vocabulary for 3D-MolT5.","This 3D structure token vocabulary enables the seamless combination of 1D sequence and 3D structure representations in a tokenized format, allowing 3D-MolT5 to encode molecular sequence (SELFIES), molecular structure, and text sequences within a unified architecture.","Alongside, we further introduce 1D and 3D joint pre-training to enhance the model's comprehension of these diverse modalities in a joint representation space and better generalize to various tasks for our foundation model.","Through instruction tuning on multiple downstream datasets, our proposed 3D-MolT5 shows superior performance than existing methods in molecular property prediction, molecule captioning, and text-based molecule generation tasks.","Our code will be available on GitHub soon."],"url":"http://arxiv.org/abs/2406.05797v1","category":"q-bio.BM"}
{"created":"2024-06-09 14:11:19","title":"RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation","abstract":"Retrieval-augmented generation (RAG) frame work is showing state-of-the-art performance on open-domain question answering tasks by referencing external knowledge. However, the RAG system faces challenges with performance degradation when it is fed contexts of low relevance or when the relative relevance among the input contexts is inaccurately assessed. In this work, we propose a RE-RAG framework that injects an explicit context relevance estimator (RE) into the RAG system. RE-RAG re-evaluates the retrieved contexts with the proposed context RE and passes the more relevant contexts along with their measure importance to the generator. To train context RE, we propose an unsupervised learning method, which does not utilize any labeled document ranking data to train the context RE. To examine the efficacy of RE-RAG, we examine its performance on Natural Questions and TriviaQA datasets. RE-RAG achieves on-par performance compared to the FiD variants while utilizing fewer contexts (0.25x). We show that the proposed context RE, which was trained with the T5 model, is also applicable to RAG with LLMs(ChatGPT) by improving the performance on NQ (+6.4EM) and TQA (+2.8EM), respecitvely. Lastly, we display that RE can add interpretability to RAG framework as RE score highly correlates with the RE-RAG accuracy. Consequently, RE can be utilized to filter out unanswerable scenarios where context does not contain answers with 38.9%-51.3% accuracy just by examining a set of retrieved contexts.","sentences":["Retrieval-augmented generation (RAG) frame work is showing state-of-the-art performance on open-domain question answering tasks by referencing external knowledge.","However, the RAG system faces challenges with performance degradation when it is fed contexts of low relevance or when the relative relevance among the input contexts is inaccurately assessed.","In this work, we propose a RE-RAG framework that injects an explicit context relevance estimator (RE) into the RAG system.","RE-RAG re-evaluates the retrieved contexts with the proposed context RE and passes the more relevant contexts along with their measure importance to the generator.","To train context RE, we propose an unsupervised learning method, which does not utilize any labeled document ranking data to train the context RE.","To examine the efficacy of RE-RAG, we examine its performance on Natural Questions and TriviaQA datasets.","RE-RAG achieves on-par performance compared to the FiD variants while utilizing fewer contexts (0.25x).","We show that the proposed context RE, which was trained with the T5 model, is also applicable to RAG with LLMs(ChatGPT) by improving the performance on NQ (+6.4EM) and TQA (+2.8EM), respecitvely.","Lastly, we display that RE can add interpretability to RAG framework as RE score highly correlates with the RE-RAG accuracy.","Consequently, RE can be utilized to filter out unanswerable scenarios where context does not contain answers with 38.9%-51.3% accuracy just by examining a set of retrieved contexts."],"url":"http://arxiv.org/abs/2406.05794v1","category":"cs.CL"}
{"created":"2024-06-09 13:52:12","title":"A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions","abstract":"Text-guided 3D visual grounding (T-3DVG), which aims to locate a specific object that semantically corresponds to a language query from a complicated 3D scene, has drawn increasing attention in the 3D research community over the past few years. Compared to 2D visual grounding, this task presents great potential and challenges due to its closer proximity to the real world and the complexity of data collection and 3D point cloud source processing. In this survey, we attempt to provide a comprehensive overview of the T-3DVG progress, including its fundamental elements, recent research advances, and future research directions. To the best of our knowledge, this is the first systematic survey on the T-3DVG task. Specifically, we first provide a general structure of the T-3DVG pipeline with detailed components in a tutorial style, presenting a complete background overview. Then, we summarize the existing T-3DVG approaches into different categories and analyze their strengths and weaknesses. We also present the benchmark datasets and evaluation metrics to assess their performances. Finally, we discuss the potential limitations of existing T-3DVG and share some insights on several promising research directions. The latest papers are continually collected at https://github.com/liudaizong/Awesome-3D-Visual-Grounding.","sentences":["Text-guided 3D visual grounding (T-3DVG), which aims to locate a specific object that semantically corresponds to a language query from a complicated 3D scene, has drawn increasing attention in the 3D research community over the past few years.","Compared to 2D visual grounding, this task presents great potential and challenges due to its closer proximity to the real world and the complexity of data collection and 3D point cloud source processing.","In this survey, we attempt to provide a comprehensive overview of the T-3DVG progress, including its fundamental elements, recent research advances, and future research directions.","To the best of our knowledge, this is the first systematic survey on the T-3DVG task.","Specifically, we first provide a general structure of the T-3DVG pipeline with detailed components in a tutorial style, presenting a complete background overview.","Then, we summarize the existing T-3DVG approaches into different categories and analyze their strengths and weaknesses.","We also present the benchmark datasets and evaluation metrics to assess their performances.","Finally, we discuss the potential limitations of existing T-3DVG and share some insights on several promising research directions.","The latest papers are continually collected at https://github.com/liudaizong/Awesome-3D-Visual-Grounding."],"url":"http://arxiv.org/abs/2406.05785v1","category":"cs.CV"}
{"created":"2024-06-09 13:35:40","title":"Two-Stage Resource Allocation in Reconfigurable Intelligent Surface Assisted Hybrid Networks via Multi-Player Bandits","abstract":"This paper considers a resource allocation problem where several Internet-of-Things (IoT) devices send data to a base station (BS) with or without the help of the reconfigurable intelligent surface (RIS) assisted cellular network. The objective is to maximize the sum rate of all IoT devices by finding the optimal RIS and spreading factor (SF) for each device. Since these IoT devices lack prior information on the RISs or the channel state information (CSI), a distributed resource allocation framework with low complexity and learning features is required to achieve this goal. Therefore, we model this problem as a two-stage multi-player multi-armed bandit (MPMAB) framework to learn the optimal RIS and SF sequentially. Then, we put forth an exploration and exploitation boosting (E2Boost) algorithm to solve this two-stage MPMAB problem by combining the $\\epsilon$-greedy algorithm, Thompson sampling (TS) algorithm, and non-cooperation game method. We derive an upper regret bound for the proposed algorithm, i.e., $\\mathcal{O}(\\log^{1+\\delta}_2 T)$, increasing logarithmically with the time horizon $T$. Numerical results show that the E2Boost algorithm has the best performance among the existing methods and exhibits a fast convergence rate. More importantly, the proposed algorithm is not sensitive to the number of combinations of the RISs and SFs thanks to the two-stage allocation mechanism, which can benefit high-density networks.","sentences":["This paper considers a resource allocation problem where several Internet-of-Things (IoT) devices send data to a base station (BS) with or without the help of the reconfigurable intelligent surface (RIS) assisted cellular network.","The objective is to maximize the sum rate of all IoT devices by finding the optimal RIS and spreading factor (SF) for each device.","Since these IoT devices lack prior information on the RISs or the channel state information (CSI), a distributed resource allocation framework with low complexity and learning features is required to achieve this goal.","Therefore, we model this problem as a two-stage multi-player multi-armed bandit (MPMAB) framework to learn the optimal RIS and SF sequentially.","Then, we put forth an exploration and exploitation boosting (E2Boost) algorithm to solve this two-stage MPMAB problem by combining the $\\epsilon$-greedy algorithm, Thompson sampling (TS) algorithm, and non-cooperation game method.","We derive an upper regret bound for the proposed algorithm, i.e., $\\mathcal{O}(\\log^{1+\\delta}_2 T)$, increasing logarithmically with the time horizon $T$. Numerical results show that the E2Boost algorithm has the best performance among the existing methods and exhibits a fast convergence rate.","More importantly, the proposed algorithm is not sensitive to the number of combinations of the RISs and SFs thanks to the two-stage allocation mechanism, which can benefit high-density networks."],"url":"http://arxiv.org/abs/2406.05780v1","category":"eess.SP"}
{"created":"2024-06-09 12:55:50","title":"MLCM: Multistep Consistency Distillation of Latent Diffusion Model","abstract":"Distilling large latent diffusion models (LDMs) into ones that are fast to sample from is attracting growing research interest. However, the majority of existing methods face a dilemma where they either (i) depend on multiple individual distilled models for different sampling budgets, or (ii) sacrifice generation quality with limited (e.g., 2-4) and/or moderate (e.g., 5-8) sampling steps. To address these, we extend the recent multistep consistency distillation (MCD) strategy to representative LDMs, establishing the Multistep Latent Consistency Models (MLCMs) approach for low-cost high-quality image synthesis. MLCM serves as a unified model for various sampling steps due to the promise of MCD. We further augment MCD with a progressive training strategy to strengthen inter-segment consistency to boost the quality of few-step generations. We take the states from the sampling trajectories of the teacher model as training data for MLCMs to lift the requirements for high-quality training datasets and to bridge the gap between the training and inference of the distilled model. MLCM is compatible with preference learning strategies for further improvement of visual quality and aesthetic appeal. Empirically, MLCM can generate high-quality, delightful images with only 2-8 sampling steps. On the MSCOCO-2017 5K benchmark, MLCM distilled from SDXL gets a CLIP Score of 33.30, Aesthetic Score of 6.19, and Image Reward of 1.20 with only 4 steps, substantially surpassing 4-step LCM [23], 8-step SDXL-Lightning [17], and 8-step HyperSD [33]. We also demonstrate the versatility of MLCMs in applications including controllable generation, image style transfer, and Chinese-to-image generation.","sentences":["Distilling large latent diffusion models (LDMs) into ones that are fast to sample from is attracting growing research interest.","However, the majority of existing methods face a dilemma where they either (i) depend on multiple individual distilled models for different sampling budgets, or (ii) sacrifice generation quality with limited (e.g., 2-4) and/or moderate (e.g., 5-8) sampling steps.","To address these, we extend the recent multistep consistency distillation (MCD) strategy to representative LDMs, establishing the Multistep Latent Consistency Models (MLCMs) approach for low-cost high-quality image synthesis.","MLCM serves as a unified model for various sampling steps due to the promise of MCD.","We further augment MCD with a progressive training strategy to strengthen inter-segment consistency to boost the quality of few-step generations.","We take the states from the sampling trajectories of the teacher model as training data for MLCMs to lift the requirements for high-quality training datasets and to bridge the gap between the training and inference of the distilled model.","MLCM is compatible with preference learning strategies for further improvement of visual quality and aesthetic appeal.","Empirically, MLCM can generate high-quality, delightful images with only 2-8 sampling steps.","On the MSCOCO-2017 5K benchmark, MLCM distilled from SDXL gets a CLIP Score of 33.30, Aesthetic Score of 6.19, and Image Reward of 1.20 with only 4 steps, substantially surpassing 4-step LCM","[23], 8-step SDXL-Lightning [17], and 8-step HyperSD","[33].","We also demonstrate the versatility of MLCMs in applications including controllable generation, image style transfer, and Chinese-to-image generation."],"url":"http://arxiv.org/abs/2406.05768v1","category":"cs.CV"}
{"created":"2024-06-09 12:41:14","title":"Gentle-CLIP: Exploring Aligned Semantic In Low-Quality Multimodal Data With Soft Alignment","abstract":"Multimodal fusion breaks through the barriers between diverse modalities and has already yielded numerous impressive performances. However, in various specialized fields, it is struggling to obtain sufficient alignment data for the training process, which seriously limits the use of previously elegant models. Thus, semi-supervised learning attempts to achieve multimodal alignment with fewer matched pairs but traditional methods like pseudo-labeling are difficult to apply in domains with no label information. To address these problems, we transform semi-supervised multimodal alignment into a manifold matching problem and propose a new method based on CLIP, named Gentle-CLIP. Specifically, we design a novel semantic density distribution loss to explore implicit semantic alignment information from unpaired multimodal data by constraining the latent representation distribution with fine granularity, thus eliminating the need for numerous strictly matched pairs. Meanwhile, we introduce multi-kernel maximum mean discrepancy as well as self-supervised contrastive loss to pull separate modality distributions closer and enhance the stability of the representation distribution. In addition, the contrastive loss used in CLIP is employed on the supervised matched data to prevent negative optimization. Extensive experiments conducted on a range of tasks in various fields, including protein, remote sensing, and the general vision-language field, demonstrate the effectiveness of our proposed Gentle-CLIP.","sentences":["Multimodal fusion breaks through the barriers between diverse modalities and has already yielded numerous impressive performances.","However, in various specialized fields, it is struggling to obtain sufficient alignment data for the training process, which seriously limits the use of previously elegant models.","Thus, semi-supervised learning attempts to achieve multimodal alignment with fewer matched pairs but traditional methods like pseudo-labeling are difficult to apply in domains with no label information.","To address these problems, we transform semi-supervised multimodal alignment into a manifold matching problem and propose a new method based on CLIP, named Gentle-CLIP.","Specifically, we design a novel semantic density distribution loss to explore implicit semantic alignment information from unpaired multimodal data by constraining the latent representation distribution with fine granularity, thus eliminating the need for numerous strictly matched pairs.","Meanwhile, we introduce multi-kernel maximum mean discrepancy as well as self-supervised contrastive loss to pull separate modality distributions closer and enhance the stability of the representation distribution.","In addition, the contrastive loss used in CLIP is employed on the supervised matched data to prevent negative optimization.","Extensive experiments conducted on a range of tasks in various fields, including protein, remote sensing, and the general vision-language field, demonstrate the effectiveness of our proposed Gentle-CLIP."],"url":"http://arxiv.org/abs/2406.05766v1","category":"cs.LG"}
{"created":"2024-06-09 12:36:38","title":"Global Sensitivity Analysis of Uncertain Parameters in Bayesian Networks","abstract":"Traditionally, the sensitivity analysis of a Bayesian network studies the impact of individually modifying the entries of its conditional probability tables in a one-at-a-time (OAT) fashion. However, this approach fails to give a comprehensive account of each inputs' relevance, since simultaneous perturbations in two or more parameters often entail higher-order effects that cannot be captured by an OAT analysis. We propose to conduct global variance-based sensitivity analysis instead, whereby $n$ parameters are viewed as uncertain at once and their importance is assessed jointly. Our method works by encoding the uncertainties as $n$ additional variables of the network. To prevent the curse of dimensionality while adding these dimensions, we use low-rank tensor decomposition to break down the new potentials into smaller factors. Last, we apply the method of Sobol to the resulting network to obtain $n$ global sensitivity indices. Using a benchmark array of both expert-elicited and learned Bayesian networks, we demonstrate that the Sobol indices can significantly differ from the OAT indices, thus revealing the true influence of uncertain parameters and their interactions.","sentences":["Traditionally, the sensitivity analysis of a Bayesian network studies the impact of individually modifying the entries of its conditional probability tables in a one-at-a-time (OAT) fashion.","However, this approach fails to give a comprehensive account of each inputs' relevance, since simultaneous perturbations in two or more parameters often entail higher-order effects that cannot be captured by an OAT analysis.","We propose to conduct global variance-based sensitivity analysis instead, whereby $n$ parameters are viewed as uncertain at once and their importance is assessed jointly.","Our method works by encoding the uncertainties as $n$ additional variables of the network.","To prevent the curse of dimensionality while adding these dimensions, we use low-rank tensor decomposition to break down the new potentials into smaller factors.","Last, we apply the method of Sobol to the resulting network to obtain $n$ global sensitivity indices.","Using a benchmark array of both expert-elicited and learned Bayesian networks, we demonstrate that the Sobol indices can significantly differ from the OAT indices, thus revealing the true influence of uncertain parameters and their interactions."],"url":"http://arxiv.org/abs/2406.05764v1","category":"cs.AI"}
{"created":"2024-06-09 12:23:14","title":"EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models","abstract":"The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks.However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs.The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective.Experiments expose the insufficient capacity of current LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding.","sentences":["The recent rapid development of Large Vision-Language Models (LVLMs) has indicated their potential for embodied tasks.","However, the critical skill of spatial understanding in embodied environments has not been thoroughly evaluated, leaving the gap between current LVLMs and qualified embodied intelligence unknown.","Therefore, we construct EmbSpatial-Bench, a benchmark for evaluating embodied spatial understanding of LVLMs.","The benchmark is automatically derived from embodied scenes and covers 6 spatial relationships from an egocentric perspective.","Experiments expose the insufficient capacity of current LVLMs (even GPT-4V).","We further present EmbSpatial-SFT, an instruction-tuning dataset designed to improve LVLMs' embodied spatial understanding."],"url":"http://arxiv.org/abs/2406.05756v1","category":"cs.AI"}
{"created":"2024-06-09 12:17:05","title":"Numerical solution of a PDE arising from prediction with expert advice","abstract":"This work investigates the online machine learning problem of prediction with expert advice in an adversarial setting through numerical analysis of, and experiments with, a related partial differential equation. The problem is a repeated two-person game involving decision-making at each step informed by $n$ experts in an adversarial environment. The continuum limit of this game over a large number of steps is a degenerate elliptic equation whose solution encodes the optimal strategies for both players. We develop numerical methods for approximating the solution of this equation in relatively high dimensions ($n\\leq 10$) by exploiting symmetries in the equation and the solution to drastically reduce the size of the computational domain. Based on our numerical results we make a number of conjectures about the optimality of various adversarial strategies, in particular about the non-optimality of the COMB strategy.","sentences":["This work investigates the online machine learning problem of prediction with expert advice in an adversarial setting through numerical analysis of, and experiments with, a related partial differential equation.","The problem is a repeated two-person game involving decision-making at each step informed by $n$ experts in an adversarial environment.","The continuum limit of this game over a large number of steps is a degenerate elliptic equation whose solution encodes the optimal strategies for both players.","We develop numerical methods for approximating the solution of this equation in relatively high dimensions ($n\\leq 10$) by exploiting symmetries in the equation and the solution to drastically reduce the size of the computational domain.","Based on our numerical results we make a number of conjectures about the optimality of various adversarial strategies, in particular about the non-optimality of the COMB strategy."],"url":"http://arxiv.org/abs/2406.05754v1","category":"math.NA"}
{"created":"2024-06-09 12:16:30","title":"Grounding Continuous Representations in Geometry: Equivariant Neural Fields","abstract":"Recently, Neural Fields have emerged as a powerful modelling paradigm to represent continuous signals. In a conditional neural field, a field is represented by a latent variable that conditions the NeF, whose parametrisation is otherwise shared over an entire dataset. We propose Equivariant Neural Fields based on cross attention transformers, in which NeFs are conditioned on a geometric conditioning variable, a latent point cloud, that enables an equivariant decoding from latent to field. Our equivariant approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws if the field transforms, the latent represents transforms accordingly and vice versa. Crucially, the equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weightsharing over spatially similar patterns, allowing for efficient learning of datasets of fields. These main properties are validated using classification experiments and a verification of the capability of fitting entire datasets, in comparison to other non-equivariant NeF approaches. We further validate the potential of ENFs by demonstrate unique local field editing properties.","sentences":["Recently, Neural Fields have emerged as a powerful modelling paradigm to represent continuous signals.","In a conditional neural field, a field is represented by a latent variable that conditions the NeF, whose parametrisation is otherwise shared over an entire dataset.","We propose Equivariant Neural Fields based on cross attention transformers, in which NeFs are conditioned on a geometric conditioning variable, a latent point cloud, that enables an equivariant decoding from latent to field.","Our equivariant approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws if the field transforms, the latent represents transforms accordingly and vice versa.","Crucially, the equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weightsharing over spatially similar patterns, allowing for efficient learning of datasets of fields.","These main properties are validated using classification experiments and a verification of the capability of fitting entire datasets, in comparison to other non-equivariant NeF approaches.","We further validate the potential of ENFs by demonstrate unique local field editing properties."],"url":"http://arxiv.org/abs/2406.05753v1","category":"cs.LG"}
{"created":"2024-06-09 12:06:45","title":"Analysis of Bessel Beam Generation Using MetaMaterials in Photonic Integrated Circuits","abstract":"Bessel beams, known for their unique non-diffracting property, maintain their shape and intensity over long distances, making them invaluable for applications in optical trapping, imaging, and communications. This work presents a comprehensive theoretical analysis of micro-photonic antennas designed to generate Bessel beams within the Terahertz (THz) and optical frequency ranges. The technique is demonstrated by generating far-field patterns of Bessel waves at these frequencies. The design employs metasurface patterns arranged as arrays of concentric rings atop rectangular silicon waveguides, collectively creating a Bessel beam. Dyadic Greens function integral equation techniques are used to model the transverse electric (TE) and transverse magnetic (TM) fields in the metasurface radiation zone. Utilizing orthogonal vector wave functions, Bloch theorem, Floquet harmonics, and the transverse resonance technique, a photonic chip is designed to achieve a non-diffracting range of 500 um at the optical telecom wavelength of 1.5 um for a metasurface radius of 25 um. A radiation efficiency exceeding 80% is achieved by optimizing the attenuation constant (alpha) along the structure. The theoretical models are validated through simulations for both optical 1.5 um and Terahertz 14 um wavelengths, demonstrating significant alignment between predictions and simulation results.","sentences":["Bessel beams, known for their unique non-diffracting property, maintain their shape and intensity over long distances, making them invaluable for applications in optical trapping, imaging, and communications.","This work presents a comprehensive theoretical analysis of micro-photonic antennas designed to generate Bessel beams within the Terahertz (THz) and optical frequency ranges.","The technique is demonstrated by generating far-field patterns of Bessel waves at these frequencies.","The design employs metasurface patterns arranged as arrays of concentric rings atop rectangular silicon waveguides, collectively creating a Bessel beam.","Dyadic Greens function integral equation techniques are used to model the transverse electric (TE) and transverse magnetic (TM) fields in the metasurface radiation zone.","Utilizing orthogonal vector wave functions, Bloch theorem, Floquet harmonics, and the transverse resonance technique, a photonic chip is designed to achieve a non-diffracting range of 500 um at the optical telecom wavelength of 1.5 um for a metasurface radius of 25 um.","A radiation efficiency exceeding 80% is achieved by optimizing the attenuation constant (alpha) along the structure.","The theoretical models are validated through simulations for both optical 1.5 um and Terahertz 14 um wavelengths, demonstrating significant alignment between predictions and simulation results."],"url":"http://arxiv.org/abs/2406.05751v1","category":"physics.optics"}
{"created":"2024-06-09 11:37:45","title":"Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance","abstract":"AI-aided clinical diagnosis is desired in medical care. Existing deep learning models lack explainability and mainly focus on image analysis. The recently developed Dynamic Uncertain Causality Graph (DUCG) approach is causality-driven, explainable, and invariant across different application scenarios, without problems of data collection, labeling, fitting, privacy, bias, generalization, high cost and high energy consumption. Through close collaboration between clinical experts and DUCG technicians, 46 DUCG models covering 54 chief complaints were constructed. Over 1,000 diseases can be diagnosed without triage. Before being applied in real-world, the 46 DUCG models were retrospectively verified by third-party hospitals. The verified diagnostic precisions were no less than 95%, in which the diagnostic precision for every disease including uncommon ones was no less than 80%. After verifications, the 46 DUCG models were applied in the real-world in China. Over one million real diagnosis cases have been performed, with only 17 incorrect diagnoses identified. Due to DUCG's transparency, the mistakes causing the incorrect diagnoses were found and corrected. The diagnostic abilities of the clinicians who applied DUCG frequently were improved significantly. Following the introduction to the earlier presented DUCG methodology, the recommendation algorithm for potential medical checks is presented and the key idea of DUCG is extracted.","sentences":["AI-aided clinical diagnosis is desired in medical care.","Existing deep learning models lack explainability and mainly focus on image analysis.","The recently developed Dynamic Uncertain Causality Graph (DUCG) approach is causality-driven, explainable, and invariant across different application scenarios, without problems of data collection, labeling, fitting, privacy, bias, generalization, high cost and high energy consumption.","Through close collaboration between clinical experts and DUCG technicians, 46 DUCG models covering 54 chief complaints were constructed.","Over 1,000 diseases can be diagnosed without triage.","Before being applied in real-world, the 46 DUCG models were retrospectively verified by third-party hospitals.","The verified diagnostic precisions were no less than 95%, in which the diagnostic precision for every disease including uncommon ones was no less than 80%.","After verifications, the 46 DUCG models were applied in the real-world in China.","Over one million real diagnosis cases have been performed, with only 17 incorrect diagnoses identified.","Due to DUCG's transparency, the mistakes causing the incorrect diagnoses were found and corrected.","The diagnostic abilities of the clinicians who applied DUCG frequently were improved significantly.","Following the introduction to the earlier presented DUCG methodology, the recommendation algorithm for potential medical checks is presented and the key idea of DUCG is extracted."],"url":"http://arxiv.org/abs/2406.05746v1","category":"cs.AI"}
{"created":"2024-06-09 11:36:36","title":"Structured Learning of Compositional Sequential Interventions","abstract":"We consider sequential treatment regimes where each unit is exposed to combinations of interventions over time. When interventions are described by qualitative labels, such as ``close schools for a month due to a pandemic'' or ``promote this podcast to this user during this week'', it is unclear which appropriate structural assumptions allow us to generalize behavioral predictions to previously unseen combinatorial sequences. Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces. To approach that, we pose an explicit model for \\emph{composition}, that is, how the effect of sequential interventions can be isolated into modules, clarifying which data conditions allow for the identification of their combined effect at different units and time steps. We show the identification properties of our compositional model, inspired by advances in causal matrix factorization methods but focusing on predictive models for novel compositions of interventions instead of matrix completion tasks and causal effect estimation. We compare our approach to flexible but generic black-box models to illustrate how structure aids prediction in sparse data conditions.","sentences":["We consider sequential treatment regimes where each unit is exposed to combinations of interventions over time.","When interventions are described by qualitative labels, such as ``close schools for a month due to a pandemic'' or ``promote this podcast to this user during this week'', it is unclear which appropriate structural assumptions allow us to generalize behavioral predictions to previously unseen combinatorial sequences.","Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces.","To approach that, we pose an explicit model for \\emph{composition}, that is, how the effect of sequential interventions can be isolated into modules, clarifying which data conditions allow for the identification of their combined effect at different units and time steps.","We show the identification properties of our compositional model, inspired by advances in causal matrix factorization methods but focusing on predictive models for novel compositions of interventions instead of matrix completion tasks and causal effect estimation.","We compare our approach to flexible but generic black-box models to illustrate how structure aids prediction in sparse data conditions."],"url":"http://arxiv.org/abs/2406.05745v1","category":"stat.ML"}
{"created":"2024-06-09 11:16:11","title":"Digital Business Model Analysis Using a Large Language Model","abstract":"Digital transformation (DX) has recently become a pressing issue for many companies as the latest digital technologies, such as artificial intelligence and the Internet of Things, can be easily utilized. However, devising new business models is not easy for compa-nies, though they can improve their operations through digital technologies. Thus, business model design support methods are needed by people who lack digital tech-nology expertise. In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily. A business model design support system that utilizes these technologies has great potential. However, research on this area is scant. Accordingly, this study proposes an LLM-based method for comparing and analyzing similar companies from different business do-mains as a first step toward business model design support utilizing LLMs. This method can support idea generation in digital business model design.","sentences":["Digital transformation (DX) has recently become a pressing issue for many companies as the latest digital technologies, such as artificial intelligence and the Internet of Things, can be easily utilized.","However, devising new business models is not easy for compa-nies, though they can improve their operations through digital technologies.","Thus, business model design support methods are needed by people who lack digital tech-nology expertise.","In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily.","A business model design support system that utilizes these technologies has great potential.","However, research on this area is scant.","Accordingly, this study proposes an LLM-based method for comparing and analyzing similar companies from different business do-mains as a first step toward business model design support utilizing LLMs.","This method can support idea generation in digital business model design."],"url":"http://arxiv.org/abs/2406.05741v1","category":"cs.OH"}
{"created":"2024-06-09 11:11:17","title":"Search for a resonance decaying to a W boson and a photon in proton-proton collisions at $\\sqrt{s}$ = 13 TeV using leptonic W boson decays","abstract":"A search for a new charged particle X with mass between 0.3 and 2.0 TeV decaying to a W boson and a photon is presented, using proton-proton collision data at a center-of-mass energy of 13 TeV, collected by the CMS experiment and corresponding to an integrated luminosity of 138 fb$^{-1}$. Particle X has electric charge $\\pm$1 and is assumed to have spin 0. The search is performed using the electron and muon decays of the W boson. No significant excess above the predicted background is observed. The upper limit at 95% confidence level on the product of the production cross section of the X and its branching fraction to a W boson and a photon is found to be 94 (137) fb for a 0.3 TeV resonance and 0.75 (0.81) fb for a 2.0 TeV resonance, for an X width-to-mass ratio of 0.01% (5%). This search presents the most stringent constraints to date on the existence of such resonances across the probed mass range. A statistical combination with an earlier study based on the hadronic decay mode of the W boson is also performed, and the upper limit at 95% confidence level for a 2.0 TeV resonance is reduced to 0.50 (0.63) fb for an X width-to-mass ratio of 0.01% (5%).","sentences":["A search for a new charged particle X with mass between 0.3 and 2.0 TeV decaying to a W boson and a photon is presented, using proton-proton collision data at a center-of-mass energy of 13 TeV, collected by the CMS experiment and corresponding to an integrated luminosity of 138 fb$^{-1}$. Particle X has electric charge $\\pm$1 and is assumed to have spin 0.","The search is performed using the electron and muon decays of the W boson.","No significant excess above the predicted background is observed.","The upper limit at 95% confidence level on the product of the production cross section of the X and its branching fraction to a W boson and a photon is found to be 94 (137) fb for a 0.3 TeV resonance and 0.75 (0.81) fb for a 2.0 TeV resonance, for an X width-to-mass ratio of 0.01% (5%).","This search presents the most stringent constraints to date on the existence of such resonances across the probed mass range.","A statistical combination with an earlier study based on the hadronic decay mode of the W boson is also performed, and the upper limit at 95% confidence level for a 2.0 TeV resonance is reduced to 0.50 (0.63) fb for an X width-to-mass ratio of 0.01% (5%)."],"url":"http://arxiv.org/abs/2406.05737v1","category":"hep-ex"}
{"created":"2024-06-09 10:31:26","title":"Deception Analysis with Artificial Intelligence: An Interdisciplinary Perspective","abstract":"Humans and machines interact more frequently than ever and our societies are becoming increasingly hybrid. A consequence of this hybridisation is the degradation of societal trust due to the prevalence of AI-enabled deception. Yet, despite our understanding of the role of trust in AI in the recent years, we still do not have a computational theory to be able to fully understand and explain the role deception plays in this context. This is a problem because while our ability to explain deception in hybrid societies is delayed, the design of AI agents may keep advancing towards fully autonomous deceptive machines, which would pose new challenges to dealing with deception. In this paper we build a timely and meaningful interdisciplinary perspective on deceptive AI and reinforce a 20 year old socio-cognitive perspective on trust and deception, by proposing the development of DAMAS -- a holistic Multi-Agent Systems (MAS) framework for the socio-cognitive modelling and analysis of deception. In a nutshell this paper covers the topic of modelling and explaining deception using AI approaches from the perspectives of Computer Science, Philosophy, Psychology, Ethics, and Intelligence Analysis.","sentences":["Humans and machines interact more frequently than ever and our societies are becoming increasingly hybrid.","A consequence of this hybridisation is the degradation of societal trust due to the prevalence of AI-enabled deception.","Yet, despite our understanding of the role of trust in AI in the recent years, we still do not have a computational theory to be able to fully understand and explain the role deception plays in this context.","This is a problem because while our ability to explain deception in hybrid societies is delayed, the design of AI agents may keep advancing towards fully autonomous deceptive machines, which would pose new challenges to dealing with deception.","In this paper we build a timely and meaningful interdisciplinary perspective on deceptive AI and reinforce a 20 year old socio-cognitive perspective on trust and deception, by proposing the development of DAMAS -- a holistic Multi-Agent Systems (MAS) framework for the socio-cognitive modelling and analysis of deception.","In a nutshell this paper covers the topic of modelling and explaining deception using AI approaches from the perspectives of Computer Science, Philosophy, Psychology, Ethics, and Intelligence Analysis."],"url":"http://arxiv.org/abs/2406.05724v1","category":"cs.MA"}
{"created":"2024-06-09 10:21:47","title":"VillagerAgent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in Minecraft","abstract":"In this paper, we aim to evaluate multi-agent systems against complex dependencies, including spatial, causal, and temporal constraints. First, we construct a new benchmark, named VillagerBench, within the Minecraft environment.VillagerBench comprises diverse tasks crafted to test various aspects of multi-agent collaboration, from workload distribution to dynamic adaptation and synchronized task execution. Second, we introduce a Directed Acyclic Graph Multi-Agent Framework VillagerAgent to resolve complex inter-agent dependencies and enhance collaborative efficiency. This solution incorporates a task decomposer that creates a directed acyclic graph (DAG) for structured task management, an agent controller for task distribution, and a state manager for tracking environmental and agent data. Our empirical evaluation on VillagerBench demonstrates that VillagerAgent outperforms the existing AgentVerse model, reducing hallucinations and improving task decomposition efficacy. The results underscore VillagerAgent's potential in advancing multi-agent collaboration, offering a scalable and generalizable solution in dynamic environments. The source code is open-source on GitHub (https://github.com/cnsdqd-dyb/VillagerAgent).","sentences":["In this paper, we aim to evaluate multi-agent systems against complex dependencies, including spatial, causal, and temporal constraints.","First, we construct a new benchmark, named VillagerBench, within the Minecraft environment.","VillagerBench comprises diverse tasks crafted to test various aspects of multi-agent collaboration, from workload distribution to dynamic adaptation and synchronized task execution.","Second, we introduce a Directed Acyclic Graph Multi-Agent Framework VillagerAgent to resolve complex inter-agent dependencies and enhance collaborative efficiency.","This solution incorporates a task decomposer that creates a directed acyclic graph (DAG) for structured task management, an agent controller for task distribution, and a state manager for tracking environmental and agent data.","Our empirical evaluation on VillagerBench demonstrates that VillagerAgent outperforms the existing AgentVerse model, reducing hallucinations and improving task decomposition efficacy.","The results underscore VillagerAgent's potential in advancing multi-agent collaboration, offering a scalable and generalizable solution in dynamic environments.","The source code is open-source on GitHub (https://github.com/cnsdqd-dyb/VillagerAgent)."],"url":"http://arxiv.org/abs/2406.05720v1","category":"cs.AI"}
{"created":"2024-06-09 09:55:04","title":"TR2MTL: LLM based framework for Metric Temporal Logic Formalization of Traffic Rules","abstract":"Traffic rules formalization is crucial for verifying the compliance and safety of autonomous vehicles (AVs). However, manual translation of natural language traffic rules as formal specification requires domain knowledge and logic expertise, which limits its adaptation. This paper introduces TR2MTL, a framework that employs large language models (LLMs) to automatically translate traffic rules (TR) into metric temporal logic (MTL). It is envisioned as a human-in-loop system for AV rule formalization. It utilizes a chain-of-thought in-context learning approach to guide the LLM in step-by-step translation and generating valid and grammatically correct MTL formulas. It can be extended to various forms of temporal logic and rules. We evaluated the framework on a challenging dataset of traffic rules we created from various sources and compared it against LLMs using different in-context learning methods. Results show that TR2MTL is domain-agnostic, achieving high accuracy and generalization capability even with a small dataset. Moreover, the method effectively predicts formulas with varying degrees of logical and semantic structure in unstructured traffic rules.","sentences":["Traffic rules formalization is crucial for verifying the compliance and safety of autonomous vehicles (AVs).","However, manual translation of natural language traffic rules as formal specification requires domain knowledge and logic expertise, which limits its adaptation.","This paper introduces TR2MTL, a framework that employs large language models (LLMs) to automatically translate traffic rules (TR) into metric temporal logic (MTL).","It is envisioned as a human-in-loop system for AV rule formalization.","It utilizes a chain-of-thought in-context learning approach to guide the LLM in step-by-step translation and generating valid and grammatically correct MTL formulas.","It can be extended to various forms of temporal logic and rules.","We evaluated the framework on a challenging dataset of traffic rules we created from various sources and compared it against LLMs using different in-context learning methods.","Results show that TR2MTL is domain-agnostic, achieving high accuracy and generalization capability even with a small dataset.","Moreover, the method effectively predicts formulas with varying degrees of logical and semantic structure in unstructured traffic rules."],"url":"http://arxiv.org/abs/2406.05709v1","category":"cs.RO"}
{"created":"2024-06-09 09:51:55","title":"QGEval: A Benchmark for Question Generation Evaluation","abstract":"Automatically generated questions often suffer from problems such as unclear expression or factual inaccuracies, requiring a reliable and comprehensive evaluation of their quality. Human evaluation is frequently used in the field of question generation (QG) and is one of the most accurate evaluation methods. It also serves as the standard for automatic metrics. However, there is a lack of unified evaluation criteria, which hampers the development of both QG technologies and automatic evaluation methods. To address this, we propose QGEval, a multi-dimensional Evaluation benchmark for Question Generation, which evaluates both generated questions and existing automatic metrics across 7 dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency. We demonstrate the appropriateness of these dimensions by examining their correlations and distinctions. Analysis with QGEval reveals that 1) most QG models perform unsatisfactorily in terms of answerability and answer consistency, and 2) existing metrics fail to align well with human assessments when evaluating generated questions across the 7 dimensions. We expect this work to foster the development of both QG technologies and automatic metrics for QG.","sentences":["Automatically generated questions often suffer from problems such as unclear expression or factual inaccuracies, requiring a reliable and comprehensive evaluation of their quality.","Human evaluation is frequently used in the field of question generation (QG) and is one of the most accurate evaluation methods.","It also serves as the standard for automatic metrics.","However, there is a lack of unified evaluation criteria, which hampers the development of both QG technologies and automatic evaluation methods.","To address this, we propose QGEval, a multi-dimensional Evaluation benchmark for Question Generation, which evaluates both generated questions and existing automatic metrics across 7 dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency.","We demonstrate the appropriateness of these dimensions by examining their correlations and distinctions.","Analysis with QGEval reveals that 1) most QG models perform unsatisfactorily in terms of answerability and answer consistency, and 2) existing metrics fail to align well with human assessments when evaluating generated questions across the 7 dimensions.","We expect this work to foster the development of both QG technologies and automatic metrics for QG."],"url":"http://arxiv.org/abs/2406.05707v1","category":"cs.CL"}
{"created":"2024-06-09 09:03:44","title":"Good plasmons in a bad metal","abstract":"Correlated materials may exhibit unusually high resistivity increasing linearly in temperature, breaking through the Mott-Ioffe-Regel bound, above which coherent quasiparticles are destroyed. The fate of collective charge excitations, or plasmons, in these systems is a subject of debate. Several studies suggest plasmons are overdamped while others detect unrenormalized plasmons. Here, we present direct optical images of low-loss hyperbolic plasmon polaritons (HPPs) in the correlated van der Waals metal MoOCl2. HPPs are plasmon-photon modes that waveguide through extremely anisotropic media and are remarkably long-lived in MoOCl2. Many-body theory supported by photoemission results reveals that MoOCl2 is in an orbital-selective and highly incoherent Peierls phase. Different orbitals acquire markedly different bonding-antibonding character, producing a highly-anisotropic, isolated Fermi surface. The Fermi surface is further reconstructed and made partly incoherent by electronic interactions, renormalizing the plasma frequency. HPPs remain long-lived in spite of this, allowing us to uncover previously unseen imprints of electronic correlations on plasmonic collective modes.","sentences":["Correlated materials may exhibit unusually high resistivity increasing linearly in temperature, breaking through the Mott-Ioffe-Regel bound, above which coherent quasiparticles are destroyed.","The fate of collective charge excitations, or plasmons, in these systems is a subject of debate.","Several studies suggest plasmons are overdamped while others detect unrenormalized plasmons.","Here, we present direct optical images of low-loss hyperbolic plasmon polaritons (HPPs) in the correlated van der Waals metal MoOCl2.","HPPs are plasmon-photon modes that waveguide through extremely anisotropic media and are remarkably long-lived in MoOCl2.","Many-body theory supported by photoemission results reveals that MoOCl2 is in an orbital-selective and highly incoherent Peierls phase.","Different orbitals acquire markedly different bonding-antibonding character, producing a highly-anisotropic, isolated Fermi surface.","The Fermi surface is further reconstructed and made partly incoherent by electronic interactions, renormalizing the plasma frequency.","HPPs remain long-lived in spite of this, allowing us to uncover previously unseen imprints of electronic correlations on plasmonic collective modes."],"url":"http://arxiv.org/abs/2406.05703v1","category":"cond-mat.str-el"}
{"created":"2024-06-09 08:51:50","title":"An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS","abstract":"Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any speaker's voice from a short audio prompt, have made rapid advancements. However, the quality of the generated speech significantly deteriorates when the audio prompt contains noise, and limited research has been conducted to address this issue. In this paper, we explored various strategies to enhance the quality of audio generated from noisy audio prompts within the context of flow-matching-based zero-shot TTS. Our investigation includes comprehensive training strategies: unsupervised pre-training with masked speech denoising, multi-speaker detection and DNSMOS-based data filtering on the pre-training data, and fine-tuning with random noise mixing. The results of our experiments demonstrate significant improvements in intelligibility, speaker similarity, and overall audio quality compared to the approach of applying speech enhancement to the audio prompt.","sentences":["Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any speaker's voice from a short audio prompt, have made rapid advancements.","However, the quality of the generated speech significantly deteriorates when the audio prompt contains noise, and limited research has been conducted to address this issue.","In this paper, we explored various strategies to enhance the quality of audio generated from noisy audio prompts within the context of flow-matching-based zero-shot TTS.","Our investigation includes comprehensive training strategies: unsupervised pre-training with masked speech denoising, multi-speaker detection and DNSMOS-based data filtering on the pre-training data, and fine-tuning with random noise mixing.","The results of our experiments demonstrate significant improvements in intelligibility, speaker similarity, and overall audio quality compared to the approach of applying speech enhancement to the audio prompt."],"url":"http://arxiv.org/abs/2406.05699v1","category":"eess.AS"}
{"created":"2024-06-09 08:41:25","title":"Two Power Allocation and Beamforming Strategies for Active IRS-aided Wireless Network via Machine Learning","abstract":"This paper models an active intelligent reflecting surface (IRS) -assisted wireless communication network, which has the ability to adjust power between BS and IRS. We aim to maximize the signal-to-noise ratio of user by jointly designing power allocation (PA) factor, active IRS phase shift matrix, and beamforming vector of BS, subject to a total power constraint. To tackle this non-convex problem, we solve this problem by alternately optimizing these variables. Firstly, the PA factor is designed via polynomial regression method. Next, BS beamforming vector and IRS phase shift matrix are obtained by Dinkelbach's transform and successive convex approximation methods. To reduce the high computational complexity of the above proposed algorithm, we maximize achievable rate (AR) and use closed-form fractional programming method to transform the original problem into an equivalent form. Then, we address this problem by iteratively optimizing auxiliary variables, BS and IRS beamformings. Simulation results show that the proposed algorithms can effectively improve the AR performance compared to fixed PA strategies, aided by passive IRS, and without IRS.","sentences":["This paper models an active intelligent reflecting surface (IRS) -assisted wireless communication network, which has the ability to adjust power between BS and IRS.","We aim to maximize the signal-to-noise ratio of user by jointly designing power allocation (PA) factor, active IRS phase shift matrix, and beamforming vector of BS, subject to a total power constraint.","To tackle this non-convex problem, we solve this problem by alternately optimizing these variables.","Firstly, the PA factor is designed via polynomial regression method.","Next, BS beamforming vector and IRS phase shift matrix are obtained by Dinkelbach's transform and successive convex approximation methods.","To reduce the high computational complexity of the above proposed algorithm, we maximize achievable rate (AR) and use closed-form fractional programming method to transform the original problem into an equivalent form.","Then, we address this problem by iteratively optimizing auxiliary variables, BS and IRS beamformings.","Simulation results show that the proposed algorithms can effectively improve the AR performance compared to fixed PA strategies, aided by passive IRS, and without IRS."],"url":"http://arxiv.org/abs/2406.05696v1","category":"eess.SP"}
{"created":"2024-06-09 08:24:17","title":"Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions","abstract":"Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process. However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews. In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers. We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal. This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process. Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations. We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions. It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area. We open-source the dataset at https://github.com/chengtan9907/ReviewMT.","sentences":["Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process.","However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews.","In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers.","We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal.","This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process.","Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations.","We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions.","It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area.","We open-source the dataset at https://github.com/chengtan9907/ReviewMT."],"url":"http://arxiv.org/abs/2406.05688v1","category":"cs.CL"}
{"created":"2024-06-09 07:41:48","title":"Simulating Parton Fragmentation on Quantum Computers","abstract":"Parton fragmentation functions (FFs) are indispensable for understanding processes of hadron production ubiquitously existing in high-energy collisions, but their first principle determination has never been realized due to the insurmountable difficulties in encoding their operator definition using traditional lattice methodology. We propose a framework that makes a first step for evaluating FFs utilizing quantum computing methodology. The key element is to construct a semi-inclusive hadron operator for filtering out hadrons of desired types in a collection of particles encoded in the quantum state. We illustrate the framework by elaborating on the Nambu-Jona-Lasinio model with numeral simulations. Remarkably, We show that the semi-inclusive hadron operator can be constructed efficiently with a variational quantum algorithm. Moreover, we develop error mitigation techniques tailed for accurately calculating the FFs in the presence of quantum noises. Our work opens a new avenue for investigating QCD hadronization on near-term quantum computers.","sentences":["Parton fragmentation functions (FFs) are indispensable for understanding processes of hadron production ubiquitously existing in high-energy collisions, but their first principle determination has never been realized due to the insurmountable difficulties in encoding their operator definition using traditional lattice methodology.","We propose a framework that makes a first step for evaluating FFs utilizing quantum computing methodology.","The key element is to construct a semi-inclusive hadron operator for filtering out hadrons of desired types in a collection of particles encoded in the quantum state.","We illustrate the framework by elaborating on the Nambu-Jona-Lasinio model with numeral simulations.","Remarkably, We show that the semi-inclusive hadron operator can be constructed efficiently with a variational quantum algorithm.","Moreover, we develop error mitigation techniques tailed for accurately calculating the FFs in the presence of quantum noises.","Our work opens a new avenue for investigating QCD hadronization on near-term quantum computers."],"url":"http://arxiv.org/abs/2406.05683v1","category":"hep-ph"}
{"created":"2024-06-09 07:41:03","title":"From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR","abstract":"Electronic Health Records (EHRs) contain rich patient information and are crucial for clinical research and practice. In recent years, deep learning models have been applied to EHRs, but they often rely on massive features, which may not be readily available for all patients. We propose HTP-Star, which leverages hypergraph structures with a pretrain-then-finetune framework for modeling EHR data, enabling seamless integration of additional features. Additionally, we design two techniques, namely (1) Smoothness-inducing Regularization and (2) Group-balanced Reweighting, to enhance the model's robustness during fine-tuning. Through experiments conducted on two real EHR datasets, we demonstrate that HTP-Star consistently outperforms various baselines while striking a balance between patients with basic and extra features.","sentences":["Electronic Health Records (EHRs) contain rich patient information and are crucial for clinical research and practice.","In recent years, deep learning models have been applied to EHRs, but they often rely on massive features, which may not be readily available for all patients.","We propose HTP-Star, which leverages hypergraph structures with a pretrain-then-finetune framework for modeling EHR data, enabling seamless integration of additional features.","Additionally, we design two techniques, namely (1) Smoothness-inducing Regularization and (2) Group-balanced Reweighting, to enhance the model's robustness during fine-tuning.","Through experiments conducted on two real EHR datasets, we demonstrate that HTP-Star consistently outperforms various baselines while striking a balance between patients with basic and extra features."],"url":"http://arxiv.org/abs/2406.05682v1","category":"cs.LG"}
{"created":"2024-06-09 07:23:34","title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","abstract":"Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.","sentences":["Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge.","This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation.","The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements.","LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention.","However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention.","This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups.","To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning.","Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference.","Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset.","All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}."],"url":"http://arxiv.org/abs/2406.05678v1","category":"cs.CL"}
{"created":"2024-06-09 07:06:58","title":"Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking","abstract":"Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving. For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization. Large language models (LLMs) often struggle with generating high-quality, diverse reasoning. While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions. Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking. To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states. The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward. Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA). Code is available at https://github.com/Yu-Fangxu/FoR.","sentences":["Divergent thinking, the cognitive process of generating diverse solutions, is a hallmark of human creativity and problem-solving.","For machines, sampling diverse solution trajectories in complex reasoning problems is crucial for robust outcomes, data augmentation, and enhanced model generalization.","Large language models (LLMs) often struggle with generating high-quality, diverse reasoning.","While supervised fine-tuning helps with quality, it requires extensive supervision data to capture the full diversity of solutions.","Alternatively, reinforcement learning methods like PPO aim to find limited highest-reward solutions while neglecting the solution diversity, akin to convergent thinking.","To address these limitations, we propose Flow of Reasoning (FoR) -- an efficient LLM training approach enabling diverse reasoning with minimal data.","FoR formulates multi-step LLM reasoning as a Markovian flow from an initial state to terminal states.","The formulation allows to adapt principled GFlowNet approaches to train the LLM as a policy, which is able to sample multiple reasoning paths with probabilities proportional to the unnormalized reward.","Empirical results show that, with limited training data (e.g., 15 examples), FoR can discover diverse high-quality solutions that excel greatly beyond current state-of-the-art methods across three tasks, including embodied reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning (PrOntoQA).","Code is available at https://github.com/Yu-Fangxu/FoR."],"url":"http://arxiv.org/abs/2406.05673v1","category":"cs.AI"}
{"created":"2024-06-09 05:57:59","title":"Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses","abstract":"Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one's own thought processes. Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions. Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios. Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions. Our study utilizes posts from Reddit's ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses. Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations. To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance. However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning. This research highlights the deficiencies in LLMs' social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness.","sentences":["Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one's own thought processes.","Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions.","Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios.","Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions.","Our study utilizes posts from Reddit's ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses.","Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations.","To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance.","However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning.","This research highlights the deficiencies in LLMs' social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness."],"url":"http://arxiv.org/abs/2406.05659v1","category":"cs.CL"}
{"created":"2024-06-09 05:57:40","title":"Visual Prompt Tuning in Null Space for Continual Learning","abstract":"Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the vision-transformer models. On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL. However, different from the orthogonal projection in the traditional CNN architecture, the prompt gradient orthogonal projection in the ViT architecture shows completely different and greater challenges, i.e., 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. Theoretically, we have finally deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. In practice, an effective null-space-based approximation solution has been proposed to implement the prompt gradient orthogonal projection. Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. Our code is available in the supplemental material.","sentences":["Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the vision-transformer models.","On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL.","However, different from the orthogonal projection in the traditional CNN architecture, the prompt gradient orthogonal projection in the ViT architecture shows completely different and greater challenges, i.e., 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block.","Theoretically, we have finally deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning.","In practice, an effective null-space-based approximation solution has been proposed to implement the prompt gradient orthogonal projection.","Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods.","Our code is available in the supplemental material."],"url":"http://arxiv.org/abs/2406.05658v1","category":"cs.CV"}
{"created":"2024-06-09 05:30:05","title":"Heart Sound Segmentation Using Deep Learning Techniques","abstract":"Heart disease remains a leading cause of mortality worldwide. Auscultation, the process of listening to heart sounds, can be enhanced through computer-aided analysis using Phonocardiogram (PCG) signals. This paper presents a novel approach for heart sound segmentation and classification into S1 (LUB) and S2 (DUB) sounds. We employ FFT-based filtering, dynamic programming for event detection, and a Siamese network for robust classification. Our method demonstrates superior performance on the PASCAL heart sound dataset compared to existing approaches.","sentences":["Heart disease remains a leading cause of mortality worldwide.","Auscultation, the process of listening to heart sounds, can be enhanced through computer-aided analysis using Phonocardiogram (PCG) signals.","This paper presents a novel approach for heart sound segmentation and classification into S1 (LUB) and S2 (DUB) sounds.","We employ FFT-based filtering, dynamic programming for event detection, and a Siamese network for robust classification.","Our method demonstrates superior performance on the PASCAL heart sound dataset compared to existing approaches."],"url":"http://arxiv.org/abs/2406.05653v1","category":"cs.SD"}
{"created":"2024-06-09 05:19:24","title":"GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement","abstract":"We propose a novel approach for 3D mesh reconstruction from multi-view images. Our method takes inspiration from large reconstruction models like LRM that use a transformer-based triplane generator and a Neural Radiance Field (NeRF) model trained on multi-view images. However, in our method, we introduce several important modifications that allow us to significantly enhance 3D reconstruction quality. First of all, we examine the original LRM architecture and find several shortcomings. Subsequently, we introduce respective modifications to the LRM architecture, which lead to improved multi-view image representation and more computationally efficient training. Second, in order to improve geometry reconstruction and enable supervision at full image resolution, we extract meshes from the NeRF field in a differentiable manner and fine-tune the NeRF model through mesh rendering. These modifications allow us to achieve state-of-the-art performance on both 2D and 3D evaluation metrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset. Despite these superior results, our feed-forward model still struggles to reconstruct complex textures, such as text and portraits on assets. To address this, we introduce a lightweight per-instance texture refinement procedure. This procedure fine-tunes the triplane representation and the NeRF color estimation model on the mesh surface using the input multi-view images in just 4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful reconstruction of complex textures, such as text. Additionally, our approach enables various downstream applications, including text- or image-to-3D generation.","sentences":["We propose a novel approach for 3D mesh reconstruction from multi-view images.","Our method takes inspiration from large reconstruction models like LRM that use a transformer-based triplane generator and a Neural Radiance Field (NeRF) model trained on multi-view images.","However, in our method, we introduce several important modifications that allow us to significantly enhance 3D reconstruction quality.","First of all, we examine the original LRM architecture and find several shortcomings.","Subsequently, we introduce respective modifications to the LRM architecture, which lead to improved multi-view image representation and more computationally efficient training.","Second, in order to improve geometry reconstruction and enable supervision at full image resolution, we extract meshes from the NeRF field in a differentiable manner and fine-tune the NeRF model through mesh rendering.","These modifications allow us to achieve state-of-the-art performance on both 2D and 3D evaluation metrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.","Despite these superior results, our feed-forward model still struggles to reconstruct complex textures, such as text and portraits on assets.","To address this, we introduce a lightweight per-instance texture refinement procedure.","This procedure fine-tunes the triplane representation and the NeRF color estimation model on the mesh surface using the input multi-view images in just 4 seconds.","This refinement improves the PSNR to 29.79 and achieves faithful reconstruction of complex textures, such as text.","Additionally, our approach enables various downstream applications, including text- or image-to-3D generation."],"url":"http://arxiv.org/abs/2406.05649v1","category":"cs.CV"}
{"created":"2024-06-09 05:12:27","title":"Sustainable Wireless Networks via Reconfigurable Intelligent Surfaces (RISs): Overview of the ETSI ISG RIS","abstract":"Reconfigurable Intelligent Surfaces (RISs) are a novel form of ultra-low power devices that are capable to increase the communication data rates as well as the cell coverage in a cost- and energy-efficient way. This is attributed to their programmable operation that enables them to dynamically manipulate the wireless propagation environment, a feature that has lately inspired numerous research investigations and applications. To pave the way to the formal standardization of RISs, the European Telecommunications Standards Institute (ETSI) launched the Industry Specification Group (ISG) on the RIS technology in September 2021. This article provides a comprehensive overview of the status of the work conducted by the ETSI ISG RIS, covering typical deployment scenarios of reconfigurable metasurfaces, use cases and operating applications, requirements, emerging hardware architectures and operating modes, as well as the latest insights regarding future directions of RISs and the resulting smart wireless environments.","sentences":["Reconfigurable Intelligent Surfaces (RISs) are a novel form of ultra-low power devices that are capable to increase the communication data rates as well as the cell coverage in a cost- and energy-efficient way.","This is attributed to their programmable operation that enables them to dynamically manipulate the wireless propagation environment, a feature that has lately inspired numerous research investigations and applications.","To pave the way to the formal standardization of RISs, the European Telecommunications Standards Institute (ETSI) launched the Industry Specification Group (ISG) on the RIS technology in September 2021.","This article provides a comprehensive overview of the status of the work conducted by the ETSI ISG RIS, covering typical deployment scenarios of reconfigurable metasurfaces, use cases and operating applications, requirements, emerging hardware architectures and operating modes, as well as the latest insights regarding future directions of RISs and the resulting smart wireless environments."],"url":"http://arxiv.org/abs/2406.05647v1","category":"eess.SP"}
{"created":"2024-06-09 05:04:37","title":"How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States","abstract":"Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns.","sentences":["Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs.","Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety.","Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate.","In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states.","We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers.","Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations.","Jailbreak disturbs the transformation of early unethical classification into negative emotions.","We conduct experiments on models from 7B to 70B across various model families to prove our conclusion.","Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns."],"url":"http://arxiv.org/abs/2406.05644v1","category":"cs.CL"}
{"created":"2024-06-09 03:52:21","title":"CCSI: Continual Class-Specific Impression for Data-free Class Incremental Learning","abstract":"In real-world clinical settings, traditional deep learning-based classification methods struggle with diagnosing newly introduced disease types because they require samples from all disease classes for offline training. Class incremental learning offers a promising solution by adapting a deep network trained on specific disease classes to handle new diseases. However, catastrophic forgetting occurs, decreasing the performance of earlier classes when adapting the model to new data. Prior proposed methodologies to overcome this require perpetual storage of previous samples, posing potential practical concerns regarding privacy and storage regulations in healthcare. To this end, we propose a novel data-free class incremental learning framework that utilizes data synthesis on learned classes instead of data storage from previous classes. Our key contributions include acquiring synthetic data known as Continual Class-Specific Impression (CCSI) for previously inaccessible trained classes and presenting a methodology to effectively utilize this data for updating networks when introducing new classes. We obtain CCSI by employing data inversion over gradients of the trained classification model on previous classes starting from the mean image of each class inspired by common landmarks shared among medical images and utilizing continual normalization layers statistics as a regularizer in this pixel-wise optimization process. Subsequently, we update the network by combining the synthesized data with new class data and incorporate several losses, including an intra-domain contrastive loss to generalize the deep network trained on the synthesized data to real data, a margin loss to increase separation among previous classes and new ones, and a cosine-normalized cross-entropy loss to alleviate the adverse effects of imbalanced distributions in training data.","sentences":["In real-world clinical settings, traditional deep learning-based classification methods struggle with diagnosing newly introduced disease types because they require samples from all disease classes for offline training.","Class incremental learning offers a promising solution by adapting a deep network trained on specific disease classes to handle new diseases.","However, catastrophic forgetting occurs, decreasing the performance of earlier classes when adapting the model to new data.","Prior proposed methodologies to overcome this require perpetual storage of previous samples, posing potential practical concerns regarding privacy and storage regulations in healthcare.","To this end, we propose a novel data-free class incremental learning framework that utilizes data synthesis on learned classes instead of data storage from previous classes.","Our key contributions include acquiring synthetic data known as Continual Class-Specific Impression (CCSI) for previously inaccessible trained classes and presenting a methodology to effectively utilize this data for updating networks when introducing new classes.","We obtain CCSI by employing data inversion over gradients of the trained classification model on previous classes starting from the mean image of each class inspired by common landmarks shared among medical images and utilizing continual normalization layers statistics as a regularizer in this pixel-wise optimization process.","Subsequently, we update the network by combining the synthesized data with new class data and incorporate several losses, including an intra-domain contrastive loss to generalize the deep network trained on the synthesized data to real data, a margin loss to increase separation among previous classes and new ones, and a cosine-normalized cross-entropy loss to alleviate the adverse effects of imbalanced distributions in training data."],"url":"http://arxiv.org/abs/2406.05631v1","category":"cs.LG"}
{"created":"2024-06-09 03:15:29","title":"Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2024","abstract":"In the Soccer Simulation 2D environment, accurate observation is crucial for effective decision making. However, challenges such as partial observation and noisy data can hinder performance. To address these issues, we propose a denoising algorithm that leverages predictive modeling and intersection analysis to enhance the accuracy of observations. Our approach aims to mitigate the impact of noise and partial data, leading to improved gameplay performance. This paper presents the framework, implementation, and preliminary results of our algorithm, demonstrating its potential in refining observations in Soccer Simulation 2D. Cyrus 2D Team is using a combination of Helios, Gliders, and Cyrus base codes.","sentences":["In the Soccer Simulation 2D environment, accurate observation is crucial for effective decision making.","However, challenges such as partial observation and noisy data can hinder performance.","To address these issues, we propose a denoising algorithm that leverages predictive modeling and intersection analysis to enhance the accuracy of observations.","Our approach aims to mitigate the impact of noise and partial data, leading to improved gameplay performance.","This paper presents the framework, implementation, and preliminary results of our algorithm, demonstrating its potential in refining observations in Soccer Simulation 2D. Cyrus 2D Team is using a combination of Helios, Gliders, and Cyrus base codes."],"url":"http://arxiv.org/abs/2406.05623v1","category":"cs.RO"}
{"created":"2024-06-09 03:11:40","title":"Cross Language Soccer Framework: An Open Source Framework for the RoboCup 2D Soccer Simulation","abstract":"RoboCup Soccer Simulation 2D (SS2D) research is hampered by the complexity of existing Cpp-based codes like Helios, Cyrus, and Gliders, which also suffer from limited integration with modern machine learning frameworks. This development paper introduces a transformative solution a gRPC-based, language-agnostic framework that seamlessly integrates with the high-performance Helios base code. This approach not only facilitates the use of diverse programming languages including CSharp, JavaScript, and Python but also maintains the computational efficiency critical for real time decision making in SS2D. By breaking down language barriers, our framework significantly enhances collaborative potential and flexibility, empowering researchers to innovate without the overhead of mastering or developing extensive base codes. We invite the global research community to leverage and contribute to the Cross Language Soccer (CLS) framework, which is openly available under the MIT License, to drive forward the capabilities of multi-agent systems in soccer simulations.","sentences":["RoboCup Soccer Simulation 2D (SS2D) research is hampered by the complexity of existing Cpp-based codes like Helios, Cyrus, and Gliders, which also suffer from limited integration with modern machine learning frameworks.","This development paper introduces a transformative solution a gRPC-based, language-agnostic framework that seamlessly integrates with the high-performance Helios base code.","This approach not only facilitates the use of diverse programming languages including CSharp, JavaScript, and Python but also maintains the computational efficiency critical for real time decision making in SS2D. By breaking down language barriers, our framework significantly enhances collaborative potential and flexibility, empowering researchers to innovate without the overhead of mastering or developing extensive base codes.","We invite the global research community to leverage and contribute to the Cross Language Soccer (CLS) framework, which is openly available under the MIT License, to drive forward the capabilities of multi-agent systems in soccer simulations."],"url":"http://arxiv.org/abs/2406.05621v1","category":"cs.RO"}
{"created":"2024-06-09 02:39:51","title":"Physically-Consistent Modeling and Optimization of Non-local RIS-Assisted Multi-User MIMO Communication Systems","abstract":"Mutual Coupling (MC) emerges as an inherent feature in Reconfigurable Intelligent Surfaces (RISs), particularly, when they are fabricated with sub-wavelength inter-element spacing. Hence, any physically-consistent model of the RIS operation needs to accurately describe MC-induced effects. In addition, the design of the ElectroMagnetic (EM) transmit/receive radiation patterns constitutes another critical factor for efficient RIS operation. The latter two factors lead naturally to the emergence of non-local RIS structures, whose operation can be effectively described via non-diagonal phase shift matrices. In this paper, we focus on jointly optimizing MC and the radiation patterns in multi-user MIMO communication systems assisted by non-local RISs, which are modeled via the scattering parameters. We particularly present a novel problem formulation for the joint optimization of MC, radiation patterns, and the active and passive beamforming in a physically-consistent manner, considering either reflective or transmissive RIS setups. Differently from the current approaches that design the former two parameters on the fly, we present an offline optimization method which is solved for both considered RIS functionalities. Our extensive simulation results, using both parametric and geometric channel models, showcase the validity of the proposed optimization framework over benchmark schemes, indicating that improved performance is achievable without the need for optimizing MC and the radiation patterns of the RIS on the fly, which can be rather cumbersome.","sentences":["Mutual Coupling (MC) emerges as an inherent feature in Reconfigurable Intelligent Surfaces (RISs), particularly, when they are fabricated with sub-wavelength inter-element spacing.","Hence, any physically-consistent model of the RIS operation needs to accurately describe MC-induced effects.","In addition, the design of the ElectroMagnetic (EM) transmit/receive radiation patterns constitutes another critical factor for efficient RIS operation.","The latter two factors lead naturally to the emergence of non-local RIS structures, whose operation can be effectively described via non-diagonal phase shift matrices.","In this paper, we focus on jointly optimizing MC and the radiation patterns in multi-user MIMO communication systems assisted by non-local RISs, which are modeled via the scattering parameters.","We particularly present a novel problem formulation for the joint optimization of MC, radiation patterns, and the active and passive beamforming in a physically-consistent manner, considering either reflective or transmissive RIS setups.","Differently from the current approaches that design the former two parameters on the fly, we present an offline optimization method which is solved for both considered RIS functionalities.","Our extensive simulation results, using both parametric and geometric channel models, showcase the validity of the proposed optimization framework over benchmark schemes, indicating that improved performance is achievable without the need for optimizing MC and the radiation patterns of the RIS on the fly, which can be rather cumbersome."],"url":"http://arxiv.org/abs/2406.05617v1","category":"cs.IT"}
{"created":"2024-06-09 02:01:25","title":"Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision","abstract":"In contemporary computer vision applications, particularly image classification, architectural backbones pre-trained on large datasets like ImageNet are commonly employed as feature extractors. Despite the widespread use of these pre-trained convolutional neural networks (CNNs), there remains a gap in understanding the performance of various resource-efficient backbones across diverse domains and dataset sizes. Our study systematically evaluates multiple lightweight, pre-trained CNN backbones under consistent training settings across a variety of datasets, including natural images, medical images, galaxy images, and remote sensing images. This comprehensive analysis aims to aid machine learning practitioners in selecting the most suitable backbone for their specific problem, especially in scenarios involving small datasets where fine-tuning a pre-trained network is crucial. Even though attention-based architectures are gaining popularity, we observed that they tend to perform poorly under low data finetuning tasks compared to CNNs. We also observed that some CNN architectures such as ConvNeXt, RegNet and EfficientNet performs well compared to others on a diverse set of domains consistently. Our findings provide actionable insights into the performance trade-offs and effectiveness of different backbones, facilitating informed decision-making in model selection for a broad spectrum of computer vision domains. Our code is available here: https://github.com/pranavphoenix/Backbones","sentences":["In contemporary computer vision applications, particularly image classification, architectural backbones pre-trained on large datasets like ImageNet are commonly employed as feature extractors.","Despite the widespread use of these pre-trained convolutional neural networks (CNNs), there remains a gap in understanding the performance of various resource-efficient backbones across diverse domains and dataset sizes.","Our study systematically evaluates multiple lightweight, pre-trained CNN backbones under consistent training settings across a variety of datasets, including natural images, medical images, galaxy images, and remote sensing images.","This comprehensive analysis aims to aid machine learning practitioners in selecting the most suitable backbone for their specific problem, especially in scenarios involving small datasets where fine-tuning a pre-trained network is crucial.","Even though attention-based architectures are gaining popularity, we observed that they tend to perform poorly under low data finetuning tasks compared to CNNs.","We also observed that some CNN architectures such as ConvNeXt, RegNet and EfficientNet performs well compared to others on a diverse set of domains consistently.","Our findings provide actionable insights into the performance trade-offs and effectiveness of different backbones, facilitating informed decision-making in model selection for a broad spectrum of computer vision domains.","Our code is available here: https://github.com/pranavphoenix/Backbones"],"url":"http://arxiv.org/abs/2406.05612v1","category":"cs.CV"}
{"created":"2024-06-09 01:56:38","title":"Analysis of non-equilibrium fluctuations in a number of COVID-19 cases and deaths based on time-convolutionless projection-operator method","abstract":"The non-equilibrium fluctuations observed in a number of COVID-19 cases and deaths are analyzed from a statistical-dynamical point view. By investigating the data observed around the world which were collected from January 15, 2020 to April 28, 2023 at https://coronavirus.jhu.edu/, we first show that the dynamics of the fluctuations is described by a stochastic equation whose stochastic force is a multiplicative type. By employing the time-convolutionless projection-operator method in open systems previously proposed by the present author, we then transform it into a Langevin-type equation with an additive-type stochastic force together with the corresponding Fokker-Planck type equation. Thus, we explore the stochastic properties of a Langevin-type stochastic force not only analytically but also numerically from a unified point of view. Finally, we emphasize that the dynamical behavior in deaths resembles that in cases very much not only for the causal motion but also for the fluctuation.","sentences":["The non-equilibrium fluctuations observed in a number of COVID-19 cases and deaths are analyzed from a statistical-dynamical point view.","By investigating the data observed around the world which were collected from January 15, 2020 to April 28, 2023 at https://coronavirus.jhu.edu/, we first show that the dynamics of the fluctuations is described by a stochastic equation whose stochastic force is a multiplicative type.","By employing the time-convolutionless projection-operator method in open systems previously proposed by the present author, we then transform it into a Langevin-type equation with an additive-type stochastic force together with the corresponding Fokker-Planck type equation.","Thus, we explore the stochastic properties of a Langevin-type stochastic force not only analytically but also numerically from a unified point of view.","Finally, we emphasize that the dynamical behavior in deaths resembles that in cases very much not only for the causal motion but also for the fluctuation."],"url":"http://arxiv.org/abs/2406.05611v1","category":"physics.bio-ph"}
{"created":"2024-06-09 01:12:41","title":"Deep Learning to Predict Glaucoma Progression using Structural Changes in the Eye","abstract":"Glaucoma is a chronic eye disease characterized by optic neuropathy, leading to irreversible vision loss. It progresses gradually, often remaining undiagnosed until advanced stages. Early detection is crucial to monitor atrophy and develop treatment strategies to prevent further vision impairment. Data-centric methods have enabled computer-aided algorithms for precise glaucoma diagnosis.   In this study, we use deep learning models to identify complex disease traits and progression criteria, detecting subtle changes indicative of glaucoma. We explore the structure-function relationship in glaucoma progression and predict functional impairment from structural eye deterioration. We analyze statistical and machine learning methods, including deep learning techniques with optical coherence tomography (OCT) scans for accurate progression prediction.   Addressing challenges like age variability, data imbalances, and noisy labels, we develop novel semi-supervised time-series algorithms:   1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to encode spatiotemporal features from OCT scans. This approach uses age-related progression and positive-unlabeled data to establish robust pseudo-progression criteria, bypassing gold-standard labels.   2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression Analysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture learns from potentially mislabeled data to improve prediction accuracy.   Our methods outperform conventional and state-of-the-art techniques.","sentences":["Glaucoma is a chronic eye disease characterized by optic neuropathy, leading to irreversible vision loss.","It progresses gradually, often remaining undiagnosed until advanced stages.","Early detection is crucial to monitor atrophy and develop treatment strategies to prevent further vision impairment.","Data-centric methods have enabled computer-aided algorithms for precise glaucoma diagnosis.   ","In this study, we use deep learning models to identify complex disease traits and progression criteria, detecting subtle changes indicative of glaucoma.","We explore the structure-function relationship in glaucoma progression and predict functional impairment from structural eye deterioration.","We analyze statistical and machine learning methods, including deep learning techniques with optical coherence tomography (OCT) scans for accurate progression prediction.   ","Addressing challenges like age variability, data imbalances, and noisy labels, we develop novel semi-supervised time-series algorithms:   1.","Weakly-Supervised Time-Series Learning:","We create a CNN-LSTM model to encode spatiotemporal features from OCT scans.","This approach uses age-related progression and positive-unlabeled data to establish robust pseudo-progression criteria, bypassing gold-standard labels.   ","2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression Analysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture learns from potentially mislabeled data to improve prediction accuracy.   ","Our methods outperform conventional and state-of-the-art techniques."],"url":"http://arxiv.org/abs/2406.05605v1","category":"cs.CV"}
{"created":"2024-06-09 00:58:39","title":"A Knowledge-Component-Based Methodology for Evaluating AI Assistants","abstract":"We evaluate an automatic hint generator for CS1 programming assignments powered by GPT-4, a large language model. This system provides natural language guidance about how students can improve their incorrect solutions to short programming exercises. A hint can be requested each time a student fails a test case. Our evaluation addresses three Research Questions:   RQ1: Do the hints help students improve their code? RQ2: How effectively do the hints capture problems in student code? RQ3: Are the issues that students resolve the same as the issues addressed in the hints?   To address these research questions quantitatively, we identified a set of fine-grained knowledge components and determined which ones apply to each exercise, incorrect solution, and generated hint. Comparing data from two large CS1 offerings, we found that access to the hints helps students to address problems with their code more quickly, that hints are able to consistently capture the most pressing errors in students' code, and that hints that address a few issues at once rather than a single bug are more likely to lead to direct student progress.","sentences":["We evaluate an automatic hint generator for CS1 programming assignments powered by GPT-4, a large language model.","This system provides natural language guidance about how students can improve their incorrect solutions to short programming exercises.","A hint can be requested each time a student fails a test case.","Our evaluation addresses three Research Questions:   RQ1:","Do the hints help students improve their code?","RQ2: How effectively do the hints capture problems in student code?","RQ3: Are the issues that students resolve the same as the issues addressed in the hints?   ","To address these research questions quantitatively, we identified a set of fine-grained knowledge components and determined which ones apply to each exercise, incorrect solution, and generated hint.","Comparing data from two large CS1 offerings, we found that access to the hints helps students to address problems with their code more quickly, that hints are able to consistently capture the most pressing errors in students' code, and that hints that address a few issues at once rather than a single bug are more likely to lead to direct student progress."],"url":"http://arxiv.org/abs/2406.05603v1","category":"cs.CY"}
{"created":"2024-06-08 22:21:42","title":"NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security","abstract":"Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized dataset, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our dataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.","sentences":["Large Language Models (LLMs) are being deployed across various domains today.","However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated.","To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications.","This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions.","Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls.","Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models.","This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning.","By providing a specialized dataset, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution.","Evaluating LLMs on these challenges and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management.","We make our dataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Database along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation."],"url":"http://arxiv.org/abs/2406.05590v1","category":"cs.CR"}
{"created":"2024-06-08 22:17:52","title":"CERET: Cost-Effective Extrinsic Refinement for Text Generation","abstract":"Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt. Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves. Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability. In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures. Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by ~1.6% in Rouge-1 for abstractive summarization and ~3.5% in hit rate for question answering. Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective.","sentences":["Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt.","Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves.","Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability.","In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures.","Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by ~1.6% in Rouge-1 for abstractive summarization and ~3.5% in hit rate for question answering.","Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective."],"url":"http://arxiv.org/abs/2406.05588v1","category":"cs.CL"}
{"created":"2024-06-08 22:14:51","title":"Creativity Has Left the Chat: The Price of Debiasing Language Models","abstract":"Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content. While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored. We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series. Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards \"attractor states\", indicating limited output diversity. Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation. The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application. We also discuss the importance of prompt engineering in harnessing the creative potential of base models.","sentences":["Large Language Models (LLMs) have revolutionized natural language processing but can exhibit biases and may generate toxic content.","While alignment techniques like Reinforcement Learning from Human Feedback (RLHF) reduce these issues, their impact on creativity, defined as syntactic and semantic diversity, remains unexplored.","We investigate the unintended consequences of RLHF on the creativity of LLMs through three experiments focusing on the Llama-2 series.","Our findings reveal that aligned models exhibit lower entropy in token predictions, form distinct clusters in the embedding space, and gravitate towards \"attractor states\", indicating limited output diversity.","Our findings have significant implications for marketers who rely on LLMs for creative tasks such as copywriting, ad creation, and customer persona generation.","The trade-off between consistency and creativity in aligned models should be carefully considered when selecting the appropriate model for a given application.","We also discuss the importance of prompt engineering in harnessing the creative potential of base models."],"url":"http://arxiv.org/abs/2406.05587v1","category":"cs.CL"}
{"created":"2024-06-08 20:56:14","title":"Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and Constraint Satisfaction","abstract":"Recent developments in pretrained large language models (LLMs) applied to robotics have demonstrated their capacity for sequencing a set of discrete skills to achieve open-ended goals in simple robotic tasks. In this paper, we examine the topic of LLM planning for a set of continuously parameterized skills whose execution must avoid violations of a set of kinematic, geometric, and physical constraints. We prompt the LLM to output code for a function with open parameters, which, together with environmental constraints, can be viewed as a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved through sampling or optimization to find a skill sequence and continuous parameter settings that achieve the goal while avoiding constraint violations. Additionally, we consider cases where the LLM proposes unsatisfiable CCSPs, such as those that are kinematically infeasible, dynamically unstable, or lead to collisions, and re-prompt the LLM to form a new CCSP accordingly. Experiments across three different simulated 3D domains demonstrate that our proposed strategy, PRoC3S, is capable of solving a wide range of complex manipulation tasks with realistic constraints on continuous parameters much more efficiently and effectively than existing baselines.","sentences":["Recent developments in pretrained large language models (LLMs) applied to robotics have demonstrated their capacity for sequencing a set of discrete skills to achieve open-ended goals in simple robotic tasks.","In this paper, we examine the topic of LLM planning for a set of continuously parameterized skills whose execution must avoid violations of a set of kinematic, geometric, and physical constraints.","We prompt the LLM to output code for a function with open parameters, which, together with environmental constraints, can be viewed as a Continuous Constraint Satisfaction Problem (CCSP).","This CCSP can be solved through sampling or optimization to find a skill sequence and continuous parameter settings that achieve the goal while avoiding constraint violations.","Additionally, we consider cases where the LLM proposes unsatisfiable CCSPs, such as those that are kinematically infeasible, dynamically unstable, or lead to collisions, and re-prompt the LLM to form a new CCSP accordingly.","Experiments across three different simulated 3D domains demonstrate that our proposed strategy, PRoC3S, is capable of solving a wide range of complex manipulation tasks with realistic constraints on continuous parameters much more efficiently and effectively than existing baselines."],"url":"http://arxiv.org/abs/2406.05572v1","category":"cs.RO"}
{"created":"2024-06-08 20:07:24","title":"Automata Extraction from Transformers","abstract":"In modern machine (ML) learning systems, Transformer-based architectures have achieved milestone success across a broad spectrum of tasks, yet understanding their operational mechanisms remains an open problem. To improve the transparency of ML systems, automata extraction methods, which interpret stateful ML models as automata typically through formal languages, have proven effective for explaining the mechanism of recurrent neural networks (RNNs). However, few works have been applied to this paradigm to Transformer models. In particular, understanding their processing of formal languages and identifying their limitations in this area remains unexplored. In this paper, we propose an automata extraction algorithm specifically designed for Transformer models. Treating the Transformer model as a black-box system, we track the model through the transformation process of their internal latent representations during their operations, and then use classical pedagogical approaches like L* algorithm to interpret them as deterministic finite-state automata (DFA). Overall, our study reveals how the Transformer model comprehends the structure of formal languages, which not only enhances the interpretability of the Transformer-based ML systems but also marks a crucial step toward a deeper understanding of how ML systems process formal languages. Code and data are available at https://github.com/Zhang-Yihao/Transfomer2DFA.","sentences":["In modern machine (ML) learning systems, Transformer-based architectures have achieved milestone success across a broad spectrum of tasks, yet understanding their operational mechanisms remains an open problem.","To improve the transparency of ML systems, automata extraction methods, which interpret stateful ML models as automata typically through formal languages, have proven effective for explaining the mechanism of recurrent neural networks (RNNs).","However, few works have been applied to this paradigm to Transformer models.","In particular, understanding their processing of formal languages and identifying their limitations in this area remains unexplored.","In this paper, we propose an automata extraction algorithm specifically designed for Transformer models.","Treating the Transformer model as a black-box system, we track the model through the transformation process of their internal latent representations during their operations, and then use classical pedagogical approaches like L* algorithm to interpret them as deterministic finite-state automata (DFA).","Overall, our study reveals how the Transformer model comprehends the structure of formal languages, which not only enhances the interpretability of the Transformer-based ML systems but also marks a crucial step toward a deeper understanding of how ML systems process formal languages.","Code and data are available at https://github.com/Zhang-Yihao/Transfomer2DFA."],"url":"http://arxiv.org/abs/2406.05564v1","category":"cs.LG"}
{"created":"2024-06-08 19:24:17","title":"ThatiAR: Subjectivity Detection in Arabic News Sentences","abstract":"Detecting subjectivity in news sentences is crucial for identifying media bias, enhancing credibility, and combating misinformation by flagging opinion-based content. It provides insights into public sentiment, empowers readers to make informed decisions, and encourages critical thinking. While research has developed methods and systems for this purpose, most efforts have focused on English and other high-resourced languages. In this study, we present the first large dataset for subjectivity detection in Arabic, consisting of ~3.6K manually annotated sentences, and GPT-4o based explanation. In addition, we included instructions (both in English and Arabic) to facilitate LLM based fine-tuning. We provide an in-depth analysis of the dataset, annotation process, and extensive benchmark results, including PLMs and LLMs. Our analysis of the annotation process highlights that annotators were strongly influenced by their political, cultural, and religious backgrounds, especially at the beginning of the annotation process. The experimental results suggest that LLMs with in-context learning provide better performance. We aim to release the dataset and resources for the community.","sentences":["Detecting subjectivity in news sentences is crucial for identifying media bias, enhancing credibility, and combating misinformation by flagging opinion-based content.","It provides insights into public sentiment, empowers readers to make informed decisions, and encourages critical thinking.","While research has developed methods and systems for this purpose, most efforts have focused on English and other high-resourced languages.","In this study, we present the first large dataset for subjectivity detection in Arabic, consisting of ~3.6K manually annotated sentences, and GPT-4o based explanation.","In addition, we included instructions (both in English and Arabic) to facilitate LLM based fine-tuning.","We provide an in-depth analysis of the dataset, annotation process, and extensive benchmark results, including PLMs and LLMs.","Our analysis of the annotation process highlights that annotators were strongly influenced by their political, cultural, and religious backgrounds, especially at the beginning of the annotation process.","The experimental results suggest that LLMs with in-context learning provide better performance.","We aim to release the dataset and resources for the community."],"url":"http://arxiv.org/abs/2406.05559v1","category":"cs.CL"}
{"created":"2024-06-08 19:14:11","title":"GuidelineExplorer -- Navigating through the Forrest of Actionable Guidelines on Node-Link Graph Visualization","abstract":"Creating graph visualizations involves many decisions, such as layout, node and edge appearance, and color choices. These decisions are challenging due to the multitude of options available. For instance, graph layout can be force-directed or orthogonal, and edges can be curved, tapered, partially drawn, or animated. Thus, research offers a multitude of guidelines to optimize graph visualizations for human perception and usability. Guidelines can be actionable, providing direct instructions, or non-actionable, specifying what to avoid. This work focuses on actionable guidelines for node-link diagrams, aiding designers in making better decisions.   Given the abundance of graph visualization research and the difficulty in navigating it, this work aims to collect and structure actionable guidelines for node-linkvisualizations. To demonstrate the general applicability of our approach to structuring actionable guidelines for node-link diagrams, we also included guidelines for visualizing graphs as matrices. It also proposes a visual interactive system, GuidelineExplorer, to apply guidelines directly to graphs, streamlining the design process and promoting collaboration within the research community.","sentences":["Creating graph visualizations involves many decisions, such as layout, node and edge appearance, and color choices.","These decisions are challenging due to the multitude of options available.","For instance, graph layout can be force-directed or orthogonal, and edges can be curved, tapered, partially drawn, or animated.","Thus, research offers a multitude of guidelines to optimize graph visualizations for human perception and usability.","Guidelines can be actionable, providing direct instructions, or non-actionable, specifying what to avoid.","This work focuses on actionable guidelines for node-link diagrams, aiding designers in making better decisions.   ","Given the abundance of graph visualization research and the difficulty in navigating it, this work aims to collect and structure actionable guidelines for node-linkvisualizations.","To demonstrate the general applicability of our approach to structuring actionable guidelines for node-link diagrams, we also included guidelines for visualizing graphs as matrices.","It also proposes a visual interactive system, GuidelineExplorer, to apply guidelines directly to graphs, streamlining the design process and promoting collaboration within the research community."],"url":"http://arxiv.org/abs/2406.05558v1","category":"cs.HC"}
{"created":"2024-06-08 19:03:41","title":"Joint Reflection and Power Splitting Optimization for RIS-assisted OAM-SWIPT","abstract":"Simultaneous wireless information and power transfer (SWIPT) can enhance the spectrum and power efficiencies of wireless communications networks. Line-of-sight (LOS) transmission is a typical SWIPT scenario. However, the strong channel correlation limits the spectrum and energy efficiencies of SWIPT in the LOS channel. Due to the orthogonal wavefronts, orbital angular momentum (OAM) waves can facilitate the SWIPT in LOS channels. With the assistance of the reconfigurable intelligent surface (RIS), both the energy efficiency and capacity can be further improved for the OAM-SWIPT systems. In this paper, we model the RIS-assisted OAM-SWIPT transmission and derive the optimal reflection coefficients and power splitting ratio for it. We first give the system and channel models. Then, we propose the transmission scheme. Based on the transmission scheme, we formulate the capacity and energy harvesting (EH) trade-off problem. We solve the problem by developing an alternating optimization algorithm. Simulations validate the capacity and EH enhancements brought by the RIS for OAM-SWIPT.","sentences":["Simultaneous wireless information and power transfer (SWIPT) can enhance the spectrum and power efficiencies of wireless communications networks.","Line-of-sight (LOS) transmission is a typical SWIPT scenario.","However, the strong channel correlation limits the spectrum and energy efficiencies of SWIPT in the LOS channel.","Due to the orthogonal wavefronts, orbital angular momentum (OAM) waves can facilitate the SWIPT in LOS channels.","With the assistance of the reconfigurable intelligent surface (RIS), both the energy efficiency and capacity can be further improved for the OAM-SWIPT systems.","In this paper, we model the RIS-assisted OAM-SWIPT transmission and derive the optimal reflection coefficients and power splitting ratio for it.","We first give the system and channel models.","Then, we propose the transmission scheme.","Based on the transmission scheme, we formulate the capacity and energy harvesting (EH) trade-off problem.","We solve the problem by developing an alternating optimization algorithm.","Simulations validate the capacity and EH enhancements brought by the RIS for OAM-SWIPT."],"url":"http://arxiv.org/abs/2406.05552v1","category":"eess.SP"}
{"created":"2024-06-08 18:57:13","title":"Autoregressive Diffusion Transformer for Text-to-Speech Synthesis","abstract":"Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .","sentences":["Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols.","Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy.","When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities.","To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT).","Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models.","High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing.","Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples.","Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step.","Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling.","Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance.","Audio samples are available at http://ardit-tts.github.io/ ."],"url":"http://arxiv.org/abs/2406.05551v1","category":"eess.AS"}
{"created":"2024-06-08 18:31:56","title":"Training Through Failure: Effects of Data Consistency in Parallel Machine Learning Training","abstract":"In this study, we explore the impact of relaxing data consistency in parallel machine learning training during a failure using various parameter server configurations. Our failure recovery strategies include traditional checkpointing, chain replication (which ensures a backup server takes over in case of failure), and a novel stateless parameter server approach. In the stateless approach, workers continue generating gradient updates even if the parameter server is down, applying these updates once the server is back online. We compare these techniques to a standard checkpointing approach, where the training job is resumed from the latest checkpoint.   To assess the resilience and performance of each configuration, we intentionally killed the parameter server during training for each experiment. Our experiment results indicate that the stateless parameter server approach continues to train towards convergence and improves accuracy as much as 10\\% in the face of a failure despite using stale weights and gradients. The chain replication and checkpointing techniques demonstrate convergence but suffer from setbacks in accuracy due to restarting from old checkpoints. These results suggest that allowing workers to continue generating updates during server downtime and applying these updates later can effectively improve hardware utilization. Furthermore, despite higher resource usage, the stateless parameter server method incurs similar monetary costs in terms of hardware usage compared to standard checkpointing methods due to the pricing structure of common cloud providers.","sentences":["In this study, we explore the impact of relaxing data consistency in parallel machine learning training during a failure using various parameter server configurations.","Our failure recovery strategies include traditional checkpointing, chain replication (which ensures a backup server takes over in case of failure), and a novel stateless parameter server approach.","In the stateless approach, workers continue generating gradient updates even if the parameter server is down, applying these updates once the server is back online.","We compare these techniques to a standard checkpointing approach, where the training job is resumed from the latest checkpoint.   ","To assess the resilience and performance of each configuration, we intentionally killed the parameter server during training for each experiment.","Our experiment results indicate that the stateless parameter server approach continues to train towards convergence and improves accuracy as much as 10\\% in the face of a failure despite using stale weights and gradients.","The chain replication and checkpointing techniques demonstrate convergence but suffer from setbacks in accuracy due to restarting from old checkpoints.","These results suggest that allowing workers to continue generating updates during server downtime and applying these updates later can effectively improve hardware utilization.","Furthermore, despite higher resource usage, the stateless parameter server method incurs similar monetary costs in terms of hardware usage compared to standard checkpointing methods due to the pricing structure of common cloud providers."],"url":"http://arxiv.org/abs/2406.05546v1","category":"cs.DC"}
{"created":"2024-06-08 18:17:09","title":"VP-LLM: Text-Driven 3D Volume Completion with Large Language Models through Patchification","abstract":"Recent conditional 3D completion works have mainly relied on CLIP or BERT to encode textual information, which cannot support complex instruction. Meanwhile, large language models (LLMs) have shown great potential in multi-modal understanding and generation tasks. Inspired by the recent advancements of LLM, we present Volume Patch LLM (VP-LLM), which leverages LLMs to perform conditional 3D completion in a single-forward pass. To integrate a 3D model into the LLM tokenization configuration, the incomplete 3D object is first divided into small patches that can be encoded independently. These encoded patches are then fed into an LLM along with the text prompt, instructing the LLM to capture the relations between these patches as well as injecting semantic meanings into the 3D object. Our results demonstrate a strong ability of LLMs to interpret complex text instructions and understand 3D objects, surpassing state-of-the-art diffusion-based 3D completion models in generation quality.","sentences":["Recent conditional 3D completion works have mainly relied on CLIP or BERT to encode textual information, which cannot support complex instruction.","Meanwhile, large language models (LLMs) have shown great potential in multi-modal understanding and generation tasks.","Inspired by the recent advancements of LLM, we present Volume Patch LLM (VP-LLM), which leverages LLMs to perform conditional 3D completion in a single-forward pass.","To integrate a 3D model into the LLM tokenization configuration, the incomplete 3D object is first divided into small patches that can be encoded independently.","These encoded patches are then fed into an LLM along with the text prompt, instructing the LLM to capture the relations between these patches as well as injecting semantic meanings into the 3D object.","Our results demonstrate a strong ability of LLMs to interpret complex text instructions and understand 3D objects, surpassing state-of-the-art diffusion-based 3D completion models in generation quality."],"url":"http://arxiv.org/abs/2406.05543v1","category":"cs.CV"}
{"created":"2024-06-08 18:14:11","title":"Two stage decoherence of optical phonons in long oligomers","abstract":"Intramolecular energy transport is generally responsible for chemical energy balance in molecular systems. The transport is fast and efficient if energy is transferred by optical phonons in periodic oligomers, but its efficiently is limited by decoherence emerging due to anharmonic interactions with acoustic phonons. We show that in the most common case of the optical phonon band being narrower than the acoustic bands decoherence takes place in two stages. The faster stage involves optical phonon multiple forward scattering due to absorption and emission of transverse acoustic phonons, i. e. collective bending modes with a quadratic spectrum; the transport remains ballistic and the speed can be altered. The subsequent slower stage involves phonon backscattering in multiphonon processes involving two or more acostic phonons resulting is a switch to diffusive transport. If the initially excited optical phonon possesses a relatively small group velocity, then its equilibration in the first stage is accompanied by its acceleration due to its transitions to states propagating faster. This theoretical expectation is consistent with the recent measurements of optical phonon transport in alkane chains, accelerating with increasing the chain length.","sentences":["Intramolecular energy transport is generally responsible for chemical energy balance in molecular systems.","The transport is fast and efficient if energy is transferred by optical phonons in periodic oligomers, but its efficiently is limited by decoherence emerging due to anharmonic interactions with acoustic phonons.","We show that in the most common case of the optical phonon band being narrower than the acoustic bands decoherence takes place in two stages.","The faster stage involves optical phonon multiple forward scattering due to absorption and emission of transverse acoustic phonons, i. e. collective bending modes with a quadratic spectrum; the transport remains ballistic and the speed can be altered.","The subsequent slower stage involves phonon backscattering in multiphonon processes involving two or more acostic phonons resulting is a switch to diffusive transport.","If the initially excited optical phonon possesses a relatively small group velocity, then its equilibration in the first stage is accompanied by its acceleration due to its transitions to states propagating faster.","This theoretical expectation is consistent with the recent measurements of optical phonon transport in alkane chains, accelerating with increasing the chain length."],"url":"http://arxiv.org/abs/2406.05541v1","category":"physics.chem-ph"}
{"created":"2024-06-08 18:11:30","title":"A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding","abstract":"The parallels between protein sequences and natural language in their sequential structures have inspired the application of large language models (LLMs) to protein understanding. Despite the success of LLMs in NLP, their effectiveness in comprehending protein sequences remains an open question, largely due to the absence of datasets linking protein sequences to descriptive text. Researchers have then attempted to adapt LLMs for protein understanding by integrating a protein sequence encoder with a pre-trained LLM. However, this adaptation raises a fundamental question: \"Can LLMs, originally designed for NLP, effectively comprehend protein sequences as a form of language?\" Current datasets fall short in addressing this question due to the lack of a direct correlation between protein sequences and corresponding text descriptions, limiting the ability to train and evaluate LLMs for protein understanding effectively. To bridge this gap, we introduce ProteinLMDataset, a dataset specifically designed for further self-supervised pretraining and supervised fine-tuning (SFT) of LLMs to enhance their capability for protein sequence comprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for pretraining and 893,000 instructions for SFT. Additionally, we present ProteinLMBench, the first benchmark dataset consisting of 944 manually verified multiple-choice questions for assessing the protein understanding capabilities of LLMs. ProteinLMBench incorporates protein-related details and sequences in multiple languages, establishing a new standard for evaluating LLMs' abilities in protein comprehension. The large language model InternLM2-7B, pretrained and fine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench, achieving the highest accuracy score. The dataset and the benchmark are available at https://huggingface.co/datasets/tsynbio/ProteinLMBench.","sentences":["The parallels between protein sequences and natural language in their sequential structures have inspired the application of large language models (LLMs) to protein understanding.","Despite the success of LLMs in NLP, their effectiveness in comprehending protein sequences remains an open question, largely due to the absence of datasets linking protein sequences to descriptive text.","Researchers have then attempted to adapt LLMs for protein understanding by integrating a protein sequence encoder with a pre-trained LLM.","However, this adaptation raises a fundamental question: \"Can LLMs, originally designed for NLP, effectively comprehend protein sequences as a form of language?\"","Current datasets fall short in addressing this question due to the lack of a direct correlation between protein sequences and corresponding text descriptions, limiting the ability to train and evaluate LLMs for protein understanding effectively.","To bridge this gap, we introduce ProteinLMDataset, a dataset specifically designed for further self-supervised pretraining and supervised fine-tuning (SFT) of LLMs to enhance their capability for protein sequence comprehension.","Specifically, ProteinLMDataset includes 17.46 billion tokens for pretraining and 893,000 instructions for SFT.","Additionally, we present ProteinLMBench, the first benchmark dataset consisting of 944 manually verified multiple-choice questions for assessing the protein understanding capabilities of LLMs.","ProteinLMBench incorporates protein-related details and sequences in multiple languages, establishing a new standard for evaluating LLMs' abilities in protein comprehension.","The large language model InternLM2-7B, pretrained and fine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench, achieving the highest accuracy score.","The dataset and the benchmark are available at https://huggingface.co/datasets/tsynbio/ProteinLMBench."],"url":"http://arxiv.org/abs/2406.05540v1","category":"q-bio.QM"}
{"created":"2024-06-08 17:33:23","title":"Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability","abstract":"The transferability of adversarial perturbations provides an effective shortcut for black-box attacks. Targeted perturbations have greater practicality but are more difficult to transfer between models. In this paper, we experimentally and theoretically demonstrated that neural networks trained on the same dataset have more consistent performance in High-Sample-Density-Regions (HSDR) of each class instead of low sample density regions. Therefore, in the target setting, adding perturbations towards HSDR of the target class is more effective in improving transferability. However, density estimation is challenging in high-dimensional scenarios. Further theoretical and experimental verification demonstrates that easy samples with low loss are more likely to be located in HSDR. Perturbations towards such easy samples in the target class can avoid density estimation for HSDR location. Based on the above facts, we verified that adding perturbations to easy samples in the target class improves targeted adversarial transferability of existing attack methods. A generative targeted attack strategy named Easy Sample Matching Attack (ESMA) is proposed, which has a higher success rate for targeted attacks and outperforms the SOTA generative method. Moreover, ESMA requires only 5% of the storage space and much less computation time comparing to the current SOTA, as ESMA attacks all classes with only one model instead of seperate models for each class. Our code is available at https://github.com/gjq100/ESMA.","sentences":["The transferability of adversarial perturbations provides an effective shortcut for black-box attacks.","Targeted perturbations have greater practicality but are more difficult to transfer between models.","In this paper, we experimentally and theoretically demonstrated that neural networks trained on the same dataset have more consistent performance in High-Sample-Density-Regions (HSDR) of each class instead of low sample density regions.","Therefore, in the target setting, adding perturbations towards HSDR of the target class is more effective in improving transferability.","However, density estimation is challenging in high-dimensional scenarios.","Further theoretical and experimental verification demonstrates that easy samples with low loss are more likely to be located in HSDR.","Perturbations towards such easy samples in the target class can avoid density estimation for HSDR location.","Based on the above facts, we verified that adding perturbations to easy samples in the target class improves targeted adversarial transferability of existing attack methods.","A generative targeted attack strategy named Easy Sample Matching Attack (ESMA) is proposed, which has a higher success rate for targeted attacks and outperforms the SOTA generative method.","Moreover, ESMA requires only 5% of the storage space and much less computation time comparing to the current SOTA, as ESMA attacks all classes with only one model instead of seperate models for each class.","Our code is available at https://github.com/gjq100/ESMA."],"url":"http://arxiv.org/abs/2406.05535v1","category":"cs.LG"}
{"created":"2024-06-08 17:30:54","title":"Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing","abstract":"Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.","sentences":["Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models.","However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency.","Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation.","Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern.","Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning.","To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO).","This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment.","Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios."],"url":"http://arxiv.org/abs/2406.05534v1","category":"cs.AI"}
{"created":"2024-06-08 17:27:27","title":"PAPR in Motion: Seamless Point-level 3D Scene Interpolation","abstract":"We propose the problem of point-level 3D scene interpolation, which aims to simultaneously reconstruct a 3D scene in two states from multiple views, synthesize smooth point-level interpolations between them, and render the scene from novel viewpoints, all without any supervision between the states. The primary challenge is on achieving a smooth transition between states that may involve significant and non-rigid changes. To address these challenges, we introduce \"PAPR in Motion\", a novel approach that builds upon the recent Proximity Attention Point Rendering (PAPR) technique, which can deform a point cloud to match a significantly different shape and render a visually coherent scene even after non-rigid deformations. Our approach is specifically designed to maintain the temporal consistency of the geometric structure by introducing various regularization techniques for PAPR. The result is a method that can effectively bridge large scene changes and produce visually coherent and temporally smooth interpolations in both geometry and appearance. Evaluation across diverse motion types demonstrates that \"PAPR in Motion\" outperforms the leading neural renderer for dynamic scenes. For more results and code, please visit our project website at https://niopeng.github.io/PAPR-in-Motion/ .","sentences":["We propose the problem of point-level 3D scene interpolation, which aims to simultaneously reconstruct a 3D scene in two states from multiple views, synthesize smooth point-level interpolations between them, and render the scene from novel viewpoints, all without any supervision between the states.","The primary challenge is on achieving a smooth transition between states that may involve significant and non-rigid changes.","To address these challenges, we introduce \"PAPR in Motion\", a novel approach that builds upon the recent Proximity Attention Point Rendering (PAPR) technique, which can deform a point cloud to match a significantly different shape and render a visually coherent scene even after non-rigid deformations.","Our approach is specifically designed to maintain the temporal consistency of the geometric structure by introducing various regularization techniques for PAPR.","The result is a method that can effectively bridge large scene changes and produce visually coherent and temporally smooth interpolations in both geometry and appearance.","Evaluation across diverse motion types demonstrates that \"PAPR in Motion\" outperforms the leading neural renderer for dynamic scenes.","For more results and code, please visit our project website at https://niopeng.github.io/PAPR-in-Motion/ ."],"url":"http://arxiv.org/abs/2406.05533v1","category":"cs.CV"}
{"created":"2024-06-08 17:25:48","title":"Exploring Adversarial Robustness of Deep State Space Models","abstract":"Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments. Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures. However, its effectiveness in improving the AR of SSMs remains unclear. While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored. To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance. We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components. Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues. To understand these phenomena, we empirically and theoretically analyze the output error of SSMs under AP. We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion. Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity. Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO.","sentences":["Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments.","Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures.","However, its effectiveness in improving the AR of SSMs remains unclear.","While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored.","To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance.","We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components.","Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues.","To understand these phenomena, we empirically and theoretically analyze the output error of SSMs under AP.","We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion.","Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity.","Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO."],"url":"http://arxiv.org/abs/2406.05532v1","category":"cs.LG"}
{"created":"2024-06-08 17:25:31","title":"Enhancing Adversarial Transferability via Information Bottleneck Constraints","abstract":"From the perspective of information bottleneck (IB) theory, we propose a novel framework for performing black-box transferable adversarial attacks named IBTA, which leverages advancements in invariant features. Intuitively, diminishing the reliance of adversarial perturbations on the original data, under equivalent attack performance constraints, encourages a greater reliance on invariant features that contributes most to classification, thereby enhancing the transferability of adversarial attacks. Building on this motivation, we redefine the optimization of transferable attacks using a novel theoretical framework that centers around IB. Specifically, to overcome the challenge of unoptimizable mutual information, we propose a simple and efficient mutual information lower bound (MILB) for approximating computation. Moreover, to quantitatively evaluate mutual information, we utilize the Mutual Information Neural Estimator (MINE) to perform a thorough analysis. Our experiments on the ImageNet dataset well demonstrate the efficiency and scalability of IBTA and derived MILB. Our code is available at https://github.com/Biqing-Qi/Enhancing-Adversarial-Transferability-via-Information-Bottleneck-Constraints.","sentences":["From the perspective of information bottleneck (IB) theory, we propose a novel framework for performing black-box transferable adversarial attacks named IBTA, which leverages advancements in invariant features.","Intuitively, diminishing the reliance of adversarial perturbations on the original data, under equivalent attack performance constraints, encourages a greater reliance on invariant features that contributes most to classification, thereby enhancing the transferability of adversarial attacks.","Building on this motivation, we redefine the optimization of transferable attacks using a novel theoretical framework that centers around IB.","Specifically, to overcome the challenge of unoptimizable mutual information, we propose a simple and efficient mutual information lower bound (MILB) for approximating computation.","Moreover, to quantitatively evaluate mutual information, we utilize the Mutual Information Neural Estimator (MINE) to perform a thorough analysis.","Our experiments on the ImageNet dataset well demonstrate the efficiency and scalability of IBTA and derived MILB.","Our code is available at https://github.com/Biqing-Qi/Enhancing-Adversarial-Transferability-via-Information-Bottleneck-Constraints."],"url":"http://arxiv.org/abs/2406.05531v1","category":"cs.LG"}
{"created":"2024-06-08 17:00:06","title":"A preprocessing-based planning framework for utilizing contacts in high-precision insertion tasks","abstract":"In manipulation tasks like plug insertion or assembly that have low tolerance to errors in pose estimation (errors of the order of 2mm can cause task failure), the utilization of touch/contact modality can aid in accurately localizing the object of interest. Motivated by this, in this work we model high-precision insertion tasks as planning problems under pose uncertainty, where we effectively utilize the occurrence of contacts (or the lack thereof) as observations to reduce uncertainty and reliably complete the task. We present a preprocessing-based planning framework for high-precision insertion in repetitive and time-critical settings, where the set of initial pose distributions (identified by a perception system) is finite. The finite set allows us to enumerate the possible planning problems that can be encountered online and preprocess a database of policies. Due to the computational complexity of constructing this database, we propose a general experience-based POMDP solver, E-RTDP-Bel, that uses the solutions of similar planning problems as experience to speed up planning queries and use it to efficiently construct the database. We show that the developed algorithm speeds up database creation by over a factor of 100, making the process computationally tractable. We demonstrate the effectiveness of the proposed framework in a real-world plug insertion task in the presence of port position uncertainty and a pipe assembly task in simulation in the presence of pipe pose uncertainty.","sentences":["In manipulation tasks like plug insertion or assembly that have low tolerance to errors in pose estimation (errors of the order of 2mm can cause task failure), the utilization of touch/contact modality can aid in accurately localizing the object of interest.","Motivated by this, in this work we model high-precision insertion tasks as planning problems under pose uncertainty, where we effectively utilize the occurrence of contacts (or the lack thereof) as observations to reduce uncertainty and reliably complete the task.","We present a preprocessing-based planning framework for high-precision insertion in repetitive and time-critical settings, where the set of initial pose distributions (identified by a perception system) is finite.","The finite set allows us to enumerate the possible planning problems that can be encountered online and preprocess a database of policies.","Due to the computational complexity of constructing this database, we propose a general experience-based POMDP solver, E-RTDP-Bel, that uses the solutions of similar planning problems as experience to speed up planning queries and use it to efficiently construct the database.","We show that the developed algorithm speeds up database creation by over a factor of 100, making the process computationally tractable.","We demonstrate the effectiveness of the proposed framework in a real-world plug insertion task in the presence of port position uncertainty and a pipe assembly task in simulation in the presence of pipe pose uncertainty."],"url":"http://arxiv.org/abs/2406.05522v1","category":"cs.RO"}
{"created":"2024-06-10 17:56:21","title":"Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor Microenvironment Subtype Prediction","abstract":"The characterization of Tumor MicroEnvironment (TME) is challenging due to its complexity and heterogeneity. Relatively consistent TME characteristics embedded within highly specific tissue features, render them difficult to predict. The capability to accurately classify TME subtypes is of critical significance for clinical tumor diagnosis and precision medicine. Based on the observation that tumors with different origins share similar microenvironment patterns, we propose PathoTME, a genomics-guided Siamese representation learning framework employing Whole Slide Image (WSI) for pan-cancer TME subtypes prediction. Specifically, we utilize Siamese network to leverage genomic information as a regularization factor to assist WSI embeddings learning during the training phase. Additionally, we employ Domain Adversarial Neural Network (DANN) to mitigate the impact of tissue type variations. To eliminate domain bias, a dynamic WSI prompt is designed to further unleash the model's capabilities. Our model achieves better performance than other state-of-the-art methods across 23 cancer types on TCGA dataset. Our code is available at https://github.com/Mengflz/PathoTME.","sentences":["The characterization of Tumor MicroEnvironment (TME) is challenging due to its complexity and heterogeneity.","Relatively consistent TME characteristics embedded within highly specific tissue features, render them difficult to predict.","The capability to accurately classify TME subtypes is of critical significance for clinical tumor diagnosis and precision medicine.","Based on the observation that tumors with different origins share similar microenvironment patterns, we propose PathoTME, a genomics-guided Siamese representation learning framework employing Whole Slide Image (WSI) for pan-cancer TME subtypes prediction.","Specifically, we utilize Siamese network to leverage genomic information as a regularization factor to assist WSI embeddings learning during the training phase.","Additionally, we employ Domain Adversarial Neural Network (DANN) to mitigate the impact of tissue type variations.","To eliminate domain bias, a dynamic WSI prompt is designed to further unleash the model's capabilities.","Our model achieves better performance than other state-of-the-art methods across 23 cancer types on TCGA dataset.","Our code is available at https://github.com/Mengflz/PathoTME."],"url":"http://arxiv.org/abs/2406.06517v1","category":"cs.CV"}
{"created":"2024-06-10 17:55:43","title":"Distribution-Free Predictive Inference under Unknown Temporal Drift","abstract":"Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.","sentences":["Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models.","Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time.","In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets.","The window is selected by optimizing an estimated bias-variance tradeoff.","We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift.","We also illustrate its efficacy through numerical experiments on synthetic and real data."],"url":"http://arxiv.org/abs/2406.06516v1","category":"stat.ME"}
{"created":"2024-06-10 17:54:57","title":"Random Features Approximation for Control-Affine Systems","abstract":"Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of interest are control affine. We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence. Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost. We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF). Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods.","sentences":["Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback.","Many nonlinear dynamical systems of interest are control affine.","We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence.","Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost.","We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce.","We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF).","Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods."],"url":"http://arxiv.org/abs/2406.06514v1","category":"cs.LG"}
{"created":"2024-06-10 17:50:56","title":"Quantifying fault tolerant simulation of strongly correlated systems using the Fermi-Hubbard model","abstract":"Understanding the physics of strongly correlated materials is one of the grand challenge problems for physics today. A large class of scientifically interesting materials, from high-$T_c$ superconductors to spin liquids, involve medium to strong correlations, and building a holistic understanding of these materials is critical. Doing so is hindered by the competition between the kinetic energy and Coulomb repulsion, which renders both analytic and numerical methods unsatisfactory for describing interacting materials. Fault-tolerant quantum computers have been proposed as a path forward to overcome these difficulties, but this potential capability has not yet been fully assessed. Here, using the multi-orbital Fermi-Hubbard model as a representative model and a source of scalable problem specifications, we estimate the resource costs needed to use fault-tolerant quantum computers for obtaining experimentally relevant quantities such as correlation function estimation. We find that advances in quantum algorithms and hardware will be needed in order to reduce quantum resources and feasibly address utility-scale problem instances.","sentences":["Understanding the physics of strongly correlated materials is one of the grand challenge problems for physics today.","A large class of scientifically interesting materials, from high-$T_c$ superconductors to spin liquids, involve medium to strong correlations, and building a holistic understanding of these materials is critical.","Doing so is hindered by the competition between the kinetic energy and Coulomb repulsion, which renders both analytic and numerical methods unsatisfactory for describing interacting materials.","Fault-tolerant quantum computers have been proposed as a path forward to overcome these difficulties, but this potential capability has not yet been fully assessed.","Here, using the multi-orbital Fermi-Hubbard model as a representative model and a source of scalable problem specifications, we estimate the resource costs needed to use fault-tolerant quantum computers for obtaining experimentally relevant quantities such as correlation function estimation.","We find that advances in quantum algorithms and hardware will be needed in order to reduce quantum resources and feasibly address utility-scale problem instances."],"url":"http://arxiv.org/abs/2406.06511v1","category":"quant-ph"}
{"created":"2024-06-10 17:50:16","title":"Most nearby young star clusters formed in three massive complexes","abstract":"Efforts to unveil the structure of the local interstellar medium and its recent star formation history have spanned the past seventy years. Recent studies utilizing precise data from space astrometry missions have revealed nearby, newly formed star clusters with connected origins. Nonetheless, mapping young clusters across the entire sky back to their natal regions has been hindered by a lack of clusters with precise radial velocity data. Here we show that 155 out of 272 (57 percent) high-quality young clusters within one kiloparsec of the Sun arise from three distinct spatial volumes. This conclusion is based upon the analysis of data from the third Gaia release and other large-scale spectroscopic surveys. Currently dispersed throughout the Solar Neighborhood, their past positions over 30 Myr ago reveal that these families of clusters each formed in one of three compact, massive star-forming complexes. One of these families includes all of the young clusters near the Sun -- the Taurus and Sco-Cen star-forming complexes. We estimate that over 200 supernovae were produced from these families and argue that these clustered supernovae produced both the Local Bubble and the largest nearby supershell GSH 238+00+09, both of which are clearly visible in modern three-dimensional dust maps.","sentences":["Efforts to unveil the structure of the local interstellar medium and its recent star formation history have spanned the past seventy years.","Recent studies utilizing precise data from space astrometry missions have revealed nearby, newly formed star clusters with connected origins.","Nonetheless, mapping young clusters across the entire sky back to their natal regions has been hindered by a lack of clusters with precise radial velocity data.","Here we show that 155 out of 272 (57 percent) high-quality young clusters within one kiloparsec of the Sun arise from three distinct spatial volumes.","This conclusion is based upon the analysis of data from the third Gaia release and other large-scale spectroscopic surveys.","Currently dispersed throughout the Solar Neighborhood, their past positions over 30 Myr ago reveal that these families of clusters each formed in one of three compact, massive star-forming complexes.","One of these families includes all of the young clusters near the Sun -- the Taurus and Sco-Cen star-forming complexes.","We estimate that over 200 supernovae were produced from these families and argue that these clustered supernovae produced both the Local Bubble and the largest nearby supershell GSH 238+00+09, both of which are clearly visible in modern three-dimensional dust maps."],"url":"http://arxiv.org/abs/2406.06510v1","category":"astro-ph.GA"}
{"created":"2024-06-10 17:33:44","title":"Demonstrating HumanTHOR: A Simulation Platform and Benchmark for Human-Robot Collaboration in a Shared Workspace","abstract":"Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest. However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability. To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace. To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms. The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC. The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms. The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace. More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/.","sentences":["Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest.","However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability.","To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace.","To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms.","The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC.","The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms.","The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace.","More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/."],"url":"http://arxiv.org/abs/2406.06498v1","category":"cs.RO"}
{"created":"2024-06-10 17:29:03","title":"Input Driven Synchronization of Chaotic Neural Networks with Analyticaly Determined Conditional Lyapunov Exponents","abstract":"Recurrent neural networks (RNNs) with random, but sufficiently strong and balanced coupling display a well known high-dimensional chaotic dynamics. Here, we investigate if externally applied inputs to these RNNs can stabilize globally synchronous, input-dependent solutions, in spite of the strong chaos-inducing coupling. We find that when the balance between excitation and inhibition is exact, that is when the row-sum of the weights is constant and 0, a globally applied input can readily synchronize all neurons onto a synchronous solution. The stability of the synchronous solution is analytically explored in this work with a master stability function. For any synchronous solution to the network dynamics, the conditional Lyapunov spectrum can be readily determined, with the stability of the synchronous solution critically dependent on the largest real eigenvalue component of the RNN weight matrix. We find that the smaller the maximum real component of the weight matrix eigenvalues, the more readily the network synchronizes. Further, the conditional Lyapunov exponents are easily computed numerically for any synchronization signal without simulating the RNN. Finally, for certain oscillatory synchronization signals, the conditional Lyapunov exponents can be determined analytically.","sentences":["Recurrent neural networks (RNNs) with random, but sufficiently strong and balanced coupling display a well known high-dimensional chaotic dynamics.","Here, we investigate if externally applied inputs to these RNNs can stabilize globally synchronous, input-dependent solutions, in spite of the strong chaos-inducing coupling.","We find that when the balance between excitation and inhibition is exact, that is when the row-sum of the weights is constant and 0, a globally applied input can readily synchronize all neurons onto a synchronous solution.","The stability of the synchronous solution is analytically explored in this work with a master stability function.","For any synchronous solution to the network dynamics, the conditional Lyapunov spectrum can be readily determined, with the stability of the synchronous solution critically dependent on the largest real eigenvalue component of the RNN weight matrix.","We find that the smaller the maximum real component of the weight matrix eigenvalues, the more readily the network synchronizes.","Further, the conditional Lyapunov exponents are easily computed numerically for any synchronization signal without simulating the RNN.","Finally, for certain oscillatory synchronization signals, the conditional Lyapunov exponents can be determined analytically."],"url":"http://arxiv.org/abs/2406.06491v1","category":"nlin.CD"}
{"created":"2024-06-10 17:27:07","title":"Computationally efficient permutation tests for the multivariate two-sample problem based on energy distance or maximum mean discrepancy statistics","abstract":"Non-parametric two-sample tests based on energy distance or maximum mean discrepancy are widely used statistical tests for comparing multivariate data from two populations. While these tests enjoy desirable statistical properties, their test statistics can be expensive to compute as they require the computation of 3 distinct Euclidean distance (or kernel) matrices between samples, where the time complexity of each of these computations (namely, $O(n_{x}^2 p)$, $O(n_{y}^2 p)$, and $O(n_{x} n_{y} p)$) scales quadratically with the number of samples ($n_x$, $n_y$) and linearly with the number of variables ($p$). Since the standard permutation test requires repeated re-computations of these expensive statistics it's application to large datasets can become unfeasible. While several statistical approaches have been proposed to mitigate this issue, they all sacrifice desirable statistical properties to decrease the computational cost (e.g., trade computation speed by a decrease in statistical power). A better computational strategy is to first pre-compute the Euclidean distance (kernel) matrix of the concatenated data, and then permute indexes and retrieve the corresponding elements to compute the re-sampled statistics. While this strategy can reduce the computation cost relative to the standard permutation test, it relies on the computation of a larger Euclidean distance (kernel) matrix with complexity $O((n_x + n_y)^2 p)$. In this paper, we present a novel computationally efficient permutation algorithm which only requires the pre-computation of the 3 smaller matrices and achieves large computational speedups without sacrificing finite-sample validity or statistical power. We illustrate its computational gains in a series of experiments and compare its statistical power to the current state-of-the-art approach for balancing computational cost and statistical performance.","sentences":["Non-parametric two-sample tests based on energy distance or maximum mean discrepancy are widely used statistical tests for comparing multivariate data from two populations.","While these tests enjoy desirable statistical properties, their test statistics can be expensive to compute as they require the computation of 3 distinct Euclidean distance (or kernel) matrices between samples, where the time complexity of each of these computations (namely, $O(n_{x}^2 p)$, $O(n_{y}^2 p)$, and $O(n_{x} n_{y} p)$) scales quadratically with the number of samples ($n_x$, $n_y$) and linearly with the number of variables ($p$).","Since the standard permutation test requires repeated re-computations of these expensive statistics it's application to large datasets can become unfeasible.","While several statistical approaches have been proposed to mitigate this issue, they all sacrifice desirable statistical properties to decrease the computational cost (e.g., trade computation speed by a decrease in statistical power).","A better computational strategy is to first pre-compute the Euclidean distance (kernel) matrix of the concatenated data, and then permute indexes and retrieve the corresponding elements to compute the re-sampled statistics.","While this strategy can reduce the computation cost relative to the standard permutation test, it relies on the computation of a larger Euclidean distance (kernel) matrix with complexity $O((n_x +","n_y)^2 p)$. In this paper, we present a novel computationally efficient permutation algorithm which only requires the pre-computation of the 3 smaller matrices and achieves large computational speedups without sacrificing finite-sample validity or statistical power.","We illustrate its computational gains in a series of experiments and compare its statistical power to the current state-of-the-art approach for balancing computational cost and statistical performance."],"url":"http://arxiv.org/abs/2406.06488v1","category":"stat.CO"}
{"created":"2024-06-10 17:22:17","title":"A Taxonomy and Comparative Analysis of IPv4 ID Selection Correctness, Security, and Performance","abstract":"The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.","sentences":["The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols.","Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches.","This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods.","By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security."],"url":"http://arxiv.org/abs/2406.06483v1","category":"cs.NI"}
{"created":"2024-06-10 16:55:08","title":"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies","abstract":"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.","sentences":["A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models.","However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute.","By overlooking this aspect, a skewed view of strategy efficiency is often presented.","This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.","In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated.","When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature.","In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized."],"url":"http://arxiv.org/abs/2406.06461v1","category":"cs.CL"}
{"created":"2024-06-10 16:45:20","title":"Evidence of 3$d$-4$f$ antiferromagnetic coupling in strain-tuned PrCo$_{0.5}$Ni$_{0.5}$O$_{3-\u03b4}$ epitaxial films","abstract":"The strong exchange interaction between 3$d$-4$f$ magnetic sublattice in rare-earth perovskites introduces a variety of complex magnetic states hosting fascinating electronic ground states with exotic properties. Especially when it comes to rare-earth nickelate and cobaltite perovskites, tuning their rich magnetic phase diagram and spin-state transitions make them potential candidates for spintronic applications. Here, we report the observation of antiferromagnetic coupling between Pr 4$f$ and Ni/Co 3$d$ magnetic sublattices and its tunability with strain in PrCo$_{0.5}$Ni$_{0.5}$O$_{3-\\delta}$ (PCNO) thin films. SQUID magnetization measurements reveal ferromagnetic (FM) ordering around 25 K, followed by a spin glass transition at low temperatures subject to spin reorientation. Competing magnetic interactions arise owing to the 3$d$-4$f$ antiferromagnetic (AFM) coupling between Pr and Co/Ni sublattice as revealed by the X-ray absorption spectroscopy (XAS) and X-ray magnetic circular dichroism (XMCD) at the Pr $M_{4,5}$ and Co/Ni $L_{2,3}$ absorption edges. Strain dependence on these AFM coupling reveals an increase (decrease) in the AFM exchange interaction for tensile (compressive) strained films, leading to a net decrease (increase) in the magnetization of PCNO films at low temperatures. The relative increase in low-temperature negative magnetoresistance for compressively strained films also reflects the enhanced ferromagnetic ordering in the system. The angle-dependent magnetoresistance measurements reveal a two-fold anisotropic magnetoresistance (AMR) in tensile strained PCNO films. In contrast, temperature-dependent switching of AMR accompanied by a two- to four-fold symmetry crossover is observed for LaAlO$_3$-grown compressive strained films.","sentences":["The strong exchange interaction between 3$d$-4$f$ magnetic sublattice in rare-earth perovskites introduces a variety of complex magnetic states hosting fascinating electronic ground states with exotic properties.","Especially when it comes to rare-earth nickelate and cobaltite perovskites, tuning their rich magnetic phase diagram and spin-state transitions make them potential candidates for spintronic applications.","Here, we report the observation of antiferromagnetic coupling between Pr 4$f$ and Ni/Co 3$d$ magnetic sublattices and its tunability with strain in PrCo$_{0.5}$Ni$_{0.5}$O$_{3-\\delta}$ (PCNO) thin films.","SQUID magnetization measurements reveal ferromagnetic (FM) ordering around 25 K, followed by a spin glass transition at low temperatures subject to spin reorientation.","Competing magnetic interactions arise owing to the 3$d$-4$f$ antiferromagnetic (AFM) coupling between Pr and Co/Ni sublattice as revealed by the X-ray absorption spectroscopy (XAS) and X-ray magnetic circular dichroism (XMCD) at the Pr $M_{4,5}$ and Co/Ni $L_{2,3}$ absorption edges.","Strain dependence on these AFM coupling reveals an increase (decrease) in the AFM exchange interaction for tensile (compressive) strained films, leading to a net decrease (increase) in the magnetization of PCNO films at low temperatures.","The relative increase in low-temperature negative magnetoresistance for compressively strained films also reflects the enhanced ferromagnetic ordering in the system.","The angle-dependent magnetoresistance measurements reveal a two-fold anisotropic magnetoresistance (AMR) in tensile strained PCNO films.","In contrast, temperature-dependent switching of AMR accompanied by a two- to four-fold symmetry crossover is observed for LaAlO$_3$-grown compressive strained films."],"url":"http://arxiv.org/abs/2406.06456v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 16:35:22","title":"QSSEP describes the fluctuations of quantum coherences in the Anderson model","abstract":"Using the transfer matrix method, we numerically investigate the structure of spatial coherences and their fluctuations in the 3d Anderson model in the metallic phase when driven out-of-equilibrium by external leads at zero temperature and in linear response. We find that the stationary state entails non-local non-Gaussian correlations in the longitudinal direction, which are characteristic of diffusive non equilibrium steady states. These correlations are quantitatively matched, at least up to third order, by those analytically derived in the Quantum Symmetric Simple Exclusion Process (QSSEP) which describes diffusive fermions in 1d subject to dynamical disorder. Furthermore, the large deviation scaling and $U(1)$ invariance of these correlations imply a link between the Anderson model and free probability theory. Our findings suggest the existence of a universal structure of correlations in non-interacting diffusive quantum systems that might be captured by QSSEP.","sentences":["Using the transfer matrix method, we numerically investigate the structure of spatial coherences and their fluctuations in the 3d Anderson model in the metallic phase when driven out-of-equilibrium by external leads at zero temperature and in linear response.","We find that the stationary state entails non-local non-Gaussian correlations in the longitudinal direction, which are characteristic of diffusive non equilibrium steady states.","These correlations are quantitatively matched, at least up to third order, by those analytically derived in the Quantum Symmetric Simple Exclusion Process (QSSEP) which describes diffusive fermions in 1d subject to dynamical disorder.","Furthermore, the large deviation scaling and $U(1)$ invariance of these correlations imply a link between the Anderson model and free probability theory.","Our findings suggest the existence of a universal structure of correlations in non-interacting diffusive quantum systems that might be captured by QSSEP."],"url":"http://arxiv.org/abs/2406.06444v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-10 16:24:04","title":"CR functions at CR singularities: approximation, extension, and hulls","abstract":"We study three possible definitions of the notion of CR functions at CR singular points, their extension to a fixed-neighborhood of the singular point, and analogues of the Baouendi-Tr\\'eves approximation in a fixed neighborhood. In particular, given the existence of a large enough disc hull shrinking to a point, we find the fixed-neighborhood extension and hence approximation properties. We provide examples showing the distinctions between the classes and the various properties studied.","sentences":["We study three possible definitions of the notion of CR functions at CR singular points, their extension to a fixed-neighborhood of the singular point, and analogues of the Baouendi-Tr\\'eves approximation in a fixed neighborhood.","In particular, given the existence of a large enough disc hull shrinking to a point, we find the fixed-neighborhood extension and hence approximation properties.","We provide examples showing the distinctions between the classes and the various properties studied."],"url":"http://arxiv.org/abs/2406.06431v1","category":"math.CV"}
{"created":"2024-06-10 16:15:30","title":"Notes on Kalman Filter (KF, EKF, ESKF, IEKF, IESKF)","abstract":"The Kalman Filter (KF) is a powerful mathematical tool widely used for state estimation in various domains, including Simultaneous Localization and Mapping (SLAM). This paper presents an in-depth introduction to the Kalman Filter and explores its several extensions: the Extended Kalman Filter (EKF), the Error-State Kalman Filter (ESKF), the Iterated Extended Kalman Filter (IEKF), and the Iterated Error-State Kalman Filter (IESKF). Each variant is meticulously examined, with detailed derivations of their mathematical formulations and discussions on their respective advantages and limitations. By providing a comprehensive overview of these techniques, this paper aims to offer valuable insights into their applications in SLAM and enhance the understanding of state estimation methodologies in complex environments.","sentences":["The Kalman Filter (KF) is a powerful mathematical tool widely used for state estimation in various domains, including Simultaneous Localization and Mapping (SLAM).","This paper presents an in-depth introduction to the Kalman Filter and explores its several extensions: the Extended Kalman Filter (EKF), the Error-State Kalman Filter (ESKF), the Iterated Extended Kalman Filter (IEKF), and the Iterated Error-State Kalman Filter (IESKF).","Each variant is meticulously examined, with detailed derivations of their mathematical formulations and discussions on their respective advantages and limitations.","By providing a comprehensive overview of these techniques, this paper aims to offer valuable insights into their applications in SLAM and enhance the understanding of state estimation methodologies in complex environments."],"url":"http://arxiv.org/abs/2406.06427v1","category":"cs.RO"}
{"created":"2024-06-10 16:13:11","title":"Notes on Various Errors and Jacobian Derivations for SLAM","abstract":"This paper delves into critical concepts and meticulous calculations pertinent to Simultaneous Localization and Mapping (SLAM), with a focus on error analysis and Jacobian matrices. We introduce various types of errors commonly encountered in SLAM, including reprojection error, photometric error, relative pose error, and line reprojection error, alongside their mathematical formulations. The fundamental role of error as the discrepancy between observed and predicted values in SLAM optimization is examined, emphasizing non-linear least squares methods for optimization.   We provide a detailed analysis of: - Reprojection Error: Including Jacobian calculations for camera poses and map points, highlighting both theoretical underpinnings and practical consequences. - Photometric Error: Addressing errors from image intensity variations, essential for direct method-based SLAM. - Relative Pose Error: Discussing its significance in pose graph optimization, especially in loop closure scenarios. The paper also presents extensive derivations of Jacobian matrices for various SLAM components such as camera poses, map points, and motion parameters. We explore the application of Lie theory to optimize rotation representations and transformations, improving computational efficiency. Specific software implementations are referenced, offering practical insights into the real-world application of these theories in SLAM systems.   Additionally, advanced topics such as line reprojection errors and IMU measurement errors are explored, discussing their impact on SLAM accuracy and performance. This comprehensive examination aims to enhance understanding and implementation of error analysis and Jacobian derivation in SLAM, contributing to more accurate and efficient state estimation in complex environments.","sentences":["This paper delves into critical concepts and meticulous calculations pertinent to Simultaneous Localization and Mapping (SLAM), with a focus on error analysis and Jacobian matrices.","We introduce various types of errors commonly encountered in SLAM, including reprojection error, photometric error, relative pose error, and line reprojection error, alongside their mathematical formulations.","The fundamental role of error as the discrepancy between observed and predicted values in SLAM optimization is examined, emphasizing non-linear least squares methods for optimization.   ","We provide a detailed analysis of: - Reprojection Error: Including Jacobian calculations for camera poses and map points, highlighting both theoretical underpinnings and practical consequences.","- Photometric Error:","Addressing errors from image intensity variations, essential for direct method-based SLAM.","- Relative Pose Error: Discussing its significance in pose graph optimization, especially in loop closure scenarios.","The paper also presents extensive derivations of Jacobian matrices for various SLAM components such as camera poses, map points, and motion parameters.","We explore the application of Lie theory to optimize rotation representations and transformations, improving computational efficiency.","Specific software implementations are referenced, offering practical insights into the real-world application of these theories in SLAM systems.   ","Additionally, advanced topics such as line reprojection errors and IMU measurement errors are explored, discussing their impact on SLAM accuracy and performance.","This comprehensive examination aims to enhance understanding and implementation of error analysis and Jacobian derivation in SLAM, contributing to more accurate and efficient state estimation in complex environments."],"url":"http://arxiv.org/abs/2406.06422v1","category":"cs.RO"}
{"created":"2024-06-10 16:12:00","title":"Foundation Inference Models for Markov Jump Processes","abstract":"Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process. Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets.","sentences":["Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces.","These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial.","In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components.","First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process.","Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way.","We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities.","Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models.","What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets."],"url":"http://arxiv.org/abs/2406.06419v1","category":"cs.LG"}
{"created":"2024-06-10 16:09:45","title":"Bridging magic and non-Gaussian resources via Gottesman-Kitaev-Preskill encoding","abstract":"Although the similarity between non-stabilizer states -- also known as magic states -- in discrete-variable systems and non-Gaussian states in continuous-variable systems has widely been recognized, the precise connections between these two notions have still been unclear. We establish a fundamental link between these two quantum resources via the Gottesman-Kitaev-Preskill (GKP) encoding. We show that the negativity of the continuous-variable Wigner function for an encoded GKP state coincides with a magic measure we introduce, which matches the negativity of the discrete Wigner function for odd dimensions. We also provide a continuous-variable representation of the stabilizer R\\'enyi entropy -- a recent proposal for a magic measure for multi-qubit states. With this in hand, we give a classical simulation algorithm with runtime scaling with the resource contents, quantified by our magic measures. We also employ our results to prove that implementing a multi-qubit logical non-Clifford operation in the GKP code subspace requires a non-Gaussian operation even at the limit of perfect encoding, despite the fact that the ideal GKP states already come with a large amount of non-Gaussianity.","sentences":["Although the similarity between non-stabilizer states -- also known as magic states -- in discrete-variable systems and non-Gaussian states in continuous-variable systems has widely been recognized, the precise connections between these two notions have still been unclear.","We establish a fundamental link between these two quantum resources via the Gottesman-Kitaev-Preskill (GKP) encoding.","We show that the negativity of the continuous-variable Wigner function for an encoded GKP state coincides with a magic measure we introduce, which matches the negativity of the discrete Wigner function for odd dimensions.","We also provide a continuous-variable representation of the stabilizer R\\'enyi entropy -- a recent proposal for a magic measure for multi-qubit states.","With this in hand, we give a classical simulation algorithm with runtime scaling with the resource contents, quantified by our magic measures.","We also employ our results to prove that implementing a multi-qubit logical non-Clifford operation in the GKP code subspace requires a non-Gaussian operation even at the limit of perfect encoding, despite the fact that the ideal GKP states already come with a large amount of non-Gaussianity."],"url":"http://arxiv.org/abs/2406.06418v1","category":"quant-ph"}
{"created":"2024-06-10 16:08:33","title":"Rigorous lower bound of dynamic critical exponents in critical frustration-free systems","abstract":"The dynamic critical exponent $z$ characterizes the finite-size gap in gapless quantum many-body systems. We establish a rigorous lower bound $z \\geq 2$ for frustration-free Hamiltonians on any lattice in any spatial dimension, given that their ground state exhibits a power-law decaying correlation function. This bound applies to representative classes of frustration-free Hamiltonians, including Rokhsar-Kivelson Hamiltonians, which are in one-to-one correspondence to Markov chains with locality, as well as parent Hamiltonians of critical projected entangled pair states with either a unique ground state or topologically degenerate ground states, and Hamiltonians with a plane-wave ground state.","sentences":["The dynamic critical exponent $z$ characterizes the finite-size gap in gapless quantum many-body systems.","We establish a rigorous lower bound $z \\geq 2$ for frustration-free Hamiltonians on any lattice in any spatial dimension, given that their ground state exhibits a power-law decaying correlation function.","This bound applies to representative classes of frustration-free Hamiltonians, including Rokhsar-Kivelson Hamiltonians, which are in one-to-one correspondence to Markov chains with locality, as well as parent Hamiltonians of critical projected entangled pair states with either a unique ground state or topologically degenerate ground states, and Hamiltonians with a plane-wave ground state."],"url":"http://arxiv.org/abs/2406.06415v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 16:04:58","title":"Bose-Einstein condensates of microwave-shielded polar molecules","abstract":"We investigate the ground-state properties of the ultracold gases of bosonic microwave-shielded polar molecules. To account for the large shielding core of the inter-molecular potential, we adopt a variational ansatz incorporating the Jastrow correlation factor. We show that the system is always stable and supports a self-bound gas phase and an expanding gas phase. We also calculate the condensate fraction which is significantly reduced when the size of the shielding core of the two-body potential becomes comparable to the inter-molecular distance. Our studies distinguish the molecular condensates from the atomic ones and invalidate the application of the Gross-Pitaevskii equation to the microwave-shielded molecular gases. Our work paves the way for studying the Bose-Einstein condensations of ultracold gases of microwave-shielded polar molecules.","sentences":["We investigate the ground-state properties of the ultracold gases of bosonic microwave-shielded polar molecules.","To account for the large shielding core of the inter-molecular potential, we adopt a variational ansatz incorporating the Jastrow correlation factor.","We show that the system is always stable and supports a self-bound gas phase and an expanding gas phase.","We also calculate the condensate fraction which is significantly reduced when the size of the shielding core of the two-body potential becomes comparable to the inter-molecular distance.","Our studies distinguish the molecular condensates from the atomic ones and invalidate the application of the Gross-Pitaevskii equation to the microwave-shielded molecular gases.","Our work paves the way for studying the Bose-Einstein condensations of ultracold gases of microwave-shielded polar molecules."],"url":"http://arxiv.org/abs/2406.06412v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-10 16:03:47","title":"Spatial dependence of local density of states in semiconductor-superconductor hybrids","abstract":"Majorana bound states are expected to appear in one-dimensional semiconductor-superconductor hybrid systems, provided they are homogenous enough to host a global topological phase. In order to experimentally investigate the uniformity of the system, we study the spatial dependence of the local density of states in multiprobe devices where several local tunnelling probes are positioned along a gate-defined wire in a two-dimensional electron gas. Spectroscopy at each probe reveals a hard induced gap, and an absence of subgap states at zero magnetic field. However, subgap states emerging at finite magnetic field are not always correlated between different probes. Moreover, we find that the extracted critical field and effective $g$-factor of the lowest energy subgap state varies significantly across the length of the wire. Upon studying several such devices we do however find examples of striking correlations in the local density of states measured at different tunnel probes. We discuss possible sources of variations across devices.","sentences":["Majorana bound states are expected to appear in one-dimensional semiconductor-superconductor hybrid systems, provided they are homogenous enough to host a global topological phase.","In order to experimentally investigate the uniformity of the system, we study the spatial dependence of the local density of states in multiprobe devices where several local tunnelling probes are positioned along a gate-defined wire in a two-dimensional electron gas.","Spectroscopy at each probe reveals a hard induced gap, and an absence of subgap states at zero magnetic field.","However, subgap states emerging at finite magnetic field are not always correlated between different probes.","Moreover, we find that the extracted critical field and effective $g$-factor of the lowest energy subgap state varies significantly across the length of the wire.","Upon studying several such devices we do however find examples of striking correlations in the local density of states measured at different tunnel probes.","We discuss possible sources of variations across devices."],"url":"http://arxiv.org/abs/2406.06410v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 16:02:48","title":"Differentially Private Best-Arm Identification","abstract":"Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy regimes. In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation. In the low-privacy regime, the lower bounds reduce to the non-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively. For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response. For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.","sentences":["Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies.","Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP).","First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP.","Our lower bounds suggest the existence of two privacy regimes.","In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation.","In the low-privacy regime, the lower bounds reduce to the non-private lower bounds.","We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively.","For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response.","For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off.","By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants."],"url":"http://arxiv.org/abs/2406.06408v1","category":"stat.ML"}
{"created":"2024-06-10 15:59:08","title":"A Taxonomy of Challenges to Curating Fair Datasets","abstract":"Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.","sentences":["Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation.","Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle.","Our findings underscore overarching issues within the broader fairness landscape that impact data curation.","We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices."],"url":"http://arxiv.org/abs/2406.06407v1","category":"cs.LG"}
{"created":"2024-06-10 15:56:14","title":"Early Acceptance Matching Game for User-Centric Clustering in Scalable Cell-free MIMO Networks","abstract":"The canonical setup is the primary approach adopted in cell-free multiple-input multiple-output (MIMO) networks, in which all access points (APs) jointly serve every user equipment (UE). This approach is not scalable in terms of computational complexity and fronthaul signaling becoming impractical in large networks. This work adopts a user-centric approach, a scalable alternative in which only a set of preferred APs jointly serve a UE. Forming the optimal cluster of APs for each UE is a challenging task, especially, when it needs to be dynamically adjusted to meet the quality of service (QoS) requirements of the UE. This complexity is even exacerbated when considering the constrained fronthaul capacity of the UE and the AP. We solve this problem with a novel many-to-many matching game. More specifically, we devise an early acceptance matching algorithm, which immediately admits or rejects UEs based on their requests and available radio resources. The proposed solution significantly reduces the fronthaul signaling while satisfying the maximum of UEs in terms of requested QoS compared to state-of-the-art approaches.","sentences":["The canonical setup is the primary approach adopted in cell-free multiple-input multiple-output (MIMO) networks, in which all access points (APs) jointly serve every user equipment (UE).","This approach is not scalable in terms of computational complexity and fronthaul signaling becoming impractical in large networks.","This work adopts a user-centric approach, a scalable alternative in which only a set of preferred APs jointly serve a UE.","Forming the optimal cluster of APs for each UE is a challenging task, especially, when it needs to be dynamically adjusted to meet the quality of service (QoS) requirements of the UE.","This complexity is even exacerbated when considering the constrained fronthaul capacity of the UE and the AP.","We solve this problem with a novel many-to-many matching game.","More specifically, we devise an early acceptance matching algorithm, which immediately admits or rejects UEs based on their requests and available radio resources.","The proposed solution significantly reduces the fronthaul signaling while satisfying the maximum of UEs in terms of requested QoS compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2406.06402v1","category":"eess.SP"}
{"created":"2024-06-10 15:52:27","title":"Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction","abstract":"We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations.","sentences":["We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle.","This setting covers many interesting examples including H\\\"older smooth problems and various inexact computations of the stochastic gradient.","Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants.","We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results.","First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded.","We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer.","Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms.","In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds.","As a direct corollary of our results, we obtain universal stochastic gradient methods for H\\\"older smooth problems which can be used in all situations."],"url":"http://arxiv.org/abs/2406.06398v1","category":"math.OC"}
{"created":"2024-06-10 15:46:43","title":"Tackling Delayed CSI in a Distributed Multi-Satellite MIMO Communication System","abstract":"In this study, we explore the integration of satellites with ground-based communication networks. Specifically, we analyze downlink data transmission from a constellation of satellites to terrestrial users and address the issue of delayed channel state information (CSI). The satellites cooperate in data transmission within a cluster to create a unified, distributed massive multiple input, multiple output (MIMO) system. The CSI used for this process is inherently outdated, particularly due to the delay from the most distant satellite in the cluster. Therefore, in this paper, we develop a precoding strategy that leverages the long-term characteristics of CSI uncertainty to compensate for the undesirable impact of these unavoidable delays. Our proposed method is computationally efficient and particularly effective in lower frequency bands. As such, it holds significant promise for facilitating the integration of satellite and terrestrial communication, especially within frequency bands of up to 1 GHz.","sentences":["In this study, we explore the integration of satellites with ground-based communication networks.","Specifically, we analyze downlink data transmission from a constellation of satellites to terrestrial users and address the issue of delayed channel state information (CSI).","The satellites cooperate in data transmission within a cluster to create a unified, distributed massive multiple input, multiple output (MIMO) system.","The CSI used for this process is inherently outdated, particularly due to the delay from the most distant satellite in the cluster.","Therefore, in this paper, we develop a precoding strategy that leverages the long-term characteristics of CSI uncertainty to compensate for the undesirable impact of these unavoidable delays.","Our proposed method is computationally efficient and particularly effective in lower frequency bands.","As such, it holds significant promise for facilitating the integration of satellite and terrestrial communication, especially within frequency bands of up to 1 GHz."],"url":"http://arxiv.org/abs/2406.06392v1","category":"eess.SP"}
{"created":"2024-06-10 15:44:49","title":"Time-tronics: from temporal printed circuit board to quantum computer","abstract":"Time crystalline structures can be created in periodically driven systems. They are temporal lattices which can reveal different condensed matter behaviours ranging from Anderson localization in time to temporal analogues of many-body localization or topological insulators. However, the potential practical applications of time crystalline structures have yet to be explored. Here, we pave the way for time-tronics where temporal lattices are like printed circuit boards for realization of a broad range of quantum devices. The elements of these devices can correspond to structures of dimensions higher than three and can be arbitrarily connected and reconfigured at any moment. Moreover, our approach allows for the construction of a quantum computer, enabling quantum gate operations for all possible pairs of qubits. Our findings indicate that the limitations faced in building devices using conventional spatial crystals can be overcome by adopting crystalline structures in time.","sentences":["Time crystalline structures can be created in periodically driven systems.","They are temporal lattices which can reveal different condensed matter behaviours ranging from Anderson localization in time to temporal analogues of many-body localization or topological insulators.","However, the potential practical applications of time crystalline structures have yet to be explored.","Here, we pave the way for time-tronics where temporal lattices are like printed circuit boards for realization of a broad range of quantum devices.","The elements of these devices can correspond to structures of dimensions higher than three and can be arbitrarily connected and reconfigured at any moment.","Moreover, our approach allows for the construction of a quantum computer, enabling quantum gate operations for all possible pairs of qubits.","Our findings indicate that the limitations faced in building devices using conventional spatial crystals can be overcome by adopting crystalline structures in time."],"url":"http://arxiv.org/abs/2406.06387v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-10 15:39:55","title":"Quantum simulation of one-dimensional fermionic systems with Ising Hamiltonians","abstract":"In recent years, analog quantum simulators have reached unprecedented quality, both in qubit numbers and coherence times. Most of these simulators natively implement Ising-type Hamiltonians, which limits the class of models that can be simulated efficiently. We propose a method to overcome this limitation and simulate the time-evolution of a large class of spinless fermionic systems in 1D using simple Ising-type Hamiltonians with local transverse fields. The time complexity of the simulation scales with the square root of the inverse error, and thus favorably compared to the worst-case error of first-order product formulas. Our method is based on domain wall encoding, which is implemented via strong (anti-)ferromagnetic couplings $|J|$. We show that in the limit of strong $|J|$, the domain walls behave like spinless fermions in 1D. The Ising Hamiltonians are one-dimensional chains with nearest-neighbor and, optionally, next-nearest-neighbor interactions. As a proof-of-concept, we perform numerical simulations of various 1D-fermionic systems using domain wall evolution and accurately reproduce the systems' properties, such as topological edge states, Anderson localization, quantum chaotic time evolution and time-reversal symmetry breaking via Floquet-engineering. Our approach makes the simulation of a large class of fermionic many-body systems feasible on analogue quantum hardware that natively implements Ising-type Hamiltonians with transverse fields.","sentences":["In recent years, analog quantum simulators have reached unprecedented quality, both in qubit numbers and coherence times.","Most of these simulators natively implement Ising-type Hamiltonians, which limits the class of models that can be simulated efficiently.","We propose a method to overcome this limitation and simulate the time-evolution of a large class of spinless fermionic systems in 1D using simple Ising-type Hamiltonians with local transverse fields.","The time complexity of the simulation scales with the square root of the inverse error, and thus favorably compared to the worst-case error of first-order product formulas.","Our method is based on domain wall encoding, which is implemented via strong (anti-)ferromagnetic couplings $|J|$.","We show that in the limit of strong $|J|$, the domain walls behave like spinless fermions in 1D.","The Ising Hamiltonians are one-dimensional chains with nearest-neighbor and, optionally, next-nearest-neighbor interactions.","As a proof-of-concept, we perform numerical simulations of various 1D-fermionic systems using domain wall evolution and accurately reproduce the systems' properties, such as topological edge states, Anderson localization, quantum chaotic time evolution and time-reversal symmetry breaking via Floquet-engineering.","Our approach makes the simulation of a large class of fermionic many-body systems feasible on analogue quantum hardware that natively implements Ising-type Hamiltonians with transverse fields."],"url":"http://arxiv.org/abs/2406.06378v1","category":"quant-ph"}
{"created":"2024-06-10 15:36:23","title":"Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual Localization and Navigation","abstract":"This paper presents a novel approach to visual simultaneous localization and mapping (SLAM) using multiple RGB-D cameras. The proposed method, Multicam-SLAM, significantly enhances the robustness and accuracy of SLAM systems by capturing more comprehensive spatial information from various perspectives. This method enables the accurate determination of pose relationships among multiple cameras without the need for overlapping fields of view. The proposed Muticam-SLAM includes a unique multi-camera model, a multi-keyframes structure, and several parallel SLAM threads. The multi-camera model allows for the integration of data from multiple cameras, while the multi-keyframes and parallel SLAM threads ensure efficient and accurate pose estimation and mapping. Extensive experiments in various environments demonstrate the superior accuracy and robustness of the proposed method compared to conventional single-camera SLAM systems. The results highlight the potential of the proposed Multicam-SLAM for more complex and challenging applications. Code is available at \\url{https://github.com/AlterPang/Multi_ORB_SLAM}.","sentences":["This paper presents a novel approach to visual simultaneous localization and mapping (SLAM) using multiple RGB-D cameras.","The proposed method, Multicam-SLAM, significantly enhances the robustness and accuracy of SLAM systems by capturing more comprehensive spatial information from various perspectives.","This method enables the accurate determination of pose relationships among multiple cameras without the need for overlapping fields of view.","The proposed Muticam-SLAM includes a unique multi-camera model, a multi-keyframes structure, and several parallel SLAM threads.","The multi-camera model allows for the integration of data from multiple cameras, while the multi-keyframes and parallel SLAM threads ensure efficient and accurate pose estimation and mapping.","Extensive experiments in various environments demonstrate the superior accuracy and robustness of the proposed method compared to conventional single-camera SLAM systems.","The results highlight the potential of the proposed Multicam-SLAM for more complex and challenging applications.","Code is available at \\url{https://github.com/AlterPang/Multi_ORB_SLAM}."],"url":"http://arxiv.org/abs/2406.06374v1","category":"cs.RO"}
{"created":"2024-06-10 15:24:15","title":"Symmetric Dot-Product Attention for Efficient Training of BERT Language Models","abstract":"Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing. Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources. The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research. In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture. This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention. When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half.","sentences":["Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing.","Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources.","The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research.","In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture.","This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention.","When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half."],"url":"http://arxiv.org/abs/2406.06366v1","category":"cs.CL"}
{"created":"2024-06-10 15:21:30","title":"Challenges with Differentiable Quantum Dynamics","abstract":"Differentiable quantum dynamics require automatic differentiation of a complex-valued initial value problem, which numerically integrates a system of ordinary differential equations from a specified initial condition, as well as the eigendecomposition of a matrix. We explored several automatic differentiation frameworks for these tasks, finding that no framework natively supports our application requirements. We therefore demonstrate a need for broader support of complex-valued, differentiable numerical integration in scientific computing libraries.","sentences":["Differentiable quantum dynamics require automatic differentiation of a complex-valued initial value problem, which numerically integrates a system of ordinary differential equations from a specified initial condition, as well as the eigendecomposition of a matrix.","We explored several automatic differentiation frameworks for these tasks, finding that no framework natively supports our application requirements.","We therefore demonstrate a need for broader support of complex-valued, differentiable numerical integration in scientific computing libraries."],"url":"http://arxiv.org/abs/2406.06361v1","category":"quant-ph"}
{"created":"2024-06-10 15:19:49","title":"A Field-Theoretic Example for Hodge Theory in 3D","abstract":"We focus on the continuous symmetry transformations for the three ($2 + 1$)-dimensional (3D) system of a combination of the free Abelian 1-form and 2-form gauge theories within the framework of Becchi-Rouet-Stora-Tyutin (BRST) formalism. We establish that this combined system is a tractable field-theoretic model of Hodge theory. The symmetry operators of our present theory provide the physical realizations of the de Rham cohomological operators of differential geometry at the algebraic level. Our present investigation is important in the sense that, for the first time, we are able to establish an odd dimensional (i.e. $D = 3$) field-theoretic system to be an example for Hodge theory (besides earlier works on a few interesting ($0 + 1$)-dimensional toy models as well as a set of well-known ${\\mathcal N} = 2$ SUSY quantum mechanical systems of physical interest).","sentences":["We focus on the continuous symmetry transformations for the three ($2 + 1$)-dimensional (3D) system of a combination of the free Abelian 1-form and 2-form gauge theories within the framework of Becchi-Rouet-Stora-Tyutin (BRST) formalism.","We establish that this combined system is a tractable field-theoretic model of Hodge theory.","The symmetry operators of our present theory provide the physical realizations of the de Rham cohomological operators of differential geometry at the algebraic level.","Our present investigation is important in the sense that, for the first time, we are able to establish an odd dimensional (i.e. $D = 3$) field-theoretic system to be an example for Hodge theory (besides earlier works on a few interesting ($0 + 1$)-dimensional toy models as well as a set of well-known ${\\mathcal N} = 2$ SUSY quantum mechanical systems of physical interest)."],"url":"http://arxiv.org/abs/2406.06358v1","category":"hep-th"}
{"created":"2024-06-10 15:13:57","title":"A quantitative investigation for deployment of mobile collaborative robots in high-value manufacturing","abstract":"Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies. Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts. However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors. Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors. Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms. The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range. The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations. Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%. Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs. Providing a highly diverse alternative to traditional fixed-base industrial manipulators.","sentences":["Component inspection is often the bottleneck in high-value manufacturing, driving industries like aerospace toward automated inspection technologies.","Current systems often employ fixed arm robots, but they lack the flexibility in adapting to new components or orientations Advanced mobile robotic platforms with updated sensor technologies and algorithms have improved localization and path planning capabilities, making them ideal for bringing inspection processes directly to parts.","However, mobile platforms introduce challenges in localization and maneuverability, leading to potential errors.","Their positional uncertainty is higher than fixed systems due to the lack of a fixed calibrated location, posing challenges for position-sensitive inspection sensors.","Therefore, it's essential to assess the positional accuracy and repeatability of mobile manipulator platforms.","The KUKA KMR iiwa was chosen for its collaborative features, robust build, and scalability within the KUKA product range.","The accuracy and repeatability of the mobile platform were evaluated through a series of tests to evaluate the performance of its integrated feature mapping, the effect of various speeds on positional accuracy, and the efficiency of the omnidirectional wheels for a range of translation orientations.","Experimental evaluation revealed that enabling feature mapping substantially improves the KUKA KMR iiwa's performance, with accuracy gains and error reductions exceeding 90%.","Repeatability errors were under 7 mm with mapping activated and around 2.5 mm in practical scenarios, demonstrating that mobile manipulators, incorporating both the manipulator and platform, can fulfil the precise requirements of industries with high precision needs.","Providing a highly diverse alternative to traditional fixed-base industrial manipulators."],"url":"http://arxiv.org/abs/2406.06353v1","category":"cs.RO"}
{"created":"2024-06-10 15:03:19","title":"Parametric kernel low-rank approximations using tensor train decomposition","abstract":"Computing low-rank approximations of kernel matrices is an important problem with many applications in scientific computing and data science. We propose methods to efficiently approximate and store low-rank approximations to kernel matrices that depend on certain hyperparameters. The main idea behind our method is to use multivariate Chebyshev function approximation along with the tensor train decomposition of the coefficient tensor. The computations are in two stages: an offline stage, which dominates the computational cost and is parameter-independent, and an online stage, which is inexpensive and instantiated for specific hyperparameters. A variation of this method addresses the case that the kernel matrix is symmetric and positive semi-definite. The resulting algorithms have linear complexity in terms of the sizes of the kernel matrices. We investigate the efficiency and accuracy of our method on parametric kernel matrices induced by various kernels, such as the Mat\\'ern kernel, through various numerical experiments. Our methods have speedups up to $200\\times$ in the online time compared to other methods with similar complexity and comparable accuracy.","sentences":["Computing low-rank approximations of kernel matrices is an important problem with many applications in scientific computing and data science.","We propose methods to efficiently approximate and store low-rank approximations to kernel matrices that depend on certain hyperparameters.","The main idea behind our method is to use multivariate Chebyshev function approximation along with the tensor train decomposition of the coefficient tensor.","The computations are in two stages: an offline stage, which dominates the computational cost and is parameter-independent, and an online stage, which is inexpensive and instantiated for specific hyperparameters.","A variation of this method addresses the case that the kernel matrix is symmetric and positive semi-definite.","The resulting algorithms have linear complexity in terms of the sizes of the kernel matrices.","We investigate the efficiency and accuracy of our method on parametric kernel matrices induced by various kernels, such as the Mat\\'ern kernel, through various numerical experiments.","Our methods have speedups up to $200\\times$ in the online time compared to other methods with similar complexity and comparable accuracy."],"url":"http://arxiv.org/abs/2406.06344v1","category":"math.NA"}
{"created":"2024-06-10 14:51:48","title":"Cell seeding dynamics in a porous scaffold material designed for meniscus tissue regeneration","abstract":"We study the dynamics of a seeding experiment where a fibrous scaffold material is colonized by two types of cell populations. The specific application that we have in mind is related to the idea of meniscus tissue regeneration. In order to support the development of a promising replacement material, we discuss certain rate equations for the densities of human mesenchymal stem cells and chondrocytes and for the production of collagen-containing extracellular matrix. For qualitative studies, we start with a system of ordinary differential equations and refine then the model to include spatial effects of the underlying nonwoven scaffold structure. Numerical experiments as well as a complete set of parameters for future benchmarking are provided.","sentences":["We study the dynamics of a seeding experiment where a fibrous scaffold material is colonized by two types of cell populations.","The specific application that we have in mind is related to the idea of meniscus tissue regeneration.","In order to support the development of a promising replacement material, we discuss certain rate equations for the densities of human mesenchymal stem cells and chondrocytes and for the production of collagen-containing extracellular matrix.","For qualitative studies, we start with a system of ordinary differential equations and refine then the model to include spatial effects of the underlying nonwoven scaffold structure.","Numerical experiments as well as a complete set of parameters for future benchmarking are provided."],"url":"http://arxiv.org/abs/2406.06334v1","category":"math.NA"}
{"created":"2024-06-10 14:42:37","title":"Leveraging Hyperscanning EEG and VR Omnidirectional Treadmill to Explore Inter-Brain Synchrony in Collaborative Spatial Navigation","abstract":"Navigating through a physical environment to reach a desired location involves a complex interplay of cognitive, sensory, and motor functions. When navigating with others, experiencing a degree of behavioral and cognitive synchronization is both natural and ubiquitous. This synchronization facilitates a harmonious effort toward achieving a common goal, reflecting how individuals instinctively align their actions and thoughts in collaborative settings. Collaborative spatial tasks, which are crucial in daily and professional settings, require coordinated navigation and problem-solving skills. This study explores the neural mechanisms underlying such tasks by using hyperscanning electroencephalography (EEG) technology to examine brain dynamics in dyadic route planning within a virtual reality setting. By analyzing intra- and inter-brain couplings across delta, theta, alpha, beta, and gamma EEG bands using both functional and effective connectivity measures, we identified significant neural synchronization patterns associated with collaborative task performance in both leaders and followers. Functional intra-brain connectivity analyses revealed distinct neural engagement across EEG frequency bands, with increased delta couplings observed in both leaders and followers. Theta connectivity was particularly enhanced in followers, whereas the alpha band exhibited divergent patterns that indicate role-specific neural strategies. Inter-brain analysis revealed increased delta causality between interacting members but decreased theta and gamma couplings from followers to leaders. Additionally, inter-brain analysis indicated decreased couplings in faster-performing dyads, especially in theta bands. These insights enhance our understanding of the neural mechanisms driving collaborative spatial navigation and demonstrate the effectiveness of hyperscanning in studying complex brain-to-brain interactions.","sentences":["Navigating through a physical environment to reach a desired location involves a complex interplay of cognitive, sensory, and motor functions.","When navigating with others, experiencing a degree of behavioral and cognitive synchronization is both natural and ubiquitous.","This synchronization facilitates a harmonious effort toward achieving a common goal, reflecting how individuals instinctively align their actions and thoughts in collaborative settings.","Collaborative spatial tasks, which are crucial in daily and professional settings, require coordinated navigation and problem-solving skills.","This study explores the neural mechanisms underlying such tasks by using hyperscanning electroencephalography (EEG) technology to examine brain dynamics in dyadic route planning within a virtual reality setting.","By analyzing intra- and inter-brain couplings across delta, theta, alpha, beta, and gamma EEG bands using both functional and effective connectivity measures, we identified significant neural synchronization patterns associated with collaborative task performance in both leaders and followers.","Functional intra-brain connectivity analyses revealed distinct neural engagement across EEG frequency bands, with increased delta couplings observed in both leaders and followers.","Theta connectivity was particularly enhanced in followers, whereas the alpha band exhibited divergent patterns that indicate role-specific neural strategies.","Inter-brain analysis revealed increased delta causality between interacting members but decreased theta and gamma couplings from followers to leaders.","Additionally, inter-brain analysis indicated decreased couplings in faster-performing dyads, especially in theta bands.","These insights enhance our understanding of the neural mechanisms driving collaborative spatial navigation and demonstrate the effectiveness of hyperscanning in studying complex brain-to-brain interactions."],"url":"http://arxiv.org/abs/2406.06327v1","category":"q-bio.NC"}
{"created":"2024-06-10 14:41:35","title":"n Distinguishable Particles on the Real Line interacting via Two Body Delta Potentials","abstract":"This paper studies a system of $n \\in \\mathbb{N}: \\, n \\geq 2$ non-relativistic, spinless quantum particles moving on the real line and interacting via a two-body delta potential. The Hamiltonian of such a system is proved to be affiliated to the resolvent algebra of the case, $\\mathcal{R}\\left( \\mathbb{R}^{2n},\\sigma \\right)$; it is further shown the existence of a $\\text{C}^{\\ast}-$dynamical system and of a subalgebra $\\pi_S\\left( \\mathfrak{S}_0 \\right)^{-1} \\subset \\mathcal{R}\\left( \\mathbb{R}^{2n},\\sigma \\right)$, stable under time evolution, where $\\pi_S$ is the Schr{\\\"o}dinger representation of the algebra.","sentences":["This paper studies a system of $n \\in \\mathbb{N}: \\, n \\geq 2$ non-relativistic, spinless quantum particles moving on the real line and interacting via a two-body delta potential.","The Hamiltonian of such a system is proved to be affiliated to the resolvent algebra of the case, $\\mathcal{R}\\left( \\mathbb{R}^{2n},\\sigma \\right)$; it is further shown the existence of a $\\text{C}^{\\ast}-$dynamical system and of a subalgebra $\\pi_S\\left( \\mathfrak{S}_0 \\right)^{-1} \\subset \\mathcal{R}\\left( \\mathbb{R}^{2n},\\sigma \\right)$, stable under time evolution, where $\\pi_S$ is the Schr{\\\"o}dinger representation of the algebra."],"url":"http://arxiv.org/abs/2406.06325v1","category":"math-ph"}
{"created":"2024-06-10 14:38:46","title":"Feasibility of accelerating incompressible computational fluid dynamics simulations with fault-tolerant quantum computers","abstract":"Across industries, traditional design and engineering workflows are being upgraded to simulation-driven processes. Many workflows include computational fluid dynamics (CFD). Simulations of turbulent flow are notorious for high compute costs and reliance on approximate methods that compromise accuracy. Improvements in the speed and accuracy of CFD calculations would potentially reduce design workflow costs by reducing computational costs and eliminating the need for experimental testing. This study explores the feasibility of using fault-tolerant quantum computers to improve the speed and accuracy of CFD simulations in the incompressible or weakly compressible regime. For the example of simulation-driven ship design, we consider simulations for calculating the drag force in steady-state flows, and provide analysis on economic utility and classical hardness. As a waypoint toward assessing the feasibility of our chosen quantum approach, we estimate the quantum resources required for the simpler case of drag force on a sphere. We estimate the product of logical qubits $\\times$ $T$ gates to range from $10^{22}$ to $10^{28}$. These high initial estimates suggest that future quantum computers are unlikely to provide utility for incompressible CFD applications unless significant algorithmic advancements or alternative quantum approaches are developed. Encouraged by applications in quantum chemistry that have realized orders-of-magnitude improvements as they matured, we identify the most promising next steps for quantum resource reduction as we work to scale up our estimates from spheres to utility-scale problems with more complex geometry.","sentences":["Across industries, traditional design and engineering workflows are being upgraded to simulation-driven processes.","Many workflows include computational fluid dynamics (CFD).","Simulations of turbulent flow are notorious for high compute costs and reliance on approximate methods that compromise accuracy.","Improvements in the speed and accuracy of CFD calculations would potentially reduce design workflow costs by reducing computational costs and eliminating the need for experimental testing.","This study explores the feasibility of using fault-tolerant quantum computers to improve the speed and accuracy of CFD simulations in the incompressible or weakly compressible regime.","For the example of simulation-driven ship design, we consider simulations for calculating the drag force in steady-state flows, and provide analysis on economic utility and classical hardness.","As a waypoint toward assessing the feasibility of our chosen quantum approach, we estimate the quantum resources required for the simpler case of drag force on a sphere.","We estimate the product of logical qubits $\\times$ $T$ gates to range from $10^{22}$ to $10^{28}$. These high initial estimates suggest that future quantum computers are unlikely to provide utility for incompressible CFD applications unless significant algorithmic advancements or alternative quantum approaches are developed.","Encouraged by applications in quantum chemistry that have realized orders-of-magnitude improvements as they matured, we identify the most promising next steps for quantum resource reduction as we work to scale up our estimates from spheres to utility-scale problems with more complex geometry."],"url":"http://arxiv.org/abs/2406.06323v1","category":"quant-ph"}
{"created":"2024-06-10 14:33:59","title":"Should my Blockchain Learn to Drive? A Study of Hyperledger Fabric","abstract":"Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions. However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters. This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention. We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains. We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved. Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings. Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future.","sentences":["Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions.","However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters.","This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention.","We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains.","We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved.","Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings.","Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future."],"url":"http://arxiv.org/abs/2406.06318v1","category":"cs.DC"}
{"created":"2024-06-10 14:29:53","title":"Memory Complexity of Entropy Estimation","abstract":"We observe an infinite sequence of independent identically distributed random variables $X_1,X_2,\\ldots$ drawn from an unknown distribution $p$ over $[n]$, and our goal is to estimate the entropy $H(p)=-\\mathbb{E}[\\log p(X)]$ within an $\\varepsilon$-additive error. To that end, at each time point we are allowed to update a finite-state machine with $S$ states, using a possibly randomized but time-invariant rule, where each state of the machine is assigned an entropy estimate. Our goal is to characterize the minimax memory complexity $S^*$ of this problem, which is the minimal number of states for which the estimation task is feasible with probability at least $1-\\delta$ asymptotically, uniformly in $p$. Specifically, we show that there exist universal constants $C_1$ and $C_2$ such that $ S^* \\leq C_1\\cdot\\frac{n (\\log n)^4}{\\varepsilon^2\\delta}$ for $\\varepsilon$ not too small, and $S^* \\geq C_2 \\cdot \\max \\{n, \\frac{\\log n}{\\varepsilon}\\}$ for $\\varepsilon$ not too large. The upper bound is proved using approximate counting to estimate the logarithm of $p$, and a finite memory bias estimation machine to estimate the expectation operation. The lower bound is proved via a reduction of entropy estimation to uniformity testing. We also apply these results to derive bounds on the memory complexity of mutual information estimation.","sentences":["We observe an infinite sequence of independent identically distributed random variables $X_1,X_2,\\ldots$ drawn from an unknown distribution $p$ over $[n]$, and our goal is to estimate the entropy $H(p)=-\\mathbb{E}[\\log p(X)]$ within an $\\varepsilon$-additive error.","To that end, at each time point we are allowed to update a finite-state machine with $S$ states, using a possibly randomized but time-invariant rule, where each state of the machine is assigned an entropy estimate.","Our goal is to characterize the minimax memory complexity $S^*$ of this problem, which is the minimal number of states for which the estimation task is feasible with probability at least $1-\\delta$ asymptotically, uniformly in $p$. Specifically, we show that there exist universal constants $C_1$ and $C_2$ such that $ S^* \\leq C_1\\cdot\\frac{n (\\log n)^4}{\\varepsilon^2\\delta}$ for $\\varepsilon$ not too small, and $S^* \\geq C_2 \\cdot \\max \\{n, \\frac{\\log n}{\\varepsilon}\\}$ for $\\varepsilon$ not too large.","The upper bound is proved using approximate counting to estimate the logarithm of $p$, and a finite memory bias estimation machine to estimate the expectation operation.","The lower bound is proved via a reduction of entropy estimation to uniformity testing.","We also apply these results to derive bounds on the memory complexity of mutual information estimation."],"url":"http://arxiv.org/abs/2406.06312v1","category":"cs.IT"}
{"created":"2024-06-10 14:20:48","title":"NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking Neural Networks","abstract":"Recently, brain-inspired spiking neural networks (SNNs) have attracted great research attention owing to their inherent bio-interpretability, event-triggered properties and powerful perception of spatiotemporal information, which is beneficial to handling event-based neuromorphic datasets. In contrast to conventional static image datasets, event-based neuromorphic datasets present heightened complexity in feature extraction due to their distinctive time series and sparsity characteristics, which influences their classification accuracy. To overcome this challenge, a novel approach termed Neuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in this paper by extending the benefits of self-supervised pre-training to SNNs to effectively stimulate their potential. This is the first time that self-supervised learning (SSL) based on momentum contrastive learning is realized in SNNs. In addition, we devise a novel loss function named MixInfoNCE tailored to their temporal characteristics to further increase the classification accuracy of neuromorphic datasets, which is verified through rigorous ablation experiments. Finally, experiments on DVS-CIFAR10, DVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper establishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256), 98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively.","sentences":["Recently, brain-inspired spiking neural networks (SNNs) have attracted great research attention owing to their inherent bio-interpretability, event-triggered properties and powerful perception of spatiotemporal information, which is beneficial to handling event-based neuromorphic datasets.","In contrast to conventional static image datasets, event-based neuromorphic datasets present heightened complexity in feature extraction due to their distinctive time series and sparsity characteristics, which influences their classification accuracy.","To overcome this challenge, a novel approach termed Neuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in this paper by extending the benefits of self-supervised pre-training to SNNs to effectively stimulate their potential.","This is the first time that self-supervised learning (SSL) based on momentum contrastive learning is realized in SNNs.","In addition, we devise a novel loss function named MixInfoNCE tailored to their temporal characteristics to further increase the classification accuracy of neuromorphic datasets, which is verified through rigorous ablation experiments.","Finally, experiments on DVS-CIFAR10, DVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper establishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256), 98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively."],"url":"http://arxiv.org/abs/2406.06305v1","category":"cs.CV"}
{"created":"2024-06-10 14:17:26","title":"Learning-based cognitive architecture for enhancing coordination in human groups","abstract":"As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups. This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training. Extensive numerical validation consistently demonstrates improvements in synchronization. Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans.","sentences":["As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups.","This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training.","Extensive numerical validation consistently demonstrates improvements in synchronization.","Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans."],"url":"http://arxiv.org/abs/2406.06297v1","category":"eess.SY"}
{"created":"2024-06-10 14:12:33","title":"Geometric sparsification in recurrent neural networks","abstract":"A common technique for ameliorating the computational costs of running large neural models is sparsification, or the removal of neural connections during training. Sparse models are capable of maintaining the high accuracy of state of the art models, while functioning at the cost of more parsimonious models. The structures which underlie sparse architectures are, however, poorly understood and not consistent between differently trained models and sparsification schemes. In this paper, we propose a new technique for sparsification of recurrent neural nets (RNNs), called moduli regularization, in combination with magnitude pruning. Moduli regularization leverages the dynamical system induced by the recurrent structure to induce a geometric relationship between neurons in the hidden state of the RNN. By making our regularizing term explicitly geometric, we provide the first, to our knowledge, a priori description of the desired sparse architecture of our neural net. We verify the effectiveness of our scheme for navigation and natural language processing RNNs. Navigation is a structurally geometric task, for which there are known moduli spaces, and we show that regularization can be used to reach 90% sparsity while maintaining model performance only when coefficients are chosen in accordance with a suitable moduli space. Natural language processing, however, has no known moduli space in which computations are performed. Nevertheless, we show that moduli regularization induces more stable recurrent neural nets with a variety of moduli regularizers, and achieves high fidelity models at 98% sparsity.","sentences":["A common technique for ameliorating the computational costs of running large neural models is sparsification, or the removal of neural connections during training.","Sparse models are capable of maintaining the high accuracy of state of the art models, while functioning at the cost of more parsimonious models.","The structures which underlie sparse architectures are, however, poorly understood and not consistent between differently trained models and sparsification schemes.","In this paper, we propose a new technique for sparsification of recurrent neural nets (RNNs), called moduli regularization, in combination with magnitude pruning.","Moduli regularization leverages the dynamical system induced by the recurrent structure to induce a geometric relationship between neurons in the hidden state of the RNN.","By making our regularizing term explicitly geometric, we provide the first, to our knowledge, a priori description of the desired sparse architecture of our neural net.","We verify the effectiveness of our scheme for navigation and natural language processing RNNs.","Navigation is a structurally geometric task, for which there are known moduli spaces, and we show that regularization can be used to reach 90% sparsity while maintaining model performance only when coefficients are chosen in accordance with a suitable moduli space.","Natural language processing, however, has no known moduli space in which computations are performed.","Nevertheless, we show that moduli regularization induces more stable recurrent neural nets with a variety of moduli regularizers, and achieves high fidelity models at 98% sparsity."],"url":"http://arxiv.org/abs/2406.06290v1","category":"cs.LG"}
{"created":"2024-06-10 14:11:27","title":"Identifying Bottlenecks of NISQ-friendly HHL algorithms","abstract":"Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures. For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise. In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE. We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package. Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here. Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle. These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE. Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions.","sentences":["Quantum computing promises enabling solving large problem instances, e.g. large linear equation systems with HHL algorithm, once the hardware stack matures.","For the foreseeable future quantum computing will remain in the so-called NISQ era, in which the algorithms need to account for the flaws of the hardware such as noise.","In this work, we perform an empirical study to test scaling properties and directly related noise resilience of the the most resources-intense component of the HHL algorithm, namely QPE and its NISQ-adaptation Iterative QPE.","We explore the effectiveness of noise mitigation techniques for these algorithms and investigate whether we can keep the gate number low by enforcing sparsity constraints on the input or using circuit optimization techniques provided by Qiskit package.","Our results indicate that currently available noise mitigation techniques, such as Qiskit readout and Mthree readout packages, are insufficient for enabling results recovery even in the small instances tested here.","Moreover, our results indicate that the scaling of these algorithms with increase in precision seems to be the most substantial obstacle.","These insights allowed us to deduce an approximate bottleneck for algorithms that consider a similar time evolution as QPE.","Such observations provide evidence of weaknesses of such algorithms on NISQ devices and help us formulate meaningful future research directions."],"url":"http://arxiv.org/abs/2406.06288v1","category":"quant-ph"}
{"created":"2024-06-10 14:11:08","title":"A Twisted Adiabatic Limit Approach to Vanishing Theorems for Complex Line Bundles","abstract":"Given an $n$-dimensional compact complex Hermitian manifold $X$, a $C^\\infty$ complex line bundle $L$ equipped with a connection $D$ whose $(0,\\,1)$-component $D''$ squares to zero and a real-valued function $\\eta$ on $X$, we prove that the $D''$-cohomology group of $L$ of any bidegree $(p,\\,q)$ such that either $(p>q \\hspace{1ex}\\mbox{and}\\hspace{1ex} p+q\\geq n+1)$ or $(p<q \\hspace{1ex}\\mbox{and}\\hspace{1ex} p+q\\leq n-1)$ vanishes when two extra hypotheses are made. The first hypothesis requires a certain real-valued, not necessarily closed, $(1,\\,1)$-form depending on $p,\\,q$, on the curvature of $D$ and on a $(1,\\,1)$-form induced by $\\eta$ to be positive definite. The second hypothesis requires the norm of $\\partial\\eta$ to be small relative to $|\\eta|$. This theorem, for which we also give a number of variants, is proved by generalising our very recent twisted adiabatic limit construction for complex structures to connections on complex line bundles. This twisting of $D$ induces first-order differential operators acting on the $L$-valued forms, for which we obtain commutation relations involving their formal adjoints, and two twisted Laplacians for which we obtain a comparison formula reminiscent of the classical Bochner-Kodaira-Nakano identity. The main features of our results are that $X$ need not be K\\\"ahler, $L$ need not be holomorphic and the types of $C^\\infty$ functions that $X$ supports play a key role in our hypotheses, thus capturing some of their links with the geometry of manifolds.","sentences":["Given an $n$-dimensional compact complex Hermitian manifold $X$, a $C^\\infty$ complex line bundle $L$ equipped with a connection $D$ whose $(0,\\,1)$-component $D''$ squares to zero and a real-valued function $\\eta$ on $X$, we prove that the $D''$-cohomology group of $L$ of any bidegree $(p,\\,q)$ such that either $(p>q \\hspace{1ex}\\mbox{and}\\hspace{1ex} p+q\\geq n+1)$ or $(p<q \\hspace{1ex}\\mbox{and}\\hspace{1ex} p+q\\leq n-1)$ vanishes when two extra hypotheses are made.","The first hypothesis requires a certain real-valued, not necessarily closed, $(1,\\,1)$-form depending on $p,\\,q$, on the curvature of $D$ and on a $(1,\\,1)$-form induced by $\\eta$ to be positive definite.","The second hypothesis requires the norm of $\\partial\\eta$ to be small relative to $|\\eta|$. This theorem, for which we also give a number of variants, is proved by generalising our very recent twisted adiabatic limit construction for complex structures to connections on complex line bundles.","This twisting of $D$ induces first-order differential operators acting on the $L$-valued forms, for which we obtain commutation relations involving their formal adjoints, and two twisted Laplacians for which we obtain a comparison formula reminiscent of the classical Bochner-Kodaira-Nakano identity.","The main features of our results are that $X$ need not be K\\\"ahler, $L$ need not be holomorphic and the types of $C^\\infty$ functions that $X$ supports play a key role in our hypotheses, thus capturing some of their links with the geometry of manifolds."],"url":"http://arxiv.org/abs/2406.06286v1","category":"math.DG"}
{"created":"2024-06-10 14:09:21","title":"Holographic complex potential of a quarkonium from deep learning","abstract":"Utilizing an emergent metric developed from deep learning techniques, we determine the complex potential associated with static quarkonium. This study explores the disintegration process of quarkonium by analyzing the real component of this potential, which is crucial for understanding its stability in various conditions. We show that the dissociation length, the critical distance at which a quark and antiquark pair disintegrate, decreases as the temperature increases. Furthermore, our assessment of the imaginary component of the potential indicates an increase in the magnitude of the imaginary potential for quarkonium as temperatures rise. This enhancement contributes to the quarkonium's suppression within the quark-gluon plasma, mirroring the anticipated outcomes from QCD. Our findings not only confirm the theoretical predictions but also demonstrate the efficacy of deep learning methods in advancing our understanding of high-energy particle physics.","sentences":["Utilizing an emergent metric developed from deep learning techniques, we determine the complex potential associated with static quarkonium.","This study explores the disintegration process of quarkonium by analyzing the real component of this potential, which is crucial for understanding its stability in various conditions.","We show that the dissociation length, the critical distance at which a quark and antiquark pair disintegrate, decreases as the temperature increases.","Furthermore, our assessment of the imaginary component of the potential indicates an increase in the magnitude of the imaginary potential for quarkonium as temperatures rise.","This enhancement contributes to the quarkonium's suppression within the quark-gluon plasma, mirroring the anticipated outcomes from QCD.","Our findings not only confirm the theoretical predictions but also demonstrate the efficacy of deep learning methods in advancing our understanding of high-energy particle physics."],"url":"http://arxiv.org/abs/2406.06285v1","category":"hep-ph"}
{"created":"2024-06-10 14:06:56","title":"An ODMA-Based Unsourced Random Access Scheme with a Multiple Antenna Receiver","abstract":"We investigate the unsourced random access scheme assuming that the base station is equipped with multiple antennas, and propose a high-performing solution utilizing on-off-division multiple access. We assume that each user spreads its pilot sequence and polar codeword to the pilot and data parts of the transmission frame, respectively, based on a transmission pattern. The iterative receiver operation consists of pilot and pattern detection followed by channel vector and symbol estimation, polar decoding, and successive interference cancellation. Numerical findings demonstrate that the proposed scheme has superior performance compared to the state-of-the-art in various antenna settings.","sentences":["We investigate the unsourced random access scheme assuming that the base station is equipped with multiple antennas, and propose a high-performing solution utilizing on-off-division multiple access.","We assume that each user spreads its pilot sequence and polar codeword to the pilot and data parts of the transmission frame, respectively, based on a transmission pattern.","The iterative receiver operation consists of pilot and pattern detection followed by channel vector and symbol estimation, polar decoding, and successive interference cancellation.","Numerical findings demonstrate that the proposed scheme has superior performance compared to the state-of-the-art in various antenna settings."],"url":"http://arxiv.org/abs/2406.06284v1","category":"cs.IT"}
{"created":"2024-06-10 14:00:53","title":"Applications and resource estimates for open system simulation on a quantum computer","abstract":"We study applications of a fully controllable open system quantum simulator. A universal quantum computer can realize such a simulator, and some classical methods can also approximate it. We introduce two concrete computational problems, such that finding their solution would have direct scientific or industrial utility. The scientific utility is exemplified by a computation of nonequilibrium behavior of Ca$_3$Co$_2$O$_6$, which has been studied in the costly MagLab experiments. Specifically, an order of \\$2M per material can be saved if an affordable quantum computer simulation is used to screen the materials before sending them to MagLab. For industrial utility, we develop a methodology that allows researchers of various backgrounds to estimate the economic value of an emerging technology consistently. We then apply our approach to the applications of materials with a Metal-Insulator Transition. These materials have not been used commercially yet, but many potential applications, including alternative transistors, neuromorphic computing, and smart windows, amount in total to \\$20M for the entire material search performed on a quantum computer. We provide zeroth-order resource estimates for both algorithms on superconducting qubit hardware, finding that simulating long-lifetime nonequilibrium effects presents a new challenge for quantum simulators. Finally, we introduce planted solution problems and their obfuscated versions to test the capabilities of the future quantum device. The intended use of those problems is to match the size of application benchmarks so that solving one guarantees the ability to solve the other.","sentences":["We study applications of a fully controllable open system quantum simulator.","A universal quantum computer can realize such a simulator, and some classical methods can also approximate it.","We introduce two concrete computational problems, such that finding their solution would have direct scientific or industrial utility.","The scientific utility is exemplified by a computation of nonequilibrium behavior of Ca$_3$Co$_2$O$_6$, which has been studied in the costly MagLab experiments.","Specifically, an order of \\$2M per material can be saved if an affordable quantum computer simulation is used to screen the materials before sending them to MagLab.","For industrial utility, we develop a methodology that allows researchers of various backgrounds to estimate the economic value of an emerging technology consistently.","We then apply our approach to the applications of materials with a Metal-Insulator Transition.","These materials have not been used commercially yet, but many potential applications, including alternative transistors, neuromorphic computing, and smart windows, amount in total to \\$20M for the entire material search performed on a quantum computer.","We provide zeroth-order resource estimates for both algorithms on superconducting qubit hardware, finding that simulating long-lifetime nonequilibrium effects presents a new challenge for quantum simulators.","Finally, we introduce planted solution problems and their obfuscated versions to test the capabilities of the future quantum device.","The intended use of those problems is to match the size of application benchmarks so that solving one guarantees the ability to solve the other."],"url":"http://arxiv.org/abs/2406.06281v1","category":"quant-ph"}
{"created":"2024-06-10 13:59:28","title":"Optimal sensing policy with interference-model uncertainty","abstract":"Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters. Sensing enables to improve the model knowledge and therefore perform a better link adaptation. However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication. We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication. % the following question therefore arises: At a given time slot, should one sense or communicate? We first show that this problem can be modelled in the Markov decision process (MDP) framework. We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm.","sentences":["Assume that an interferer behaves according to a parametric model but one does not know the value of the model parameters.","Sensing enables to improve the model knowledge and therefore perform a better link adaptation.","However, we consider a half-duplex scenario where, at each time slot, the communication system should decide between sensing and communication.","We thus propose to investigate the optimal policy to maximize the expected sum rate given a finite-time communication.","% the following question therefore arises: At a given time slot, should one sense or communicate?","We first show that this problem can be modelled in the Markov decision process (MDP) framework.","We then demonstrate that the optimal open-loop and closed-loop policies can be found significantly faster than the standard backward-induction algorithm."],"url":"http://arxiv.org/abs/2406.06280v1","category":"cs.IT"}
{"created":"2024-06-10 13:55:28","title":"Structural, magnetic and x-ray absorption spectroscopy studies of new Cr-based low, medium and high-entropy spinel oxides","abstract":"The emergence of high-entropy oxides has spurred significant research interest in recent times. These compounds exhibit exotic functional properties that often transcend simple linear combinations of their constituent elements. Herein, we present a new series of Cr-based low, medium, and high entropy spinel oxides with composition NiCr2O4, [Ni0.5Mn0.5]Cr2O4, [Ni0.33Mn0.33Co0.33]Cr2O4, [Ni0.25Mn0.25Co0.25Cu0.25]Cr2O4, [Ni0.2Mn0.2Co0.2Cu0.2Zn0.2]Cr2O4, and [Ni0.2Mg0.2Co0.2Cu0.2Zn0.2]Cr2O4. We conducted detailed structural (X-ray and Neutron diffraction), microstructural, Raman spectroscopy, magnetic, and X-ray absorption spectroscopy measurements on these materials. Our study reveals that the incorporation of multiple cations at the A-site of the structure (AB2O4) significantly modulates the magnetic properties. These compounds exhibit transitions from complex ferrimagnetic ([Ni0.2Mn0.2Co0.2Cu0.2Zn0.2]Cr2O4) to antiferromagnetic ([Ni0.2Mg0.2Co0.2Cu0.2Zn0.2]Cr2O4) states, with remarkable coercivity variations, demonstrating the ability to tailor magnetic responses through compositional design.","sentences":["The emergence of high-entropy oxides has spurred significant research interest in recent times.","These compounds exhibit exotic functional properties that often transcend simple linear combinations of their constituent elements.","Herein, we present a new series of Cr-based low, medium, and high entropy spinel oxides with composition NiCr2O4, [Ni0.5Mn0.5]Cr2O4, [Ni0.33Mn0.33Co0.33]Cr2O4, [Ni0.25Mn0.25Co0.25Cu0.25]Cr2O4, [Ni0.2Mn0.2Co0.2Cu0.2Zn0.2]Cr2O4, and [Ni0.2Mg0.2Co0.2Cu0.2Zn0.2]Cr2O4.","We conducted detailed structural (X-ray and Neutron diffraction), microstructural, Raman spectroscopy, magnetic, and X-ray absorption spectroscopy measurements on these materials.","Our study reveals that the incorporation of multiple cations at the A-site of the structure (AB2O4) significantly modulates the magnetic properties.","These compounds exhibit transitions from complex ferrimagnetic ([Ni0.2Mn0.2Co0.2Cu0.2Zn0.2]Cr2O4) to antiferromagnetic ([Ni0.2Mg0.2Co0.2Cu0.2Zn0.2]Cr2O4) states, with remarkable coercivity variations, demonstrating the ability to tailor magnetic responses through compositional design."],"url":"http://arxiv.org/abs/2406.06276v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 13:55:10","title":"Asymptotic limit of the compressible Navier-Stokes system on domains with rough boundaries","abstract":"In this paper, we study the asymptotic behavior of solutions to the compressible Navier-Stokes system considered on a sequence of spatial domains, whose boundaries exhibit fast oscillations with amplitude and characteristic wave length proportional to a small parameter. Imposing the full-slip boundary conditions we show that in the asymptotic limit the fluid sticks completely to the boundary, provided the oscillations are non-degenerate, meaning not oriented in a single direction.","sentences":["In this paper, we study the asymptotic behavior of solutions to the compressible Navier-Stokes system considered on a sequence of spatial domains, whose boundaries exhibit fast oscillations with amplitude and characteristic wave length proportional to a small parameter.","Imposing the full-slip boundary conditions we show that in the asymptotic limit the fluid sticks completely to the boundary, provided the oscillations are non-degenerate, meaning not oriented in a single direction."],"url":"http://arxiv.org/abs/2406.06275v1","category":"math.AP"}
{"created":"2024-06-10 13:53:31","title":"Quantum enhancements and entropic constraints to Boundary Time Crystals as sensors of AC fields","abstract":"We investigate the use of a boundary time crystals (BTCs) as sensors of AC fields. Boundary time crystals are non-equilibrium phases of matter in contact to an environment, for which a macroscopic fraction of the many-body system breaks the time translation symmetry. We find an enhanced sensitivity of the BTC when its spins are resonant with the applied AC field, as quantified by the quantum Fisher information (QFI). The QFI dynamics in this regime is shown to be captured by a relatively simple ansatz consisting of an initial power-law growth and late-time exponential decay. We study the scaling of the ansatz parameters with resources (encoding time and number of spins) and identify a moderate quantum enhancement in the sensor performance through comparison with classical QFI bounds. Investigating the precise source of this performance, we find that despite of its long coherence time and multipartite correlations (advantageous properties for quantum metrology), the entropic cost of the BTC (which grows indefinitely in the thermodynamic limit) hinders an optimal decoding of the AC field information. This result has implications for future candidates of quantum sensors in open system and we hope it will encourage future study into the role of entropy in quantum metrology.","sentences":["We investigate the use of a boundary time crystals (BTCs) as sensors of AC fields.","Boundary time crystals are non-equilibrium phases of matter in contact to an environment, for which a macroscopic fraction of the many-body system breaks the time translation symmetry.","We find an enhanced sensitivity of the BTC when its spins are resonant with the applied AC field, as quantified by the quantum Fisher information (QFI).","The QFI dynamics in this regime is shown to be captured by a relatively simple ansatz consisting of an initial power-law growth and late-time exponential decay.","We study the scaling of the ansatz parameters with resources (encoding time and number of spins) and identify a moderate quantum enhancement in the sensor performance through comparison with classical QFI bounds.","Investigating the precise source of this performance, we find that despite of its long coherence time and multipartite correlations (advantageous properties for quantum metrology), the entropic cost of the BTC (which grows indefinitely in the thermodynamic limit) hinders an optimal decoding of the AC field information.","This result has implications for future candidates of quantum sensors in open system and we hope it will encourage future study into the role of entropy in quantum metrology."],"url":"http://arxiv.org/abs/2406.06273v1","category":"quant-ph"}
{"created":"2024-06-10 13:34:43","title":"Stabilized Adaptive Steering for 3D Sonar Microphone Arrays with IMU Sensor Fusion","abstract":"This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars. Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts. Furthermore, mechanical stabilization can be more costly and prone to failure. We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground. Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images. We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements.","sentences":["This paper presents a novel software-based approach to stabilizing the acoustic images for in-air 3D sonars.","Due to uneven terrain, traditional static beamforming techniques can be misaligned, causing inaccurate measurements and imaging artifacts.","Furthermore, mechanical stabilization can be more costly and prone to failure.","We propose using an adaptive conventional beamforming approach by fusing it with real-time IMU data to adjust the sonar array's steering matrix dynamically based on the elevation tilt angle caused by the uneven ground.","Additionally, we propose gaining compensation to offset emission energy loss due to the transducer's directivity pattern and validate our approach through various experiments, which show significant improvements in temporal consistency in the acoustic images.","We implemented a GPU-accelerated software system that operates in real-time with an average execution time of 210ms, meeting autonomous navigation requirements."],"url":"http://arxiv.org/abs/2406.06255v1","category":"cs.RO"}
{"created":"2024-06-10 13:34:19","title":"PretVM: Predictable, Efficient Virtual Machine for Real-Time Concurrency","abstract":"This paper introduces the Precision-Timed Virtual Machine (PretVM), an intermediate platform facilitating the execution of quasi-static schedules compiled from a subset of programs written in the Lingua Franca (LF) coordination language. The subset consists of those programs that in principle should have statically verifiable and predictable timing behavior. The PretVM provides a schedule with well-defined worst-case timing bounds. The PretVM provides a clean separation between application logic and coordination logic, yielding more analyzable program executions. Experiments compare the PretVM against the default (more dynamic) LF scheduler and show that it delivers time-accurate deterministic execution.","sentences":["This paper introduces the Precision-Timed Virtual Machine (PretVM), an intermediate platform facilitating the execution of quasi-static schedules compiled from a subset of programs written in the Lingua Franca (LF) coordination language.","The subset consists of those programs that in principle should have statically verifiable and predictable timing behavior.","The PretVM provides a schedule with well-defined worst-case timing bounds.","The PretVM provides a clean separation between application logic and coordination logic, yielding more analyzable program executions.","Experiments compare the PretVM against the default (more dynamic) LF scheduler and show that it delivers time-accurate deterministic execution."],"url":"http://arxiv.org/abs/2406.06253v1","category":"eess.SY"}
{"created":"2024-06-10 13:29:22","title":"Asymptotic properties of infinitesimal characters and applications","abstract":"Inspired by classical results by Benoist, we introduce and study natural objects associated to an integrable tangent vector to the character variety $\\mathfrak X(\\Gamma,\\mathsf G)$ of a semi-group $\\Gamma$, with values on a semi-simple real algebraic group $\\mathsf G$ of the non-compact type. We obtain non-empty interior of the cone of Jordan variations and, when $\\mathsf G$ is split and Benoist's limit cone is sparse, we obtain double-density results associated to these variations, giving in particular non-empty interior of the set of length-normalized variations. We then apply the developed techniques to study pressure forms on the space of Anosov representations, in particular to higher-rank Teichm\\\"uller spaces. Among other things we exhibit an explicit functional $\\varphi\\in\\mathfrak a^*$ whose pressure form is compatible with Goldman's symplectic form at the Fuchsian points of the Hitchin component. We finally exhibit a Diophantine equation that governs the degeneration of the Hausdorff dimension of higher-quasi-circles.","sentences":["Inspired by classical results by Benoist, we introduce and study natural objects associated to an integrable tangent vector to the character variety $\\mathfrak X(\\Gamma,\\mathsf G)$ of a semi-group $\\Gamma$, with values on a semi-simple real algebraic group $\\mathsf G$ of the non-compact type.","We obtain non-empty interior of the cone of Jordan variations and, when $\\mathsf G$ is split and Benoist's limit cone is sparse, we obtain double-density results associated to these variations, giving in particular non-empty interior of the set of length-normalized variations.","We then apply the developed techniques to study pressure forms on the space of Anosov representations, in particular to higher-rank Teichm\\\"uller spaces.","Among other things we exhibit an explicit functional $\\varphi\\in\\mathfrak a^*$ whose pressure form is compatible with Goldman's symplectic form at the Fuchsian points of the Hitchin component.","We finally exhibit a Diophantine equation that governs the degeneration of the Hausdorff dimension of higher-quasi-circles."],"url":"http://arxiv.org/abs/2406.06250v1","category":"math.GR"}
{"created":"2024-06-10 13:26:08","title":"Hierarchical Cubes: Gibbs Measures and Decay of Correlations","abstract":"We study a hierarchical model of non-overlapping cubes of sidelengths $2^j$, $j \\in \\mathbb{Z}$. The model allows for cubes of arbitrarily small size and the activities need not be translationally invariant. It can also be recast as a spin system on a tree with long-range hard-core interaction. We prove necessary and sufficient conditions for the existence and uniqueness of Gibbs measures, discuss fragmentation and condensation, and prove bounds on the decay of two-point correlation functions.","sentences":["We study a hierarchical model of non-overlapping cubes of sidelengths $2^j$, $j \\in \\mathbb{Z}$. The model allows for cubes of arbitrarily small size and the activities need not be translationally invariant.","It can also be recast as a spin system on a tree with long-range hard-core interaction.","We prove necessary and sufficient conditions for the existence and uniqueness of Gibbs measures, discuss fragmentation and condensation, and prove bounds on the decay of two-point correlation functions."],"url":"http://arxiv.org/abs/2406.06249v1","category":"math-ph"}
{"created":"2024-06-10 13:19:29","title":"Exploring nonlinear dynamics in periodically driven time crystal: from synchronized to chaotic motion","abstract":"The coupled electron-nuclear spin system in an InGaAs semiconductor as testbed of nonlinear dynamics can develop auto-oscillations, resembling time-crystalline behavior, when continuously excited by a circularly polarized laser. We expose this system to deviations from continuous driving by periodic modulation of the excitation polarization, revealing a plethora of nonlinear phenomena that depend on modulation frequency and depth. We find ranges in which the system's oscillations are entrained with the modulation frequency. The width of these ranges depends on the polarization modulation depth, resulting in an Arnold tongue. Outside the tongue, the system shows a variety of fractional subharmonic responses connected through bifurcation jets when varying the modulation frequency. Here, each branch in the frequency spectrum forms a devil's staircase. When an entrainment range is approached by going through an increasing order of bifurcations, chaotic behavior emerges. These findings can be described by an advanced model of the periodically pumped electron-nuclear spin system. We discuss the connection of the obtained results to different phases of time matter.","sentences":["The coupled electron-nuclear spin system in an InGaAs semiconductor as testbed of nonlinear dynamics can develop auto-oscillations, resembling time-crystalline behavior, when continuously excited by a circularly polarized laser.","We expose this system to deviations from continuous driving by periodic modulation of the excitation polarization, revealing a plethora of nonlinear phenomena that depend on modulation frequency and depth.","We find ranges in which the system's oscillations are entrained with the modulation frequency.","The width of these ranges depends on the polarization modulation depth, resulting in an Arnold tongue.","Outside the tongue, the system shows a variety of fractional subharmonic responses connected through bifurcation jets when varying the modulation frequency.","Here, each branch in the frequency spectrum forms a devil's staircase.","When an entrainment range is approached by going through an increasing order of bifurcations, chaotic behavior emerges.","These findings can be described by an advanced model of the periodically pumped electron-nuclear spin system.","We discuss the connection of the obtained results to different phases of time matter."],"url":"http://arxiv.org/abs/2406.06243v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 13:14:31","title":"Practical Boolean Decomposition for Delay-driven LUT Mapping","abstract":"Ashenhurst-Curtis decomposition (ACD) is a decomposition technique used, in particular, to map combinational logic into lookup tables (LUTs) structures when synthesizing hardware designs. However, available implementations of ACD suffer from excessive complexity, search-space restrictions, and slow run time, which limit their applicability and scalability. This paper presents a novel fast and versatile technique of ACD suitable for delay optimization. We use this new formulation to compute two-level decompositions into a variable number of LUTs and enhance delay-driven LUT mapping by performing ACD on the fly. Compared to state-of-the-art technology mapping, experiments on heavily optimized benchmarks demonstrate an average delay improvement of 12.39%, and area reduction of 2.20% with affordable run time. Additionally, our method improves 4 of the best delay results in the EPFL synthesis competition without employing design-space exploration techniques.","sentences":["Ashenhurst-Curtis decomposition (ACD) is a decomposition technique used, in particular, to map combinational logic into lookup tables (LUTs) structures when synthesizing hardware designs.","However, available implementations of ACD suffer from excessive complexity, search-space restrictions, and slow run time, which limit their applicability and scalability.","This paper presents a novel fast and versatile technique of ACD suitable for delay optimization.","We use this new formulation to compute two-level decompositions into a variable number of LUTs and enhance delay-driven LUT mapping by performing ACD on the fly.","Compared to state-of-the-art technology mapping, experiments on heavily optimized benchmarks demonstrate an average delay improvement of 12.39%, and area reduction of 2.20% with affordable run time.","Additionally, our method improves 4 of the best delay results in the EPFL synthesis competition without employing design-space exploration techniques."],"url":"http://arxiv.org/abs/2406.06241v1","category":"cs.LO"}
{"created":"2024-06-10 13:03:37","title":"Exploring Altermagnetism in Orthorhombic $Pnma$ structure through Group Theory and DFT Calculations","abstract":"Antiferromagnetism, initially considered interesting but useless, recently emerged as one of the most promising magnetic phases for technology. Recently, a low symmetry antiferromagnetic phase, known as altermagnetic phase, have been discovered, where no time reversal ($\\mathcal{T}$) symmetry is observed in spite of a vanishing net magnetization, leading to non-degenerate bands from the opposite magnetic sublattices. In this work, we consider two representatives of orthorhombic $Pnma$ space group, namely, BiFeO$_3$ and CaMnO$_3$ and find altermagnetic lowest energy phase in both from our density functional theory calculations. We find a substantial spin-splitting in both systems along a high-symmetry path in the Brillouin zone without considering the spin-orbit interaction (SOI). Detailed features of the band dispersion obtained from our calculation confirm the lifting of sublattice spin degeneracy only in the $k_y$-$k_z$ plane while preserving the spin degeneracy in the other planes of the Brillouin zone. We provide a comprehensive symmetry analysis based on the magnetic space group (MSG) to explain our DFT findings and an insightful symmetry-allowed model Hamiltonian, which qualitatively agrees with our results. Additionally, we extend our symmetry analysis to encompass two other potential MSGs within the $Pnma$ space group that may host the spin-splitting phenomenon without considering SOI and the likely form of their Hamiltonian. These detailed studies pave the way for a deeper understanding of the spin-splitting phenomena within the $Pnma$ space group, offering insights into the intricate interplay between symmetry and electronic as well as magnetic properties.","sentences":["Antiferromagnetism, initially considered interesting but useless, recently emerged as one of the most promising magnetic phases for technology.","Recently, a low symmetry antiferromagnetic phase, known as altermagnetic phase, have been discovered, where no time reversal ($\\mathcal{T}$) symmetry is observed in spite of a vanishing net magnetization, leading to non-degenerate bands from the opposite magnetic sublattices.","In this work, we consider two representatives of orthorhombic $Pnma$ space group, namely, BiFeO$_3$ and CaMnO$_3$ and find altermagnetic lowest energy phase in both from our density functional theory calculations.","We find a substantial spin-splitting in both systems along a high-symmetry path in the Brillouin zone without considering the spin-orbit interaction (SOI).","Detailed features of the band dispersion obtained from our calculation confirm the lifting of sublattice spin degeneracy only in the $k_y$-$k_z$ plane while preserving the spin degeneracy in the other planes of the Brillouin zone.","We provide a comprehensive symmetry analysis based on the magnetic space group (MSG) to explain our DFT findings and an insightful symmetry-allowed model Hamiltonian, which qualitatively agrees with our results.","Additionally, we extend our symmetry analysis to encompass two other potential MSGs within the $Pnma$ space group that may host the spin-splitting phenomenon without considering SOI and the likely form of their Hamiltonian.","These detailed studies pave the way for a deeper understanding of the spin-splitting phenomena within the $Pnma$ space group, offering insights into the intricate interplay between symmetry and electronic as well as magnetic properties."],"url":"http://arxiv.org/abs/2406.06232v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 12:47:49","title":"Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis","abstract":"Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis. Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments. The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats. The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement. Data protection within the honeypot is fortified with probabilistic encryption. Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement. Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries. The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats.","sentences":["Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis.","Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments.","The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats.","The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement.","Data protection within the honeypot is fortified with probabilistic encryption.","Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement.","Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries.","The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats."],"url":"http://arxiv.org/abs/2406.06225v1","category":"cs.CR"}
{"created":"2024-06-10 12:35:09","title":"Synchronous Programming with Refinement Types","abstract":"Cyber-Physical Systems (CPS) consist of software interacting with the physical world, such as robots, vehicles, and industrial processes. CPS are frequently responsible for the safety of lives, property, or the environment, and so software correctness must be determined with a high degree of certainty. To that end, simply testing a CPS is insufficient, as its interactions with the physical world may be difficult to predict, and unsafe conditions may not be immediately obvious. Formal verification can provide stronger safety guarantees but relies on the accuracy of the verified system in representing the real system. Bringing together verification and implementation can be challenging, as languages that are typically used to implement CPS are not easy to formally verify, and languages that lend themselves well to verification often abstract away low-level implementation details. Translation between verification and implementation languages is possible, but requires additional assurances in the translation process and increases software complexity; having both in a single language is desirable. This paper presents a formalization of MARVeLus, a CPS language which combines verification and implementation. We develop a metatheory for its synchronous refinement type system and demonstrate verified synchronous programs executing on real systems.","sentences":["Cyber-Physical Systems (CPS) consist of software interacting with the physical world, such as robots, vehicles, and industrial processes.","CPS are frequently responsible for the safety of lives, property, or the environment, and so software correctness must be determined with a high degree of certainty.","To that end, simply testing a CPS is insufficient, as its interactions with the physical world may be difficult to predict, and unsafe conditions may not be immediately obvious.","Formal verification can provide stronger safety guarantees but relies on the accuracy of the verified system in representing the real system.","Bringing together verification and implementation can be challenging, as languages that are typically used to implement CPS are not easy to formally verify, and languages that lend themselves well to verification often abstract away low-level implementation details.","Translation between verification and implementation languages is possible, but requires additional assurances in the translation process and increases software complexity; having both in a single language is desirable.","This paper presents a formalization of MARVeLus, a CPS language which combines verification and implementation.","We develop a metatheory for its synchronous refinement type system and demonstrate verified synchronous programs executing on real systems."],"url":"http://arxiv.org/abs/2406.06221v1","category":"cs.PL"}
{"created":"2024-06-10 12:33:22","title":"Completeness classes in algebraic complexity theory","abstract":"The purpose of this overview is to explain the enormous impact of Les Valiant's eponymous short conference contribution from 1979 on the development of algebraic complexity.","sentences":["The purpose of this overview is to explain the enormous impact of Les Valiant's eponymous short conference contribution from 1979 on the development of algebraic complexity."],"url":"http://arxiv.org/abs/2406.06217v1","category":"cs.CC"}
{"created":"2024-06-10 12:24:07","title":"Sub-Landau levels in two-dimensional electron system in magnetic field","abstract":"Considering the role of electron correlation in a two-dimensional (2D) system in strong magnetic field, we investigate the quantum states of two interacting electrons in the quantum Hall effect regime. We introduce sub-Landau levels of the two-electron states by determining their energies and degeneracies. The effects of electron correlation, Zeeman splitting and energy-level broadening on the stability of the electron-pair states are discussed. It is shown that the two electrons in the triplet state with spins aligned parallel to the magnetic field can form a stable electron pair in the 2D system in GaAs/AlGaAs heterojunction. A wavefunction of many electron pairs are presented and compared with the Laughlin wavefunction.","sentences":["Considering the role of electron correlation in a two-dimensional (2D) system in strong magnetic field, we investigate the quantum states of two interacting electrons in the quantum Hall effect regime.","We introduce sub-Landau levels of the two-electron states by determining their energies and degeneracies.","The effects of electron correlation, Zeeman splitting and energy-level broadening on the stability of the electron-pair states are discussed.","It is shown that the two electrons in the triplet state with spins aligned parallel to the magnetic field can form a stable electron pair in the 2D system in GaAs/","AlGaAs heterojunction.","A wavefunction of many electron pairs are presented and compared with the Laughlin wavefunction."],"url":"http://arxiv.org/abs/2406.06212v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 12:09:56","title":"Rooted trees with level structures, $\u03a9$-classes and double ramification cycles","abstract":"We prove a new system of relations in the tautological ring of the moduli space of curves involving stable rooted trees with level structure decorated by the top Chern class of the Hodge bundle and $\\Omega$-classes and double ramification structures. In particular, this resolves a recent conjecture on these relations as well as connects with one of the two sides of the recently established DR/DZ equivalence between the integrable hierarchies constructions of Buryak and of Dubrovin--Zhang.","sentences":["We prove a new system of relations in the tautological ring of the moduli space of curves involving stable rooted trees with level structure decorated by the top Chern class of the Hodge bundle and $\\Omega$-classes and double ramification structures.","In particular, this resolves a recent conjecture on these relations as well as connects with one of the two sides of the recently established DR/DZ equivalence between the integrable hierarchies constructions of Buryak and of Dubrovin--Zhang."],"url":"http://arxiv.org/abs/2406.06205v1","category":"math.AG"}
{"created":"2024-06-10 11:50:36","title":"Bilard protonowy w LHC","abstract":"Proton-proton elastic scattering at high energy is a process with simple kinematics but surprisingly complex dynamics. Measurements of these processes at particle colliders require dedicated detectors and unusual measurement conditions. In the article, I describe the physics of elastic processes and the experimental method in the context of the measurement performed by the ATLAS Collaboration at the LHC accelerator. I present the key elements of the data analysis and discuss the consequences of the obtained results for our understanding of the strong interactions.   --   Rozpraszanie elastyczne proton-proton przy wysokich energiach jest procesem o prostej kinematyce, lecz zaskakuj\\k{a}co z\\l{}o\\.zonej dynamice. Pomiary tych proces\\'ow na zderzaczach cz\\k{a}stek wymagaj\\k{a} dedykowanych detektor\\'ow oraz nietypowych warunk\\'ow pomiarowych. W artykule opisuj\\k{e} fizyk\\k{e} proces\\'ow elastycznych oraz metod\\k{e} eksperymentaln\\k{a} w kontek\\'scie pomiaru wykonanego w ramach Wsp\\'o\\l{}pracy ATLAS na akceleratorze LHC. Przedstawiam kluczowe elementy analizy danych, a tak\\.ze omawiam konsekwencje uzyskanego wyniku dla naszego zrozumienia oddzia\\l{}ywa\\'n silnych.","sentences":["Proton-proton elastic scattering at high energy is a process with simple kinematics but surprisingly complex dynamics.","Measurements of these processes at particle colliders require dedicated detectors and unusual measurement conditions.","In the article, I describe the physics of elastic processes and the experimental method in the context of the measurement performed by the ATLAS Collaboration at the LHC accelerator.","I present the key elements of the data analysis and discuss the consequences of the obtained results for our understanding of the strong interactions.   ","--   Rozpraszanie elastyczne proton-proton przy wysokich energiach jest procesem o prostej kinematyce, lecz zaskakuj\\k{a}co","z\\l{}o\\.zonej dynamice.","Pomiary tych proces\\'ow na zderzaczach cz\\k{a}stek wymagaj\\k{a} dedykowanych detektor\\'ow oraz nietypowych warunk\\'ow pomiarowych.","W artykule opisuj\\k{e} fizyk\\k{e} proces\\'ow elastycznych oraz metod\\k{e} eksperymentaln\\k{a} w kontek\\'scie pomiaru wykonanego w ramach Wsp\\'o\\l{}pracy","ATLAS na akceleratorze LHC.","Przedstawiam kluczowe elementy analizy danych, a tak\\.ze omawiam konsekwencje uzyskanego wyniku dla naszego zrozumienia","oddzia\\l{}ywa\\'n silnych."],"url":"http://arxiv.org/abs/2406.06197v1","category":"hep-ex"}
{"created":"2024-06-10 11:50:09","title":"2D Moore CA with new boundary conditions and its reversibility","abstract":"In this paper, under certain conditions we consider two-dimensional cellular automata with the Moore neighborhood. Namely, the characterization of 2D linear cellular automata defined by the Moore neighborhood with some mixed boundary conditions over the field $\\mathbb{Z}_{p}$ is studied. Furthermore, we investigate the rule matrices of 2D Moore CA under some mixed boundary conditions by applying rotation. Finally, we give the conditions under which the obtained rule matrices for 2D finite CAs are reversible.","sentences":["In this paper, under certain conditions we consider two-dimensional cellular automata with the Moore neighborhood.","Namely, the characterization of 2D linear cellular automata defined by the Moore neighborhood with some mixed boundary conditions over the field $\\mathbb{Z}_{p}$ is studied.","Furthermore, we investigate the rule matrices of 2D Moore CA under some mixed boundary conditions by applying rotation.","Finally, we give the conditions under which the obtained rule matrices for 2D finite CAs are reversible."],"url":"http://arxiv.org/abs/2406.06195v1","category":"math.DS"}
{"created":"2024-06-10 11:25:09","title":"From microscopic to macroscopic: the large number dynamics of agents and cells, possibly interacting with a chemical background","abstract":"In this paper we review some recent results dealing with the transition between microscopic and macroscopic scales in different fields, including kinetic theory, cells movement in biology, chemotaxis, flocking phenomena and agent systems. The methodology of the mean-field approach of this study uses the concept of marginals instead of the empirical measure paradigm. Numerical computations showing some theoretically unexpected features are presented at the end of the paper.","sentences":["In this paper we review some recent results dealing with the transition between microscopic and macroscopic scales in different fields, including kinetic theory, cells movement in biology, chemotaxis, flocking phenomena and agent systems.","The methodology of the mean-field approach of this study uses the concept of marginals instead of the empirical measure paradigm.","Numerical computations showing some theoretically unexpected features are presented at the end of the paper."],"url":"http://arxiv.org/abs/2406.06180v1","category":"math.AP"}
{"created":"2024-06-10 11:13:42","title":"The wave function of stabilizer states and the Wehrl conjecture","abstract":"We focus on quantum systems represented by a Hilbert space $L^2(A)$, where $A$ is a locally compact Abelian group that contains a compact open subgroup. We examine two interconnected issues related to Weyl-Heisenberg operators. First, we provide a complete and elegant solution to the problem of describing the stabilizer states in terms of their wave functions, an issue that arises in quantum information theory. Subsequently, we demonstrate that the stabilizer states are precisely the minimizers of the Wehrl entropy functional, thereby resolving the analog of the Wehrl conjecture for any such group. Additionally, we construct a moduli space for the set of stabilizer states, that is, a parameterization of this set, that endows it with a natural algebraic structure, and we derive a formula for the number of stabilizer states when $A$ is finite. Notably, these results are novel even for finite Abelian groups.","sentences":["We focus on quantum systems represented by a Hilbert space $L^2(A)$, where $A$ is a locally compact Abelian group that contains a compact open subgroup.","We examine two interconnected issues related to Weyl-Heisenberg operators.","First, we provide a complete and elegant solution to the problem of describing the stabilizer states in terms of their wave functions, an issue that arises in quantum information theory.","Subsequently, we demonstrate that the stabilizer states are precisely the minimizers of the Wehrl entropy functional, thereby resolving the analog of the Wehrl conjecture for any such group.","Additionally, we construct a moduli space for the set of stabilizer states, that is, a parameterization of this set, that endows it with a natural algebraic structure, and we derive a formula for the number of stabilizer states when $A$ is finite.","Notably, these results are novel even for finite Abelian groups."],"url":"http://arxiv.org/abs/2406.06173v1","category":"math-ph"}
{"created":"2024-06-10 11:10:20","title":"Exact Entanglement correlation complements the chemical bond description","abstract":"We analyze the properties of the exact solution obtained by us recently for the extended Hetiler-London model for chemical bonding which has an analytic form. The emphasis is put on defining two-particle entanglement correlation as the complementary characterization of the chemical bond and relating it to the partial atomicity and the so-called true covalency. The newly introduced characteristics remove the deficiency of the standard definition of covalency which now vanishes in the limit of the separated atoms. In effect, a gradual evolution of the system of two indistinguishable electrons in a bound state into their distinguishable correspondents can be traced systematically. The present analysis has a universal meaning and may also be applied to more complex systems.","sentences":["We analyze the properties of the exact solution obtained by us recently for the extended Hetiler-London model for chemical bonding which has an analytic form.","The emphasis is put on defining two-particle entanglement correlation as the complementary characterization of the chemical bond and relating it to the partial atomicity and the so-called true covalency.","The newly introduced characteristics remove the deficiency of the standard definition of covalency which now vanishes in the limit of the separated atoms.","In effect, a gradual evolution of the system of two indistinguishable electrons in a bound state into their distinguishable correspondents can be traced systematically.","The present analysis has a universal meaning and may also be applied to more complex systems."],"url":"http://arxiv.org/abs/2406.06171v1","category":"cond-mat.str-el"}
{"created":"2024-06-10 10:31:39","title":"Towards a real-time distributed feedback system for the transportation assistance of PwD","abstract":"In this work we propose the design principles of an integrated distributed system for the augment of the transportation for people with disabilities inside the road network of a city area utilizing the IT technologies. We propose the basis of our system upon the utilization of a distributed sensor network that will be incorporated by a real-time integrated feedback system. The main components of the proposed architecture include the Inaccessible City Point System, the Live Data Analysis and Response System, and the Obstruction Detection and Prevention System. The incorporation of these subsystems will provide real-time feedback assisting the transportation of individuals with mobility problems informing them on real-time about blocked ramps across the path defined to their destination, being also responsible for the information of the authorities about incidents regarding the collision of accessibility in place where the sensors detect an inaccessible point. The proposed design allows the addition of further extensions regarding the assistance of individuals with mobility problems providing a basis for its further implementation and improvement. In this work we provide the fundamental parts regarding the interconnection of the proposed architecture's components as also its potential deployment regarding the proposed architecture and its application in the area of a city.","sentences":["In this work we propose the design principles of an integrated distributed system for the augment of the transportation for people with disabilities inside the road network of a city area utilizing the IT technologies.","We propose the basis of our system upon the utilization of a distributed sensor network that will be incorporated by a real-time integrated feedback system.","The main components of the proposed architecture include the Inaccessible City Point System, the Live Data Analysis and Response System, and the Obstruction Detection and Prevention System.","The incorporation of these subsystems will provide real-time feedback assisting the transportation of individuals with mobility problems informing them on real-time about blocked ramps across the path defined to their destination, being also responsible for the information of the authorities about incidents regarding the collision of accessibility in place where the sensors detect an inaccessible point.","The proposed design allows the addition of further extensions regarding the assistance of individuals with mobility problems providing a basis for its further implementation and improvement.","In this work we provide the fundamental parts regarding the interconnection of the proposed architecture's components as also its potential deployment regarding the proposed architecture and its application in the area of a city."],"url":"http://arxiv.org/abs/2406.06154v1","category":"cs.CY"}
{"created":"2024-06-10 10:30:43","title":"Gameful Introduction to Cryptography for Dyslexic Students","abstract":"Cryptography has a pivotal role in securing our digital world. Nonetheless, it is a challenging topic to learn. In this paper, we show that despite its complex nature, dyslexia$-$a learning disorder that influences reading and writing skills$-$does not hinder one's ability to comprehend cryptography. In particular, we conducted a gameful workshop with 14 high-school dyslexic students and taught them fundamental encryption methods. The students engaged well, learned the techniques, and enjoyed the training. We conclude that with a proper approach, dyslexia cannot hinder learning a complex subject such as cryptography.","sentences":["Cryptography has a pivotal role in securing our digital world.","Nonetheless, it is a challenging topic to learn.","In this paper, we show that despite its complex nature, dyslexia$-$a learning disorder that influences reading and writing skills$-$does not hinder one's ability to comprehend cryptography.","In particular, we conducted a gameful workshop with 14 high-school dyslexic students and taught them fundamental encryption methods.","The students engaged well, learned the techniques, and enjoyed the training.","We conclude that with a proper approach, dyslexia cannot hinder learning a complex subject such as cryptography."],"url":"http://arxiv.org/abs/2406.06153v1","category":"cs.SE"}
{"created":"2024-06-10 10:25:36","title":"On the Complexity of Inverse Bivariate Multi-unit Assignment Valuation Problems","abstract":"Inverse and bilevel optimization problems play a central role in both theory and applications. These two classes are known to be closely related due to the pioneering work of Dempe and Lohse (2006), and thus have often been discussed together ever since. In this paper, we consider inverse problems for multi-unit assignment valuations. Multi-unit assignment valuations form a subclass of strong-substitutes valuations that can be represented by edge-weighted complete bipartite graphs. These valuations play a key role in auction theory as the strong substitutes condition implies the existence of a Walrasian equilibrium. A recent line of research concentrated on the problem of deciding whether a bivariate valuation function is an assignment valuation or not. In this paper, we consider an \\emph{inverse} variant of the problem: we are given a bivariate function $g$, and our goal is to find a bivariate multi-unit assignment valuation function $f$ that is as close to $g$ as possible. The difference between $f$ and $g$ can be measured either in $\\ell_1$- or $\\ell_\\infty$-norm. Using tools from discrete convex analysis, we show that the problem is strongly NP-hard. On the other hand, we derive linear programming formulations that solve relaxed versions of the problem.","sentences":["Inverse and bilevel optimization problems play a central role in both theory and applications.","These two classes are known to be closely related due to the pioneering work of Dempe and Lohse (2006), and thus have often been discussed together ever since.","In this paper, we consider inverse problems for multi-unit assignment valuations.","Multi-unit assignment valuations form a subclass of strong-substitutes valuations that can be represented by edge-weighted complete bipartite graphs.","These valuations play a key role in auction theory as the strong substitutes condition implies the existence of a Walrasian equilibrium.","A recent line of research concentrated on the problem of deciding whether a bivariate valuation function is an assignment valuation or not.","In this paper, we consider an \\emph{inverse} variant of the problem: we are given a bivariate function $g$, and our goal is to find a bivariate multi-unit assignment valuation function $f$ that is as close to $g$ as possible.","The difference between $f$ and $g$ can be measured either in $\\ell_1$- or $\\ell_\\infty$-norm.","Using tools from discrete convex analysis, we show that the problem is strongly NP-hard.","On the other hand, we derive linear programming formulations that solve relaxed versions of the problem."],"url":"http://arxiv.org/abs/2406.06152v1","category":"math.OC"}
{"created":"2024-06-10 10:17:06","title":"Physics-Informed Bayesian Optimization of Variational Quantum Circuits","abstract":"In this paper, we propose a novel and powerful method to harness Bayesian optimization for Variational Quantum Eigensolvers (VQEs) -- a hybrid quantum-classical protocol used to approximate the ground state of a quantum Hamiltonian. Specifically, we derive a VQE-kernel which incorporates important prior information about quantum circuits: the kernel feature map of the VQE-kernel exactly matches the known functional form of the VQE's objective function and thereby significantly reduces the posterior uncertainty. Moreover, we propose a novel acquisition function for Bayesian optimization called Expected Maximum Improvement over Confident Regions (EMICoRe) which can actively exploit the inductive bias of the VQE-kernel by treating regions with low predictive uncertainty as indirectly ``observed''. As a result, observations at as few as three points in the search domain are sufficient to determine the complete objective function along an entire one-dimensional subspace of the optimization landscape. Our numerical experiments demonstrate that our approach improves over state-of-the-art baselines.","sentences":["In this paper, we propose a novel and powerful method to harness Bayesian optimization for Variational Quantum Eigensolvers (VQEs) -- a hybrid quantum-classical protocol used to approximate the ground state of a quantum Hamiltonian.","Specifically, we derive a VQE-kernel which incorporates important prior information about quantum circuits: the kernel feature map of the VQE-kernel exactly matches the known functional form of the VQE's objective function and thereby significantly reduces the posterior uncertainty.","Moreover, we propose a novel acquisition function for Bayesian optimization called Expected Maximum Improvement over Confident Regions (EMICoRe) which can actively exploit the inductive bias of the VQE-kernel by treating regions with low predictive uncertainty as indirectly ``observed''.","As a result, observations at as few as three points in the search domain are sufficient to determine the complete objective function along an entire one-dimensional subspace of the optimization landscape.","Our numerical experiments demonstrate that our approach improves over state-of-the-art baselines."],"url":"http://arxiv.org/abs/2406.06150v1","category":"cs.LG"}
{"created":"2024-06-10 09:47:29","title":"A story of cooperation: Centrosome-cytoskeleton interactions and implications","abstract":"A structural link between cell's centrosome and cytoskeleton has been proposed years ago. Centrosomes are usually located in the proximity to the nuclei and maintain nucleus-centrosome axis. This positioning aids in determining the polarity of interphase cells and ensure spindle assembly in mitotic cells. Centrosome also maintains physical interaction with different forms of cytoskeleton to trade-off between internal architecture and cell polarity in tissue specific as well as development specific manner. Several crosslinkers are also available to support this interaction and consequently promote cytoskeleton nucleation as well as centrosome nucleation. We present an overview of coordinated action of cytoskeletal elements on centrosomes and vice versa to modulate complex cellular functions, as diverse as cell migration, cell adhesion and cell division.","sentences":["A structural link between cell's centrosome and cytoskeleton has been proposed years ago.","Centrosomes are usually located in the proximity to the nuclei and maintain nucleus-centrosome axis.","This positioning aids in determining the polarity of interphase cells and ensure spindle assembly in mitotic cells.","Centrosome also maintains physical interaction with different forms of cytoskeleton to trade-off between internal architecture and cell polarity in tissue specific as well as development specific manner.","Several crosslinkers are also available to support this interaction and consequently promote cytoskeleton nucleation as well as centrosome nucleation.","We present an overview of coordinated action of cytoskeletal elements on centrosomes and vice versa to modulate complex cellular functions, as diverse as cell migration, cell adhesion and cell division."],"url":"http://arxiv.org/abs/2406.06135v1","category":"physics.bio-ph"}
{"created":"2024-06-10 09:24:48","title":"Rates for maps and flows in a deterministic multidimensional weak invariance principle","abstract":"We present the first rates of convergence to an $N$-dimensional Brownian motion when $N\\ge2$ for discrete and continuous time dynamical systems. Additionally, we provide the first rates for continuous time in any dimension. Our results hold for nonuniformly hyperbolic and expanding systems, such as Axiom A flows, suspensions over a Young tower with exponential tails, and some classes of intermittent solenoids.","sentences":["We present the first rates of convergence to an $N$-dimensional Brownian motion when $N\\ge2$ for discrete and continuous time dynamical systems.","Additionally, we provide the first rates for continuous time in any dimension.","Our results hold for nonuniformly hyperbolic and expanding systems, such as Axiom A flows, suspensions over a Young tower with exponential tails, and some classes of intermittent solenoids."],"url":"http://arxiv.org/abs/2406.06123v1","category":"math.DS"}
{"created":"2024-06-10 09:16:21","title":"Hawking-Page and entanglement phase transition in 2d CFT on curved backgrounds","abstract":"The thermodynamics and the entanglement properties of two-dimensional conformal field theories ($2$d CFTs) on curved backgrounds are studied. By means of conformal mapping we study the equivalent system on flat space governed by the deformed Hamiltonian, which is a spatial integral of the Hamiltonian density modulated by an enveloping function. Focusing on holographic CFTs, we observe Hawking-Page like phase transition for the thermal and the entanglement entropy as we vary the background metric. We also compute the mutual information to study the information theoretic correlation between parts of the curved spacetime. The gravity dual of 2d CFTs on curved background is also discussed.","sentences":["The thermodynamics and the entanglement properties of two-dimensional conformal field theories ($2$d CFTs) on curved backgrounds are studied.","By means of conformal mapping we study the equivalent system on flat space governed by the deformed Hamiltonian, which is a spatial integral of the Hamiltonian density modulated by an enveloping function.","Focusing on holographic CFTs, we observe Hawking-Page like phase transition for the thermal and the entanglement entropy as we vary the background metric.","We also compute the mutual information to study the information theoretic correlation between parts of the curved spacetime.","The gravity dual of 2d CFTs on curved background is also discussed."],"url":"http://arxiv.org/abs/2406.06121v1","category":"hep-th"}
{"created":"2024-06-10 09:02:07","title":"Model Updating for Nonlinear Systems with Stability Guarantees","abstract":"To improve the predictive capacity of system models in the input-output sense, this paper presents a framework for model updating via learning of modeling uncertainties in locally (and thus also in globally) Lipschitz nonlinear systems. First, we introduce a method to extend an existing known model with an uncertainty model so that stability of the extended model is guaranteed in the sense of set invariance and input-to-state stability. To achieve this, we provide two tractable semi-definite programs. These programs allow obtaining optimal uncertainty model parameters for both locally and globally Lipschitz nonlinear models, given uncertainty and state trajectories. Subsequently, in order to extract this data from the available input-output trajectories, we introduce a filter that incorporates an approximated internal model of the uncertainty and asymptotically estimates uncertainty and state realizations. This filter is also synthesized using semi-definite programs with guaranteed robustness with respect to uncertainty model mismatches, disturbances, and noise. Numerical simulations for a large data-set of a roll plane model of a vehicle illustrate the effectiveness and practicality of the proposed methodology in improving model accuracy, while guaranteeing stability.","sentences":["To improve the predictive capacity of system models in the input-output sense, this paper presents a framework for model updating via learning of modeling uncertainties in locally (and thus also in globally)","Lipschitz nonlinear systems.","First, we introduce a method to extend an existing known model with an uncertainty model so that stability of the extended model is guaranteed in the sense of set invariance and input-to-state stability.","To achieve this, we provide two tractable semi-definite programs.","These programs allow obtaining optimal uncertainty model parameters for both locally and globally Lipschitz nonlinear models, given uncertainty and state trajectories.","Subsequently, in order to extract this data from the available input-output trajectories, we introduce a filter that incorporates an approximated internal model of the uncertainty and asymptotically estimates uncertainty and state realizations.","This filter is also synthesized using semi-definite programs with guaranteed robustness with respect to uncertainty model mismatches, disturbances, and noise.","Numerical simulations for a large data-set of a roll plane model of a vehicle illustrate the effectiveness and practicality of the proposed methodology in improving model accuracy, while guaranteeing stability."],"url":"http://arxiv.org/abs/2406.06116v1","category":"eess.SY"}
{"created":"2024-06-10 08:38:57","title":"Correlated electrons of the flat band in charge density wave state of 4Hb-TaSexS2-x","abstract":"Many intriguing quantum states of matter, such as unconventional superconductivity, magnetic phases and fractional quantum Hall physics, emergent from the spatially-correlated localized electrons in the flat band of solid materials. By using scanning tunneling microscopy and spectroscopy (STM/STS), we report the real-space investigation of correlated electrons in the flat band of superlattice 4Hb-TaSexS2-x. In contrast with the pristine 4Hb-TaS2, the selenium (Se) substitutions significantly affect the interfacial transfer of correlated electrons between the CDW states of 1T- and 1H-TaS2 layers, and contribute a real-space fractional electron-filling configurations with the distributed electron-filled and -void SoD clusters of 1T-layer. The site-specific STS spectra directly reveal their respective prominent spectra weight above EF and symmetric Mott-like spectra. In addition, the spatial distributions of these electron-filled SoDs in the 1T-layer of 4Hb-TaSe0.7S1.3 demonstrate different local short-range patterning, clearly indicating the complex neighboring interactions among the localized electrons in the flat band of 1T-layer. Our results not only provide an in-depth insight of correlated electrons in the flat CDW band, and provide a simple platform to manipulate the electron-correlation-related quantum states.","sentences":["Many intriguing quantum states of matter, such as unconventional superconductivity, magnetic phases and fractional quantum Hall physics, emergent from the spatially-correlated localized electrons in the flat band of solid materials.","By using scanning tunneling microscopy and spectroscopy (STM/STS), we report the real-space investigation of correlated electrons in the flat band of superlattice 4Hb-TaSexS2-x.","In contrast with the pristine 4Hb-TaS2, the selenium (Se) substitutions significantly affect the interfacial transfer of correlated electrons between the CDW states of 1T- and 1H-TaS2 layers, and contribute a real-space fractional electron-filling configurations with the distributed electron-filled and -void SoD clusters of 1T-layer.","The site-specific STS spectra directly reveal their respective prominent spectra weight above EF and symmetric Mott-like spectra.","In addition, the spatial distributions of these electron-filled SoDs in the 1T-layer of 4Hb-TaSe0.7S1.3 demonstrate different local short-range patterning, clearly indicating the complex neighboring interactions among the localized electrons in the flat band of 1T-layer.","Our results not only provide an in-depth insight of correlated electrons in the flat CDW band, and provide a simple platform to manipulate the electron-correlation-related quantum states."],"url":"http://arxiv.org/abs/2406.06104v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 08:34:59","title":"Primitive Heavy-ball Dynamics Achieves $O(\\varepsilon^{-7/4})$ Convergence for Nonconvex Optimization","abstract":"First-order optimization methods for nonconvex functions with Lipschitz continuous gradients and Hessian have been studied intensively in both machine learning and optimization. State-of-the-art methods finding an $\\varepsilon$-stationary point within $O(\\varepsilon^{-{7/4}})$ or $\\tilde{O}(\\varepsilon^{-{7/4}})$ gradient evaluations are based on Nesterov's accelerated gradient descent (AGD) or Polyak's heavy-ball method. However, these algorithms employ additional mechanisms, such as restart schemes and negative curvature exploitation, which complicate the algorithms' behavior and make it challenging to apply them to more advanced settings (e.g., stochastic optimization). To realize a simpler algorithm, we investigate the heavy-ball differential equation, a continuous-time analogy of the AGD and heavy-ball methods; we prove that the dynamics attains an $\\varepsilon$-stationary point within $O(\\varepsilon^{-{7/4}})$ time. We also show that a vanilla heavy-ball algorithm, obtained by discretizing the dynamics, achieves the complexity of $O(\\varepsilon^{-{7/4}})$ under an additional assumption.","sentences":["First-order optimization methods for nonconvex functions with Lipschitz continuous gradients and Hessian have been studied intensively in both machine learning and optimization.","State-of-the-art methods finding an $\\varepsilon$-stationary point within $O(\\varepsilon^{-{7/4}})$ or $\\tilde{O}(\\varepsilon^{-{7/4}})$ gradient evaluations are based on Nesterov's accelerated gradient descent (AGD) or Polyak's heavy-ball method.","However, these algorithms employ additional mechanisms, such as restart schemes and negative curvature exploitation, which complicate the algorithms' behavior and make it challenging to apply them to more advanced settings (e.g., stochastic optimization).","To realize a simpler algorithm, we investigate the heavy-ball differential equation, a continuous-time analogy of the AGD and heavy-ball methods; we prove that the dynamics attains an $\\varepsilon$-stationary point within $O(\\varepsilon^{-{7/4}})$ time.","We also show that a vanilla heavy-ball algorithm, obtained by discretizing the dynamics, achieves the complexity of $O(\\varepsilon^{-{7/4}})$ under an additional assumption."],"url":"http://arxiv.org/abs/2406.06100v1","category":"math.OC"}
{"created":"2024-06-10 08:32:11","title":"Economic Model Predictive Control of Water Distribution Systems with Accelerated Optimization Algorithm","abstract":"Model predictive control (MPC) has emerged as an effective strategy for water distribution systems (WDSs) management. However, it is hampered by the computational burden for large-scale WDSs due to the combinatorial growth of possible control actions that must be evaluated at each time step. Therefore, a fast computation algorithm to implement MPC in WDSs can be obtained using a move-blocking approach that simplifies control decisions while ensuring solution feasibility. This paper introduces a least-restrictive move-blocking that interpolates the blocked control rate of change, aiming at balancing computational efficiency with operational effectiveness. The proposed control strategy is demonstrated on aggregated WDSs, encompassing multiple hydraulic elements. This implementation is incorporated into a multi-objective optimization framework that concurrently optimizes water level security of the storage tanks, smoothness of the control actions, and cost-effective objectives. A fair comparison between the proposed approach with the non-blocking Economic MPC is provided.","sentences":["Model predictive control (MPC) has emerged as an effective strategy for water distribution systems (WDSs) management.","However, it is hampered by the computational burden for large-scale WDSs due to the combinatorial growth of possible control actions that must be evaluated at each time step.","Therefore, a fast computation algorithm to implement MPC in WDSs can be obtained using a move-blocking approach that simplifies control decisions while ensuring solution feasibility.","This paper introduces a least-restrictive move-blocking that interpolates the blocked control rate of change, aiming at balancing computational efficiency with operational effectiveness.","The proposed control strategy is demonstrated on aggregated WDSs, encompassing multiple hydraulic elements.","This implementation is incorporated into a multi-objective optimization framework that concurrently optimizes water level security of the storage tanks, smoothness of the control actions, and cost-effective objectives.","A fair comparison between the proposed approach with the non-blocking Economic MPC is provided."],"url":"http://arxiv.org/abs/2406.06098v1","category":"eess.SY"}
{"created":"2024-06-10 08:27:43","title":"Space-Time Hopfion Crystals","abstract":"Hopfions, higher-dimensional topological quasiparticles with sophisticated 3D knotted spin textures discovered in condensed matter and photonic systems, show promise in high-density data storage and transfer. Here we present crystalline structures of hopfions lying in space-time constructed by spatiotemporally structured light. A practical methodology using bichromatic structured light beams or dipole arrays to assemble 1D and higher dimensional hopfion lattices is proposed and a technique for tailoring topological orders is elucidated. The birth of photonic hopfion crystals heralds a new era in high-dimensional, condensed, and robust topological information processing.","sentences":["Hopfions, higher-dimensional topological quasiparticles with sophisticated 3D knotted spin textures discovered in condensed matter and photonic systems, show promise in high-density data storage and transfer.","Here we present crystalline structures of hopfions lying in space-time constructed by spatiotemporally structured light.","A practical methodology using bichromatic structured light beams or dipole arrays to assemble 1D and higher dimensional hopfion lattices is proposed and a technique for tailoring topological orders is elucidated.","The birth of photonic hopfion crystals heralds a new era in high-dimensional, condensed, and robust topological information processing."],"url":"http://arxiv.org/abs/2406.06096v1","category":"physics.optics"}
{"created":"2024-06-10 08:25:38","title":"Elliptic Units Above Fields With Exactly One Complex Place","abstract":"In this work we explore the construction of abelian extensions of fields with exactly one complex place using multivariate analytic functions in the spirit of Hilbert's 12th problem. To this end we study the special values of the multiple elliptic Gamma functions introduced in the early 2000s by Nishizawa following the work of Felder and Varchenko on the elliptic Gamma function of Ruijsenaars. We construct geometric variants of these functions enjoying transformation properties under an action of $\\mathrm{SL}_{d}(\\mathbb{Z})$ for $d \\geq 2$. The evaluation of these functions at points of a degree $d$ field $\\mathbb{K}$ with exactly one complex place following the scheme of a recent article by Bergeron, Charollois and Garc\\'ia (arXiv:2311.04110) seems to produce algebraic numbers. More precisely, we conjecture that such infinite products yield algebraic units in abelian extensions of $\\mathbb{K}$ related to conjectural Stark units and we provide numerical evidence to support this conjecture for cubic, quartic and quintic fields.","sentences":["In this work we explore the construction of abelian extensions of fields with exactly one complex place using multivariate analytic functions in the spirit of Hilbert's 12th problem.","To this end we study the special values of the multiple elliptic Gamma functions introduced in the early 2000s by Nishizawa following the work of Felder and Varchenko on the elliptic Gamma function of Ruijsenaars.","We construct geometric variants of these functions enjoying transformation properties under an action of $\\mathrm{SL}_{d}(\\mathbb{Z})$ for $d \\geq 2$.","The evaluation of these functions at points of a degree $d$ field $\\mathbb{K}$ with exactly one complex place following the scheme of a recent article by Bergeron, Charollois and Garc\\'ia (arXiv:2311.04110) seems to produce algebraic numbers.","More precisely, we conjecture that such infinite products yield algebraic units in abelian extensions of $\\mathbb{K}$ related to conjectural Stark units and we provide numerical evidence to support this conjecture for cubic, quartic and quintic fields."],"url":"http://arxiv.org/abs/2406.06094v1","category":"math.NT"}
{"created":"2024-06-10 08:03:37","title":"The class and dynamics of $\u03b1$-balanced Polish groups","abstract":"For each ordinal $\\alpha<\\omega_1$, we introduce the class of $\\alpha$-balanced Polish groups. These classes form a hierarchy that completely stratifies the space between the class of Polish groups admitting a two-side-invariant metric (TSI) and the class of Polish groups admitting a complete left-invariant metric (CLI). We establish various closure properties, provide connections to model theory, and we develop a boundedness principle for CLI groups by showing that $\\alpha$-balancedness is an initial segment of a regular coanalytic rank.   In the spirit of Hjorth's turbulence theory we also introduce \"generic $\\alpha$-unbalancedness\": a new dynamical condition for Polish $G$-spaces which serves as an obstruction to classification by actions of $\\alpha$-balanced Polish groups. We use this to provide, for each $\\alpha<\\omega_1$, an action of an $\\alpha$-balanced Polish group whose orbit equivalence relation is strongly generically ergodic against actions of any $\\beta$-balanced Polish group with $\\beta<\\alpha$.","sentences":["For each ordinal $\\alpha<\\omega_1$, we introduce the class of $\\alpha$-balanced Polish groups.","These classes form a hierarchy that completely stratifies the space between the class of Polish groups admitting a two-side-invariant metric (TSI) and the class of Polish groups admitting a complete left-invariant metric (CLI).","We establish various closure properties, provide connections to model theory, and we develop a boundedness principle for CLI groups by showing that $\\alpha$-balancedness is an initial segment of a regular coanalytic rank.   ","In the spirit of Hjorth's turbulence theory we also introduce \"generic $\\alpha$-unbalancedness\": a new dynamical condition for Polish $G$-spaces which serves as an obstruction to classification by actions of $\\alpha$-balanced Polish groups.","We use this to provide, for each $\\alpha<\\omega_1$, an action of an $\\alpha$-balanced Polish group whose orbit equivalence relation is strongly generically ergodic against actions of any $\\beta$-balanced Polish group with $\\beta<\\alpha$."],"url":"http://arxiv.org/abs/2406.06082v1","category":"math.LO"}
{"created":"2024-06-10 07:49:51","title":"Supervised Radio Frequency Interference Detection with SNNs","abstract":"Radio Frequency Interference (RFI) poses a significant challenge in radio astronomy, arising from terrestrial and celestial sources, disrupting observations conducted by radio telescopes. Addressing RFI involves intricate heuristic algorithms, manual examination, and, increasingly, machine learning methods. Given the dynamic and temporal nature of radio astronomy observations, Spiking Neural Networks (SNNs) emerge as a promising approach. In this study, we cast RFI detection as a supervised multi-variate time-series segmentation problem. Notably, our investigation explores the encoding of radio astronomy visibility data for SNN inference, considering six encoding schemes: rate, latency, delta-modulation, and three variations of the step-forward algorithm. We train a small two-layer fully connected SNN on simulated data derived from the Hydrogen Epoch of Reionization Array (HERA) telescope and perform extensive hyper-parameter optimization. Results reveal that latency encoding exhibits superior performance, achieving a per-pixel accuracy of 98.8% and an f1-score of 0.761. Remarkably, these metrics approach those of contemporary RFI detection algorithms, notwithstanding the simplicity and compactness of our proposed network architecture. This study underscores the potential of RFI detection as a benchmark problem for SNN researchers, emphasizing the efficacy of SNNs in addressing complex time-series segmentation tasks in radio astronomy.","sentences":["Radio Frequency Interference (RFI) poses a significant challenge in radio astronomy, arising from terrestrial and celestial sources, disrupting observations conducted by radio telescopes.","Addressing RFI involves intricate heuristic algorithms, manual examination, and, increasingly, machine learning methods.","Given the dynamic and temporal nature of radio astronomy observations, Spiking Neural Networks (SNNs) emerge as a promising approach.","In this study, we cast RFI detection as a supervised multi-variate time-series segmentation problem.","Notably, our investigation explores the encoding of radio astronomy visibility data for SNN inference, considering six encoding schemes: rate, latency, delta-modulation, and three variations of the step-forward algorithm.","We train a small two-layer fully connected SNN on simulated data derived from the Hydrogen Epoch of Reionization Array (HERA) telescope and perform extensive hyper-parameter optimization.","Results reveal that latency encoding exhibits superior performance, achieving a per-pixel accuracy of 98.8% and an f1-score of 0.761.","Remarkably, these metrics approach those of contemporary RFI detection algorithms, notwithstanding the simplicity and compactness of our proposed network architecture.","This study underscores the potential of RFI detection as a benchmark problem for SNN researchers, emphasizing the efficacy of SNNs in addressing complex time-series segmentation tasks in radio astronomy."],"url":"http://arxiv.org/abs/2406.06075v1","category":"cs.NE"}
{"created":"2024-06-10 07:27:01","title":"Surface criticality in the mixed-field Ising model with sign-inverted next-nearest-neighbor interaction","abstract":"Rydberg atoms in an optical tweezer array have been used as a quantum simulator of the spin-$1/2$ antiferromagnetic Ising model with longitudinal and transverse fields. We suggest how to implement the next-nearest-neighbor (NNN) interaction whose sign is opposite to that of the nearest neighbor one in the Rydberg atom systems. We show that this can be achieved by weakly coupling one Rydberg state with another Rydberg state. We further study the surface criticality associated with the first-order quantum phase transition between the antiferromagnetic and paramagnetic phases, which emerges due to the sign-inverted NNN interaction. From the microscopic model, we derive a Ginzburg-Landau (GL) equation, which describes static and dynamic properties of the antiferromagnetic order parameter near the transition. Using both analytical GL theory and numerical method based on a mean-field theory, we calculate the order parameter in the proximity of a boundary of the system in order to show that the healing length of the order parameter logarithmically diverges, signaling the surface criticality.","sentences":["Rydberg atoms in an optical tweezer array have been used as a quantum simulator of the spin-$1/2$ antiferromagnetic Ising model with longitudinal and transverse fields.","We suggest how to implement the next-nearest-neighbor (NNN) interaction whose sign is opposite to that of the nearest neighbor one in the Rydberg atom systems.","We show that this can be achieved by weakly coupling one Rydberg state with another Rydberg state.","We further study the surface criticality associated with the first-order quantum phase transition between the antiferromagnetic and paramagnetic phases, which emerges due to the sign-inverted NNN interaction.","From the microscopic model, we derive a Ginzburg-Landau (GL) equation, which describes static and dynamic properties of the antiferromagnetic order parameter near the transition.","Using both analytical GL theory and numerical method based on a mean-field theory, we calculate the order parameter in the proximity of a boundary of the system in order to show that the healing length of the order parameter logarithmically diverges, signaling the surface criticality."],"url":"http://arxiv.org/abs/2406.06070v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-10 07:24:22","title":"PointABM:Integrating Bidirectional State Space Model with Multi-Head Self-Attention for Point Cloud Analysis","abstract":"Mamba, based on state space model (SSM) with its linear complexity and great success in classification provide its superiority in 3D point cloud analysis. Prior to that, Transformer has emerged as one of the most prominent and successful architectures for point cloud analysis. We present PointABM, a hybrid model that integrates the Mamba and Transformer architectures for enhancing local feature to improve performance of 3D point cloud analysis. In order to enhance the extraction of global features, we introduce a bidirectional SSM (bi-SSM) framework, which comprises both a traditional token forward SSM and an innovative backward SSM. To enhance the bi-SSM's capability of capturing more comprehensive features without disrupting the sequence relationships required by the bidirectional Mamba, we introduce Transformer, utilizing its self-attention mechanism to process point clouds. Extensive experimental results demonstrate that integrating Mamba with Transformer significantly enhance the model's capability to analysis 3D point cloud.","sentences":["Mamba, based on state space model (SSM) with its linear complexity and great success in classification provide its superiority in 3D point cloud analysis.","Prior to that, Transformer has emerged as one of the most prominent and successful architectures for point cloud analysis.","We present PointABM, a hybrid model that integrates the Mamba and Transformer architectures for enhancing local feature to improve performance of 3D point cloud analysis.","In order to enhance the extraction of global features, we introduce a bidirectional SSM (bi-SSM) framework, which comprises both a traditional token forward SSM and an innovative backward SSM.","To enhance the bi-SSM's capability of capturing more comprehensive features without disrupting the sequence relationships required by the bidirectional Mamba, we introduce Transformer, utilizing its self-attention mechanism to process point clouds.","Extensive experimental results demonstrate that integrating Mamba with Transformer significantly enhance the model's capability to analysis 3D point cloud."],"url":"http://arxiv.org/abs/2406.06069v1","category":"cs.CV"}
{"created":"2024-06-10 07:21:43","title":"6DMA Enhanced Wireless Network with Flexible Antenna Position and Rotation: Opportunities and Challenges","abstract":"6DMA (six-dimensional movable antenna) is a new and revolutionizing technology that fully exploits the wireless channel spatial variation at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and 3D rotations of distributed antennas/antenna surfaces (arrays). In this article, we provide an overview of 6DMA for unveiling its great potential in wireless networks, including its motivation and competitive advantages over existing technologies, system/channel modeling, and practical implementation. In particular, we present a variety of 6DMA-enabled performance enhancement in terms of array gain, spatial multiplexing, interference suppression, and geometric gain. Furthermore, we illustrate the main applications of 6DMA in wireless communication and sensing, and elaborate their design challenges as well as promising solutions. Finally, numerical results are provided to demonstrate the significant capacity improvement of 6DMA-aided communication in wireless network.","sentences":["6DMA (six-dimensional movable antenna) is a new and revolutionizing technology that fully exploits the wireless channel spatial variation at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and 3D rotations of distributed antennas/antenna surfaces (arrays).","In this article, we provide an overview of 6DMA for unveiling its great potential in wireless networks, including its motivation and competitive advantages over existing technologies, system/channel modeling, and practical implementation.","In particular, we present a variety of 6DMA-enabled performance enhancement in terms of array gain, spatial multiplexing, interference suppression, and geometric gain.","Furthermore, we illustrate the main applications of 6DMA in wireless communication and sensing, and elaborate their design challenges as well as promising solutions.","Finally, numerical results are provided to demonstrate the significant capacity improvement of 6DMA-aided communication in wireless network."],"url":"http://arxiv.org/abs/2406.06064v1","category":"cs.IT"}
{"created":"2024-06-10 07:14:56","title":"Learning Physical Simulation with Message Passing Transformer","abstract":"Machine learning methods for physical simulation have achieved significant success in recent years. We propose a new universal architecture based on Graph Neural Network, the Message Passing Transformer, which incorporates a Message Passing framework, employs an Encoder-Processor-Decoder structure, and applies Graph Fourier Loss as loss function for model optimization. To take advantage of the past message passing state information, we propose Hadamard-Product Attention to update the node attribute in the Processor, Hadamard-Product Attention is a variant of Dot-Product Attention that focuses on more fine-grained semantics and emphasizes on assigning attention weights over each feature dimension rather than each position in the sequence relative to others. We further introduce Graph Fourier Loss (GFL) to balance high-energy and low-energy components. To improve time performance, we precompute the graph's Laplacian eigenvectors before the training process. Our architecture achieves significant accuracy improvements in long-term rollouts for both Lagrangian and Eulerian dynamical systems over current methods.","sentences":["Machine learning methods for physical simulation have achieved significant success in recent years.","We propose a new universal architecture based on Graph Neural Network, the Message Passing Transformer, which incorporates a Message Passing framework, employs an Encoder-Processor-Decoder structure, and applies Graph Fourier Loss as loss function for model optimization.","To take advantage of the past message passing state information, we propose Hadamard-Product Attention to update the node attribute in the Processor, Hadamard-Product Attention is a variant of Dot-Product Attention that focuses on more fine-grained semantics and emphasizes on assigning attention weights over each feature dimension rather than each position in the sequence relative to others.","We further introduce Graph Fourier Loss (GFL) to balance high-energy and low-energy components.","To improve time performance, we precompute the graph's Laplacian eigenvectors before the training process.","Our architecture achieves significant accuracy improvements in long-term rollouts for both Lagrangian and Eulerian dynamical systems over current methods."],"url":"http://arxiv.org/abs/2406.06060v1","category":"cs.LG"}
{"created":"2024-06-10 07:11:11","title":"Feasibility of the observation of $\u03b7^{\\prime}$ mesic nuclei in the semi-exclusive $^{12}$C($p, dp$) reaction","abstract":"We study theoretically the feasibility of the semi-exclusive $^{12}$C($p,dp$)$X$ reaction for the observation of $\\eta^\\prime$ mesic nuclei using the transport model JAM. The semi-exclusive measurements of the ($p,d$) reaction with protons from $\\eta^\\prime$ non-mesonic two-body absorption ($\\eta^\\prime NN \\to NN$) are found to be critically important for the observation of the $\\eta^\\prime$ bound states. The Green's function method is used to calculate the momentum spectrum of forward going deuterons corresponding to the excitation energy spectrum of the $\\eta^\\prime \\otimes {}^{11}$C system in the semi-exclusive measurement. The semi-exclusive measurements are considered to be important in general for the $\\eta^\\prime$ mesic nucleus formation.","sentences":["We study theoretically the feasibility of the semi-exclusive $^{12}$C($p,dp$)$X$ reaction for the observation of $\\eta^\\prime$ mesic nuclei using the transport model JAM.","The semi-exclusive measurements of the ($p,d$) reaction with protons from $\\eta^\\prime$ non-mesonic two-body absorption ($\\eta^\\prime NN \\to NN$) are found to be critically important for the observation of the $\\eta^\\prime$ bound states.","The Green's function method is used to calculate the momentum spectrum of forward going deuterons corresponding to the excitation energy spectrum of the $\\eta^\\prime \\otimes {}^{11}$C system in the semi-exclusive measurement.","The semi-exclusive measurements are considered to be important in general for the $\\eta^\\prime$ mesic nucleus formation."],"url":"http://arxiv.org/abs/2406.06058v1","category":"nucl-th"}
{"created":"2024-06-10 07:06:27","title":"Mean-field games for harvesting problems: Uniqueness, long-time behaviour and weak KAM theory","abstract":"The goal of this paper is to study a Mean Field Game (MFG) system stemming from the harvesting of resources. Modelling the latter through a reaction-diffusion equation and the harvesters as competing rational agents, we are led to a non-local (in time and space) MFG system that consists of three equations, the study of which is quite delicate. The main focus of this paper is on the derivation of analytical results (e.g existence, uniqueness) and of long time behaviour (here, convergence to the ergodic system). We provide some explicit solutions to this ergodic system.","sentences":["The goal of this paper is to study a Mean Field Game (MFG) system stemming from the harvesting of resources.","Modelling the latter through a reaction-diffusion equation and the harvesters as competing rational agents, we are led to a non-local (in time and space) MFG system that consists of three equations, the study of which is quite delicate.","The main focus of this paper is on the derivation of analytical results (e.g existence, uniqueness) and of long time behaviour (here, convergence to the ergodic system).","We provide some explicit solutions to this ergodic system."],"url":"http://arxiv.org/abs/2406.06057v1","category":"math.AP"}
{"created":"2024-06-10 06:51:50","title":"Influence of Motion Restrictions in an Ankle Exoskeleton on Gait Kinematics and Stability in Straight Walking","abstract":"Exoskeleton devices impose kinematic constraints on a user's motion and affect their stability due to added mass but also due to the simplified mechanical design. This paper investigates how these constraints resulting from simplified mechanical designs impact the gait kinematics and stability of users by wearing an ankle exoskeleton with changeable degree of freedom (DoF). The exoskeleton used in this paper allows one, two, or three DoF at the ankle, simulating different levels of mechanical complexity. This effect was evaluated in a pilot study consisting of six participants walking on a straight path. The results show that increasing the exoskeleton DoF results in an improvement of several metrics, including kinematics and gait parameters. The transition from 1 DoF to 2 DoF is shown to have a larger effect than the transition from 2 DoF to 3 DoF for an ankle exoskeleton. However, an exoskeleton with 3 DoF at the ankle featured the best results. Increasing the number of DoF resulted in stability values closer the values when walking without the exoskeleton, despite the added weight of the exoskeleton.","sentences":["Exoskeleton devices impose kinematic constraints on a user's motion and affect their stability due to added mass but also due to the simplified mechanical design.","This paper investigates how these constraints resulting from simplified mechanical designs impact the gait kinematics and stability of users by wearing an ankle exoskeleton with changeable degree of freedom (DoF).","The exoskeleton used in this paper allows one, two, or three DoF at the ankle, simulating different levels of mechanical complexity.","This effect was evaluated in a pilot study consisting of six participants walking on a straight path.","The results show that increasing the exoskeleton DoF results in an improvement of several metrics, including kinematics and gait parameters.","The transition from 1 DoF to 2 DoF is shown to have a larger effect than the transition from 2 DoF to 3 DoF for an ankle exoskeleton.","However, an exoskeleton with 3 DoF at the ankle featured the best results.","Increasing the number of DoF resulted in stability values closer the values when walking without the exoskeleton, despite the added weight of the exoskeleton."],"url":"http://arxiv.org/abs/2406.06054v1","category":"cs.RO"}
{"created":"2024-06-10 06:30:45","title":"Enhancing Food Safety in Supply Chains: The Potential Role of Large Language Models in Preventing Campylobacter Contamination","abstract":"Foodborne diseases pose a significant global public health challenge, primarily driven by bacterial infections. Among these, Campylobacter spp. is notable, causing over 95 million cases annually. In response, the Hazard Analysis and Critical Control Points (HACCP) system, a food safety management framework, has been developed and is considered the most effective approach for systematically managing foodborne safety risks, including the prevention of bacterial contaminations, throughout the supply chain. Despite its efficacy, the adoption of HACCP is often incomplete across different sectors of the food industry. This limited implementation can be attributed to factors such as a lack of awareness, complex guidelines, confusing terminology, and insufficient training on the HACCP system's implementation. This study explores the potential of large language models (LLMs), specifically generative pre-trained transformers (GPTs), to mitigate Campylobacter contamination across four typical stages of the supply chain: primary production, food processing, distribution and retail, and preparation and consumption. While the interaction between LLMs and food safety presents a promising potential, it remains largely underexplored. To demonstrate the possible applications of LLMs in this domain, we further configure an open-access customized GPT trained on the FAO's HACCP toolbox and the 12 steps of HACCP implementation, and test it in the context of commercial food preparation. The study also considers critical barriers to implementing GPTs at each step of the supply chain and proposes initial measures to overcome these obstacles.","sentences":["Foodborne diseases pose a significant global public health challenge, primarily driven by bacterial infections.","Among these, Campylobacter spp.","is notable, causing over 95 million cases annually.","In response, the Hazard Analysis and Critical Control Points (HACCP) system, a food safety management framework, has been developed and is considered the most effective approach for systematically managing foodborne safety risks, including the prevention of bacterial contaminations, throughout the supply chain.","Despite its efficacy, the adoption of HACCP is often incomplete across different sectors of the food industry.","This limited implementation can be attributed to factors such as a lack of awareness, complex guidelines, confusing terminology, and insufficient training on the HACCP system's implementation.","This study explores the potential of large language models (LLMs), specifically generative pre-trained transformers (GPTs), to mitigate Campylobacter contamination across four typical stages of the supply chain: primary production, food processing, distribution and retail, and preparation and consumption.","While the interaction between LLMs and food safety presents a promising potential, it remains largely underexplored.","To demonstrate the possible applications of LLMs in this domain, we further configure an open-access customized GPT trained on the FAO's HACCP toolbox and the 12 steps of HACCP implementation, and test it in the context of commercial food preparation.","The study also considers critical barriers to implementing GPTs at each step of the supply chain and proposes initial measures to overcome these obstacles."],"url":"http://arxiv.org/abs/2406.06049v1","category":"cs.CY"}
{"created":"2024-06-10 06:28:51","title":"Wick rotation in the lapse: admissible complex metrics and the associated heat kernel","abstract":"A Wick rotation in the lapse (not in time) is introduced that interpolates between Riemannian and Lorentzian metrics on real manifolds admitting a co-dimension one foliation. The definition refers to a fiducial foliation but covariance under foliation changing diffeomorphisms is ensured. In particular, the resulting complex metrics are admissible in the sense of Kontsevich-Segal in all (fiducial and non-fiducial) foliations. This setting is used to construct a Wick rotated heat semigroup, which remains well-defined into the near Lorentzian regime. Among the results established for the Wick rotated version are: (i) Existence as an analytic semigroup uniquely determined by its sectorial generator. (ii) Construction of an integral kernel that is jointly smooth in the semigroup time and both spacetime arguments. (iii) Existence of an asymptotic expansion for the kernel's diagonal in (shifted) powers of the semigroup time whose coefficients are the Seeley-deWitt coefficients evaluated on the complex metrics. (iv) Emergence of a Schr\\\"{o}dinger evolution group in the strict Lorentzian limit. The toolbox includes local regularity results for admissible complex metrics.","sentences":["A Wick rotation in the lapse (not in time) is introduced that interpolates between Riemannian and Lorentzian metrics on real manifolds admitting a co-dimension one foliation.","The definition refers to a fiducial foliation but covariance under foliation changing diffeomorphisms is ensured.","In particular, the resulting complex metrics are admissible in the sense of Kontsevich-Segal in all (fiducial and non-fiducial) foliations.","This setting is used to construct a Wick rotated heat semigroup, which remains well-defined into the near Lorentzian regime.","Among the results established for the Wick rotated version are: (i) Existence as an analytic semigroup uniquely determined by its sectorial generator.","(ii) Construction of an integral kernel that is jointly smooth in the semigroup time and both spacetime arguments.","(iii) Existence of an asymptotic expansion for the kernel's diagonal in (shifted) powers of the semigroup time whose coefficients are the Seeley-deWitt coefficients evaluated on the complex metrics.","(iv) Emergence of a Schr\\\"{o}dinger evolution group in the strict Lorentzian limit.","The toolbox includes local regularity results for admissible complex metrics."],"url":"http://arxiv.org/abs/2406.06047v1","category":"math-ph"}
{"created":"2024-06-10 06:24:19","title":"FRAG: Frequency Adapting Group for Diffusion Video Editing","abstract":"In video editing, the hallmark of a quality edit lies in its consistent and unobtrusive adjustment. Modification, when integrated, must be smooth and subtle, preserving the natural flow and aligning seamlessly with the original vision. Therefore, our primary focus is on overcoming the current challenges in high quality edit to ensure that each edit enhances the final product without disrupting its intended essence. However, quality deterioration such as blurring and flickering is routinely observed in recent diffusion video editing systems. We confirm that this deterioration often stems from high-frequency leak: the diffusion model fails to accurately synthesize high-frequency components during denoising process. To this end, we devise Frequency Adapting Group (FRAG) which enhances the video quality in terms of consistency and fidelity by introducing a novel receptive field branch to preserve high-frequency components during the denoising process. FRAG is performed in a model-agnostic manner without additional training and validates the effectiveness on video editing benchmarks (i.e., TGVE, DAVIS).","sentences":["In video editing, the hallmark of a quality edit lies in its consistent and unobtrusive adjustment.","Modification, when integrated, must be smooth and subtle, preserving the natural flow and aligning seamlessly with the original vision.","Therefore, our primary focus is on overcoming the current challenges in high quality edit to ensure that each edit enhances the final product without disrupting its intended essence.","However, quality deterioration such as blurring and flickering is routinely observed in recent diffusion video editing systems.","We confirm that this deterioration often stems from high-frequency leak: the diffusion model fails to accurately synthesize high-frequency components during denoising process.","To this end, we devise Frequency Adapting Group (FRAG) which enhances the video quality in terms of consistency and fidelity by introducing a novel receptive field branch to preserve high-frequency components during the denoising process.","FRAG is performed in a model-agnostic manner without additional training and validates the effectiveness on video editing benchmarks (i.e., TGVE, DAVIS)."],"url":"http://arxiv.org/abs/2406.06044v1","category":"cs.CV"}
{"created":"2024-06-10 06:22:18","title":"Modeling User Retention through Generative Flow Networks","abstract":"Recommender systems aim to fulfill the user's daily demands. While most existing research focuses on maximizing the user's engagement with the system, it has recently been pointed out that how frequently the users come back for the service also reflects the quality and stability of recommendations. However, optimizing this user retention behavior is non-trivial and poses several challenges including the intractable leave-and-return user activities, the sparse and delayed signal, and the uncertain relations between users' retention and their immediate feedback towards each item in the recommendation list. In this work, we regard the retention signal as an overall estimation of the user's end-of-session satisfaction and propose to estimate this signal through a probabilistic flow. This flow-based modeling technique can back-propagate the retention reward towards each recommended item in the user session, and we show that the flow combined with traditional learning-to-rank objectives eventually optimizes a non-discounted cumulative reward for both immediate user feedback and user retention. We verify the effectiveness of our method through both offline empirical studies on two public datasets and online A/B tests in an industrial platform.","sentences":["Recommender systems aim to fulfill the user's daily demands.","While most existing research focuses on maximizing the user's engagement with the system, it has recently been pointed out that how frequently the users come back for the service also reflects the quality and stability of recommendations.","However, optimizing this user retention behavior is non-trivial and poses several challenges including the intractable leave-and-return user activities, the sparse and delayed signal, and the uncertain relations between users' retention and their immediate feedback towards each item in the recommendation list.","In this work, we regard the retention signal as an overall estimation of the user's end-of-session satisfaction and propose to estimate this signal through a probabilistic flow.","This flow-based modeling technique can back-propagate the retention reward towards each recommended item in the user session, and we show that the flow combined with traditional learning-to-rank objectives eventually optimizes a non-discounted cumulative reward for both immediate user feedback and user retention.","We verify the effectiveness of our method through both offline empirical studies on two public datasets and online A/B tests in an industrial platform."],"url":"http://arxiv.org/abs/2406.06043v1","category":"cs.IR"}
{"created":"2024-06-10 06:20:42","title":"Cartan monopoles","abstract":"The effective Hamiltonians for chiral supersymmetric gauge theories at small spatial volume are generalizations of the Hamiltonians describing the motion of a scalar or a spinor particle in a field of Dirac monopoles (we are dealing in fact with a certain lattice of monopoles supplemented with a periodic singular potential). The gauge fields in such Hamiltonians belong to the Cartan subalgebras of the corresponding gauge algebras. Such a construction exists for all groups admitting complex representations, i.e. for $SU(N \\geq 3), \\ Spin(4n+2)$ with $n \\geq 1$ and $E_6$. We give explicit expressions for these Hamiltonians for $SU(3)$, $SU(4) \\simeq Spin(6)$ and for $SU(5)$. The simplified version of such a Hamiltonian, deprived of fermion terms, of the extra scalar potential and when only one node of the lattice is taken into consideration, describe a $3r$-dimensional motion ($r$ being the rank of the group) in the field what we call a {\\it Cartan monopole}. As is the case for the ordinary monopole, the Lagrangian of this system enjoys gauge symmetry, rotational symmetry, and the parameter, generalizing the notion of magnetic charge for Cartan monopoles, is quantized.","sentences":["The effective Hamiltonians for chiral supersymmetric gauge theories at small spatial volume are generalizations of the Hamiltonians describing the motion of a scalar or a spinor particle in a field of Dirac monopoles (we are dealing in fact with a certain lattice of monopoles supplemented with a periodic singular potential).","The gauge fields in such Hamiltonians belong to the Cartan subalgebras of the corresponding gauge algebras.","Such a construction exists for all groups admitting complex representations, i.e. for $SU(N \\geq 3), \\ Spin(4n+2)$ with $n \\geq 1$ and $E_6$. We give explicit expressions for these Hamiltonians for $SU(3)$, $SU(4) \\simeq Spin(6)$ and for $SU(5)$. The simplified version of such a Hamiltonian, deprived of fermion terms, of the extra scalar potential and when only one node of the lattice is taken into consideration, describe a $3r$-dimensional motion ($r$ being the rank of the group) in the field what we call a {\\it Cartan monopole}.","As is the case for the ordinary monopole, the Lagrangian of this system enjoys gauge symmetry, rotational symmetry, and the parameter, generalizing the notion of magnetic charge for Cartan monopoles, is quantized."],"url":"http://arxiv.org/abs/2406.06042v1","category":"hep-th"}
{"created":"2024-06-10 06:19:33","title":"Risk Sensitivity in Markov Games and Multi-Agent Reinforcement Learning: A Systematic Review","abstract":"Markov games (MGs) and multi-agent reinforcement learning (MARL) are studied to model decision making in multi-agent systems. Traditionally, the objective in MG and MARL has been risk-neutral, i.e., agents are assumed to optimize a performance metric such as expected return, without taking into account subjective or cognitive preferences of themselves or of other agents. However, ignoring such preferences leads to inaccurate models of decision making in many real-world scenarios in finance, operations research, and behavioral economics. Therefore, when these preferences are present, it is necessary to incorporate a suitable measure of risk into the optimization objective of agents, which opens the door to risk-sensitive MG and MARL. In this paper, we systemically review the literature on risk sensitivity in MG and MARL that has been growing in recent years alongside other areas of reinforcement learning and game theory. We define and mathematically describe different risk measures used in MG and MARL and individually for each measure, discuss articles that incorporate it. Finally, we identify recent trends in theoretical and applied works in the field and discuss possible directions of future research.","sentences":["Markov games (MGs) and multi-agent reinforcement learning (MARL) are studied to model decision making in multi-agent systems.","Traditionally, the objective in MG and MARL has been risk-neutral, i.e., agents are assumed to optimize a performance metric such as expected return, without taking into account subjective or cognitive preferences of themselves or of other agents.","However, ignoring such preferences leads to inaccurate models of decision making in many real-world scenarios in finance, operations research, and behavioral economics.","Therefore, when these preferences are present, it is necessary to incorporate a suitable measure of risk into the optimization objective of agents, which opens the door to risk-sensitive MG and MARL.","In this paper, we systemically review the literature on risk sensitivity in MG and MARL that has been growing in recent years alongside other areas of reinforcement learning and game theory.","We define and mathematically describe different risk measures used in MG and MARL and individually for each measure, discuss articles that incorporate it.","Finally, we identify recent trends in theoretical and applied works in the field and discuss possible directions of future research."],"url":"http://arxiv.org/abs/2406.06041v1","category":"cs.GT"}
{"created":"2024-06-10 06:17:33","title":"Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset","abstract":"With the breakthrough of large models, Segment Anything Model (SAM) and its extensions have been attempted to apply in diverse tasks of computer vision. Underwater salient instance segmentation is a foundational and vital step for various underwater vision tasks, which often suffer from low segmentation accuracy due to the complex underwater circumstances and the adaptive ability of models. Moreover, the lack of large-scale datasets with pixel-level salient instance annotations has impeded the development of machine learning techniques in this field. To address these issues, we construct the first large-scale underwater salient instance segmentation dataset (USIS10K), which contains 10,632 underwater images with pixel-level annotations in 7 categories from various underwater scenes. Then, we propose an Underwater Salient Instance Segmentation architecture based on Segment Anything Model (USIS-SAM) specifically for the underwater domain. We devise an Underwater Adaptive Visual Transformer (UA-ViT) encoder to incorporate underwater domain visual prompts into the segmentation network. We further design an out-of-the-box underwater Salient Feature Prompter Generator (SFPG) to automatically generate salient prompters instead of explicitly providing foreground points or boxes as prompts in SAM. Comprehensive experimental results show that our USIS-SAM method can achieve superior performance on USIS10K datasets compared to the state-of-the-art methods. Datasets and codes are released on https://github.com/LiamLian0727/USIS10K.","sentences":["With the breakthrough of large models, Segment Anything Model (SAM) and its extensions have been attempted to apply in diverse tasks of computer vision.","Underwater salient instance segmentation is a foundational and vital step for various underwater vision tasks, which often suffer from low segmentation accuracy due to the complex underwater circumstances and the adaptive ability of models.","Moreover, the lack of large-scale datasets with pixel-level salient instance annotations has impeded the development of machine learning techniques in this field.","To address these issues, we construct the first large-scale underwater salient instance segmentation dataset (USIS10K), which contains 10,632 underwater images with pixel-level annotations in 7 categories from various underwater scenes.","Then, we propose an Underwater Salient Instance Segmentation architecture based on Segment Anything Model (USIS-SAM) specifically for the underwater domain.","We devise an Underwater Adaptive Visual Transformer (UA-ViT) encoder to incorporate underwater domain visual prompts into the segmentation network.","We further design an out-of-the-box underwater Salient Feature Prompter Generator (SFPG) to automatically generate salient prompters instead of explicitly providing foreground points or boxes as prompts in SAM.","Comprehensive experimental results show that our USIS-SAM method can achieve superior performance on USIS10K datasets compared to the state-of-the-art methods.","Datasets and codes are released on https://github.com/LiamLian0727/USIS10K."],"url":"http://arxiv.org/abs/2406.06039v1","category":"cs.CV"}
{"created":"2024-06-10 05:56:34","title":"Shesha: Multi-head Microarchitectural Leakage Discovery in new-generation Intel Processors","abstract":"Transient execution attacks have been one of the widely explored microarchitectural side channels since the discovery of Spectre and Meltdown. However, much of the research has been driven by manual discovery of new transient paths through well-known speculative events. Although a few attempts exist in literature on automating transient leakage discovery, such tools focus on finding variants of known transient attacks and explore a small subset of instruction set. Further, they take a random fuzzing approach that does not scale as the complexity of search space increases. In this work, we identify that the search space of bad speculation is disjointedly fragmented into equivalence classes, and then use this observation to develop a framework named Shesha, inspired by Particle Swarm Optimization, which exhibits faster convergence rates than state-of-the-art fuzzing techniques for automatic discovery of transient execution attacks. We then use Shesha to explore the vast search space of extensions to the x86 Instruction Set Architecture (ISEs), thereby focusing on previously unexplored avenues of bad speculation. As such, we report five previously unreported transient execution paths in Instruction Set Extensions (ISEs) on new generation of Intel processors. We then perform extensive reverse engineering of each of the transient execution paths and provide root-cause analysis. Using the discovered transient execution paths, we develop attack building blocks to exhibit exploitable transient windows. Finally, we demonstrate data leakage from Fused Multiply-Add instructions through SIMD buffer and extract victim data from various cryptographic implementations.","sentences":["Transient execution attacks have been one of the widely explored microarchitectural side channels since the discovery of Spectre and Meltdown.","However, much of the research has been driven by manual discovery of new transient paths through well-known speculative events.","Although a few attempts exist in literature on automating transient leakage discovery, such tools focus on finding variants of known transient attacks and explore a small subset of instruction set.","Further, they take a random fuzzing approach that does not scale as the complexity of search space increases.","In this work, we identify that the search space of bad speculation is disjointedly fragmented into equivalence classes, and then use this observation to develop a framework named Shesha, inspired by Particle Swarm Optimization, which exhibits faster convergence rates than state-of-the-art fuzzing techniques for automatic discovery of transient execution attacks.","We then use Shesha to explore the vast search space of extensions to the x86 Instruction Set Architecture (ISEs), thereby focusing on previously unexplored avenues of bad speculation.","As such, we report five previously unreported transient execution paths in Instruction Set Extensions (ISEs) on new generation of Intel processors.","We then perform extensive reverse engineering of each of the transient execution paths and provide root-cause analysis.","Using the discovered transient execution paths, we develop attack building blocks to exhibit exploitable transient windows.","Finally, we demonstrate data leakage from Fused Multiply-Add instructions through SIMD buffer and extract victim data from various cryptographic implementations."],"url":"http://arxiv.org/abs/2406.06034v1","category":"cs.CR"}
{"created":"2024-06-10 05:53:42","title":"Elasticity of fibres prefers the chaos of turbulence","abstract":"The dynamics of fibres, modelled as a sequence of inertial beads linked via elastic springs, in turbulent flows is dictated by a non-trivial interplay of their inertia and elasticity. Such elastic, inertial fibres preferentially sample a three-dimensional turbulent flow in a manner qualitatively similar to that in two-dimensions [Singh et al., Phys. Rev. E 101, 053105 (2020)]. Inertia and elasticity have competing effects on fibre dynamics: Inertia drives fibres away from vortices while elasticity tends to trap them inside. However, both these effects are reversed at large values. A large inertia makes the fibres sample the flow more uniformly while a very large elasticity facilitates the sampling of straining regions. This complex sampling behaviour is further corroborated by quantifying the chaotic nature of sampled flow regions. This is achieved by evaluating the maximal Lagrangian Lyapunov Exponents associated with the flow along fibre trajectories.","sentences":["The dynamics of fibres, modelled as a sequence of inertial beads linked via elastic springs, in turbulent flows is dictated by a non-trivial interplay of their inertia and elasticity.","Such elastic, inertial fibres preferentially sample a three-dimensional turbulent flow in a manner qualitatively similar to that in two-dimensions","[Singh et al., Phys.","Rev. E 101, 053105 (2020)].","Inertia and elasticity have competing effects on fibre dynamics: Inertia drives fibres away from vortices while elasticity tends to trap them inside.","However, both these effects are reversed at large values.","A large inertia makes the fibres sample the flow more uniformly while a very large elasticity facilitates the sampling of straining regions.","This complex sampling behaviour is further corroborated by quantifying the chaotic nature of sampled flow regions.","This is achieved by evaluating the maximal Lagrangian Lyapunov Exponents associated with the flow along fibre trajectories."],"url":"http://arxiv.org/abs/2406.06033v1","category":"physics.flu-dyn"}
{"created":"2024-06-10 05:43:55","title":"A WT-ResNet based fault diagnosis model for the urban rail train transmission system","abstract":"This study presents a novel fault diagnosis model for urban rail transit systems based on Wavelet Transform Residual Neural Network (WT-ResNet). The model integrates the advantages of wavelet transform for feature extraction and ResNet for pattern recognition, offering enhanced diagnostic accuracy and robustness. Experimental results demonstrate the effectiveness of the proposed model in identifying faults in urban rail trains, paving the way for improved maintenance strategies and reduced downtime.","sentences":["This study presents a novel fault diagnosis model for urban rail transit systems based on Wavelet Transform Residual Neural Network (WT-ResNet).","The model integrates the advantages of wavelet transform for feature extraction and ResNet for pattern recognition, offering enhanced diagnostic accuracy and robustness.","Experimental results demonstrate the effectiveness of the proposed model in identifying faults in urban rail trains, paving the way for improved maintenance strategies and reduced downtime."],"url":"http://arxiv.org/abs/2406.06031v1","category":"cs.IR"}
{"created":"2024-06-10 05:40:03","title":"ReCon1M:A Large-scale Benchmark Dataset for Relation Comprehension in Remote Sensing Imagery","abstract":"Scene Graph Generation (SGG) is a high-level visual understanding and reasoning task aimed at extracting entities (such as objects) and their interrelationships from images. Significant progress has been made in the study of SGG in natural images in recent years, but its exploration in the domain of remote sensing images remains very limited. The complex characteristics of remote sensing images necessitate higher time and manual interpretation costs for annotation compared to natural images. The lack of a large-scale public SGG benchmark is a major impediment to the advancement of SGG-related research in aerial imagery. In this paper, we introduce the first publicly available large-scale, million-level relation dataset in the field of remote sensing images which is named as ReCon1M. Specifically, our dataset is built upon Fair1M and comprises 21,392 images. It includes annotations for 859,751 object bounding boxes across 60 different categories, and 1,149,342 relation triplets across 64 categories based on these bounding boxes. We provide a detailed description of the dataset's characteristics and statistical information. We conducted two object detection tasks and three sub-tasks within SGG on this dataset, assessing the performance of mainstream methods on these tasks.","sentences":["Scene Graph Generation (SGG) is a high-level visual understanding and reasoning task aimed at extracting entities (such as objects) and their interrelationships from images.","Significant progress has been made in the study of SGG in natural images in recent years, but its exploration in the domain of remote sensing images remains very limited.","The complex characteristics of remote sensing images necessitate higher time and manual interpretation costs for annotation compared to natural images.","The lack of a large-scale public SGG benchmark is a major impediment to the advancement of SGG-related research in aerial imagery.","In this paper, we introduce the first publicly available large-scale, million-level relation dataset in the field of remote sensing images which is named as ReCon1M. Specifically, our dataset is built upon Fair1M and comprises 21,392 images.","It includes annotations for 859,751 object bounding boxes across 60 different categories, and 1,149,342 relation triplets across 64 categories based on these bounding boxes.","We provide a detailed description of the dataset's characteristics and statistical information.","We conducted two object detection tasks and three sub-tasks within SGG on this dataset, assessing the performance of mainstream methods on these tasks."],"url":"http://arxiv.org/abs/2406.06028v1","category":"cs.CV"}
{"created":"2024-06-10 05:22:49","title":"HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs","abstract":"Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to $67\\%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method. Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue).","sentences":["Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions.","However, as the complexity of the questions increase, the performance of LLMs degrade.","We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text.","Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing.","However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context.","To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information.","The use of our compressed distilled KG as input to the LLM results in our method utilizing up to $67\\%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method.","Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue)."],"url":"http://arxiv.org/abs/2406.06027v1","category":"cs.CL"}
{"created":"2024-06-10 05:08:14","title":"Zak-OTFS and Turbo Signal Processing for Joint Sensing and Communication","abstract":"The Zak-OTFS input/output (I/O) relation is predictable and non-fading when the delay and Doppler periods are greater than the effective channel delay and Doppler spreads, a condition which we refer to as the crystallization condition. The filter taps can simply be read off from the response to a single Zak-OTFS pilot pulsone, and the I/O relation can be reconstructed for a sampled system that operates under finite duration and bandwidth constraints. In previous work we had measured BER performance of a baseline system where we used separate Zak-OTFS subframes for sensing and data transmission. In this Letter we demonstrate how to use turbo signal processing to match BER performance of this baseline system when we integrate sensing and communication within the same Zak-OTFS subframe. The turbo decoder alternates between channel sensing using a noise-like waveform (spread pulsone) and recovery of data transmitted using point pulsones.","sentences":["The Zak-OTFS input/output (I/O) relation is predictable and non-fading when the delay and Doppler periods are greater than the effective channel delay and Doppler spreads, a condition which we refer to as the crystallization condition.","The filter taps can simply be read off from the response to a single Zak-OTFS pilot pulsone, and the I/O relation can be reconstructed for a sampled system that operates under finite duration and bandwidth constraints.","In previous work we had measured BER performance of a baseline system where we used separate Zak-OTFS subframes for sensing and data transmission.","In this Letter we demonstrate how to use turbo signal processing to match BER performance of this baseline system when we integrate sensing and communication within the same Zak-OTFS subframe.","The turbo decoder alternates between channel sensing using a noise-like waveform (spread pulsone) and recovery of data transmitted using point pulsones."],"url":"http://arxiv.org/abs/2406.06024v1","category":"eess.SP"}
{"created":"2024-06-10 04:56:16","title":"GraphStorm: all-in-one graph machine learning framework for industry applications","abstract":"Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a dozen billion-scale industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.","sentences":["Graph machine learning (GML) is effective in many business applications.","However, making GML easy to use and applicable to industry applications with massive datasets remain challenging.","We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference.","GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code.","GraphStorm has been used and deployed for over a dozen billion-scale industry applications after its release in May 2023.","It is open-sourced in Github: https://github.com/awslabs/graphstorm."],"url":"http://arxiv.org/abs/2406.06022v1","category":"cs.LG"}
{"created":"2024-06-10 04:41:48","title":"Quantifying dissipation in stochastic complex oscillations","abstract":"Fluctuations-driven complex oscillations are experimentally observed in cellular systems such as hepatocytes, cardiac cells, neuronal cells, etc. These systems are generally operating in regimes far from thermodynamic equilibrium. To study nonequilibrium thermodynamic properties such as energy dissipation in stochastic complex oscillations, we consider stochastic modeling of two nonlinear biological oscillators, namely, the intracellular calcium (Ca$^{2+}$) oscillation model and the Hindmarsh-Rose model for neuronal dynamics. These models exhibit various types of complex oscillations like bursting and quasi-periodic oscillations for various system parameter values. In this work, we formulate open chemical reaction schemes for the two model systems driving the systems far from thermodynamic equilibrium. We then analyze the steady-state total entropy production rate (EPR) in the various types of stochastic complex oscillations. Our results show higher values of steady-state total EPR in stochastic complex oscillations than simple periodic oscillations. Moreover, in the Hindmarsh-Rose neuronal model, we observe an order-to-disorder transition from periodic (organized) bursts of spikes to chaotic (unorganized) oscillations with distinct behaviors of steady-state total EPR. Our results reveal that stochastic complex oscillations are produced at the cost of higher energy consumption and that it requires a higher thermodynamic cost to maintain the periodic bursts than chaotic oscillations. Our findings indicate that complex cellular regulatory or signaling processes by Ca$^{2+}$ that help perform complex tasks of the nervous system or rich information coding by neurons involve a higher thermodynamic cost. The results deepen our understanding of energy dissipation in nonlinear, nonequilibrium biological systems with stochastic complex oscillatory dynamics.","sentences":["Fluctuations-driven complex oscillations are experimentally observed in cellular systems such as hepatocytes, cardiac cells, neuronal cells, etc.","These systems are generally operating in regimes far from thermodynamic equilibrium.","To study nonequilibrium thermodynamic properties such as energy dissipation in stochastic complex oscillations, we consider stochastic modeling of two nonlinear biological oscillators, namely, the intracellular calcium (Ca$^{2+}$) oscillation model and the Hindmarsh-Rose model for neuronal dynamics.","These models exhibit various types of complex oscillations like bursting and quasi-periodic oscillations for various system parameter values.","In this work, we formulate open chemical reaction schemes for the two model systems driving the systems far from thermodynamic equilibrium.","We then analyze the steady-state total entropy production rate (EPR) in the various types of stochastic complex oscillations.","Our results show higher values of steady-state total EPR in stochastic complex oscillations than simple periodic oscillations.","Moreover, in the Hindmarsh-Rose neuronal model, we observe an order-to-disorder transition from periodic (organized) bursts of spikes to chaotic (unorganized) oscillations with distinct behaviors of steady-state total EPR.","Our results reveal that stochastic complex oscillations are produced at the cost of higher energy consumption and that it requires a higher thermodynamic cost to maintain the periodic bursts than chaotic oscillations.","Our findings indicate that complex cellular regulatory or signaling processes by Ca$^{2+}$ that help perform complex tasks of the nervous system or rich information coding by neurons involve a higher thermodynamic cost.","The results deepen our understanding of energy dissipation in nonlinear, nonequilibrium biological systems with stochastic complex oscillatory dynamics."],"url":"http://arxiv.org/abs/2406.06019v1","category":"physics.chem-ph"}
{"created":"2024-06-10 04:41:11","title":"Delayed supermartingale convergence lemmas for stochastic approximation with Nesterov momentum","abstract":"This paper focus on the convergence of stochastic approximation with Nesterov momentum. Nesterov acceleration has proven effective in machine learning for its ability to reduce computational complexity. The issue of delayed information in the acceleration term remains a challenge to achieving the almost sure convergence. Based on the delayed supermatingale convergence lemmas, we give a series of framework for almost sure convergence. Our framework applies to several widely-used random iterative methods, such as stochastic subgradient methods, the proximal Robbins-Monro method for general stochastic optimization, and the proximal stochastic subgradient method for composite optimization. Through the applications of our framework, these methods with Nesterov acceleration achieve almost sure convergence. And three groups of numerical experiments is to check out theoretical results.","sentences":["This paper focus on the convergence of stochastic approximation with Nesterov momentum.","Nesterov acceleration has proven effective in machine learning for its ability to reduce computational complexity.","The issue of delayed information in the acceleration term remains a challenge to achieving the almost sure convergence.","Based on the delayed supermatingale convergence lemmas, we give a series of framework for almost sure convergence.","Our framework applies to several widely-used random iterative methods, such as stochastic subgradient methods, the proximal Robbins-Monro method for general stochastic optimization, and the proximal stochastic subgradient method for composite optimization.","Through the applications of our framework, these methods with Nesterov acceleration achieve almost sure convergence.","And three groups of numerical experiments is to check out theoretical results."],"url":"http://arxiv.org/abs/2406.06018v1","category":"math.OC"}
{"created":"2024-06-10 04:30:48","title":"Fault-tolerant resource estimation using graph-state compilation on a modular superconducting architecture","abstract":"The development of fault-tolerant quantum computers (FTQCs) is gaining increased attention within the quantum computing community. Like their digital counterparts, FTQCs, equipped with error correction and large qubit numbers, promise to solve some of humanity's grand challenges. Estimates of the resource requirements for future FTQC systems are essential to making design choices and prioritizing R&D efforts to develop critical technologies. Here, we present a resource estimation framework and software tool that estimates the physical resources required to execute specific quantum algorithms, compiled into their graph-state form, and laid out onto a modular superconducting hardware architecture. This tool can predict the size, power consumption, and execution time of these algorithms at as they approach utility-scale according to explicit assumptions about the system's physical layout, thermal load, and modular connectivity. We use this tool to study the total resources on a proposed modular architecture and the impact of tradeoffs between and inter-module connectivity, latency and resource requirements.","sentences":["The development of fault-tolerant quantum computers (FTQCs) is gaining increased attention within the quantum computing community.","Like their digital counterparts, FTQCs, equipped with error correction and large qubit numbers, promise to solve some of humanity's grand challenges.","Estimates of the resource requirements for future FTQC systems are essential to making design choices and prioritizing R&D efforts to develop critical technologies.","Here, we present a resource estimation framework and software tool that estimates the physical resources required to execute specific quantum algorithms, compiled into their graph-state form, and laid out onto a modular superconducting hardware architecture.","This tool can predict the size, power consumption, and execution time of these algorithms at as they approach utility-scale according to explicit assumptions about the system's physical layout, thermal load, and modular connectivity.","We use this tool to study the total resources on a proposed modular architecture and the impact of tradeoffs between and inter-module connectivity, latency and resource requirements."],"url":"http://arxiv.org/abs/2406.06015v1","category":"quant-ph"}
{"created":"2024-06-10 04:28:37","title":"Network two-sample test for block models","abstract":"We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model. Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons. We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models. The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs. We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test. We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test. Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data.","sentences":["We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model.","Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons.","We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models.","The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs.","We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test.","We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test.","Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data."],"url":"http://arxiv.org/abs/2406.06014v1","category":"math.ST"}
{"created":"2024-06-10 04:22:14","title":"Nanoparticle uptake by a semi-permeable, spherical cell from an external planar diffusive field. II. Numerical study of temporal and spatial development validated using FEM","abstract":"In this paper we present a mathematical study of particle diffusion inside and outside a spherical biological cell that has been exposed on one side to a propagating planar diffusive front. The media inside and outside the spherical cell are differentiated by their respective diffusion constants. A closed form, large-time, asymptotic solution is derived by the combined means of Laplace transform, separation of variables and asymptotic series development. The solution process is assisted by means of an effective far-field boundary condition, which is instrumental in resolving the conflict of planar and spherical geometries. The focus of the paper is on a numerical comparison to determine the accuracy of the asymptotic solution relative to a fully numerical solution obtained using the finite element method. The asymptotic solution is shown to be highly effective in capturing the dynamic behaviour of the system, both internal and external to the cell, under a range of diffusive conditions.","sentences":["In this paper we present a mathematical study of particle diffusion inside and outside a spherical biological cell that has been exposed on one side to a propagating planar diffusive front.","The media inside and outside the spherical cell are differentiated by their respective diffusion constants.","A closed form, large-time, asymptotic solution is derived by the combined means of Laplace transform, separation of variables and asymptotic series development.","The solution process is assisted by means of an effective far-field boundary condition, which is instrumental in resolving the conflict of planar and spherical geometries.","The focus of the paper is on a numerical comparison to determine the accuracy of the asymptotic solution relative to a fully numerical solution obtained using the finite element method.","The asymptotic solution is shown to be highly effective in capturing the dynamic behaviour of the system, both internal and external to the cell, under a range of diffusive conditions."],"url":"http://arxiv.org/abs/2406.06013v1","category":"physics.bio-ph"}
{"created":"2024-06-10 04:21:27","title":"Quantum Sparse Coding and Decoding Based on Quantum Network","abstract":"Sparse coding provides a versatile framework for efficiently capturing and representing crucial data (information) concisely, which plays an essential role in various computer science fields, including data compression, feature extraction, and general signal processing. In this study, we propose a symmetric quantum neural network for realizing sparse coding and decoding algorithms. Our networks consist of multi-layer, two-level unitary transformations that are naturally suited for optical circuits. Each gate is described by two real parameters, corresponding to reflectivity and phase shift. Specifically, the two networks can be efficiently trained together or separately using a quantum natural gradient descent algorithm, either simultaneously or independently. Utilizing the trained model, we achieve sparse coding and decoding of binary and grayscale images in classical problems, as well as that of complex quantum states in quantum problems separately. The results demonstrate an accuracy of 98.77\\% for image reconstruction and a fidelity of 97.68\\% for quantum state revivification. Our quantum sparse coding and decoding model offers improved generalization and robustness compared to the classical model, laying the groundwork for widespread practical applications in the emerging quantum era.","sentences":["Sparse coding provides a versatile framework for efficiently capturing and representing crucial data (information) concisely, which plays an essential role in various computer science fields, including data compression, feature extraction, and general signal processing.","In this study, we propose a symmetric quantum neural network for realizing sparse coding and decoding algorithms.","Our networks consist of multi-layer, two-level unitary transformations that are naturally suited for optical circuits.","Each gate is described by two real parameters, corresponding to reflectivity and phase shift.","Specifically, the two networks can be efficiently trained together or separately using a quantum natural gradient descent algorithm, either simultaneously or independently.","Utilizing the trained model, we achieve sparse coding and decoding of binary and grayscale images in classical problems, as well as that of complex quantum states in quantum problems separately.","The results demonstrate an accuracy of 98.77\\% for image reconstruction and a fidelity of 97.68\\% for quantum state revivification.","Our quantum sparse coding and decoding model offers improved generalization and robustness compared to the classical model, laying the groundwork for widespread practical applications in the emerging quantum era."],"url":"http://arxiv.org/abs/2406.06012v1","category":"quant-ph"}
{"created":"2024-06-10 04:12:57","title":"Turbulent spectra of 2D inertial gravity waves","abstract":"We find the turbulent energy spectrum of weakly interacting 2D internal gravity waves without relying on the hydrostatic approximation. This spectrum is an exact solution of the kinetic equation after the shear modes are removed, and it agrees with the oceanic Garrett-Munk spectrum for frequencies large compared to the Coriolis frequency and vertical scales small compared to the depth of the ocean. Among a continuous family of solutions to the kinetic equation, we show that the turbulent solution is the special solution with non zero angular dependent radial flux. Our solution provides an interesting insight of how turbulent energy cascade proceeds in anisotropic systems - similarly to isotropic turbulence it is self-similar in scale, but its angular part is peaked along the curve of vanishing frequency.","sentences":["We find the turbulent energy spectrum of weakly interacting 2D internal gravity waves without relying on the hydrostatic approximation.","This spectrum is an exact solution of the kinetic equation after the shear modes are removed, and it agrees with the oceanic Garrett-Munk spectrum for frequencies large compared to the Coriolis frequency and vertical scales small compared to the depth of the ocean.","Among a continuous family of solutions to the kinetic equation, we show that the turbulent solution is the special solution with non zero angular dependent radial flux.","Our solution provides an interesting insight of how turbulent energy cascade proceeds in anisotropic systems - similarly to isotropic turbulence it is self-similar in scale, but its angular part is peaked along the curve of vanishing frequency."],"url":"http://arxiv.org/abs/2406.06010v1","category":"physics.flu-dyn"}
{"created":"2024-06-10 04:06:55","title":"Thanking the World: Exploring Gender-Based Differences in Acknowledgment Patterns and Support Systems in Theses","abstract":"Research on acknowledgment sections of scientific papers has gained significant attention, but there remains a dearth of studies examining acknowledgments in the context of Electronic Theses and Dissertations. This paper addresses this gap by investigating the sources of support for male and female researchers in completing their master's or doctoral theses, focusing on the discipline of Library and Information Science. We utilize a novel method of extracting the various types of support systems that are acknowledged in 1252 ETDs using RoBERTa-based models. The most prominent forms of support acknowledged by researchers are academic, moral, financial, and religious support. While there are no significant gender-based differences in religious and financial support, the ratio of academic to moral support acknowledged by researchers shows strong gender-based variation. Additionally, advisors display a preference for supervising same-gender researchers. By comprehending the nuances of support systems and the unique challenges faced by researchers of different genders, we can foster a more inclusive and supportive academic environment. The insights gained from this research have implications for improving mentoring practices and promoting gender equality in academia.","sentences":["Research on acknowledgment sections of scientific papers has gained significant attention, but there remains a dearth of studies examining acknowledgments in the context of Electronic Theses and Dissertations.","This paper addresses this gap by investigating the sources of support for male and female researchers in completing their master's or doctoral theses, focusing on the discipline of Library and Information Science.","We utilize a novel method of extracting the various types of support systems that are acknowledged in 1252 ETDs using RoBERTa-based models.","The most prominent forms of support acknowledged by researchers are academic, moral, financial, and religious support.","While there are no significant gender-based differences in religious and financial support, the ratio of academic to moral support acknowledged by researchers shows strong gender-based variation.","Additionally, advisors display a preference for supervising same-gender researchers.","By comprehending the nuances of support systems and the unique challenges faced by researchers of different genders, we can foster a more inclusive and supportive academic environment.","The insights gained from this research have implications for improving mentoring practices and promoting gender equality in academia."],"url":"http://arxiv.org/abs/2406.06006v1","category":"cs.DL"}
{"created":"2024-06-10 04:00:55","title":"WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts","abstract":"Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models. Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences. In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages. Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task. We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing. We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks.","sentences":["Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models.","Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences.","In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages.","Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task.","We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing.","We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks."],"url":"http://arxiv.org/abs/2406.06005v1","category":"cs.RO"}
{"created":"2024-06-10 03:51:38","title":"Computational and Statistical Guarantees for Tensor-on-Tensor Regression with Tensor Train Decomposition","abstract":"Recently, a tensor-on-tensor (ToT) regression model has been proposed to generalize tensor recovery, encompassing scenarios like scalar-on-tensor regression and tensor-on-vector regression. However, the exponential growth in tensor complexity poses challenges for storage and computation in ToT regression. To overcome this hurdle, tensor decompositions have been introduced, with the tensor train (TT)-based ToT model proving efficient in practice due to reduced memory requirements, enhanced computational efficiency, and decreased sampling complexity. Despite these practical benefits, a disparity exists between theoretical analysis and real-world performance. In this paper, we delve into the theoretical and algorithmic aspects of the TT-based ToT regression model. Assuming the regression operator satisfies the restricted isometry property (RIP), we conduct an error analysis for the solution to a constrained least-squares optimization problem. This analysis includes upper error bound and minimax lower bound, revealing that such error bounds polynomially depend on the order $N+M$. To efficiently find solutions meeting such error bounds, we propose two optimization algorithms: the iterative hard thresholding (IHT) algorithm (employing gradient descent with TT-singular value decomposition (TT-SVD)) and the factorization approach using the Riemannian gradient descent (RGD) algorithm. When RIP is satisfied, spectral initialization facilitates proper initialization, and we establish the linear convergence rate of both IHT and RGD.","sentences":["Recently, a tensor-on-tensor (ToT) regression model has been proposed to generalize tensor recovery, encompassing scenarios like scalar-on-tensor regression and tensor-on-vector regression.","However, the exponential growth in tensor complexity poses challenges for storage and computation in ToT regression.","To overcome this hurdle, tensor decompositions have been introduced, with the tensor train (TT)-based ToT model proving efficient in practice due to reduced memory requirements, enhanced computational efficiency, and decreased sampling complexity.","Despite these practical benefits, a disparity exists between theoretical analysis and real-world performance.","In this paper, we delve into the theoretical and algorithmic aspects of the TT-based ToT regression model.","Assuming the regression operator satisfies the restricted isometry property (RIP), we conduct an error analysis for the solution to a constrained least-squares optimization problem.","This analysis includes upper error bound and minimax lower bound, revealing that such error bounds polynomially depend on the order $N+M$. To efficiently find solutions meeting such error bounds, we propose two optimization algorithms: the iterative hard thresholding (IHT) algorithm (employing gradient descent with TT-singular value decomposition (TT-SVD)) and the factorization approach using the Riemannian gradient descent (RGD) algorithm.","When RIP is satisfied, spectral initialization facilitates proper initialization, and we establish the linear convergence rate of both IHT and RGD."],"url":"http://arxiv.org/abs/2406.06002v1","category":"cs.LG"}
{"created":"2024-06-10 03:34:27","title":"Symmetries in the linear theory of the deformation of thin shells","abstract":"In this paper, we show that symmetries, which are known in the theory of integrable systems, naturally appeared in the classical linear theory of deformations of thin shells. Our result shows that if the middle surface of a shell becomes `integrable', infinitely many deformations exist that have no shear strains and twisting of the coordinate lines.","sentences":["In this paper, we show that symmetries, which are known in the theory of integrable systems, naturally appeared in the classical linear theory of deformations of thin shells.","Our result shows that if the middle surface of a shell becomes `integrable', infinitely many deformations exist that have no shear strains and twisting of the coordinate lines."],"url":"http://arxiv.org/abs/2406.05997v1","category":"math.DG"}
{"created":"2024-06-10 17:44:59","title":"Verification-Guided Shielding for Deep Reinforcement Learning","abstract":"In recent years, Deep Reinforcement Learning (DRL) has emerged as an effective approach to solving real-world tasks. However, despite their successes, DRL-based policies suffer from poor reliability, which limits their deployment in safety-critical domains. As a result, various methods have been put forth to address this issue by providing formal safety guarantees. Two main approaches include shielding and verification. While shielding ensures the safe behavior of the policy by employing an external online component (i.e., a ``shield'') that overruns potentially dangerous actions, this approach has a significant computational cost as the shield must be invoked at runtime to validate every decision. On the other hand, verification is an offline process that can identify policies that are unsafe, prior to their deployment, yet, without providing alternative actions when such a policy is deemed unsafe. In this work, we present verification-guided shielding -- a novel approach that bridges the DRL reliability gap by integrating these two methods. Our approach combines both formal and probabilistic verification tools to partition the input domain into safe and unsafe regions. In addition, we employ clustering and symbolic representation procedures that compress the unsafe regions into a compact representation. This, in turn, allows to temporarily activate the shield solely in (potentially) unsafe regions, in an efficient manner. Our novel approach allows to significantly reduce runtime overhead while still preserving formal safety guarantees. We extensively evaluate our approach on two benchmarks from the robotic navigation domain, as well as provide an in-depth analysis of its scalability and completeness.","sentences":["In recent years, Deep Reinforcement Learning (DRL) has emerged as an effective approach to solving real-world tasks.","However, despite their successes, DRL-based policies suffer from poor reliability, which limits their deployment in safety-critical domains.","As a result, various methods have been put forth to address this issue by providing formal safety guarantees.","Two main approaches include shielding and verification.","While shielding ensures the safe behavior of the policy by employing an external online component (i.e., a ``shield'') that overruns potentially dangerous actions, this approach has a significant computational cost as the shield must be invoked at runtime to validate every decision.","On the other hand, verification is an offline process that can identify policies that are unsafe, prior to their deployment, yet, without providing alternative actions when such a policy is deemed unsafe.","In this work, we present verification-guided shielding -- a novel approach that bridges the DRL reliability gap by integrating these two methods.","Our approach combines both formal and probabilistic verification tools to partition the input domain into safe and unsafe regions.","In addition, we employ clustering and symbolic representation procedures that compress the unsafe regions into a compact representation.","This, in turn, allows to temporarily activate the shield solely in (potentially) unsafe regions, in an efficient manner.","Our novel approach allows to significantly reduce runtime overhead while still preserving formal safety guarantees.","We extensively evaluate our approach on two benchmarks from the robotic navigation domain, as well as provide an in-depth analysis of its scalability and completeness."],"url":"http://arxiv.org/abs/2406.06507v1","category":"cs.LG"}
{"created":"2024-06-10 17:20:13","title":"Graph-Based Bidirectional Transformer Decision Threshold Adjustment Algorithm for Class-Imbalanced Molecular Data","abstract":"Data sets with imbalanced class sizes, often where one class size is much smaller than that of others, occur extremely often in various applications, including those with biological foundations, such as drug discovery and disease diagnosis. Thus, it is extremely important to be able to identify data elements of classes of various sizes, as a failure to detect can result in heavy costs. However, many data classification algorithms do not perform well on imbalanced data sets as they often fail to detect elements belonging to underrepresented classes. In this paper, we propose the BTDT-MBO algorithm, incorporating Merriman-Bence-Osher (MBO) techniques and a bidirectional transformer, as well as distance correlation and decision threshold adjustments, for data classification problems on highly imbalanced molecular data sets, where the sizes of the classes vary greatly. The proposed method not only integrates adjustments in the classification threshold for the MBO algorithm in order to help deal with the class imbalance, but also uses a bidirectional transformer model based on an attention mechanism for self-supervised learning. Additionally, the method implements distance correlation as a weight function for the similarity graph-based framework on which the adjusted MBO algorithm operates. The proposed model is validated using six molecular data sets, and we also provide a thorough comparison to other competing algorithms. The computational experiments show that the proposed method performs better than competing techniques even when the class imbalance ratio is very high.","sentences":["Data sets with imbalanced class sizes, often where one class size is much smaller than that of others, occur extremely often in various applications, including those with biological foundations, such as drug discovery and disease diagnosis.","Thus, it is extremely important to be able to identify data elements of classes of various sizes, as a failure to detect can result in heavy costs.","However, many data classification algorithms do not perform well on imbalanced data sets as they often fail to detect elements belonging to underrepresented classes.","In this paper, we propose the BTDT-MBO algorithm, incorporating Merriman-Bence-Osher (MBO) techniques and a bidirectional transformer, as well as distance correlation and decision threshold adjustments, for data classification problems on highly imbalanced molecular data sets, where the sizes of the classes vary greatly.","The proposed method not only integrates adjustments in the classification threshold for the MBO algorithm in order to help deal with the class imbalance, but also uses a bidirectional transformer model based on an attention mechanism for self-supervised learning.","Additionally, the method implements distance correlation as a weight function for the similarity graph-based framework on which the adjusted MBO algorithm operates.","The proposed model is validated using six molecular data sets, and we also provide a thorough comparison to other competing algorithms.","The computational experiments show that the proposed method performs better than competing techniques even when the class imbalance ratio is very high."],"url":"http://arxiv.org/abs/2406.06479v1","category":"cs.LG"}
{"created":"2024-06-10 17:00:46","title":"Galaxy lens reconstruction based on strongly lensed gravitational waves: similarity transformation degeneracy and mass-sheet degeneracy","abstract":"Gravitational wave (GW) galaxy lens reconstruction is a crucial step for many GW lensing science applications. However, dark siren GW lensing (i.e. lensed GW without observed electromagnetic (EM) counterpart) suffers from similarity transformation degeneracy and mass-sheet degeneracy. We review these two degeneracies and discuss their implications on GW-based lens reconstruction and two well-known GW lensing science cases: the Hubble constant measurement and test for modified GW propagation. Building upon previous works, our conclusions are:1) GWs can only infer the scale-free lens mass model parameters, the dimensionless source position, the GW luminosity distance and the time delay scaling (a combination of Einstein radius, lens redshift, and cosmology).2) Lens reconstruction (of singular isothermal ellipsoid lens) with only two GW signals is unlikely to yield a complete lens model, while four (three) signals can measure all the above parameters accurately (with large uncertainties).3) The similarity transformation degeneracy causes the lens redshift/Einstein radius/cosmology to be degenerate in dark siren measurements. Breaking the degeneracy can be achieved by supplementing the GWs with EM observation of lens redshifts/Einstein radius (source redshift is not required).4) The mass-sheet degeneracy causes the GW luminosity distance to be entirely degenerate with a constant mass sheet.5) Contrary to expectation, the Hubble constant is degenerate with the mass-sheet even when supplemented with lens reconstruction/redshift/Einstein radius and can only be lifted with lens galaxy velocity dispersion measurement, while modified GW propagation test discussed in prior literature is unaffected by the degeneracy. These properties highlight the need for GW observations to be supplemented by EM observations, which could become accessible through a lens archival search or a rapid EM follow-up.","sentences":["Gravitational wave (GW) galaxy lens reconstruction is a crucial step for many GW lensing science applications.","However, dark siren GW lensing (i.e. lensed GW without observed electromagnetic (EM) counterpart) suffers from similarity transformation degeneracy and mass-sheet degeneracy.","We review these two degeneracies and discuss their implications on GW-based lens reconstruction and two well-known GW lensing science cases: the Hubble constant measurement and test for modified GW propagation.","Building upon previous works, our conclusions are:1) GWs can only infer the scale-free lens mass model parameters, the dimensionless source position, the GW luminosity distance and the time delay scaling (a combination of Einstein radius, lens redshift, and cosmology).2) Lens reconstruction (of singular isothermal ellipsoid lens) with only two GW signals is unlikely to yield a complete lens model, while four (three) signals can measure all the above parameters accurately (with large uncertainties).3)","The similarity transformation degeneracy causes the lens redshift/Einstein radius/cosmology to be degenerate in dark siren measurements.","Breaking the degeneracy can be achieved by supplementing the GWs with EM observation of lens redshifts/Einstein radius (source redshift is not required).4)","The mass-sheet degeneracy causes the GW luminosity distance to be entirely degenerate with a constant mass sheet.5) Contrary to expectation, the Hubble constant is degenerate with the mass-sheet even when supplemented with lens reconstruction/redshift/Einstein radius and can only be lifted with lens galaxy velocity dispersion measurement, while modified GW propagation test discussed in prior literature is unaffected by the degeneracy.","These properties highlight the need for GW observations to be supplemented by EM observations, which could become accessible through a lens archival search or a rapid EM follow-up."],"url":"http://arxiv.org/abs/2406.06463v1","category":"astro-ph.HE"}
{"created":"2024-06-10 16:53:58","title":"How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization?","abstract":"Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs. In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign. To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process. The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process. Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO. This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly.","sentences":["Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking.","However, scientists often have strong intuition, and thus human feedback is still useful.","Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs.","In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign.","To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process.","The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process.","Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO.","This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly."],"url":"http://arxiv.org/abs/2406.06459v1","category":"cs.LG"}
{"created":"2024-06-10 16:34:43","title":"LLM Dataset Inference: Did you train on my dataset?","abstract":"The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values < 0.1, without any false positives.","sentences":["The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet.","Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs).","We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model).","This distribution shift makes membership inference appear successful.","However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time).","Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions.","Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models.","This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph.","While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset.","Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values < 0.1, without any false positives."],"url":"http://arxiv.org/abs/2406.06443v1","category":"cs.LG"}
{"created":"2024-06-10 16:23:26","title":"Decomposition of higher Deligne-Lusztig representations","abstract":"A higher Deligne-Lusztig representation is a virtual smooth representation of a parahoric subgroup in a $p$-adic group, which is constructed from the Deligne-Lusztig induction. In this note, under a mild condition on $p$ we provide a decomposition of higher Deligne-Lusztig representations, attached to an unramified elliptic maximal torus, into irreducible representations used in Yu's construction of supercuspidal representations of $p$-adic groups. As a consequence, we show that all the unramified supercuspidal representations are direct summands of inductions of higher Deligne-Lusztig representations.","sentences":["A higher Deligne-Lusztig representation is a virtual smooth representation of a parahoric subgroup in a $p$-adic group, which is constructed from the Deligne-Lusztig induction.","In this note, under a mild condition on $p$ we provide a decomposition of higher Deligne-Lusztig representations, attached to an unramified elliptic maximal torus, into irreducible representations used in Yu's construction of supercuspidal representations of $p$-adic groups.","As a consequence, we show that all the unramified supercuspidal representations are direct summands of inductions of higher Deligne-Lusztig representations."],"url":"http://arxiv.org/abs/2406.06430v1","category":"math.RT"}
{"created":"2024-06-10 16:14:50","title":"Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking","abstract":"Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.","sentences":["Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes.","While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes.","By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost.","Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic.","Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm.","We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics.","Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models."],"url":"http://arxiv.org/abs/2406.06425v1","category":"stat.ML"}
{"created":"2024-06-10 15:43:51","title":"Dual-cavity controllable quantum battery","abstract":"With the increasing development of quantum science and technology, quantum batteries are gradually emerging. But there are still many unsolved problems in the field of quantum batteries. Such as: how to increase the space utilization rate of quantum batteries? How to increase and control the charging power of quantum batteries? And how to have better quantum batterie energy storage without reducing the power of quantum batteries. Therefore, we propose a controllable dual-cavity quantum batterie. It can increase the charging power of the quantum batterie by manipulating the number of atoms without consuming other resources, and can make the power of the quantum batterie effectively adjust between $N^2$ and $N^{2.5}$. And the advantage of regulation to a certain extent is greater than the advantage of the interaction force between atoms.","sentences":["With the increasing development of quantum science and technology, quantum batteries are gradually emerging.","But there are still many unsolved problems in the field of quantum batteries.","Such as: how to increase the space utilization rate of quantum batteries?","How to increase and control the charging power of quantum batteries?","And how to have better quantum batterie energy storage without reducing the power of quantum batteries.","Therefore, we propose a controllable dual-cavity quantum batterie.","It can increase the charging power of the quantum batterie by manipulating the number of atoms without consuming other resources, and can make the power of the quantum batterie effectively adjust between $N^2$ and $N^{2.5}$. And the advantage of regulation to a certain extent is greater than the advantage of the interaction force between atoms."],"url":"http://arxiv.org/abs/2406.06383v1","category":"quant-ph"}
{"created":"2024-06-10 15:20:23","title":"Non-Markov quantum belief propagation","abstract":"We provide a rigorous proof of the approximate convergence of sliding-window quantum belief-propagation as outlined heuristically in the work of Bilgin and Poulin (Ref. [1]), in the absence of the quantum Markov property. In particular, we confirm the hypothesis outlined in this work that the approximation error of each step in the belief-propagation algorithm decreases exponentially with the sliding-window size, under the assumption that the underlying state on which belief-propagation is being performed possesses a so-called thermal boundedness property: a relaxation of the Markov property required for exact convergence.","sentences":["We provide a rigorous proof of the approximate convergence of sliding-window quantum belief-propagation as outlined heuristically in the work of Bilgin and Poulin (Ref.","[1]), in the absence of the quantum Markov property.","In particular, we confirm the hypothesis outlined in this work that the approximation error of each step in the belief-propagation algorithm decreases exponentially with the sliding-window size, under the assumption that the underlying state on which belief-propagation is being performed possesses a so-called thermal boundedness property: a relaxation of the Markov property required for exact convergence."],"url":"http://arxiv.org/abs/2406.06360v1","category":"quant-ph"}
{"created":"2024-06-10 15:13:07","title":"Cascading Unknown Detection with Known Classification for Open Set Recognition","abstract":"Deep learners tend to perform well when trained under the closed set assumption but struggle when deployed under open set conditions. This motivates the field of Open Set Recognition in which we seek to give deep learners the ability to recognize whether a data sample belongs to the known classes trained on or comes from the surrounding infinite world. Existing open set recognition methods typically rely upon a single function for the dual task of distinguishing between knowns and unknowns as well as making known class distinction. This dual process leaves performance on the table as the function is not specialized for either task. In this work, we introduce Cascading Unknown Detection with Known Classification (Cas-DC), where we instead learn specialized functions in a cascading fashion for both known/unknown detection and fine class classification amongst the world of knowns. Our experiments and analysis demonstrate that Cas-DC handily outperforms modern methods in open set recognition when compared using AUROC scores and correct classification rate at various true positive rates.","sentences":["Deep learners tend to perform well when trained under the closed set assumption but struggle when deployed under open set conditions.","This motivates the field of Open Set Recognition in which we seek to give deep learners the ability to recognize whether a data sample belongs to the known classes trained on or comes from the surrounding infinite world.","Existing open set recognition methods typically rely upon a single function for the dual task of distinguishing between knowns and unknowns as well as making known class distinction.","This dual process leaves performance on the table as the function is not specialized for either task.","In this work, we introduce Cascading Unknown Detection with Known Classification (Cas-DC), where we instead learn specialized functions in a cascading fashion for both known/unknown detection and fine class classification amongst the world of knowns.","Our experiments and analysis demonstrate that Cas-DC handily outperforms modern methods in open set recognition when compared using AUROC scores and correct classification rate at various true positive rates."],"url":"http://arxiv.org/abs/2406.06351v1","category":"cs.CV"}
{"created":"2024-06-10 14:14:23","title":"Sample Rate Independent Recurrent Neural Networks for Audio Effects Processing","abstract":"In recent years, machine learning approaches to modelling guitar amplifiers and effects pedals have been widely investigated and have become standard practice in some consumer products. In particular, recurrent neural networks (RNNs) are a popular choice for modelling non-linear devices such as vacuum tube amplifiers and distortion circuitry. One limitation of such models is that they are trained on audio at a specific sample rate and therefore give unreliable results when operating at another rate. Here, we investigate several methods of modifying RNN structures to make them approximately sample rate independent, with a focus on oversampling. In the case of integer oversampling, we demonstrate that a previously proposed delay-based approach provides high fidelity sample rate conversion whilst additionally reducing aliasing. For non-integer sample rate adjustment, we propose two novel methods and show that one of these, based on cubic Lagrange interpolation of a delay-line, provides a significant improvement over existing methods. To our knowledge, this work provides the first in-depth study into this problem.","sentences":["In recent years, machine learning approaches to modelling guitar amplifiers and effects pedals have been widely investigated and have become standard practice in some consumer products.","In particular, recurrent neural networks (RNNs) are a popular choice for modelling non-linear devices such as vacuum tube amplifiers and distortion circuitry.","One limitation of such models is that they are trained on audio at a specific sample rate and therefore give unreliable results when operating at another rate.","Here, we investigate several methods of modifying RNN structures to make them approximately sample rate independent, with a focus on oversampling.","In the case of integer oversampling, we demonstrate that a previously proposed delay-based approach provides high fidelity sample rate conversion whilst additionally reducing aliasing.","For non-integer sample rate adjustment, we propose two novel methods and show that one of these, based on cubic Lagrange interpolation of a delay-line, provides a significant improvement over existing methods.","To our knowledge, this work provides the first in-depth study into this problem."],"url":"http://arxiv.org/abs/2406.06293v1","category":"eess.AS"}
{"created":"2024-06-10 14:12:37","title":"The first-order structural phase transition at low-temperature in GaPt$_{5}$P and its rapid enhancement with pressure","abstract":"Single crystals of XPt$_{5}$P (X = Al, Ga, and In) were grown from a Pt-P solution at high temperatures, and ambient-pressure measurements of temperature-dependent magnetization, resistivity, and X-ray diffraction were made. Also, the ambient-pressure Hall resistivity and temperature-dependent resistance under pressure were measured on GaPt$_{5}$P. All three compounds have tetragonal $P4/mmm$ crystal structure at room-temperature with metallic transport and weak diamagnetism over the $2-300$~K temperature range. Surprisingly, at ambient pressure, both the transport and magnetization measurements on GaPt$_{5}$P show a step-like feature in $70-90$~K region suggesting a possible structural phase transition, and no such features were observed in (Al/In)Pt$_{5}$P. Both the hysteretic nature and sharpness of the feature suggest the first-order transition, and single-crystal X-ray diffraction measurements provided further details of the structural transition with a crystal symmetry likely different than $P4/mmm$ below transition. The transition is characterized by anisotropic changes in the lattice parameters, a volume collapse, and satellite peaks at two distinct wave-vectors. Density functional theory calculations present phonon softening as a possible driving mechanism. Additionally, the structural transition temperature increases rapidly with increasing pressure, reaching room temperature by $\\sim 2.2$~GPa, highlighting the high degree of pressure sensitivity and fragile nature of GaPt$_{5}$P room-temperature structure. Although the volume collapse and extreme pressure sensitivity suggest chemical pressure should drive a similar structural change in AlPt$_{5}$P, with smaller unit cell dimensions and volume, its structure is found to be $P4/mmm$ as well. Overall, GaPt$_{5}$P stands out as a sole member of the 1-5-1 family of compounds with a temperature-driven structural change.","sentences":["Single crystals of XPt$_{5}$P (X = Al, Ga, and In) were grown from a Pt-P solution at high temperatures, and ambient-pressure measurements of temperature-dependent magnetization, resistivity, and X-ray diffraction were made.","Also, the ambient-pressure Hall resistivity and temperature-dependent resistance under pressure were measured on GaPt$_{5}$P. All three compounds have tetragonal $P4/mmm$ crystal structure at room-temperature with metallic transport and weak diamagnetism over the $2-300$~K temperature range.","Surprisingly, at ambient pressure, both the transport and magnetization measurements on GaPt$_{5}$P show a step-like feature in $70-90$~K region suggesting a possible structural phase transition, and no such features were observed in (Al/In)Pt$_{5}$P. Both the hysteretic nature and sharpness of the feature suggest the first-order transition, and single-crystal X-ray diffraction measurements provided further details of the structural transition with a crystal symmetry likely different than $P4/mmm$ below transition.","The transition is characterized by anisotropic changes in the lattice parameters, a volume collapse, and satellite peaks at two distinct wave-vectors.","Density functional theory calculations present phonon softening as a possible driving mechanism.","Additionally, the structural transition temperature increases rapidly with increasing pressure, reaching room temperature by $\\sim 2.2$~GPa, highlighting the high degree of pressure sensitivity and fragile nature of GaPt$_{5}$P room-temperature structure.","Although the volume collapse and extreme pressure sensitivity suggest chemical pressure should drive a similar structural change in AlPt$_{5}$P, with smaller unit cell dimensions and volume, its structure is found to be $P4/mmm$ as well.","Overall, GaPt$_{5}$P stands out as a sole member of the 1-5-1 family of compounds with a temperature-driven structural change."],"url":"http://arxiv.org/abs/2406.06291v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 13:58:46","title":"Multi-Prompting Decoder Helps Better Language Understanding","abstract":"Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting.","sentences":["Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting.","To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM.","Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt.","In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation.","The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores for subsequent decoding.","Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs.","Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores.","Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting."],"url":"http://arxiv.org/abs/2406.06279v1","category":"cs.CL"}
{"created":"2024-06-10 13:57:28","title":"Three super-Earths and a possible water world from TESS and ESPRESSO","abstract":"Since 2018, the ESPRESSO spectrograph at the VLT has been hunting for planets in the Southern skies via the RV method. One of its goals is to follow up candidate planets from transit surveys such as the TESS mission, particularly small planets. We analyzed photometry from TESS and ground-based facilities, high-resolution imaging, and RVs from ESPRESSO, HARPS, and HIRES, to confirm and characterize three new planets: TOI-260 b, transiting a late K-dwarf, and TOI-286 b and c, orbiting an early K-dwarf. We also update parameters for the known super-Earth TOI-134 b , hosted by an M-dwarf. TOI-260 b has a $13.475853^{+0.000013}_{-0.000011}$ d period, $4.23 \\pm1.60 \\mathrm{M_\\oplus}$ mass and $1.71\\pm0.08\\mathrm{R_\\oplus}$ radius. For TOI-286 b we find a $4.5117244^{+0.0000031}_{-0.0000027}$ d period, $4.53\\pm0.78\\mathrm{M_\\oplus}$ mass and $1.42\\pm0.10\\mathrm{R_\\oplus}$ radius; for TOI-286 c, a $39.361826^{+0.000070}_{-0.000081}$ d period, $3.72\\pm2.22\\mathrm{M_\\oplus}$ mass and $1.88\\pm 0.12\\mathrm{R_\\oplus}$ radius. For TOI-134 b we obtain a $1.40152604^{+0.00000074}_{-0.00000082}$ d period, $4.07\\pm0.45\\mathrm{M_\\oplus}$ mass, and $1.63\\pm0.14\\mathrm{R_\\oplus}$ radius. Circular models are preferred for all, although for TOI-260 b the eccentricity is not well-constrained. We compute bulk densities and place the planets in the context of composition models. TOI-260 b lies within the radius valley, and is most likely a rocky planet. However, the uncertainty on the eccentricity and thus on the mass renders its composition hard to determine. TOI-286 b and c span the radius valley, with TOI-286 b lying below it and having a likely rocky composition, while TOI-286 c is within the valley, close to the upper border, and probably has a significant water fraction. With our updated parameters for TOI-134 b, we obtain a lower density than previous findings, giving a rocky or Earth-like composition.","sentences":["Since 2018, the ESPRESSO spectrograph at the VLT has been hunting for planets in the Southern skies via the RV method.","One of its goals is to follow up candidate planets from transit surveys such as the TESS mission, particularly small planets.","We analyzed photometry from TESS and ground-based facilities, high-resolution imaging, and RVs from ESPRESSO, HARPS, and HIRES, to confirm and characterize three new planets: TOI-260 b, transiting a late K-dwarf, and TOI-286 b and c, orbiting an early K-dwarf.","We also update parameters for the known super-Earth TOI-134 b , hosted by an M-dwarf.","TOI-260 b has a $13.475853^{+0.000013}_{-0.000011}$ d period, $4.23 \\pm1.60 \\mathrm{M_\\oplus}$ mass and $1.71\\pm0.08\\mathrm{R_\\oplus}$ radius.","For TOI-286 b we find a $4.5117244^{+0.0000031}_{-0.0000027}$ d period, $4.53\\pm0.78\\mathrm{M_\\oplus}$ mass and $1.42\\pm0.10\\mathrm{R_\\oplus}$ radius; for TOI-286 c, a $39.361826^{+0.000070}_{-0.000081}$ d period, $3.72\\pm2.22\\mathrm{M_\\oplus}$ mass and $1.88\\pm 0.12\\mathrm{R_\\oplus}$ radius.","For TOI-134 b we obtain a $1.40152604^{+0.00000074}_{-0.00000082}$ d period, $4.07\\pm0.45\\mathrm{M_\\oplus}$ mass, and $1.63\\pm0.14\\mathrm{R_\\oplus}$ radius.","Circular models are preferred for all, although for TOI-260 b the eccentricity is not well-constrained.","We compute bulk densities and place the planets in the context of composition models.","TOI-260 b lies within the radius valley, and is most likely a rocky planet.","However, the uncertainty on the eccentricity and thus on the mass renders its composition hard to determine.","TOI-286 b and c span the radius valley, with TOI-286 b lying below it and having a likely rocky composition, while TOI-286 c is within the valley, close to the upper border, and probably has a significant water fraction.","With our updated parameters for TOI-134 b, we obtain a lower density than previous findings, giving a rocky or Earth-like composition."],"url":"http://arxiv.org/abs/2406.06278v1","category":"astro-ph.EP"}
{"created":"2024-06-10 13:36:29","title":"Study on Kelvin Helmholtz shear flows subjected to differential rotation","abstract":"A numerical simulation of Kelvin-Helmholtz Instability (KHI) in parallel shear flows subjected to external rotation is carried out using a pseudo-spectral technique. The Coriolis force, arising in a rotation frame under the beta plane approximation, tends to suppress the growth of KHI modes. The numerical results show a close qualitative agreement with the analytical results obtained for a step-wise shear flow profile. Experimental evidence demonstrates that particles in a rotating frame experience the Coriolis force, mathematically equivalent to the Lorentz force. Therefore, the Coriolis force affects fluid dynamics in a manner similar to the Lorentz force in magnetized shear flows. This paper exploits the analogy between the magnetic field and rotation to study effects equivalent to a magnetic field on KHI in a rotating frame. Similar to the magnetic field case, the Coriolis force suppresses KHI and tends to form compressed and elongated KH vortex structures. However, the magnetic field and Coriolis force act on different scales, with the latter suppressing long-wavelength mode perturbations. A higher number of vortices are observed in the presence of rotation compared to non-rotating cases","sentences":["A numerical simulation of Kelvin-Helmholtz Instability (KHI) in parallel shear flows subjected to external rotation is carried out using a pseudo-spectral technique.","The Coriolis force, arising in a rotation frame under the beta plane approximation, tends to suppress the growth of KHI modes.","The numerical results show a close qualitative agreement with the analytical results obtained for a step-wise shear flow profile.","Experimental evidence demonstrates that particles in a rotating frame experience the Coriolis force, mathematically equivalent to the Lorentz force.","Therefore, the Coriolis force affects fluid dynamics in a manner similar to the Lorentz force in magnetized shear flows.","This paper exploits the analogy between the magnetic field and rotation to study effects equivalent to a magnetic field on KHI in a rotating frame.","Similar to the magnetic field case, the Coriolis force suppresses KHI and tends to form compressed and elongated KH vortex structures.","However, the magnetic field and Coriolis force act on different scales, with the latter suppressing long-wavelength mode perturbations.","A higher number of vortices are observed in the presence of rotation compared to non-rotating cases"],"url":"http://arxiv.org/abs/2406.06256v1","category":"physics.flu-dyn"}
{"created":"2024-06-10 13:03:20","title":"Statistical Inference for Privatized Data with Unknown Sample Size","abstract":"We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that ABC-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.","sentences":["We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection.","We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that ABC-type posterior distributions converge under similar assumptions.","We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE.","In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022).","We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP.","We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution."],"url":"http://arxiv.org/abs/2406.06231v1","category":"math.ST"}
{"created":"2024-06-10 11:20:51","title":"Inestability presented in the estimating of the Nelson-Siegel-Svensson model","abstract":"The literature shows the possible existence of a problem called collinearity in both Nelson-Siegel and Nelson-Siegel-Svensson models due to the relationship between the slope and curvature components. The presence of this problem and the estimation of both models by Ordinary Least Squares would lead to coefficients estimates that may be unstable among other consequences. However, these estimates are used to make monetary policy decisions. For this reason, it is important to try mitigating this collinearity problem. Consequently, some authors propose traditional procedures for the treatment of collinearity such as: non-linear optimisation, to fix the shape parameter or ridge regression. Nevertheless, all these processes have their disadvantages. Alternatively, a new method with good properties called raise regression is proposed in this paper. Finally, the methodologies are illustrated with an empirical comparison on Euribor Overnight Index Swap and Euribor Interest Rates Swap data between 2011 and 2021.","sentences":["The literature shows the possible existence of a problem called collinearity in both Nelson-Siegel and Nelson-Siegel-Svensson models due to the relationship between the slope and curvature components.","The presence of this problem and the estimation of both models by Ordinary Least Squares would lead to coefficients estimates that may be unstable among other consequences.","However, these estimates are used to make monetary policy decisions.","For this reason, it is important to try mitigating this collinearity problem.","Consequently, some authors propose traditional procedures for the treatment of collinearity such as: non-linear optimisation, to fix the shape parameter or ridge regression.","Nevertheless, all these processes have their disadvantages.","Alternatively, a new method with good properties called raise regression is proposed in this paper.","Finally, the methodologies are illustrated with an empirical comparison on Euribor Overnight Index Swap and Euribor Interest Rates Swap data between 2011 and 2021."],"url":"http://arxiv.org/abs/2406.06177v1","category":"stat.AP"}
{"created":"2024-06-10 10:08:50","title":"Revisiting 3D Cartesian Scatterplots with a Novel Plotting Framework and a Survey","abstract":"3D scatter plots are a powerful visualisation method by being able to represent 3 dimensions spatially. It can also enable the representation of additional dimensions, such as by using a colour map. An important issue with the current state of plotting software is the limited use of physical properties from the real world such as shadows to improve the effectiveness of the plots. A popular example is with the use of isometric axes in combination with same-sized points, which is equivalent to removing one whole dimension (depth perception). In static snapshot images, as found in digital and hard prints, as well with discrete data, additional cues such as movement are not present to mitigate for the loss of spatial information.   In this paper we present a novel plotting framework that features a wide range of techniques to improve the information transfer from 3D scatterplots for multi-dimensional data. We evaluate the resulting plots by surveying 57 participants from an academic institution to get important insights on what makes 3D scatterplots effective in communicating data of more than two dimensions.","sentences":["3D scatter plots are a powerful visualisation method by being able to represent 3 dimensions spatially.","It can also enable the representation of additional dimensions, such as by using a colour map.","An important issue with the current state of plotting software is the limited use of physical properties from the real world such as shadows to improve the effectiveness of the plots.","A popular example is with the use of isometric axes in combination with same-sized points, which is equivalent to removing one whole dimension (depth perception).","In static snapshot images, as found in digital and hard prints, as well with discrete data, additional cues such as movement are not present to mitigate for the loss of spatial information.   ","In this paper we present a novel plotting framework that features a wide range of techniques to improve the information transfer from 3D scatterplots for multi-dimensional data.","We evaluate the resulting plots by surveying 57 participants from an academic institution to get important insights on what makes 3D scatterplots effective in communicating data of more than two dimensions."],"url":"http://arxiv.org/abs/2406.06146v1","category":"cs.HC"}
{"created":"2024-06-10 09:11:30","title":"A Survey on Incomplete Multi-label Learning: Recent Advances and Future Trends","abstract":"In reality, data often exhibit associations with multiple labels, making multi-label learning (MLL) become a prominent research topic. The last two decades have witnessed the success of MLL, which is indispensable from complete and accurate supervised information. However, obtaining such information in practice is always laborious and sometimes even impossible. To circumvent this dilemma, incomplete multi-label learning (InMLL) has emerged, aiming to learn from incomplete labeled data. To date, enormous InMLL works have been proposed to narrow the performance gap with complete MLL, whereas a systematic review for InMLL is still absent. In this paper, we not only attempt to fill the lacuna but also strive to pave the way for innovative research. Specifically, we retrospect the origin of InMLL, analyze the challenges of InMLL, and make a taxonomy of InMLL from the data-oriented and algorithm-oriented perspectives, respectively. Besides, we also present real applications of InMLL in various domains. More importantly, we highlight several potential future trends, including four open problems that are more in line with practice and three under-explored/unexplored techniques in addressing the challenges of InMLL, which may shed new light on developing novel research directions in the field of InMLL.","sentences":["In reality, data often exhibit associations with multiple labels, making multi-label learning (MLL) become a prominent research topic.","The last two decades have witnessed the success of MLL, which is indispensable from complete and accurate supervised information.","However, obtaining such information in practice is always laborious and sometimes even impossible.","To circumvent this dilemma, incomplete multi-label learning (InMLL) has emerged, aiming to learn from incomplete labeled data.","To date, enormous InMLL works have been proposed to narrow the performance gap with complete MLL, whereas a systematic review for InMLL is still absent.","In this paper, we not only attempt to fill the lacuna but also strive to pave the way for innovative research.","Specifically, we retrospect the origin of InMLL, analyze the challenges of InMLL, and make a taxonomy of InMLL from the data-oriented and algorithm-oriented perspectives, respectively.","Besides, we also present real applications of InMLL in various domains.","More importantly, we highlight several potential future trends, including four open problems that are more in line with practice and three under-explored/unexplored techniques in addressing the challenges of InMLL, which may shed new light on developing novel research directions in the field of InMLL."],"url":"http://arxiv.org/abs/2406.06119v1","category":"cs.LG"}
{"created":"2024-06-10 08:58:50","title":"Heterogeneous extremes in the presence of random covariates and censoring","abstract":"The task of analyzing extreme events with censoring effects is considered under a framework allowing for random covariate information. A wide class of estimators that can be cast as product-limit integrals is considered, for when the conditional distributions belong to the Frechet max-domain of attraction. The main mathematical contribution is establishing uniform conditions on the families of the regularly varying tails for which the asymptotic behaviour of the resulting estimators is tractable. In particular, a decomposition of the integral estimators in terms of exchangeable sums is provided, which leads to a law of large numbers and several central limit theorems. Subsequently, the finite-sample behaviour of the estimators is explored through a simulation study, and through the analysis of two real-life datasets. In particular, the inclusion of covariates makes the model significantly versatile and, as a consequence, practically relevant.","sentences":["The task of analyzing extreme events with censoring effects is considered under a framework allowing for random covariate information.","A wide class of estimators that can be cast as product-limit integrals is considered, for when the conditional distributions belong to the Frechet max-domain of attraction.","The main mathematical contribution is establishing uniform conditions on the families of the regularly varying tails for which the asymptotic behaviour of the resulting estimators is tractable.","In particular, a decomposition of the integral estimators in terms of exchangeable sums is provided, which leads to a law of large numbers and several central limit theorems.","Subsequently, the finite-sample behaviour of the estimators is explored through a simulation study, and through the analysis of two real-life datasets.","In particular, the inclusion of covariates makes the model significantly versatile and, as a consequence, practically relevant."],"url":"http://arxiv.org/abs/2406.06113v1","category":"math.ST"}
{"created":"2024-06-10 08:35:50","title":"Equivalence of Polarizability and Circuit Models for Waveguide-Fed Metamaterial Elements","abstract":"A common variant of a metasurface antenna consists of an array of metamaterial elements coupled to a waveguide feed. The guided wave excites the metamaterial elements, coupling energy from the waveguide mode to radiation. Under appropriate conditions, each sub-wavelength metamaterial element can be modeled as a polarizable dipole, with the polarizability determined by an extraction procedure from the computed or measured waveguide scattering parameters. Here we establish the equivalence of this polarizability description of a metamaterial element with an equivalent circuit model, providing an additional tool for metasurface design that offers significant insight and a path towards efficiently modeling very large apertures. With this equivalence established, more complicated external circuits that include lumped elements and devices such as diodes and transistors can be integrated into the metamaterial element, which can then be transformed into an equivalent polarizability for modeling in the coupled dipole framework. We derive appropriate circuit models for several basic metamaterial elements, which provide direct relationships between the equivalent circuit parameters of an element and its effective polarizability. These expressions are confirmed using scattering parameters for several example structures obtained via full-wave simulations.","sentences":["A common variant of a metasurface antenna consists of an array of metamaterial elements coupled to a waveguide feed.","The guided wave excites the metamaterial elements, coupling energy from the waveguide mode to radiation.","Under appropriate conditions, each sub-wavelength metamaterial element can be modeled as a polarizable dipole, with the polarizability determined by an extraction procedure from the computed or measured waveguide scattering parameters.","Here we establish the equivalence of this polarizability description of a metamaterial element with an equivalent circuit model, providing an additional tool for metasurface design that offers significant insight and a path towards efficiently modeling very large apertures.","With this equivalence established, more complicated external circuits that include lumped elements and devices such as diodes and transistors can be integrated into the metamaterial element, which can then be transformed into an equivalent polarizability for modeling in the coupled dipole framework.","We derive appropriate circuit models for several basic metamaterial elements, which provide direct relationships between the equivalent circuit parameters of an element and its effective polarizability.","These expressions are confirmed using scattering parameters for several example structures obtained via full-wave simulations."],"url":"http://arxiv.org/abs/2406.06102v1","category":"physics.app-ph"}
{"created":"2024-06-10 08:19:51","title":"Algorithms for Multi-Criteria Decision-Making and Efficiency Analysis Problems","abstract":"Multi-criteria decision-making (MCDM) problems involve the evaluation of alternatives based on various minimization and maximization criteria. Similarly, efficiency evaluation (EA) methods assess decision-making units (DMUs) by analyzing their input consumption and output production. MCDM and EA methods face challenges in managing alternatives and DMUs with varying capacities across different criteria (inputs and outputs). That leads to performance assessments often skewed by subjective biases in criteria weighting. We introduce two innovative scenarios utilizing linear programming-based Virtual Gap Analysis (VGA) models to address these limitations. This dual-scenario approach aims to mitigate traditional biases, offering robust solutions for comprehensively assessing alternatives and DMUs. Our methodology allows for the influential ranking of alternatives in MCDM problems and enables each DMU to adjust its input and output ratios to achieve efficiency.","sentences":["Multi-criteria decision-making (MCDM) problems involve the evaluation of alternatives based on various minimization and maximization criteria.","Similarly, efficiency evaluation (EA) methods assess decision-making units (DMUs) by analyzing their input consumption and output production.","MCDM and EA methods face challenges in managing alternatives and DMUs with varying capacities across different criteria (inputs and outputs).","That leads to performance assessments often skewed by subjective biases in criteria weighting.","We introduce two innovative scenarios utilizing linear programming-based Virtual Gap Analysis (VGA) models to address these limitations.","This dual-scenario approach aims to mitigate traditional biases, offering robust solutions for comprehensively assessing alternatives and DMUs.","Our methodology allows for the influential ranking of alternatives in MCDM problems and enables each DMU to adjust its input and output ratios to achieve efficiency."],"url":"http://arxiv.org/abs/2406.06090v1","category":"math.OC"}
{"created":"2024-06-10 07:50:41","title":"Text Analysis of ETDs in ProQuest Dissertations and Theses (PQDT) Global (2016-2018)","abstract":"The information explosion in the form of ETDs poses the challenge of management and extraction of appropriate knowledge for decision-making. Thus, the present study forwards a solution to the above problem by applying topic mining and prediction modeling tools to 263 ETDs submitted to the PQDT Global database during 2016-18 in the field of library science. This study was divided into two phases. The first phase determined the core topics from the ETDs using Topic-Modeling-Tool (TMT), which was based on latent dirichlet allocation (LDA), whereas the second phase employed prediction analysis using RapidMinerplatform to annotate the future research articles on the basis of the modeled topics. The core topics (tags) for the studied period were found to be book history, school librarian, public library, communicative ecology, and informatics followed by text network and trend analysis on the high probability cooccurred words. Lastly, a prediction model using Support Vector Machine (SVM) classifier was created in order to accurately predict the placement of future ETDs going to be submitted to PQDT Global under the five modeled topics (a to e). The tested dataset against the trained data set for the predictive performed perfectly.","sentences":["The information explosion in the form of ETDs poses the challenge of management and extraction of appropriate knowledge for decision-making.","Thus, the present study forwards a solution to the above problem by applying topic mining and prediction modeling tools to 263 ETDs submitted to the PQDT Global database during 2016-18 in the field of library science.","This study was divided into two phases.","The first phase determined the core topics from the ETDs using Topic-Modeling-Tool (TMT), which was based on latent dirichlet allocation (LDA), whereas the second phase employed prediction analysis using RapidMinerplatform to annotate the future research articles on the basis of the modeled topics.","The core topics (tags) for the studied period were found to be book history, school librarian, public library, communicative ecology, and informatics followed by text network and trend analysis on the high probability cooccurred words.","Lastly, a prediction model using Support Vector Machine (SVM) classifier was created in order to accurately predict the placement of future ETDs going to be submitted to PQDT Global under the five modeled topics (a to e).","The tested dataset against the trained data set for the predictive performed perfectly."],"url":"http://arxiv.org/abs/2406.06076v1","category":"cs.DL"}
{"created":"2024-06-10 07:28:27","title":"Bayesian Parametric Methods for Deriving Distribution of Restricted Mean Survival Time","abstract":"We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster. This manuscript has not been published elsewhere. The manuscript is not under consideration in whole or in part by another journal.","sentences":["We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time.","We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value.","Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty.","We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences.","In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval.","Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster.","This manuscript has not been published elsewhere.","The manuscript is not under consideration in whole or in part by another journal."],"url":"http://arxiv.org/abs/2406.06071v1","category":"stat.ME"}
{"created":"2024-06-10 07:03:36","title":"Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text","abstract":"Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text. Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets. However, existing SBDH datasets exhibit substantial limitations in their availability and coverage. In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories. We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities. Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 62.5% macro-F improvements. Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints. Human evaluation demonstrates a Human-LLM alignment of 71.06% and uncovers areas for future refinements.","sentences":["Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text.","Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets.","However, existing SBDH datasets exhibit substantial limitations in their availability and coverage.","In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories.","We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities.","Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 62.5% macro-F improvements.","Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints.","Human evaluation demonstrates a Human-LLM alignment of 71.06% and uncovers areas for future refinements."],"url":"http://arxiv.org/abs/2406.06056v1","category":"cs.CL"}
{"created":"2024-06-10 06:46:10","title":"Steady-state dynamics and non-local correlations in thermoelectric Cooper pair splitters","abstract":"Recent experiments on Cooper pair splitters using superconductor-quantum dot hybrids have embarked on creating entanglement in the solid-state, by engineering the sub-gap processes in the superconducting region. Using the thermoelectric Cooper pair splitter setup [Nat. Comm., 12, 21, (2021)] as a prototype, we develop a detailed analysis of the observed transport signal to bring out vital insights into the regimes of operation and establish the non-local nature of the correlations arising from the crossed Andreev processes. As a striking consequence, contact induced level broadening of the quantum dot's discrete energy spectrum and its hybridization with the superconducting segment, results in a parity reversal of the thermoelectric current along with shifted resonances of the crossed Andreev processes. We conclusively establish the presence of non-local correlations by making a clear nexus with quantum discord. Our detailed analysis thereby provides insights into the gate voltage control of the entanglement generation in superconducting-hybrid Cooper pair splitters.","sentences":["Recent experiments on Cooper pair splitters using superconductor-quantum dot hybrids have embarked on creating entanglement in the solid-state, by engineering the sub-gap processes in the superconducting region.","Using the thermoelectric Cooper pair splitter setup","[Nat. Comm., 12, 21, (2021)] as a prototype, we develop a detailed analysis of the observed transport signal to bring out vital insights into the regimes of operation and establish the non-local nature of the correlations arising from the crossed Andreev processes.","As a striking consequence, contact induced level broadening of the quantum dot's discrete energy spectrum and its hybridization with the superconducting segment, results in a parity reversal of the thermoelectric current along with shifted resonances of the crossed Andreev processes.","We conclusively establish the presence of non-local correlations by making a clear nexus with quantum discord.","Our detailed analysis thereby provides insights into the gate voltage control of the entanglement generation in superconducting-hybrid Cooper pair splitters."],"url":"http://arxiv.org/abs/2406.06053v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 05:42:54","title":"Improved bounds on the size of permutation codes under Kendall $\u03c4$-metric","abstract":"In order to overcome the challenges caused by flash memories and also to protect against errors related to reading information stored in DNA molecules in the shotgun sequencing method, the rank modulation is proposed. In the rank modulation framework, codewords are permutations. In this paper, we study the largest size $P(n, d)$ of permutation codes of length $n$, i.e., subsets of the set $S_n$ of all permutations on $\\{1,\\ldots, n\\}$ with the minimum distance at least $d\\in\\{1,\\ldots ,\\binom{n}{2}\\}$ under the Kendall $\\tau$-metric. By presenting an algorithm and some theorems, we managed to improve the known lower and upper bounds for $P(n,d)$. In particular, we show that $P(n,d)=4$ for all $n\\geq 6$ and $\\frac{3}{5}\\binom{n}{2}< d \\leq \\frac{2}{3} \\binom{n}{2}$. Additionally, we prove that for any prime number $n$ and integer $r\\leq \\frac{n}{6}$, $ P(n,3)\\leq (n-1)!-\\dfrac{n-6r}{\\sqrt{n^2-8rn+20r^2}}\\sqrt{\\dfrac{(n-1)!}{n(n-r)!}}. $ This result greatly improves the upper bound of $P(n,3)$ for all primes $n\\geq 37$.","sentences":["In order to overcome the challenges caused by flash memories and also to protect against errors related to reading information stored in DNA molecules in the shotgun sequencing method, the rank modulation is proposed.","In the rank modulation framework, codewords are permutations.","In this paper, we study the largest size $P(n, d)$ of permutation codes of length $n$, i.e., subsets of the set $S_n$ of all permutations on $\\{1,\\ldots, n\\}$ with the minimum distance at least $d\\in\\{1,\\ldots ,\\binom{n}{2}\\}$ under the Kendall $\\tau$-metric.","By presenting an algorithm and some theorems, we managed to improve the known lower and upper bounds for $P(n,d)$.","In particular, we show that $P(n,d)=4$ for all $n\\geq 6$ and $\\frac{3}{5}\\binom{n}{2}< d \\leq \\frac{2}{3} \\binom{n}{2}$. Additionally, we prove that for any prime number $n$ and integer $r\\leq \\frac{n}{6}$, $ P(n,3)\\leq (n-1)!-\\dfrac{n-6r}{\\sqrt{n^2-8rn+20r^2}}\\sqrt{\\dfrac{(n-1)!}{n(n-r)!}}.","$","This result greatly improves the upper bound of $P(n,3)$ for all primes $n\\geq 37$."],"url":"http://arxiv.org/abs/2406.06029v1","category":"cs.IT"}
{"created":"2024-06-10 05:22:33","title":"Third Degree Price Discrimination Under Costly Information Acquisition","abstract":"This paper investigates third-degree price discrimination under endogenous market segmentation. Segmenting a market requires access to information about consumers, and this information comes with a cost. I explore the trade-offs between the benefits of segmentation and the costs of information acquisition, revealing a non-monotonic relationship between consumer surplus and the cost of information acquisition for monopolist. I show that in some markets, allowing the monopolist easier access to customer data can also benefit customers. I also analyzed how social welfare reacts to changes in the cost level of information acquisition and showed that the non-monotonicity result is also valid in social welfare analysis.","sentences":["This paper investigates third-degree price discrimination under endogenous market segmentation.","Segmenting a market requires access to information about consumers, and this information comes with a cost.","I explore the trade-offs between the benefits of segmentation and the costs of information acquisition, revealing a non-monotonic relationship between consumer surplus and the cost of information acquisition for monopolist.","I show that in some markets, allowing the monopolist easier access to customer data can also benefit customers.","I also analyzed how social welfare reacts to changes in the cost level of information acquisition and showed that the non-monotonicity result is also valid in social welfare analysis."],"url":"http://arxiv.org/abs/2406.06026v1","category":"econ.TH"}
{"created":"2024-06-10 04:47:16","title":"Effect of Strain on the Band Gap of Monolayer MoS$_2$","abstract":"Monolayer $\\mathrm{MoS_2}$ under strain has many interesting properties and possible applications in technology. A recent experimental study examined the effect of strain on the bandgap of monolayer $\\mathrm{MoS_2}$ on a mildly curved graphite surface, reporting that under biaxial strain with a Poisson's ratio of 0.44, the bandgap decreases at a rate of 400 meV/% strain. In this work, we performed density functional theory (DFT) calculations for a free-standing $\\mathrm{MoS_2}$ monolayer, using the generalized gradient approximation (GGA) PBE, the hybrid functional HSE06, and many-body perturbation theory with the GW approximation using PBE wavefunctions (G0W0@PBE). We found that under biaxial strain with the experimental Poisson's ratio, the bandgap decreases at rates of 63 meV/% strain (PBE), 73 meV/% strain (HSE06), and 43 meV/% strain (G0W0@PBE), which are significantly smaller than the experimental rate. We also found that PBE predicts a similarly smaller rate (90 meV/% strain) for a different Poisson's ratio of 0.25. Spin-orbit correction (SOC) has little effect on the gap or its strain dependence. Additionally, we observed a semiconductor-to-metal transition under an equal tensile biaxial strain of 10% and a transition from a direct to an indirect bandgap, consistent with previous theoretical work.","sentences":["Monolayer $\\mathrm{MoS_2}$ under strain has many interesting properties and possible applications in technology.","A recent experimental study examined the effect of strain on the bandgap of monolayer $\\mathrm{MoS_2}$ on a mildly curved graphite surface, reporting that under biaxial strain with a Poisson's ratio of 0.44, the bandgap decreases at a rate of 400 meV/% strain.","In this work, we performed density functional theory (DFT) calculations for a free-standing $\\mathrm{MoS_2}$ monolayer, using the generalized gradient approximation (GGA) PBE, the hybrid functional HSE06, and many-body perturbation theory with the GW approximation using PBE wavefunctions (G0W0@PBE).","We found that under biaxial strain with the experimental Poisson's ratio, the bandgap decreases at rates of 63 meV/% strain (PBE), 73 meV/% strain (HSE06), and 43 meV/% strain (G0W0@PBE), which are significantly smaller than the experimental rate.","We also found that PBE predicts a similarly smaller rate (90 meV/% strain) for a different Poisson's ratio of 0.25.","Spin-orbit correction (SOC) has little effect on the gap or its strain dependence.","Additionally, we observed a semiconductor-to-metal transition under an equal tensile biaxial strain of 10% and a transition from a direct to an indirect bandgap, consistent with previous theoretical work."],"url":"http://arxiv.org/abs/2406.06020v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 04:35:14","title":"EpiLearn: A Python Library for Machine Learning in Epidemic Modeling","abstract":"EpiLearn is a Python toolkit developed for modeling, simulating, and analyzing epidemic data. Although there exist several packages that also deal with epidemic modeling, they are often restricted to mechanistic models or traditional statistical tools. As machine learning continues to shape the world, the gap between these packages and the latest models has become larger. To bridge the gap and inspire innovative research in epidemic modeling, EpiLearn not only provides support for evaluating epidemic models based on machine learning, but also incorporates comprehensive tools for analyzing epidemic data, such as simulation, visualization, transformations, etc. For the convenience of both epidemiologists and data scientists, we provide a unified framework for training and evaluation of epidemic models on two tasks: Forecasting and Source Detection. To facilitate the development of new models, EpiLearn follows a modular design, making it flexible and easy to use. In addition, an interactive web application is also developed to visualize the real-world or simulated epidemic data. Our package is available at https://github.com/Emory-Melody/EpiLearn.","sentences":["EpiLearn is a Python toolkit developed for modeling, simulating, and analyzing epidemic data.","Although there exist several packages that also deal with epidemic modeling, they are often restricted to mechanistic models or traditional statistical tools.","As machine learning continues to shape the world, the gap between these packages and the latest models has become larger.","To bridge the gap and inspire innovative research in epidemic modeling, EpiLearn not only provides support for evaluating epidemic models based on machine learning, but also incorporates comprehensive tools for analyzing epidemic data, such as simulation, visualization, transformations, etc.","For the convenience of both epidemiologists and data scientists, we provide a unified framework for training and evaluation of epidemic models on two tasks: Forecasting and Source Detection.","To facilitate the development of new models, EpiLearn follows a modular design, making it flexible and easy to use.","In addition, an interactive web application is also developed to visualize the real-world or simulated epidemic data.","Our package is available at https://github.com/Emory-Melody/EpiLearn."],"url":"http://arxiv.org/abs/2406.06016v1","category":"cs.LG"}
{"created":"2024-06-10 03:50:43","title":"From yield stress to elastic instabilities: Tuning the extensional behavior of elastoviscoplastic fluid","abstract":"In this study, we delve into the intricacies of elastoviscoplastic (EVP) fluids, particularly focusing on how polymer additives influence their extensional behavior. Our findings reveal that polymer additives significantly alter the extensional properties of the EVP fluids, such as relaxation time and extensional stresses, while having negligible impact on the shear rheology. Interestingly, the modified fluids exhibit a transition from yield stress-like behavior to viscoelastic-like behavior under high extensional rates, ultimately leading to destabilization under extreme deformation. This research enhances the fundamental understanding of EVP fluids and highlights potential advancements in applications, especially in precision-demanding fields like 3D printing.","sentences":["In this study, we delve into the intricacies of elastoviscoplastic (EVP) fluids, particularly focusing on how polymer additives influence their extensional behavior.","Our findings reveal that polymer additives significantly alter the extensional properties of the EVP fluids, such as relaxation time and extensional stresses, while having negligible impact on the shear rheology.","Interestingly, the modified fluids exhibit a transition from yield stress-like behavior to viscoelastic-like behavior under high extensional rates, ultimately leading to destabilization under extreme deformation.","This research enhances the fundamental understanding of EVP fluids and highlights potential advancements in applications, especially in precision-demanding fields like 3D printing."],"url":"http://arxiv.org/abs/2406.06001v1","category":"physics.flu-dyn"}
{"created":"2024-06-10 03:47:24","title":"ThaiCoref: Thai Coreference Resolution Dataset","abstract":"While coreference resolution is a well-established research area in Natural Language Processing (NLP), research focusing on Thai language remains limited due to the lack of large annotated corpora. In this work, we introduce ThaiCoref, a dataset for Thai coreference resolution. Our dataset comprises 777,271 tokens, 44,082 mentions and 10,429 entities across four text genres: university essays, newspapers, speeches, and Wikipedia. Our annotation scheme is built upon the OntoNotes benchmark with adjustments to address Thai-specific phenomena. Utilizing ThaiCoref, we train models employing a multilingual encoder and cross-lingual transfer techniques, achieving a best F1 score of 67.88\\% on the test set. Error analysis reveals challenges posed by Thai's unique linguistic features. To benefit the NLP community, we make the dataset and the model publicly available at http://www.github.com/nlp-chula/thai-coref .","sentences":["While coreference resolution is a well-established research area in Natural Language Processing (NLP), research focusing on Thai language remains limited due to the lack of large annotated corpora.","In this work, we introduce ThaiCoref, a dataset for Thai coreference resolution.","Our dataset comprises 777,271 tokens, 44,082 mentions and 10,429 entities across four text genres: university essays, newspapers, speeches, and Wikipedia.","Our annotation scheme is built upon the OntoNotes benchmark with adjustments to address Thai-specific phenomena.","Utilizing ThaiCoref, we train models employing a multilingual encoder and cross-lingual transfer techniques, achieving a best F1 score of 67.88\\% on the test set.","Error analysis reveals challenges posed by Thai's unique linguistic features.","To benefit the NLP community, we make the dataset and the model publicly available at http://www.github.com/nlp-chula/thai-coref ."],"url":"http://arxiv.org/abs/2406.06000v1","category":"cs.CL"}
{"created":"2024-06-10 03:37:17","title":"Stability and Finiteness of Wasserstein Spaces","abstract":"Under Gromov--Hausdorff convergence, and equivariant Gromov--Hausdorff convergence, we prove stability results of Wasserstein spaces over certain classes of singular and non-singular spaces. For example, we obtain an analogue of Perelman's stability theorem on Wasserstein spaces.","sentences":["Under Gromov--Hausdorff convergence, and equivariant Gromov--Hausdorff convergence, we prove stability results of Wasserstein spaces over certain classes of singular and non-singular spaces.","For example, we obtain an analogue of Perelman's stability theorem on Wasserstein spaces."],"url":"http://arxiv.org/abs/2406.05998v1","category":"math.MG"}
{"created":"2024-06-10 03:10:29","title":"Cosmography of the minimally extended Varying Speed of Light Model","abstract":"Cosmography, as an integral branch of cosmology, strives to characterize the Universe without relying on pre-determined cosmological models. This model-independent approach utilizes Taylor series expansions around the current epoch, providing a direct correlation with cosmological observations and the potential to constrain theoretical models. Cosmologists can describe many measurable aspects of cosmology by using various combinations of cosmographic parameters. The varying speed of light model can be naturally implemented, provided that we do not make any further assumptions from the Robertson-Walker metric for cosmological time dilation. Therefore, we apply cosmography to the so-called minimally extended varying speed of light model. In this case, other cosmographic parameters can be used to construct the Hubble parameter for both the standard model and the varying speed-of-light model. On the other hand, distinct combinations of cosmographic values for the luminosity distance indicate the two models. Hence, luminosity distance might provide a method to constrain the parameters in varying speed-of-light models.","sentences":["Cosmography, as an integral branch of cosmology, strives to characterize the Universe without relying on pre-determined cosmological models.","This model-independent approach utilizes Taylor series expansions around the current epoch, providing a direct correlation with cosmological observations and the potential to constrain theoretical models.","Cosmologists can describe many measurable aspects of cosmology by using various combinations of cosmographic parameters.","The varying speed of light model can be naturally implemented, provided that we do not make any further assumptions from the Robertson-Walker metric for cosmological time dilation.","Therefore, we apply cosmography to the so-called minimally extended varying speed of light model.","In this case, other cosmographic parameters can be used to construct the Hubble parameter for both the standard model and the varying speed-of-light model.","On the other hand, distinct combinations of cosmographic values for the luminosity distance indicate the two models.","Hence, luminosity distance might provide a method to constrain the parameters in varying speed-of-light models."],"url":"http://arxiv.org/abs/2406.05990v1","category":"physics.gen-ph"}
{"created":"2024-06-10 03:06:09","title":"Data-Driven Real-time Coupon Allocation in the Online Platform","abstract":"Traditionally, firms have offered coupons to customer groups at predetermined discount rates. However, advancements in machine learning and the availability of abundant customer data now enable platforms to provide real-time customized coupons to individuals. In this study, we partner with Meituan, a leading shopping platform, to develop a real-time, end-to-end coupon allocation system that is fast and effective in stimulating demand while adhering to marketing budgets when faced with uncertain traffic from a diverse customer base. Leveraging comprehensive customer and product features, we estimate Conversion Rates (CVR) under various coupon values and employ isotonic regression to ensure the monotonicity of predicted CVRs with respect to coupon value. Using calibrated CVR predictions as input, we propose a Lagrangian Dual-based algorithm that efficiently determines optimal coupon values for each arriving customer within 50 milliseconds. We theoretically and numerically investigate the model performance under parameter misspecifications and apply a control loop to adapt to real-time updated information, thereby better adhering to the marketing budget. Finally, we demonstrate through large-scale field experiments and observational data that our proposed coupon allocation algorithm outperforms traditional approaches in terms of both higher conversion rates and increased revenue. As of May 2024, Meituan has implemented our framework to distribute coupons to over 100 million users across more than 110 major cities in China, resulting in an additional CNY 8 million in annual profit. We demonstrate how to integrate a machine learning prediction model for estimating customer CVR, a Lagrangian Dual-based coupon value optimizer, and a control system to achieve real-time coupon delivery while dynamically adapting to random customer arrival patterns.","sentences":["Traditionally, firms have offered coupons to customer groups at predetermined discount rates.","However, advancements in machine learning and the availability of abundant customer data now enable platforms to provide real-time customized coupons to individuals.","In this study, we partner with Meituan, a leading shopping platform, to develop a real-time, end-to-end coupon allocation system that is fast and effective in stimulating demand while adhering to marketing budgets when faced with uncertain traffic from a diverse customer base.","Leveraging comprehensive customer and product features, we estimate Conversion Rates (CVR) under various coupon values and employ isotonic regression to ensure the monotonicity of predicted CVRs with respect to coupon value.","Using calibrated CVR predictions as input, we propose a Lagrangian Dual-based algorithm that efficiently determines optimal coupon values for each arriving customer within 50 milliseconds.","We theoretically and numerically investigate the model performance under parameter misspecifications and apply a control loop to adapt to real-time updated information, thereby better adhering to the marketing budget.","Finally, we demonstrate through large-scale field experiments and observational data that our proposed coupon allocation algorithm outperforms traditional approaches in terms of both higher conversion rates and increased revenue.","As of May 2024, Meituan has implemented our framework to distribute coupons to over 100 million users across more than 110 major cities in China, resulting in an additional CNY 8 million in annual profit.","We demonstrate how to integrate a machine learning prediction model for estimating customer CVR, a Lagrangian Dual-based coupon value optimizer, and a control system to achieve real-time coupon delivery while dynamically adapting to random customer arrival patterns."],"url":"http://arxiv.org/abs/2406.05987v1","category":"econ.EM"}
{"created":"2024-06-10 03:00:28","title":"Neural-g: A Deep Learning Framework for Mixing Density Estimation","abstract":"Mixing (or prior) density estimation is an important problem in machine learning and statistics, especially in empirical Bayes $g$-modeling where accurately estimating the prior is necessary for making good posterior inferences. In this paper, we propose neural-$g$, a new neural network-based estimator for $g$-modeling. Neural-$g$ uses a softmax output layer to ensure that the estimated prior is a valid probability density. Under default hyperparameters, we show that neural-$g$ is very flexible and capable of capturing many unknown densities, including those with flat regions, heavy tails, and/or discontinuities. In contrast, existing methods struggle to capture all of these prior shapes. We provide justification for neural-$g$ by establishing a new universal approximation theorem regarding the capability of neural networks to learn arbitrary probability mass functions. To accelerate convergence of our numerical implementation, we utilize a weighted average gradient descent approach to update the network parameters. Finally, we extend neural-$g$ to multivariate prior density estimation. We illustrate the efficacy of our approach through simulations and analyses of real datasets. A software package to implement neural-$g$ is publicly available at https://github.com/shijiew97/neuralG.","sentences":["Mixing (or prior) density estimation is an important problem in machine learning and statistics, especially in empirical Bayes $g$-modeling where accurately estimating the prior is necessary for making good posterior inferences.","In this paper, we propose neural-$g$, a new neural network-based estimator for $g$-modeling.","Neural-$g$ uses a softmax output layer to ensure that the estimated prior is a valid probability density.","Under default hyperparameters, we show that neural-$g$ is very flexible and capable of capturing many unknown densities, including those with flat regions, heavy tails, and/or discontinuities.","In contrast, existing methods struggle to capture all of these prior shapes.","We provide justification for neural-$g$ by establishing a new universal approximation theorem regarding the capability of neural networks to learn arbitrary probability mass functions.","To accelerate convergence of our numerical implementation, we utilize a weighted average gradient descent approach to update the network parameters.","Finally, we extend neural-$g$ to multivariate prior density estimation.","We illustrate the efficacy of our approach through simulations and analyses of real datasets.","A software package to implement neural-$g$ is publicly available at https://github.com/shijiew97/neuralG."],"url":"http://arxiv.org/abs/2406.05986v1","category":"stat.ML"}
{"created":"2024-06-10 02:42:25","title":"Causality-inspired Latent Feature Augmentation for Single Domain Generalization","abstract":"Single domain generalization (Single-DG) intends to develop a generalizable model with only one single training domain to perform well on other unknown target domains. Under the domain-hungry configuration, how to expand the coverage of source domain and find intrinsic causal features across different distributions is the key to enhancing the models' generalization ability. Existing methods mainly depend on the meticulous design of finite image-level transformation techniques and learning invariant features across domains based on statistical correlation between samples and labels in source domain. This makes it difficult to capture stable semantics between source and target domains, which hinders the improvement of the model's generalization performance. In this paper, we propose a novel causality-inspired latent feature augmentation method for Single-DG by learning the meta-knowledge of feature-level transformation based on causal learning and interventions. Instead of strongly relying on the finite image-level transformation, with the learned meta-knowledge, we can generate diverse implicit feature-level transformations in latent space based on the consistency of causal features and diversity of non-causal features, which can better compensate for the domain-hungry defect and reduce the strong reliance on initial finite image-level transformations and capture more stable domain-invariant causal features for generalization. Extensive experiments on several open-access benchmarks demonstrate the outstanding performance of our model over other state-of-the-art single domain generalization and also multi-source domain generalization methods.","sentences":["Single domain generalization (Single-DG) intends to develop a generalizable model with only one single training domain to perform well on other unknown target domains.","Under the domain-hungry configuration, how to expand the coverage of source domain and find intrinsic causal features across different distributions is the key to enhancing the models' generalization ability.","Existing methods mainly depend on the meticulous design of finite image-level transformation techniques and learning invariant features across domains based on statistical correlation between samples and labels in source domain.","This makes it difficult to capture stable semantics between source and target domains, which hinders the improvement of the model's generalization performance.","In this paper, we propose a novel causality-inspired latent feature augmentation method for Single-DG by learning the meta-knowledge of feature-level transformation based on causal learning and interventions.","Instead of strongly relying on the finite image-level transformation, with the learned meta-knowledge, we can generate diverse implicit feature-level transformations in latent space based on the consistency of causal features and diversity of non-causal features, which can better compensate for the domain-hungry defect and reduce the strong reliance on initial finite image-level transformations and capture more stable domain-invariant causal features for generalization.","Extensive experiments on several open-access benchmarks demonstrate the outstanding performance of our model over other state-of-the-art single domain generalization and also multi-source domain generalization methods."],"url":"http://arxiv.org/abs/2406.05980v1","category":"cs.CV"}
{"created":"2024-06-10 02:06:54","title":"Visual-Inertial SLAM as Simple as A, B, VINS","abstract":"We present AB-VINS, a different kind of visual-inertial SLAM system. Unlike most VINS systems which only use hand-crafted techniques, AB-VINS makes use of three different deep networks. Instead of estimating sparse feature positions, AB-VINS only estimates the scale and bias parameters (a and b) of monocular depth maps, as well as other terms to correct the depth using multi-view information which results in a compressed feature state. Despite being an optimization-based system, the main VIO thread of AB-VINS surpasses the efficiency of a state-of-the-art filter-based method while also providing dense depth. While state-of-the-art loop-closing SLAM systems have to relinearize a number of variables linear the number of keyframes, AB-VINS can perform loop closures while only affecting a constant number of variables. This is due to a novel data structure called the memory tree, in which the keyframe poses are defined relative to each other rather than all in one global frame, allowing for all but a few states to be fixed. AB-VINS is not as accurate as state-of-the-art VINS systems, but it is shown through careful experimentation to be more robust.","sentences":["We present AB-VINS, a different kind of visual-inertial SLAM system.","Unlike most VINS systems which only use hand-crafted techniques, AB-VINS makes use of three different deep networks.","Instead of estimating sparse feature positions, AB-VINS only estimates the scale and bias parameters (a and b) of monocular depth maps, as well as other terms to correct the depth using multi-view information which results in a compressed feature state.","Despite being an optimization-based system, the main VIO thread of AB-VINS surpasses the efficiency of a state-of-the-art filter-based method while also providing dense depth.","While state-of-the-art loop-closing SLAM systems have to relinearize a number of variables linear the number of keyframes, AB-VINS can perform loop closures while only affecting a constant number of variables.","This is due to a novel data structure called the memory tree, in which the keyframe poses are defined relative to each other rather than all in one global frame, allowing for all but a few states to be fixed.","AB-VINS is not as accurate as state-of-the-art VINS systems, but it is shown through careful experimentation to be more robust."],"url":"http://arxiv.org/abs/2406.05969v1","category":"cs.RO"}
{"created":"2024-06-10 01:46:42","title":"Distributionally Robust Safe Sample Screening","abstract":"In this study, we propose a machine learning method called Distributionally Robust Safe Sample Screening (DRSSS). DRSSS aims to identify unnecessary training samples, even when the distribution of the training samples changes in the future. To achieve this, we effectively combine the distributionally robust (DR) paradigm, which aims to enhance model robustness against variations in data distribution, with the safe sample screening (SSS), which identifies unnecessary training samples prior to model training. Since we need to consider an infinite number of scenarios regarding changes in the distribution, we applied SSS because it does not require model training after the change of the distribution. In this paper, we employed the covariate shift framework to represent the distribution of training samples and reformulated the DR covariate-shift problem as a weighted empirical risk minimization problem, where the weights are subject to uncertainty within a predetermined range. By extending the existing SSS technique to accommodate this weight uncertainty, the DRSSS method is capable of reliably identifying unnecessary samples under any future distribution within a specified range. We provide a theoretical guarantee for the DRSSS method and validate its performance through numerical experiments on both synthetic and real-world datasets.","sentences":["In this study, we propose a machine learning method called Distributionally Robust Safe Sample Screening (DRSSS).","DRSSS aims to identify unnecessary training samples, even when the distribution of the training samples changes in the future.","To achieve this, we effectively combine the distributionally robust (DR) paradigm, which aims to enhance model robustness against variations in data distribution, with the safe sample screening (SSS), which identifies unnecessary training samples prior to model training.","Since we need to consider an infinite number of scenarios regarding changes in the distribution, we applied SSS because it does not require model training after the change of the distribution.","In this paper, we employed the covariate shift framework to represent the distribution of training samples and reformulated the DR covariate-shift problem as a weighted empirical risk minimization problem, where the weights are subject to uncertainty within a predetermined range.","By extending the existing SSS technique to accommodate this weight uncertainty, the DRSSS method is capable of reliably identifying unnecessary samples under any future distribution within a specified range.","We provide a theoretical guarantee for the DRSSS method and validate its performance through numerical experiments on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2406.05964v1","category":"stat.ML"}
{"created":"2024-06-10 01:39:04","title":"MAGNOLIA: Matching Algorithms via GNNs for Online Value-to-go Approximation","abstract":"Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, including advertising, crowdsourcing, ridesharing, and kidney exchange. We introduce a graph neural network (GNN) approach that emulates the problem's combinatorially-complex optimal online algorithm, which selects actions (e.g., which nodes to match) by computing each action's value-to-go (VTG) -- the expected weight of the final matching if the algorithm takes that action, then acts optimally in the future. We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks. Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs. This structure matches the local behavior of GNNs, providing theoretical justification for our approach.","sentences":["Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, including advertising, crowdsourcing, ridesharing, and kidney exchange.","We introduce a graph neural network (GNN) approach that emulates the problem's combinatorially-complex optimal online algorithm, which selects actions (e.g., which nodes to match) by computing each action's value-to-go (VTG) -- the expected weight of the final matching if the algorithm takes that action, then acts optimally in the future.","We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks.","Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs.","This structure matches the local behavior of GNNs, providing theoretical justification for our approach."],"url":"http://arxiv.org/abs/2406.05959v1","category":"cs.LG"}
{"created":"2024-06-10 00:59:09","title":"Economic and Environmental Sustainability Through Reshoring: A Case Study","abstract":"Not too long ago, offshoring was considered a panacea for many U.S. companies to achieve economic sustainability. Offshoring also created an unnecessary movement of goods between the point of consumption and the point of sourcing and hence contributed to greenhouse gas emissions. With many things changed, hundreds of U.S. companies have started Reshoring. Due to supply chain disruptions and increased tax implications, including tariffs, there is a growing desire among companies to achieve economic and environmental sustainability through reshoring. This model case study highlighted the common offshoring challenges and demonstrated new methods/solutions for the companies to save their bottom line. Using the Reshorability Index (RI) and Total Cost of Ownership (TCO) we developed a model to show which products or components we should bring back to the U.S. instead of continuing offshoring. From this study, we have found out that reshoring is not only an economically profitable decision but also has a positive impact on reducing GHG (Greenhouse Gas) emissions. Our research found that the companies that currently offshore heavy products will benefit more from implementing our developed model. Leveraging this model, industries can identify, and compare the ownership cost of their purchased materials and take the decision on potential reshoring. Additionally, companies will be able to calculate the GHG emission and identify the reduction of such emissions due to reshoring.","sentences":["Not too long ago, offshoring was considered a panacea for many U.S. companies to achieve economic sustainability.","Offshoring also created an unnecessary movement of goods between the point of consumption and the point of sourcing and hence contributed to greenhouse gas emissions.","With many things changed, hundreds of U.S. companies have started Reshoring.","Due to supply chain disruptions and increased tax implications, including tariffs, there is a growing desire among companies to achieve economic and environmental sustainability through reshoring.","This model case study highlighted the common offshoring challenges and demonstrated new methods/solutions for the companies to save their bottom line.","Using the Reshorability Index (RI) and Total Cost of Ownership (TCO) we developed a model to show which products or components we should bring back to the U.S. instead of continuing offshoring.","From this study, we have found out that reshoring is not only an economically profitable decision but also has a positive impact on reducing GHG (Greenhouse Gas) emissions.","Our research found that the companies that currently offshore heavy products will benefit more from implementing our developed model.","Leveraging this model, industries can identify, and compare the ownership cost of their purchased materials and take the decision on potential reshoring.","Additionally, companies will be able to calculate the GHG emission and identify the reduction of such emissions due to reshoring."],"url":"http://arxiv.org/abs/2406.05950v1","category":"eess.SY"}
{"created":"2024-06-09 23:56:49","title":"Linear Causal Representation Learning from Unknown Multi-node Interventions","abstract":"Despite the multifaceted recent advances in interventional causal representation learning (CRL), they primarily focus on the stylized assumption of single-node interventions. This assumption is not valid in a wide range of applications, and generally, the subset of nodes intervened in an interventional environment is fully unknown. This paper focuses on interventional CRL under unknown multi-node (UMN) interventional environments and establishes the first identifiability results for general latent causal models (parametric or nonparametric) under stochastic interventions (soft or hard) and linear transformation from the latent to observed space. Specifically, it is established that given sufficiently diverse interventional environments, (i) identifiability up to ancestors is possible using only soft interventions, and (ii) perfect identifiability is possible using hard interventions. Remarkably, these guarantees match the best-known results for more restrictive single-node interventions. Furthermore, CRL algorithms are also provided that achieve the identifiability guarantees. A central step in designing these algorithms is establishing the relationships between UMN interventional CRL and score functions associated with the statistical models of different interventional environments. Establishing these relationships also serves as constructive proof of the identifiability guarantees.","sentences":["Despite the multifaceted recent advances in interventional causal representation learning (CRL), they primarily focus on the stylized assumption of single-node interventions.","This assumption is not valid in a wide range of applications, and generally, the subset of nodes intervened in an interventional environment is fully unknown.","This paper focuses on interventional CRL under unknown multi-node (UMN) interventional environments and establishes the first identifiability results for general latent causal models (parametric or nonparametric) under stochastic interventions (soft or hard) and linear transformation from the latent to observed space.","Specifically, it is established that given sufficiently diverse interventional environments, (i) identifiability up to ancestors is possible using only soft interventions, and (ii) perfect identifiability is possible using hard interventions.","Remarkably, these guarantees match the best-known results for more restrictive single-node interventions.","Furthermore, CRL algorithms are also provided that achieve the identifiability guarantees.","A central step in designing these algorithms is establishing the relationships between UMN interventional CRL and score functions associated with the statistical models of different interventional environments.","Establishing these relationships also serves as constructive proof of the identifiability guarantees."],"url":"http://arxiv.org/abs/2406.05937v1","category":"cs.LG"}
{"created":"2024-06-09 23:55:14","title":"Multi-UAV Trajectory Design for Fair and Secure Communication","abstract":"Unmanned aerial vehicles (UAVs) play an essential role in future wireless communication networks due to their high mobility, low cost, and on-demand deployment. In air-to-ground links, UAVs are widely used to enhance the performance of wireless communication systems due to the presence of high-probability line-of-sight (LoS) links. However, the high probability of LoS links also increases the risk of being eavesdropped, posing a significant challenge to the security of wireless communications. In this work, the secure communication problem in a multi-UAV-assisted communication system is investigated in a moving airborne eavesdropping scenario. To improve the secrecy performance of the considered communication system, aerial eavesdropping capability is suppressed by sending jamming signals from a friendly UAV. An optimization problem under flight conditions, fairness, and limited energy consumption constraints of multiple UAVs is formulated to maximize the fair sum secrecy throughput. Given the complexity and non-convex nature of the problem, we propose a two-step-based optimization approach. The first step employs the $K$-means algorithm to cluster users and associate them with multiple communication UAVs. Then, a multi-agent deep deterministic policy gradient-based algorithm is introduced to solve this optimization problem. The effectiveness of this proposed algorithm is not only theoretically but also rigorously verified by simulation results.","sentences":["Unmanned aerial vehicles (UAVs) play an essential role in future wireless communication networks due to their high mobility, low cost, and on-demand deployment.","In air-to-ground links, UAVs are widely used to enhance the performance of wireless communication systems due to the presence of high-probability line-of-sight (LoS) links.","However, the high probability of LoS links also increases the risk of being eavesdropped, posing a significant challenge to the security of wireless communications.","In this work, the secure communication problem in a multi-UAV-assisted communication system is investigated in a moving airborne eavesdropping scenario.","To improve the secrecy performance of the considered communication system, aerial eavesdropping capability is suppressed by sending jamming signals from a friendly UAV.","An optimization problem under flight conditions, fairness, and limited energy consumption constraints of multiple UAVs is formulated to maximize the fair sum secrecy throughput.","Given the complexity and non-convex nature of the problem, we propose a two-step-based optimization approach.","The first step employs the $K$-means algorithm to cluster users and associate them with multiple communication UAVs.","Then, a multi-agent deep deterministic policy gradient-based algorithm is introduced to solve this optimization problem.","The effectiveness of this proposed algorithm is not only theoretically but also rigorously verified by simulation results."],"url":"http://arxiv.org/abs/2406.05936v1","category":"cs.IT"}
{"created":"2024-06-09 22:30:29","title":"Stabler Neo-Hookean Simulation: Absolute Eigenvalue Filtering for Projected Newton","abstract":"Volume-preserving hyperelastic materials are widely used to model near-incompressible materials such as rubber and soft tissues. However, the numerical simulation of volume-preserving hyperelastic materials is notoriously challenging within this regime due to the non-convexity of the energy function. In this work, we identify the pitfalls of the popular eigenvalue clamping strategy for projecting Hessian matrices to positive semi-definiteness during Newton's method. We introduce a novel eigenvalue filtering strategy for projected Newton's method to stabilize the optimization of Neo-Hookean energy and other volume-preserving variants under high Poisson's ratio (near 0.5) and large initial volume change. Our method only requires a single line of code change in the existing projected Newton framework, while achieving significant improvement in both stability and convergence speed. We demonstrate the effectiveness and efficiency of our eigenvalue projection scheme on a variety of challenging examples and over different deformations on a large dataset.","sentences":["Volume-preserving hyperelastic materials are widely used to model near-incompressible materials such as rubber and soft tissues.","However, the numerical simulation of volume-preserving hyperelastic materials is notoriously challenging within this regime due to the non-convexity of the energy function.","In this work, we identify the pitfalls of the popular eigenvalue clamping strategy for projecting Hessian matrices to positive semi-definiteness during Newton's method.","We introduce a novel eigenvalue filtering strategy for projected Newton's method to stabilize the optimization of Neo-Hookean energy and other volume-preserving variants under high Poisson's ratio (near 0.5) and large initial volume change.","Our method only requires a single line of code change in the existing projected Newton framework, while achieving significant improvement in both stability and convergence speed.","We demonstrate the effectiveness and efficiency of our eigenvalue projection scheme on a variety of challenging examples and over different deformations on a large dataset."],"url":"http://arxiv.org/abs/2406.05928v1","category":"cs.GR"}
{"created":"2024-06-09 21:53:41","title":"Imageless Contraband Detection Using a Millimeter-Wave Dynamic Antenna Array via Spatial Fourier Domain Sampling","abstract":"We demonstrate an imageless method of concealed contraband detection using a real-time 75 GHz rotationally dynamic antenna array. The array measures information in the two-dimensional Fourier domain and captures a set of samples that is sufficient for detecting concealed objects yet insufficient for generating full image, thereby preserving the privacy of screened subjects. The small set of Fourier samples contains sharp spatial frequency features in the Fourier domain which correspond to sharp edges of man-made objects such as handguns. We evaluate a set of classification methods: threshold-based, K-nearest neighbor, and support vector machine using radial basis function; all operating on arithmetic features directly extracted from the sampled Fourier-domain responses measured by a dynamically rotating millimeter-wave active interferometer. Noise transmitters are used to produce thermal-like radiation from scenes, enabling direct Fourier-domain sampling, while the rotational dynamics circularly sample the two-dimensional Fourier domain, capturing the sharp-edge induced responses. We experimentally demonstrate the detection of concealed metallic gun-shape object beneath clothing on a real person in a laboratory environment and achieved an accuracy and F1-score both at 0.986. The presented technique not only prevents image formation due to efficient Fourier-domain space sub-sampling but also requires only 211 ms from measurement to decision.","sentences":["We demonstrate an imageless method of concealed contraband detection using a real-time 75 GHz rotationally dynamic antenna array.","The array measures information in the two-dimensional Fourier domain and captures a set of samples that is sufficient for detecting concealed objects yet insufficient for generating full image, thereby preserving the privacy of screened subjects.","The small set of Fourier samples contains sharp spatial frequency features in the Fourier domain which correspond to sharp edges of man-made objects such as handguns.","We evaluate a set of classification methods: threshold-based, K-nearest neighbor, and support vector machine using radial basis function; all operating on arithmetic features directly extracted from the sampled Fourier-domain responses measured by a dynamically rotating millimeter-wave active interferometer.","Noise transmitters are used to produce thermal-like radiation from scenes, enabling direct Fourier-domain sampling, while the rotational dynamics circularly sample the two-dimensional Fourier domain, capturing the sharp-edge induced responses.","We experimentally demonstrate the detection of concealed metallic gun-shape object beneath clothing on a real person in a laboratory environment and achieved an accuracy and F1-score both at 0.986.","The presented technique not only prevents image formation due to efficient Fourier-domain space sub-sampling but also requires only 211 ms from measurement to decision."],"url":"http://arxiv.org/abs/2406.05924v1","category":"eess.SP"}
{"created":"2024-06-09 20:58:32","title":"Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for Direct Rendering","abstract":"Point cloud is a promising 3D representation for volumetric streaming in emerging AR/VR applications. Despite recent advances in point cloud compression, decoding and rendering high-quality images from lossy compressed point clouds is still challenging in terms of quality and complexity, making it a major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In this paper, we address this problem by developing a point cloud compression scheme that generates a bit stream that can be directly decoded to renderable 3D Gaussians. The encoder and decoder are jointly optimized to consider both bit-rates and rendering quality. It significantly improves the rendering quality while substantially reducing decoding and rendering time, compared to existing point cloud compression methods. Furthermore, the proposed scheme generates a scalable bit stream, allowing multiple levels of details at different bit-rate ranges. Our method supports real-time color decoding and rendering of high quality point clouds, thus paving the way for interactive 3D streaming applications with free view points.","sentences":["Point cloud is a promising 3D representation for volumetric streaming in emerging AR/VR applications.","Despite recent advances in point cloud compression, decoding and rendering high-quality images from lossy compressed point clouds is still challenging in terms of quality and complexity, making it a major roadblock to achieve real-time 6-Degree-of-Freedom video streaming.","In this paper, we address this problem by developing a point cloud compression scheme that generates a bit stream that can be directly decoded to renderable 3D Gaussians.","The encoder and decoder are jointly optimized to consider both bit-rates and rendering quality.","It significantly improves the rendering quality while substantially reducing decoding and rendering time, compared to existing point cloud compression methods.","Furthermore, the proposed scheme generates a scalable bit stream, allowing multiple levels of details at different bit-rate ranges.","Our method supports real-time color decoding and rendering of high quality point clouds, thus paving the way for interactive 3D streaming applications with free view points."],"url":"http://arxiv.org/abs/2406.05915v1","category":"cs.CV"}
{"created":"2024-06-09 20:19:50","title":"Boundary corrections for splitting methods in the time integration of multidimensional parabolic problems","abstract":"This work considers two boundary correction techniques to mitigate the reduction in the temporal order of convergence in PDE sense (i.e., when both the space and time resolutions tend to zero independently of each other) of $d$ dimension space-discretized parabolic problems on a rectangular domain subject to time dependent boundary conditions. We make use of the MoL approach (method of lines) where the space discretization is made with central differences of order four and the time integration is carried out with $s$-stage AMF-W-methods. The time integrators are of ADI-type (alternating direction implicit by using a directional splitting) and of higher order than the usual ones appearing in the literature which only reach order 2. Besides, the techniques here explained also work for most of splitting methods, when directional splitting is used. A remarkable fact is that with these techniques, the time integrators recover the temporal order of PDE-convergence at the level of time-independent boundary conditions.","sentences":["This work considers two boundary correction techniques to mitigate the reduction in the temporal order of convergence in PDE sense (i.e., when both the space and time resolutions tend to zero independently of each other) of $d$ dimension space-discretized parabolic problems on a rectangular domain subject to time dependent boundary conditions.","We make use of the MoL approach (method of lines) where the space discretization is made with central differences of order four and the time integration is carried out with $s$-stage AMF-W-methods.","The time integrators are of ADI-type (alternating direction implicit by using a directional splitting) and of higher order than the usual ones appearing in the literature which only reach order 2.","Besides, the techniques here explained also work for most of splitting methods, when directional splitting is used.","A remarkable fact is that with these techniques, the time integrators recover the temporal order of PDE-convergence at the level of time-independent boundary conditions."],"url":"http://arxiv.org/abs/2406.05907v1","category":"math.NA"}
{"created":"2024-06-09 20:01:01","title":"An optimal control strategy to design passive thermal cloaks of arbitrary shape","abstract":"In this paper we describe a numerical framework for achieving passive thermal cloaking of arbitrary shapes in both static and transient regimes. The design strategy is cast as the solution of an optimal control problem (OCP) for the heat equation where the coefficients of the thermal diffusivity matrix take the role of control functions and the distance between the uncloaked and the cloaked field is minimized in a suitable observation domain. The control actions enter bilinearly in the heat equation, thus making the resulting OCP nonlinear, and its analysis nontrivial. We show that optimal diffusivity coefficients exist both for the static and the transient case; we derive a system of first-order necessary optimality conditions; finally, we carry out their numerical approximation using the Finite Element Method. A series of numerical test cases assess the capability of our strategy to tackle passive thermal cloaking of arbitrarily complex two-dimensional objects.","sentences":["In this paper we describe a numerical framework for achieving passive thermal cloaking of arbitrary shapes in both static and transient regimes.","The design strategy is cast as the solution of an optimal control problem (OCP) for the heat equation where the coefficients of the thermal diffusivity matrix take the role of control functions and the distance between the uncloaked and the cloaked field is minimized in a suitable observation domain.","The control actions enter bilinearly in the heat equation, thus making the resulting OCP nonlinear, and its analysis nontrivial.","We show that optimal diffusivity coefficients exist both for the static and the transient case; we derive a system of first-order necessary optimality conditions; finally, we carry out their numerical approximation using the Finite Element Method.","A series of numerical test cases assess the capability of our strategy to tackle passive thermal cloaking of arbitrarily complex two-dimensional objects."],"url":"http://arxiv.org/abs/2406.05905v1","category":"math.OC"}
{"created":"2024-06-09 19:53:48","title":"Aegis: A Decentralized Expansion Blockchain","abstract":"Blockchains implement monetary systems operated by committees of nodes. The robustness of established blockchains presents an opportunity to leverage their infrastructure for creating expansion chains. Expansion chains can provide additional functionality to the primary chain they leverage or implement separate functionalities, while benefiting from the primary chain's security and the stability of its tokens. Indeed, tools like Ethereum's EigenLayer enable nodes to stake (deposit collateral) on a primary chain to form a committee responsible for operating an expansion chain.   But here is the rub. Classical protocols assume correct, well-behaved nodes stay correct indefinitely. Yet in our case, the stake incentivizes correctness--it will be slashed (revoked) if its owner deviates. Once a node withdraws its stake, there is no basis to assume its correctness.   To address the new challenge, we present Aegis, an expansion chain based on primary-chain stake, assuming a bounded primary-chain write time. Aegis uses references from Aegis blocks to primary blocks to define committees, checkpoints on the primary chain to perpetuate decisions, and resets on the primary chain to establish a new committee if the previous one becomes obsolete. It ensures safety at all times and rapid progress when latency among Aegis nodes is low.","sentences":["Blockchains implement monetary systems operated by committees of nodes.","The robustness of established blockchains presents an opportunity to leverage their infrastructure for creating expansion chains.","Expansion chains can provide additional functionality to the primary chain they leverage or implement separate functionalities, while benefiting from the primary chain's security and the stability of its tokens.","Indeed, tools like Ethereum's EigenLayer enable nodes to stake (deposit collateral) on a primary chain to form a committee responsible for operating an expansion chain.   ","But here is the rub.","Classical protocols assume correct, well-behaved nodes stay correct indefinitely.","Yet in our case, the stake incentivizes correctness--it will be slashed (revoked) if its owner deviates.","Once a node withdraws its stake, there is no basis to assume its correctness.   ","To address the new challenge, we present Aegis, an expansion chain based on primary-chain stake, assuming a bounded primary-chain write time.","Aegis uses references from Aegis blocks to primary blocks to define committees, checkpoints on the primary chain to perpetuate decisions, and resets on the primary chain to establish a new committee if the previous one becomes obsolete.","It ensures safety at all times and rapid progress when latency among Aegis nodes is low."],"url":"http://arxiv.org/abs/2406.05904v1","category":"cs.DC"}
{"created":"2024-06-09 19:38:27","title":"Large Language Models Memorize Sensor Datasets! Implications on Human Activity Recognition Research","abstract":"The astonishing success of Large Language Models (LLMs) in Natural Language Processing (NLP) has spurred their use in many application domains beyond text analysis, including wearable sensor-based Human Activity Recognition (HAR). In such scenarios, often sensor data are directly fed into an LLM along with text instructions for the model to perform activity classification. Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field. Yet, we argue, care has to be taken when evaluating LLM-based HAR systems in such a traditional way. Most contemporary LLMs are trained on virtually the entire (accessible) internet -- potentially including standard HAR datasets. With that, it is not unlikely that LLMs actually had access to the test data used in such benchmark experiments.The resulting contamination of training data would render these experimental evaluations meaningless. In this paper we investigate whether LLMs indeed have had access to standard HAR datasets during training. We apply memorization tests to LLMs, which involves instructing the models to extend given snippets of data. When comparing the LLM-generated output to the original data we found a non-negligible amount of matches which suggests that the LLM under investigation seems to indeed have seen wearable sensor data from the benchmark datasets during training. For the Daphnet dataset in particular, GPT-4 is able to reproduce blocks of sensor readings. We report on our investigations and discuss potential implications on HAR research, especially with regards to reporting results on experimental evaluation","sentences":["The astonishing success of Large Language Models (LLMs) in Natural Language Processing (NLP) has spurred their use in many application domains beyond text analysis, including wearable sensor-based Human Activity Recognition (HAR).","In such scenarios, often sensor data are directly fed into an LLM along with text instructions for the model to perform activity classification.","Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field.","Yet, we argue, care has to be taken when evaluating LLM-based HAR systems in such a traditional way.","Most contemporary LLMs are trained on virtually the entire (accessible) internet -- potentially including standard HAR datasets.","With that, it is not unlikely that LLMs actually had access to the test data used in such benchmark experiments.","The resulting contamination of training data would render these experimental evaluations meaningless.","In this paper we investigate whether LLMs indeed have had access to standard HAR datasets during training.","We apply memorization tests to LLMs, which involves instructing the models to extend given snippets of data.","When comparing the LLM-generated output to the original data we found a non-negligible amount of matches which suggests that the LLM under investigation seems to indeed have seen wearable sensor data from the benchmark datasets during training.","For the Daphnet dataset in particular, GPT-4 is able to reproduce blocks of sensor readings.","We report on our investigations and discuss potential implications on HAR research, especially with regards to reporting results on experimental evaluation"],"url":"http://arxiv.org/abs/2406.05900v1","category":"cs.LG"}
{"created":"2024-06-09 19:33:32","title":"InfoGaussian: Structure-Aware Dynamic Gaussians through Lightweight Information Shaping","abstract":"3D Gaussians, as a low-level scene representation, typically involve thousands to millions of Gaussians. This makes it difficult to control the scene in ways that reflect the underlying dynamic structure, where the number of independent entities is typically much smaller. In particular, it can be challenging to animate and move objects in the scene, which requires coordination among many Gaussians. To address this issue, we develop a mutual information shaping technique that enforces movement resonance between correlated Gaussians in a motion network. Such correlations can be learned from putative 2D object masks in different views. By approximating the mutual information with the Jacobians of the motions, our method ensures consistent movements of the Gaussians composing different objects under various perturbations. In particular, we develop an efficient contrastive training pipeline with lightweight optimization to shape the motion network, avoiding the need for re-shaping throughout the motion sequence. Notably, our training only touches a small fraction of all Gaussians in the scene yet attains the desired compositional behavior according to the underlying dynamic structure. The proposed technique is evaluated on challenging scenes and demonstrates significant performance improvement in promoting consistent movements and 3D object segmentation while inducing low computation and memory requirements.","sentences":["3D Gaussians, as a low-level scene representation, typically involve thousands to millions of Gaussians.","This makes it difficult to control the scene in ways that reflect the underlying dynamic structure, where the number of independent entities is typically much smaller.","In particular, it can be challenging to animate and move objects in the scene, which requires coordination among many Gaussians.","To address this issue, we develop a mutual information shaping technique that enforces movement resonance between correlated Gaussians in a motion network.","Such correlations can be learned from putative 2D object masks in different views.","By approximating the mutual information with the Jacobians of the motions, our method ensures consistent movements of the Gaussians composing different objects under various perturbations.","In particular, we develop an efficient contrastive training pipeline with lightweight optimization to shape the motion network, avoiding the need for re-shaping throughout the motion sequence.","Notably, our training only touches a small fraction of all Gaussians in the scene yet attains the desired compositional behavior according to the underlying dynamic structure.","The proposed technique is evaluated on challenging scenes and demonstrates significant performance improvement in promoting consistent movements and 3D object segmentation while inducing low computation and memory requirements."],"url":"http://arxiv.org/abs/2406.05897v1","category":"cs.CV"}
{"created":"2024-06-09 19:27:02","title":"Large population limit of interacting population dynamics via generalized gradient structures","abstract":"This chapter focuses on the derivation of a doubly nonlocal Fisher-KPP model, which is a macroscopic nonlocal evolution equation describing population dynamics in the large population limit. The derivation starts from a microscopic individual-based model described as a stochastic process on the space of atomic measures with jump rates that satisfy detailed balance w.r.t. to a reference measure. We make use of the so-called `cosh' generalized gradient structure for the law of the process to pass to the large population limit using evolutionary Gamma-convergence. In addition to characterizing the large population limit as the solution of the nonlocal Fisher-KPP model, our variational approach further provides a generalized gradient flow structure for the limit equation as well as an entropic propagation of chaos result.","sentences":["This chapter focuses on the derivation of a doubly nonlocal Fisher-KPP model, which is a macroscopic nonlocal evolution equation describing population dynamics in the large population limit.","The derivation starts from a microscopic individual-based model described as a stochastic process on the space of atomic measures with jump rates that satisfy detailed balance w.r.t.","to a reference measure.","We make use of the so-called `cosh' generalized gradient structure for the law of the process to pass to the large population limit using evolutionary Gamma-convergence.","In addition to characterizing the large population limit as the solution of the nonlocal Fisher-KPP model, our variational approach further provides a generalized gradient flow structure for the limit equation as well as an entropic propagation of chaos result."],"url":"http://arxiv.org/abs/2406.05894v1","category":"math.AP"}
{"created":"2024-06-09 19:23:20","title":"Event prediction and causality inference despite incomplete information","abstract":"We explored the challenge of predicting and explaining the occurrence of events within sequences of data points. Our focus was particularly on scenarios in which unknown triggers causing the occurrence of events may consist of non-consecutive, masked, noisy data points. This scenario is akin to an agent tasked with learning to predict and explain the occurrence of events without understanding the underlying processes or having access to crucial information. Such scenarios are encountered across various fields, such as genomics, hardware and software verification, and financial time series prediction. We combined analytical, simulation, and machine learning (ML) approaches to investigate, quantify, and provide solutions to this challenge. We deduced and validated equations generally applicable to any variation of the underlying challenge. Using these equations, we (1) described how the level of complexity changes with various parameters (e.g., number of apparent and hidden states, trigger length, confidence, etc.) and (2) quantified the data needed to successfully train an ML model. We then (3) proved our ML solution learns and subsequently identifies unknown triggers and predicts the occurrence of events. If the complexity of the challenge is too high, our ML solution can identify trigger candidates to be used to interactively probe the system under investigation to determine the true trigger in a way considerably more efficient than brute force methods. By sharing our findings, we aim to assist others grappling with similar challenges, enabling estimates on the complexity of their problem, the data required and a solution to solve it.","sentences":["We explored the challenge of predicting and explaining the occurrence of events within sequences of data points.","Our focus was particularly on scenarios in which unknown triggers causing the occurrence of events may consist of non-consecutive, masked, noisy data points.","This scenario is akin to an agent tasked with learning to predict and explain the occurrence of events without understanding the underlying processes or having access to crucial information.","Such scenarios are encountered across various fields, such as genomics, hardware and software verification, and financial time series prediction.","We combined analytical, simulation, and machine learning (ML) approaches to investigate, quantify, and provide solutions to this challenge.","We deduced and validated equations generally applicable to any variation of the underlying challenge.","Using these equations, we (1) described how the level of complexity changes with various parameters (e.g., number of apparent and hidden states, trigger length, confidence, etc.)","and (2) quantified the data needed to successfully train an ML model.","We then (3) proved our ML solution learns and subsequently identifies unknown triggers and predicts the occurrence of events.","If the complexity of the challenge is too high, our ML solution can identify trigger candidates to be used to interactively probe the system under investigation to determine the true trigger in a way considerably more efficient than brute force methods.","By sharing our findings, we aim to assist others grappling with similar challenges, enabling estimates on the complexity of their problem, the data required and a solution to solve it."],"url":"http://arxiv.org/abs/2406.05893v1","category":"cs.LG"}
{"created":"2024-06-09 19:17:14","title":"GCtx-UNet: Efficient Network for Medical Image Segmentation","abstract":"Medical image segmentation is crucial for disease diagnosis and monitoring. Though effective, the current segmentation networks such as UNet struggle with capturing long-range features. More accurate models such as TransUNet, Swin-UNet, and CS-UNet have higher computation complexity. To address this problem, we propose GCtx-UNet, a lightweight segmentation architecture that can capture global and local image features with accuracy better or comparable to the state-of-the-art approaches. GCtx-UNet uses vision transformer that leverages global context self-attention modules joined with local self-attention to model long and short range spatial dependencies. GCtx-UNet is evaluated on the Synapse multi-organ abdominal CT dataset, the ACDC cardiac MRI dataset, and several polyp segmentation datasets. In terms of Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD) metrics, GCtx-UNet outperformed CNN-based and Transformer-based approaches, with notable gains in the segmentation of complex and small anatomical structures. Moreover, GCtx-UNet is much more efficient than the state-of-the-art approaches with smaller model size, lower computation workload, and faster training and inference speed, making it a practical choice for clinical applications.","sentences":["Medical image segmentation is crucial for disease diagnosis and monitoring.","Though effective, the current segmentation networks such as UNet struggle with capturing long-range features.","More accurate models such as TransUNet, Swin-UNet, and CS-UNet have higher computation complexity.","To address this problem, we propose GCtx-UNet, a lightweight segmentation architecture that can capture global and local image features with accuracy better or comparable to the state-of-the-art approaches.","GCtx-UNet uses vision transformer that leverages global context self-attention modules joined with local self-attention to model long and short range spatial dependencies.","GCtx-UNet is evaluated on the Synapse multi-organ abdominal CT dataset, the ACDC cardiac MRI dataset, and several polyp segmentation datasets.","In terms of Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD) metrics, GCtx-UNet outperformed CNN-based and Transformer-based approaches, with notable gains in the segmentation of complex and small anatomical structures.","Moreover, GCtx-UNet is much more efficient than the state-of-the-art approaches with smaller model size, lower computation workload, and faster training and inference speed, making it a practical choice for clinical applications."],"url":"http://arxiv.org/abs/2406.05891v1","category":"eess.IV"}
{"created":"2024-06-09 18:45:41","title":"Are Large Language Models Actually Good at Text Style Transfer?","abstract":"We analyze the performance of large language models (LLMs) on Text Style Transfer (TST), specifically focusing on sentiment transfer and text detoxification across three languages: English, Hindi, and Bengali. Text Style Transfer involves modifying the linguistic style of a text while preserving its core content. We evaluate the capabilities of pre-trained LLMs using zero-shot and few-shot prompting as well as parameter-efficient finetuning on publicly available datasets. Our evaluation using automatic metrics, GPT-4 and human evaluations reveals that while some prompted LLMs perform well in English, their performance in on other languages (Hindi, Bengali) remains average. However, finetuning significantly improves results compared to zero-shot and few-shot prompting, making them comparable to previous state-of-the-art. This underscores the necessity of dedicated datasets and specialized models for effective TST.","sentences":["We analyze the performance of large language models (LLMs) on Text Style Transfer (TST), specifically focusing on sentiment transfer and text detoxification across three languages: English, Hindi, and Bengali.","Text Style Transfer involves modifying the linguistic style of a text while preserving its core content.","We evaluate the capabilities of pre-trained LLMs using zero-shot and few-shot prompting as well as parameter-efficient finetuning on publicly available datasets.","Our evaluation using automatic metrics, GPT-4 and human evaluations reveals that while some prompted LLMs perform well in English, their performance in on other languages (Hindi, Bengali) remains average.","However, finetuning significantly improves results compared to zero-shot and few-shot prompting, making them comparable to previous state-of-the-art.","This underscores the necessity of dedicated datasets and specialized models for effective TST."],"url":"http://arxiv.org/abs/2406.05885v1","category":"cs.CL"}
{"created":"2024-06-09 18:41:50","title":"Information Theoretic Guarantees For Policy Alignment In Large Language Models","abstract":"Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an $f$-divergence such as the $\\mathsf{KL}$ divergence. The best of $n$ alignment policy selects a sample from the reference policy that has the maximum reward among $n$ independent samples. For both cases (policy alignment and best of $n$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like $\\sqrt{\\mathsf{KL}}$, with an explicit bound in $n$ on the $\\mathsf{KL}$ for the best of $n$ policy. We show in this paper that the $\\sqrt{\\mathsf{KL}}$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of $n$ policy, that the $\\mathsf{KL}$ upper bound can be obtained for any $f$-divergence via a reduction to exponential order statistics owing to the R\\'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R\\'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.","sentences":["Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an $f$-divergence such as the $\\mathsf{KL}$ divergence.","The best of $n$ alignment policy selects a sample from the reference policy that has the maximum reward among $n$ independent samples.","For both cases (policy alignment and best of $n$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like $\\sqrt{\\mathsf{KL}}$, with an explicit bound in $n$ on the $\\mathsf{KL}$ for the best of $n$ policy.","We show in this paper that the $\\sqrt{\\mathsf{KL}}$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails.","Moreover, we prove for the best of $n$ policy, that the $\\mathsf{KL}$ upper bound can be obtained for any $f$-divergence via a reduction to exponential order statistics owing to the R\\'enyi representation of order statistics, and a data processing inequality.","If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R\\'enyi divergence.","Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward."],"url":"http://arxiv.org/abs/2406.05883v1","category":"cs.LG"}
{"created":"2024-06-09 18:41:05","title":"Distributional Preference Alignment of LLMs via Optimal Transport","abstract":"Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level. We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures. We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate. Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval.","sentences":["Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level.","We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs.","AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples.","We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost.","Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures.","We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples.","We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate.","Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval."],"url":"http://arxiv.org/abs/2406.05882v1","category":"cs.LG"}
{"created":"2024-06-09 18:28:24","title":"Imaging of seabed topography from the scattering of water waves","abstract":"We consider the problem of reconstructing the seabed topography from observations of surface gravity waves. We formulate the problem as a classical inverse scattering problem using the mild-slope equation, and analyze the topographic dependence of the forward and inverse problem. Moreover, we propose a useful model simplification that makes the inverse problem much more tractable. As water waves allow for observations of the full wave field, it differs quite a lot from the classical inverse scattering problems, and we utilize this to prove a conditional stability result for the inverse problem. Last, we develop a simple and fast numerical inversion method and test it on synthetic data to verify our analysis.","sentences":["We consider the problem of reconstructing the seabed topography from observations of surface gravity waves.","We formulate the problem as a classical inverse scattering problem using the mild-slope equation, and analyze the topographic dependence of the forward and inverse problem.","Moreover, we propose a useful model simplification that makes the inverse problem much more tractable.","As water waves allow for observations of the full wave field, it differs quite a lot from the classical inverse scattering problems, and we utilize this to prove a conditional stability result for the inverse problem.","Last, we develop a simple and fast numerical inversion method and test it on synthetic data to verify our analysis."],"url":"http://arxiv.org/abs/2406.05878v1","category":"math.AP"}
{"created":"2024-06-09 18:18:21","title":"The Nodal Sets of Solutions to Parabolic Equations","abstract":"In this paper, we study the parabolic equations $\\partial_t u=\\partial_j\\left(a^{ij}(x,t)\\partial_iu\\right)+b^j(x,t)\\partial_ju+c(x,t)u$ in a domain of $\\mathbb{R}^n$ under the condition that $a^{ij}$ are Lipschitz continuous. Consider the nodal set $Z_t=\\{x: u(x,t)=0\\}$ at a time $t$-slice. Simple examples show that the singular set $\\mathcal{S}_t=\\{x: u(x,t)=|\\nabla_x u|(x,t)=0\\}$ may coincide with nodal set. This makes the methods used in the study of nodal sets for elliptic equations fail, rendering the parabolic case much more complicated.   The current strongest results in the literature establish the finiteness of the $(n-1)$-dimensional Hausdorff measure of $Z_t$, assuming either $n=1$ by Angenent or that the coefficients are time-independent and analytic by Lin. With general coefficients, the codimension-one estimate was obtained under some doubling assumption by Han-Lin but only for space-time nodal sets. In the first part, we prove that $\\mathcal{H}^{n-1}(Z_t) < \\infty$ in full generality, i.e. for any dimension, with time-dependent coefficients and with merely Lipschitz regular leading coefficients $a^{ij}$.   In the second part, we study the evolutionary behavior of nodal sets. When $n=1$, it is proved by Angenent that the number of nodal points is non-increasing in time. For the $n$-dimensional case, we construct examples showing that measure monotonicity fails. In contrast, we prove dimension monotonicity, i.e., the Hausdorff dimension of the nodal set is non-increasing in time. This is the first monotonicity property for nodal sets in general dimensions. All the assumptions here are sharp.","sentences":["In this paper, we study the parabolic equations $\\partial_t u=\\partial_j\\left(a^{ij}(x,t)\\partial_iu\\right)+b^j(x,t)\\partial_ju+c(x,t)u$ in a domain of $\\mathbb{R}^n$ under the condition that $a^{ij}$ are Lipschitz continuous.","Consider the nodal set $Z_t=\\{x: u(x,t)=0\\}$ at a time $t$-slice.","Simple examples show that the singular set $\\mathcal{S}_t=\\{x: u(x,t)=|\\nabla_x u|(x,t)=0\\}$ may coincide with nodal set.","This makes the methods used in the study of nodal sets for elliptic equations fail, rendering the parabolic case much more complicated.   ","The current strongest results in the literature establish the finiteness of the $(n-1)$-dimensional Hausdorff measure of $Z_t$, assuming either $n=1$ by Angenent or that the coefficients are time-independent and analytic by Lin.","With general coefficients, the codimension-one estimate was obtained under some doubling assumption by Han-Lin but only for space-time nodal sets.","In the first part, we prove that $\\mathcal{H}^{n-1}(Z_t) < \\infty$ in full generality, i.e. for any dimension, with time-dependent coefficients and with merely Lipschitz regular leading coefficients $a^{ij}$.   In the second part, we study the evolutionary behavior of nodal sets.","When $n=1$, it is proved by Angenent that the number of nodal points is non-increasing in time.","For the $n$-dimensional case, we construct examples showing that measure monotonicity fails.","In contrast, we prove dimension monotonicity, i.e., the Hausdorff dimension of the nodal set is non-increasing in time.","This is the first monotonicity property for nodal sets in general dimensions.","All the assumptions here are sharp."],"url":"http://arxiv.org/abs/2406.05877v1","category":"math.DG"}
{"created":"2024-06-09 18:11:06","title":"Stealthy Targeted Backdoor Attacks against Image Captioning","abstract":"In recent years, there has been an explosive growth in multimodal learning. Image captioning, a classical multimodal task, has demonstrated promising applications and attracted extensive research attention. However, recent studies have shown that image caption models are vulnerable to some security threats such as backdoor attacks. Existing backdoor attacks against image captioning typically pair a trigger either with a predefined sentence or a single word as the targeted output, yet they are unrelated to the image content, making them easily noticeable as anomalies by humans. In this paper, we present a novel method to craft targeted backdoor attacks against image caption models, which are designed to be stealthier than prior attacks. Specifically, our method first learns a special trigger by leveraging universal perturbation techniques for object detection, then places the learned trigger in the center of some specific source object and modifies the corresponding object name in the output caption to a predefined target name. During the prediction phase, the caption produced by the backdoored model for input images with the trigger can accurately convey the semantic information of the rest of the whole image, while incorrectly recognizing the source object as the predefined target. Extensive experiments demonstrate that our approach can achieve a high attack success rate while having a negligible impact on model clean performance. In addition, we show our method is stealthy in that the produced backdoor samples are indistinguishable from clean samples in both image and text domains, which can successfully bypass existing backdoor defenses, highlighting the need for better defensive mechanisms against such stealthy backdoor attacks.","sentences":["In recent years, there has been an explosive growth in multimodal learning.","Image captioning, a classical multimodal task, has demonstrated promising applications and attracted extensive research attention.","However, recent studies have shown that image caption models are vulnerable to some security threats such as backdoor attacks.","Existing backdoor attacks against image captioning typically pair a trigger either with a predefined sentence or a single word as the targeted output, yet they are unrelated to the image content, making them easily noticeable as anomalies by humans.","In this paper, we present a novel method to craft targeted backdoor attacks against image caption models, which are designed to be stealthier than prior attacks.","Specifically, our method first learns a special trigger by leveraging universal perturbation techniques for object detection, then places the learned trigger in the center of some specific source object and modifies the corresponding object name in the output caption to a predefined target name.","During the prediction phase, the caption produced by the backdoored model for input images with the trigger can accurately convey the semantic information of the rest of the whole image, while incorrectly recognizing the source object as the predefined target.","Extensive experiments demonstrate that our approach can achieve a high attack success rate while having a negligible impact on model clean performance.","In addition, we show our method is stealthy in that the produced backdoor samples are indistinguishable from clean samples in both image and text domains, which can successfully bypass existing backdoor defenses, highlighting the need for better defensive mechanisms against such stealthy backdoor attacks."],"url":"http://arxiv.org/abs/2406.05874v1","category":"cs.CR"}
{"created":"2024-06-09 18:03:47","title":"OmniControlNet: Dual-stage Integration for Conditional Image Generation","abstract":"We provide a two-way integration for the widely adopted ControlNet by integrating external condition generation algorithms into a single dense prediction method and incorporating its individually trained image generation processes into a single model. Despite its tremendous success, the ControlNet of a two-stage pipeline bears limitations in being not self-contained (e.g. calls the external condition generation algorithms) with a large model redundancy (separately trained models for different types of conditioning inputs). Our proposed OmniControlNet consolidates 1) the condition generation (e.g., HED edges, depth maps, user scribble, and animal pose) by a single multi-tasking dense prediction algorithm under the task embedding guidance and 2) the image generation process for different conditioning types under the textual embedding guidance. OmniControlNet achieves significantly reduced model complexity and redundancy while capable of producing images of comparable quality for conditioned text-to-image generation.","sentences":["We provide a two-way integration for the widely adopted ControlNet by integrating external condition generation algorithms into a single dense prediction method and incorporating its individually trained image generation processes into a single model.","Despite its tremendous success, the ControlNet of a two-stage pipeline bears limitations in being not self-contained (e.g. calls the external condition generation algorithms) with a large model redundancy (separately trained models for different types of conditioning inputs).","Our proposed OmniControlNet consolidates 1) the condition generation (e.g., HED edges, depth maps, user scribble, and animal pose) by a single multi-tasking dense prediction algorithm under the task embedding guidance and 2) the image generation process for different conditioning types under the textual embedding guidance.","OmniControlNet achieves significantly reduced model complexity and redundancy while capable of producing images of comparable quality for conditioned text-to-image generation."],"url":"http://arxiv.org/abs/2406.05871v1","category":"cs.CV"}
{"created":"2024-06-09 17:53:54","title":"An Analysis of Elo Rating Systems via Markov Chains","abstract":"We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting. In particular, we study Elo under the Bradley--Terry--Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state of the art. We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem.","sentences":["We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting.","In particular, we study Elo under the Bradley--Terry--Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state of the art.","We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem."],"url":"http://arxiv.org/abs/2406.05869v1","category":"math.PR"}
{"created":"2024-06-09 17:45:09","title":"GANSky -- fast curved sky weak lensing simulations using Generative Adversarial Networks","abstract":"Extracting non-Gaussian information from the next generation weak lensing surveys will require fast and accurate full-sky simulations. This is difficult to achieve in practice with existing simulation methods: ray-traced $N$-body simulations are computationally expensive, and approximate simulation methods (such as lognormal mocks) are not accurate enough. Here, we present GANSky, an interpretable machine learning method that uses Generative Adversarial Networks (GANs) to produce fast and accurate full-sky tomographic weak lensing maps. The input to our GAN are lognormal maps that approximately describe the late-time convergence field of the Universe. Starting from these lognormal maps, we use GANs to learn how to locally redistribute mass to achieve simulation-quality maps. This can be achieved using remarkably small networks ($\\approx 10^3$ parameters). We validate the GAN maps by computing a number of summary statistics in both simulated and GANSky maps. We show that GANSky maps correctly reproduce both the mean and $\\chi^2$ distribution for several statistics, specifically: the 2-pt function, 1-pt PDF, peak and void counts, and the equilateral, folded and squeezed bispectra. These successes makes GANSky an attractive tool to compute the covariances of these statistics. In addition to being useful for rapidly generating large ensembles of artificial data sets, our method can be used to extract non-Gaussian information from weak lensing data with field-level or simulation-based inference.","sentences":["Extracting non-Gaussian information from the next generation weak lensing surveys will require fast and accurate full-sky simulations.","This is difficult to achieve in practice with existing simulation methods: ray-traced $N$-body simulations are computationally expensive, and approximate simulation methods (such as lognormal mocks) are not accurate enough.","Here, we present GANSky, an interpretable machine learning method that uses Generative Adversarial Networks (GANs) to produce fast and accurate full-sky tomographic weak lensing maps.","The input to our GAN are lognormal maps that approximately describe the late-time convergence field of the Universe.","Starting from these lognormal maps, we use GANs to learn how to locally redistribute mass to achieve simulation-quality maps.","This can be achieved using remarkably small networks ($\\approx 10^3$ parameters).","We validate the GAN maps by computing a number of summary statistics in both simulated and GANSky maps.","We show that GANSky maps correctly reproduce both the mean and $\\chi^2$ distribution for several statistics, specifically: the 2-pt function, 1-pt PDF, peak and void counts, and the equilateral, folded and squeezed bispectra.","These successes makes GANSky an attractive tool to compute the covariances of these statistics.","In addition to being useful for rapidly generating large ensembles of artificial data sets, our method can be used to extract non-Gaussian information from weak lensing data with field-level or simulation-based inference."],"url":"http://arxiv.org/abs/2406.05867v1","category":"astro-ph.CO"}
{"created":"2024-06-09 17:06:09","title":"Strong Convergence of Vorticities in the 2D Viscosity Limit on a Bounded Domain","abstract":"In the vanishing viscosity limit from the Navier-Stokes to Euler equations on domains with boundaries, a main difficulty comes from the mismatch of boundary conditions and, consequently, the possible formation of a boundary layer. Within a purely interior framework, Constantin and Vicol showed that the two-dimensional viscosity limit is justified for any arbitrary but finite time under the assumption that on each compactly contained subset of the domain, the enstrophies are bounded uniformly along the viscosity sequence. Within this framework, we upgrade to local strong convergence of the vorticities under a similar assumption on the $p$-enstrophies, $p>2$. The key novel idea is the analysis of the evolution of the weak convergence defect.","sentences":["In the vanishing viscosity limit from the Navier-Stokes to Euler equations on domains with boundaries, a main difficulty comes from the mismatch of boundary conditions and, consequently, the possible formation of a boundary layer.","Within a purely interior framework, Constantin and Vicol showed that the two-dimensional viscosity limit is justified for any arbitrary but finite time under the assumption that on each compactly contained subset of the domain, the enstrophies are bounded uniformly along the viscosity sequence.","Within this framework, we upgrade to local strong convergence of the vorticities under a similar assumption on the $p$-enstrophies, $p>2$. The key novel idea is the analysis of the evolution of the weak convergence defect."],"url":"http://arxiv.org/abs/2406.05860v1","category":"math.AP"}
{"created":"2024-06-09 16:49:30","title":"The Effect of Torsion on Neutron Star Structure in Einstein-Cartan Gravity","abstract":"Einstein--Cartan gravity is a close historical sibling of general relativity that allows for spacetime torsion. As a result, angular momentum couples to spacetime geometry in a similar way to energy. While consequences of this are well studied on cosmological scales, their role in neutron star physics is largely under-explored. We study the effects that torsion, sourced by either microphysical spin or macroscopic angular momentum, has on neutron stars. For this, we use a simplified polytropic model to quantify the microphysical coupling to torsion. We also derive expressions to model rotation-induced torsion effects and estimate the consequences for rotating neutron stars with different rotation rates. We find that the presence of torsion in general leads to neutron stars with smaller radii and masses, but higher central densities. Realistic models for microphysical spin lead to torsion effects that have no relevant influence on the neutron star structure. Rotation-induced torsion effects however, can decrease the radius by up to $900\\,m$, which is comparable to the increase due to centrifugal forces. Depending on which effect dominates, this leads to a torsion-induced spin-up or spin-down of the neutron star. We conclude that torsion effects due to rotation can not be neglected and are large enough to be tested using current or near-future technology.","sentences":["Einstein--Cartan gravity is a close historical sibling of general relativity that allows for spacetime torsion.","As a result, angular momentum couples to spacetime geometry in a similar way to energy.","While consequences of this are well studied on cosmological scales, their role in neutron star physics is largely under-explored.","We study the effects that torsion, sourced by either microphysical spin or macroscopic angular momentum, has on neutron stars.","For this, we use a simplified polytropic model to quantify the microphysical coupling to torsion.","We also derive expressions to model rotation-induced torsion effects and estimate the consequences for rotating neutron stars with different rotation rates.","We find that the presence of torsion in general leads to neutron stars with smaller radii and masses, but higher central densities.","Realistic models for microphysical spin lead to torsion effects that have no relevant influence on the neutron star structure.","Rotation-induced torsion effects however, can decrease the radius by up to $900\\,m$, which is comparable to the increase due to centrifugal forces.","Depending on which effect dominates, this leads to a torsion-induced spin-up or spin-down of the neutron star.","We conclude that torsion effects due to rotation can not be neglected and are large enough to be tested using current or near-future technology."],"url":"http://arxiv.org/abs/2406.05851v1","category":"gr-qc"}
{"created":"2024-06-09 15:56:14","title":"Stochastic ordering of series and parallel systems lifetime in Archimedean copula under random shock","abstract":"In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.","sentences":["In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures.","We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion.","We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures.","These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework.","Additionally, we provide examples and graphical representations to elucidate our theoretical findings."],"url":"http://arxiv.org/abs/2406.05834v1","category":"math.ST"}
{"created":"2024-06-09 15:54:23","title":"BOSC: A toolbox for aerial imagery mapping","abstract":"Accurate and efficient label of aerial images is essential for informed decision-making and resource allocation, whether in identifying crop types or delineating land-use patterns. The development of a comprehensive toolbox for manipulating and annotating aerial imagery represents a significant leap forward in remote sensing and spatial analysis. In this report, we introduce BOSC, a toolbox that enables researchers and practitioners to extract actionable insights with unprecedented accuracy and efficiency, addressing a critical need in today's abundance of drone and satellite resources. For more information or to explore BOSC, please visit our repository.","sentences":["Accurate and efficient label of aerial images is essential for informed decision-making and resource allocation, whether in identifying crop types or delineating land-use patterns.","The development of a comprehensive toolbox for manipulating and annotating aerial imagery represents a significant leap forward in remote sensing and spatial analysis.","In this report, we introduce BOSC, a toolbox that enables researchers and practitioners to extract actionable insights with unprecedented accuracy and efficiency, addressing a critical need in today's abundance of drone and satellite resources.","For more information or to explore BOSC, please visit our repository."],"url":"http://arxiv.org/abs/2406.05833v1","category":"cs.CV"}
{"created":"2024-06-09 15:41:42","title":"Electronic, optical, and transport properties of alkali metal oxides (Cs2O): A DFT study","abstract":"The electronic, structural, optical, and thermoelectric properties of the Cs2O cubic structure have been investigated using density functional theory (DFT). The calculations utilize a full relativistic version of the full-potential augmented plane-wave plus local orbitals method, which is based on density functional theory, employing both the GGA and LDA approximations. Additionally, we employed the GGA proposed by Trans-Blaha (GGA-mBJ) for band structure computations, revealing the indirect band gap nature of Cs2O. The optical properties are also addressed by computing the refractive index, extinction coefficient, and complex dielectric tensor. The electrical conductivity, Seebeck coefficient, and thermal conductivity exhibit temperature-dependent variations, indicating the formation of a thermoelectric material. Our findings indicate that the compound under investigation is categorized as a p-type semiconductor, with the majority of charge carriers responsible for conduction being holes rather than electrons.","sentences":["The electronic, structural, optical, and thermoelectric properties of the Cs2O cubic structure have been investigated using density functional theory (DFT).","The calculations utilize a full relativistic version of the full-potential augmented plane-wave plus local orbitals method, which is based on density functional theory, employing both the GGA and LDA approximations.","Additionally, we employed the GGA proposed by Trans-Blaha (GGA-mBJ) for band structure computations, revealing the indirect band gap nature of Cs2O.","The optical properties are also addressed by computing the refractive index, extinction coefficient, and complex dielectric tensor.","The electrical conductivity, Seebeck coefficient, and thermal conductivity exhibit temperature-dependent variations, indicating the formation of a thermoelectric material.","Our findings indicate that the compound under investigation is categorized as a p-type semiconductor, with the majority of charge carriers responsible for conduction being holes rather than electrons."],"url":"http://arxiv.org/abs/2406.05831v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-09 15:20:45","title":"Manipulating magnetism and transport properties of EuCd$_2$P$_2$ with a low carrier concentration","abstract":"Materials that exhibit strongly coupled magnetic order and electronic properties are crucial for both fundamental research and technological applications. However, finding a material that not only shows remarkable magnetoresistive responses but also has an easily tunable ground state remains a challenge. Here, we report successful manipulation of the magnetic and transport properties of EuCd$_2$P$_2$, which is transformed from an A-type antiferromagnet ($T_\\mathrm{N}$ = 11 K) exhibiting colossal magnetoresistance into a ferromagnet ($T_\\mathrm{C}$ = 47 K) with metallic behavior. The dramatic alteration results from a low hole concentration of $10^{19}$ cm$^{-3}$ induced by changing the growth conditions. Electronic structure and total energy calculations confirm the tunability of magnetism with a small carrier concentration for EuCd$_2$P$_2$. It is feasible to switch between the magnetic states by using field-effect to control the carrier density, thereby changing the magneto-electronic response. The controllable magnetism and electrical transport of EuCd$_2$P$_2$ make it a potential candidate for spintronics.","sentences":["Materials that exhibit strongly coupled magnetic order and electronic properties are crucial for both fundamental research and technological applications.","However, finding a material that not only shows remarkable magnetoresistive responses but also has an easily tunable ground state remains a challenge.","Here, we report successful manipulation of the magnetic and transport properties of EuCd$_2$P$_2$, which is transformed from an A-type antiferromagnet ($T_\\mathrm{N}$ = 11 K) exhibiting colossal magnetoresistance into a ferromagnet ($T_\\mathrm{C}$ = 47 K) with metallic behavior.","The dramatic alteration results from a low hole concentration of $10^{19}$ cm$^{-3}$ induced by changing the growth conditions.","Electronic structure and total energy calculations confirm the tunability of magnetism with a small carrier concentration for EuCd$_2$P$_2$.","It is feasible to switch between the magnetic states by using field-effect to control the carrier density, thereby changing the magneto-electronic response.","The controllable magnetism and electrical transport of EuCd$_2$P$_2$ make it a potential candidate for spintronics."],"url":"http://arxiv.org/abs/2406.05823v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-09 15:13:40","title":"Carrier-induced transition from antiferromagnetic insulator to ferromagnetic metal in the layered phosphide EuZn$_2$P$_2$","abstract":"EuZn$_2$P$_2$ was reported to be an insulating antiferromagnet with $T_\\mathrm{N}$ of 23.5 K. In this study, single crystals of EuZn$_2$P$_2$ exhibiting metallic behavior and a ferromagnetic order of 72 K ($T_\\mathrm{C}$) are successfully synthesized via a salt flux method. The presence of hole carriers induced by the Eu vacancies in the lattice is found to be crucial for the drastic changes in magnetism and electrical transport. The carriers mediate the interlayer ferromagnetic interaction, and the coupling strength is directly related to $T_\\mathrm{C}$, as evidenced by the linear dependence of $T_\\mathrm{C}$ and the fitted Curie-Weiss temperatures on the Eu-layer distances for ferromagnetic Eu$M_2X_2$ ($M$ = Zn, Cd; $X$ = P, As). The ferromagnetic EuZn$_2$P$_2$ shows conspicuous negative magnetoresistance (MR) near $T_\\mathrm{C}$, owing to strong magnetic scattering. The MR behavior is consistent with the Majumdar-Littlewood model, indicating that the MR can be enhanced by decreasing the carrier density. Our findings suggest that Eu$M_2X_2$ has highly tunable magnetism and charge transport, making it a promising material family for potential applications in spintronics.","sentences":["EuZn$_2$P$_2$ was reported to be an insulating antiferromagnet with $T_\\mathrm{N}$ of 23.5 K.","In this study, single crystals of EuZn$_2$P$_2$ exhibiting metallic behavior and a ferromagnetic order of 72 K ($T_\\mathrm{C}$) are successfully synthesized via a salt flux method.","The presence of hole carriers induced by the Eu vacancies in the lattice is found to be crucial for the drastic changes in magnetism and electrical transport.","The carriers mediate the interlayer ferromagnetic interaction, and the coupling strength is directly related to $T_\\mathrm{C}$, as evidenced by the linear dependence of $T_\\mathrm{C}$ and the fitted Curie-Weiss temperatures on the Eu-layer distances for ferromagnetic Eu$M_2X_2$ ($M$ = Zn, Cd; $X$ = P, As).","The ferromagnetic EuZn$_2$P$_2$ shows conspicuous negative magnetoresistance (MR) near $T_\\mathrm{C}$, owing to strong magnetic scattering.","The MR behavior is consistent with the Majumdar-Littlewood model, indicating that the MR can be enhanced by decreasing the carrier density.","Our findings suggest that Eu$M_2X_2$ has highly tunable magnetism and charge transport, making it a promising material family for potential applications in spintronics."],"url":"http://arxiv.org/abs/2406.05819v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-09 15:12:54","title":"Oracle modalities","abstract":"We give a new formulation of Turing reducibility in terms of higher modalities, inspired by an embedding of the Turing degrees in the lattice of subtoposes of the effective topos discovered by Hyland. In this definition, higher modalities play a similar role to I/O monads or dialogue trees in allowing a function to receive input from an external oracle. However, in homotopy type theory they have better logical properties than monads: they are compatible with higher types, and each modality corresponds to a reflective subuniverse that under suitable conditions is itself a model of homotopy type theory.   We give synthetic proofs of some basic results about Turing reducibility in cubical type theory making use of two axioms of Markov induction and computable choice. Both axioms are variants of axioms already studied in the effective topos. We show they hold in certain reflective subuniverses of cubical assemblies, demonstrate their use in some simple proofs in synthetic computability theory using modalities, and show they are downwards absolute for oracle modalities. These results have been formalised using cubical mode of the Agda proof assistant.   We explore some first connections between Turing reducibility and homotopy theory. This includes a synthetic proof that two Turing degrees are equal as soon as they induce isomorphic permutation groups on the natural numbers, making essential use of both Markov induction and the formulation of groups in HoTT as pointed, connected, 1-truncated types. We also give some simple non-topological examples of modalities in cubical assemblies based on these ideas, to illustrate what we expect higher dimensional analogues of the Turing degrees to look like.","sentences":["We give a new formulation of Turing reducibility in terms of higher modalities, inspired by an embedding of the Turing degrees in the lattice of subtoposes of the effective topos discovered by Hyland.","In this definition, higher modalities play a similar role to I/O monads or dialogue trees in allowing a function to receive input from an external oracle.","However, in homotopy type theory they have better logical properties than monads: they are compatible with higher types, and each modality corresponds to a reflective subuniverse that under suitable conditions is itself a model of homotopy type theory.   ","We give synthetic proofs of some basic results about Turing reducibility in cubical type theory making use of two axioms of Markov induction and computable choice.","Both axioms are variants of axioms already studied in the effective topos.","We show they hold in certain reflective subuniverses of cubical assemblies, demonstrate their use in some simple proofs in synthetic computability theory using modalities, and show they are downwards absolute for oracle modalities.","These results have been formalised using cubical mode of the Agda proof assistant.   ","We explore some first connections between Turing reducibility and homotopy theory.","This includes a synthetic proof that two Turing degrees are equal as soon as they induce isomorphic permutation groups on the natural numbers, making essential use of both Markov induction and the formulation of groups in HoTT as pointed, connected, 1-truncated types.","We also give some simple non-topological examples of modalities in cubical assemblies based on these ideas, to illustrate what we expect higher dimensional analogues of the Turing degrees to look like."],"url":"http://arxiv.org/abs/2406.05818v1","category":"math.LO"}
{"created":"2024-06-09 15:10:36","title":"Convex-area-wise Linear Regression and Algorithms for Data Analysis","abstract":"This paper introduces a new type of regression methodology named as Convex-Area-Wise Linear Regression(CALR), which separates given datasets by disjoint convex areas and fits different linear regression models for different areas. This regression model is highly interpretable, and it is able to interpolate any given datasets, even when the underlying relationship between explanatory and response variables are non-linear and discontinuous. In order to solve CALR problem, 3 accurate algorithms are proposed under different assumptions. The analysis of correctness and time complexity of the algorithms are given, indicating that the problem can be solved in $o(n^2)$ time accurately when the input datasets have some special features. Besides, this paper introduces an equivalent mixed integer programming problem of CALR which can be approximately solved using existing optimization solvers.","sentences":["This paper introduces a new type of regression methodology named as Convex-Area-Wise Linear Regression(CALR), which separates given datasets by disjoint convex areas and fits different linear regression models for different areas.","This regression model is highly interpretable, and it is able to interpolate any given datasets, even when the underlying relationship between explanatory and response variables are non-linear and discontinuous.","In order to solve CALR problem, 3 accurate algorithms are proposed under different assumptions.","The analysis of correctness and time complexity of the algorithms are given, indicating that the problem can be solved in $o(n^2)$ time accurately when the input datasets have some special features.","Besides, this paper introduces an equivalent mixed integer programming problem of CALR which can be approximately solved using existing optimization solvers."],"url":"http://arxiv.org/abs/2406.05817v1","category":"cs.DB"}
{"created":"2024-06-09 14:53:58","title":"CLT for Generalized Linear Spectral Statistics of High-dimensional Sample Covariance Matrices and Applications","abstract":"In this paper, we introduce the $\\mathbf{G}$eneralized $\\mathbf{L}$inear $\\mathbf{S}$pectral $\\mathbf{S}$tatistics (GLSS) of a high-dimensional sample covariance matrix $\\mathbf{S}_n$, denoted as $\\operatorname{tr}f(\\mathbf{S}_n)\\mathbf{B}_n$, which effectively captures distinct spectral properties of $\\mathbf{S}_n$ by involving an ancillary matrix $\\mathbf{B}_n$ and a test function $f$. The joint asymptotic normality of GLSS associated with different test functions is established under weak assumptions on $\\mathbf{B}_n$ and the underlying distribution, when the dimension $n$ and sample size $N$ are comparable. Specifically, we allow the rank of $\\mathbf{B}_n$ to diverge with $n$. The convergence rate of GLSS is determined by $\\sqrt{{N}/{\\operatorname{rank}(\\mathbf{B}_n)}}$. As a natural application, we propose a novel approach based on GLSS for hypothesis testing on eigenspaces of spiked covariance matrices. The theoretical accuracy of the results established for GLSS and the advantages of the newly suggested testing procedure are demonstrated through various numerical studies.","sentences":["In this paper, we introduce the $\\mathbf{G}$eneralized $\\mathbf{L}$inear $\\mathbf{S}$pectral $\\mathbf{S}$tatistics (GLSS) of a high-dimensional sample covariance matrix $\\mathbf{S}_n$, denoted as $\\operatorname{tr}f(\\mathbf{S}_n)\\mathbf{B}_n$, which effectively captures distinct spectral properties of $\\mathbf{S}_n$ by involving an ancillary matrix $\\mathbf{B}_n$ and a test function $f$.","The joint asymptotic normality of GLSS associated with different test functions is established under weak assumptions on $\\mathbf{B}_n$ and the underlying distribution, when the dimension $n$ and sample size $N$ are comparable.","Specifically, we allow the rank of $\\mathbf{B}_n$ to diverge with $n$. The convergence rate of GLSS is determined by $\\sqrt{{N}/{\\operatorname{rank}(\\mathbf{B}_n)}}$. As a natural application, we propose a novel approach based on GLSS for hypothesis testing on eigenspaces of spiked covariance matrices.","The theoretical accuracy of the results established for GLSS and the advantages of the newly suggested testing procedure are demonstrated through various numerical studies."],"url":"http://arxiv.org/abs/2406.05811v1","category":"math.ST"}
{"created":"2024-06-09 14:53:50","title":"ControlLoc: Physical-World Hijacking Attack on Visual Perception in Autonomous Driving","abstract":"Recent research in adversarial machine learning has focused on visual perception in Autonomous Driving (AD) and has shown that printed adversarial patches can attack object detectors. However, it is important to note that AD visual perception encompasses more than just object detection; it also includes Multiple Object Tracking (MOT). MOT enhances the robustness by compensating for object detection errors and requiring consistent object detection results across multiple frames before influencing tracking results and driving decisions. Thus, MOT makes attacks on object detection alone less effective. To attack such robust AD visual perception, a digital hijacking attack has been proposed to cause dangerous driving scenarios. However, this attack has limited effectiveness.   In this paper, we introduce a novel physical-world adversarial patch attack, ControlLoc, designed to exploit hijacking vulnerabilities in entire AD visual perception. ControlLoc utilizes a two-stage process: initially identifying the optimal location for the adversarial patch, and subsequently generating the patch that can modify the perceived location and shape of objects with the optimal location. Extensive evaluations demonstrate the superior performance of ControlLoc, achieving an impressive average attack success rate of around 98.1% across various AD visual perceptions and datasets, which is four times greater effectiveness than the existing hijacking attack. The effectiveness of ControlLoc is further validated in physical-world conditions, including real vehicle tests under different conditions such as outdoor light conditions with an average attack success rate of 77.5%. AD system-level impact assessments are also included, such as vehicle collision, using industry-grade AD systems and production-grade AD simulators with an average vehicle collision rate and unnecessary emergency stop rate of 81.3%.","sentences":["Recent research in adversarial machine learning has focused on visual perception in Autonomous Driving (AD) and has shown that printed adversarial patches can attack object detectors.","However, it is important to note that AD visual perception encompasses more than just object detection; it also includes Multiple Object Tracking (MOT).","MOT enhances the robustness by compensating for object detection errors and requiring consistent object detection results across multiple frames before influencing tracking results and driving decisions.","Thus, MOT makes attacks on object detection alone less effective.","To attack such robust AD visual perception, a digital hijacking attack has been proposed to cause dangerous driving scenarios.","However, this attack has limited effectiveness.   ","In this paper, we introduce a novel physical-world adversarial patch attack, ControlLoc, designed to exploit hijacking vulnerabilities in entire AD visual perception.","ControlLoc utilizes a two-stage process: initially identifying the optimal location for the adversarial patch, and subsequently generating the patch that can modify the perceived location and shape of objects with the optimal location.","Extensive evaluations demonstrate the superior performance of ControlLoc, achieving an impressive average attack success rate of around 98.1% across various AD visual perceptions and datasets, which is four times greater effectiveness than the existing hijacking attack.","The effectiveness of ControlLoc is further validated in physical-world conditions, including real vehicle tests under different conditions such as outdoor light conditions with an average attack success rate of 77.5%.","AD system-level impact assessments are also included, such as vehicle collision, using industry-grade AD systems and production-grade AD simulators with an average vehicle collision rate and unnecessary emergency stop rate of 81.3%."],"url":"http://arxiv.org/abs/2406.05810v1","category":"cs.CV"}
{"created":"2024-06-09 14:34:07","title":"Production and distribution planning, scheduling, and routing optimization in a yogurt supply chain under demand uncertainty: A case study","abstract":"Considering the evolution of the food industry and its challenges, like high perishability, managing the food industry supply chain is a key focus for researchers and decision-makers. Uncertainty in decision-making has gained importance, particularly in the yogurt industry, known for its complexity. This study addresses production and distribution planning, scheduling, and routing in the yogurt supply chain. The problem is characterized by multiple products, a single plant, multiple distribution centers, multiple periods, and various transportation methods. A mixed-integer non-linear programming (MINLP) model is used to minimize total costs, including production, setup, overtime, unmet demand, and transportation. Additionally, a robust fuzzy programming approach is applied under uncertainty, with linearization procedures proposed to convert it into a linearized mixed-integer programming formulation. The problem is tested with two data types: a sample problem in three sizes (small, medium, and large) and real data from Kalle Dairy Company, Iran. A Genetic Algorithm (GA) is developed to solve the problem, with necessary modifications made for its application. The GA's performance is compared to an exact algorithm (Branch & Cut), showing that the company's production policy adapts daily to meet demand precisely. The shift to smaller batch production and longer shelf life allows better stock allocation and avoids shortages in uncertain conditions. The company's policies adapt to severe fluctuations in the business environment, though this requires high costs, such as inventory maintenance.","sentences":["Considering the evolution of the food industry and its challenges, like high perishability, managing the food industry supply chain is a key focus for researchers and decision-makers.","Uncertainty in decision-making has gained importance, particularly in the yogurt industry, known for its complexity.","This study addresses production and distribution planning, scheduling, and routing in the yogurt supply chain.","The problem is characterized by multiple products, a single plant, multiple distribution centers, multiple periods, and various transportation methods.","A mixed-integer non-linear programming (MINLP) model is used to minimize total costs, including production, setup, overtime, unmet demand, and transportation.","Additionally, a robust fuzzy programming approach is applied under uncertainty, with linearization procedures proposed to convert it into a linearized mixed-integer programming formulation.","The problem is tested with two data types: a sample problem in three sizes (small, medium, and large) and real data from Kalle Dairy Company, Iran.","A Genetic Algorithm (GA) is developed to solve the problem, with necessary modifications made for its application.","The GA's performance is compared to an exact algorithm (Branch & Cut), showing that the company's production policy adapts daily to meet demand precisely.","The shift to smaller batch production and longer shelf life allows better stock allocation and avoids shortages in uncertain conditions.","The company's policies adapt to severe fluctuations in the business environment, though this requires high costs, such as inventory maintenance."],"url":"http://arxiv.org/abs/2406.05803v1","category":"math.OC"}
{"created":"2024-06-09 14:30:18","title":"SlowPerception: Physical-World Latency Attack against Visual Perception in Autonomous Driving","abstract":"Autonomous Driving (AD) systems critically depend on visual perception for real-time object detection and multiple object tracking (MOT) to ensure safe driving. However, high latency in these visual perception components can lead to significant safety risks, such as vehicle collisions. While previous research has extensively explored latency attacks within the digital realm, translating these methods effectively to the physical world presents challenges. For instance, existing attacks rely on perturbations that are unrealistic or impractical for AD, such as adversarial perturbations affecting areas like the sky, or requiring large patches that obscure most of a camera's view, thus making them impossible to be conducted effectively in the real world.   In this paper, we introduce SlowPerception, the first physical-world latency attack against AD perception, via generating projector-based universal perturbations. SlowPerception strategically creates numerous phantom objects on various surfaces in the environment, significantly increasing the computational load of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantial latency. Our SlowPerception achieves second-level latency in physical-world settings, with an average latency of 2.5 seconds across different AD perception systems, scenarios, and hardware configurations. This performance significantly outperforms existing state-of-the-art latency attacks. Additionally, we conduct AD system-level impact assessments, such as vehicle collisions, using industry-grade AD systems with production-grade AD simulators with a 97% average rate. We hope that our analyses can inspire further research in this critical domain, enhancing the robustness of AD systems against emerging vulnerabilities.","sentences":["Autonomous Driving (AD) systems critically depend on visual perception for real-time object detection and multiple object tracking (MOT) to ensure safe driving.","However, high latency in these visual perception components can lead to significant safety risks, such as vehicle collisions.","While previous research has extensively explored latency attacks within the digital realm, translating these methods effectively to the physical world presents challenges.","For instance, existing attacks rely on perturbations that are unrealistic or impractical for AD, such as adversarial perturbations affecting areas like the sky, or requiring large patches that obscure most of a camera's view, thus making them impossible to be conducted effectively in the real world.   ","In this paper, we introduce SlowPerception, the first physical-world latency attack against AD perception, via generating projector-based universal perturbations.","SlowPerception strategically creates numerous phantom objects on various surfaces in the environment, significantly increasing the computational load of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantial latency.","Our SlowPerception achieves second-level latency in physical-world settings, with an average latency of 2.5 seconds across different AD perception systems, scenarios, and hardware configurations.","This performance significantly outperforms existing state-of-the-art latency attacks.","Additionally, we conduct AD system-level impact assessments, such as vehicle collisions, using industry-grade AD systems with production-grade AD simulators with a 97% average rate.","We hope that our analyses can inspire further research in this critical domain, enhancing the robustness of AD systems against emerging vulnerabilities."],"url":"http://arxiv.org/abs/2406.05800v1","category":"cs.CV"}
{"created":"2024-06-09 14:03:44","title":"Integrated Sensing and Communication for Anti-Jamming with OAM","abstract":"The spectrum share and open nature of wireless channels enable integrated sensing and communication (ISAC) susceptible to hostile jamming attacks. Due to the intrinsic orthogonality and rich azimuth angle information of orbital angular momentum (OAM), vortex electromagnetic waves with helical phase fronts have shown great potential to achieve high-resolution imaging and strong anti-jamming capability of wireless communication. Focusing on significantly enhancing the anti-jamming results of ISAC systems with limited bandwidth under hostile jamming, in this paper we propose a novel ISAC for anti-jamming with OAM scheme, where the OAM legitimate transmitter can simultaneously sense the position of jammers with dynamic behavior and send data to multiple OAM legitimate users. Specifically, the OAM modes for sensing and communications are respectively hopped according to pre-set index modulation information to suppress jamming. To acquire the position of the jammer, we develop the enhanced multiple-signal-classification-based three-dimension position estimation scheme with continuous sensing in both frequency and angular domains, where the OAM transmitter is designed with the concentric uniform-circular-array mono-static method, to significantly increase the azimuthal resolution. Then, based on the acquired jamming channel state information, we develop the joint transmit-receive beamforming and power allocation scheme, where the transmit and receive beamforming matrices are dynamically adjusted to mitigate the mixed interference containing inter-mode interference, inter-user interference, and jamming, thus maximizing the achievable sum rates (ASRs) of all users. Numerical results demonstrate that our proposed scheme can significantly increase the ASR under broadband jamming attacks and achieve high detection accuracy of targets .","sentences":["The spectrum share and open nature of wireless channels enable integrated sensing and communication (ISAC) susceptible to hostile jamming attacks.","Due to the intrinsic orthogonality and rich azimuth angle information of orbital angular momentum (OAM), vortex electromagnetic waves with helical phase fronts have shown great potential to achieve high-resolution imaging and strong anti-jamming capability of wireless communication.","Focusing on significantly enhancing the anti-jamming results of ISAC systems with limited bandwidth under hostile jamming, in this paper we propose a novel ISAC for anti-jamming with OAM scheme, where the OAM legitimate transmitter can simultaneously sense the position of jammers with dynamic behavior and send data to multiple OAM legitimate users.","Specifically, the OAM modes for sensing and communications are respectively hopped according to pre-set index modulation information to suppress jamming.","To acquire the position of the jammer, we develop the enhanced multiple-signal-classification-based three-dimension position estimation scheme with continuous sensing in both frequency and angular domains, where the OAM transmitter is designed with the concentric uniform-circular-array mono-static method, to significantly increase the azimuthal resolution.","Then, based on the acquired jamming channel state information, we develop the joint transmit-receive beamforming and power allocation scheme, where the transmit and receive beamforming matrices are dynamically adjusted to mitigate the mixed interference containing inter-mode interference, inter-user interference, and jamming, thus maximizing the achievable sum rates (ASRs) of all users.","Numerical results demonstrate that our proposed scheme can significantly increase the ASR under broadband jamming attacks and achieve high detection accuracy of targets ."],"url":"http://arxiv.org/abs/2406.05790v1","category":"eess.SP"}
{"created":"2024-06-09 13:53:05","title":"Convolution and Attention-Free Mamba-based Cardiac Image Segmentation","abstract":"Convolutional Neural Networks (CNNs) and Transformer-based self-attention models have become standard for medical image segmentation. This paper demonstrates that convolution and self-attention, while widely used, are not the only effective methods for segmentation. Breaking with convention, we present a Convolution and self-Attention Free Mamba-based semantic Segmentation Network named CAF-MambaSegNet. Specifically, we design a Mamba-based Channel Aggregator and Spatial Aggregator, which are applied independently in each encoder-decoder stage. The Channel Aggregator extracts information across different channels, and the Spatial Aggregator learns features across different spatial locations. We also propose a Linearly Interconnected Factorized Mamba (LIFM) Block to reduce the computational complexity of a Mamba and to enhance its decision function by introducing a non-linearity between two factorized Mamba blocks. Our goal is not to outperform state-of-the-art results but to show how this innovative, convolution and self-attention-free method can inspire further research beyond well-established CNNs and Transformers, achieving linear complexity and reducing the number of parameters. Source code and pre-trained models will be publicly available.","sentences":["Convolutional Neural Networks (CNNs) and Transformer-based self-attention models have become standard for medical image segmentation.","This paper demonstrates that convolution and self-attention, while widely used, are not the only effective methods for segmentation.","Breaking with convention, we present a Convolution and self-Attention Free Mamba-based semantic Segmentation Network named CAF-MambaSegNet.","Specifically, we design a Mamba-based Channel Aggregator and Spatial Aggregator, which are applied independently in each encoder-decoder stage.","The Channel Aggregator extracts information across different channels, and the Spatial Aggregator learns features across different spatial locations.","We also propose a Linearly Interconnected Factorized Mamba (LIFM) Block to reduce the computational complexity of a Mamba and to enhance its decision function by introducing a non-linearity between two factorized Mamba blocks.","Our goal is not to outperform state-of-the-art results but to show how this innovative, convolution and self-attention-free method can inspire further research beyond well-established CNNs and Transformers, achieving linear complexity and reducing the number of parameters.","Source code and pre-trained models will be publicly available."],"url":"http://arxiv.org/abs/2406.05786v1","category":"cs.CV"}
{"created":"2024-06-09 13:42:51","title":"Optimizing Multi-Stuttered Speech Classification: Leveraging Whisper's Encoder for Efficient Parameter Reduction in Automated Assessment","abstract":"The automated classification of stuttered speech has significant implications for timely assessments providing assistance to speech language pathologists. Despite notable advancements in the field, the cases in which multiple disfluencies occur in speech require attention. We have taken a progressive approach to fill this gap by classifying multi-stuttered speech more efficiently. The problem has been addressed by firstly curating a dataset of multi-stuttered disfluencies from SEP-28k audio clips. Secondly, employing Whisper, a state-of-the-art speech recognition model has been leveraged by using its encoder and taking the problem as multi-label classification. Thirdly, using a 6 encoder layer Whisper and experimenting with various layer freezing strategies, a computationally efficient configuration of the model was identified. The proposed configuration achieved micro, macro, and weighted F1- scores of 0.88, 0.85, and 0.87, correspondingly on an external test dataset i.e. Fluency-Bank. In addition, through layer freezing strategies, we were able to achieve the aforementioned results by fine-tuning a single encoder layer, consequently, reducing the model's trainable parameters from 20.27 million to 3.29 million. This research study unveils the contribution of the last encoder layer in the identification of disfluencies in stuttered speech. Consequently, it has led to a computationally efficient approach which makes the model more adaptable for various dialects and languages.","sentences":["The automated classification of stuttered speech has significant implications for timely assessments providing assistance to speech language pathologists.","Despite notable advancements in the field, the cases in which multiple disfluencies occur in speech require attention.","We have taken a progressive approach to fill this gap by classifying multi-stuttered speech more efficiently.","The problem has been addressed by firstly curating a dataset of multi-stuttered disfluencies from SEP-28k audio clips.","Secondly, employing Whisper, a state-of-the-art speech recognition model has been leveraged by using its encoder and taking the problem as multi-label classification.","Thirdly, using a 6 encoder layer Whisper and experimenting with various layer freezing strategies, a computationally efficient configuration of the model was identified.","The proposed configuration achieved micro, macro, and weighted F1- scores of 0.88, 0.85, and 0.87, correspondingly on an external test dataset i.e. Fluency-Bank.","In addition, through layer freezing strategies, we were able to achieve the aforementioned results by fine-tuning a single encoder layer, consequently, reducing the model's trainable parameters from 20.27 million to 3.29 million.","This research study unveils the contribution of the last encoder layer in the identification of disfluencies in stuttered speech.","Consequently, it has led to a computationally efficient approach which makes the model more adaptable for various dialects and languages."],"url":"http://arxiv.org/abs/2406.05784v1","category":"cs.SD"}
{"created":"2024-06-09 13:25:02","title":"Learning to utilize gradient information for crisp edge detection","abstract":"Edge detection is a fundamental task in computer vision and it has made great progress under the development of deep convolutional neural networks (DCNNs), some of them have achieved a beyond human-level performance. However, recent top-performing edge detection methods tend to generate thick and blurred edge lines. In this work, we propose an effective method to solve this problem. Our approach consists of a lightweight pre-trained backbone, multi-scale contextual enhancement module aggregating gradient information (MCGI), boundary correction module (BCM), and boundary refinement module (BRM). In addition to this, we construct a novel hybrid loss function based on the Tversky index for solving the issue of imbalanced pixel distribution. We test our method on three standard benchmarks and the experiment results illustrate that our method improves the visual effect of edge maps and achieves a top performance among several state-of-the-art methods on the BSDS500 dataset (ODS F-score in standard evaluation is 0.829, in crispness evaluation is 0.720), NYUD-V2 dataset (ODS F-score in standard evaluation is 0.768, in crispness evaluation is \\textbf{0.546}), and BIPED dataset (ODS F-score in standard evaluation is 0.903).","sentences":["Edge detection is a fundamental task in computer vision and it has made great progress under the development of deep convolutional neural networks (DCNNs), some of them have achieved a beyond human-level performance.","However, recent top-performing edge detection methods tend to generate thick and blurred edge lines.","In this work, we propose an effective method to solve this problem.","Our approach consists of a lightweight pre-trained backbone, multi-scale contextual enhancement module aggregating gradient information (MCGI), boundary correction module (BCM), and boundary refinement module (BRM).","In addition to this, we construct a novel hybrid loss function based on the Tversky index for solving the issue of imbalanced pixel distribution.","We test our method on three standard benchmarks and the experiment results illustrate that our method improves the visual effect of edge maps and achieves a top performance among several state-of-the-art methods on the BSDS500 dataset (ODS F-score in standard evaluation is 0.829, in crispness evaluation is 0.720), NYUD-V2 dataset (ODS F-score in standard evaluation is 0.768, in crispness evaluation is \\textbf{0.546}), and BIPED dataset (ODS F-score in standard evaluation is 0.903)."],"url":"http://arxiv.org/abs/2406.05779v1","category":"cs.CV"}
{"created":"2024-06-09 13:21:29","title":"Identification of Intermediate-mass Black Hole Candidates Among a Sample of Sd Galaxies","abstract":"We analyzed images of every northern hemisphere Sd galaxy listed in the Third Reference Catalogue of Bright Galaxies (RC3) with a relatively face-on inclination ($\\theta\\leq30{\\deg}$). Specifically, we measured the spiral arms' winding angle, $\\phi$, in 85 galaxies. We applied a novel black hole mass planar scaling relation involving the rotational velocities (from the literature) and pitch angles of each galaxy to predict central black hole masses. This yielded 23 galaxies, each having at least a 50% chance of hosting a central intermediate-mass black hole (IMBH), $10^2<M_\\mathrm{BH}\\leq10^5\\,\\mathrm{M}_\\odot$. These 23 nearby ($\\lesssim$50 Mpc) targets may be suitable for an array of follow-up observations to check for active nuclei. Based on our full sample of 85 Sd galaxies, we estimate that the typical Sd galaxy (which tends to be bulgeless) harbors a black hole with $\\log(M_\\mathrm{BH}/\\mathrm{M}_\\odot)=6.00\\pm0.14$, but with a 27.7% chance of hosting an IMBH, making this morphological type of galaxy fertile ground for hunting elusive IMBHs. Thus, we find that a $\\sim$$10^6\\,\\mathrm{M}_\\odot$ black hole corresponds roughly to the onset of bulge development and serves as a conspicuous waypoint along the galaxy-SMBH coevolution journey. Our survey suggests that $>$1.22% of bright galaxies ($B_{\\rm T}\\lesssim15.5$ mag) in the local Universe host an IMBH (i.e., the \"occupation fraction\"), which implies a number density $>$$4.96\\times10^{-6}$ Mpc$^{-3}$ for central IMBHs. Finally, we observe that Sd galaxies exhibit an unexpected diversity of properties that resemble the general population of spiral galaxies, albeit with an enhanced signature of the eponymous prototypical traits (i.e., low masses, loosely wound spiral arms, and smaller rotational velocities).","sentences":["We analyzed images of every northern hemisphere","Sd galaxy listed in the Third Reference Catalogue of Bright Galaxies (RC3) with a relatively face-on inclination ($\\theta\\leq30{\\deg}$).","Specifically, we measured the spiral arms' winding angle, $\\phi$, in 85 galaxies.","We applied a novel black hole mass planar scaling relation involving the rotational velocities (from the literature) and pitch angles of each galaxy to predict central black hole masses.","This yielded 23 galaxies, each having at least a 50% chance of hosting a central intermediate-mass black hole (IMBH), $10^2<M_\\mathrm{BH}\\leq10^5\\,\\mathrm{M}_\\odot$.","These 23 nearby ($\\lesssim$50 Mpc) targets may be suitable for an array of follow-up observations to check for active nuclei.","Based on our full sample of 85 Sd galaxies, we estimate that the typical Sd galaxy (which tends to be bulgeless) harbors a black hole with $\\log(M_\\mathrm{BH}/\\mathrm{M}_\\odot)=6.00\\pm0.14$, but with a 27.7% chance of hosting an IMBH, making this morphological type of galaxy fertile ground for hunting elusive IMBHs.","Thus, we find that a $\\sim$$10^6\\,\\mathrm{M}_\\odot$ black hole corresponds roughly to the onset of bulge development and serves as a conspicuous waypoint along the galaxy-SMBH coevolution journey.","Our survey suggests that $>$1.22% of bright galaxies ($B_{\\rm T}\\lesssim15.5$ mag) in the local Universe host an IMBH (i.e., the \"occupation fraction\"), which implies a number density $>$$4.96\\times10^{-6}$ Mpc$^{-3}$ for central IMBHs.","Finally, we observe that Sd galaxies exhibit an unexpected diversity of properties that resemble the general population of spiral galaxies, albeit with an enhanced signature of the eponymous prototypical traits (i.e., low masses, loosely wound spiral arms, and smaller rotational velocities)."],"url":"http://arxiv.org/abs/2406.05778v1","category":"astro-ph.GA"}
{"created":"2024-06-09 13:19:20","title":"Utilizing Grounded SAM for self-supervised frugal camouflaged human detection","abstract":"Visually detecting camouflaged objects is a hard problem for both humans and computer vision algorithms. Strong similarities between object and background appearance make the task significantly more challenging than traditional object detection or segmentation tasks. Current state-of-the-art models use either convolutional neural networks or vision transformers as feature extractors. They are trained in a fully supervised manner and thus need a large amount of labeled training data. In this paper, both self-supervised and frugal learning methods are introduced to the task of Camouflaged Object Detection (COD). The overall goal is to fine-tune two COD reference methods, namely SINet-V2 and HitNet, pre-trained for camouflaged animal detection to the task of camouflaged human detection. Therefore, we use the public dataset CPD1K that contains camouflaged humans in a forest environment. We create a strong baseline using supervised frugal transfer learning for the fine-tuning task. Then, we analyze three pseudo-labeling approaches to perform the fine-tuning task in a self-supervised manner. Our experiments show that we achieve similar performance by pure self-supervision compared to fully supervised frugal learning.","sentences":["Visually detecting camouflaged objects is a hard problem for both humans and computer vision algorithms.","Strong similarities between object and background appearance make the task significantly more challenging than traditional object detection or segmentation tasks.","Current state-of-the-art models use either convolutional neural networks or vision transformers as feature extractors.","They are trained in a fully supervised manner and thus need a large amount of labeled training data.","In this paper, both self-supervised and frugal learning methods are introduced to the task of Camouflaged Object Detection (COD).","The overall goal is to fine-tune two COD reference methods, namely SINet-V2 and HitNet, pre-trained for camouflaged animal detection to the task of camouflaged human detection.","Therefore, we use the public dataset CPD1K that contains camouflaged humans in a forest environment.","We create a strong baseline using supervised frugal transfer learning for the fine-tuning task.","Then, we analyze three pseudo-labeling approaches to perform the fine-tuning task in a self-supervised manner.","Our experiments show that we achieve similar performance by pure self-supervision compared to fully supervised frugal learning."],"url":"http://arxiv.org/abs/2406.05776v1","category":"cs.CV"}
{"created":"2024-06-09 12:23:22","title":"Vision Mamba: Cutting-Edge Classification of Alzheimer's Disease with 3D MRI Scans","abstract":"Classifying 3D MRI images for early detection of Alzheimer's disease is a critical task in medical imaging. Traditional approaches using Convolutional Neural Networks (CNNs) and Transformers face significant challenges in this domain. CNNs, while effective in capturing local spatial features, struggle with long-range dependencies and often require extensive computational resources for high-resolution 3D data. Transformers, on the other hand, excel in capturing global context but suffer from quadratic complexity in inference time and require substantial memory, making them less efficient for large-scale 3D MRI data. To address these limitations, we propose the use of Vision Mamba, an advanced model based on State Space Models (SSMs), for the classification of 3D MRI images to detect Alzheimer's disease. Vision Mamba leverages dynamic state representations and the selective scan algorithm, allowing it to efficiently capture and retain important spatial information across 3D volumes. By dynamically adjusting state transitions based on input features, Vision Mamba can selectively retain relevant information, leading to more accurate and computationally efficient processing of 3D MRI data. Our approach combines the parallelizable nature of convolutional operations during training with the efficient, recurrent processing of states during inference. This architecture not only improves computational efficiency but also enhances the model's ability to handle long-range dependencies within 3D medical images. Experimental results demonstrate that Vision Mamba outperforms traditional CNN and Transformer models accuracy, making it a promising tool for the early detection of Alzheimer's disease using 3D MRI data.","sentences":["Classifying 3D MRI images for early detection of Alzheimer's disease is a critical task in medical imaging.","Traditional approaches using Convolutional Neural Networks (CNNs) and Transformers face significant challenges in this domain.","CNNs, while effective in capturing local spatial features, struggle with long-range dependencies and often require extensive computational resources for high-resolution 3D data.","Transformers, on the other hand, excel in capturing global context but suffer from quadratic complexity in inference time and require substantial memory, making them less efficient for large-scale 3D MRI data.","To address these limitations, we propose the use of Vision Mamba, an advanced model based on State Space Models (SSMs), for the classification of 3D MRI images to detect Alzheimer's disease.","Vision Mamba leverages dynamic state representations and the selective scan algorithm, allowing it to efficiently capture and retain important spatial information across 3D volumes.","By dynamically adjusting state transitions based on input features, Vision Mamba can selectively retain relevant information, leading to more accurate and computationally efficient processing of 3D MRI data.","Our approach combines the parallelizable nature of convolutional operations during training with the efficient, recurrent processing of states during inference.","This architecture not only improves computational efficiency but also enhances the model's ability to handle long-range dependencies within 3D medical images.","Experimental results demonstrate that Vision Mamba outperforms traditional CNN and Transformer models accuracy, making it a promising tool for the early detection of Alzheimer's disease using 3D MRI data."],"url":"http://arxiv.org/abs/2406.05757v1","category":"cs.CV"}
{"created":"2024-06-09 12:11:34","title":"The quantum-refractive evolution of polarization states in pulsar emission","abstract":"Highly magnetized neutron stars have quantum refraction effects on pulsar emission due to the non-linearity of the quantum electrodynamics (QED) action. In this paper, we investigate the evolution of the polarization states under the quantum refraction effects combined with the frequency dependence of pulsar emission; we solve a system of evolution equations of the Stokes vector, where the birefringent vector, in which such effects are encoded, acts on the Stokes vector. At a fixed frequency of emission, depending on the magnitude of the birefringent vector, dominated mostly by the magnetic field strength, the evolution of the Stokes vector largely exhibits three different patterns: (i) monotonic, or (ii) half-oscillatory, or (iii) highly oscillatory behaviours. These features are understood and confirmed by means of approximate analytical solutions to the evolution equations.","sentences":["Highly magnetized neutron stars have quantum refraction effects on pulsar emission due to the non-linearity of the quantum electrodynamics (QED) action.","In this paper, we investigate the evolution of the polarization states under the quantum refraction effects combined with the frequency dependence of pulsar emission; we solve a system of evolution equations of the Stokes vector, where the birefringent vector, in which such effects are encoded, acts on the Stokes vector.","At a fixed frequency of emission, depending on the magnitude of the birefringent vector, dominated mostly by the magnetic field strength, the evolution of the Stokes vector largely exhibits three different patterns: (i) monotonic, or (ii) half-oscillatory, or (iii) highly oscillatory behaviours.","These features are understood and confirmed by means of approximate analytical solutions to the evolution equations."],"url":"http://arxiv.org/abs/2406.05752v1","category":"astro-ph.HE"}
{"created":"2024-06-09 11:45:02","title":"Rapid Optimization of Superposition Codes for Multi-Hop NOMA MANETs via Deep Unfolding","abstract":"Various communication technologies are expected to utilize mobile ad hoc networks (MANETs). By combining MANETs with non-orthogonal multiple access (NOMA) communications, one can support scalable, spectrally efficient, and flexible network topologies. To achieve these benefits of NOMA MANETs, one should determine the transmission protocol, particularly the superposition code. However, the latter involves lengthy optimization that has to be repeated when the topology changes. In this work, we propose an algorithm for rapidly optimizing superposition codes in multi-hop NOMA MANETs. To achieve reliable tunning with few iterations, we adopt the emerging deep unfolding methodology, leveraging data to boost reliable settings. Our superposition coding optimization algorithm utilizes a small number of projected gradient steps while learning its per-user hyperparameters to maximize the minimal rate over past channels in an unsupervised manner. The learned optimizer is designed for both settings with full channel state information, as well as when the channel coefficients are to be estimated from pilots. We show that the combination of principled optimization and machine learning yields a scalable optimizer, that once trained, can be applied to different topologies. We cope with the non-convex nature of the optimization problem by applying parallel-learned optimization with different starting points as a form of ensemble learning. Our numerical results demonstrate that the proposed method enables the rapid setting of high-rate superposition codes for various channels.","sentences":["Various communication technologies are expected to utilize mobile ad hoc networks (MANETs).","By combining MANETs with non-orthogonal multiple access (NOMA) communications, one can support scalable, spectrally efficient, and flexible network topologies.","To achieve these benefits of NOMA MANETs, one should determine the transmission protocol, particularly the superposition code.","However, the latter involves lengthy optimization that has to be repeated when the topology changes.","In this work, we propose an algorithm for rapidly optimizing superposition codes in multi-hop NOMA MANETs.","To achieve reliable tunning with few iterations, we adopt the emerging deep unfolding methodology, leveraging data to boost reliable settings.","Our superposition coding optimization algorithm utilizes a small number of projected gradient steps while learning its per-user hyperparameters to maximize the minimal rate over past channels in an unsupervised manner.","The learned optimizer is designed for both settings with full channel state information, as well as when the channel coefficients are to be estimated from pilots.","We show that the combination of principled optimization and machine learning yields a scalable optimizer, that once trained, can be applied to different topologies.","We cope with the non-convex nature of the optimization problem by applying parallel-learned optimization with different starting points as a form of ensemble learning.","Our numerical results demonstrate that the proposed method enables the rapid setting of high-rate superposition codes for various channels."],"url":"http://arxiv.org/abs/2406.05747v1","category":"cs.IT"}
{"created":"2024-06-09 11:13:05","title":"Amalgamating inverse semigroups over ample semigroups","abstract":"A semigroup amalgam (S; T1, T2) is known to be non-embeddable if T1 and T2 are both groups (completely regular semigroups, Clifford semigroups) but S is not such. We prove some non-embeddability conditions for semigroup amalgams (S; T1, T2) in which T1 and T2 are inverse semigroups but S is not such. We also introduce some sufficient conditions that make the inverse hulls of the copies of S in T1 and T2 isomorphic; making, in turn, the amalgam (S; T1, T2) weakly embeddable.","sentences":["A semigroup amalgam (S; T1, T2) is known to be non-embeddable if T1 and T2 are both groups (completely regular semigroups, Clifford semigroups) but S is not such.","We prove some non-embeddability conditions for semigroup amalgams (S; T1, T2) in which T1 and T2 are inverse semigroups but S is not such.","We also introduce some sufficient conditions that make the inverse hulls of the copies of S in T1 and T2 isomorphic; making, in turn, the amalgam (S; T1, T2) weakly embeddable."],"url":"http://arxiv.org/abs/2406.05739v1","category":"math.GR"}
{"created":"2024-06-09 10:19:05","title":"Banach algebras associated to twisted \u00e9tale groupoids: simplicity and pure infiniteness","abstract":"We define reduced and essential Banach algebras associated to a twisted \\'{e}tale (not necessarily Hausdorff) groupoid $(\\mathcal{G},\\mathcal{L})$ and extend some fundamental results from $C^*$-algebras to this context. For instance, we prove that for topologically free and minimal groupoids every essential Banach algebra is simple, and we give conditions under which reduced algebras are essential (for example Hausdorffness of $\\mathcal{G}$ is sufficient). This in particular solves the simplicity problem posed recently by Gardella-Lupini for $L^p$-operator algebras associated to $\\mathcal{G}$. In addition, using either the $n$-filling or locally contracting condition we give pure infiniteness criteria for essential simple Banach algebras associated to $(\\mathcal{G},\\mathcal{L})$. This extends the corresponding $C^*$-algebraic results that were previously known to hold in the untwisted Hausdorff case. The results work nicely, and allow for characterisation of the generalized intersection property, in the realm of $L^P$-operator algebras where $P \\subseteq [1,\\infty]$ is a non-empty set of parameters. Such algebras cover in particular $L^p$-operator algebras, for $p\\in [1,\\infty]$, and their Banach $*$-algebra versions.   We apply our results to Banach algebra crossed products by twisted partial group actions, Roe-type Banach algebras with coefficients in finite-rank operators on a Banach space, twisted tight $L^P$-operator algebras of inverse semigroups, graph $L^P$-operator algebras, and algebras associated to self-similar group actions on graphs. We also interpret our results in terms of twisted inverse semigroup actions and their crossed products.","sentences":["We define reduced and essential Banach algebras associated to a twisted \\'{e}tale (not necessarily Hausdorff) groupoid $(\\mathcal{G},\\mathcal{L})$ and extend some fundamental results from $C^*$-algebras to this context.","For instance, we prove that for topologically free and minimal groupoids every essential Banach algebra is simple, and we give conditions under which reduced algebras are essential (for example Hausdorffness of $\\mathcal{G}$ is sufficient).","This in particular solves the simplicity problem posed recently by Gardella-Lupini for $L^p$-operator algebras associated to $\\mathcal{G}$. In addition, using either the $n$-filling or locally contracting condition we give pure infiniteness criteria for essential simple Banach algebras associated to $(\\mathcal{G},\\mathcal{L})$. This extends the corresponding $C^*$-algebraic results that were previously known to hold in the untwisted Hausdorff case.","The results work nicely, and allow for characterisation of the generalized intersection property, in the realm of $L^P$-operator algebras where $P \\subseteq [1,\\infty]$ is a non-empty set of parameters.","Such algebras cover in particular $L^p$-operator algebras, for $p\\in [1,\\infty]$, and their Banach $*$-algebra versions.   ","We apply our results to Banach algebra crossed products by twisted partial group actions, Roe-type Banach algebras with coefficients in finite-rank operators on a Banach space, twisted tight $L^P$-operator algebras of inverse semigroups, graph $L^P$-operator algebras, and algebras associated to self-similar group actions on graphs.","We also interpret our results in terms of twisted inverse semigroup actions and their crossed products."],"url":"http://arxiv.org/abs/2406.05717v1","category":"math.FA"}
{"created":"2024-06-09 10:13:26","title":"Error analysis of vertical test for CEPC 650 MHz superconducting radio-frequency cavity","abstract":"Hundreds of 650 MHz superconducting radio-frequency (SRF) cavities with high intrinsic quality factor (Q0) and accelerating gradient (Eacc) will be adopted for Circular Electron Positron Collider (CEPC). The values of Q0 and Eacc are obtained during vertical test at 2.0 K. Hence, high accuracy of vertical test is essential for evaluating the performance of SRF cavity. The 650 MHz SRF cavities achieved very high Q0 (6E10) and Eacc (40 MV/m) during the vertical test. In our study, the error analysis of vertical test was conducted in the scalar case, in order to achieve high accuracy. The uncertainties of vertical test were obtained through calculation, which was approximately 3% for Eacc and less than 5% for Q0. This result was reasonable and acceptable.","sentences":["Hundreds of 650 MHz superconducting radio-frequency (SRF) cavities with high intrinsic quality factor (Q0) and accelerating gradient (Eacc) will be adopted for Circular Electron Positron Collider (CEPC).","The values of Q0 and Eacc are obtained during vertical test at 2.0 K. Hence, high accuracy of vertical test is essential for evaluating the performance of SRF cavity.","The 650 MHz SRF cavities achieved very high Q0 (6E10) and Eacc (40 MV/m) during the vertical test.","In our study, the error analysis of vertical test was conducted in the scalar case, in order to achieve high accuracy.","The uncertainties of vertical test were obtained through calculation, which was approximately 3% for Eacc and less than 5% for Q0.","This result was reasonable and acceptable."],"url":"http://arxiv.org/abs/2406.05715v1","category":"physics.acc-ph"}
{"created":"2024-06-09 10:06:50","title":"Data-Driven Upper Confidence Bounds with Near-Optimal Regret for Heavy-Tailed Bandits","abstract":"Stochastic multi-armed bandits (MABs) provide a fundamental reinforcement learning model to study sequential decision making in uncertain environments. The upper confidence bounds (UCB) algorithm gave birth to the renaissance of bandit algorithms, as it achieves near-optimal regret rates under various moment assumptions. Up until recently most UCB methods relied on concentration inequalities leading to confidence bounds which depend on moment parameters, such as the variance proxy, that are usually unknown in practice. In this paper, we propose a new distribution-free, data-driven UCB algorithm for symmetric reward distributions, which needs no moment information. The key idea is to combine a refined, one-sided version of the recently developed resampled median-of-means (RMM) method with UCB. We prove a near-optimal regret bound for the proposed anytime, parameter-free RMM-UCB method, even for heavy-tailed distributions.","sentences":["Stochastic multi-armed bandits (MABs) provide a fundamental reinforcement learning model to study sequential decision making in uncertain environments.","The upper confidence bounds (UCB) algorithm gave birth to the renaissance of bandit algorithms, as it achieves near-optimal regret rates under various moment assumptions.","Up until recently most UCB methods relied on concentration inequalities leading to confidence bounds which depend on moment parameters, such as the variance proxy, that are usually unknown in practice.","In this paper, we propose a new distribution-free, data-driven UCB algorithm for symmetric reward distributions, which needs no moment information.","The key idea is to combine a refined, one-sided version of the recently developed resampled median-of-means (RMM) method with UCB.","We prove a near-optimal regret bound for the proposed anytime, parameter-free RMM-UCB method, even for heavy-tailed distributions."],"url":"http://arxiv.org/abs/2406.05710v1","category":"cs.LG"}
{"created":"2024-06-09 09:53:58","title":"Towards A General-Purpose Motion Planning for Autonomous Vehicles Using Fluid Dynamics","abstract":"General-purpose motion planners for automated/autonomous vehicles promise to handle the task of motion planning (including tactical decision-making and trajectory generation) for various automated driving functions (ADF) in a diverse range of operational design domains (ODDs). The challenges of designing a general-purpose motion planner arise from several factors: a) A plethora of scenarios with different semantic information in each driving scene should be addressed, b) a strong coupling between long-term decision-making and short-term trajectory generation shall be taken into account, c) the nonholonomic constraints of the vehicle dynamics must be considered, and d) the motion planner must be computationally efficient to run in real-time. The existing methods in the literature are either limited to specific scenarios (logic-based) or are data-driven (learning-based) and therefore lack explainability, which is important for safety-critical automated driving systems (ADS). This paper proposes a novel general-purpose motion planning solution for ADS inspired by the theory of fluid mechanics. A computationally efficient technique, i.e., the lattice Boltzmann method, is then adopted to generate a spatiotemporal vector field, which in accordance with the nonholonomic dynamic model of the Ego vehicle is employed to generate feasible candidate trajectories. The trajectory optimising ride quality, efficiency and safety is finally selected to calculate the imminent control signals, i.e., throttle/brake and steering angle. The performance of the proposed approach is evaluated by simulations in highway driving, on-ramp merging, and intersection crossing scenarios, and it is found to outperform traditional motion planning solutions based on model predictive control (MPC).","sentences":["General-purpose motion planners for automated/autonomous vehicles promise to handle the task of motion planning (including tactical decision-making and trajectory generation) for various automated driving functions (ADF) in a diverse range of operational design domains (ODDs).","The challenges of designing a general-purpose motion planner arise from several factors: a) A plethora of scenarios with different semantic information in each driving scene should be addressed, b) a strong coupling between long-term decision-making and short-term trajectory generation shall be taken into account, c) the nonholonomic constraints of the vehicle dynamics must be considered, and d)","the motion planner must be computationally efficient to run in real-time.","The existing methods in the literature are either limited to specific scenarios (logic-based) or are data-driven (learning-based) and therefore lack explainability, which is important for safety-critical automated driving systems (ADS).","This paper proposes a novel general-purpose motion planning solution for ADS inspired by the theory of fluid mechanics.","A computationally efficient technique, i.e., the lattice Boltzmann method, is then adopted to generate a spatiotemporal vector field, which in accordance with the nonholonomic dynamic model of the Ego vehicle is employed to generate feasible candidate trajectories.","The trajectory optimising ride quality, efficiency and safety is finally selected to calculate the imminent control signals, i.e., throttle/brake and steering angle.","The performance of the proposed approach is evaluated by simulations in highway driving, on-ramp merging, and intersection crossing scenarios, and it is found to outperform traditional motion planning solutions based on model predictive control (MPC)."],"url":"http://arxiv.org/abs/2406.05708v1","category":"cs.RO"}
{"created":"2024-06-09 08:57:05","title":"Intrinsic second-order topological insulators in two-dimensional polymorphic graphyne with sublattice approximation","abstract":"In two dimensions, intrinsic second-order topological insulators (SOTIs) are characterized by topological corner states that emerge at the intersections of distinct edges with reversed mass signs, enforced by spatial symmetries. Here, we present a comprehensive investigation within the class BDI to clarify the symmetry conditions ensuring the presence of intrinsic SOTIs in two dimensions. We reveal that the (anti-)commutation relationship between spatial symmetries and chiral symmetry is a reliable indicator of intrinsic corner states. Through first-principles calculations, we identify several ideal candidates within carbon-based polymorphic graphyne structures for realizing intrinsic SOTIs under sublattice approximation. Furthermore, we show that the corner states in these materials persist even in the absence of sublattice approximation. Our findings not only deepen the understanding of higher-order topological phases but also open new pathways for realizing topological corner states that are readily observable.","sentences":["In two dimensions, intrinsic second-order topological insulators (SOTIs) are characterized by topological corner states that emerge at the intersections of distinct edges with reversed mass signs, enforced by spatial symmetries.","Here, we present a comprehensive investigation within the class BDI to clarify the symmetry conditions ensuring the presence of intrinsic SOTIs in two dimensions.","We reveal that the (anti-)commutation relationship between spatial symmetries and chiral symmetry is a reliable indicator of intrinsic corner states.","Through first-principles calculations, we identify several ideal candidates within carbon-based polymorphic graphyne structures for realizing intrinsic SOTIs under sublattice approximation.","Furthermore, we show that the corner states in these materials persist even in the absence of sublattice approximation.","Our findings not only deepen the understanding of higher-order topological phases but also open new pathways for realizing topological corner states that are readily observable."],"url":"http://arxiv.org/abs/2406.05701v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-09 08:49:42","title":"Single Gateway Placement in Wireless Mesh Networks","abstract":"Wireless Mesh Networks (WMNs) are crucial for various sectors due to their adaptability and scalability, providing robust connectivity where traditional wired networks are impractical. WMNs facilitate smart city initiatives, disaster recovery efforts, and industrial automation, playing a pivotal role in modern networking applications. Their versatility also extends to rural connectivity, highlighting their relevance in diverse scenarios. Recent research in WMNs has focused on optimizing gateway placement and selection to enhance network performance and ensure efficient data transmission. This paper introduces a novel approach to maximize average throughput by strategically positioning gateways within the mesh topology. Inspired by Coulomb's law, which has been used in network analysis, this approach aims to improve network performance through strategic gateway positioning. Comprehensive simulations and analyses demonstrate the effectiveness of the proposed method in enhancing both throughput and network efficiency. By leveraging physics-based models like Coulomb's law, the study offers an objective means to optimize gateway placement, a critical component in WMN design. These findings provide valuable insights for network designers and operators, guiding informed decision-making for gateway deployment across various WMN deployments. This research significantly contributes to the ongoing evolution of WMN optimization strategies, reaffirming the essential role of gateway placement in establishing resilient and efficient wireless communication infrastructures.","sentences":["Wireless Mesh Networks (WMNs) are crucial for various sectors due to their adaptability and scalability, providing robust connectivity where traditional wired networks are impractical.","WMNs facilitate smart city initiatives, disaster recovery efforts, and industrial automation, playing a pivotal role in modern networking applications.","Their versatility also extends to rural connectivity, highlighting their relevance in diverse scenarios.","Recent research in WMNs has focused on optimizing gateway placement and selection to enhance network performance and ensure efficient data transmission.","This paper introduces a novel approach to maximize average throughput by strategically positioning gateways within the mesh topology.","Inspired by Coulomb's law, which has been used in network analysis, this approach aims to improve network performance through strategic gateway positioning.","Comprehensive simulations and analyses demonstrate the effectiveness of the proposed method in enhancing both throughput and network efficiency.","By leveraging physics-based models like Coulomb's law, the study offers an objective means to optimize gateway placement, a critical component in WMN design.","These findings provide valuable insights for network designers and operators, guiding informed decision-making for gateway deployment across various WMN deployments.","This research significantly contributes to the ongoing evolution of WMN optimization strategies, reaffirming the essential role of gateway placement in establishing resilient and efficient wireless communication infrastructures."],"url":"http://arxiv.org/abs/2406.05698v1","category":"cs.NI"}
{"created":"2024-06-09 08:45:18","title":"Decision-Focused Surrogate Modeling for Mixed-Integer Linear Optimization","abstract":"Mixed-integer optimization is at the core of many online decision-making systems that demand frequent updates of decisions in real time. However, due to their combinatorial nature, mixed-integer linear programs (MILPs) can be difficult to solve, rendering them often unsuitable for time-critical online applications. To address this challenge, we develop a data-driven approach for constructing surrogate optimization models in the form of linear programs (LPs) that can be solved much more efficiently than the corresponding MILPs. We train these surrogate LPs in a decision-focused manner such that for different model inputs, they achieve the same or close to the same optimal solutions as the original MILPs. One key advantage of the proposed method is that it allows the incorporation of all the original MILP's linear constraints, which significantly increases the likelihood of obtaining feasible predicted solutions. Results from two computational case studies indicate that this decision-focused surrogate modeling approach is highly data-efficient and provides very accurate predictions of the optimal solutions. In these examples, it outperforms more commonly used neural-network-based optimization proxies.","sentences":["Mixed-integer optimization is at the core of many online decision-making systems that demand frequent updates of decisions in real time.","However, due to their combinatorial nature, mixed-integer linear programs (MILPs) can be difficult to solve, rendering them often unsuitable for time-critical online applications.","To address this challenge, we develop a data-driven approach for constructing surrogate optimization models in the form of linear programs (LPs) that can be solved much more efficiently than the corresponding MILPs.","We train these surrogate LPs in a decision-focused manner such that for different model inputs, they achieve the same or close to the same optimal solutions as the original MILPs.","One key advantage of the proposed method is that it allows the incorporation of all the original MILP's linear constraints, which significantly increases the likelihood of obtaining feasible predicted solutions.","Results from two computational case studies indicate that this decision-focused surrogate modeling approach is highly data-efficient and provides very accurate predictions of the optimal solutions.","In these examples, it outperforms more commonly used neural-network-based optimization proxies."],"url":"http://arxiv.org/abs/2406.05697v1","category":"math.OC"}
{"created":"2024-06-09 08:31:14","title":"MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation","abstract":"A story premise succinctly defines a story's main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Precollect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories. Code: https://github.com/GAIR-NLP/MoPS.","sentences":["A story premise succinctly defines a story's main idea, foundation, and trajectory.","It serves as the initial trigger in automatic story generation.","Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale.","In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation.","MoPS consists of three phases: (1) Precollect a consistent set of candidates for each module to form a nested dictionary.","(2) Extract a key path from the nested dictionary as the premise design.","(3) Instruct an LLM to integrate the design into a coherent premise sentence.","Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets.","Similarly, the extended novels and scripts generated from our premises also exhibit higher quality.","In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories.","Code: https://github.com/GAIR-NLP/MoPS."],"url":"http://arxiv.org/abs/2406.05690v1","category":"cs.CL"}
{"created":"2024-06-10 17:58:02","title":"Data Augmentation for Multivariate Time Series Classification: An Experimental Study","abstract":"Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive. Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models. This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision. Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification. Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models. Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy. This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability.","sentences":["Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive.","Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models.","This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision.","Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification.","Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models.","Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy.","This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability."],"url":"http://arxiv.org/abs/2406.06518v1","category":"cs.LG"}
{"created":"2024-06-10 17:31:07","title":"Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity","abstract":"For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences. This enables RL agents to adapt their behavior based on human desires. However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task. It becomes essential that agents learn to focus on the subset of task-relevant environment features. Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features. This can result in algorithms that may not effectively transfer to a more noisy real-world setting. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments.","sentences":["For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings.","Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences.","This enables RL agents to adapt their behavior based on human desires.","However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task.","It becomes essential that agents learn to focus on the subset of task-relevant environment features.","Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features.","This can result in algorithms that may not effectively transfer to a more noisy real-world setting.","To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features.","We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions.","In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments."],"url":"http://arxiv.org/abs/2406.06495v1","category":"cs.LG"}
{"created":"2024-06-10 16:15:14","title":"Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome","abstract":"Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.","sentences":["Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials.","With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial.","While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context.","Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects.","In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption.","This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment.","We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation.","We present a numerical example of re-designing an oncology trial.","An extensive simulation study is conducted to evaluate the performance of the proposed design."],"url":"http://arxiv.org/abs/2406.06426v1","category":"stat.ME"}
{"created":"2024-06-10 15:46:25","title":"Towards Lifelong Learning of Large Language Models: A Survey","abstract":"As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.","sentences":["As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial.","Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information.","Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting.","This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge.","Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios.","External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters.","The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era.","Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications."],"url":"http://arxiv.org/abs/2406.06391v1","category":"cs.LG"}
{"created":"2024-06-10 14:46:07","title":"A Parameter-efficient Language Extension Framework for Multilingual ASR","abstract":"Covering all languages with a multilingual speech recognition model (MASR) is very difficult. Performing language extension on top of an existing MASR is a desirable choice. In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems. Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE. PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language. Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA. Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes. The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting. Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter.","sentences":["Covering all languages with a multilingual speech recognition model (MASR) is very difficult.","Performing language extension on top of an existing MASR is a desirable choice.","In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems.","Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE.","PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language.","Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA.","Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes.","The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting.","Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter."],"url":"http://arxiv.org/abs/2406.06329v1","category":"cs.CL"}
{"created":"2024-06-10 13:53:31","title":"Global-in-time energy stability analysis for the exponential time differencing Runge-Kutta scheme for the phase field crystal equation","abstract":"The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution. To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time. In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time. An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis. Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified. This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished. This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional. The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations.","sentences":["The global-in-time energy estimate is derived for the second-order accurate exponential time differencing Runge-Kutta (ETDRK2) numerical scheme to the phase field crystal (PFC) equation, a sixth-order parabolic equation modeling crystal evolution.","To recover the value of stabilization constant, some local-in-time convergence analysis has been reported, and the energy stability becomes available over a fixed final time.","In this work, we develop a global-in-time energy estimate for the ETDRK2 numerical scheme to the PFC equation by showing the energy dissipation property for any final time.","An a priori assumption at the previous time step, combined with a single-step $H^2$ estimate of the numerical solution, is the key point in the analysis.","Such an $H^2$ estimate recovers the maximum norm bound of the numerical solution at the next time step, and then the value of the stabilization parameter can be theoretically justified.","This justification ensures the energy dissipation at the next time step, so that the mathematical induction can be effectively applied, by then the global-in-time energy estimate is accomplished.","This paper represents the first effort to theoretically establish a global-in-time energy stability analysis for a second-order stabilized numerical scheme in terms of the original free energy functional.","The presented methodology is expected to be available for many other Runge-Kutta numerical schemes to the gradient flow equations."],"url":"http://arxiv.org/abs/2406.06272v1","category":"math.NA"}
{"created":"2024-06-10 13:24:18","title":"Image Compression with Isotropic and Anisotropic Shepard Inpainting","abstract":"Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting. Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations. This requires some numerical expertise if efficient implementations are necessary. Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression. They can be implemented efficiently when we localise their weighting function. To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality. Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure. On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results. In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels. Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs. Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios.","sentences":["Inpainting-based codecs store sparse selected pixel data and decode by reconstructing the discarded image parts by inpainting.","Successful codecs (coders and decoders) traditionally use inpainting operators that solve partial differential equations.","This requires some numerical expertise if efficient implementations are necessary.","Our goal is to investigate variants of Shepard inpainting as simple alternatives for inpainting-based compression.","They can be implemented efficiently when we localise their weighting function.","To turn them into viable codecs, we have to introduce novel extensions of classical Shepard interpolation that adapt successful ideas from previous codecs: Anisotropy allows direction-dependent inpainting, which improves reconstruction quality.","Additionally, we incorporate data selection by subdivision as an efficient way to tailor the stored information to the image structure.","On the encoding side, we introduce the novel concept of joint inpainting and prediction for isotropic Shepard codecs, where storage cost can be reduced based on intermediate inpainting results.","In an ablation study, we show the usefulness of these individual contributions and demonstrate that they offer synergies which elevate the performance of Shepard inpainting to surprising levels.","Our resulting approaches offer a more favourable trade-off between simplicity and quality than traditional inpainting-based codecs.","Experiments show that they can outperform JPEG and JPEG2000 at high compression ratios."],"url":"http://arxiv.org/abs/2406.06247v1","category":"eess.IV"}
{"created":"2024-06-10 13:20:46","title":"Lower eigenvalue bounds with hybrid high-order methods","abstract":"This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds. These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms. The involved constants arise from local embeddings and are available for all polynomial degrees. Applications include the linear elasticity and Steklov eigenvalue problem.","sentences":["This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds.","These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms.","The involved constants arise from local embeddings and are available for all polynomial degrees.","Applications include the linear elasticity and Steklov eigenvalue problem."],"url":"http://arxiv.org/abs/2406.06244v1","category":"math.NA"}
{"created":"2024-06-10 11:09:11","title":"Non-equilibrium cotunneling in interacting Josephson junctions","abstract":"We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach. The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling. With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy. We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current. In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions. The latter signal the interplay between strong interactions and tunneling at finite bias.","sentences":["We investigate non-equilibrium transport through interacting superconducting nanojunctions using a Liouville space approach.","The formalism allows us to study finite gap effects, and to account for both quasiparticle and Cooper pair tunneling.","With focus on the weak tunneling limit, we study the stationary dc and ac current up to second order (cotunneling) in the hybridization energy.","We identify the characteristic virtual processes that yield the Andreev and Josephson current and obtain the dependence on the gate and bias voltage for the dc current, the critical current and the phase-dependent dissipative current.","In particular, the critical current is characterized by regions in the stability diagram in which its sign changes from positive to negative, resulting in a multitude of 0-\\pi transitions.","The latter signal the interplay between strong interactions and tunneling at finite bias."],"url":"http://arxiv.org/abs/2406.06170v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 10:53:23","title":"Extending Segment Anything Model into Auditory and Temporal Dimensions for Audio-Visual Segmentation","abstract":"Audio-visual segmentation (AVS) aims to segment sound sources in the video sequence, requiring a pixel-level understanding of audio-visual correspondence. As the Segment Anything Model (SAM) has strongly impacted extensive fields of dense prediction problems, prior works have investigated the introduction of SAM into AVS with audio as a new modality of the prompt. Nevertheless, constrained by SAM's single-frame segmentation scheme, the temporal context across multiple frames of audio-visual data remains insufficiently utilized. To this end, we study the extension of SAM's capabilities to the sequence of audio-visual scenes by analyzing contextual cross-modal relationships across the frames. To achieve this, we propose a Spatio-Temporal, Bidirectional Audio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's image encoder and mask decoder. It adaptively updates the audio-visual features to convey the spatio-temporal correspondence between the video frames and audio streams. Extensive experiments demonstrate that our proposed model outperforms the state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU gain on a challenging multi-sources subset.","sentences":["Audio-visual segmentation (AVS) aims to segment sound sources in the video sequence, requiring a pixel-level understanding of audio-visual correspondence.","As the Segment Anything Model (SAM) has strongly impacted extensive fields of dense prediction problems, prior works have investigated the introduction of SAM into AVS with audio as a new modality of the prompt.","Nevertheless, constrained by SAM's single-frame segmentation scheme, the temporal context across multiple frames of audio-visual data remains insufficiently utilized.","To this end, we study the extension of SAM's capabilities to the sequence of audio-visual scenes by analyzing contextual cross-modal relationships across the frames.","To achieve this, we propose a Spatio-Temporal, Bidirectional Audio-Visual Attention (ST-BAVA) module integrated into the middle of SAM's image encoder and mask decoder.","It adaptively updates the audio-visual features to convey the spatio-temporal correspondence between the video frames and audio streams.","Extensive experiments demonstrate that our proposed model outperforms the state-of-the-art methods on AVS benchmarks, especially with an 8.3% mIoU gain on a challenging multi-sources subset."],"url":"http://arxiv.org/abs/2406.06163v1","category":"cs.CV"}
{"created":"2024-06-10 07:36:55","title":"Efficient k-Nearest-Neighbor Machine Translation with Dynamic Retrieval","abstract":"To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine Translation ($k$NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a $k$NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at each timestep leads to substantial time overhead. To address this issue, dominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR), which dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is less than a fixed threshold. Unfortunately, $k$NN-MT-AR does not yield satisfactory results. In this paper, we first conduct a preliminary study to reveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for $k$NN retrieval at different timesteps. To mitigate these limitations, we then propose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly extends vanilla $k$NN-MT in two aspects. Firstly, we equip $k$NN-MT with a MLP-based classifier for determining whether to skip $k$NN retrieval at each timestep. Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier. Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model. Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model.\\footnote{Our code is available at \\url{https://github.com/DeepLearnXMU/knn-mt-dr}.","sentences":["To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine Translation ($k$NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a $k$NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at each timestep leads to substantial time overhead.","To address this issue, dominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR), which dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is less than a fixed threshold.","Unfortunately, $k$NN-MT-AR does not yield satisfactory results.","In this paper, we first conduct a preliminary study to reveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for $k$NN retrieval at different timesteps.","To mitigate these limitations, we then propose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly extends vanilla $k$NN-MT in two aspects.","Firstly, we equip $k$NN-MT with a MLP-based classifier for determining whether to skip $k$NN retrieval at each timestep.","Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier.","Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model.","Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model.\\footnote{Our code is available at \\url{https://github.com/DeepLearnXMU/knn-mt-dr}."],"url":"http://arxiv.org/abs/2406.06073v1","category":"cs.CL"}
{"created":"2024-06-10 07:36:24","title":"Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control","abstract":"Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases.","sentences":["Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias.","However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases.","Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer.","To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control.","We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases."],"url":"http://arxiv.org/abs/2406.06072v1","category":"cs.CV"}
{"created":"2024-06-10 07:21:23","title":"Enabling Large-Scale and High-Precision Fluid Simulations on Near-Term Quantum Computers","abstract":"Quantum computational fluid dynamics (QCFD) offers a promising alternative to classical computational fluid dynamics (CFD) by leveraging quantum algorithms for higher efficiency. This paper introduces a comprehensive QCFD method implemented on a superconducting quantum computer, demonstrating successful simulations of steady Poiseuille flow and unsteady acoustic wave propagation. The Poiseuille flow simulation achieved a relative error of less than $0.2\\%$, and the unsteady acoustic wave simulation solved a 5043-dimension matrix, marking the largest simulation on a quantum computer to date. Our approach bridges quantum and classical computing, adapting to quantum hardware constraints and offering scalable solutions for large-scale CFD problems, which paves the way for practical applications of near-term quantum computers in computational science.","sentences":["Quantum computational fluid dynamics (QCFD) offers a promising alternative to classical computational fluid dynamics (CFD) by leveraging quantum algorithms for higher efficiency.","This paper introduces a comprehensive QCFD method implemented on a superconducting quantum computer, demonstrating successful simulations of steady Poiseuille flow and unsteady acoustic wave propagation.","The Poiseuille flow simulation achieved a relative error of less than $0.2\\%$, and the unsteady acoustic wave simulation solved a 5043-dimension matrix, marking the largest simulation on a quantum computer to date.","Our approach bridges quantum and classical computing, adapting to quantum hardware constraints and offering scalable solutions for large-scale CFD problems, which paves the way for practical applications of near-term quantum computers in computational science."],"url":"http://arxiv.org/abs/2406.06063v1","category":"physics.comp-ph"}
{"created":"2024-06-10 06:38:11","title":"Generalizable Human Gaussians from Single-View Image","abstract":"In this work, we tackle the task of learning generalizable 3D human Gaussians from a single image. The main challenge for this task is to recover detailed geometry and appearance, especially for the unobserved regions. To this end, we propose single-view generalizable Human Gaussian model (HGM), a diffusion-guided framework for 3D human modeling from a single image. We design a diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted to refine novel-view images rendered from a coarse human Gaussian model. The refined images are then used together with the input image to learn a refined human Gaussian model. Although effective in hallucinating the unobserved views, the approach may generate unrealistic human pose and shapes due to the lack of supervision. We circumvent this problem by further encoding the geometric priors from SMPL model. Specifically, we propagate geometric features from SMPL volume to the predicted Gaussians via sparse convolution and attention mechanism. We validate our approach on publicly available datasets and demonstrate that it significantly surpasses state-of-the-art methods in terms of PSNR and SSIM. Additionally, our method exhibits strong generalization for in-the-wild images.","sentences":["In this work, we tackle the task of learning generalizable 3D human Gaussians from a single image.","The main challenge for this task is to recover detailed geometry and appearance, especially for the unobserved regions.","To this end, we propose single-view generalizable Human Gaussian model (HGM), a diffusion-guided framework for 3D human modeling from a single image.","We design a diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted to refine novel-view images rendered from a coarse human Gaussian model.","The refined images are then used together with the input image to learn a refined human Gaussian model.","Although effective in hallucinating the unobserved views, the approach may generate unrealistic human pose and shapes due to the lack of supervision.","We circumvent this problem by further encoding the geometric priors from SMPL model.","Specifically, we propagate geometric features from SMPL volume to the predicted Gaussians via sparse convolution and attention mechanism.","We validate our approach on publicly available datasets and demonstrate that it significantly surpasses state-of-the-art methods in terms of PSNR and SSIM.","Additionally, our method exhibits strong generalization for in-the-wild images."],"url":"http://arxiv.org/abs/2406.06050v1","category":"cs.CV"}
{"created":"2024-06-10 03:25:49","title":"Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning","abstract":"Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment. Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies. In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL. We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance. Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL.","sentences":["Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment.","Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies.","In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL.","We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance.","Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL."],"url":"http://arxiv.org/abs/2406.05993v1","category":"cs.LG"}
{"created":"2024-06-09 23:57:47","title":"Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs","abstract":"Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective precondition.   Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. Despite notable empirical observations, theoretical foundations are still lacking. In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings. We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution. Our theory is validated by numerical results.","sentences":["Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming.","Many applications require real-time/fast solutions, though not necessarily with high precision.","Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method.","For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective precondition.   ","Recently, graph neural networks (GNNs) opened new possibilities for QP.","Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution.","Despite notable empirical observations, theoretical foundations are still lacking.","In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings.","We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution.","Our theory is validated by numerical results."],"url":"http://arxiv.org/abs/2406.05938v1","category":"cs.LG"}
{"created":"2024-06-09 20:42:02","title":"A primer on the analogue black hole bomb with capillary-gravity waves","abstract":"Draining vortices with a free surface are frequently employed as rotating black hole simulators, both in theory and experiments. However, most theoretical work is restricted to the idealised regime, where wave dispersion and dissipation are neglected. We investigate the role of these effects on the analogue black hole bomb, an instability resulting from rotational superradiant amplification in confined systems. We reveal that the dispersion of deep water capillary-gravity waves significantly modifies the unstable mode eigenfrequencies, whereas viscosity only affects those with high frequencies. Furthermore, if the circulation is less than an order 1 multiple of the drain rate, superradiance does not occur and the vortex is stable. The instability is maximised in small systems with high flow velocities, provided there is sufficient space between the vortex and the outer boundary for the first excited state to lie inside the superradiant bandwidth. Implications for experiments on analogue black holes and free surface vortices are discussed.","sentences":["Draining vortices with a free surface are frequently employed as rotating black hole simulators, both in theory and experiments.","However, most theoretical work is restricted to the idealised regime, where wave dispersion and dissipation are neglected.","We investigate the role of these effects on the analogue black hole bomb, an instability resulting from rotational superradiant amplification in confined systems.","We reveal that the dispersion of deep water capillary-gravity waves significantly modifies the unstable mode eigenfrequencies, whereas viscosity only affects those with high frequencies.","Furthermore, if the circulation is less than an order 1 multiple of the drain rate, superradiance does not occur and the vortex is stable.","The instability is maximised in small systems with high flow velocities, provided there is sufficient space between the vortex and the outer boundary for the first excited state to lie inside the superradiant bandwidth.","Implications for experiments on analogue black holes and free surface vortices are discussed."],"url":"http://arxiv.org/abs/2406.05910v1","category":"gr-qc"}
{"created":"2024-06-09 19:14:44","title":"Structure and energy transfer in homogeneous turbulence below a free surface","abstract":"We investigate the turbulence below a quasi-flat free surface, focusing on the energy transport in space and across scales. We leverage a large zero-mean-flow tank where homogeneous turbulence is generated by randomly actuated jets. A wide range of Reynolds number is spanned, reaching sufficient scale separation for the emergence of an inertial sub-range. Unlike previous studies, the forcing extends through the source layer, though the surface deformation remains millimetric. Particle image velocimetry along a surface-normal plane resolves from the dissipative to the integral scales. Both vertical and horizontal components of the turbulent kinetic energy approach the prediction based on rapid distortion theory as the Reynolds number is increased, indicating that discrepancies among previous studies are likely due to differences in the forcing. At odds with the theory, however, the integral scale of the horizontal fluctuations grows as the surface is approached. This is rooted in the profound influence exerted by the surface on the inter-scale energy transfer: along horizontal separations, the direct cascade of horizontal energy is hindered, while an inverse cascade of vertical energy is established. This is connected to the structure of upwellings and downwellings. The former, characterized by somewhat larger spatial extent and stronger intensity, are associated to extensional surface-parallel motions. They thus transfer energy to the larger horizontal scales, prevailing over downwellings which favour the compression (and concurrent vertical stretching) of the eddies. Both types of structures extend to depths between the integral and Taylor microscales.","sentences":["We investigate the turbulence below a quasi-flat free surface, focusing on the energy transport in space and across scales.","We leverage a large zero-mean-flow tank where homogeneous turbulence is generated by randomly actuated jets.","A wide range of Reynolds number is spanned, reaching sufficient scale separation for the emergence of an inertial sub-range.","Unlike previous studies, the forcing extends through the source layer, though the surface deformation remains millimetric.","Particle image velocimetry along a surface-normal plane resolves from the dissipative to the integral scales.","Both vertical and horizontal components of the turbulent kinetic energy approach the prediction based on rapid distortion theory as the Reynolds number is increased, indicating that discrepancies among previous studies are likely due to differences in the forcing.","At odds with the theory, however, the integral scale of the horizontal fluctuations grows as the surface is approached.","This is rooted in the profound influence exerted by the surface on the inter-scale energy transfer: along horizontal separations, the direct cascade of horizontal energy is hindered, while an inverse cascade of vertical energy is established.","This is connected to the structure of upwellings and downwellings.","The former, characterized by somewhat larger spatial extent and stronger intensity, are associated to extensional surface-parallel motions.","They thus transfer energy to the larger horizontal scales, prevailing over downwellings which favour the compression (and concurrent vertical stretching) of the eddies.","Both types of structures extend to depths between the integral and Taylor microscales."],"url":"http://arxiv.org/abs/2406.05889v1","category":"physics.flu-dyn"}
{"created":"2024-06-09 17:27:20","title":"Source -Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels","abstract":"Domain adaptation is often hampered by exceedingly small target datasets and inaccessible source data. These conditions are prevalent in speech verification, where privacy policies and/or languages with scarce speech resources limit the availability of sufficient data. This paper explored techniques of sourcefree domain adaptation unto a limited target speech dataset for speaker verificationin data-scarce languages. Both language and channel mis-match between source and target were investigated. Fine-tuning methods were evaluated and compared across different sizes of labeled target data. A novel iterative cluster-learn algorithm was studied for unlabeled target datasets.","sentences":["Domain adaptation is often hampered by exceedingly small target datasets and inaccessible source data.","These conditions are prevalent in speech verification, where privacy policies and/or languages with scarce speech resources limit the availability of sufficient data.","This paper explored techniques of sourcefree domain adaptation unto a limited target speech dataset for speaker verificationin data-scarce languages.","Both language and channel mis-match between source and target were investigated.","Fine-tuning methods were evaluated and compared across different sizes of labeled target data.","A novel iterative cluster-learn algorithm was studied for unlabeled target datasets."],"url":"http://arxiv.org/abs/2406.05863v1","category":"cs.SD"}
{"created":"2024-06-09 16:53:48","title":"Non-uniqueness of weak solutions for a logarithmically supercritical hyperdissipative Navier-Stokes system","abstract":"The existence of non-unique solutions of finite kinetic energy for the three dimensional Navier-Stokes equations is proved in the slightly supercritical hyper-dissipative setting introduced by Tao. The result is based on the convex integration techniques of Buckmaster and Vicol and extends Luo and Titi result in the slightly supercritical setting. To be able to be closer to the threshold identified by Tao, we introduce the impulsed Beltrami flows, a variant of the intermittent Beltrami flows of Buckmaster and Vicol.","sentences":["The existence of non-unique solutions of finite kinetic energy for the three dimensional Navier-Stokes equations is proved in the slightly supercritical hyper-dissipative setting introduced by Tao.","The result is based on the convex integration techniques of Buckmaster and Vicol and extends Luo and Titi result in the slightly supercritical setting.","To be able to be closer to the threshold identified by Tao, we introduce the impulsed Beltrami flows, a variant of the intermittent Beltrami flows of Buckmaster and Vicol."],"url":"http://arxiv.org/abs/2406.05853v1","category":"math.AP"}
{"created":"2024-06-09 16:48:27","title":"MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps","abstract":"Creating 3D semantic reconstructions of environments is fundamental to many applications, especially when related to autonomous agent operation (e.g., goal-oriented navigation or object interaction and manipulation). Commonly, 3D semantic reconstruction systems capture the entire scene in the same level of detail. However, certain tasks (e.g., object interaction) require a fine-grained and high-resolution map, particularly if the objects to interact are of small size or intricate geometry. In recent practice, this leads to the entire map being in the same high-quality resolution, which results in increased computational and storage costs. To address this challenge, we propose MAP-ADAPT, a real-time method for quality-adaptive semantic 3D reconstruction using RGBD frames. MAP-ADAPT is the first adaptive semantic 3D mapping algorithm that, unlike prior work, generates directly a single map with regions of different quality based on both the semantic information and the geometric complexity of the scene. Leveraging a semantic SLAM pipeline for pose and semantic estimation, we achieve comparable or superior results to state-of-the-art methods on synthetic and real-world data, while significantly reducing storage and computation requirements.","sentences":["Creating 3D semantic reconstructions of environments is fundamental to many applications, especially when related to autonomous agent operation (e.g., goal-oriented navigation or object interaction and manipulation).","Commonly, 3D semantic reconstruction systems capture the entire scene in the same level of detail.","However, certain tasks (e.g., object interaction) require a fine-grained and high-resolution map, particularly if the objects to interact are of small size or intricate geometry.","In recent practice, this leads to the entire map being in the same high-quality resolution, which results in increased computational and storage costs.","To address this challenge, we propose MAP-ADAPT, a real-time method for quality-adaptive semantic 3D reconstruction using RGBD frames.","MAP-ADAPT is the first adaptive semantic 3D mapping algorithm that, unlike prior work, generates directly a single map with regions of different quality based on both the semantic information and the geometric complexity of the scene.","Leveraging a semantic SLAM pipeline for pose and semantic estimation, we achieve comparable or superior results to state-of-the-art methods on synthetic and real-world data, while significantly reducing storage and computation requirements."],"url":"http://arxiv.org/abs/2406.05849v1","category":"cs.RO"}
{"created":"2024-06-09 15:56:19","title":"Mamba YOLO: SSMs-Based YOLO For Object Detection","abstract":"Propelled by the rapid advancement of deep learning technologies, the YOLO series has set a new benchmark for real-time object detectors. Researchers have continuously explored innovative applications of reparameterization, efficient layer aggregation networks, and anchor-free techniques on the foundation of YOLO. To further enhance detection performance, Transformer-based structures have been introduced, significantly expanding the model's receptive field and achieving notable performance gains. However, such improvements come at a cost, as the quadratic complexity of the self-attention mechanism increases the computational burden of the model. Fortunately, the emergence of State Space Models (SSM) as an innovative technology has effectively mitigated the issues caused by quadratic complexity. In light of these advancements, we introduce Mamba-YOLO a novel object detection model based on SSM. Mamba-YOLO not only optimizes the SSM foundation but also adapts specifically for object detection tasks. Given the potential limitations of SSM in sequence modeling, such as insufficient receptive field and weak image locality, we have designed the LSBlock and RGBlock. These modules enable more precise capture of local image dependencies and significantly enhance the robustness of the model. Extensive experimental results on the publicly available benchmark datasets COCO and VOC demonstrate that Mamba-YOLO surpasses the existing YOLO series models in both performance and competitiveness, showcasing its substantial potential and competitive edge.The PyTorch code is available at:\\url{https://github.com/HZAI-ZJNU/Mamba-YOLO}","sentences":["Propelled by the rapid advancement of deep learning technologies, the YOLO series has set a new benchmark for real-time object detectors.","Researchers have continuously explored innovative applications of reparameterization, efficient layer aggregation networks, and anchor-free techniques on the foundation of YOLO.","To further enhance detection performance, Transformer-based structures have been introduced, significantly expanding the model's receptive field and achieving notable performance gains.","However, such improvements come at a cost, as the quadratic complexity of the self-attention mechanism increases the computational burden of the model.","Fortunately, the emergence of State Space Models (SSM) as an innovative technology has effectively mitigated the issues caused by quadratic complexity.","In light of these advancements, we introduce Mamba-YOLO a novel object detection model based on SSM.","Mamba-YOLO not only optimizes the SSM foundation but also adapts specifically for object detection tasks.","Given the potential limitations of SSM in sequence modeling, such as insufficient receptive field and weak image locality, we have designed the LSBlock and RGBlock.","These modules enable more precise capture of local image dependencies and significantly enhance the robustness of the model.","Extensive experimental results on the publicly available benchmark datasets COCO and VOC demonstrate that Mamba-YOLO surpasses the existing YOLO series models in both performance and competitiveness, showcasing its substantial potential and competitive edge.","The PyTorch code is available at:\\url{https://github.com/HZAI-ZJNU/Mamba-YOLO}"],"url":"http://arxiv.org/abs/2406.05835v1","category":"cs.CV"}
{"created":"2024-06-09 12:39:37","title":"A detailed survey of the parallel mean free path of solar energetic particle protons and electrons","abstract":"In this work, more than a dozen solar energetic particle (SEP) events are identified where the source region is magnetically well-connected to at least one spacecraft at 1~au. The observed intensity-time profiles, for all available proton and electron energy channels, are compared to results computed using a numerical 1D SEP transport model in order to derive the parallel mean free paths (pMFPs) as a function of energy (or rigidity) at 1~au. These inversion results are then compared to theoretical estimates of the pMFP, using observed turbulence quantities with observationally-motivated variations as input. For protons, a very good comparison between inversion and theoretical results is obtained. It is shown that the observed inter-event variations in the inversion pMFP values can be explained by natural variations in the background turbulence values. For electrons, there is relatively good agreement with pMFPs derived assuming the damping model of dynamical turbulence, although the theoretical values are extremely sensitive to the details of the turbulence dissipation range which themselves display a high level of variation.","sentences":["In this work, more than a dozen solar energetic particle (SEP) events are identified where the source region is magnetically well-connected to at least one spacecraft at 1~au.","The observed intensity-time profiles, for all available proton and electron energy channels, are compared to results computed using a numerical 1D SEP transport model in order to derive the parallel mean free paths (pMFPs) as a function of energy (or rigidity) at 1~au.","These inversion results are then compared to theoretical estimates of the pMFP, using observed turbulence quantities with observationally-motivated variations as input.","For protons, a very good comparison between inversion and theoretical results is obtained.","It is shown that the observed inter-event variations in the inversion pMFP values can be explained by natural variations in the background turbulence values.","For electrons, there is relatively good agreement with pMFPs derived assuming the damping model of dynamical turbulence, although the theoretical values are extremely sensitive to the details of the turbulence dissipation range which themselves display a high level of variation."],"url":"http://arxiv.org/abs/2406.05765v1","category":"astro-ph.SR"}
{"created":"2024-06-09 11:32:47","title":"Direct observations of cross-scale energy transfer in space plasmas","abstract":"The collisionless plasmas in space and astrophysical environments are intrinsically multiscale in nature, behaving as conducting fluids at macroscales and kinetically at microscales comparable to ion- and/or electron-gyroradii. A fundamental question in understanding the plasma dynamics is how energy is transported and dissipated across different scales. Here, we present spacecraft measurements in the solar wind upstream of the terrestrial bow shock, in which the macroscale ultra-low-frequency waves and microscale whistler waves simultaneously resonate with the ions. The ion acceleration from ultra-low-frequency waves leads to velocity distributions unstable to the growth of whistler waves, which in turn resonate with the electrons to complete cross-scale energy transfer. These observations, consistent with numerical simulations in the occurrence of phase-bunched ion and electron distributions, also highlight the importance of anomalous resonance, a nonlinear modification of the classical cyclotron resonance, in the cross-scale wave coupling and energy transfer processes.","sentences":["The collisionless plasmas in space and astrophysical environments are intrinsically multiscale in nature, behaving as conducting fluids at macroscales and kinetically at microscales comparable to ion- and/or electron-gyroradii.","A fundamental question in understanding the plasma dynamics is how energy is transported and dissipated across different scales.","Here, we present spacecraft measurements in the solar wind upstream of the terrestrial bow shock, in which the macroscale ultra-low-frequency waves and microscale whistler waves simultaneously resonate with the ions.","The ion acceleration from ultra-low-frequency waves leads to velocity distributions unstable to the growth of whistler waves, which in turn resonate with the electrons to complete cross-scale energy transfer.","These observations, consistent with numerical simulations in the occurrence of phase-bunched ion and electron distributions, also highlight the importance of anomalous resonance, a nonlinear modification of the classical cyclotron resonance, in the cross-scale wave coupling and energy transfer processes."],"url":"http://arxiv.org/abs/2406.05744v1","category":"physics.space-ph"}
{"created":"2024-06-09 10:36:27","title":"A Variational Approach to Learning Photonic Unitary Operators","abstract":"Structured light, light tailored in its internal degrees of freedom, has become topical in numerous quantum and classical information processing protocols. In this work, we harness the high dimensional nature of structured light modulated in the transverse spatial degree of freedom to realise an adaptable scheme for learning unitary operations. Our approach borrows from concepts in variational quantum computing, where a search or optimisation problem is mapped onto the task of finding a minimum ground state energy for a given energy/goal function. We achieve this by a pseudo-random walk procedure over the parameter space of the unitary operation, implemented with optical matrix-vector multiplication enacted on arrays of Gaussian modes by exploiting the partial Fourier transforming capabilities of a cylindrical lens in the transverse degree of freedom for the measurement. We outline the concept theoretically, and experimentally demonstrate that we are able to learn optical unitary matrices for dimensions d = 2, 4, 8 and 16 with average fidelities of >90%. Our work advances high dimensional information processing and can be adapted to both process and quantum state tomography of unknown states and channels.","sentences":["Structured light, light tailored in its internal degrees of freedom, has become topical in numerous quantum and classical information processing protocols.","In this work, we harness the high dimensional nature of structured light modulated in the transverse spatial degree of freedom to realise an adaptable scheme for learning unitary operations.","Our approach borrows from concepts in variational quantum computing, where a search or optimisation problem is mapped onto the task of finding a minimum ground state energy for a given energy/goal function.","We achieve this by a pseudo-random walk procedure over the parameter space of the unitary operation, implemented with optical matrix-vector multiplication enacted on arrays of Gaussian modes by exploiting the partial Fourier transforming capabilities of a cylindrical lens in the transverse degree of freedom for the measurement.","We outline the concept theoretically, and experimentally demonstrate that we are able to learn optical unitary matrices for dimensions d = 2, 4, 8 and 16 with average fidelities of >90%.","Our work advances high dimensional information processing and can be adapted to both process and quantum state tomography of unknown states and channels."],"url":"http://arxiv.org/abs/2406.05727v1","category":"physics.optics"}
{"created":"2024-06-09 04:25:10","title":"A Generalized Version of Chung's Lemma and its Applications","abstract":"Chung's lemma is a classical tool for establishing asymptotic convergence rates of (stochastic) optimization methods under strong convexity-type assumptions and appropriate polynomial diminishing step sizes. In this work, we develop a generalized version of Chung's lemma, which provides a simple non-asymptotic convergence framework for a more general family of step size rules. We demonstrate broad applicability of the proposed generalized Chung's lemma by deriving tight non-asymptotic convergence rates for a large variety of stochastic methods. In particular, we obtain partially new non-asymptotic complexity results for stochastic optimization methods, such as stochastic gradient descent and random reshuffling, under a general $(\\theta,\\mu)$-Polyak-Lojasiewicz (PL) condition and for various step sizes strategies, including polynomial, constant, exponential, and cosine step sizes rules. Notably, as a by-product of our analysis, we observe that exponential step sizes can adapt to the objective function's geometry, achieving the optimal convergence rate without requiring exact knowledge of the underlying landscape. Our results demonstrate that the developed variant of Chung's lemma offers a versatile, systematic, and streamlined approach to establish non-asymptotic convergence rates under general step size rules.","sentences":["Chung's lemma is a classical tool for establishing asymptotic convergence rates of (stochastic) optimization methods under strong convexity-type assumptions and appropriate polynomial diminishing step sizes.","In this work, we develop a generalized version of Chung's lemma, which provides a simple non-asymptotic convergence framework for a more general family of step size rules.","We demonstrate broad applicability of the proposed generalized Chung's lemma by deriving tight non-asymptotic convergence rates for a large variety of stochastic methods.","In particular, we obtain partially new non-asymptotic complexity results for stochastic optimization methods, such as stochastic gradient descent and random reshuffling, under a general $(\\theta,\\mu)$-Polyak-Lojasiewicz (PL) condition and for various step sizes strategies, including polynomial, constant, exponential, and cosine step sizes rules.","Notably, as a by-product of our analysis, we observe that exponential step sizes can adapt to the objective function's geometry, achieving the optimal convergence rate without requiring exact knowledge of the underlying landscape.","Our results demonstrate that the developed variant of Chung's lemma offers a versatile, systematic, and streamlined approach to establish non-asymptotic convergence rates under general step size rules."],"url":"http://arxiv.org/abs/2406.05637v1","category":"math.OC"}
{"created":"2024-06-09 01:27:57","title":"HAL-based Plugin Estimation of the Causal Dose-Response Curve","abstract":"Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn't pathwise differentiable, and then there is no $\\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.","sentences":["Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields.","In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve.","In contrast, non-parametric models face difficulties as the dose-response curve isn't pathwise differentiable, and then there is no $\\sqrt{n}$-consistent estimator.","The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem.","This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator.","We assess these estimators through extensive simulations, employing detailed evaluation metrics.","Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve."],"url":"http://arxiv.org/abs/2406.05607v1","category":"stat.ME"}
{"created":"2024-06-09 01:16:04","title":"GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?","abstract":"In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks. To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval. Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models.","sentences":["In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated.","This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks.","To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge.","Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated.","Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval.","Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models."],"url":"http://arxiv.org/abs/2406.05606v1","category":"cs.CL"}
{"created":"2024-06-08 23:53:48","title":"Reliable Quantum Memories with Unreliable Components","abstract":"Quantum memory systems are vital in quantum information processing for dependable storage and retrieval of quantum states. Inspired by classical reliability theories that synthesize reliable computing systems from unreliable components, we formalize the problem of reliable storage of quantum information using noisy components. We introduce the notion of stable quantum memories and define the storage rate as the ratio of the number of logical qubits to the total number of physical qubits, as well as the circuit complexity of the decoder, which includes both quantum gates and measurements. We demonstrate that a strictly positive storage rate can be achieved by constructing a quantum memory system with quantum expander codes. Moreover, by reducing the reliable storage problem to reliable quantum communication, we provide upper bounds on the achievable storage capacity. In the case of physical qubits corrupted by noise satisfying hypercontractivity conditions, we provide a tighter upper bound on storage capacity using an entropy dissipation argument. Furthermore, observing that the time complexity of the decoder scales non-trivially with the number of physical qubits, achieving asymptotic rates may not be possible due to the induced dependence of the noise on the number of physical qubits. In this constrained non-asymptotic setting, we derive upper bounds on storage capacity using finite blocklength communication bounds. Finally, we numerically analyze the gap between upper and lower bounds in both asymptotic and non-asymptotic cases, and provide suggestions to tighten the gap.","sentences":["Quantum memory systems are vital in quantum information processing for dependable storage and retrieval of quantum states.","Inspired by classical reliability theories that synthesize reliable computing systems from unreliable components, we formalize the problem of reliable storage of quantum information using noisy components.","We introduce the notion of stable quantum memories and define the storage rate as the ratio of the number of logical qubits to the total number of physical qubits, as well as the circuit complexity of the decoder, which includes both quantum gates and measurements.","We demonstrate that a strictly positive storage rate can be achieved by constructing a quantum memory system with quantum expander codes.","Moreover, by reducing the reliable storage problem to reliable quantum communication, we provide upper bounds on the achievable storage capacity.","In the case of physical qubits corrupted by noise satisfying hypercontractivity conditions, we provide a tighter upper bound on storage capacity using an entropy dissipation argument.","Furthermore, observing that the time complexity of the decoder scales non-trivially with the number of physical qubits, achieving asymptotic rates may not be possible due to the induced dependence of the noise on the number of physical qubits.","In this constrained non-asymptotic setting, we derive upper bounds on storage capacity using finite blocklength communication bounds.","Finally, we numerically analyze the gap between upper and lower bounds in both asymptotic and non-asymptotic cases, and provide suggestions to tighten the gap."],"url":"http://arxiv.org/abs/2406.05599v1","category":"quant-ph"}
{"created":"2024-06-08 21:32:04","title":"Adaptive Output Tracking Control with Reference Model System Uncertainties","abstract":"This paper develops adaptive output tracking control schemes with the reference output signal generated from an unknown reference system whose output derivatives are also unknown. To deal with such reference system uncertainties, an expanded adaptive controller structure is developed to include a parametrized estimator of the equivalent reference input signal. Without using the knowledge of the reference system transfer function and equivalent input, both are the critical components of a traditional model reference adaptive control (MRAC) scheme, the developed new MRAC schemes designed for various cases plant and reference model uncertainties, ensure completely parametrized error systems and stable parameter adaptation, leading to the desired closed-loop system stability and asymptotic output tracking.","sentences":["This paper develops adaptive output tracking control schemes with the reference output signal generated from an unknown reference system whose output derivatives are also unknown.","To deal with such reference system uncertainties, an expanded adaptive controller structure is developed to include a parametrized estimator of the equivalent reference input signal.","Without using the knowledge of the reference system transfer function and equivalent input, both are the critical components of a traditional model reference adaptive control (MRAC) scheme, the developed new MRAC schemes designed for various cases plant and reference model uncertainties, ensure completely parametrized error systems and stable parameter adaptation, leading to the desired closed-loop system stability and asymptotic output tracking."],"url":"http://arxiv.org/abs/2406.05580v1","category":"eess.SY"}
{"created":"2024-06-08 21:25:24","title":"Prioritizing Potential Wetland Areas via Region-to-Region Knowledge Transfer and Adaptive Propagation","abstract":"Wetlands are important to communities, offering benefits ranging from water purification, and flood protection to recreation and tourism. Therefore, identifying and prioritizing potential wetland areas is a critical decision problem. While data-driven solutions are feasible, this is complicated by significant data sparsity due to the low proportion of wetlands (3-6\\%) in many areas of interest in the southwestern US. This makes it hard to develop data-driven models that can help guide the identification of additional wetland areas. To solve this limitation, we propose two strategies: (1) The first of these is knowledge transfer from regions with rich wetlands (such as the Eastern US) to sparser regions (such as the Southwestern area with few wetlands). Recognizing that these regions are likely to be very different from each other in terms of soil characteristics, population distribution, and land use, we propose a domain disentanglement strategy that identifies and transfers only the applicable aspects of the learned model. (2) We complement this with a spatial data enrichment strategy that relies on an adaptive propagation mechanism. This mechanism differentiates between node pairs that have positive and negative impacts on each other for Graph Neural Networks (GNNs). To summarize, given two spatial cells belonging to different regions, we identify domain-specific and domain-shareable features, and, for each region, we rely on adaptive propagation to enrich features with the features of surrounding cells. We conduct rigorous experiments to substantiate our proposed method's effectiveness, robustness, and scalability compared to state-of-the-art baselines. Additionally, an ablation study demonstrates that each module is essential in prioritizing potential wetlands, which justifies our assumption.","sentences":["Wetlands are important to communities, offering benefits ranging from water purification, and flood protection to recreation and tourism.","Therefore, identifying and prioritizing potential wetland areas is a critical decision problem.","While data-driven solutions are feasible, this is complicated by significant data sparsity due to the low proportion of wetlands (3-6\\%) in many areas of interest in the southwestern US.","This makes it hard to develop data-driven models that can help guide the identification of additional wetland areas.","To solve this limitation, we propose two strategies: (1) The first of these is knowledge transfer from regions with rich wetlands (such as the Eastern US) to sparser regions (such as the Southwestern area with few wetlands).","Recognizing that these regions are likely to be very different from each other in terms of soil characteristics, population distribution, and land use, we propose a domain disentanglement strategy that identifies and transfers only the applicable aspects of the learned model.","(2) We complement this with a spatial data enrichment strategy that relies on an adaptive propagation mechanism.","This mechanism differentiates between node pairs that have positive and negative impacts on each other for Graph Neural Networks (GNNs).","To summarize, given two spatial cells belonging to different regions, we identify domain-specific and domain-shareable features, and, for each region, we rely on adaptive propagation to enrich features with the features of surrounding cells.","We conduct rigorous experiments to substantiate our proposed method's effectiveness, robustness, and scalability compared to state-of-the-art baselines.","Additionally, an ablation study demonstrates that each module is essential in prioritizing potential wetlands, which justifies our assumption."],"url":"http://arxiv.org/abs/2406.05578v1","category":"cs.IR"}
{"created":"2024-06-08 20:10:48","title":"Spectral Prescribed Mean Curvature","abstract":"We consider prescribed mean curvature equations whose solutions are minimal surfaces, constant mean curvature surfaces, or capillary surfaces. We consider both Dirichlet boundary conditions for Plateau problems and nonlinear Neumann boundary conditions for capillary problems and we consider domains in $\\mathbf{R}^2$ to be rectangles, disks, or annuli.   We present spectral methods for approximating solutions of the associated boundary value problems. These are either based on Chebyshev or Chebyshev-Fourier methods depending on the geometry of the domain. The non-linearity in the prescribed mean curvature equations is treated with a Newton method. The algorithms are designed to be adaptive; if the prescribed tolerances are not met then the resolution of the solution is increased until the tolerances are achieved. 22","sentences":["We consider prescribed mean curvature equations whose solutions are minimal surfaces, constant mean curvature surfaces, or capillary surfaces.","We consider both Dirichlet boundary conditions for Plateau problems and nonlinear Neumann boundary conditions for capillary problems and we consider domains in $\\mathbf{R}^2$ to be rectangles, disks, or annuli.   ","We present spectral methods for approximating solutions of the associated boundary value problems.","These are either based on Chebyshev or Chebyshev-Fourier methods depending on the geometry of the domain.","The non-linearity in the prescribed mean curvature equations is treated with a Newton method.","The algorithms are designed to be adaptive; if the prescribed tolerances are not met then the resolution of the solution is increased until the tolerances are achieved.","22"],"url":"http://arxiv.org/abs/2406.05566v1","category":"math.NA"}
{"created":"2024-06-08 20:07:39","title":"Medical Vision Generalist: Unifying Medical Imaging Tasks in Context","abstract":"This study presents Medical Vision Generalist (MVG), the first foundation model capable of handling various medical imaging tasks -- such as cross-modal synthesis, image segmentation, denoising, and inpainting -- within a unified image-to-image generation framework. Specifically, MVG employs an in-context generation strategy that standardizes the handling of inputs and outputs as images. By treating these tasks as an image generation process conditioned on prompt image-label pairs and input images, this approach enables a flexible unification of various tasks, even those spanning different modalities and datasets. To capitalize on both local and global context, we design a hybrid method combining masked image modeling with autoregressive training for conditional image generation. This hybrid approach yields the most robust performance across all involved medical imaging tasks. To rigorously evaluate MVG's capabilities, we curated the first comprehensive generalist medical vision benchmark, comprising 13 datasets and spanning four imaging modalities (CT, MRI, X-ray, and micro-ultrasound). Our results consistently establish MVG's superior performance, outperforming existing vision generalists, such as Painter and LVM. Furthermore, MVG exhibits strong scalability, with its performance demonstrably improving when trained on a more diverse set of tasks, and can be effectively adapted to unseen datasets with only minimal task-specific samples. The code is available at \\url{https://github.com/OliverRensu/MVG}.","sentences":["This study presents Medical Vision Generalist (MVG), the first foundation model capable of handling various medical imaging tasks -- such as cross-modal synthesis, image segmentation, denoising, and inpainting -- within a unified image-to-image generation framework.","Specifically, MVG employs an in-context generation strategy that standardizes the handling of inputs and outputs as images.","By treating these tasks as an image generation process conditioned on prompt image-label pairs and input images, this approach enables a flexible unification of various tasks, even those spanning different modalities and datasets.","To capitalize on both local and global context, we design a hybrid method combining masked image modeling with autoregressive training for conditional image generation.","This hybrid approach yields the most robust performance across all involved medical imaging tasks.","To rigorously evaluate MVG's capabilities, we curated the first comprehensive generalist medical vision benchmark, comprising 13 datasets and spanning four imaging modalities (CT, MRI, X-ray, and micro-ultrasound).","Our results consistently establish MVG's superior performance, outperforming existing vision generalists, such as Painter and LVM.","Furthermore, MVG exhibits strong scalability, with its performance demonstrably improving when trained on a more diverse set of tasks, and can be effectively adapted to unseen datasets with only minimal task-specific samples.","The code is available at \\url{https://github.com/OliverRensu/MVG}."],"url":"http://arxiv.org/abs/2406.05565v1","category":"cs.CV"}
{"created":"2024-06-08 15:30:46","title":"Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities","abstract":"Multimodal models are expected to be a critical component to future advances in artificial intelligence. This field is starting to grow rapidly with a surge of new design elements motivated by the success of foundation models in natural language processing (NLP) and vision. It is widely hoped that further extending the foundation models to multiple modalities (e.g., text, image, video, sensor, time series, graph, etc.) will ultimately lead to generalist multimodal models, i.e. one model across different data modalities and tasks. However, there is little research that systematically analyzes recent multimodal models (particularly the ones that work beyond text and vision) with respect to the underling architecture proposed. Therefore, this work provides a fresh perspective on generalist multimodal models (GMMs) via a novel architecture and training configuration specific taxonomy. This includes factors such as Unifiability, Modularity, and Adaptability that are pertinent and essential to the wide adoption and application of GMMs. The review further highlights key challenges and prospects for the field and guide the researchers into the new advancements.","sentences":["Multimodal models are expected to be a critical component to future advances in artificial intelligence.","This field is starting to grow rapidly with a surge of new design elements motivated by the success of foundation models in natural language processing (NLP) and vision.","It is widely hoped that further extending the foundation models to multiple modalities (e.g., text, image, video, sensor, time series, graph, etc.) will ultimately lead to generalist multimodal models, i.e. one model across different data modalities and tasks.","However, there is little research that systematically analyzes recent multimodal models (particularly the ones that work beyond text and vision) with respect to the underling architecture proposed.","Therefore, this work provides a fresh perspective on generalist multimodal models (GMMs) via a novel architecture and training configuration specific taxonomy.","This includes factors such as Unifiability, Modularity, and Adaptability that are pertinent and essential to the wide adoption and application of GMMs.","The review further highlights key challenges and prospects for the field and guide the researchers into the new advancements."],"url":"http://arxiv.org/abs/2406.05496v1","category":"cs.CL"}
{"created":"2024-06-08 14:25:57","title":"Training-Free Robust Interactive Video Object Segmentation","abstract":"Interactive video object segmentation is a crucial video task, having various applications from video editing to data annotating. However, current approaches struggle to accurately segment objects across diverse domains. Recently, Segment Anything Model (SAM) introduces interactive visual prompts and demonstrates impressive performance across different domains. In this paper, we propose a training-free prompt tracking framework for interactive video object segmentation (I-PT), leveraging the powerful generalization of SAM. Although point tracking efficiently captures the pixel-wise information of objects in a video, points tend to be unstable when tracked over a long period, resulting in incorrect segmentation. Towards fast and robust interaction, we jointly adopt sparse points and boxes tracking, filtering out unstable points and capturing object-wise information. To better integrate reference information from multiple interactions, we introduce a cross-round space-time module (CRSTM), which adaptively aggregates mask features from previous rounds and frames, enhancing the segmentation stability. Our framework has demonstrated robust zero-shot video segmentation results on popular VOS datasets with interaction types, including DAVIS 2017, YouTube-VOS 2018, and MOSE 2023, maintaining a good tradeoff between performance and interaction time.","sentences":["Interactive video object segmentation is a crucial video task, having various applications from video editing to data annotating.","However, current approaches struggle to accurately segment objects across diverse domains.","Recently, Segment Anything Model (SAM) introduces interactive visual prompts and demonstrates impressive performance across different domains.","In this paper, we propose a training-free prompt tracking framework for interactive video object segmentation (I-PT), leveraging the powerful generalization of SAM.","Although point tracking efficiently captures the pixel-wise information of objects in a video, points tend to be unstable when tracked over a long period, resulting in incorrect segmentation.","Towards fast and robust interaction, we jointly adopt sparse points and boxes tracking, filtering out unstable points and capturing object-wise information.","To better integrate reference information from multiple interactions, we introduce a cross-round space-time module (CRSTM), which adaptively aggregates mask features from previous rounds and frames, enhancing the segmentation stability.","Our framework has demonstrated robust zero-shot video segmentation results on popular VOS datasets with interaction types, including DAVIS 2017, YouTube-VOS 2018, and MOSE 2023, maintaining a good tradeoff between performance and interaction time."],"url":"http://arxiv.org/abs/2406.05485v1","category":"cs.CV"}
{"created":"2024-06-08 14:09:14","title":"Joint Cooperative Clustering and Power Control for Energy-Efficient Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning","abstract":"In this paper, we investigate the amalgamation of cell-free (CF) and extremely large-scale multiple-input multiple-output (XL-MIMO) technologies, referred to as a CF XL-MIMO, as a promising advancement for enabling future mobile networks. To address the computational complexity and communication power consumption associated with conventional centralized optimization, we focus on user-centric dynamic networks in which each user is served by an adaptive subset of access points (AP) rather than all of them. We begin our research by analyzing a joint resource allocation problem for energy-efficient CF XL-MIMO systems, encompassing cooperative clustering and power control design, where all clusters are adaptively adjustable. Then, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme, which offers an effective strategy to tackle the challenges of high-dimensional signal processing. In the section of numerical results, we compare various algorithms with different network architectures. These comparisons reveal that the proposed MARL-based cooperative architecture can effectively strike a balance between system performance and communication overhead, thereby improving energy efficiency performance. It is important to note that increasing the number of user equipments participating in information sharing can effectively enhance SE performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the number of participants and EE performance.","sentences":["In this paper, we investigate the amalgamation of cell-free (CF) and extremely large-scale multiple-input multiple-output (XL-MIMO) technologies, referred to as a CF XL-MIMO, as a promising advancement for enabling future mobile networks.","To address the computational complexity and communication power consumption associated with conventional centralized optimization, we focus on user-centric dynamic networks in which each user is served by an adaptive subset of access points (AP) rather than all of them.","We begin our research by analyzing a joint resource allocation problem for energy-efficient CF XL-MIMO systems, encompassing cooperative clustering and power control design, where all clusters are adaptively adjustable.","Then, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme, which offers an effective strategy to tackle the challenges of high-dimensional signal processing.","In the section of numerical results, we compare various algorithms with different network architectures.","These comparisons reveal that the proposed MARL-based cooperative architecture can effectively strike a balance between system performance and communication overhead, thereby improving energy efficiency performance.","It is important to note that increasing the number of user equipments participating in information sharing can effectively enhance SE performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the number of participants and EE performance."],"url":"http://arxiv.org/abs/2406.05481v1","category":"cs.IT"}
{"created":"2024-06-08 13:28:50","title":"A Novel Generative AI-Based Framework for Anomaly Detection in Multicast Messages in Smart Grid Communications","abstract":"Cybersecurity breaches in digital substations can pose significant challenges to the stability and reliability of power system operations. To address these challenges, defense and mitigation techniques are required. Identifying and detecting anomalies in information and communication technology (ICT) is crucial to ensure secure device interactions within digital substations. This paper proposes a task-oriented dialogue (ToD) system for anomaly detection (AD) in datasets of multicast messages e.g., generic object oriented substation event (GOOSE) and sampled value (SV) in digital substations using large language models (LLMs). This model has a lower potential error and better scalability and adaptability than a process that considers the cybersecurity guidelines recommended by humans, known as the human-in-the-loop (HITL) process. Also, this methodology significantly reduces the effort required when addressing new cyber threats or anomalies compared with machine learning (ML) techniques, since it leaves the models complexity and precision unaffected and offers a faster implementation. These findings present a comparative assessment, conducted utilizing standard and advanced performance evaluation metrics for the proposed AD framework and the HITL process. To generate and extract datasets of IEC 61850 communications, a hardware-in-the-loop (HIL) testbed was employed.","sentences":["Cybersecurity breaches in digital substations can pose significant challenges to the stability and reliability of power system operations.","To address these challenges, defense and mitigation techniques are required.","Identifying and detecting anomalies in information and communication technology (ICT) is crucial to ensure secure device interactions within digital substations.","This paper proposes a task-oriented dialogue (ToD) system for anomaly detection (AD) in datasets of multicast messages e.g., generic object oriented substation event (GOOSE) and sampled value (SV) in digital substations using large language models (LLMs).","This model has a lower potential error and better scalability and adaptability than a process that considers the cybersecurity guidelines recommended by humans, known as the human-in-the-loop (HITL) process.","Also, this methodology significantly reduces the effort required when addressing new cyber threats or anomalies compared with machine learning (ML) techniques, since it leaves the models complexity and precision unaffected and offers a faster implementation.","These findings present a comparative assessment, conducted utilizing standard and advanced performance evaluation metrics for the proposed AD framework and the HITL process.","To generate and extract datasets of IEC 61850 communications, a hardware-in-the-loop (HIL) testbed was employed."],"url":"http://arxiv.org/abs/2406.05472v1","category":"cs.CR"}
{"created":"2024-06-08 12:58:13","title":"DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models","abstract":"Self-supervised speech models have shown to be useful for various tasks, but their large size limits the use in devices with low computing power and memory. In this work, we explore early exit, an approach for reducing latency by exiting the forward process of a network early. Most approaches of early exit need a separate early exit model for each task, with some even requiring fine-tuning of the entire pretrained model. We introduce Data Adaptive Self-Supervised Early Exit (DAISY), an approach that decides when to exit based on the self-supervised loss, eliminating the need for multiple round of training and fine-tuning. DAISY matches the performance of HuBERT on the MiniSUPERB benchmark, but with much faster inference times. Our analysis on the adaptivity of DAISY shows that the model exits early (using fewer layers) on clean data while exits late (using more layers) on noisy data, dynamically adjusting the computational cost of inference based on the noise level of each sample.","sentences":["Self-supervised speech models have shown to be useful for various tasks, but their large size limits the use in devices with low computing power and memory.","In this work, we explore early exit, an approach for reducing latency by exiting the forward process of a network early.","Most approaches of early exit need a separate early exit model for each task, with some even requiring fine-tuning of the entire pretrained model.","We introduce Data Adaptive Self-Supervised Early Exit (DAISY), an approach that decides when to exit based on the self-supervised loss, eliminating the need for multiple round of training and fine-tuning.","DAISY matches the performance of HuBERT on the MiniSUPERB benchmark, but with much faster inference times.","Our analysis on the adaptivity of DAISY shows that the model exits early (using fewer layers) on clean data while exits late (using more layers) on noisy data, dynamically adjusting the computational cost of inference based on the noise level of each sample."],"url":"http://arxiv.org/abs/2406.05464v1","category":"cs.SD"}
{"created":"2024-06-08 12:36:30","title":"Fighting Against the Repetitive Training and Sample Dependency Problem in Few-shot Named Entity Recognition","abstract":"Few-shot named entity recognition (NER) systems recognize entities using a few labeled training examples. The general pipeline consists of a span detector to identify entity spans in text and an entity-type classifier to assign types to entities. Current span detectors rely on extensive manual labeling to guide training. Almost every span detector requires initial training on basic span features followed by adaptation to task-specific features. This process leads to repetitive training of the basic span features among span detectors. Additionally, metric-based entity-type classifiers, such as prototypical networks, typically employ a specific metric that gauges the distance between the query sample and entity-type referents, ultimately assigning the most probable entity type to the query sample. However, these classifiers encounter the sample dependency problem, primarily stemming from the limited samples available for each entity-type referent. To address these challenges, we proposed an improved few-shot NER pipeline. First, we introduce a steppingstone span detector that is pre-trained on open-domain Wikipedia data. It can be used to initialize the pipeline span detector to reduce the repetitive training of basic features. Second, we leverage a large language model (LLM) to set reliable entity-type referents, eliminating reliance on few-shot samples of each type. Our model exhibits superior performance with fewer training steps and human-labeled data compared with baselines, as demonstrated through extensive experiments on various datasets. Particularly in fine-grained few-shot NER settings, our model outperforms strong baselines, including ChatGPT. We will publicly release the code, datasets, LLM outputs, and model checkpoints.","sentences":["Few-shot named entity recognition (NER) systems recognize entities using a few labeled training examples.","The general pipeline consists of a span detector to identify entity spans in text and an entity-type classifier to assign types to entities.","Current span detectors rely on extensive manual labeling to guide training.","Almost every span detector requires initial training on basic span features followed by adaptation to task-specific features.","This process leads to repetitive training of the basic span features among span detectors.","Additionally, metric-based entity-type classifiers, such as prototypical networks, typically employ a specific metric that gauges the distance between the query sample and entity-type referents, ultimately assigning the most probable entity type to the query sample.","However, these classifiers encounter the sample dependency problem, primarily stemming from the limited samples available for each entity-type referent.","To address these challenges, we proposed an improved few-shot NER pipeline.","First, we introduce a steppingstone span detector that is pre-trained on open-domain Wikipedia data.","It can be used to initialize the pipeline span detector to reduce the repetitive training of basic features.","Second, we leverage a large language model (LLM) to set reliable entity-type referents, eliminating reliance on few-shot samples of each type.","Our model exhibits superior performance with fewer training steps and human-labeled data compared with baselines, as demonstrated through extensive experiments on various datasets.","Particularly in fine-grained few-shot NER settings, our model outperforms strong baselines, including ChatGPT.","We will publicly release the code, datasets, LLM outputs, and model checkpoints."],"url":"http://arxiv.org/abs/2406.05460v1","category":"cs.CL"}
{"created":"2024-06-08 10:45:07","title":"Large Language Model Assisted Adversarial Robustness Neural Architecture Search","abstract":"Motivated by the potential of large language models (LLMs) as optimizers for solving combinatorial optimization problems, this paper proposes a novel LLM-assisted optimizer (LLMO) to address adversarial robustness neural architecture search (ARNAS), a specific application of combinatorial optimization. We design the prompt using the standard CRISPE framework (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment). In this study, we employ Gemini, a powerful LLM developed by Google. We iteratively refine the prompt, and the responses from Gemini are adapted as solutions to ARNAS instances. Numerical experiments are conducted on NAS-Bench-201-based ARNAS tasks with CIFAR-10 and CIFAR-100 datasets. Six well-known meta-heuristic algorithms (MHAs) including genetic algorithm (GA), particle swarm optimization (PSO), differential evolution (DE), and its variants serve as baselines. The experimental results confirm the competitiveness of the proposed LLMO and highlight the potential of LLMs as effective combinatorial optimizers. The source code of this research can be downloaded from \\url{https://github.com/RuiZhong961230/LLMO}.","sentences":["Motivated by the potential of large language models (LLMs) as optimizers for solving combinatorial optimization problems, this paper proposes a novel LLM-assisted optimizer (LLMO) to address adversarial robustness neural architecture search (ARNAS), a specific application of combinatorial optimization.","We design the prompt using the standard CRISPE framework (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment).","In this study, we employ Gemini, a powerful LLM developed by Google.","We iteratively refine the prompt, and the responses from Gemini are adapted as solutions to ARNAS instances.","Numerical experiments are conducted on NAS-Bench-201-based ARNAS tasks with CIFAR-10 and CIFAR-100 datasets.","Six well-known meta-heuristic algorithms (MHAs) including genetic algorithm (GA), particle swarm optimization (PSO), differential evolution (DE), and its variants serve as baselines.","The experimental results confirm the competitiveness of the proposed LLMO and highlight the potential of LLMs as effective combinatorial optimizers.","The source code of this research can be downloaded from \\url{https://github.com/RuiZhong961230/LLMO}."],"url":"http://arxiv.org/abs/2406.05433v1","category":"cs.NE"}
{"created":"2024-06-08 09:37:09","title":"Focusing of concentric free-surface waves","abstract":"Gravito-capillary waves at free-surfaces are ubiquitous in several natural and industrial processes involving quiescent liquid pools bounded by cylindrical walls. These waves emanate from the relaxation of initial interface distortions, which often take the form of a cavity (depression) centred on the symmetry axis of the container. These surface waves reflect from the container walls leading to a radially inward propagating wave-train converging (focussing) onto the symmetry axis. Under the inviscid approximation and for sufficiently shallow cavities, the relaxation is well-described by the linearised potential-flow equations. Naturally, adding viscosity to such a system introduces viscous dissipation that enervates energy and dampens the oscillations at the symmetry axis.   However, for viscous liquids and deeper cavities, these equations are qualitatively inaccurate. In this study, we elucidate a modal approach to study the initial-value problem for concentric gravito-capillary waves generated on a free-surface for inviscid as well as viscous liquids. For a sufficiently deep cavity, the inward focusing of waves results in large interfacial oscillations at the axis, necessitating a second-order nonlinear theory. We demonstrate that this theory effectively models the interfacial behavior and highlights the crucial role of nonlinearity near the symmetry axis. Contrary to expectations, the addition of slight viscosity further intensifies the oscillations at the symmetry axis. This finding underscores the limitations of the potential flow model and suggests avenues for more accurate modelling of such complex free-surface flows.","sentences":["Gravito-capillary waves at free-surfaces are ubiquitous in several natural and industrial processes involving quiescent liquid pools bounded by cylindrical walls.","These waves emanate from the relaxation of initial interface distortions, which often take the form of a cavity (depression) centred on the symmetry axis of the container.","These surface waves reflect from the container walls leading to a radially inward propagating wave-train converging (focussing) onto the symmetry axis.","Under the inviscid approximation and for sufficiently shallow cavities, the relaxation is well-described by the linearised potential-flow equations.","Naturally, adding viscosity to such a system introduces viscous dissipation that enervates energy and dampens the oscillations at the symmetry axis.   ","However, for viscous liquids and deeper cavities, these equations are qualitatively inaccurate.","In this study, we elucidate a modal approach to study the initial-value problem for concentric gravito-capillary waves generated on a free-surface for inviscid as well as viscous liquids.","For a sufficiently deep cavity, the inward focusing of waves results in large interfacial oscillations at the axis, necessitating a second-order nonlinear theory.","We demonstrate that this theory effectively models the interfacial behavior and highlights the crucial role of nonlinearity near the symmetry axis.","Contrary to expectations, the addition of slight viscosity further intensifies the oscillations at the symmetry axis.","This finding underscores the limitations of the potential flow model and suggests avenues for more accurate modelling of such complex free-surface flows."],"url":"http://arxiv.org/abs/2406.05416v1","category":"physics.flu-dyn"}
{"created":"2024-06-08 09:22:32","title":"Discover Your Neighbors: Advanced Stable Test-Time Adaptation in Dynamic World","abstract":"Despite progress, deep neural networks still suffer performance declines under distribution shifts between training and test domains, leading to a substantial decrease in Quality of Experience (QoE) for multimedia applications. Existing test-time adaptation (TTA) methods are challenged by dynamic, multiple test distributions within batches. This work provides a new perspective on analyzing batch normalization techniques through class-related and class-irrelevant features, our observations reveal combining source and test batch normalization statistics robustly characterizes target distributions. However, test statistics must have high similarity. We thus propose Discover Your Neighbours (DYN), the first backward-free approach specialized for dynamic TTA. The core innovation is identifying similar samples via instance normalization statistics and clustering into groups which provides consistent class-irrelevant representations. Specifically, Our DYN consists of layer-wise instance statistics clustering (LISC) and cluster-aware batch normalization (CABN). In LISC, we perform layer-wise clustering of approximate feature samples at each BN layer by calculating the cosine similarity of instance normalization statistics across the batch. CABN then aggregates SBN and TCN statistics to collaboratively characterize the target distribution, enabling more robust representations. Experimental results validate DYN's robustness and effectiveness, demonstrating maintained performance under dynamic data stream patterns.","sentences":["Despite progress, deep neural networks still suffer performance declines under distribution shifts between training and test domains, leading to a substantial decrease in Quality of Experience (QoE) for multimedia applications.","Existing test-time adaptation (TTA) methods are challenged by dynamic, multiple test distributions within batches.","This work provides a new perspective on analyzing batch normalization techniques through class-related and class-irrelevant features, our observations reveal combining source and test batch normalization statistics robustly characterizes target distributions.","However, test statistics must have high similarity.","We thus propose Discover Your Neighbours (DYN), the first backward-free approach specialized for dynamic TTA.","The core innovation is identifying similar samples via instance normalization statistics and clustering into groups which provides consistent class-irrelevant representations.","Specifically, Our DYN consists of layer-wise instance statistics clustering (LISC) and cluster-aware batch normalization (CABN).","In LISC, we perform layer-wise clustering of approximate feature samples at each BN layer by calculating the cosine similarity of instance normalization statistics across the batch.","CABN then aggregates SBN and TCN statistics to collaboratively characterize the target distribution, enabling more robust representations.","Experimental results validate DYN's robustness and effectiveness, demonstrating maintained performance under dynamic data stream patterns."],"url":"http://arxiv.org/abs/2406.05413v1","category":"cs.LG"}
{"created":"2024-06-08 09:20:30","title":"Generalized symmetry in non-Hermitian systems","abstract":"Despite acute interest in the dynamics of non-Hermitian systems, there is a lack of consensus in the mathematical formulation of non-Hermitian quantum mechanics in the community. Different methodologies are used in the literature to study non-Hermitian dynamics. This ranges from consistent frameworks like biorthogonal quantum mechanics and metric approach characterized by modified inner products, to normalization by time-dependent norms inspired by open quantum systems. In this work, we systematically explore the similarities and differences among these various methods. Utilizing illustrative models with exact solutions, we demonstrate that these methods produce not only quantitatively different results but also distinct physical interpretations. For dissipative systems where non-Hermiticity arises as an approximation, we find that the normalization method in the $\\mathcal{PT}$-broken regime closely aligns with the full master equation solutions. In contrast, for quantum systems where non-Hermiticity can be engineered exactly, incorporating metric dynamics is crucial for the probabilistic interpretation of quantum mechanics, necessitating the generalization of unitary symmetry to non-Hermitian systems. This study lays the groundwork for further exploration of non-Hermitian Hamiltonians, potentially leveraging generalized symmetries for novel physical phenomena.","sentences":["Despite acute interest in the dynamics of non-Hermitian systems, there is a lack of consensus in the mathematical formulation of non-Hermitian quantum mechanics in the community.","Different methodologies are used in the literature to study non-Hermitian dynamics.","This ranges from consistent frameworks like biorthogonal quantum mechanics and metric approach characterized by modified inner products, to normalization by time-dependent norms inspired by open quantum systems.","In this work, we systematically explore the similarities and differences among these various methods.","Utilizing illustrative models with exact solutions, we demonstrate that these methods produce not only quantitatively different results but also distinct physical interpretations.","For dissipative systems where non-Hermiticity arises as an approximation, we find that the normalization method in the $\\mathcal{PT}$-broken regime closely aligns with the full master equation solutions.","In contrast, for quantum systems where non-Hermiticity can be engineered exactly, incorporating metric dynamics is crucial for the probabilistic interpretation of quantum mechanics, necessitating the generalization of unitary symmetry to non-Hermitian systems.","This study lays the groundwork for further exploration of non-Hermitian Hamiltonians, potentially leveraging generalized symmetries for novel physical phenomena."],"url":"http://arxiv.org/abs/2406.05411v1","category":"quant-ph"}
{"created":"2024-06-08 08:54:22","title":"Self-Adaptive Integrated Photonic Receiver for Turbulence Compensation in Free Space Optical Links","abstract":"In Free Space Optical (FSO) communication systems, atmospheric turbulence distorts the propagating beams, causing a random fading in the received power. This perturbation can be compensated using a multi-aperture receiver that samples the distorted wavefront on different points and adds the various signals coherently. In this work, we report on an adaptive optical receiver that compensates in real time for scintillation in FSO links. The optical front-end of the receiver is entirely integrated in a silicon photonic chip hosting a 2D Optical Antenna Array and a self-adaptive analog Programmable Optical Processor made of a mesh of tunable Mach-Zehnder interferometers. The photonic chip acts as an adaptive interface to couple turbulent FSO beams to single-mode guided optics, enabling energy and cost-effective operation, scalability to systems with a larger number of apertures, modulation-format and data-protocol transparency, and pluggability with commercial fiber optics transceivers. Experimental results demonstrate the effectiveness of the proposed receiver with optical signals at a data rate of 10 Gbit/s transmitted in indoor FSO links where different turbulent conditions, even stronger than those expected in outdoor links of hundreds of meters, are reproduced.","sentences":["In Free Space Optical (FSO) communication systems, atmospheric turbulence distorts the propagating beams, causing a random fading in the received power.","This perturbation can be compensated using a multi-aperture receiver that samples the distorted wavefront on different points and adds the various signals coherently.","In this work, we report on an adaptive optical receiver that compensates in real time for scintillation in FSO links.","The optical front-end of the receiver is entirely integrated in a silicon photonic chip hosting a 2D Optical Antenna Array and a self-adaptive analog Programmable Optical Processor made of a mesh of tunable Mach-Zehnder interferometers.","The photonic chip acts as an adaptive interface to couple turbulent FSO beams to single-mode guided optics, enabling energy and cost-effective operation, scalability to systems with a larger number of apertures, modulation-format and data-protocol transparency, and pluggability with commercial fiber optics transceivers.","Experimental results demonstrate the effectiveness of the proposed receiver with optical signals at a data rate of 10 Gbit/s transmitted in indoor FSO links where different turbulent conditions, even stronger than those expected in outdoor links of hundreds of meters, are reproduced."],"url":"http://arxiv.org/abs/2406.05402v1","category":"physics.optics"}
{"created":"2024-06-08 08:41:12","title":"Metric Convolutions: A Unifying Theory to Adaptive Convolutions","abstract":"Standard convolutions are prevalent in image processing and deep learning, but their fixed kernel design limits adaptability. Several deformation strategies of the reference kernel grid have been proposed. Yet, they lack a unified theoretical framework. By returning to a metric perspective for images, now seen as two-dimensional manifolds equipped with notions of local and geodesic distances, either symmetric (Riemannian metrics) or not (Finsler metrics), we provide a unifying principle: the kernel positions are samples of unit balls of implicit metrics. With this new perspective, we also propose metric convolutions, a novel approach that samples unit balls from explicit signal-dependent metrics, providing interpretable operators with geometric regularisation. This framework, compatible with gradient-based optimisation, can directly replace existing convolutions applied to either input images or deep features of neural networks. Metric convolutions typically require fewer parameters and provide better generalisation. Our approach shows competitive performance in standard denoising and classification tasks.","sentences":["Standard convolutions are prevalent in image processing and deep learning, but their fixed kernel design limits adaptability.","Several deformation strategies of the reference kernel grid have been proposed.","Yet, they lack a unified theoretical framework.","By returning to a metric perspective for images, now seen as two-dimensional manifolds equipped with notions of local and geodesic distances, either symmetric (Riemannian metrics) or not (Finsler metrics), we provide a unifying principle: the kernel positions are samples of unit balls of implicit metrics.","With this new perspective, we also propose metric convolutions, a novel approach that samples unit balls from explicit signal-dependent metrics, providing interpretable operators with geometric regularisation.","This framework, compatible with gradient-based optimisation, can directly replace existing convolutions applied to either input images or deep features of neural networks.","Metric convolutions typically require fewer parameters and provide better generalisation.","Our approach shows competitive performance in standard denoising and classification tasks."],"url":"http://arxiv.org/abs/2406.05400v1","category":"cs.CV"}
{"created":"2024-06-10 17:43:39","title":"Phragm\u00e8n-Lindel\u00f6f type theorems for elliptic equations on infinite graphs","abstract":"We investigate the validity of the Phragm\\`en-Lindel\\\"of principle for a class of elliptic equations with a potential, posed on infinite graphs. Consequently, we get uniqueness, in the class of solutions satisfying a suitable growth condition at infinity. We suppose that the {\\it outer degree (or outer curvature)} of the graph is bounded from above, and we allow the potential to go to zero at infinity in a controlled way. Finally, we discuss the optimality of the condition on the potential on symmetric trees and on the integer lattice.","sentences":["We investigate the validity of the Phragm\\`en-Lindel\\\"of principle for a class of elliptic equations with a potential, posed on infinite graphs.","Consequently, we get uniqueness, in the class of solutions satisfying a suitable growth condition at infinity.","We suppose that the {\\it outer degree (or outer curvature)} of the graph is bounded from above, and we allow the potential to go to zero at infinity in a controlled way.","Finally, we discuss the optimality of the condition on the potential on symmetric trees and on the integer lattice."],"url":"http://arxiv.org/abs/2406.06505v1","category":"math.AP"}
{"created":"2024-06-10 17:37:56","title":"Viscous shock fluctuations in KPZ","abstract":"We study ``V-shaped'' solutions to the KPZ equation, those having opposite asymptotic slopes $\\theta$ and $-\\theta$, with $\\theta>0$, at positive and negative infinity, respectively. Answering a question of Janjigian, Rassoul-Agha, and Sepp\\\"al\\\"ainen, we show that the spatial increments of V-shaped solutions cannot be statistically stationary in time. This completes the classification of statistically time-stationary spatial increments for the KPZ equation by ruling out the last case left by those authors.   To show that these V-shaped time-stationary measures do not exist, we study the location of the corresponding ``viscous shock,'' which, roughly speaking, is the location of the bottom of the V. We describe the limiting rescaled fluctuations, and in particular show that the fluctuations of the shock location are not tight, for both stationary and flat initial data. We also show that if the KPZ equation is started with V-shaped initial data, then the long-time limits of the time-averaged laws of the spatial increments of the solution are mixtures of the laws of the spatial increments of $x\\mapsto B(x)+\\theta x$ and $x\\mapsto B(x)-\\theta x$, where $B$ is a standard two-sided Brownian motion.","sentences":["We study ``V-shaped'' solutions to the KPZ equation, those having opposite asymptotic slopes $\\theta$ and $-\\theta$, with $\\theta>0$, at positive and negative infinity, respectively.","Answering a question of Janjigian, Rassoul-Agha, and Sepp\\\"al\\\"ainen, we show that the spatial increments of V-shaped solutions cannot be statistically stationary in time.","This completes the classification of statistically time-stationary spatial increments for the KPZ equation by ruling out the last case left by those authors.   ","To show that these V-shaped time-stationary measures do not exist, we study the location of the corresponding ``viscous shock,'' which, roughly speaking, is the location of the bottom of the V. We describe the limiting rescaled fluctuations, and in particular show that the fluctuations of the shock location are not tight, for both stationary and flat initial data.","We also show that if the KPZ equation is started with V-shaped initial data, then the long-time limits of the time-averaged laws of the spatial increments of the solution are mixtures of the laws of the spatial increments of $x\\mapsto B(x)+\\theta x$ and $x\\mapsto B(x)-\\theta x$, where $B$ is a standard two-sided Brownian motion."],"url":"http://arxiv.org/abs/2406.06502v1","category":"math.PR"}
{"created":"2024-06-10 16:42:44","title":"Time Series Analysis: yesterday, today, tomorrow","abstract":"Forecasts of various processes have always been a sophisticated problem for statistics and data science. Over the past decades the solution procedures were updated by deep learning and kernel methods. According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods. Here we investigate how true this point of view is.","sentences":["Forecasts of various processes have always been a sophisticated problem for statistics and data science.","Over the past decades the solution procedures were updated by deep learning and kernel methods.","According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods.","Here we investigate how true this point of view is."],"url":"http://arxiv.org/abs/2406.06453v1","category":"cs.CY"}
{"created":"2024-06-10 16:24:46","title":"Spatiotemporal Graph Neural Network Modelling Perfusion MRI","abstract":"Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking. This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients. Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling. Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions. Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization.","sentences":["Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking.","This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients.","Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling.","Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions.","Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets.","Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization."],"url":"http://arxiv.org/abs/2406.06434v1","category":"eess.IV"}
{"created":"2024-06-10 15:22:10","title":"On inverse scattering for the two-dimensional nonlinear Klein-Gordon equation","abstract":"The inverse scattering problem for the two-dimensional nonlinear Klein-Gordon equation $u_{tt}-\\Delta u + u = \\mathcal{N}(u)$ is studied. We assume that the unknown nonlinearity $\\mathcal{N}$ of the equation satisfies $\\mathcal{N}\\in C^\\infty(\\mathbb{R};\\mathbb{R})$, $\\mathcal{N}^{(k)}(y)=O(|y|^{\\max\\{ 3-k,0 \\}})$ ($y \\to 0$) and $\\mathcal{N}^{(k)}(y)=O(e^{c y^2})$ ($|y| \\to \\infty$) for any $k=0,1,2,\\cdots$. Here, $c$ is a positive constant. We establish a reconstraction formula of $\\mathcal{N}^{(k)}(0)$ ($k=3,4,5,\\cdots$) by the knowledge of the scattering operator for the equation. As an application, we also give an expression for higher order G\\^{a}teaux differentials of the scattering operator at 0.","sentences":["The inverse scattering problem for the two-dimensional nonlinear Klein-Gordon equation $u_{tt}-\\Delta u + u = \\mathcal{N}(u)$ is studied.","We assume that the unknown nonlinearity $\\mathcal{N}$ of the equation satisfies $\\mathcal{N}\\in C^\\infty(\\mathbb{R};\\mathbb{R})$, $\\mathcal{N}^{(k)}(y)=O(|y|^{\\max\\{ 3-k,0 \\}})$ ($y \\to 0$) and $\\mathcal{N}^{(k)}(y)=O(e^{c y^2})$ ($|y| \\to \\infty$) for any $k=0,1,2,\\cdots$. Here, $c$ is a positive constant.","We establish a reconstraction formula of $\\mathcal{N}^{(k)}(0)$ ($k=3,4,5,\\cdots$) by the knowledge of the scattering operator for the equation.","As an application, we also give an expression for higher order G\\^{a}teaux differentials of the scattering operator at 0."],"url":"http://arxiv.org/abs/2406.06362v1","category":"math.AP"}
{"created":"2024-06-10 14:59:01","title":"Audio-based Step-count Estimation for Running -- Windowing and Neural Network Baselines","abstract":"In recent decades, running has become an increasingly popular pastime activity due to its accessibility, ease of practice, and anticipated health benefits. However, the risk of running-related injuries is substantial for runners of different experience levels. Several common forms of injuries result from overuse -- extending beyond the recommended running time and intensity. Recently, audio-based tracking has emerged as yet another modality for monitoring running behaviour and performance, with previous studies largely concentrating on predicting runner fatigue. In this work, we investigate audio-based step count estimation during outdoor running, achieving a mean absolute error of 1.098 in window-based step-count differences and a Pearson correlation coefficient of 0.479 when predicting the number of steps in a 5-second window of audio. Our work thus showcases the feasibility of audio-based monitoring for estimating important physiological variables and lays the foundations for further utilising audio sensors for a more thorough characterisation of runner behaviour.","sentences":["In recent decades, running has become an increasingly popular pastime activity due to its accessibility, ease of practice, and anticipated health benefits.","However, the risk of running-related injuries is substantial for runners of different experience levels.","Several common forms of injuries result from overuse -- extending beyond the recommended running time and intensity.","Recently, audio-based tracking has emerged as yet another modality for monitoring running behaviour and performance, with previous studies largely concentrating on predicting runner fatigue.","In this work, we investigate audio-based step count estimation during outdoor running, achieving a mean absolute error of 1.098 in window-based step-count differences and a Pearson correlation coefficient of 0.479 when predicting the number of steps in a 5-second window of audio.","Our work thus showcases the feasibility of audio-based monitoring for estimating important physiological variables and lays the foundations for further utilising audio sensors for a more thorough characterisation of runner behaviour."],"url":"http://arxiv.org/abs/2406.06339v1","category":"cs.SD"}
{"created":"2024-06-10 14:54:32","title":"Propagation in rough waveguides: Forward and inverse problems","abstract":"We discuss here the direct and inverse problems for wave propagation in a waveguide with rough internal surface and arbitrary mean shape. The high degree of multiple scattering inside the waveguide poses significant challenges both for the forward computation and for the recovery of the surface profile, and raises important questions about scattering cross-sections. This paper falls into two parts corresponding to these issues.   We first apply Left-Right (L-R) operator splitting to calculate the scattered fields in 2- and 3-dimensional waveguides. Using this we illustrate the scattered fields and the effect of surface roughness. In the second part we formulate an algorithm for surface recovery from field measurements along the waveguide axis, which generalises recent work on surfaces in 2 dimensions. This method utilizes forward scattering assumptions in effect by formulating an integral equation in the unknown surface field, treated as a function of the surface. Although discussed in the context of waveguides, the formulae are given in a form applicable to a variety of geometries, with coefficients which will depend on and be determined by each specific application.","sentences":["We discuss here the direct and inverse problems for wave propagation in a waveguide with rough internal surface and arbitrary mean shape.","The high degree of multiple scattering inside the waveguide poses significant challenges both for the forward computation and for the recovery of the surface profile, and raises important questions about scattering cross-sections.","This paper falls into two parts corresponding to these issues.   ","We first apply Left-Right (L-R) operator splitting to calculate the scattered fields in 2- and 3-dimensional waveguides.","Using this we illustrate the scattered fields and the effect of surface roughness.","In the second part we formulate an algorithm for surface recovery from field measurements along the waveguide axis, which generalises recent work on surfaces in 2 dimensions.","This method utilizes forward scattering assumptions in effect by formulating an integral equation in the unknown surface field, treated as a function of the surface.","Although discussed in the context of waveguides, the formulae are given in a form applicable to a variety of geometries, with coefficients which will depend on and be determined by each specific application."],"url":"http://arxiv.org/abs/2406.06336v1","category":"physics.comp-ph"}
{"created":"2024-06-10 14:50:32","title":"An automatic analysis of ultrasound vocalisations for the prediction of interaction context in captive Egyptian fruit bats","abstract":"Prior work in computational bioacoustics has mostly focused on the detection of animal presence in a particular habitat. However, animal sounds contain much richer information than mere presence; among others, they encapsulate the interactions of those animals with other members of their species. Studying these interactions is almost impossible in a naturalistic setting, as the ground truth is often lacking. The use of animals in captivity instead offers a viable alternative pathway. However, most prior works follow a traditional, statistics-based approach to analysing interactions. In the present work, we go beyond this standard framework by attempting to predict the underlying context in interactions between captive \\emph{Rousettus Aegyptiacus} using deep neural networks. We reach an unweighted average recall of over 30\\% -- more than thrice the chance level -- and show error patterns that differ from our statistical analysis. This work thus represents an important step towards the automatic analysis of states in animals from sound.","sentences":["Prior work in computational bioacoustics has mostly focused on the detection of animal presence in a particular habitat.","However, animal sounds contain much richer information than mere presence; among others, they encapsulate the interactions of those animals with other members of their species.","Studying these interactions is almost impossible in a naturalistic setting, as the ground truth is often lacking.","The use of animals in captivity instead offers a viable alternative pathway.","However, most prior works follow a traditional, statistics-based approach to analysing interactions.","In the present work, we go beyond this standard framework by attempting to predict the underlying context in interactions between captive \\emph{Rousettus Aegyptiacus} using deep neural networks.","We reach an unweighted average recall of over 30\\% -- more than thrice the chance level -- and show error patterns that differ from our statistical analysis.","This work thus represents an important step towards the automatic analysis of states in animals from sound."],"url":"http://arxiv.org/abs/2406.06332v1","category":"cs.SD"}
{"created":"2024-06-10 14:31:38","title":"ProAct: Progressive Training for Hybrid Clipped Activation Function to Enhance Resilience of DNNs","abstract":"Deep Neural Networks (DNNs) are extensively employed in safety-critical applications where ensuring hardware reliability is a primary concern. To enhance the reliability of DNNs against hardware faults, activation restriction techniques significantly mitigate the fault effects at the DNN structure level, irrespective of accelerator architectures. State-of-the-art methods offer either neuron-wise or layer-wise clipping activation functions. They attempt to determine optimal clipping thresholds using heuristic and learning-based approaches. Layer-wise clipped activation functions cannot preserve DNNs resilience at high bit error rates. On the other hand, neuron-wise clipping activation functions introduce considerable memory overhead due to the addition of parameters, which increases their vulnerability to faults. Moreover, the heuristic-based optimization approach demands numerous fault injections during the search process, resulting in time-consuming threshold identification. On the other hand, learning-based techniques that train thresholds for entire layers concurrently often yield sub-optimal results. In this work, first, we demonstrate that it is not essential to incorporate neuron-wise activation functions throughout all layers in DNNs. Then, we propose a hybrid clipped activation function that integrates neuron-wise and layer-wise methods that apply neuron-wise clipping only in the last layer of DNNs. Additionally, to attain optimal thresholds in the clipping activation function, we introduce ProAct, a progressive training methodology. This approach iteratively trains the thresholds on a layer-by-layer basis, aiming to obtain optimal threshold values in each layer separately.","sentences":["Deep Neural Networks (DNNs) are extensively employed in safety-critical applications where ensuring hardware reliability is a primary concern.","To enhance the reliability of DNNs against hardware faults, activation restriction techniques significantly mitigate the fault effects at the DNN structure level, irrespective of accelerator architectures.","State-of-the-art methods offer either neuron-wise or layer-wise clipping activation functions.","They attempt to determine optimal clipping thresholds using heuristic and learning-based approaches.","Layer-wise clipped activation functions cannot preserve DNNs resilience at high bit error rates.","On the other hand, neuron-wise clipping activation functions introduce considerable memory overhead due to the addition of parameters, which increases their vulnerability to faults.","Moreover, the heuristic-based optimization approach demands numerous fault injections during the search process, resulting in time-consuming threshold identification.","On the other hand, learning-based techniques that train thresholds for entire layers concurrently often yield sub-optimal results.","In this work, first, we demonstrate that it is not essential to incorporate neuron-wise activation functions throughout all layers in DNNs.","Then, we propose a hybrid clipped activation function that integrates neuron-wise and layer-wise methods that apply neuron-wise clipping only in the last layer of DNNs.","Additionally, to attain optimal thresholds in the clipping activation function, we introduce ProAct, a progressive training methodology.","This approach iteratively trains the thresholds on a layer-by-layer basis, aiming to obtain optimal threshold values in each layer separately."],"url":"http://arxiv.org/abs/2406.06313v1","category":"cs.LG"}
{"created":"2024-06-10 14:11:15","title":"VS-PINN: A Fast and efficient training of physics-informed neural networks using variable-scaling methods for solving PDEs with stiff behavior","abstract":"Physics-informed neural networks (PINNs) have recently emerged as a promising way to compute the solutions of partial differential equations (PDEs) using deep neural networks. However, despite their significant success in various fields, it remains unclear in many aspects how to effectively train PINNs if the solutions of PDEs exhibit stiff behaviors or high frequencies. In this paper, we propose a new method for training PINNs using variable-scaling techniques. This method is simple and it can be applied to a wide range of problems including PDEs with rapidly-varying solutions. Throughout various numerical experiments, we will demonstrate the effectiveness of the proposed method for these problems and confirm that it can significantly improve the training efficiency and performance of PINNs. Furthermore, based on the analysis of the neural tangent kernel (NTK), we will provide theoretical evidence for this phenomenon and show that our methods can indeed improve the performance of PINNs.","sentences":["Physics-informed neural networks (PINNs) have recently emerged as a promising way to compute the solutions of partial differential equations (PDEs) using deep neural networks.","However, despite their significant success in various fields, it remains unclear in many aspects how to effectively train PINNs if the solutions of PDEs exhibit stiff behaviors or high frequencies.","In this paper, we propose a new method for training PINNs using variable-scaling techniques.","This method is simple and it can be applied to a wide range of problems including PDEs with rapidly-varying solutions.","Throughout various numerical experiments, we will demonstrate the effectiveness of the proposed method for these problems and confirm that it can significantly improve the training efficiency and performance of PINNs.","Furthermore, based on the analysis of the neural tangent kernel (NTK), we will provide theoretical evidence for this phenomenon and show that our methods can indeed improve the performance of PINNs."],"url":"http://arxiv.org/abs/2406.06287v1","category":"math.NA"}
{"created":"2024-06-10 13:42:06","title":"PB-groupoids vs VB-groupoids","abstract":"In this paper we introduce the notion of PB-groupoid with a structural Lie 2-groupoid, and extend the classical correspondence between vector bundles and principal bundles to VB-groupoids and PB-groupoids.","sentences":["In this paper we introduce the notion of PB-groupoid with a structural Lie 2-groupoid, and extend the classical correspondence between vector bundles and principal bundles to VB-groupoids and PB-groupoids."],"url":"http://arxiv.org/abs/2406.06259v1","category":"math.DG"}
{"created":"2024-06-10 13:07:13","title":"Efficient Neural Compression with Inference-time Decoding","abstract":"This paper explores the combination of neural network quantization and entropy coding for memory footprint minimization. Edge deployment of quantized models is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth tradeoff, causing dramatic accuracy loss below a certain bitwidth. This accuracy loss can be alleviated thanks to mixed precision quantization, allowing for more flexible bitwidth allocation. However, standard mixed precision benefits remain limited due to the 1-bit frontier, that forces each parameter to be encoded on at least 1 bit of data. This paper introduces an approach that combines mixed precision, zero-point quantization and entropy coding to push the compression boundary of Resnets beyond the 1-bit frontier with an accuracy drop below 1% on the ImageNet benchmark. From an implementation standpoint, a compact decoder architecture features reduced latency, thus allowing for inference-compatible decoding.","sentences":["This paper explores the combination of neural network quantization and entropy coding for memory footprint minimization.","Edge deployment of quantized models is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth tradeoff, causing dramatic accuracy loss below a certain bitwidth.","This accuracy loss can be alleviated thanks to mixed precision quantization, allowing for more flexible bitwidth allocation.","However, standard mixed precision benefits remain limited due to the 1-bit frontier, that forces each parameter to be encoded on at least 1 bit of data.","This paper introduces an approach that combines mixed precision, zero-point quantization and entropy coding to push the compression boundary of Resnets beyond the 1-bit frontier with an accuracy drop below 1% on the ImageNet benchmark.","From an implementation standpoint, a compact decoder architecture features reduced latency, thus allowing for inference-compatible decoding."],"url":"http://arxiv.org/abs/2406.06237v1","category":"cs.LG"}
{"created":"2024-06-10 12:50:07","title":"On uniqueness in nonlocal diffuse optical tomography","abstract":"We investigate the inverse problem of recovering the diffusion and absorption coefficients $(\\sigma,q)$ in the nonlocal diffuse optical tomography equation $(-\\text{div}( \\sigma \\nabla))^s u+q u =0$ from the (partial) Dirichlet-to-Neumann map. The purpose of this article is twofold: (i) Firstly, we show that the diffusion coefficient $\\sigma$ and absorption coefficient $q$ can be recovered simultaneously. (ii) Secondly, we prove that the absorption coefficient $q$ can be determined provided $\\sigma$ is known in a neighborhood of the boundary $\\partial\\Omega$. The key ingredients to prove these uniqueness results are the Caffarelli--Silvestre type extension technique and a novel Runge approximation related to solution spaces of two different partial differential equations, which is based on the geometric form of the Hahn--Banach theorem. The results in this work hold for any dimension $n\\geq 3$.","sentences":["We investigate the inverse problem of recovering the diffusion and absorption coefficients $(\\sigma,q)$ in the nonlocal diffuse optical tomography equation $(-\\text{div}( \\sigma \\nabla))^s u+q u =0$ from the (partial) Dirichlet-to-Neumann map.","The purpose of this article is twofold: (i) Firstly, we show that the diffusion coefficient $\\sigma$ and absorption coefficient $q$ can be recovered simultaneously.","(ii) Secondly, we prove that the absorption coefficient $q$ can be determined provided $\\sigma$ is known in a neighborhood of the boundary $\\partial\\Omega$. The key ingredients to prove these uniqueness results are the Caffarelli--Silvestre type extension technique and a novel Runge approximation related to solution spaces of two different partial differential equations, which is based on the geometric form of the Hahn--Banach theorem.","The results in this work hold for any dimension $n\\geq 3$."],"url":"http://arxiv.org/abs/2406.06226v1","category":"math.AP"}
{"created":"2024-06-10 11:38:16","title":"On the unique solvability of the simultaneous Pell equations $x^2-ay^2 = 1$ and $z^2-bx^2 = 1$","abstract":"We consider the simultaneous Pell equations $$x^2 - ay^2 = 1, \\qquad z^2 - bx^2 = 1,$$ where $a > b\\geq 2$ are positive integers. We describe a procedure which, for any fixed $b$, either confirms that the simultaneous Pell equations have at most one solution in positive integers, or finds all exceptions for which we have proved that there are at most finitely many.","sentences":["We consider the simultaneous Pell equations $$x^2 - ay^2 = 1, \\qquad z^2 - bx^2 = 1,$$ where $a > b\\geq 2$ are positive integers.","We describe a procedure which, for any fixed $b$, either confirms that the simultaneous Pell equations have at most one solution in positive integers, or finds all exceptions for which we have proved that there are at most finitely many."],"url":"http://arxiv.org/abs/2406.06191v1","category":"math.NT"}
{"created":"2024-06-10 11:36:56","title":"Two-level Nonlinear Preconditioning Methods for Flood Models Posed on Perforated Domains","abstract":"This article focuses on the numerical solution of the Diffusive Wave equation posed in a domain containing a large number of polygonal perforations. These numerous perforations represent structures in urban areas and this model problem is used to model urban floods. This article relies on the work done in a previous article by the same authors, in which we introduced low-dimensional coarse approximation space for the linear Poisson equation based on a coarse polygonal partitioning of the domain. Similarly to other multi-scale numerical methods, this coarse space is spanned by locally discrete harmonic basis functions. We show that this coarse space also lends itself to the linearized diffusive wave problem that is obtained at each iteration of Newton's method. Furthermore, we present nonlinear preconditioning techniques, including a two-level RASPEN and Two-step method, which can significantly reduce the number of iterations compared to the traditional Newton's method. Numerical examples illustrating the performance of the proposed methods include a large-scale test case based on topographical data from the city of Nice.","sentences":["This article focuses on the numerical solution of the Diffusive Wave equation posed in a domain containing a large number of polygonal perforations.","These numerous perforations represent structures in urban areas and this model problem is used to model urban floods.","This article relies on the work done in a previous article by the same authors, in which we introduced low-dimensional coarse approximation space for the linear Poisson equation based on a coarse polygonal partitioning of the domain.","Similarly to other multi-scale numerical methods, this coarse space is spanned by locally discrete harmonic basis functions.","We show that this coarse space also lends itself to the linearized diffusive wave problem that is obtained at each iteration of Newton's method.","Furthermore, we present nonlinear preconditioning techniques, including a two-level RASPEN and Two-step method, which can significantly reduce the number of iterations compared to the traditional Newton's method.","Numerical examples illustrating the performance of the proposed methods include a large-scale test case based on topographical data from the city of Nice."],"url":"http://arxiv.org/abs/2406.06189v1","category":"math.NA"}
{"created":"2024-06-10 11:12:43","title":"Higgs cross-section ( including di-Higgs ) with CMS and ATLAS","abstract":"Since the discovery of the Higgs boson in 2012 by the ATLAS and CMS Collaborations a lot of progress has been made in verifying the nature of this new bosonic particle. Still, questions remain as to whether this new particle is the standard model (SM) Higgs boson, and whether it gives us hints of where physics beyond the SM (BSM) might be hidden. This overview tries to tackle these questions by looking at three types of analyses 1) Total cross section measurements of the various Higgs boson production and decay modes, allowing us access to the various Higgs boson couplings to SM bosons and fermions, 2) Differential Higgs boson cross section measurements such as in the STXS framework allowing for a model independent search for BSM and, finally, 3) The searches for di-Higgs production which give access to the trilinear Higgs self coupling $\\lambda$ and the Higgs potential itself.","sentences":["Since the discovery of the Higgs boson in 2012 by the ATLAS and CMS Collaborations a lot of progress has been made in verifying the nature of this new bosonic particle.","Still, questions remain as to whether this new particle is the standard model (SM) Higgs boson, and whether it gives us hints of where physics beyond the SM (BSM) might be hidden.","This overview tries to tackle these questions by looking at three types of analyses 1) Total cross section measurements of the various Higgs boson production and decay modes, allowing us access to the various Higgs boson couplings to SM bosons and fermions, 2) Differential Higgs boson cross section measurements such as in the STXS framework allowing for a model independent search for BSM and, finally, 3) The searches for di-Higgs production which give access to the trilinear Higgs self coupling $\\lambda$ and the Higgs potential itself."],"url":"http://arxiv.org/abs/2406.06172v1","category":"hep-ex"}
{"created":"2024-06-10 10:50:45","title":"On local well-posedness of the stochastic incompressible density-dependent Euler equations","abstract":"In this paper we study the stochastic inhomogeneous incompressible Euler equations in the whole space $\\mathbb{R}^3$. We show the existence and pathwise uniqueness of local solutions with a multiplicative stochastic noise. Our approach is based on reducing our problem to the random problem and some estimations for type transport equations.","sentences":["In this paper we study the stochastic inhomogeneous incompressible Euler equations in the whole space $\\mathbb{R}^3$. We show the existence and pathwise uniqueness of local solutions with a multiplicative stochastic noise.","Our approach is based on reducing our problem to the random problem and some estimations for type transport equations."],"url":"http://arxiv.org/abs/2406.06161v1","category":"math.AP"}
{"created":"2024-06-10 09:44:06","title":"ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models","abstract":"We propose ExtraNeRF, a novel method for extrapolating the range of views handled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs to model scene-specific, fine-grained details, while capitalizing on diffusion models to extrapolate beyond our observed data. A key ingredient is to track visibility to determine what portions of the scene have not been observed, and focus on reconstructing those regions consistently with diffusion models. Our primary contributions include a visibility-aware diffusion-based inpainting module that is fine-tuned on the input imagery, yielding an initial NeRF with moderate quality (often blurry) inpainted regions, followed by a second diffusion model trained on the input imagery to consistently enhance, notably sharpen, the inpainted imagery from the first pass. We demonstrate high-quality results, extrapolating beyond a small number of (typically six or fewer) input views, effectively outpainting the NeRF as well as inpainting newly disoccluded regions inside the original viewing volume. We compare with related work both quantitatively and qualitatively and show significant gains over prior art.","sentences":["We propose ExtraNeRF, a novel method for extrapolating the range of views handled by a Neural Radiance Field (NeRF).","Our main idea is to leverage NeRFs to model scene-specific, fine-grained details, while capitalizing on diffusion models to extrapolate beyond our observed data.","A key ingredient is to track visibility to determine what portions of the scene have not been observed, and focus on reconstructing those regions consistently with diffusion models.","Our primary contributions include a visibility-aware diffusion-based inpainting module that is fine-tuned on the input imagery, yielding an initial NeRF with moderate quality (often blurry) inpainted regions, followed by a second diffusion model trained on the input imagery to consistently enhance, notably sharpen, the inpainted imagery from the first pass.","We demonstrate high-quality results, extrapolating beyond a small number of (typically six or fewer) input views, effectively outpainting the NeRF as well as inpainting newly disoccluded regions inside the original viewing volume.","We compare with related work both quantitatively and qualitatively and show significant gains over prior art."],"url":"http://arxiv.org/abs/2406.06133v1","category":"cs.CV"}
{"created":"2024-06-10 09:38:30","title":"Direct and inverse scattering by unbounded penetrable rough surfaces","abstract":"In this paper, we investigate on the direct and inverse scattering problem by an unbounded penetrable rough surface in a lossless medium. The cases that the transmission coefficient $\\mu\\neq1$ and $\\mu=1$, which creates certain difficulties in the direct and inverse problem, respectively, are both considered. We first estalish the well-posedness of the direct problem using the integral equation method through an elaborate analysis. Then we carefully consider the singularity of the solutions to the problem with incident point source or hypersingular point source, where a simple and novel perspective is given for the derivation of the singularity. Finally, a global uniqueness result is proven for the inverse problem on the unique determination of the unbounded rough surface, the transmission coefficient and the wave number in the lower half plane from the measurements of the near field only on a line segment above the interface at a fixed frequency.","sentences":["In this paper, we investigate on the direct and inverse scattering problem by an unbounded penetrable rough surface in a lossless medium.","The cases that the transmission coefficient $\\mu\\neq1$ and $\\mu=1$, which creates certain difficulties in the direct and inverse problem, respectively, are both considered.","We first estalish the well-posedness of the direct problem using the integral equation method through an elaborate analysis.","Then we carefully consider the singularity of the solutions to the problem with incident point source or hypersingular point source, where a simple and novel perspective is given for the derivation of the singularity.","Finally, a global uniqueness result is proven for the inverse problem on the unique determination of the unbounded rough surface, the transmission coefficient and the wave number in the lower half plane from the measurements of the near field only on a line segment above the interface at a fixed frequency."],"url":"http://arxiv.org/abs/2406.06129v1","category":"math.AP"}
{"created":"2024-06-10 09:33:24","title":"On the biharmonic scattering by impenetrable obstacles","abstract":"In this paper, we consider the direct and inverse biharmonic obstacle scattering problems in both two and three dimensions with mainly the Dirichlet boundary condition being investgated. We first derive some basic properties for the biharmonic scattering solutions, which leads to a simple criterion for the uniqueness of the direct problem. Furthermore, a new type far-field pattern for biharmonic scattering is defined, and the correspondence between the far-field pattern and scattered field is proved. Then we derive the well-posedness of the direct problem by establishing the boundary integral equation method. Finally, the inverse problem for determining the obstacle is studied. Utilizing the reciprocity relations of the far-field pattern and scattered field, we show that the obstacle can be uniquely recovered from the measurements at a fixed frequency.","sentences":["In this paper, we consider the direct and inverse biharmonic obstacle scattering problems in both two and three dimensions with mainly the Dirichlet boundary condition being investgated.","We first derive some basic properties for the biharmonic scattering solutions, which leads to a simple criterion for the uniqueness of the direct problem.","Furthermore, a new type far-field pattern for biharmonic scattering is defined, and the correspondence between the far-field pattern and scattered field is proved.","Then we derive the well-posedness of the direct problem by establishing the boundary integral equation method.","Finally, the inverse problem for determining the obstacle is studied.","Utilizing the reciprocity relations of the far-field pattern and scattered field, we show that the obstacle can be uniquely recovered from the measurements at a fixed frequency."],"url":"http://arxiv.org/abs/2406.06126v1","category":"math.AP"}
{"created":"2024-06-10 07:22:59","title":"Fisher's Mirage: Noise Tightening of Cosmological Constraints in Simulation-Based Inference","abstract":"We systematically analyze the implications of statistical noise within numerical derivatives on simulation-based Fisher forecasts for large scale structure surveys. Noisy numerical derivatives resulting from a finite number of simulations, $N_{sims}$, act to bias the associated Fisher forecast such that the resulting marginalized constraints can be significantly tighter than the noise-free limit. We show the source of this effect can be traced to the influence of the noise on the marginalization process. Parameters such as the neutrino mass, $\\M$, for which higher-order forward differentiation schemes are commonly used, are more prone to noise; the predicted constraints can be akin to those purely from a random instance of statistical noise even using $(1\\mathrm{Gpc}/h)^{3}$ simulations with $N_{sims}=500$ realizations. We demonstrate how derivative noise can artificially reduce parameter degeneracies and seemingly null the effects of adding nuisance parameters to the forecast, such as HOD fitting parameters. We mathematically characterize these effects through a full statistical analysis, and demonstrate how confidence intervals for the true noise-free, $N_{sims} \\rightarrow \\infty$, Fisher constraints can be recovered even when noise comprises a consequential component of the measured signal. The findings and approaches developed here are important for ensuring simulation-based analyses can be accurately used to assess upcoming survey capabilities.","sentences":["We systematically analyze the implications of statistical noise within numerical derivatives on simulation-based Fisher forecasts for large scale structure surveys.","Noisy numerical derivatives resulting from a finite number of simulations, $N_{sims}$, act to bias the associated Fisher forecast such that the resulting marginalized constraints can be significantly tighter than the noise-free limit.","We show the source of this effect can be traced to the influence of the noise on the marginalization process.","Parameters such as the neutrino mass, $\\M$, for which higher-order forward differentiation schemes are commonly used, are more prone to noise; the predicted constraints can be akin to those purely from a random instance of statistical noise even using $(1\\mathrm{Gpc}/h)^{3}$ simulations with $N_{sims}=500$ realizations.","We demonstrate how derivative noise can artificially reduce parameter degeneracies and seemingly null the effects of adding nuisance parameters to the forecast, such as HOD fitting parameters.","We mathematically characterize these effects through a full statistical analysis, and demonstrate how confidence intervals for the true noise-free, $N_{sims} \\rightarrow \\infty$, Fisher constraints can be recovered even when noise comprises a consequential component of the measured signal.","The findings and approaches developed here are important for ensuring simulation-based analyses can be accurately used to assess upcoming survey capabilities."],"url":"http://arxiv.org/abs/2406.06067v1","category":"astro-ph.CO"}
{"created":"2024-06-10 02:57:57","title":"LOP-Field: Brain-inspired Layout-Object-Position Fields for Robotic Scene Understanding","abstract":"Spatial cognition empowers animals with remarkably efficient navigation abilities, largely depending on the scene-level understanding of spatial environments. Recently, it has been found that a neural population in the postrhinal cortex of rat brains is more strongly tuned to the spatial layout rather than objects in a scene. Inspired by the representations of spatial layout in local scenes to encode different regions separately, we proposed LOP-Field that realizes the Layout-Object-Position(LOP) association to model the hierarchical representations for robotic scene understanding. Powered by foundation models and implicit scene representation, a neural field is implemented as a scene memory for robots, storing a queryable representation of scenes with position-wise, object-wise, and layout-wise information. To validate the built LOP association, the model is tested to infer region information from 3D positions with quantitative metrics, achieving an average accuracy of more than 88\\%. It is also shown that the proposed method using region information can achieve improved object and view localization results with text and RGB input compared to state-of-the-art localization methods.","sentences":["Spatial cognition empowers animals with remarkably efficient navigation abilities, largely depending on the scene-level understanding of spatial environments.","Recently, it has been found that a neural population in the postrhinal cortex of rat brains is more strongly tuned to the spatial layout rather than objects in a scene.","Inspired by the representations of spatial layout in local scenes to encode different regions separately, we proposed LOP-Field that realizes the Layout-Object-Position(LOP) association to model the hierarchical representations for robotic scene understanding.","Powered by foundation models and implicit scene representation, a neural field is implemented as a scene memory for robots, storing a queryable representation of scenes with position-wise, object-wise, and layout-wise information.","To validate the built LOP association, the model is tested to infer region information from 3D positions with quantitative metrics, achieving an average accuracy of more than 88\\%.","It is also shown that the proposed method using region information can achieve improved object and view localization results with text and RGB input compared to state-of-the-art localization methods."],"url":"http://arxiv.org/abs/2406.05985v1","category":"cs.RO"}
{"created":"2024-06-10 02:15:49","title":"Well-posedness for a class of pseudo-differential hyperbolic equations on the torus","abstract":"In this paper we establish the well-posedness of the Cauchy problem for a class of pseudo-differential hyperbolic equations on the torus. The class considered here includes a space-like fractional order Laplacians. By applying the toroidal pseudo-differential calculus we establish regularity estimates, existence and uniqueness in the scale of the standard Sobolev spaces on the torus","sentences":["In this paper we establish the well-posedness of the Cauchy problem for a class of pseudo-differential hyperbolic equations on the torus.","The class considered here includes a space-like fractional order Laplacians.","By applying the toroidal pseudo-differential calculus we establish regularity estimates, existence and uniqueness in the scale of the standard Sobolev spaces on the torus"],"url":"http://arxiv.org/abs/2406.05973v1","category":"math.AP"}
{"created":"2024-06-10 02:11:04","title":"Deformations of swallowtails in a 3-dimensional space form","abstract":"This is a continuation of the authors' earlier work on deformations of cuspidal edges. We give a representation formula for swallowtails in the Euclidean 3-space. Using this, we investigate map germs of generic swallowtails in 3-dimensional space from, and show some important properties of them. In particular, we give a representation formula giving all map germs of swallowtails in the Euclidean 3-space whose Gaussian curvatures are bounded from below by a positive constant or by a negative constant from above. Using this, we show that any swallowtails are deformed into a swallowtail of constant Gaussian curvature preserving the sign of their Gaussian curvatures.","sentences":["This is a continuation of the authors' earlier work on deformations of cuspidal edges.","We give a representation formula for swallowtails in the Euclidean 3-space.","Using this, we investigate map germs of generic swallowtails in 3-dimensional space from, and show some important properties of them.","In particular, we give a representation formula giving all map germs of swallowtails in the Euclidean 3-space whose Gaussian curvatures are bounded from below by a positive constant or by a negative constant from above.","Using this, we show that any swallowtails are deformed into a swallowtail of constant Gaussian curvature preserving the sign of their Gaussian curvatures."],"url":"http://arxiv.org/abs/2406.05971v1","category":"math.DG"}
{"created":"2024-06-10 01:39:20","title":"On Conjecture of Binomial Edge Ideals of Linear Type","abstract":"An ideal $I$ of a commutative ring $R$ is said to be of {\\emph{linear type}} when its Rees algebra and symmetric algebra exhibit isomorphism. In this paper, we investigate the conjecture put forth by Jayanthan, Kumar, and Sarkar (2021) that if $G$ is a tree or a unicyclic graph, then the binomial edge ideal of $G$ is of linear type. Our investigation validates this conjecture for trees. However, our study reveals that not all unicyclic graphs adhere to this conjecture. Furthermore, we provide an explicit description of the defining ideal of the Rees algebra of binomial edge ideals of trees and demonstrate that these algebras are Cohen-Macaulay. Finally, we conclude the equality between symbolic powers and ordinary powers of binomial edge ideals of trees.","sentences":["An ideal $I$ of a commutative ring $R$ is said to be of {\\emph{linear type}} when its Rees algebra and symmetric algebra exhibit isomorphism.","In this paper, we investigate the conjecture put forth by Jayanthan, Kumar, and Sarkar (2021) that if $G$ is a tree or a unicyclic graph, then the binomial edge ideal of $G$ is of linear type.","Our investigation validates this conjecture for trees.","However, our study reveals that not all unicyclic graphs adhere to this conjecture.","Furthermore, we provide an explicit description of the defining ideal of the Rees algebra of binomial edge ideals of trees and demonstrate that these algebras are Cohen-Macaulay.","Finally, we conclude the equality between symbolic powers and ordinary powers of binomial edge ideals of trees."],"url":"http://arxiv.org/abs/2406.05960v1","category":"math.AC"}
{"created":"2024-06-10 00:13:45","title":"Minimal Cuts and Genealogical Constraints on Feynman Integrals","abstract":"We introduce an efficient method for deriving hierarchical constraints on the discontinuities of individual Feynman integrals. This method can be applied at any loop order and particle multiplicity, and to any configuration of massive or massless virtual particles. The resulting constraints hold to all orders in dimensional regularization, and complement the extended Steinmann relations -- which restrict adjacent sequential discontinuities -- by disallowing ordered pairs of discontinuities from appearing even when separated by (any number of) other discontinuities. We focus on a preferred class of hierarchical constraints, which we refer to as \\emph{genealogical constraints}, that govern what singularities can follow from certain \\emph{minimal cuts} that act as the primogenitors of the discontinuities that appear in Feynman integrals. While deriving the full set of hierarchical constraints on a given Feynman integral generally requires identifying all solutions to the (blown up) Landau equations, these genealogical constraints can be worked out with only minimal information about what singularities may appear. We illustrate the power of this new method in examples at one, two, and three loops, and provide evidence that genealogical constraints restrict the analytic structure of Feynman integrals significantly more than the extended Steinmann relations.","sentences":["We introduce an efficient method for deriving hierarchical constraints on the discontinuities of individual Feynman integrals.","This method can be applied at any loop order and particle multiplicity, and to any configuration of massive or massless virtual particles.","The resulting constraints hold to all orders in dimensional regularization, and complement the extended Steinmann relations -- which restrict adjacent sequential discontinuities -- by disallowing ordered pairs of discontinuities from appearing even when separated by (any number of) other discontinuities.","We focus on a preferred class of hierarchical constraints, which we refer to as \\emph{genealogical constraints}, that govern what singularities can follow from certain \\emph{minimal cuts} that act as the primogenitors of the discontinuities that appear in Feynman integrals.","While deriving the full set of hierarchical constraints on a given Feynman integral generally requires identifying all solutions to the (blown up) Landau equations, these genealogical constraints can be worked out with only minimal information about what singularities may appear.","We illustrate the power of this new method in examples at one, two, and three loops, and provide evidence that genealogical constraints restrict the analytic structure of Feynman integrals significantly more than the extended Steinmann relations."],"url":"http://arxiv.org/abs/2406.05943v1","category":"hep-th"}
{"created":"2024-06-09 23:23:57","title":"Hamiltonian Structure of the Guiding-center Vlasov-Maxwell Equations with Polarization and Magnetization","abstract":"The Hamiltonian formulation of guiding-center Vlasov-Maxwell equations, which contain dipole contributions to the guiding-center polarization and magnetization, is presented in terms of a guiding-center Hamiltonian functional that is derived from the exact guiding-center Vlasov-Maxwell energy conservation law, and an antisymmetric functional bracket that satisfies the Jacobi property. Exact energy-momentum and angular momentum conservation laws are expressed in Hamiltonian form and the guiding-center Vlasov-Maxwell entropy functional is shown to be a Casimir functional.","sentences":["The Hamiltonian formulation of guiding-center Vlasov-Maxwell equations, which contain dipole contributions to the guiding-center polarization and magnetization, is presented in terms of a guiding-center Hamiltonian functional that is derived from the exact guiding-center Vlasov-Maxwell energy conservation law, and an antisymmetric functional bracket that satisfies the Jacobi property.","Exact energy-momentum and angular momentum conservation laws are expressed in Hamiltonian form and the guiding-center Vlasov-Maxwell entropy functional is shown to be a Casimir functional."],"url":"http://arxiv.org/abs/2406.05932v1","category":"physics.plasm-ph"}
{"created":"2024-06-09 23:23:48","title":"Differentiable Discrete Elastic Rods for Real-Time Modeling of Deformable Linear Objects","abstract":"This paper addresses the task of modeling Deformable Linear Objects (DLOs), such as ropes and cables, during dynamic motion over long time horizons. This task presents significant challenges due to the complex dynamics of DLOs. To address these challenges, this paper proposes differentiable Discrete Elastic Rods For deformable linear Objects with Real-time Modeling (DEFORM), a novel framework that combines a differentiable physics-based model with a learning framework to model DLOs accurately and in real-time. The performance of DEFORM is evaluated in an experimental setup involving two industrial robots and a variety of sensors. A comprehensive series of experiments demonstrate the efficacy of DEFORM in terms of accuracy, computational speed, and generalizability when compared to state-of-the-art alternatives. To further demonstrate the utility of DEFORM, this paper integrates it into a perception pipeline and illustrates its superior performance when compared to the state-of-the-art methods while tracking a DLO even in the presence of occlusions. Finally, this paper illustrates the superior performance of DEFORM when compared to state-of-the-art methods when it is applied to perform autonomous planning and control of DLOs.","sentences":["This paper addresses the task of modeling Deformable Linear Objects (DLOs), such as ropes and cables, during dynamic motion over long time horizons.","This task presents significant challenges due to the complex dynamics of DLOs.","To address these challenges, this paper proposes differentiable Discrete Elastic Rods For deformable linear Objects with Real-time Modeling (DEFORM), a novel framework that combines a differentiable physics-based model with a learning framework to model DLOs accurately and in real-time.","The performance of DEFORM is evaluated in an experimental setup involving two industrial robots and a variety of sensors.","A comprehensive series of experiments demonstrate the efficacy of DEFORM in terms of accuracy, computational speed, and generalizability when compared to state-of-the-art alternatives.","To further demonstrate the utility of DEFORM, this paper integrates it into a perception pipeline and illustrates its superior performance when compared to the state-of-the-art methods while tracking a DLO even in the presence of occlusions.","Finally, this paper illustrates the superior performance of DEFORM when compared to state-of-the-art methods when it is applied to perform autonomous planning and control of DLOs."],"url":"http://arxiv.org/abs/2406.05931v1","category":"cs.RO"}
{"created":"2024-06-09 22:46:41","title":"Semisupervised Neural Proto-Language Reconstruction","abstract":"Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.","sentences":["Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision.","However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data.","We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).","We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words.","We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task."],"url":"http://arxiv.org/abs/2406.05930v1","category":"cs.CL"}
{"created":"2024-06-09 22:14:55","title":"MeanSparse: Post-Training Robustness Enhancement Through Mean-Centered Feature Sparsification","abstract":"We present a simple yet effective method to improve the robustness of Convolutional Neural Networks (CNNs) against adversarial examples by post-processing an adversarially trained model. Our technique, MeanSparse, cascades the activation functions of a trained model with novel operators that sparsify mean-centered feature vectors. This is equivalent to reducing feature variations around the mean, and we show that such reduced variations merely affect the model's utility, yet they strongly attenuate the adversarial perturbations and decrease the attacker's success rate. Our experiments show that, when applied to the top models in the RobustBench leaderboard, it achieves a new robustness record of 72.08% (from 71.07%) and 59.64% (from 59.56%) on CIFAR-10 and ImageNet, respectively, in term of AutoAttack accuracy. Code is available at https://github.com/SPIN-UMass/MeanSparse","sentences":["We present a simple yet effective method to improve the robustness of Convolutional Neural Networks (CNNs) against adversarial examples by post-processing an adversarially trained model.","Our technique, MeanSparse, cascades the activation functions of a trained model with novel operators that sparsify mean-centered feature vectors.","This is equivalent to reducing feature variations around the mean, and we show that such reduced variations merely affect the model's utility, yet they strongly attenuate the adversarial perturbations and decrease the attacker's success rate.","Our experiments show that, when applied to the top models in the RobustBench leaderboard, it achieves a new robustness record of 72.08% (from 71.07%) and 59.64% (from 59.56%) on CIFAR-10 and ImageNet, respectively, in term of AutoAttack accuracy.","Code is available at https://github.com/SPIN-UMass/MeanSparse"],"url":"http://arxiv.org/abs/2406.05927v1","category":"cs.CV"}
{"created":"2024-06-09 22:06:38","title":"Plasma stratification in AC discharges in noble gases at low currents","abstract":"A hybrid kinetic-fluid model is used to study plasma stratification in alternating current (AC) discharges in noble gases at low plasma densities. Self-consistent coupled solutions of a non-local kinetic equation for electrons, a drift-diffusion equation of ions, and a Poisson equation for the electric field are obtained for a positive column with periodic boundary conditions and for the entire discharge to study near-electrode effects within a 1d model with radial ambipolar loss. A simplified two-level excitation-ionization model neglects the nonlinear effects due to stepwise ionization, gas heating, and Coulomb interactions among electrons. Standing striations are obtained for the reduced values of electric fields corresponding to the inelastic energy balance of electrons in a range of driving frequencies. An analog of Novak law (striation length proportional to the excitation threshold of atoms and inversely proportional to the mean square root of the electric field) is observed in simulations, indicating the nonlocal nature of standing striations in AC discharges at low plasma densities. Stratified plasma operates in a dynamic regime for a wide range of driving frequencies because heat conduction occurs much faster than the ambipolar diffusion controlled by slow ion motion. We demonstrate that standing striations in symmetric AC discharges previously observed in experiments are similar to moving striations in DC discharges and are both due to nonlocal kinetic effects at low discharge currents.","sentences":["A hybrid kinetic-fluid model is used to study plasma stratification in alternating current (AC) discharges in noble gases at low plasma densities.","Self-consistent coupled solutions of a non-local kinetic equation for electrons, a drift-diffusion equation of ions, and a Poisson equation for the electric field are obtained for a positive column with periodic boundary conditions and for the entire discharge to study near-electrode effects within a 1d model with radial ambipolar loss.","A simplified two-level excitation-ionization model neglects the nonlinear effects due to stepwise ionization, gas heating, and Coulomb interactions among electrons.","Standing striations are obtained for the reduced values of electric fields corresponding to the inelastic energy balance of electrons in a range of driving frequencies.","An analog of Novak law (striation length proportional to the excitation threshold of atoms and inversely proportional to the mean square root of the electric field) is observed in simulations, indicating the nonlocal nature of standing striations in AC discharges at low plasma densities.","Stratified plasma operates in a dynamic regime for a wide range of driving frequencies because heat conduction occurs much faster than the ambipolar diffusion controlled by slow ion motion.","We demonstrate that standing striations in symmetric AC discharges previously observed in experiments are similar to moving striations in DC discharges and are both due to nonlocal kinetic effects at low discharge currents."],"url":"http://arxiv.org/abs/2406.05926v1","category":"physics.plasm-ph"}
{"created":"2024-06-09 21:14:08","title":"Classical Models of the Electron Spin -- Comparison of the Electric Current Model and the Magnetic Charge Model","abstract":"Ferromagnetic matter finds its microscopic origin in the intrinsic electron spin, which is considered to be a purely quantum mechanical property of the electron. To incorporate the influence of the electron spin in the microscopic and macroscopic Maxwell equations -- and thereby in classical physics -- two models have been utilized: the electric current and the magnetic charge model. This paper aims to highlight fundamental problems of the commonly used current loop model, widely employed in textbooks. This work demonstrates that the behavior of a constant electric current dipole is not described by the laws of classical electrodynamics. More precisely, the electric current model is dependent on external forces, not included in Maxwells field and force equations, in order to maintain the force balance on the electric charge density inside the electron. These external forces change dynamically and do work on the system as the electron interacts with external fields. Consequently, the energies derived from classical physics (gravitational potential energy, kinetic energy, electrodynamic field energy) are not conserved in a system including constant electric current dipoles. In contrast to the electric current model, the magnetic charge model employs separate magnetic charges to model the electron spin, requiring the Maxwell equations to be extended by magnetic sources. This paper intends to illustrate that the magnetic charge model has significant advantages over the electric current model as it needs no external forces and energies, is a closed electromechanical system and is fully modeled by the classical laws of physics. This work forms the basis for the derivation and consideration of equivalent problems in macroscopic systems involving ferromagnetic matter.","sentences":["Ferromagnetic matter finds its microscopic origin in the intrinsic electron spin, which is considered to be a purely quantum mechanical property of the electron.","To incorporate the influence of the electron spin in the microscopic and macroscopic Maxwell equations -- and thereby in classical physics -- two models have been utilized: the electric current and the magnetic charge model.","This paper aims to highlight fundamental problems of the commonly used current loop model, widely employed in textbooks.","This work demonstrates that the behavior of a constant electric current dipole is not described by the laws of classical electrodynamics.","More precisely, the electric current model is dependent on external forces, not included in Maxwells field and force equations, in order to maintain the force balance on the electric charge density inside the electron.","These external forces change dynamically and do work on the system as the electron interacts with external fields.","Consequently, the energies derived from classical physics (gravitational potential energy, kinetic energy, electrodynamic field energy) are not conserved in a system including constant electric current dipoles.","In contrast to the electric current model, the magnetic charge model employs separate magnetic charges to model the electron spin, requiring the Maxwell equations to be extended by magnetic sources.","This paper intends to illustrate that the magnetic charge model has significant advantages over the electric current model as it needs no external forces and energies, is a closed electromechanical system and is fully modeled by the classical laws of physics.","This work forms the basis for the derivation and consideration of equivalent problems in macroscopic systems involving ferromagnetic matter."],"url":"http://arxiv.org/abs/2406.05919v1","category":"physics.class-ph"}
{"created":"2024-06-09 20:41:27","title":"Hilbert series for contractads and modular compactifications","abstract":"Contractads are operadic-type algebraic structures well-suited for describing configuration spaces indexed by a simple connected graph $\\Gamma$. Specifically, these configuration spaces are defined as $\\mathrm{Conf}_{\\Gamma}(X):=X^{|V(\\Gamma)|}\\setminus \\cup_{(ij)\\in E(\\Gamma)} {x_i=x_j}$. In this paper, we explore functional equations for the Hilbert series of Koszul dual contractads and provide explicit Hilbert series for fundamental contractads such as the commutative, Lie, associative and the little discs contractads.   Additionally, we focus on a particular contractad derived from the wonderful compactifications of $\\mathrm{Conf}_{\\Gamma}(\\mathbb{k})$, for $\\mathbb{k}=\\mathbb{R},\\mathbb{C}$. First, we demonstrate that for complete multipartite graphs, the associated wonderful compactifications coincide with the modular compactifications introduced by Smyth. Second, we establish that the homology of the complex points and the homology of the real locus of the wonderful contractad are both quadratic and Koszul contractads. We offer a detailed description of generators and relations, extending the concepts of the Hypercommutative operad and cacti operads, respectively.   Furthermore, using the functional equations for the Hilbert series, we describe the corresponding Hilbert series for the homology of modular compactifications.   We hope this work will highlight the concept of contractads and their applications in algebraic topology, algebraic geometry, and combinatorics.","sentences":["Contractads are operadic-type algebraic structures well-suited for describing configuration spaces indexed by a simple connected graph $\\Gamma$. Specifically, these configuration spaces are defined as $\\mathrm{Conf}_{\\Gamma}(X):=X^{|V(\\Gamma)|}\\setminus \\cup_{(ij)\\in E(\\Gamma)} {x_i=x_j}$.","In this paper, we explore functional equations for the Hilbert series of Koszul dual contractads and provide explicit Hilbert series for fundamental contractads such as the commutative, Lie, associative and the little discs contractads.   ","Additionally, we focus on a particular contractad derived from the wonderful compactifications of $\\mathrm{Conf}_{\\Gamma}(\\mathbb{k})$, for $\\mathbb{k}=\\mathbb{R},\\mathbb{C}$. First, we demonstrate that for complete multipartite graphs, the associated wonderful compactifications coincide with the modular compactifications introduced by Smyth.","Second, we establish that the homology of the complex points and the homology of the real locus of the wonderful contractad are both quadratic and Koszul contractads.","We offer a detailed description of generators and relations, extending the concepts of the Hypercommutative operad and cacti operads, respectively.   ","Furthermore, using the functional equations for the Hilbert series, we describe the corresponding Hilbert series for the homology of modular compactifications.   ","We hope this work will highlight the concept of contractads and their applications in algebraic topology, algebraic geometry, and combinatorics."],"url":"http://arxiv.org/abs/2406.05909v1","category":"math.QA"}
{"created":"2024-06-09 20:32:23","title":"Charged black holes in quadratic gravity","abstract":"We study electrically charged, static, spherically symmetric black holes in quadratic gravity using the conformal-to-Kundt technique, which leads to a considerable simplification of the field equations. We study the solutions using a Frobenius-like approach of power-series expansions. The indicial equations restrict the set of possible leading powers to a few cases, describing, e.g., black holes, wormholes, or naked singularities.   We focus on the black hole case and derive recurrent formulas for all series coefficients of the infinite power-series expansion around the horizon. The solution is characterized by electric charge $q$, the black-hole radius $a_0$, and the Bach parameter $b$ related to the strength of the Bach tensor at the horizon. However, the Bach parameter has to be fine-tuned to ensure asymptotic flatness. The fine-tuning of $b$ for a given $q$ and $a_0$ returns up to two values, describing two branches of asymptotically flat, static, spherically symmetric, charged black holes in quadratic gravity. This is in agreement with previous numerical works.   We discuss various physical properties of these black holes, such as their asymptotic mass, temperature, photon spheres, and black-hole shadows. A straightforward generalization to dyonic black holes in quadratic gravity is also briefly mentioned.","sentences":["We study electrically charged, static, spherically symmetric black holes in quadratic gravity using the conformal-to-Kundt technique, which leads to a considerable simplification of the field equations.","We study the solutions using a Frobenius-like approach of power-series expansions.","The indicial equations restrict the set of possible leading powers to a few cases, describing, e.g., black holes, wormholes, or naked singularities.   ","We focus on the black hole case and derive recurrent formulas for all series coefficients of the infinite power-series expansion around the horizon.","The solution is characterized by electric charge $q$, the black-hole radius $a_0$, and the Bach parameter $b$ related to the strength of the Bach tensor at the horizon.","However, the Bach parameter has to be fine-tuned to ensure asymptotic flatness.","The fine-tuning of $b$ for a given $q$ and $a_0$ returns up to two values, describing two branches of asymptotically flat, static, spherically symmetric, charged black holes in quadratic gravity.","This is in agreement with previous numerical works.   ","We discuss various physical properties of these black holes, such as their asymptotic mass, temperature, photon spheres, and black-hole shadows.","A straightforward generalization to dyonic black holes in quadratic gravity is also briefly mentioned."],"url":"http://arxiv.org/abs/2406.05908v1","category":"gr-qc"}
{"created":"2024-06-09 19:30:12","title":"A model for slowing particles in random media","abstract":"We present a simple model in dimension $d\\geq 2$ for slowing particles in random media, where point particles move in straight lines among and inside spherical identical obstacles with Poisson distributed centres. When crossing an obstacle, a particle is slowed down according to the law $\\dot{V}= -\\frac{\\kappa}{\\epsilon} S(|V|) V$, where $V$ is the velocity of the point particle, $\\kappa$ is a positive constant, $\\epsilon$ is the radius of the obstacle and $S(|V|)$ is a given slowing profile. With this choice, the slowing rate in the obstacles is such that the variation of speed at each crossing is of order $1$. We study the asymptotic limit of the particle system when $\\epsilon$ vanishes and the mean free path of the point particles stays finite. We prove the convergence of the point particles density measure to the solution of a kinetic-like equation with a collision term which includes a contribution proportional to a $\\delta$ function in $v=0$; this contribution guarantees the conservation of mass for the limit equation.","sentences":["We present a simple model in dimension $d\\geq 2$ for slowing particles in random media, where point particles move in straight lines among and inside spherical identical obstacles with Poisson distributed centres.","When crossing an obstacle, a particle is slowed down according to the law $\\dot{V}= -\\frac{\\kappa}{\\epsilon} S(|V|) V$, where $V$ is the velocity of the point particle, $\\kappa$ is a positive constant, $\\epsilon$ is the radius of the obstacle and $S(|V|)$ is a given slowing profile.","With this choice, the slowing rate in the obstacles is such that the variation of speed at each crossing is of order $1$. We study the asymptotic limit of the particle system when $\\epsilon$ vanishes and the mean free path of the point particles stays finite.","We prove the convergence of the point particles density measure to the solution of a kinetic-like equation with a collision term which includes a contribution proportional to a $\\delta$ function in $v=0$; this contribution guarantees the conservation of mass for the limit equation."],"url":"http://arxiv.org/abs/2406.05895v1","category":"math-ph"}
{"created":"2024-06-09 19:18:05","title":"Security Vulnerability Detection with Multitask Self-Instructed Fine-Tuning of Large Language Models","abstract":"Software security vulnerabilities allow attackers to perform malicious activities to disrupt software operations. Recent Transformer-based language models have significantly advanced vulnerability detection, surpassing the capabilities of static analysis based deep learning models. However, language models trained solely on code tokens do not capture either the explanation of vulnerability type or the data flow structure information of code, both of which are crucial for vulnerability detection. We propose a novel technique that integrates a multitask sequence-to-sequence LLM with pro-gram control flow graphs encoded as a graph neural network to achieve sequence-to-classification vulnerability detection. We introduce MSIVD, multitask self-instructed fine-tuning for vulnerability detection, inspired by chain-of-thought prompting and LLM self-instruction. Our experiments demonstrate that MSIVD achieves superior performance, outperforming the highest LLM-based vulnerability detector baseline (LineVul), with a F1 score of 0.92 on the BigVul dataset, and 0.48 on the PreciseBugs dataset. By training LLMs and GNNs simultaneously using a combination of code and explanatory metrics of a vulnerable program, MSIVD represents a promising direction for advancing LLM-based vulnerability detection that generalizes to unseen data. Based on our findings, we further discuss the necessity for new labelled security vulnerability datasets, as recent LLMs have seen or memorized prior datasets' held-out evaluation data.","sentences":["Software security vulnerabilities allow attackers to perform malicious activities to disrupt software operations.","Recent Transformer-based language models have significantly advanced vulnerability detection, surpassing the capabilities of static analysis based deep learning models.","However, language models trained solely on code tokens do not capture either the explanation of vulnerability type or the data flow structure information of code, both of which are crucial for vulnerability detection.","We propose a novel technique that integrates a multitask sequence-to-sequence LLM with pro-gram control flow graphs encoded as a graph neural network to achieve sequence-to-classification vulnerability detection.","We introduce MSIVD, multitask self-instructed fine-tuning for vulnerability detection, inspired by chain-of-thought prompting and LLM self-instruction.","Our experiments demonstrate that MSIVD achieves superior performance, outperforming the highest LLM-based vulnerability detector baseline (LineVul), with a F1 score of 0.92 on the BigVul dataset, and 0.48 on the PreciseBugs dataset.","By training LLMs and GNNs simultaneously using a combination of code and explanatory metrics of a vulnerable program, MSIVD represents a promising direction for advancing LLM-based vulnerability detection that generalizes to unseen data.","Based on our findings, we further discuss the necessity for new labelled security vulnerability datasets, as recent LLMs have seen or memorized prior datasets' held-out evaluation data."],"url":"http://arxiv.org/abs/2406.05892v1","category":"cs.CR"}
{"created":"2024-06-09 18:54:49","title":"Electronic Wigner-Molecule Polymeric Chains in Elongated Silicon Quantum Dots and Finite-Length Quantum Wires","abstract":"The spectral properties of electrons confined in a wire-like quasi-one-dimensional (1D) elongated quantum dot (EQD) coupler between silicon qubits, are investigated with a newly developed valley-augmented unrestricted Hartree-Fock (va-UHF) method, generalized to include the valley degree of freedom treated as an isospin, allowing calculations for a large number of electrons. The lower energy symmetry-broken solutions of the self-consistent generalized Pople-Nesbet equations exhibit, for a confinement that has been modeled after an experimentally fabricated one in silicon, formation of Wigner-molecular polymeric (longitudinal) chains, initiating through charge accumulation at the edges of the finite-length quasi-1D wire. An increasing number of parallel zig-zag chains form as the number of electrons loaded into the confinement is increased, with the formation of newly added chains determined by the strength of the transverse harmonic confinement. The broken-symmetry va-UHF solutions, subsequently augmented by the quantum-mechanically required parity-restoration, go beyond the va-UHF single-determinant solution, predicting formation of entangled Wigner-molecular chains whose charge distributions obliterate the zig-zag organization of the broken-symmetry solutions. The symmetry-restored va-UHF methodology enables systematic investigations of multi-electron complex nano-scale confined structures that could be targeted for future imaging microscopy experiments in silicon and other materials (e.g., 1D domain walls in TMD materials), and quantum information utilization.","sentences":["The spectral properties of electrons confined in a wire-like quasi-one-dimensional (1D) elongated quantum dot (EQD) coupler between silicon qubits, are investigated with a newly developed valley-augmented unrestricted Hartree-Fock (va-UHF) method, generalized to include the valley degree of freedom treated as an isospin, allowing calculations for a large number of electrons.","The lower energy symmetry-broken solutions of the self-consistent generalized Pople-Nesbet equations exhibit, for a confinement that has been modeled after an experimentally fabricated one in silicon, formation of Wigner-molecular polymeric (longitudinal) chains, initiating through charge accumulation at the edges of the finite-length quasi-1D wire.","An increasing number of parallel zig-zag chains form as the number of electrons loaded into the confinement is increased, with the formation of newly added chains determined by the strength of the transverse harmonic confinement.","The broken-symmetry va-UHF solutions, subsequently augmented by the quantum-mechanically required parity-restoration, go beyond the va-UHF single-determinant solution, predicting formation of entangled Wigner-molecular chains whose charge distributions obliterate the zig-zag organization of the broken-symmetry solutions.","The symmetry-restored va-UHF methodology enables systematic investigations of multi-electron complex nano-scale confined structures that could be targeted for future imaging microscopy experiments in silicon and other materials (e.g., 1D domain walls in TMD materials), and quantum information utilization."],"url":"http://arxiv.org/abs/2406.05886v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-09 17:38:50","title":"Information scrambling in quantum-walks","abstract":"We study information scrambling -- a spread of initially localized quantum information into the system's many degree of freedom -- in discrete-time quantum walks. We consider out-of-time-ordered correlators (OTOC) and K-complexity as probe of information scrambling. The OTOC for local spin operators in all directions has a light-cone structure which is ``shell-like''. As the wavefront passes, the OTOC approaches to zero in the long-time limit, showing no signature of scrambling. The introduction of spatial or temporal disorder changes the shape of the light-cone akin to localization of wavefuction. We formulate the K-complexity in system with discrete-time evolution, and show that it grows linearly in discrete-time quantum walk. The presence of disorder modifies this growth to sub-linear. Our study present interesting case to explore many-body phenomenon in discrete-time quantum walk using scrambling.","sentences":["We study information scrambling -- a spread of initially localized quantum information into the system's many degree of freedom -- in discrete-time quantum walks.","We consider out-of-time-ordered correlators (OTOC) and K-complexity as probe of information scrambling.","The OTOC for local spin operators in all directions has a light-cone structure which is ``shell-like''.","As the wavefront passes, the OTOC approaches to zero in the long-time limit, showing no signature of scrambling.","The introduction of spatial or temporal disorder changes the shape of the light-cone akin to localization of wavefuction.","We formulate the K-complexity in system with discrete-time evolution, and show that it grows linearly in discrete-time quantum walk.","The presence of disorder modifies this growth to sub-linear.","Our study present interesting case to explore many-body phenomenon in discrete-time quantum walk using scrambling."],"url":"http://arxiv.org/abs/2406.05865v1","category":"quant-ph"}
{"created":"2024-06-09 17:03:56","title":"Comments on \"Federated Learning with Differential Privacy: Algorithms and Performance Analysis\"","abstract":"In the paper by Wei et al. (\"Federated Learning with Differential Privacy: Algorithms and Performance Analysis\"), the convergence performance of the proposed differential privacy algorithm in federated learning (FL), known as Noising before Model Aggregation FL (NbAFL), was studied. However, the presented convergence upper bound of NbAFL (Theorem 2) is incorrect. This comment aims to present the correct form of the convergence upper bound for NbAFL.","sentences":["In the paper by Wei et al. (\"Federated Learning with Differential Privacy: Algorithms and Performance Analysis\"), the convergence performance of the proposed differential privacy algorithm in federated learning (FL), known as Noising before Model Aggregation FL (NbAFL), was studied.","However, the presented convergence upper bound of NbAFL (Theorem 2) is incorrect.","This comment aims to present the correct form of the convergence upper bound for NbAFL."],"url":"http://arxiv.org/abs/2406.05858v1","category":"cs.DC"}
{"created":"2024-06-09 16:49:39","title":"RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for Realistic Rendering","abstract":"3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of neural rendering, 3D scene reconstruction, and novel view synthesis. Nevertheless, 3D-GS encounters the main challenge when it comes to accurately representing physical reflections, especially in the case of total reflection and semi-reflection that are commonly found in real-world scenes. This limitation causes reflections to be mistakenly treated as independent elements with physical presence, leading to imprecise reconstructions. Herein, to tackle this challenge, we propose RefGaussian to disentangle reflections from 3D-GS for realistically modeling reflections. Specifically, we propose to split a scene into transmitted and reflected components and represent these components using two Spherical Harmonics (SH). Given that this decomposition is not fully determined, we employ local regularization techniques to ensure local smoothness for both the transmitted and reflected components, thereby achieving more plausible decomposition outcomes than 3D-GS. Experimental results demonstrate that our approach achieves superior novel view synthesis and accurate depth estimation outcomes. Furthermore, it enables the utilization of scene editing applications, ensuring both high-quality results and physical coherence.","sentences":["3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of neural rendering, 3D scene reconstruction, and novel view synthesis.","Nevertheless, 3D-GS encounters the main challenge when it comes to accurately representing physical reflections, especially in the case of total reflection and semi-reflection that are commonly found in real-world scenes.","This limitation causes reflections to be mistakenly treated as independent elements with physical presence, leading to imprecise reconstructions.","Herein, to tackle this challenge, we propose RefGaussian to disentangle reflections from 3D-GS for realistically modeling reflections.","Specifically, we propose to split a scene into transmitted and reflected components and represent these components using two Spherical Harmonics (SH).","Given that this decomposition is not fully determined, we employ local regularization techniques to ensure local smoothness for both the transmitted and reflected components, thereby achieving more plausible decomposition outcomes than 3D-GS.","Experimental results demonstrate that our approach achieves superior novel view synthesis and accurate depth estimation outcomes.","Furthermore, it enables the utilization of scene editing applications, ensuring both high-quality results and physical coherence."],"url":"http://arxiv.org/abs/2406.05852v1","category":"cs.CV"}
{"created":"2024-06-09 16:49:19","title":"Scaling Graph Convolutions for Mobile Vision","abstract":"To compete with existing mobile architectures, MobileViG introduces Sparse Vision Graph Attention (SVGA), a fast token-mixing operator based on the principles of GNNs. However, MobileViG scales poorly with model size, falling at most 1% behind models with similar latency. This paper introduces Mobile Graph Convolution (MGC), a new vision graph neural network (ViG) module that solves this scaling problem. Our proposed mobile vision architecture, MobileViGv2, uses MGC to demonstrate the effectiveness of our approach. MGC improves on SVGA by increasing graph sparsity and introducing conditional positional encodings to the graph operation. Our smallest model, MobileViGv2-Ti, achieves a 77.7% top-1 accuracy on ImageNet-1K, 2% higher than MobileViG-Ti, with 0.9 ms inference latency on the iPhone 13 Mini NPU. Our largest model, MobileViGv2-B, achieves an 83.4% top-1 accuracy, 0.8% higher than MobileViG-B, with 2.7 ms inference latency. Besides image classification, we show that MobileViGv2 generalizes well to other tasks. For object detection and instance segmentation on MS COCO 2017, MobileViGv2-M outperforms MobileViG-M by 1.2 $AP^{box}$ and 0.7 $AP^{mask}$, and MobileViGv2-B outperforms MobileViG-B by 1.0 $AP^{box}$ and 0.7 $AP^{mask}$. For semantic segmentation on ADE20K, MobileViGv2-M achieves 42.9% $mIoU$ and MobileViGv2-B achieves 44.3% $mIoU$. Our code can be found at \\url{https://github.com/SLDGroup/MobileViGv2}.","sentences":["To compete with existing mobile architectures, MobileViG introduces Sparse Vision Graph Attention (SVGA), a fast token-mixing operator based on the principles of GNNs.","However, MobileViG scales poorly with model size, falling at most 1% behind models with similar latency.","This paper introduces Mobile Graph Convolution (MGC), a new vision graph neural network (ViG) module that solves this scaling problem.","Our proposed mobile vision architecture, MobileViGv2, uses MGC to demonstrate the effectiveness of our approach.","MGC improves on SVGA by increasing graph sparsity and introducing conditional positional encodings to the graph operation.","Our smallest model, MobileViGv2-Ti, achieves a 77.7% top-1 accuracy on ImageNet-1K, 2% higher than MobileViG-Ti, with 0.9 ms inference latency on the iPhone 13 Mini NPU.","Our largest model, MobileViGv2-B, achieves an 83.4% top-1 accuracy, 0.8% higher than MobileViG-B, with 2.7 ms inference latency.","Besides image classification, we show that MobileViGv2 generalizes well to other tasks.","For object detection and instance segmentation on MS COCO 2017, MobileViGv2-M outperforms MobileViG-M by 1.2 $AP^{box}$ and 0.7 $AP^{mask}$, and MobileViGv2-B outperforms MobileViG-B by 1.0 $AP^{box}$ and 0.7 $AP^{mask}$. For semantic segmentation on ADE20K, MobileViGv2-M achieves 42.9% $mIoU$ and MobileViGv2-B achieves 44.3% $mIoU$. Our code can be found at \\url{https://github.com/SLDGroup/MobileViGv2}."],"url":"http://arxiv.org/abs/2406.05850v1","category":"cs.CV"}
{"created":"2024-06-09 16:06:15","title":"Replica symmetry breaking in spin glasses in the replica-free Keldysh formalism","abstract":"At asymptotically late times ultrametricity can emerge from the persistent slow aging dynamics of the glass phase. We show that this suffices to recover the breaking of replica symmetry in mean-field spin glasses from the late time limit of the time evolution using the Keldysh path integral. This provides an alternative approach to replica symmetry breaking by connecting it rigorously to the dynamic formulation. Stationary spin glasses are thereby understood to spontaneously break thermal symmetry, or the Kubo-Martin-Schwinger relation of a state in global thermal equilibrium. We demonstrate our general statements for the spherical quantum $p$-spin model and the quantum Sherrington-Kirkpatrick model in the presence of transverse and longitudinal fields. In doing so, we also derive their dynamical Ginzburg-Landau effective Keldysh actions starting from microscopic quantum models.","sentences":["At asymptotically late times ultrametricity can emerge from the persistent slow aging dynamics of the glass phase.","We show that this suffices to recover the breaking of replica symmetry in mean-field spin glasses from the late time limit of the time evolution using the Keldysh path integral.","This provides an alternative approach to replica symmetry breaking by connecting it rigorously to the dynamic formulation.","Stationary spin glasses are thereby understood to spontaneously break thermal symmetry, or the Kubo-Martin-Schwinger relation of a state in global thermal equilibrium.","We demonstrate our general statements for the spherical quantum $p$-spin model and the quantum Sherrington-Kirkpatrick model in the presence of transverse and longitudinal fields.","In doing so, we also derive their dynamical Ginzburg-Landau effective Keldysh actions starting from microscopic quantum models."],"url":"http://arxiv.org/abs/2406.05842v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-09 15:03:36","title":"What Can We Learn from State Space Models for Machine Learning on Graphs?","abstract":"Machine learning on graphs has recently found extensive applications across domains. However, the commonly used Message Passing Neural Networks (MPNNs) suffer from limited expressive power and struggle to capture long-range dependencies. Graph transformers offer a strong alternative due to their global attention mechanism, but they come with great computational overheads, especially for large graphs. In recent years, State Space Models (SSMs) have emerged as a compelling approach to replace full attention in transformers to model sequential data. It blends the strengths of RNNs and CNNs, offering a) efficient computation, b) the ability to capture long-range dependencies, and c) good generalization across sequences of various lengths. However, extending SSMs to graph-structured data presents unique challenges due to the lack of canonical node ordering in graphs. In this work, we propose Graph State Space Convolution (GSSC) as a principled extension of SSMs to graph-structured data. By leveraging global permutation-equivariant set aggregation and factorizable graph kernels that rely on relative node distances as the convolution kernels, GSSC preserves all three advantages of SSMs. We demonstrate the provably stronger expressiveness of GSSC than MPNNs in counting graph substructures and show its effectiveness across 10 real-world, widely used benchmark datasets, where GSSC achieves best results on 7 out of 10 datasets with all significant improvements compared to the state-of-the-art baselines and second-best results on the other 3 datasets. Our findings highlight the potential of GSSC as a powerful and scalable model for graph machine learning. Our code is available at https://github.com/Graph-COM/GSSC.","sentences":["Machine learning on graphs has recently found extensive applications across domains.","However, the commonly used Message Passing Neural Networks (MPNNs) suffer from limited expressive power and struggle to capture long-range dependencies.","Graph transformers offer a strong alternative due to their global attention mechanism, but they come with great computational overheads, especially for large graphs.","In recent years, State Space Models (SSMs) have emerged as a compelling approach to replace full attention in transformers to model sequential data.","It blends the strengths of RNNs and CNNs, offering a) efficient computation, b) the ability to capture long-range dependencies, and c) good generalization across sequences of various lengths.","However, extending SSMs to graph-structured data presents unique challenges due to the lack of canonical node ordering in graphs.","In this work, we propose Graph State Space Convolution (GSSC) as a principled extension of SSMs to graph-structured data.","By leveraging global permutation-equivariant set aggregation and factorizable graph kernels that rely on relative node distances as the convolution kernels, GSSC preserves all three advantages of SSMs.","We demonstrate the provably stronger expressiveness of GSSC than MPNNs in counting graph substructures and show its effectiveness across 10 real-world, widely used benchmark datasets, where GSSC achieves best results on 7 out of 10 datasets with all significant improvements compared to the state-of-the-art baselines and second-best results on the other 3 datasets.","Our findings highlight the potential of GSSC as a powerful and scalable model for graph machine learning.","Our code is available at https://github.com/Graph-COM/GSSC."],"url":"http://arxiv.org/abs/2406.05815v1","category":"cs.LG"}
{"created":"2024-06-09 14:45:54","title":"The Landau--Lifshitz--Bloch equation: Unique existence and finite element approximation","abstract":"The Landau--Lifshitz--Bloch equation (LLBE) describes the evolution of magnetic spin field in a ferromagnet at high temperatures. We consider a viscous (pseudo-parabolic) regularisation of the LLBE for temperatures higher than the Curie temperature, which we call the $\\epsilon$-LLBE. Variants of the $\\epsilon$-LLBE are applicable to model pattern formation, phase transition, and heat conduction for non-simple materials, among other things. In this paper, we show well-posedness of the $\\epsilon$-LLBE and the convergence of the solution $\\boldsymbol{u}^\\epsilon$ of the regularised equation to the solution $\\boldsymbol{u}$ of the LLBE as $\\epsilon\\to 0^+$. As a by-product of our analysis, we show the existence and uniqueness of regular solution to the LLBE for temperatures higher than the Curie temperature. Furthermore, we propose a linear fully discrete conforming finite element scheme to approximate the solution of the $\\epsilon$-LLBE. Error analysis is performed to show unconditional stability and optimal uniform-in-time convergence rate for the schemes. Several numerical simulations corroborate our theoretical results.","sentences":["The Landau--Lifshitz--Bloch equation (LLBE) describes the evolution of magnetic spin field in a ferromagnet at high temperatures.","We consider a viscous (pseudo-parabolic) regularisation of the LLBE for temperatures higher than the Curie temperature, which we call the $\\epsilon$-LLBE.","Variants of the $\\epsilon$-LLBE are applicable to model pattern formation, phase transition, and heat conduction for non-simple materials, among other things.","In this paper, we show well-posedness of the $\\epsilon$-LLBE and the convergence of the solution $\\boldsymbol{u}^\\epsilon$ of the regularised equation to the solution $\\boldsymbol{u}$ of the LLBE as $\\epsilon\\to 0^+$. As a by-product of our analysis, we show the existence and uniqueness of regular solution to the LLBE for temperatures higher than the Curie temperature.","Furthermore, we propose a linear fully discrete conforming finite element scheme to approximate the solution of the $\\epsilon$-LLBE.","Error analysis is performed to show unconditional stability and optimal uniform-in-time convergence rate for the schemes.","Several numerical simulations corroborate our theoretical results."],"url":"http://arxiv.org/abs/2406.05808v1","category":"math.NA"}
{"created":"2024-06-09 14:45:25","title":"Reflected Mckean-Vlasov stochastic differential equations with jumps in time-dependent domains","abstract":"In this paper, we investigate the deterministic multidimensional Skorokhod problem with normal reflection in a family of time-dependent convex domains that are c\\`adl\\`ag with respect to the Hausdorff metric. We then show the existence and uniqueness of solutions to multidimensional McKean-Vlasov stochastic differential equations reflected in these time-dependent domains. Additionally, we derive stability properties with respect to the initial condition and the coefficients. Finally, we establish a propagation of chaos result.","sentences":["In this paper, we investigate the deterministic multidimensional Skorokhod problem with normal reflection in a family of time-dependent convex domains that are c\\`adl\\`ag with respect to the Hausdorff metric.","We then show the existence and uniqueness of solutions to multidimensional McKean-Vlasov stochastic differential equations reflected in these time-dependent domains.","Additionally, we derive stability properties with respect to the initial condition and the coefficients.","Finally, we establish a propagation of chaos result."],"url":"http://arxiv.org/abs/2406.05807v1","category":"math.PR"}
{"created":"2024-06-09 14:11:34","title":"Time Evolution of Relativistic Quantum Fields in Spatial Subregions","abstract":"We study the time evolution of a state of a relativistic quantum field theory restricted to a spatial subregion $\\Omega$. More precisely, we use the Feynman-Vernon influence functional formalism to describe the dynamics of the field theory in the interior of $\\Omega$ arising after integrating out the degrees of freedom in the exterior. We show how the influence of the environment gets encoded in a boundary term. Furthermore, we derive a stochastic equation of motion for the field expectation value in the interior. We find that the boundary conditions obtained in this way are energy non-conserving and non-local in space and time. Our results find applications in understanding the emergence of local thermalization in relativistic quantum field theories and the relationship between quantum field theory and relativistic fluid dynamics.","sentences":["We study the time evolution of a state of a relativistic quantum field theory restricted to a spatial subregion $\\Omega$. More precisely, we use the Feynman-Vernon influence functional formalism to describe the dynamics of the field theory in the interior of $\\Omega$ arising after integrating out the degrees of freedom in the exterior.","We show how the influence of the environment gets encoded in a boundary term.","Furthermore, we derive a stochastic equation of motion for the field expectation value in the interior.","We find that the boundary conditions obtained in this way are energy non-conserving and non-local in space and time.","Our results find applications in understanding the emergence of local thermalization in relativistic quantum field theories and the relationship between quantum field theory and relativistic fluid dynamics."],"url":"http://arxiv.org/abs/2406.05795v1","category":"hep-th"}
{"created":"2024-06-09 13:21:19","title":"Open problems and perspectives on solving Friedrichs systems by Krylov approximation","abstract":"We set up, at the abstract Hilbert space setting, the general question on when an inverse linear problem induced by an operator of Friedrichs type admits solutions belonging to (the closure of) the Krylov subspace associated to such operator. Such Krylov solvability of abstract Friedrichs systems allows to predict when, for concrete differential inverse problems, truncation algorithms can or cannot reproduce the exact solutions in terms of approximants from the Krylov subspace.","sentences":["We set up, at the abstract Hilbert space setting, the general question on when an inverse linear problem induced by an operator of Friedrichs type admits solutions belonging to (the closure of) the Krylov subspace associated to such operator.","Such Krylov solvability of abstract Friedrichs systems allows to predict when, for concrete differential inverse problems, truncation algorithms can or cannot reproduce the exact solutions in terms of approximants from the Krylov subspace."],"url":"http://arxiv.org/abs/2406.05777v1","category":"math.FA"}
{"created":"2024-06-09 13:03:14","title":"White Dwarf Envelops and Temperature Corrections in Exponential $f(T)$ Gravity","abstract":"Compact stars have long served as a test bed of gravitational models and their coupling with stellar matter. In this work, we explore the behavior of an exponential model in f(T) gravity through the Tolman-Oppenheimer-Volkoff equation. This is performed for different envelope thicknesses. Finally, constraints on the models parameters are obtained, which are comparable to the results obtained using cosmological survey data. This consistency across the strong astrophysical and weak cosmological scales shows reasonable viability of the underlying model.","sentences":["Compact stars have long served as a test bed of gravitational models and their coupling with stellar matter.","In this work, we explore the behavior of an exponential model in f(T) gravity through the Tolman-Oppenheimer-Volkoff equation.","This is performed for different envelope thicknesses.","Finally, constraints on the models parameters are obtained, which are comparable to the results obtained using cosmological survey data.","This consistency across the strong astrophysical and weak cosmological scales shows reasonable viability of the underlying model."],"url":"http://arxiv.org/abs/2406.05771v1","category":"gr-qc"}
{"created":"2024-06-09 11:28:55","title":"Peptide Vaccine Design by Evolutionary Multi-Objective Optimization","abstract":"Peptide vaccines are growing in significance for fighting diverse diseases. Machine learning has improved the identification of peptides that can trigger immune responses, and the main challenge of peptide vaccine design now lies in selecting an effective subset of peptides due to the allelic diversity among individuals. Previous works mainly formulated this task as a constrained optimization problem, aiming to maximize the expected number of peptide-Major Histocompatibility Complex (peptide-MHC) bindings across a broad range of populations by selecting a subset of diverse peptides with limited size; and employed a greedy algorithm, whose performance, however, may be limited due to the greedy nature. In this paper, we propose a new framework PVD-EMO based on Evolutionary Multi-objective Optimization, which reformulates Peptide Vaccine Design as a bi-objective optimization problem that maximizes the expected number of peptide-MHC bindings and minimizes the number of selected peptides simultaneously, and employs a Multi-Objective Evolutionary Algorithm (MOEA) to solve it. We also incorporate warm-start and repair strategies into MOEAs to improve efficiency and performance. We prove that the warm-start strategy ensures that PVD-EMO maintains the same worst-case approximation guarantee as the previous greedy algorithm, and meanwhile, the EMO framework can help avoid local optima. Experiments on a peptide vaccine design for COVID-19, caused by the SARS-CoV-2 virus, demonstrate the superiority of PVD-EMO.","sentences":["Peptide vaccines are growing in significance for fighting diverse diseases.","Machine learning has improved the identification of peptides that can trigger immune responses, and the main challenge of peptide vaccine design now lies in selecting an effective subset of peptides due to the allelic diversity among individuals.","Previous works mainly formulated this task as a constrained optimization problem, aiming to maximize the expected number of peptide-Major Histocompatibility Complex (peptide-MHC) bindings across a broad range of populations by selecting a subset of diverse peptides with limited size; and employed a greedy algorithm, whose performance, however, may be limited due to the greedy nature.","In this paper, we propose a new framework PVD-EMO based on Evolutionary Multi-objective Optimization, which reformulates Peptide Vaccine Design as a bi-objective optimization problem that maximizes the expected number of peptide-MHC bindings and minimizes the number of selected peptides simultaneously, and employs a Multi-Objective Evolutionary Algorithm (MOEA) to solve it.","We also incorporate warm-start and repair strategies into MOEAs to improve efficiency and performance.","We prove that the warm-start strategy ensures that PVD-EMO maintains the same worst-case approximation guarantee as the previous greedy algorithm, and meanwhile, the EMO framework can help avoid local optima.","Experiments on a peptide vaccine design for COVID-19, caused by the SARS-CoV-2 virus, demonstrate the superiority of PVD-EMO."],"url":"http://arxiv.org/abs/2406.05743v1","category":"cs.NE"}
{"created":"2024-06-09 10:58:04","title":"Relativistic Corrections to the CBF Effective Nuclear Hamiltonian","abstract":"We discuss the inclusion of relativistic boost corrections into the CBF effective nuclear Hamiltonian, derived from a realistic model of two- and three-nucleon interactions using the formalism of correlated basis functions and the cluster expansion technique. Different procedures to take into account the effects of boost interactions are compared on the basis of the ability to reproduce the nuclear matter equation of state obtained from accurate many-body calculations. The results of our study show that the repulsive contribution of the boost interaction significantly depends on the underlying model of the non relativistic potential. On the other hand, the dominant relativistic correction turns out to be the corresponding reduction of the strength of repulsive three-nucleon interactions, leading to a significant softening of the equation of state at supranuclear densities.","sentences":["We discuss the inclusion of relativistic boost corrections into the CBF effective nuclear Hamiltonian, derived from a realistic model of two- and three-nucleon interactions using the formalism of correlated basis functions and the cluster expansion technique.","Different procedures to take into account the effects of boost interactions are compared on the basis of the ability to reproduce the nuclear matter equation of state obtained from accurate many-body calculations.","The results of our study show that the repulsive contribution of the boost interaction significantly depends on the underlying model of the non relativistic potential.","On the other hand, the dominant relativistic correction turns out to be the corresponding reduction of the strength of repulsive three-nucleon interactions, leading to a significant softening of the equation of state at supranuclear densities."],"url":"http://arxiv.org/abs/2406.05732v1","category":"nucl-th"}
{"created":"2024-06-09 10:48:31","title":"Lorenz equations and the figure eight knot","abstract":"Lorenz equations were first presented in 1963 by Edward Lorenz, they depend on three real positive parameters. For some of these parameters which are called T-points, there are two heteroclinic orbits connecting the three singular points in the equations. The heteroclinic connections can be extended into an invariant curve passing through infinity. We consider the system at the second T-point parameter, and develop a geometric model for the flow that simulates the Lorenz dynamics there. We show that the model contains infinitely many periodic orbits, and that as knots they are all positive, prime and fibered.","sentences":["Lorenz equations were first presented in 1963 by Edward Lorenz, they depend on three real positive parameters.","For some of these parameters which are called T-points, there are two heteroclinic orbits connecting the three singular points in the equations.","The heteroclinic connections can be extended into an invariant curve passing through infinity.","We consider the system at the second T-point parameter, and develop a geometric model for the flow that simulates the Lorenz dynamics there.","We show that the model contains infinitely many periodic orbits, and that as knots they are all positive, prime and fibered."],"url":"http://arxiv.org/abs/2406.05730v1","category":"math.DS"}
{"created":"2024-06-09 10:20:29","title":"Multi-temperature atomic ensemble: nonequilibrium evolution after ultrafast electronic excitation","abstract":"Ultrafast laser radiation or beams of fast charged particles primarily excite the electronic system of a solid target driving it transiently out of thermal equilibrium. Apart from the nonequilibrium between the electrons and atoms, each of the subsystems may themselves be far from equilibrium. We demonstrate that the transient state of the atomic system may be tracked with the generalized temperature approach in the configuration and momentum subspaces. It is shown that the definition of the kinetic temperature of atoms in the momentum subspace is unaffected by the excitation of the electronic system. We derive an expression for the configurational atomic temperature when the electronic temperature differs from the atomic one, applicable to the electronic-temperature-dependent interatomic potentials (such as ab-initio molecular dynamics simulations). It is revealed that upon ultrafast irradiation, the atomic system of a solid exists temporarily in a multi-temperature state: separate equilibria in the momentum and configurational subspaces. Complete equilibration between the various atomic temperatures takes place at longer timescales, forming the energy equipartition. Based on these results, we propose a formulation of multi-temperature heat transport equations.","sentences":["Ultrafast laser radiation or beams of fast charged particles primarily excite the electronic system of a solid target driving it transiently out of thermal equilibrium.","Apart from the nonequilibrium between the electrons and atoms, each of the subsystems may themselves be far from equilibrium.","We demonstrate that the transient state of the atomic system may be tracked with the generalized temperature approach in the configuration and momentum subspaces.","It is shown that the definition of the kinetic temperature of atoms in the momentum subspace is unaffected by the excitation of the electronic system.","We derive an expression for the configurational atomic temperature when the electronic temperature differs from the atomic one, applicable to the electronic-temperature-dependent interatomic potentials (such as ab-initio molecular dynamics simulations).","It is revealed that upon ultrafast irradiation, the atomic system of a solid exists temporarily in a multi-temperature state: separate equilibria in the momentum and configurational subspaces.","Complete equilibration between the various atomic temperatures takes place at longer timescales, forming the energy equipartition.","Based on these results, we propose a formulation of multi-temperature heat transport equations."],"url":"http://arxiv.org/abs/2406.05718v1","category":"cond-mat.other"}
{"created":"2024-06-09 08:37:11","title":"A Low Rank Neural Representation of Entropy Solutions","abstract":"We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension. The representation is a generalization of the method of characteristics and posseses a compositional form. While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear. This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure. Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics.","sentences":["We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension.","The representation is a generalization of the method of characteristics and posseses a compositional form.","While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear.","This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure.","Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics."],"url":"http://arxiv.org/abs/2406.05694v1","category":"math.NA"}
{"created":"2024-06-09 07:54:02","title":"Takagi-van der Waerden functions in metric spaces and its Lipschitz derivatives","abstract":"We introduce the Takagi-van der Waerden function with parameters $a{>}b{>}0$ by setting $f_{a,b}(x)=\\sum\\limits_{n=1}^\\infty b^n d\\big(x,S_n\\big)$, where $S_n$ is a maximal $\\frac1{a^n}$-separated set in a metric space $X$ without isolated points. So, if $X=\\mathbb R$ and $S_n=\\frac1{a^n}\\mathbb Z$ then $f_{2,1}$ is the Takagi function and $f_{10,1}$ is the van der Waerden function which are the famous examples of nowhere differentiable functions. Then we prove that the big Lipschitz derivative $\\mathrm{Lip} f_{a,b}=+\\infty$ if $a>b>2$. Moreover, if $X$ is a normed space then the little Lipschitz derivative $\\mathrm{lip} f_{a,b}=+\\infty$ for large enough $a>b$. Thus, we prove that for any open set $A$ in a metric (normed) space $X$ there exists a continuous function $f$ such that $\\mathrm{Lip} f(x)=+\\infty$ (and $\\mathrm{lip} f(x)=+\\infty$) exactly on $A$.","sentences":["We introduce the Takagi-van der Waerden function with parameters $a{>}b{>}0$ by setting $f_{a,b}(x)=\\sum\\limits_{n=1}^\\infty b^n d\\big(x,S_n\\big)$, where $S_n$ is a maximal $\\frac1{a^n}$-separated set in a metric space $X$ without isolated points.","So, if $X=\\mathbb R$ and $S_n=\\frac1{a^n}\\mathbb Z$ then $f_{2,1}$ is the Takagi function and $f_{10,1}$ is the van der Waerden function which are the famous examples of nowhere differentiable functions.","Then we prove that the big Lipschitz derivative $\\mathrm{Lip} f_{a,b}=+\\infty$ if $a>b>2$.","Moreover, if $X$ is a normed space then the little Lipschitz derivative $\\mathrm{lip} f_{a,b}=+\\infty$ for large enough $a>b$.","Thus, we prove that for any open set $A$ in a metric (normed) space $X$ there exists a continuous function $f$ such that $\\mathrm{Lip} f(x)=+\\infty$ (and $\\mathrm{lip} f(x)=+\\infty$) exactly on $A$."],"url":"http://arxiv.org/abs/2406.05684v1","category":"math.FA"}
{"created":"2024-06-09 07:32:52","title":"Thermodynamically consistent accreted crust of neutron stars: The role of proton shell effects","abstract":"Observations of accreting neutron stars are widely used to constrain the microphysical properties of superdense matter. A key ingredient in this analysis is the heating associated with nuclear reactions in the outer layers of the neutron star (crust), as well as the equation of state and composition of these layers. As recently shown, the neutron hydrostatic/diffusion (nHD) condition is valid in the inner part of the crust, where some of the neutrons are not bound to the nuclei, and this condition should be properly incorporated into crustal models. Here we construct models of the accreted crust of a neutron star, taking into account the nHD condition and proton shell effects in nuclei. For numerical illustration, we employ the recently proposed compressible liquid drop model, which incorporates shell effects. However, our approach is general and can also be used in future studies relying on more sophisticated nuclear physics models.","sentences":["Observations of accreting neutron stars are widely used to constrain the microphysical properties of superdense matter.","A key ingredient in this analysis is the heating associated with nuclear reactions in the outer layers of the neutron star (crust), as well as the equation of state and composition of these layers.","As recently shown, the neutron hydrostatic/diffusion (nHD) condition is valid in the inner part of the crust, where some of the neutrons are not bound to the nuclei, and this condition should be properly incorporated into crustal models.","Here we construct models of the accreted crust of a neutron star, taking into account the nHD condition and proton shell effects in nuclei.","For numerical illustration, we employ the recently proposed compressible liquid drop model, which incorporates shell effects.","However, our approach is general and can also be used in future studies relying on more sophisticated nuclear physics models."],"url":"http://arxiv.org/abs/2406.05680v1","category":"nucl-th"}
{"created":"2024-06-09 06:49:22","title":"General Distribution Learning: A theoretical framework for Deep Learning","abstract":"There remain numerous unanswered research questions on deep learning (DL) within the classical learning theory framework. These include the remarkable generalization capabilities of overparametrized neural networks (NNs), the efficient optimization performance despite non-convexity of objectives, the mechanism of flat minima in generalization, and the exceptional performance of deep architectures, among others. This paper introduces a novel theoretical learning framework known as General Distribution Learning (GD Learning), which is designed to address a comprehensive range of machine learning and statistical tasks, including classification, regression and parameter estimation. Departing from statistical machine learning, GD Learning focuses on the true underlying distribution. In GD Learning, learning error, corresponding to the expected error in classical statistical learning framework, is divided into fitting errors caused by models and fitting algorithms, as well as sampling errors introduced by limited sampling data. The framework significantly incorporates prior knowledge, especially in scenarios characterized by data scarcity. This integration of external knowledge helps to minimize learning errors across the entire dataset, thereby enhancing performance. Within the GD Learning framework, we demonstrate that the global optimal solution to non-convex optimization problems, such as minimizing fitting error, can be approached by minimizing the gradient norm and the non-uniformity of the eigenvalues of the model's Jacobian matrix. This insight has led to the development of the gradient structure control algorithm. GD Learning also offers a fresh perspective on the questions on deep learning, including overparameterization and non-convex optimizations, bias-variance trade-off, and the mechanism of flat minima.","sentences":["There remain numerous unanswered research questions on deep learning (DL) within the classical learning theory framework.","These include the remarkable generalization capabilities of overparametrized neural networks (NNs), the efficient optimization performance despite non-convexity of objectives, the mechanism of flat minima in generalization, and the exceptional performance of deep architectures, among others.","This paper introduces a novel theoretical learning framework known as General Distribution Learning (GD Learning), which is designed to address a comprehensive range of machine learning and statistical tasks, including classification, regression and parameter estimation.","Departing from statistical machine learning, GD Learning focuses on the true underlying distribution.","In GD Learning, learning error, corresponding to the expected error in classical statistical learning framework, is divided into fitting errors caused by models and fitting algorithms, as well as sampling errors introduced by limited sampling data.","The framework significantly incorporates prior knowledge, especially in scenarios characterized by data scarcity.","This integration of external knowledge helps to minimize learning errors across the entire dataset, thereby enhancing performance.","Within the GD Learning framework, we demonstrate that the global optimal solution to non-convex optimization problems, such as minimizing fitting error, can be approached by minimizing the gradient norm and the non-uniformity of the eigenvalues of the model's Jacobian matrix.","This insight has led to the development of the gradient structure control algorithm.","GD Learning also offers a fresh perspective on the questions on deep learning, including overparameterization and non-convex optimizations, bias-variance trade-off, and the mechanism of flat minima."],"url":"http://arxiv.org/abs/2406.05666v1","category":"cs.LG"}
{"created":"2024-06-09 06:37:09","title":"Macroscopic Market Making Games","abstract":"In continuation of the macroscopic market making \\`a la Avellaneda-Stoikov as a control problem, this paper explores its stochastic game. Concerning the price competition, each agent is compared with the best quote from the others. We start with the linear case. While constructing the solution directly, the ordering property and the dimension reduction in the equilibrium are revealed. For the non-linear case, extending the decoupling approach, we introduce a multidimensional characteristic equation to study the well-posedness of forward-backward stochastic differential equations. Properties of coefficients in the characteristic equation are obtained via non-smooth analysis. In addition to novel well-posedness results, the linear price impact arises and the impact function can be further decomposed into two parts in some examples.","sentences":["In continuation of the macroscopic market making \\`a la Avellaneda-Stoikov as a control problem, this paper explores its stochastic game.","Concerning the price competition, each agent is compared with the best quote from the others.","We start with the linear case.","While constructing the solution directly, the ordering property and the dimension reduction in the equilibrium are revealed.","For the non-linear case, extending the decoupling approach, we introduce a multidimensional characteristic equation to study the well-posedness of forward-backward stochastic differential equations.","Properties of coefficients in the characteristic equation are obtained via non-smooth analysis.","In addition to novel well-posedness results, the linear price impact arises and the impact function can be further decomposed into two parts in some examples."],"url":"http://arxiv.org/abs/2406.05662v1","category":"q-fin.TR"}
{"created":"2024-06-09 06:26:21","title":"Injecting Undetectable Backdoors in Deep Learning and Language Models","abstract":"As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information to the users on how to carefully perturb the least significant bits of their input to change the classification outcome to a favorable one. We develop a general strategy to plant a backdoor to neural networks while ensuring that even if the model's weights and architecture are accessible, the existence of the backdoor is still undetectable. To achieve this, we utilize techniques from cryptography such as cryptographic signatures and indistinguishability obfuscation. We further introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.","sentences":["As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks.","We investigate the threat posed by undetectable backdoors in models developed by insidious external expert firms.","When such backdoors exist, they allow the designer of the model to sell information to the users on how to carefully perturb the least significant bits of their input to change the classification outcome to a favorable one.","We develop a general strategy to plant a backdoor to neural networks while ensuring that even if the model's weights and architecture are accessible, the existence of the backdoor is still undetectable.","To achieve this, we utilize techniques from cryptography such as cryptographic signatures and indistinguishability obfuscation.","We further introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions."],"url":"http://arxiv.org/abs/2406.05660v1","category":"cs.LG"}
{"created":"2024-06-09 05:35:41","title":"Holding the fields constant: A shape-calculus approach to electromagnetic forces","abstract":"Using the mathematical theory of shape calculus as tool, we perform a rigorous investigation of the virtual work principle for the computation of electromagnetic forces in static settings. The main goal is to shed light on the widely held belief that the virtual work principle entails passively advecting the fields with the virtual displacement when computing deformation-dependent field energies gradients.   The adjoint approach to differentiation of functionals under variational constraints provides a mathematical justification for this belief. However, it also shows that passively advecting the actual electromagnetic fields is sufficient only in the case of linear materials. In the general case, also fields arising as solutions of adjoint variational problems have to be taken into account.","sentences":["Using the mathematical theory of shape calculus as tool, we perform a rigorous investigation of the virtual work principle for the computation of electromagnetic forces in static settings.","The main goal is to shed light on the widely held belief that the virtual work principle entails passively advecting the fields with the virtual displacement when computing deformation-dependent field energies gradients.   ","The adjoint approach to differentiation of functionals under variational constraints provides a mathematical justification for this belief.","However, it also shows that passively advecting the actual electromagnetic fields is sufficient only in the case of linear materials.","In the general case, also fields arising as solutions of adjoint variational problems have to be taken into account."],"url":"http://arxiv.org/abs/2406.05655v1","category":"physics.class-ph"}
{"created":"2024-06-09 05:27:23","title":"Distributed Combinatorial Optimization of Downlink User Assignment in mmWave Cell-free Massive MIMO Using Graph Neural Networks","abstract":"Millimeter wave (mmWave) cell-free massive MIMO (CF mMIMO) is a promising solution for future wireless communications. However, its optimization is non-trivial due to the challenging channel characteristics. We show that mmWave CF mMIMO optimization is largely an assignment problem between access points (APs) and users due to the high path loss of mmWave channels, the limited output power of the amplifier, and the almost orthogonal channels between users given a large number of AP antennas. The combinatorial nature of the assignment problem, the requirement for scalability, and the distributed implementation of CF mMIMO make this problem difficult. In this work, we propose an unsupervised machine learning (ML) enabled solution. In particular, a graph neural network (GNN) customized for scalability and distributed implementation is introduced. Moreover, the customized GNN architecture is hierarchically permutation-equivariant (HPE), i.e., if the APs or users of an AP are permuted, the output assignment is automatically permuted in the same way. To address the combinatorial problem, we relax it to a continuous problem, and introduce an information entropy-inspired penalty term. The training objective is then formulated using the augmented Lagrangian method (ALM). The test results show that the realized sum-rate outperforms that of the generalized serial dictatorship (GSD) algorithm and is very close to the upper bound in a small network scenario, while the upper bound is impossible to obtain in a large network scenario.","sentences":["Millimeter wave (mmWave) cell-free massive MIMO (CF mMIMO) is a promising solution for future wireless communications.","However, its optimization is non-trivial due to the challenging channel characteristics.","We show that mmWave CF mMIMO optimization is largely an assignment problem between access points (APs) and users due to the high path loss of mmWave channels, the limited output power of the amplifier, and the almost orthogonal channels between users given a large number of AP antennas.","The combinatorial nature of the assignment problem, the requirement for scalability, and the distributed implementation of CF mMIMO make this problem difficult.","In this work, we propose an unsupervised machine learning (ML) enabled solution.","In particular, a graph neural network (GNN) customized for scalability and distributed implementation is introduced.","Moreover, the customized GNN architecture is hierarchically permutation-equivariant (HPE), i.e., if the APs or users of an AP are permuted, the output assignment is automatically permuted in the same way.","To address the combinatorial problem, we relax it to a continuous problem, and introduce an information entropy-inspired penalty term.","The training objective is then formulated using the augmented Lagrangian method (ALM).","The test results show that the realized sum-rate outperforms that of the generalized serial dictatorship (GSD) algorithm and is very close to the upper bound in a small network scenario, while the upper bound is impossible to obtain in a large network scenario."],"url":"http://arxiv.org/abs/2406.05652v1","category":"eess.SP"}
{"created":"2024-06-09 05:21:43","title":"Black hole in a generalized Chaplygin-Jacobi dark fluid: shadow and light deflection angle","abstract":"We investigate a generalized Chaplygin-like gas with an anisotropic equation of state, characterizing a dark fluid within which a static spherically symmetric black hole is assumed. By solving the Einstein equations for this black hole spacetime, we explicitly derive the metric function. The spacetime is parametrized by two critical parameters, $\\mathcal{B}$ and $\\alpha$, which measure the deviation from the Schwarzschild black hole and the extent of the dark fluid's anisotropy, respectively. We explore the behavior of light rays in the vicinity of the black hole by calculating its shadow and comparing our results with the Event Horizon Telescope observations. This comparison constrains the parameters to $0 \\leq \\mathcal{B} < 0.03$ and $0 < \\alpha < 0.1$. Additionally, we calculate the deflection angles to determine the extent to which light is bent by the black hole. These calculations are further utilized to formulate possible Einstein rings, estimating the angular radius of the rings to be approximately $37.6\\,\\mathrm{\\mu as}$. Throughout this work, we present analytical solutions wherever feasible, and employ reliable approximations where necessary to provide comprehensive insights into the spacetime characteristics and their observable effects.","sentences":["We investigate a generalized Chaplygin-like gas with an anisotropic equation of state, characterizing a dark fluid within which a static spherically symmetric black hole is assumed.","By solving the Einstein equations for this black hole spacetime, we explicitly derive the metric function.","The spacetime is parametrized by two critical parameters, $\\mathcal{B}$ and $\\alpha$, which measure the deviation from the Schwarzschild black hole and the extent of the dark fluid's anisotropy, respectively.","We explore the behavior of light rays in the vicinity of the black hole by calculating its shadow and comparing our results with the Event Horizon Telescope observations.","This comparison constrains the parameters to $0 \\leq \\mathcal{B} < 0.03$ and $0 <","\\alpha < 0.1$.","Additionally, we calculate the deflection angles to determine the extent to which light is bent by the black hole.","These calculations are further utilized to formulate possible Einstein rings, estimating the angular radius of the rings to be approximately $37.6\\,\\mathrm{\\mu as}$. Throughout this work, we present analytical solutions wherever feasible, and employ reliable approximations where necessary to provide comprehensive insights into the spacetime characteristics and their observable effects."],"url":"http://arxiv.org/abs/2406.05650v1","category":"gr-qc"}
{"created":"2024-06-09 04:11:41","title":"What is my quantum computer good for? Quantum capability learning with physics-aware neural networks","abstract":"Quantum computers have the potential to revolutionize diverse fields, including quantum chemistry, materials science, and machine learning. However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail. Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer's capability-i.e., the programs it can run and how well it can run them. Previously, off-the-shelf neural network architectures have been used to model quantum computers' capabilities, but with limited success, because these networks fail to learn the complex quantum physics that determines real quantum computers' errors. We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models. Our architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs. This approach achieves up to $\\sim50\\%$ reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks.","sentences":["Quantum computers have the potential to revolutionize diverse fields, including quantum chemistry, materials science, and machine learning.","However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail.","Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer's capability-i.e., the programs it can run and how well it can run them.","Previously, off-the-shelf neural network architectures have been used to model quantum computers' capabilities, but with limited success, because these networks fail to learn the complex quantum physics that determines real quantum computers' errors.","We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models.","Our architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs.","This approach achieves up to $\\sim50\\%$ reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks."],"url":"http://arxiv.org/abs/2406.05636v1","category":"quant-ph"}
{"created":"2024-06-09 04:08:51","title":"Gauss curvature flow to the $L_p$-Gaussian chord Minkowski problem","abstract":"Recently, Huang and Qin \\cite{HY01} introduced Gaussian chord measure and $L_p$-Gaussian chord measure by variational methods. Meanwhile, they posed Gaussian chord Minkowski problem and used variational methods to obtain an origin-symmetric normalized measure solution for the Gaussian chord Minkowski problem.   Motivated by the forgoing works of $L_p$-Gaussian chord measure, we can not only propose the $L_p$-Gaussian chord Minkowski problem, but also obtain its smooth solutions. Specially, we provide a smooth even solution for $p=0$. In this paper, we obtain the existence of smooth even solutions to the $L_p$-Gaussian chord Minkowski problem with $p\\geq 0$ and $q>2$ by method of a Gauss curvature flow.","sentences":["Recently, Huang and Qin \\cite{HY01} introduced Gaussian chord measure and $L_p$-Gaussian chord measure by variational methods.","Meanwhile, they posed Gaussian chord Minkowski problem and used variational methods to obtain an origin-symmetric normalized measure solution for the Gaussian chord Minkowski problem.   ","Motivated by the forgoing works of $L_p$-Gaussian chord measure, we can not only propose the $L_p$-Gaussian chord Minkowski problem, but also obtain its smooth solutions.","Specially, we provide a smooth even solution for $p=0$. In this paper, we obtain the existence of smooth even solutions to the $L_p$-Gaussian chord Minkowski problem with $p\\geq 0$ and $q>2$ by method of a Gauss curvature flow."],"url":"http://arxiv.org/abs/2406.05635v1","category":"math.DG"}
{"created":"2024-06-09 04:06:09","title":"The non-Abelian two-dimensional Toda lattice and matrix sine-Gordon equations with self-consistent sources","abstract":"The non-Abelian two-dimensional Toda lattice and matrix sine-Gordon equations with self-consistent sources are established and solved. Two families of quasideterminant solutions are presented for the non-Abelian two-dimensional Toda lattice with self-consistent sources. By employing periodic and quasi-periodic reductions, a matrix sine-Gordon equation with self-consistent sources is constructed for the first time, for which exact solutions in terms of quasideterminants are derived.","sentences":["The non-Abelian two-dimensional Toda lattice and matrix sine-Gordon equations with self-consistent sources are established and solved.","Two families of quasideterminant solutions are presented for the non-Abelian two-dimensional Toda lattice with self-consistent sources.","By employing periodic and quasi-periodic reductions, a matrix sine-Gordon equation with self-consistent sources is constructed for the first time, for which exact solutions in terms of quasideterminants are derived."],"url":"http://arxiv.org/abs/2406.05634v1","category":"nlin.SI"}
{"created":"2024-06-09 03:57:02","title":"Best Response Strategies for Asymmetric Sensing in Linear-Quadratic Differential Games","abstract":"In this paper, we revisit the two-player continuous-time infinite-horizon linear quadratic differential game problem, where one of the players can sample the state of the system only intermittently due to a sensing constraint while the other player can do so continuously. Under these asymmetric sensing limitations between the players, we analyze the optimal sensing and control strategies for the player at a disadvantage while the other player continues to play its security strategy. We derive an optimal sensor policy within the class of stationary randomized policies. Finally, using simulations, we show that the expected cost accrued by the first player approaches its security level as its sensing limitation is relaxed.","sentences":["In this paper, we revisit the two-player continuous-time infinite-horizon linear quadratic differential game problem, where one of the players can sample the state of the system only intermittently due to a sensing constraint while the other player can do so continuously.","Under these asymmetric sensing limitations between the players, we analyze the optimal sensing and control strategies for the player at a disadvantage while the other player continues to play its security strategy.","We derive an optimal sensor policy within the class of stationary randomized policies.","Finally, using simulations, we show that the expected cost accrued by the first player approaches its security level as its sensing limitation is relaxed."],"url":"http://arxiv.org/abs/2406.05632v1","category":"math.OC"}
{"created":"2024-06-09 03:00:16","title":"Photon induced proton and anti-proton pair production with ultraperipheral heavy ion collisions at RHIC","abstract":"We investigate proton-antiproton ($p\\bar{p}$) pair production via photon-photon fusion in the ultra-peripheral collisions at RHIC, employing a joint impact parameter and transverse momentum dependent formalism. We consider proton exchange, $s$-channel resonance and hand-bag mechanisms, predicting differential distributions of $p\\bar p$ production. Our theoretical predictions can be tested against future measurements at RHIC, to enhance our understanding of photon-photon interactions in strong electromagnetic fields.","sentences":["We investigate proton-antiproton ($p\\bar{p}$) pair production via photon-photon fusion in the ultra-peripheral collisions at RHIC, employing a joint impact parameter and transverse momentum dependent formalism.","We consider proton exchange, $s$-channel resonance and hand-bag mechanisms, predicting differential distributions of $p\\bar p$ production.","Our theoretical predictions can be tested against future measurements at RHIC, to enhance our understanding of photon-photon interactions in strong electromagnetic fields."],"url":"http://arxiv.org/abs/2406.05618v1","category":"hep-ph"}
{"created":"2024-06-09 02:29:00","title":"Global well-posedness of the defocusing, cubic nonlinear wave equation outside of the ball with radial data","abstract":"We consider the defocusing, cubic nonlinear wave equation with zero Dirichlet boundary value in the exterior $\\Omega = \\R^3\\backslash \\bar{ B}(0,1)$. We make use of the distorted Fourier transform in \\cite{LiSZ:NLS, Taylor:PDE:II} to establish the dispersive estimate and the global-in-time (endpoint) Strichartz estimate of the linear wave equation outside of the ball with radial data. As an application, we combine the Fourier truncation method as those in \\cite{Bourgain98:FTM, GallPlan03:NLW, KenigPV00:NLW} with the energy method to show global well-posedness of radial solution to the defoucusing, cubic nonlinear wave equation outside of a ball in the Sobolev space $\\left(\\dot H^{s}_{D}(\\Omega) \\cap L^4(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$ with $s>3/4$. To the best of the author's knowledge, it is first result about low regularity of semilinear wave equation with zero Dirichlet boundary value on the exterior domain.","sentences":["We consider the defocusing, cubic nonlinear wave equation with zero Dirichlet boundary value in the exterior $\\Omega = \\R^3\\backslash \\bar{ B}(0,1)$. We make use of the distorted Fourier transform in \\cite{LiSZ:NLS, Taylor:PDE:II} to establish the dispersive estimate and the global-in-time (endpoint) Strichartz estimate of the linear wave equation outside of the ball with radial data.","As an application, we combine the Fourier truncation method as those in \\cite{Bourgain98:FTM, GallPlan03:NLW, KenigPV00:NLW} with the energy method to show global well-posedness of radial solution to the defoucusing, cubic nonlinear wave equation outside of a ball in the Sobolev space $\\left(\\dot H^{s}_{D}(\\Omega) \\cap L^4(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$ with $s>3/4$. To the best of the author's knowledge, it is first result about low regularity of semilinear wave equation with zero Dirichlet boundary value on the exterior domain."],"url":"http://arxiv.org/abs/2406.05614v1","category":"math.AP"}
{"created":"2024-06-08 23:53:13","title":"Understanding Inhibition Through Maximally Tense Images","abstract":"We address the functional role of 'feature inhibition' in vision models; that is, what are the mechanisms by which a neural network ensures images do not express a given feature? We observe that standard interpretability tools in the literature are not immediately suited to the inhibitory case, given the asymmetry introduced by the ReLU activation function. Given this, we propose inhibition be understood through a study of 'maximally tense images' (MTIs), i.e. those images that excite and inhibit a given feature simultaneously. We show how MTIs can be studied with two novel visualization techniques; +/- attribution inversions, which split single images into excitatory and inhibitory components, and the attribution atlas, which provides a global visualization of the various ways images can excite/inhibit a feature. Finally, we explore the difficulties introduced by superposition, as such interfering features induce the same attribution motif as MTIs.","sentences":["We address the functional role of 'feature inhibition' in vision models; that is, what are the mechanisms by which a neural network ensures images do not express a given feature?","We observe that standard interpretability tools in the literature are not immediately suited to the inhibitory case, given the asymmetry introduced by the ReLU activation function.","Given this, we propose inhibition be understood through a study of 'maximally tense images' (MTIs), i.e. those images that excite and inhibit a given feature simultaneously.","We show how MTIs can be studied with two novel visualization techniques; +/- attribution inversions, which split single images into excitatory and inhibitory components, and the attribution atlas, which provides a global visualization of the various ways images can excite/inhibit a feature.","Finally, we explore the difficulties introduced by superposition, as such interfering features induce the same attribution motif as MTIs."],"url":"http://arxiv.org/abs/2406.05598v1","category":"cs.CV"}
{"created":"2024-06-08 23:20:41","title":"Analytic gradients for equation-of-motion coupled cluster with single, double, and perturbative triple excitations","abstract":"Understanding the process of molecular photoexcitation is crucial in various fields, including drug development, materials science, photovoltaics, and more. The electronic vertical excitation energy is a critical property, for example in determining the singlet-triplet gap of chromophores. However, a full understanding of excited-state processes requires additional explorations of the excited-state potential energy surface and electronic properties, which is greatly aided by the availability of analytic energy gradients. Owing to its robust high accuracy over a wide range of chemical problems, equation-of-motion coupled-cluster with single and double excitations (EOM-CCSD) is a powerful method for predicting excited state properties, and the implementation of analytic gradients of many EOM-CCSD (excitation energies, ionization potentials, electron attachment energies, etc.) along with numerous successful applications highlights the flexibility of the method. In specific cases where a higher level of accuracy is needed or in more complex electronic structures, the inclusion of triple excitations becomes essential, for example, in the EOM-CCSD* approach of Saeh and Stanton. In this work, we derive and implement for the first time the analytic gradients of EOMEE-CCSD*, which also provides a template for analytic gradients of related excited state methods with perturbative triple excitations. The capabilities of analytic EOMEE-CCSD* gradients are illustrated by several representative examples.","sentences":["Understanding the process of molecular photoexcitation is crucial in various fields, including drug development, materials science, photovoltaics, and more.","The electronic vertical excitation energy is a critical property, for example in determining the singlet-triplet gap of chromophores.","However, a full understanding of excited-state processes requires additional explorations of the excited-state potential energy surface and electronic properties, which is greatly aided by the availability of analytic energy gradients.","Owing to its robust high accuracy over a wide range of chemical problems, equation-of-motion coupled-cluster with single and double excitations (EOM-CCSD) is a powerful method for predicting excited state properties, and the implementation of analytic gradients of many EOM-CCSD (excitation energies, ionization potentials, electron attachment energies, etc.) along with numerous successful applications highlights the flexibility of the method.","In specific cases where a higher level of accuracy is needed or in more complex electronic structures, the inclusion of triple excitations becomes essential, for example, in the EOM-CCSD* approach of Saeh and Stanton.","In this work, we derive and implement for the first time the analytic gradients of EOMEE-CCSD*, which also provides a template for analytic gradients of related excited state methods with perturbative triple excitations.","The capabilities of analytic EOMEE-CCSD* gradients are illustrated by several representative examples."],"url":"http://arxiv.org/abs/2406.05595v1","category":"physics.chem-ph"}
{"created":"2024-06-08 20:43:50","title":"A Structure-Preserving Domain Decomposition Method for Data-Driven Modeling","abstract":"We present a domain decomposition strategy for developing structure-preserving finite element discretizations from data when exact governing equations are unknown. On subdomains, trainable Whitney form elements are used to identify structure-preserving models from data, providing a Dirichlet-to-Neumann map which may be used to globally construct a mortar method. The reduced-order local elements may be trained offline to reproduce high-fidelity Dirichlet data in cases where first principles model derivation is either intractable, unknown, or computationally prohibitive. In such cases, particular care must be taken to preserve structure on both local and mortar levels without knowledge of the governing equations, as well as to ensure well-posedness and stability of the resulting monolithic data-driven system. This strategy provides a flexible means of both scaling to large systems and treating complex geometries, and is particularly attractive for multiscale problems with complex microstructure geometry. While consistency is traditionally obtained in finite element methods via quasi-optimality results and the Bramble-Hilbert lemma as the local element diameter $h\\rightarrow0$, our analysis establishes notions of accuracy and stability for finite h with accuracy coming from matching data. Numerical experiments and analysis establish properties for $H(\\operatorname{div})$ problems in small data limits ($\\mathcal{O}(1)$ reference solutions).","sentences":["We present a domain decomposition strategy for developing structure-preserving finite element discretizations from data when exact governing equations are unknown.","On subdomains, trainable Whitney form elements are used to identify structure-preserving models from data, providing a Dirichlet-to-Neumann map which may be used to globally construct a mortar method.","The reduced-order local elements may be trained offline to reproduce high-fidelity Dirichlet data in cases where first principles model derivation is either intractable, unknown, or computationally prohibitive.","In such cases, particular care must be taken to preserve structure on both local and mortar levels without knowledge of the governing equations, as well as to ensure well-posedness and stability of the resulting monolithic data-driven system.","This strategy provides a flexible means of both scaling to large systems and treating complex geometries, and is particularly attractive for multiscale problems with complex microstructure geometry.","While consistency is traditionally obtained in finite element methods via quasi-optimality results and the Bramble-Hilbert lemma as the local element diameter $h\\rightarrow0$, our analysis establishes notions of accuracy and stability for finite h with accuracy coming from matching data.","Numerical experiments and analysis establish properties for $H(\\operatorname{div})$ problems in small data limits ($\\mathcal{O}(1)$ reference solutions)."],"url":"http://arxiv.org/abs/2406.05571v1","category":"math.NA"}
{"created":"2024-06-08 20:42:28","title":"Singular extension of critical Sobolev mappings with values into complete Riemannian manifolds","abstract":"Triggered by a recent criterion, due to A.~Petrunin [17], to check if a complete, non-compact, Riemannian manifold admits an isometric embedding into a Euclidean space with positive reach, we extend to manifolds with such property the singular extension results of B.~Bulanyi and J.~Van~Schaftingen [5] for maps in the critical, nonlinear Sobolev space $W^{m/(m+1),m+1}\\left(X^m,\\mathcal{N}\\right)$, where $m \\in \\mathbb{N} \\setminus \\{0\\}$, $\\mathcal{N}$ is a compact Riemannian manifold, and $X^m$ is either the sphere $\\mathbb{S}^m = \\partial \\mathbb{B}^{m+1}_+$, the plane $\\mathbb{R}^m$, or again $\\mathbb{S}^m$ but seen as the boundary sphere of the Poincar\\'{e} ball model of the hyperbolic space $\\mathbb{H}^{m+1}$. As in [5], we obtain that the extended maps satisfy an exponential weak-type Sobolev-Marcinkiewicz estimate. Finally, we provide some illustrative examples.","sentences":["Triggered by a recent criterion, due to A.~Petrunin [17], to check if a complete, non-compact, Riemannian manifold admits an isometric embedding into a Euclidean space with positive reach, we extend to manifolds with such property the singular extension results of B.~Bulanyi and J.~Van~","Schaftingen [5] for maps in the critical, nonlinear Sobolev space $W^{m/(m+1),m+1}\\left(X^m,\\mathcal{N}\\right)$, where $m \\in \\mathbb{N} \\setminus \\{0\\}$, $\\mathcal{N}$ is a compact Riemannian manifold, and $X^m$ is either the sphere $\\mathbb{S}^m = \\partial \\mathbb{B}^{m+1}_+$, the plane $\\mathbb{R}^m$, or again $\\mathbb{S}^m$ but seen as the boundary sphere of the Poincar\\'{e} ball model of the hyperbolic space $\\mathbb{H}^{m+1}$. As in [5], we obtain that the extended maps satisfy an exponential weak-type Sobolev-Marcinkiewicz estimate.","Finally, we provide some illustrative examples."],"url":"http://arxiv.org/abs/2406.05570v1","category":"math.AP"}
{"created":"2024-06-08 18:42:46","title":"Causal Interpretation of Regressions With Ranks","abstract":"In studies of educational production functions or intergenerational mobility, it is common to transform the key variables into percentile ranks. Yet, it remains unclear what the regression coefficient estimates with ranks of the outcome or the treatment. In this paper, we derive effective causal estimands for a broad class of commonly-used regression methods, including the ordinary least squares (OLS), two-stage least squares (2SLS), difference-in-differences (DiD), and regression discontinuity designs (RDD). Specifically, we introduce a novel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE), and prove that it serves as the building block of the effective estimands of all the aforementioned econometrics methods. For 2SLS, DiD, and RDD, we show that direct applications to outcome ranks identify parameters that are difficult to interpret. To address this issue, we develop alternative methods to identify more interpretable causal parameters.","sentences":["In studies of educational production functions or intergenerational mobility, it is common to transform the key variables into percentile ranks.","Yet, it remains unclear what the regression coefficient estimates with ranks of the outcome or the treatment.","In this paper, we derive effective causal estimands for a broad class of commonly-used regression methods, including the ordinary least squares (OLS), two-stage least squares (2SLS), difference-in-differences (DiD), and regression discontinuity designs (RDD).","Specifically, we introduce a novel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE), and prove that it serves as the building block of the effective estimands of all the aforementioned econometrics methods.","For 2SLS, DiD, and RDD, we show that direct applications to outcome ranks identify parameters that are difficult to interpret.","To address this issue, we develop alternative methods to identify more interpretable causal parameters."],"url":"http://arxiv.org/abs/2406.05548v1","category":"econ.EM"}
{"created":"2024-06-08 18:21:12","title":"Privacy-Preserving Optimal Parameter Selection for Collaborative Clustering","abstract":"This study investigates the optimal selection of parameters for collaborative clustering while ensuring data privacy. We focus on key clustering algorithms within a collaborative framework, where multiple data owners combine their data. A semi-trusted server assists in recommending the most suitable clustering algorithm and its parameters. Our findings indicate that the privacy parameter ($\\epsilon$) minimally impacts the server's recommendations, but an increase in $\\epsilon$ raises the risk of membership inference attacks, where sensitive information might be inferred. To mitigate these risks, we implement differential privacy techniques, particularly the Randomized Response mechanism, to add noise and protect data privacy. Our approach demonstrates that high-quality clustering can be achieved while maintaining data confidentiality, as evidenced by metrics such as the Adjusted Rand Index and Silhouette Score. This study contributes to privacy-aware data sharing, optimal algorithm and parameter selection, and effective communication between data owners and the server.","sentences":["This study investigates the optimal selection of parameters for collaborative clustering while ensuring data privacy.","We focus on key clustering algorithms within a collaborative framework, where multiple data owners combine their data.","A semi-trusted server assists in recommending the most suitable clustering algorithm and its parameters.","Our findings indicate that the privacy parameter ($\\epsilon$) minimally impacts the server's recommendations, but an increase in $\\epsilon$ raises the risk of membership inference attacks, where sensitive information might be inferred.","To mitigate these risks, we implement differential privacy techniques, particularly the Randomized Response mechanism, to add noise and protect data privacy.","Our approach demonstrates that high-quality clustering can be achieved while maintaining data confidentiality, as evidenced by metrics such as the Adjusted Rand Index and Silhouette Score.","This study contributes to privacy-aware data sharing, optimal algorithm and parameter selection, and effective communication between data owners and the server."],"url":"http://arxiv.org/abs/2406.05545v1","category":"cs.LG"}
{"created":"2024-06-08 17:41:58","title":"First-principle screening of structural, electronic and hydrogen storage properties of Vanadium based hydride perovskites XVH3 (X = Li, K)","abstract":"V-based XVH 3 (X = Li,K) hydrides perovskites are investigated for their hydrogen storage capacity using the WIEN2K code. To verify the stability of these hydrides, first-principles investigations are employed to examine their structural, electronic and hydrogen storage properties. According to structural studies these compositions hydrides are stable and part of the cubic space group (221 Pm-3m). We have examined many aspects of these compositions throughout, using the PBE-GGA exchange correlation potential. We obtained the energy versus volume curve and found the stable phase and structural parameter of these hyrides using equation of state given by Birch-Murnaghan's. These hydrides thermodynamic stability is expressed in terms of their gravimetric hydrogen storage capacity.The goal of this study is to compute the standard enthalpy of formation and thermal desorption to ascertain the stability of these hydrides. Based on band structure and density of state plots it is found that these compositions are metallic in nature. The study presents a preliminary theoretical approach for hydrogen storage applications of thermoelectric compositions, revealing their strong thermoelectric responses and potential for green energy sources.","sentences":["V-based XVH 3 (X = Li,K) hydrides perovskites are investigated for their hydrogen storage capacity using the WIEN2K code.","To verify the stability of these hydrides, first-principles investigations are employed to examine their structural, electronic and hydrogen storage properties.","According to structural studies these compositions hydrides are stable and part of the cubic space group (221 Pm-3m).","We have examined many aspects of these compositions throughout, using the PBE-GGA exchange correlation potential.","We obtained the energy versus volume curve and found the stable phase and structural parameter of these hyrides using equation of state given by Birch-Murnaghan's.","These hydrides thermodynamic stability is expressed in terms of their gravimetric hydrogen storage capacity.","The goal of this study is to compute the standard enthalpy of formation and thermal desorption to ascertain the stability of these hydrides.","Based on band structure and density of state plots it is found that these compositions are metallic in nature.","The study presents a preliminary theoretical approach for hydrogen storage applications of thermoelectric compositions, revealing their strong thermoelectric responses and potential for green energy sources."],"url":"http://arxiv.org/abs/2406.05538v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-08 17:23:44","title":"Effects of metals (X = Zn, Co) on structure, electronic bands and gravimetric capacity of KXH3 hydrides","abstract":"Using the WIEN2K code, the hydrogen storage capabilities of lithium-based KXH3 (X = Zn, Co) hydrides perovskites are examined. To verify the stability of these hydrides, first-principles simulations are employed to examine their structural, electronic, and hydrogen storage capabilities. These compositions' structural investigation shows that the hydrides are stable and part of the cubic space group (221 Pm-3m). We have examined several aspects of these composition's features throughout, using the Perdew-Burke-Ernzerhof generalized gradient approximation. The study identifies stable phases and structural parameters of hydrides using B-E equations, assessing thermodynamic stability in terms of hydrogen storage capacities. The metallic nature of these hydrides is confirmed through band structure and density calculations using WIEN2K.","sentences":["Using the WIEN2K code, the hydrogen storage capabilities of lithium-based KXH3 (X = Zn, Co) hydrides perovskites are examined.","To verify the stability of these hydrides, first-principles simulations are employed to examine their structural, electronic, and hydrogen storage capabilities.","These compositions' structural investigation shows that the hydrides are stable and part of the cubic space group (221 Pm-3m).","We have examined several aspects of these composition's features throughout, using the Perdew-Burke-Ernzerhof generalized gradient approximation.","The study identifies stable phases and structural parameters of hydrides using B-E equations, assessing thermodynamic stability in terms of hydrogen storage capacities.","The metallic nature of these hydrides is confirmed through band structure and density calculations using WIEN2K."],"url":"http://arxiv.org/abs/2406.05530v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-08 17:16:27","title":"Ab-initio investigations of novel potential all-d metal Heusler alloys Co2MnNb","abstract":"In this study, we employ the Wien2k code to conduct ab-initio study of a novel potential all-d-metal Heusler alloy Co 2 MnNb. The analysis utilizes the comparison of local spin density approximations (LDA) with Perdew-Burke-Ernzerh parameterized Generalized Gradient Approximation (PBE-GGA) for structural optimization while modified Becke-Jones potential (mBJ) exchange-correlation potentials to examine various characteristic properties of the alloy under study. Employing Birch-Murnaghan equation of state, we construct the energy-versus-volume curve, facilitating the determination of stable phases and structural parameters of the investigated alloys. Structural optimization in both non-magnetic (NM) and spin-polarized (FM) states reveals the stability of the alloy in the FM state. The compound exhibits metallic behavior in bulk, with notable anisotropic semiconducting behavior for down spin while pure metallic behavior for up spin electrons. Partial density of states of each element of the composition is also analysed to compare their respective contribution towards the observed band structure. The anisotropic behavior of Co 2 MnNb for a specific spin state could be of importance in future spintronic and other thin films device applications.","sentences":["In this study, we employ the Wien2k code to conduct ab-initio study of a novel potential all-d-metal Heusler alloy Co 2 MnNb.","The analysis utilizes the comparison of local spin density approximations (LDA) with Perdew-Burke-Ernzerh parameterized Generalized Gradient Approximation (PBE-GGA) for structural optimization while modified Becke-Jones potential (mBJ) exchange-correlation potentials to examine various characteristic properties of the alloy under study.","Employing Birch-Murnaghan equation of state, we construct the energy-versus-volume curve, facilitating the determination of stable phases and structural parameters of the investigated alloys.","Structural optimization in both non-magnetic (NM) and spin-polarized (FM) states reveals the stability of the alloy in the FM state.","The compound exhibits metallic behavior in bulk, with notable anisotropic semiconducting behavior for down spin while pure metallic behavior for up spin electrons.","Partial density of states of each element of the composition is also analysed to compare their respective contribution towards the observed band structure.","The anisotropic behavior of Co 2 MnNb for a specific spin state could be of importance in future spintronic and other thin films device applications."],"url":"http://arxiv.org/abs/2406.05527v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-08 17:02:35","title":"Stochastic Calculus for the Theta Process","abstract":"In this paper, we study the stochastic calculus for the theta process. The theta process is a stochastic process of number theoretical origin arising as a scaling limit of quadratic Weyl sums. It has several properties in common with Brownian motion such as its H\\\"older regularity, uncorrelated increments and quadratic variation. However, crucially, we show that the theta process is not a semimartingale, making It\\^o calculus techniques inapplicable. Instead, we use the celebrated rough paths theory to develop the stochastic calculus for the theta process. We do so by constructing the iterated integrals - the ``rough path\" - above the theta process. Rough paths theory takes a signal and its iterated integrals and produces a vast and robust theory of stochastic differential equations. In addition, the rough path we construct can be described in terms of higher rank theta sums.","sentences":["In this paper, we study the stochastic calculus for the theta process.","The theta process is a stochastic process of number theoretical origin arising as a scaling limit of quadratic Weyl sums.","It has several properties in common with Brownian motion such as its H\\\"older regularity, uncorrelated increments and quadratic variation.","However, crucially, we show that the theta process is not a semimartingale, making It\\^o calculus techniques inapplicable.","Instead, we use the celebrated rough paths theory to develop the stochastic calculus for the theta process.","We do so by constructing the iterated integrals - the ``rough path\" - above the theta process.","Rough paths theory takes a signal and its iterated integrals and produces a vast and robust theory of stochastic differential equations.","In addition, the rough path we construct can be described in terms of higher rank theta sums."],"url":"http://arxiv.org/abs/2406.05523v1","category":"math.PR"}
{"created":"2024-06-08 16:55:43","title":"Synergizing Deep Learning and Phase Change Materials for Four-state Broadband Multifunctional Metasurfaces in the Visible Range","abstract":"In this article, we report, for the first time, broadband multifunctional metasurfaces with more than four distinct functionalities. The constituent meta-atoms combine two different phase change materials, $\\mathrm{VO_2}$ and $\\mathrm{Sb_2S_3}$ in a multi-stage configuration. FDTD simulations demonstrate a broadband reflection amplitude switching between the four states in visible range due to the enhanced cavity length modulation effect from the cascaded Fabry-Perot cavities, overcoming the inherent small optical contrast between the phase change material (PCM) states. This, along with the reflection phase control between the four states, allows us to incorporate both amplitude and phase-dependent properties in the same metasurface - achromatic deflection, wavelength beam splitting, achromatic focusing, and broadband absorption, overcoming the limitations of previous functionality switching mechanisms for the visible band. We have used a Tandem Neural network-based inverse design scheme to ensure the stringent requirements of different states are realized. We have used two forward networks for predicting the reflection amplitude and phase for a meta-atom within the pre-defined design space. The excellent prediction capability of these surrogate models is utilized to train the reverse network. The inverse design network, trained with a labeled data set, is capable of producing the optimized meta-units given the desired figure-of-merits in terms of reflection amplitude and phase for the four states. The optical characteristics of two inverse-designed metasurfaces have been evaluated as test cases for two different sets of design parameters in the four states. Both structures demonstrate the four desired broadband functionalities while closely matching the design requirements, suggesting their potential in visible-range portable medical imaging devices.","sentences":["In this article, we report, for the first time, broadband multifunctional metasurfaces with more than four distinct functionalities.","The constituent meta-atoms combine two different phase change materials, $\\mathrm{VO_2}$ and $\\mathrm{Sb_2S_3}$ in a multi-stage configuration.","FDTD simulations demonstrate a broadband reflection amplitude switching between the four states in visible range due to the enhanced cavity length modulation effect from the cascaded Fabry-Perot cavities, overcoming the inherent small optical contrast between the phase change material (PCM) states.","This, along with the reflection phase control between the four states, allows us to incorporate both amplitude and phase-dependent properties in the same metasurface - achromatic deflection, wavelength beam splitting, achromatic focusing, and broadband absorption, overcoming the limitations of previous functionality switching mechanisms for the visible band.","We have used a Tandem Neural network-based inverse design scheme to ensure the stringent requirements of different states are realized.","We have used two forward networks for predicting the reflection amplitude and phase for a meta-atom within the pre-defined design space.","The excellent prediction capability of these surrogate models is utilized to train the reverse network.","The inverse design network, trained with a labeled data set, is capable of producing the optimized meta-units given the desired figure-of-merits in terms of reflection amplitude and phase for the four states.","The optical characteristics of two inverse-designed metasurfaces have been evaluated as test cases for two different sets of design parameters in the four states.","Both structures demonstrate the four desired broadband functionalities while closely matching the design requirements, suggesting their potential in visible-range portable medical imaging devices."],"url":"http://arxiv.org/abs/2406.05519v1","category":"physics.optics"}
{"created":"2024-06-08 16:49:17","title":"Obstructions to almost complex structures following Massey","abstract":"We provide proofs of two theorems stated by Massey in 1961, concerning the obstructions to finding complex structures on real vector bundles. In addition, we determine the second obstruction to a complex structure on a rank six orientable real vector bundle. The obstructions are fractional parts of integral Stiefel-Whitney classes, and a fourth of an appropriate combination of Pontryagin, Chern, and Euler classes.","sentences":["We provide proofs of two theorems stated by Massey in 1961, concerning the obstructions to finding complex structures on real vector bundles.","In addition, we determine the second obstruction to a complex structure on a rank six orientable real vector bundle.","The obstructions are fractional parts of integral Stiefel-Whitney classes, and a fourth of an appropriate combination of Pontryagin, Chern, and Euler classes."],"url":"http://arxiv.org/abs/2406.05518v1","category":"math.AT"}
{"created":"2024-06-08 16:19:42","title":"Micromechanically motivated finite-strain phase-field fracture model to investigate damage in crosslinked elastomers","abstract":"A micromechanically motivated phase-field damage model is proposed to investigate the fracture behaviour in crosslinked polyurethane adhesive. The crosslinked polyurethane adhesive typically show viscoelastic behaviour with geometric nonlinearity. The finite-strain viscoelastic behaviour is modelled using a micromechanical network model considering shorter and longer chain length distribution. The micromechanical viscoelastic network model also consider the softening due to breakage/debonding of the short chains with increase in deformation. The micromechanical model is coupled with the phase-field damage model to investigate the crack initiation and propagation. Critical energy release rate is needed as a material property to solve phase-field equation. The energy release rate is formulated based on the polymer chain network. The numerical investigation is performed using finite element method. The force-displacement curves from the numerical analysis and experiments are compared to validate the proposed material model.","sentences":["A micromechanically motivated phase-field damage model is proposed to investigate the fracture behaviour in crosslinked polyurethane adhesive.","The crosslinked polyurethane adhesive typically show viscoelastic behaviour with geometric nonlinearity.","The finite-strain viscoelastic behaviour is modelled using a micromechanical network model considering shorter and longer chain length distribution.","The micromechanical viscoelastic network model also consider the softening due to breakage/debonding of the short chains with increase in deformation.","The micromechanical model is coupled with the phase-field damage model to investigate the crack initiation and propagation.","Critical energy release rate is needed as a material property to solve phase-field equation.","The energy release rate is formulated based on the polymer chain network.","The numerical investigation is performed using finite element method.","The force-displacement curves from the numerical analysis and experiments are compared to validate the proposed material model."],"url":"http://arxiv.org/abs/2406.05511v1","category":"cs.CE"}
{"created":"2024-06-08 16:01:12","title":"Bottom of the spectrum for manifolds foliated by minimal leaves","abstract":"We give a sharp estimate for the bottom of the spectrum of a Riemannian manifold which is foliated by minimal leaves and transversally negatively curved. Our proof, which uses probabilistic methods, also yields an estimate for the bottom of the sub-Riemannian spectrum.","sentences":["We give a sharp estimate for the bottom of the spectrum of a Riemannian manifold which is foliated by minimal leaves and transversally negatively curved.","Our proof, which uses probabilistic methods, also yields an estimate for the bottom of the sub-Riemannian spectrum."],"url":"http://arxiv.org/abs/2406.05503v1","category":"math.DG"}
{"created":"2024-06-08 15:55:12","title":"Survival probability, particle imbalance, and their relationship in quadratic models","abstract":"We argue that the dynamics of particle imbalance in quadratic fermionic models is, for the majority of initial many-body product states in site occupation basis, virtually indistinguishable from the dynamics of survival probabilities of single-particle states. We then generalize our statement to a similar relationship between the non-equal time and space density correlation functions in many-body states and the transition probabilities of single-particle states at nonzero distances. Finally, we study the equal time connected density-density correlation functions in many-body states, which exhibit certain qualitative analogies with the survival and transition probabilities of single-particle states. Our results are numerically tested for two paradigmatic models of single-particle localization: the 3D Anderson model and the 1D Aubry-Andr\\'e model. This work gives affirmative answer to the question whether it is possible to measure features of the single-particle survival and transition probabilities by the dynamics of observables in many-body states.","sentences":["We argue that the dynamics of particle imbalance in quadratic fermionic models is, for the majority of initial many-body product states in site occupation basis, virtually indistinguishable from the dynamics of survival probabilities of single-particle states.","We then generalize our statement to a similar relationship between the non-equal time and space density correlation functions in many-body states and the transition probabilities of single-particle states at nonzero distances.","Finally, we study the equal time connected density-density correlation functions in many-body states, which exhibit certain qualitative analogies with the survival and transition probabilities of single-particle states.","Our results are numerically tested for two paradigmatic models of single-particle localization: the 3D Anderson model and the 1D Aubry-Andr\\'e model.","This work gives affirmative answer to the question whether it is possible to measure features of the single-particle survival and transition probabilities by the dynamics of observables in many-body states."],"url":"http://arxiv.org/abs/2406.05500v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-08 15:38:15","title":"Holonomy of parabolic geometries near isolated higher-order fixed points","abstract":"For Cartan geometries admitting automorphisms with isotropies satisfying a particular, loosely dynamical property on their model geometries, we demonstrate the existence of an open subset of the geometry with trivial holonomy. This property, which generalizes characteristics of isotropies corresponding to isolated higher-order fixed points in parabolic geometries that are known to require a nearby open subset to have vanishing curvature, only relies upon the behavior of the isotropy in the model geometry, and therefore applies regardless of initial curvature assumptions, such as regularity or normality. Along the way to proving our main results, we also derive a couple of results for working with holonomy, relating to limits of sequences of developments and the existence of antidevelopments, that are useful in their own right. To showcase the effectiveness of the techniques developed, we use them to completely characterize all almost c-projective and almost quaternionic structures that admit a nontrivial automorphism with a higher-order fixed point, as well as all nondegenerate partially integrable almost CR structures that admit a higher-order fixed point with non-null isotropy.","sentences":["For Cartan geometries admitting automorphisms with isotropies satisfying a particular, loosely dynamical property on their model geometries, we demonstrate the existence of an open subset of the geometry with trivial holonomy.","This property, which generalizes characteristics of isotropies corresponding to isolated higher-order fixed points in parabolic geometries that are known to require a nearby open subset to have vanishing curvature, only relies upon the behavior of the isotropy in the model geometry, and therefore applies regardless of initial curvature assumptions, such as regularity or normality.","Along the way to proving our main results, we also derive a couple of results for working with holonomy, relating to limits of sequences of developments and the existence of antidevelopments, that are useful in their own right.","To showcase the effectiveness of the techniques developed, we use them to completely characterize all almost c-projective and almost quaternionic structures that admit a nontrivial automorphism with a higher-order fixed point, as well as all nondegenerate partially integrable almost CR structures that admit a higher-order fixed point with non-null isotropy."],"url":"http://arxiv.org/abs/2406.05497v1","category":"math.DG"}
{"created":"2024-06-08 14:45:33","title":"On a class of multi-fidelity methods for the semiclassical Schr\u00f6dinger equation with uncertainties","abstract":"In this paper, we study the semiclassical Schr\\\"odinger equation with random parameters and develop several robust multi-fidelity methods. We employ the time-splitting Fourier pseudospectral (TSFP) method for the high-fidelity solver, and consider different low-fidelity solvers including the meshless method like frozen Gaussian approximation (FGA) and the level set (LS) method for the semiclassical limit of the Schr\\\"odinger equation. With a careful choice of the low-fidelity model, we obtain an error estimate for the bi-fidelity method. We conduct numerous numerical experiments and validate the accuracy and efficiency of our proposed multi-fidelity methods, by comparing the performance of a class of bi-fidelity and tri-fidelity approximations.","sentences":["In this paper, we study the semiclassical Schr\\\"odinger equation with random parameters and develop several robust multi-fidelity methods.","We employ the time-splitting Fourier pseudospectral (TSFP) method for the high-fidelity solver, and consider different low-fidelity solvers including the meshless method like frozen Gaussian approximation (FGA) and the level set (LS) method for the semiclassical limit of the Schr\\\"odinger equation.","With a careful choice of the low-fidelity model, we obtain an error estimate for the bi-fidelity method.","We conduct numerous numerical experiments and validate the accuracy and efficiency of our proposed multi-fidelity methods, by comparing the performance of a class of bi-fidelity and tri-fidelity approximations."],"url":"http://arxiv.org/abs/2406.05489v1","category":"math.NA"}
{"created":"2024-06-08 14:22:57","title":"A Real-Valued Description of Quantum Mechanics with Schrodinger's 4th-order Matter-Wave Equation","abstract":"Using a variational formulation, we show that Schrodinger's 4th-order, real-valued matter-wave equation which involves the spatial derivatives of the potential V(r), produces the precise eigenvalues of Schrodinger's 2nd-order, complex-valued matter-wave equation together with an equal number of negative, mirror eigenvalues. Accordingly, the paper concludes that there is a real-valued description of non-relativistic quantum mechanics in association with the existence of negative (repelling) energy levels. Schrodinger's classical 2nd-order, complex-valued matter-wave equation which was constructed upon factoring the 4th-order, real-valued differential operator and retaining only one of the two conjugate complex operators is a simpler description of the matter-wave, since it does not involve the derivatives of the potential V(r), at the expense of missing the negative (repelling) energy levels.","sentences":["Using a variational formulation, we show that Schrodinger's 4th-order, real-valued matter-wave equation which involves the spatial derivatives of the potential V(r), produces the precise eigenvalues of Schrodinger's 2nd-order, complex-valued matter-wave equation together with an equal number of negative, mirror eigenvalues.","Accordingly, the paper concludes that there is a real-valued description of non-relativistic quantum mechanics in association with the existence of negative (repelling) energy levels.","Schrodinger's classical 2nd-order, complex-valued matter-wave equation which was constructed upon factoring the 4th-order, real-valued differential operator and retaining only one of the two conjugate complex operators is a simpler description of the matter-wave, since it does not involve the derivatives of the potential V(r), at the expense of missing the negative (repelling) energy levels."],"url":"http://arxiv.org/abs/2406.05484v1","category":"quant-ph"}
{"created":"2024-06-08 14:14:19","title":"Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks","abstract":"In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs. Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, TADA considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.","sentences":["In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields.","The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors.","However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc.","Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs.","Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs.","Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification.","The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method.","Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs.","Empirically, TADA considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes."],"url":"http://arxiv.org/abs/2406.05482v1","category":"cs.LG"}
{"created":"2024-06-08 13:52:02","title":"Attri-Net: A Globally and Locally Inherently Interpretable Model for Multi-Label Classification Using Class-Specific Counterfactuals","abstract":"Interpretability is crucial for machine learning algorithms in high-stakes medical applications. However, high-performing neural networks typically cannot explain their predictions. Post-hoc explanation methods provide a way to understand neural networks but have been shown to suffer from conceptual problems. Moreover, current research largely focuses on providing local explanations for individual samples rather than global explanations for the model itself. In this paper, we propose Attri-Net, an inherently interpretable model for multi-label classification that provides local and global explanations. Attri-Net first counterfactually generates class-specific attribution maps to highlight the disease evidence, then performs classification with logistic regression classifiers based solely on the attribution maps. Local explanations for each prediction can be obtained by interpreting the attribution maps weighted by the classifiers' weights. Global explanation of whole model can be obtained by jointly considering learned average representations of the attribution maps for each class (called the class centers) and the weights of the linear classifiers. To ensure the model is ``right for the right reason\", we further introduce a mechanism to guide the model's explanations to align with human knowledge. Our comprehensive evaluations show that Attri-Net can generate high-quality explanations consistent with clinical knowledge while not sacrificing classification performance.","sentences":["Interpretability is crucial for machine learning algorithms in high-stakes medical applications.","However, high-performing neural networks typically cannot explain their predictions.","Post-hoc explanation methods provide a way to understand neural networks but have been shown to suffer from conceptual problems.","Moreover, current research largely focuses on providing local explanations for individual samples rather than global explanations for the model itself.","In this paper, we propose Attri-Net, an inherently interpretable model for multi-label classification that provides local and global explanations.","Attri-Net first counterfactually generates class-specific attribution maps to highlight the disease evidence, then performs classification with logistic regression classifiers based solely on the attribution maps.","Local explanations for each prediction can be obtained by interpreting the attribution maps weighted by the classifiers' weights.","Global explanation of whole model can be obtained by jointly considering learned average representations of the attribution maps for each class (called the class centers) and the weights of the linear classifiers.","To ensure the model is ``right for the right reason\", we further introduce a mechanism to guide the model's explanations to align with human knowledge.","Our comprehensive evaluations show that Attri-Net can generate high-quality explanations consistent with clinical knowledge while not sacrificing classification performance."],"url":"http://arxiv.org/abs/2406.05477v1","category":"cs.CV"}
{"created":"2024-06-08 13:43:44","title":"HDRT: Infrared Capture for HDR Imaging","abstract":"Capturing real world lighting is a long standing challenge in imaging and most practical methods acquire High Dynamic Range (HDR) images by either fusing multiple exposures, or boosting the dynamic range of Standard Dynamic Range (SDR) images. Multiple exposure capture is problematic as it requires longer capture times which can often lead to ghosting problems. The main alternative, inverse tone mapping is an ill-defined problem that is especially challenging as single captured exposures usually contain clipped and quantized values, and are therefore missing substantial amounts of content. To alleviate this, we propose a new approach, High Dynamic Range Thermal (HDRT), for HDR acquisition using a separate, commonly available, thermal infrared (IR) sensor. We propose a novel deep neural method (HDRTNet) which combines IR and SDR content to generate HDR images. HDRTNet learns to exploit IR features linked to the RGB image and the IR-specific parameters are subsequently used in a dual branch method that fuses features at shallow layers. This produces an HDR image that is significantly superior to that generated using naive fusion approaches. To validate our method, we have created the first HDR and thermal dataset, and performed extensive experiments comparing HDRTNet with the state-of-the-art. We show substantial quantitative and qualitative quality improvements on both over- and under-exposed images, showing that our approach is robust to capturing in multiple different lighting conditions.","sentences":["Capturing real world lighting is a long standing challenge in imaging and most practical methods acquire High Dynamic Range (HDR) images by either fusing multiple exposures, or boosting the dynamic range of Standard Dynamic Range (SDR) images.","Multiple exposure capture is problematic as it requires longer capture times which can often lead to ghosting problems.","The main alternative, inverse tone mapping is an ill-defined problem that is especially challenging as single captured exposures usually contain clipped and quantized values, and are therefore missing substantial amounts of content.","To alleviate this, we propose a new approach, High Dynamic Range Thermal (HDRT), for HDR acquisition using a separate, commonly available, thermal infrared (IR) sensor.","We propose a novel deep neural method (HDRTNet) which combines IR and SDR content to generate HDR images.","HDRTNet learns to exploit IR features linked to the RGB image and the IR-specific parameters are subsequently used in a dual branch method that fuses features at shallow layers.","This produces an HDR image that is significantly superior to that generated using naive fusion approaches.","To validate our method, we have created the first HDR and thermal dataset, and performed extensive experiments comparing HDRTNet with the state-of-the-art.","We show substantial quantitative and qualitative quality improvements on both over- and under-exposed images, showing that our approach is robust to capturing in multiple different lighting conditions."],"url":"http://arxiv.org/abs/2406.05475v1","category":"cs.CV"}
{"created":"2024-06-10 17:48:36","title":"Robust Distribution Learning with Local and Global Adversarial Corruptions","abstract":"We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (*global* corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n, \\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes. We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order $2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded covariance, our finite-sample bounds match the minimax population-level optimum for large sample sizes. Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator. We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization.","sentences":["We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (*global* corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (*local* corruptions).","Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n, \\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi)","= k$.","This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes.","We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order $2+\\delta$, for constant $\\delta > 0$.","For data distributions with bounded covariance, our finite-sample bounds match the minimax population-level optimum for large sample sizes.","Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator.","We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization."],"url":"http://arxiv.org/abs/2406.06509v1","category":"cs.LG"}
{"created":"2024-06-10 17:44:11","title":"Online Newton Method for Bandit Convex Optimisation","abstract":"We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon. In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties.","sentences":["We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon.","In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties."],"url":"http://arxiv.org/abs/2406.06506v1","category":"math.OC"}
{"created":"2024-06-10 17:24:42","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence Length","abstract":"Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive outer-product update in linear transformers with the delta rule have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks (including on tasks that focus on recall). We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrid models outperform strong transformer baselines.","sentences":["Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention.","However, these models still underperform transformers especially on tasks that require in-context retrieval.","While more expressive variants of linear transformers which replace the additive outer-product update in linear transformers with the delta rule have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware.","This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices.","This algorithm allows us to scale up DeltaNet to standard language modeling settings.","We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks (including on tasks that focus on recall).","We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrid models outperform strong transformer baselines."],"url":"http://arxiv.org/abs/2406.06484v1","category":"cs.LG"}
{"created":"2024-06-10 16:40:55","title":"Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data","abstract":"Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics. Since often the treatments of interest cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to seek latent quasi-experiments in instrumental variables (IVs) for the treatment, for example, a randomized intent to treat or a randomized product recommendation. This approach, on the other hand, can suffer from low compliance, i.e., IV weakness. Some subgroups may even exhibit zero compliance meaning we cannot instrument for their CATEs at all. In this paper we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns biased CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.","sentences":["Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics.","Since often the treatments of interest cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding.","One strategy to overcome these limitations is to seek latent quasi-experiments in instrumental variables (IVs) for the treatment, for example, a randomized intent to treat or a randomized product recommendation.","This approach, on the other hand, can suffer from low compliance, i.e., IV weakness.","Some subgroups may even exhibit zero compliance meaning we cannot instrument for their CATEs at all.","In this paper we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups.","We propose a two-stage framework that first learns biased CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates.","We characterize the convergence rates of our method and validate its effectiveness through a simulation study.","Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth."],"url":"http://arxiv.org/abs/2406.06452v1","category":"stat.ME"}
{"created":"2024-06-10 16:14:33","title":"Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving","abstract":"In autonomous driving, the most challenging scenarios are the ones that can only be detected within their temporal context. Most video anomaly detection approaches focus either on surveillance or traffic accidents, which are only a subfield of autonomous driving. In this work, we present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD surveillance video anomaly detection method for autonomous driving. We learn a representation of normality from a vehicle's ego perspective and evaluate pixel-wise anomaly detections in rare and critical scenarios.","sentences":["In autonomous driving, the most challenging scenarios are the ones that can only be detected within their temporal context.","Most video anomaly detection approaches focus either on surveillance or traffic accidents, which are only a subfield of autonomous driving.","In this work, we present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD surveillance video anomaly detection method for autonomous driving.","We learn a representation of normality from a vehicle's ego perspective and evaluate pixel-wise anomaly detections in rare and critical scenarios."],"url":"http://arxiv.org/abs/2406.06423v1","category":"cs.CV"}
{"created":"2024-06-10 15:55:06","title":"INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition","abstract":"We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech emotion recognition (SER) challenge -- and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then. We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search. Results are always reported on the official test set with a separate validation set only used for early stopping. Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning. Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark. An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic.","sentences":["We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech emotion recognition (SER) challenge -- and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then.","We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search.","Results are always reported on the official test set with a separate validation set only used for early stopping.","Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning.","Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark.","An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic."],"url":"http://arxiv.org/abs/2406.06401v1","category":"cs.CL"}
{"created":"2024-06-10 14:42:20","title":"Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching","abstract":"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.","sentences":["Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world.","To keep LLMs current, existing approaches typically involve continued pre-training on new documents.","However, they frequently face difficulties in extracting stored knowledge.","Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching.","Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection.","Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning.","Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge."],"url":"http://arxiv.org/abs/2406.06326v1","category":"cs.CL"}
{"created":"2024-06-10 14:34:38","title":"Measurement of the Depth of Maximum of Air-Shower Profiles with energies between $\\mathbf{10^{18.5}}$ and $\\mathbf{10^{20}}$ eV using the Surface Detector of the Pierre Auger Observatory and Deep Learning","abstract":"We report an investigation of the mass composition of cosmic rays with energies from 3 to 100 EeV (1 EeV=$10^{18}$ eV) using the distributions of the depth of shower maximum $X_\\mathrm{max}$. The analysis relies on ${\\sim}50,000$ events recorded by the Surface Detector of the Pierre Auger Observatory and a deep-learning-based reconstruction algorithm. Above energies of 5 EeV, the data set offers a 10-fold increase in statistics with respect to fluorescence measurements at the Observatory. After cross-calibration using the Fluorescence Detector, this enables the first measurement of the evolution of the mean and the standard deviation of the $X_\\mathrm{max}$ distributions up to 100 EeV. Our findings are threefold:   (1.) The evolution of the mean logarithmic mass towards a heavier composition with increasing energy can be confirmed and is extended to 100 EeV.   (2.) The evolution of the fluctuations of $X_\\mathrm{max}$ towards a heavier and purer composition with increasing energy can be confirmed with high statistics. We report a rather heavy composition and small fluctuations in $X_\\mathrm{max}$ at the highest energies.   (3.) We find indications for a characteristic structure beyond a constant change in the mean logarithmic mass, featuring three breaks that are observed in proximity to the ankle, instep, and suppression features in the energy spectrum.","sentences":["We report an investigation of the mass composition of cosmic rays with energies from 3 to 100 EeV (1 EeV=$10^{18}$ eV) using the distributions of the depth of shower maximum $X_\\mathrm{max}$. The analysis relies on ${\\sim}50,000$ events recorded by the Surface Detector of the Pierre Auger Observatory and a deep-learning-based reconstruction algorithm.","Above energies of 5 EeV, the data set offers a 10-fold increase in statistics with respect to fluorescence measurements at the Observatory.","After cross-calibration using the Fluorescence Detector, this enables the first measurement of the evolution of the mean and the standard deviation of the $X_\\mathrm{max}$ distributions up to 100 EeV.","Our findings are threefold:   (1.)","The evolution of the mean logarithmic mass towards a heavier composition with increasing energy can be confirmed and is extended to 100 EeV.   (2.)","The evolution of the fluctuations of $X_\\mathrm{max}$ towards a heavier and purer composition with increasing energy can be confirmed with high statistics.","We report a rather heavy composition and small fluctuations in $X_\\mathrm{max}$ at the highest energies.   ","(3.)","We find indications for a characteristic structure beyond a constant change in the mean logarithmic mass, featuring three breaks that are observed in proximity to the ankle, instep, and suppression features in the energy spectrum."],"url":"http://arxiv.org/abs/2406.06319v1","category":"astro-ph.HE"}
{"created":"2024-06-10 14:32:10","title":"Inference of the Mass Composition of Cosmic Rays with energies from $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger Observatory and Deep Learning","abstract":"We present measurements of the atmospheric depth of the shower maximum $X_\\mathrm{max}$, inferred for the first time on an event-by-event level using the Surface Detector of the Pierre Auger Observatory. Using deep learning, we were able to extend measurements of the $X_\\mathrm{max}$ distributions up to energies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements, providing new insights into the mass composition of cosmic rays at extreme energies. Gaining a 10-fold increase in statistics compared to the Fluorescence Detector data, we find evidence that the rate of change of the average $X_\\mathrm{max}$ with the logarithm of energy features three breaks at $6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm 2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and $31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three prominent features (ankle, instep, suppression) of the cosmic-ray flux. The energy evolution of the mean and standard deviation of the measured $X_\\mathrm{max}$ distributions indicates that the mass composition becomes increasingly heavier and purer, thus being incompatible with a large fraction of light nuclei between 50 EeV and 100 EeV.","sentences":["We present measurements of the atmospheric depth of the shower maximum $X_\\mathrm{max}$, inferred for the first time on an event-by-event level using the Surface Detector of the Pierre Auger Observatory.","Using deep learning, we were able to extend measurements of the $X_\\mathrm{max}$ distributions up to energies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements, providing new insights into the mass composition of cosmic rays at extreme energies.","Gaining a 10-fold increase in statistics compared to the Fluorescence Detector data, we find evidence that the rate of change of the average $X_\\mathrm{max}$ with the logarithm of energy features three breaks at $6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm 2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and $31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three prominent features (ankle, instep, suppression) of the cosmic-ray flux.","The energy evolution of the mean and standard deviation of the measured $X_\\mathrm{max}$ distributions indicates that the mass composition becomes increasingly heavier and purer, thus being incompatible with a large fraction of light nuclei between 50 EeV and 100 EeV."],"url":"http://arxiv.org/abs/2406.06315v1","category":"astro-ph.HE"}
{"created":"2024-06-10 13:25:43","title":"Compute Better Spent: Replacing Dense Layers with Structured Matrices","abstract":"Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.","sentences":["Dense linear layers are the dominant computational bottleneck in foundation models.","Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain.","In this work, we systematically explore structured matrices as replacements for dense matrices.","We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale.","Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers.","Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute.","We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks.","On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs.","BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models."],"url":"http://arxiv.org/abs/2406.06248v1","category":"cs.LG"}
{"created":"2024-06-10 11:58:11","title":"Federated learning in food research","abstract":"Research in the food domain is at times limited due to data sharing obstacles, such as data ownership, privacy requirements, and regulations. While important, these obstacles can restrict data-driven methods such as machine learning. Federated learning, the approach of training models on locally kept data and only sharing the learned parameters, is a potential technique to alleviate data sharing obstacles. This systematic review investigates the use of federated learning within the food domain, structures included papers in a federated learning framework, highlights knowledge gaps, and discusses potential applications. A total of 41 papers were included in the review. The current applications include solutions to water and milk quality assessment, cybersecurity of water processing, pesticide residue risk analysis, weed detection, and fraud detection, focusing on centralized horizontal federated learning. One of the gaps found was the lack of vertical or transfer federated learning and decentralized architectures.","sentences":["Research in the food domain is at times limited due to data sharing obstacles, such as data ownership, privacy requirements, and regulations.","While important, these obstacles can restrict data-driven methods such as machine learning.","Federated learning, the approach of training models on locally kept data and only sharing the learned parameters, is a potential technique to alleviate data sharing obstacles.","This systematic review investigates the use of federated learning within the food domain, structures included papers in a federated learning framework, highlights knowledge gaps, and discusses potential applications.","A total of 41 papers were included in the review.","The current applications include solutions to water and milk quality assessment, cybersecurity of water processing, pesticide residue risk analysis, weed detection, and fraud detection, focusing on centralized horizontal federated learning.","One of the gaps found was the lack of vertical or transfer federated learning and decentralized architectures."],"url":"http://arxiv.org/abs/2406.06202v1","category":"cs.LG"}
{"created":"2024-06-10 11:33:34","title":"An Effective-Efficient Approach for Dense Multi-Label Action Detection","abstract":"Unlike the sparse label action detection task, where a single action occurs in each timestamp of a video, in a dense multi-label scenario, actions can overlap. To address this challenging task, it is necessary to simultaneously learn (i) temporal dependencies and (ii) co-occurrence action relationships. Recent approaches model temporal information by extracting multi-scale features through hierarchical transformer-based networks. However, the self-attention mechanism in transformers inherently loses temporal positional information. We argue that combining this with multiple sub-sampling processes in hierarchical designs can lead to further loss of positional information. Preserving this information is essential for accurate action detection. In this paper, we address this issue by proposing a novel transformer-based network that (a) employs a non-hierarchical structure when modelling different ranges of temporal dependencies and (b) embeds relative positional encoding in its transformer layers. Furthermore, to model co-occurrence action relationships, current methods explicitly embed class relations into the transformer network. However, these approaches are not computationally efficient, as the network needs to compute all possible pair action class relations. We also overcome this challenge by introducing a novel learning paradigm that allows the network to benefit from explicitly modelling temporal co-occurrence action dependencies without imposing their additional computational costs during inference. We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets and show that our method improves the current state-of-the-art results.","sentences":["Unlike the sparse label action detection task, where a single action occurs in each timestamp of a video, in a dense multi-label scenario, actions can overlap.","To address this challenging task, it is necessary to simultaneously learn (i) temporal dependencies and (ii) co-occurrence action relationships.","Recent approaches model temporal information by extracting multi-scale features through hierarchical transformer-based networks.","However, the self-attention mechanism in transformers inherently loses temporal positional information.","We argue that combining this with multiple sub-sampling processes in hierarchical designs can lead to further loss of positional information.","Preserving this information is essential for accurate action detection.","In this paper, we address this issue by proposing a novel transformer-based network that (a) employs a non-hierarchical structure when modelling different ranges of temporal dependencies and (b) embeds relative positional encoding in its transformer layers.","Furthermore, to model co-occurrence action relationships, current methods explicitly embed class relations into the transformer network.","However, these approaches are not computationally efficient, as the network needs to compute all possible pair action class relations.","We also overcome this challenge by introducing a novel learning paradigm that allows the network to benefit from explicitly modelling temporal co-occurrence action dependencies without imposing their additional computational costs during inference.","We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets and show that our method improves the current state-of-the-art results."],"url":"http://arxiv.org/abs/2406.06187v1","category":"cs.CV"}
{"created":"2024-06-10 11:27:46","title":"Black carbon plumes from gas flaring in North Africa identified from multi-spectral imagery with deep learning","abstract":"Black carbon (BC) is an important pollutant aerosol emitted by numerous human activities, including gas flaring. Improper combustion in flaring activities can release large amounts of BC, which is harmful to human health and has a strong climate warming effect. To our knowledge, no study has ever directly monitored BC emissions from satellite imagery. Previous works quantified BC emissions indirectly, by applying emission coefficients to flaring volumes estimated from satellite imagery. Here, we develop a deep learning framework and apply it to Sentinel-2 imagery over North Africa during 2022 to detect and quantify BC emissions from gas flaring. We find that BC emissions in this region amount to about 1 million tCO$_{2,\\mathrm{eq}}$, or 1 million passenger cars, more than a quarter of which are due to 10 sites alone. This work demonstrates the operational monitoring of BC emissions from flaring, a key step in implementing effective mitigation policies to reduce the climate impact of oil and gas operations.","sentences":["Black carbon (BC) is an important pollutant aerosol emitted by numerous human activities, including gas flaring.","Improper combustion in flaring activities can release large amounts of BC, which is harmful to human health and has a strong climate warming effect.","To our knowledge, no study has ever directly monitored BC emissions from satellite imagery.","Previous works quantified BC emissions indirectly, by applying emission coefficients to flaring volumes estimated from satellite imagery.","Here, we develop a deep learning framework and apply it to Sentinel-2 imagery over North Africa during 2022 to detect and quantify BC emissions from gas flaring.","We find that BC emissions in this region amount to about 1 million tCO$_{2,\\mathrm{eq}}$, or 1 million passenger cars, more than a quarter of which are due to 10 sites alone.","This work demonstrates the operational monitoring of BC emissions from flaring, a key step in implementing effective mitigation policies to reduce the climate impact of oil and gas operations."],"url":"http://arxiv.org/abs/2406.06183v1","category":"cs.CV"}
{"created":"2024-06-10 11:03:40","title":"Topological Analysis for Detecting Anomalies (TADA) in Time Series","abstract":"This paper introduces new methodology based on the field of Topological Data Analysis for detecting anomalies in multivariate time series, that aims to detect global changes in the dependency structure between channels. The proposed approach is lean enough to handle large scale datasets, and extensive numerical experiments back the intuition that it is more suitable for detecting global changes of correlation structures than existing methods. Some theoretical guarantees for quantization algorithms based on dependent time sequences are also provided.","sentences":["This paper introduces new methodology based on the field of Topological Data Analysis for detecting anomalies in multivariate time series, that aims to detect global changes in the dependency structure between channels.","The proposed approach is lean enough to handle large scale datasets, and extensive numerical experiments back the intuition that it is more suitable for detecting global changes of correlation structures than existing methods.","Some theoretical guarantees for quantization algorithms based on dependent time sequences are also provided."],"url":"http://arxiv.org/abs/2406.06168v1","category":"math.ST"}
{"created":"2024-06-10 07:54:56","title":"An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware Crop Yield Predictions","abstract":"Precise crop yield predictions are of national importance for ensuring food security and sustainable agricultural practices. While AI-for-science approaches have exhibited promising achievements in solving many scientific problems such as drug discovery, precipitation nowcasting, etc., the development of deep learning models for predicting crop yields is constantly hindered by the lack of an open and large-scale deep learning-ready dataset with multiple modalities to accommodate sufficient information. To remedy this, we introduce the CropNet dataset, the first terabyte-sized, publicly available, and multi-modal dataset specifically targeting climate change-aware crop yield predictions for the contiguous United States (U.S.) continent at the county level. Our CropNet dataset is composed of three modalities of data, i.e., Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over 2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate researchers in developing versatile deep learning models for timely and precisely predicting crop yields at the county-level, by accounting for the effects of both short-term growing season weather variations and long-term climate change on crop yields. Besides, we develop the CropNet package, offering three types of APIs, for facilitating researchers in downloading the CropNet data on the fly over the time and region of interest, and flexibly building their deep learning models for accurate crop yield predictions. Extensive experiments have been conducted on our CropNet dataset via employing various types of deep learning solutions, with the results validating the general applicability and the efficacy of the CropNet dataset in climate change-aware crop yield predictions.","sentences":["Precise crop yield predictions are of national importance for ensuring food security and sustainable agricultural practices.","While AI-for-science approaches have exhibited promising achievements in solving many scientific problems such as drug discovery, precipitation nowcasting, etc., the development of deep learning models for predicting crop yields is constantly hindered by the lack of an open and large-scale deep learning-ready dataset with multiple modalities to accommodate sufficient information.","To remedy this, we introduce the CropNet dataset, the first terabyte-sized, publicly available, and multi-modal dataset specifically targeting climate change-aware crop yield predictions for the contiguous United States (U.S.) continent at the county level.","Our CropNet dataset is composed of three modalities of data, i.e., Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over 2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate researchers in developing versatile deep learning models for timely and precisely predicting crop yields at the county-level, by accounting for the effects of both short-term growing season weather variations and long-term climate change on crop yields.","Besides, we develop the CropNet package, offering three types of APIs, for facilitating researchers in downloading the CropNet data on the fly over the time and region of interest, and flexibly building their deep learning models for accurate crop yield predictions.","Extensive experiments have been conducted on our CropNet dataset via employing various types of deep learning solutions, with the results validating the general applicability and the efficacy of the CropNet dataset in climate change-aware crop yield predictions."],"url":"http://arxiv.org/abs/2406.06081v1","category":"cs.LG"}
{"created":"2024-06-10 07:13:54","title":"LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement Learning","abstract":"Intent-based network automation is a promising tool to enable easier network management however certain challenges need to be effectively addressed. These are: 1) processing intents, i.e., identification of logic and necessary parameters to fulfill an intent, 2) validating an intent to align it with current network status, and 3) satisfying intents via network optimizing functions like xApps and rApps in O-RAN. This paper addresses these points via a three-fold strategy to introduce intent-based automation for O-RAN. First, intents are processed via a lightweight Large Language Model (LLM). Secondly, once an intent is processed, it is validated against future incoming traffic volume profiles (high or low). Finally, a series of network optimization applications (rApps and xApps) have been developed. With their machine learning-based functionalities, they can improve certain key performance indicators such as throughput, delay, and energy efficiency. In this final stage, using an attention-based hierarchical reinforcement learning algorithm, these applications are optimally initiated to satisfy the intent of an operator. Our simulations show that the proposed method can achieve at least 12% increase in throughput, 17.1% increase in energy efficiency, and 26.5% decrease in network delay compared to the baseline algorithms.","sentences":["Intent-based network automation is a promising tool to enable easier network management however certain challenges need to be effectively addressed.","These are: 1) processing intents, i.e., identification of logic and necessary parameters to fulfill an intent, 2) validating an intent to align it with current network status, and 3) satisfying intents via network optimizing functions like xApps and rApps in O-RAN.","This paper addresses these points via a three-fold strategy to introduce intent-based automation for O-RAN.","First, intents are processed via a lightweight Large Language Model (LLM).","Secondly, once an intent is processed, it is validated against future incoming traffic volume profiles (high or low).","Finally, a series of network optimization applications (rApps and xApps) have been developed.","With their machine learning-based functionalities, they can improve certain key performance indicators such as throughput, delay, and energy efficiency.","In this final stage, using an attention-based hierarchical reinforcement learning algorithm, these applications are optimally initiated to satisfy the intent of an operator.","Our simulations show that the proposed method can achieve at least 12% increase in throughput, 17.1% increase in energy efficiency, and 26.5% decrease in network delay compared to the baseline algorithms."],"url":"http://arxiv.org/abs/2406.06059v1","category":"cs.NI"}
{"created":"2024-06-10 06:17:55","title":"Vript: A Video Is Worth Thousands of Words","abstract":"Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance. Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips. Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets. Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc). By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs. This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance. Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works. All code, models, and datasets are available in https://github.com/mutonix/Vript.","sentences":["Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance.","Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips.","Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets.","Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc).","By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs.","This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance.","Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos.","Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works.","All code, models, and datasets are available in https://github.com/mutonix/Vript."],"url":"http://arxiv.org/abs/2406.06040v1","category":"cs.CV"}
{"created":"2024-06-10 05:50:23","title":"The Curse of Popularity: Popular Entities have Catastrophic Side Effects when Deleting Knowledge from Language Models","abstract":"Language models (LMs) encode world knowledge in their internal parameters through training. However, LMs may learn personal and confidential information from the training data, leading to privacy concerns such as data leakage. Therefore, research on knowledge deletion from LMs is essential. This study focuses on the knowledge stored in LMs and analyzes the relationship between the side effects of knowledge deletion and the entities related to the knowledge. Our findings reveal that deleting knowledge related to popular entities can have catastrophic side effects. Furthermore, this research is the first to analyze knowledge deletion in models trained on synthetic knowledge graphs, indicating a new direction for controlled experiments.","sentences":["Language models (LMs) encode world knowledge in their internal parameters through training.","However, LMs may learn personal and confidential information from the training data, leading to privacy concerns such as data leakage.","Therefore, research on knowledge deletion from LMs is essential.","This study focuses on the knowledge stored in LMs and analyzes the relationship between the side effects of knowledge deletion and the entities related to the knowledge.","Our findings reveal that deleting knowledge related to popular entities can have catastrophic side effects.","Furthermore, this research is the first to analyze knowledge deletion in models trained on synthetic knowledge graphs, indicating a new direction for controlled experiments."],"url":"http://arxiv.org/abs/2406.06032v1","category":"cs.CL"}
{"created":"2024-06-10 05:15:30","title":"RepoQA: Evaluating Long Context Code Understanding","abstract":"Recent advances have been improving the context windows of Large Language Models (LLMs). To quantify the real long-context capabilities of LLMs, evaluators such as the popular Needle in a Haystack have been developed to test LLMs over a large chunk of raw texts. While effective, current evaluations overlook the insight of how LLMs work with long-context code, i.e., repositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on long-context code understanding. Traditional needle testers ask LLMs to directly retrieve the answer from the context without necessary deep understanding. In RepoQA, we built our initial task, namely Searching Needle Function (SNF), which exercises LLMs to search functions given their natural-language description, i.e., LLMs cannot find the desired function if they cannot understand the description and code. RepoQA is multilingual and comprehensive: it includes 500 code search tasks gathered from 50 popular repositories across 5 modern programming languages. By evaluating 26 general and code-specific LLMs on RepoQA, we show (i) there is still a small gap between the best open and proprietary models; (ii) different models are good at different languages; and (iii) models may understand code better without comments.","sentences":["Recent advances have been improving the context windows of Large Language Models (LLMs).","To quantify the real long-context capabilities of LLMs, evaluators such as the popular Needle in a Haystack have been developed to test LLMs over a large chunk of raw texts.","While effective, current evaluations overlook the insight of how LLMs work with long-context code, i.e., repositories.","To this end, we initiate the RepoQA benchmark to evaluate LLMs on long-context code understanding.","Traditional needle testers ask LLMs to directly retrieve the answer from the context without necessary deep understanding.","In RepoQA, we built our initial task, namely Searching Needle Function (SNF), which exercises LLMs to search functions given their natural-language description, i.e., LLMs cannot find the desired function if they cannot understand the description and code.","RepoQA is multilingual and comprehensive: it includes 500 code search tasks gathered from 50 popular repositories across 5 modern programming languages.","By evaluating 26 general and code-specific LLMs on RepoQA, we show (i) there is still a small gap between the best open and proprietary models; (ii) different models are good at different languages; and (iii) models may understand code better without comments."],"url":"http://arxiv.org/abs/2406.06025v1","category":"cs.SE"}
{"created":"2024-06-10 02:50:54","title":"Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech Separation","abstract":"Since the success of a time-domain speech separation, further improvements have been made by expanding the length and channel of a feature sequence to increase the amount of computation. When temporally expanded to a long sequence, the feature is segmented into chunks as a dual-path model in most studies of speech separation. In particular, it is common for the process of separating features corresponding to each speaker to be located in the final stage of the network. However, it is more advantageous and intuitive to proactively expand the feature sequence to include the number of speakers as an extra dimension. In this paper, we present an asymmetric strategy in which the encoder and decoder are partitioned to perform distinct processing in separation tasks. The encoder analyzes features, and the output of the encoder is split into the number of speakers to be separated. The separated sequences are then reconstructed by the weight-shared decoder, as Siamese network, in addition to cross-speaker processing. By using the Siamese network in the decoder, without using speaker information, the network directly learns to discriminate the features using a separation objective. With a common split layer, intermediate encoder features for skip connections are also split for the reconstruction decoder based on the U-Net structure. In addition, instead of segmenting the feature into chunks as dual-path, we design global and local Transformer blocks to directly process long sequences. The experimental results demonstrated that this separation-and-reconstruction framework is effective and that the combination of proposed global and local Transformer can sufficiently replace the role of inter- and intra-chunk processing in dual-path structure. Finally, the presented model including both of these achieved state-of-the-art performance with less computation than before in various benchmark datasets.","sentences":["Since the success of a time-domain speech separation, further improvements have been made by expanding the length and channel of a feature sequence to increase the amount of computation.","When temporally expanded to a long sequence, the feature is segmented into chunks as a dual-path model in most studies of speech separation.","In particular, it is common for the process of separating features corresponding to each speaker to be located in the final stage of the network.","However, it is more advantageous and intuitive to proactively expand the feature sequence to include the number of speakers as an extra dimension.","In this paper, we present an asymmetric strategy in which the encoder and decoder are partitioned to perform distinct processing in separation tasks.","The encoder analyzes features, and the output of the encoder is split into the number of speakers to be separated.","The separated sequences are then reconstructed by the weight-shared decoder, as Siamese network, in addition to cross-speaker processing.","By using the Siamese network in the decoder, without using speaker information, the network directly learns to discriminate the features using a separation objective.","With a common split layer, intermediate encoder features for skip connections are also split for the reconstruction decoder based on the U-Net structure.","In addition, instead of segmenting the feature into chunks as dual-path, we design global and local Transformer blocks to directly process long sequences.","The experimental results demonstrated that this separation-and-reconstruction framework is effective and that the combination of proposed global and local Transformer can sufficiently replace the role of inter- and intra-chunk processing in dual-path structure.","Finally, the presented model including both of these achieved state-of-the-art performance with less computation than before in various benchmark datasets."],"url":"http://arxiv.org/abs/2406.05983v1","category":"eess.AS"}
{"created":"2024-06-10 02:29:35","title":"Weighted KL-Divergence for Document Ranking Model Refinement","abstract":"Transformer-based retrieval and reranking models for text document search are often refined through knowledge distillation together with contrastive learning. A tight distribution matching between the teacher and student models can be hard as over-calibration may degrade training effectiveness when a teacher does not perform well. This paper contrastively reweights KL divergence terms to prioritize the alignment between a student and a teacher model for proper separation of positive and negative documents. This paper analyzes and evaluates the proposed loss function on the MS MARCO and BEIR datasets to demonstrate its effectiveness in improving the relevance of tested student models.","sentences":["Transformer-based retrieval and reranking models for text document search are often refined through knowledge distillation together with contrastive learning.","A tight distribution matching between the teacher and student models can be hard as over-calibration may degrade training effectiveness when a teacher does not perform well.","This paper contrastively reweights KL divergence terms to prioritize the alignment between a student and a teacher model for proper separation of positive and negative documents.","This paper analyzes and evaluates the proposed loss function on the MS MARCO and BEIR datasets to demonstrate its effectiveness in improving the relevance of tested student models."],"url":"http://arxiv.org/abs/2406.05977v1","category":"cs.IR"}
{"created":"2024-06-10 02:20:26","title":"Inter-slice Super-resolution of Magnetic Resonance Images by Pre-training and Self-supervised Fine-tuning","abstract":"In clinical practice, 2D magnetic resonance (MR) sequences are widely adopted. While individual 2D slices can be stacked to form a 3D volume, the relatively large slice spacing can pose challenges for both image visualization and subsequent analysis tasks, which often require isotropic voxel spacing. To reduce slice spacing, deep-learning-based super-resolution techniques are widely investigated. However, most current solutions require a substantial number of paired high-resolution and low-resolution images for supervised training, which are typically unavailable in real-world scenarios. In this work, we propose a self-supervised super-resolution framework for inter-slice super-resolution of MR images. Our framework is first featured by pre-training on video dataset, as temporal correlation of videos is found beneficial for modeling the spatial relation among MR slices. Then, we use public high-quality MR dataset to fine-tune our pre-trained model, for enhancing awareness of our model to medical data. Finally, given a target dataset at hand, we utilize self-supervised fine-tuning to further ensure our model works well with user-specific super-resolution tasks. The proposed method demonstrates superior performance compared to other self-supervised methods and also holds the potential to benefit various downstream applications.","sentences":["In clinical practice, 2D magnetic resonance (MR) sequences are widely adopted.","While individual 2D slices can be stacked to form a 3D volume, the relatively large slice spacing can pose challenges for both image visualization and subsequent analysis tasks, which often require isotropic voxel spacing.","To reduce slice spacing, deep-learning-based super-resolution techniques are widely investigated.","However, most current solutions require a substantial number of paired high-resolution and low-resolution images for supervised training, which are typically unavailable in real-world scenarios.","In this work, we propose a self-supervised super-resolution framework for inter-slice super-resolution of MR images.","Our framework is first featured by pre-training on video dataset, as temporal correlation of videos is found beneficial for modeling the spatial relation among MR slices.","Then, we use public high-quality MR dataset to fine-tune our pre-trained model, for enhancing awareness of our model to medical data.","Finally, given a target dataset at hand, we utilize self-supervised fine-tuning to further ensure our model works well with user-specific super-resolution tasks.","The proposed method demonstrates superior performance compared to other self-supervised methods and also holds the potential to benefit various downstream applications."],"url":"http://arxiv.org/abs/2406.05974v1","category":"eess.IV"}
{"created":"2024-06-10 01:21:59","title":"Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters","abstract":"Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance. However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity. Simply replacing these functions with ReLU fails to achieve sufficient sparsity. Moreover, inadequate training data can further increase the risk of performance degradation. To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification. Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency. By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance. Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup. Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second. Our models are available at \\url{https://huggingface.co/PowerInfer}","sentences":["Exploiting activation sparsity is a promising approach to significantly accelerating the inference process of large language models (LLMs) without compromising performance.","However, activation sparsity is determined by activation functions, and commonly used ones like SwiGLU and GeGLU exhibit limited sparsity.","Simply replacing these functions with ReLU fails to achieve sufficient sparsity.","Moreover, inadequate training data can further increase the risk of performance degradation.","To address these challenges, we propose a novel dReLU function, which is designed to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsification.","Additionally, we leverage sparse activation patterns within the Feed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to further boost efficiency.","By applying our neuron sparsification method to the Mistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are activated per inference iteration, respectively, while achieving even more powerful model performance.","Evaluation results demonstrate that this sparsity achieves a 2-5x decoding speedup.","Remarkably, on mobile phones, our TurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.","Our models are available at \\url{https://huggingface.co/PowerInfer}"],"url":"http://arxiv.org/abs/2406.05955v1","category":"cs.LG"}
{"created":"2024-06-10 01:20:31","title":"Decoupling regularization from the action space","abstract":"Regularized reinforcement learning (RL), particularly the entropy-regularized kind, has gained traction in optimal control and inverse RL. While standard unregularized RL methods remain unaffected by changes in the number of actions, we show that it can severely impact their regularized counterparts. This paper demonstrates the importance of decoupling the regularizer from the action space: that is, to maintain a consistent level of regularization regardless of how many actions are involved to avoid over-regularization. Whereas the problem can be avoided by introducing a task-specific temperature parameter, it is often undesirable and cannot solve the problem when action spaces are state-dependent. In the state-dependent action context, different states with varying action spaces are regularized inconsistently. We introduce two solutions: a static temperature selection approach and a dynamic counterpart, universally applicable where this problem arises. Implementing these changes improves performance on the DeepMind control suite in static and dynamic temperature regimes and a biological sequence design task.","sentences":["Regularized reinforcement learning (RL), particularly the entropy-regularized kind, has gained traction in optimal control and inverse RL.","While standard unregularized RL methods remain unaffected by changes in the number of actions, we show that it can severely impact their regularized counterparts.","This paper demonstrates the importance of decoupling the regularizer from the action space: that is, to maintain a consistent level of regularization regardless of how many actions are involved to avoid over-regularization.","Whereas the problem can be avoided by introducing a task-specific temperature parameter, it is often undesirable and cannot solve the problem when action spaces are state-dependent.","In the state-dependent action context, different states with varying action spaces are regularized inconsistently.","We introduce two solutions: a static temperature selection approach and a dynamic counterpart, universally applicable where this problem arises.","Implementing these changes improves performance on the DeepMind control suite in static and dynamic temperature regimes and a biological sequence design task."],"url":"http://arxiv.org/abs/2406.05953v1","category":"cs.LG"}
{"created":"2024-06-10 00:05:49","title":"M2CVD: Multi-Model Collaboration for Code Vulnerability Detection","abstract":"Large Language Models (LLMs) have strong capabilities in code comprehension, but fine-tuning costs and semantic alignment issues limit their project-specific optimization; conversely, code models such CodeBERT are easy to fine-tune, but it is often difficult to learn vulnerability semantics from complex code languages. To address these challenges, this paper introduces the Multi-Model Collaborative Vulnerability Detection approach (M2CVD) that leverages the strong capability of analyzing vulnerability semantics from LLMs to improve the detection accuracy of code models. M2CVD employs a novel collaborative process: first enhancing the quality of vulnerability semantic description produced by LLMs through the understanding of project code by code models, and then using these improved vulnerability semantic description to boost the detection accuracy of code models. We demonstrated M2CVD's effectiveness on two real-world datasets, where M2CVD significantly outperformed the baseline. In addition, we demonstrate that the M2CVD collaborative method can extend to other different LLMs and code models to improve their accuracy in vulnerability detection tasks.","sentences":["Large Language Models (LLMs) have strong capabilities in code comprehension, but fine-tuning costs and semantic alignment issues limit their project-specific optimization; conversely, code models such CodeBERT are easy to fine-tune, but it is often difficult to learn vulnerability semantics from complex code languages.","To address these challenges, this paper introduces the Multi-Model Collaborative Vulnerability Detection approach (M2CVD) that leverages the strong capability of analyzing vulnerability semantics from LLMs to improve the detection accuracy of code models.","M2CVD employs a novel collaborative process: first enhancing the quality of vulnerability semantic description produced by LLMs through the understanding of project code by code models, and then using these improved vulnerability semantic description to boost the detection accuracy of code models.","We demonstrated M2CVD's effectiveness on two real-world datasets, where M2CVD significantly outperformed the baseline.","In addition, we demonstrate that the M2CVD collaborative method can extend to other different LLMs and code models to improve their accuracy in vulnerability detection tasks."],"url":"http://arxiv.org/abs/2406.05940v1","category":"cs.SE"}
{"created":"2024-06-09 21:44:06","title":"Contrastive Learning from Synthetic Audio Doppelgangers","abstract":"Learning robust audio representations currently demands extensive datasets of real-world sound recordings. By applying artificial transformations to these recordings, models can learn to recognize similarities despite subtle variations through techniques like contrastive learning. However, these transformations are only approximations of the true diversity found in real-world sounds, which are generated by complex interactions of physical processes, from vocal cord vibrations to the resonance of musical instruments. We propose a solution to both the data scale and transformation limitations, leveraging synthetic audio. By randomly perturbing the parameters of a sound synthesizer, we generate audio doppelg\\\"angers-synthetic positive pairs with causally manipulated variations in timbre, pitch, and temporal envelopes. These variations, difficult to achieve through transformations of existing audio, provide a rich source of contrastive information. Despite the shift to randomly generated synthetic data, our method produces strong representations, competitive with real data on standard audio classification benchmarks. Notably, our approach is lightweight, requires no data storage, and has only a single hyperparameter, which we extensively analyze. We offer this method as a complement to existing strategies for contrastive learning in audio, using synthesized sounds to reduce the data burden on practitioners.","sentences":["Learning robust audio representations currently demands extensive datasets of real-world sound recordings.","By applying artificial transformations to these recordings, models can learn to recognize similarities despite subtle variations through techniques like contrastive learning.","However, these transformations are only approximations of the true diversity found in real-world sounds, which are generated by complex interactions of physical processes, from vocal cord vibrations to the resonance of musical instruments.","We propose a solution to both the data scale and transformation limitations, leveraging synthetic audio.","By randomly perturbing the parameters of a sound synthesizer, we generate audio doppelg\\\"angers-synthetic positive pairs with causally manipulated variations in timbre, pitch, and temporal envelopes.","These variations, difficult to achieve through transformations of existing audio, provide a rich source of contrastive information.","Despite the shift to randomly generated synthetic data, our method produces strong representations, competitive with real data on standard audio classification benchmarks.","Notably, our approach is lightweight, requires no data storage, and has only a single hyperparameter, which we extensively analyze.","We offer this method as a complement to existing strategies for contrastive learning in audio, using synthesized sounds to reduce the data burden on practitioners."],"url":"http://arxiv.org/abs/2406.05923v1","category":"cs.SD"}
{"created":"2024-06-09 21:10:53","title":"China's Rising Leadership in Global Science","abstract":"Major shifts in the global system of science and technology are destabilizing the global status order and demonstrating the capacity for emerging countries like China and India to exert greater influence. In order to measure changes in the global scientific system, we develop a framework to assess the hierarchical position of countries in the international scientific collaboration network. Using a machine-learning model to identify the leaders of 5,966,623 scientific teams that collaborated across international borders, we show that Chinese scientists substantially narrowed their leadership deficit with scientists from the US, UK, and EU between 1990 and 2023 in absolute terms. Consequently, China and the US are on track to reach an equal number of team leaders engaged in bilateral collaborations between 2027 and 2028. Nevertheless, Chinese progress has been considerably slower in per-collaborator terms: after adjusting for the number of non-leaders from each country, our models do not predict parity between the US and China until after 2087. These dynamics extend to 11 critical technology areas central to ongoing diplomacy between the two nations, such AI, Semiconductors, and Advanced Communications, and to China's scientific leadership with respect to the European Union and the United Kingdom. Thus, while China's elite scientists are achieving leadership in the international scientific community, China's scientific enterprise continues to face developmental constraints. We conclude by reviewing several steps that Chinese science is taking to overcome these constraints, by increasing its engagement in scientific training and research in signatory nations to the Belt and Road Initiative.","sentences":["Major shifts in the global system of science and technology are destabilizing the global status order and demonstrating the capacity for emerging countries like China and India to exert greater influence.","In order to measure changes in the global scientific system, we develop a framework to assess the hierarchical position of countries in the international scientific collaboration network.","Using a machine-learning model to identify the leaders of 5,966,623 scientific teams that collaborated across international borders, we show that Chinese scientists substantially narrowed their leadership deficit with scientists from the US, UK, and EU between 1990 and 2023 in absolute terms.","Consequently, China and the US are on track to reach an equal number of team leaders engaged in bilateral collaborations between 2027 and 2028.","Nevertheless, Chinese progress has been considerably slower in per-collaborator terms: after adjusting for the number of non-leaders from each country, our models do not predict parity between the US and China until after 2087.","These dynamics extend to 11 critical technology areas central to ongoing diplomacy between the two nations, such AI, Semiconductors, and Advanced Communications, and to China's scientific leadership with respect to the European Union and the United Kingdom.","Thus, while China's elite scientists are achieving leadership in the international scientific community, China's scientific enterprise continues to face developmental constraints.","We conclude by reviewing several steps that Chinese science is taking to overcome these constraints, by increasing its engagement in scientific training and research in signatory nations to the Belt and Road Initiative."],"url":"http://arxiv.org/abs/2406.05917v1","category":"econ.GN"}
{"created":"2024-06-09 18:40:24","title":"LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning","abstract":"Developing interactive systems that leverage natural language instructions to solve complex robotic control tasks has been a long-desired goal in the robotics community. Large Language Models (LLMs) have demonstrated exceptional abilities in handling complex tasks, including logical reasoning, in-context learning, and code generation. However, predicting low-level robotic actions using LLMs poses significant challenges. Additionally, the complexity of such tasks usually demands the acquisition of policies to execute diverse subtasks and combine them to attain the ultimate objective. Hierarchical Reinforcement Learning (HRL) is an elegant approach for solving such tasks, which provides the intuitive benefits of temporal abstraction and improved exploration. However, HRL faces the recurring issue of non-stationarity due to unstable lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework that leverages language instructions to generate a stationary reward function for the higher-level policy. Since the language-guided reward is unaffected by the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an elegant method for leveraging language instructions to solve robotic control tasks. To analyze the efficacy of our approach, we perform empirical analysis and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our approach attains success rates exceeding 70$\\%$ in challenging, sparse-reward robotic navigation and manipulation environments where the baselines fail to achieve any significant progress. Additionally, we conduct real-world robotic manipulation experiments and demonstrate that CRISP shows impressive generalization in real-world scenarios.","sentences":["Developing interactive systems that leverage natural language instructions to solve complex robotic control tasks has been a long-desired goal in the robotics community.","Large Language Models (LLMs) have demonstrated exceptional abilities in handling complex tasks, including logical reasoning, in-context learning, and code generation.","However, predicting low-level robotic actions using LLMs poses significant challenges.","Additionally, the complexity of such tasks usually demands the acquisition of policies to execute diverse subtasks and combine them to attain the ultimate objective.","Hierarchical Reinforcement Learning (HRL) is an elegant approach for solving such tasks, which provides the intuitive benefits of temporal abstraction and improved exploration.","However, HRL faces the recurring issue of non-stationarity due to unstable lower primitive behaviour.","In this work, we propose LGR2, a novel HRL framework that leverages language instructions to generate a stationary reward function for the higher-level policy.","Since the language-guided reward is unaffected by the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an elegant method for leveraging language instructions to solve robotic control tasks.","To analyze the efficacy of our approach, we perform empirical analysis and demonstrate that LGR2 effectively alleviates non-stationarity in HRL.","Our approach attains success rates exceeding 70$\\%$ in challenging, sparse-reward robotic navigation and manipulation environments where the baselines fail to achieve any significant progress.","Additionally, we conduct real-world robotic manipulation experiments and demonstrate that CRISP shows impressive generalization in real-world scenarios."],"url":"http://arxiv.org/abs/2406.05881v1","category":"cs.LG"}
{"created":"2024-06-09 18:13:36","title":"Zero-Shot End-To-End Spoken Question Answering In Medical Domain","abstract":"In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5\\%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts.","sentences":["In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development.","Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation.","To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain.","Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems.","Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5\\%.","These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts."],"url":"http://arxiv.org/abs/2406.05876v1","category":"cs.CL"}
{"created":"2024-06-09 17:55:55","title":"Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents","abstract":"Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database, then generating an answer by applying an LLM to the retrieved documents.   We demonstrate that RAG systems that operate on databases with potentially untrusted content are vulnerable to a new class of denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and, furthermore, result in the RAG system not answering the query - ostensibly because it lacks the information or because the answer is unsafe.   We describe and analyze several methods for generating blocker documents, including a new method based on black-box optimization that does not require the adversary to know the embedding or LLM used by the target RAG system, nor access to an auxiliary LLM to generate blocker documents. We measure the efficacy of the considered methods against several LLMs and embeddings, and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.","sentences":["Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database, then generating an answer by applying an LLM to the retrieved documents.   ","We demonstrate that RAG systems that operate on databases with potentially untrusted content are vulnerable to a new class of denial-of-service attacks we call jamming.","An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and, furthermore, result in the RAG system not answering the query - ostensibly because it lacks the information or because the answer is unsafe.   ","We describe and analyze several methods for generating blocker documents, including a new method based on black-box optimization that does not require the adversary to know the embedding or LLM used by the target RAG system, nor access to an auxiliary LLM to generate blocker documents.","We measure the efficacy of the considered methods against several LLMs and embeddings, and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming.","We then discuss defenses against blocker documents."],"url":"http://arxiv.org/abs/2406.05870v1","category":"cs.CR"}
{"created":"2024-06-10 14:36:58","title":"Optimal Preprocessing for Answering On-Line Product Queries","abstract":"We examine the amount of preprocessing needed for answering certain on-line queries as fast as possible. We start with the following basic problem. Suppose we are given a semigroup $(S,\\circ )$. Let $s_1 ,\\ldots, s_n$ be elements of $S$. We want to answer on-line queries of the form, ``What is the product $s_i \\circ s_{i+1} \\circ \\cdots \\circ s_{j-1} \\circ s_j$?'' for any given $1\\le i\\le j\\le n$. We show that a preprocessing of $\\Theta(n \\lambda (k,n))$ time and space is both necessary and sufficient to answer each such query in at most $k$ steps, for any fixed $k$. The function $\\lambda (k,\\cdot)$ is the inverse of a certain function at the $\\lfloor {k/2}\\rfloor$-th level of the primitive recursive hierarchy. In case linear preprocessing is desired, we show that one can answer each such query in $O( \\alpha (n))$ steps and that this is best possible. The function $\\alpha (n)$ is the inverse Ackermann function.   We also consider the following extended problem. Let $T$ be a tree with an element of $S$ associated with each of its vertices. We want to answer on-line queries of the form, ``What is the product of the elements associated with the vertices along the path from $u$ to $v$?'' for any pair of vertices $u$ and $v$ in $T$. We derive results that are similar to the above, for the preprocessing needed for answering such queries.   All our sequential preprocessing algorithms can be parallelized efficiently to give optimal parallel algorithms which run in $O(\\log n)$ time on a CREW PRAM. These parallel algorithms are optimal in both running time and total number of operations.   Our algorithms, especially for the semigroup of the real numbers with the minimum or maximum operations, have various applications in certain graph algorithms, in the utilization of communication networks and in Database retrieval.","sentences":["We examine the amount of preprocessing needed for answering certain on-line queries as fast as possible.","We start with the following basic problem.","Suppose we are given a semigroup $(S,\\circ )$.","Let $s_1 ,\\ldots, s_n$ be elements of $S$. We want to answer on-line queries of the form, ``What is the product $s_i \\circ s_{i+1} \\circ \\cdots \\circ s_{j-1} \\circ s_j$?''","for any given $1\\le i\\le j\\le n$. We show that a preprocessing of $\\Theta(n \\lambda (k,n))$ time and space is both necessary and sufficient to answer each such query in at most $k$ steps, for any fixed $k$. The function $\\lambda (k,\\cdot)$ is the inverse of a certain function at the $\\lfloor {k/2}\\rfloor$-th level of the primitive recursive hierarchy.","In case linear preprocessing is desired, we show that one can answer each such query in $O( \\alpha (n))$ steps and that this is best possible.","The function $\\alpha (n)$ is the inverse Ackermann function.   ","We also consider the following extended problem.","Let $T$ be a tree with an element of $S$ associated with each of its vertices.","We want to answer on-line queries of the form, ``What is the product of the elements associated with the vertices along the path from $u$ to $v$?''","for any pair of vertices $u$ and $v$ in $T$. We derive results that are similar to the above, for the preprocessing needed for answering such queries.   ","All our sequential preprocessing algorithms can be parallelized efficiently to give optimal parallel algorithms which run in $O(\\log","n)$ time on a CREW PRAM.","These parallel algorithms are optimal in both running time and total number of operations.   ","Our algorithms, especially for the semigroup of the real numbers with the minimum or maximum operations, have various applications in certain graph algorithms, in the utilization of communication networks and in Database retrieval."],"url":"http://arxiv.org/abs/2406.06321v1","category":"cs.DS"}
{"created":"2024-06-10 13:51:56","title":"Mode-Coupling-Driven Frequency Stabilization in Semiconductor Lasers with Bragg Grating Waveguide","abstract":"Precisely stabilizing laser frequency is crucial for advancing laser technology and unlocking the full potential of various quantum technologies. Here, we propose a compact device for stabilizing frequency of a semiconductor laser through mode coupling effects, which provides enhanced sensitivity. Our proposed architecture features a main ridge waveguide with a Bragg grating, flanked by two curved ridge waveguides. This configuration exhibits an optical phenomenon characterized by a transmission crossing at the wavelength of the Bragg grating. Using particle swarm optimization strategy and employing efficient figures of merit, we achieve a high transmission crossing. The observed asymmetric transmission crossing not only holds the promise for an efficient and compact on-chip laser frequency stabilizer, but also fosters the development of novel sensing platforms with heightened sensitivity.","sentences":["Precisely stabilizing laser frequency is crucial for advancing laser technology and unlocking the full potential of various quantum technologies.","Here, we propose a compact device for stabilizing frequency of a semiconductor laser through mode coupling effects, which provides enhanced sensitivity.","Our proposed architecture features a main ridge waveguide with a Bragg grating, flanked by two curved ridge waveguides.","This configuration exhibits an optical phenomenon characterized by a transmission crossing at the wavelength of the Bragg grating.","Using particle swarm optimization strategy and employing efficient figures of merit, we achieve a high transmission crossing.","The observed asymmetric transmission crossing not only holds the promise for an efficient and compact on-chip laser frequency stabilizer, but also fosters the development of novel sensing platforms with heightened sensitivity."],"url":"http://arxiv.org/abs/2406.06269v1","category":"physics.optics"}
{"created":"2024-06-10 11:36:27","title":"Computing the Yaglom limit of Markov chains with a single exit state using their excursion measure","abstract":"We prove in this article the existence of the Yaglom limit for Markov chains on discrete state spaces in the setting where the absorbing state is accessible from a single non-absorbing state. We use a representation of the trajectories of this process by its excursion away from death, that allows us to link the Yaglom limit with the large deviations behaviour of the inverse of its local time at the exit state, and to compute its minimal quasi-stationary distribution with its excursion measure.","sentences":["We prove in this article the existence of the Yaglom limit for Markov chains on discrete state spaces in the setting where the absorbing state is accessible from a single non-absorbing state.","We use a representation of the trajectories of this process by its excursion away from death, that allows us to link the Yaglom limit with the large deviations behaviour of the inverse of its local time at the exit state, and to compute its minimal quasi-stationary distribution with its excursion measure."],"url":"http://arxiv.org/abs/2406.06188v1","category":"math.PR"}
{"created":"2024-06-10 08:51:11","title":"Resilient Growth of Highly Crystalline Topological Insulator-Superconductor Heterostructure Enabled by Ex-situ Nitride Film","abstract":"Highly crystalline and easily feasible topological insulator-superconductor (TI-SC) heterostructures are crucial for the development of practical topological qubit devices. The optimal superconducting layer for TI-SC heterostructures should be highly resilient against external contaminations and structurally compatible with TIs. In this study, we provide a solution to this challenge by showcasing the growth of a highly crystalline TI-SC heterostructure using refractory TiN (111) as the superconducting layer. This approach can eliminate the need for in-situ cleaving or growth. More importantly, the TiN surface shows high resilience against contaminations during air exposure, as demonstrated by the successful recyclable growth of Bi2Se3. Our findings indicate that TI-SC heterostructures based on nitride films are compatible with device fabrication techniques, paving a path to the realization of practical topological qubit devices in the future.","sentences":["Highly crystalline and easily feasible topological insulator-superconductor (TI-SC) heterostructures are crucial for the development of practical topological qubit devices.","The optimal superconducting layer for TI-SC heterostructures should be highly resilient against external contaminations and structurally compatible with TIs.","In this study, we provide a solution to this challenge by showcasing the growth of a highly crystalline TI-SC heterostructure using refractory TiN (111) as the superconducting layer.","This approach can eliminate the need for in-situ cleaving or growth.","More importantly, the TiN surface shows high resilience against contaminations during air exposure, as demonstrated by the successful recyclable growth of Bi2Se3.","Our findings indicate that TI-SC heterostructures based on nitride films are compatible with device fabrication techniques, paving a path to the realization of practical topological qubit devices in the future."],"url":"http://arxiv.org/abs/2406.06112v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-10 08:18:26","title":"Stabilizing Solution-Substrate Interaction of Perovskite Ink on PEDOT:PSS for Scalable Blade Coated Narrow Bandgap Perovskite Solar Modules by Gas Quenching","abstract":"The development of scalable 1.25 eV mixed Pb-Sn perovskite solar modules by blade coating lags behind Pb-based perovskites due to limited understanding of solution-substrate interaction of the perovskite ink on PEDOT:PSS and subsequent gas quenching. To address this challenge, we systematically studied the wet film deposition and quenching process to better understand narrow bandgap perovskite film formation on PEDOT:PSS. We found, the wetting of Pb-Sn perovskite ink on PEDOT:PSS is highly unstable over relevant coating time scales, causing the contact angles to decrease rapidly from 42{\\deg} to 16{\\deg} within seconds. This instability leads to localized irregularities in the wet film, resulting in uneven solvent extraction and inhomogeneous nuclei density. As a result, rough perovskite films with voids at the buried interface are obtained. To overcome this problem, we developed a quasi-static wetting process by reducing the blade coating speed, thereby stabilizing the wetting behavior of Pb-Sn perovskite precursor ink on PEDOT:PSS. This optimized process facilitates the deposition of high-quality, void-free Pb-Sn perovskite films with uniform thickness over 8 cm of coating length using moderate (1.4 bar) N2 quenching. We achieved 20 % efficient narrow bandgap perovskite solar cells and mini-modules with 15.8 % active area efficiency on 15.9 cm2.","sentences":["The development of scalable 1.25 eV mixed Pb-Sn perovskite solar modules by blade coating lags behind Pb-based perovskites due to limited understanding of solution-substrate interaction of the perovskite ink on PEDOT:","PSS and subsequent gas quenching.","To address this challenge, we systematically studied the wet film deposition and quenching process to better understand narrow bandgap perovskite film formation on PEDOT:PSS.","We found, the wetting of Pb-Sn perovskite ink on PEDOT:","PSS is highly unstable over relevant coating time scales, causing the contact angles to decrease rapidly from 42{\\deg} to 16{\\deg} within seconds.","This instability leads to localized irregularities in the wet film, resulting in uneven solvent extraction and inhomogeneous nuclei density.","As a result, rough perovskite films with voids at the buried interface are obtained.","To overcome this problem, we developed a quasi-static wetting process by reducing the blade coating speed, thereby stabilizing the wetting behavior of Pb-Sn perovskite precursor ink on PEDOT:PSS.","This optimized process facilitates the deposition of high-quality, void-free Pb-Sn perovskite films with uniform thickness over 8 cm of coating length using moderate (1.4 bar) N2 quenching.","We achieved 20 % efficient narrow bandgap perovskite solar cells and mini-modules with 15.8 % active area efficiency on 15.9 cm2."],"url":"http://arxiv.org/abs/2406.06088v1","category":"physics.app-ph"}
{"created":"2024-06-10 08:12:41","title":"Ecological Data Reveal Imbalances in Collision Avoidance Due to Groups' Social Interaction","abstract":"The relative dynamics in collision avoidance between individual pedestrians and dyads has been recently studied, and it was shown that individuals may intrude dyads that are not socially interacting. Building on this, our current study examines how much each party contributes to collision avoidance by measuring deviations from their intended paths. Our findings suggest that individuals prioritise trajectory efficiency in undisturbed situations, but prioritise safety when encountering dyads, deviating more from their intended path. Non-interacting dyads present a similar behavior, although their trajectories appear to be even more efficient than those of individuals in undisturbed situations, and their deviations during encounters less pronounced. On the other hand, socially interacting dyads are not very efficient in undisturbed situations, and their behavior is mostly unaffected by encounters. These results strongly suggest that group dynamics affects in two ways the behavior of pedestrians, namely it has a dynamical and a social effect. The dynamical effect stabilises their trajectory, while the social one decreases the ability to focus on the external environment, leading to reduced efficiency and safety. Another finding concerns the tendency of individuals to avoid in a more prominent way the interacting dyads as compared to non-interacting ones. This suggests that individuals may assess others' contribution to collision avoidance. An impact parameter analysis reveals that collision risk influences path deviations in pedestrian encounters. For individuals, larger behavioral differences between low and high interaction levels of the dyad occur both when the collision risk is high and during less critical encounters. For dyads, the deviation differences between low and high interaction levels are most pronounced when the individual is on course to pass close to the dyad.","sentences":["The relative dynamics in collision avoidance between individual pedestrians and dyads has been recently studied, and it was shown that individuals may intrude dyads that are not socially interacting.","Building on this, our current study examines how much each party contributes to collision avoidance by measuring deviations from their intended paths.","Our findings suggest that individuals prioritise trajectory efficiency in undisturbed situations, but prioritise safety when encountering dyads, deviating more from their intended path.","Non-interacting dyads present a similar behavior, although their trajectories appear to be even more efficient than those of individuals in undisturbed situations, and their deviations during encounters less pronounced.","On the other hand, socially interacting dyads are not very efficient in undisturbed situations, and their behavior is mostly unaffected by encounters.","These results strongly suggest that group dynamics affects in two ways the behavior of pedestrians, namely it has a dynamical and a social effect.","The dynamical effect stabilises their trajectory, while the social one decreases the ability to focus on the external environment, leading to reduced efficiency and safety.","Another finding concerns the tendency of individuals to avoid in a more prominent way the interacting dyads as compared to non-interacting ones.","This suggests that individuals may assess others' contribution to collision avoidance.","An impact parameter analysis reveals that collision risk influences path deviations in pedestrian encounters.","For individuals, larger behavioral differences between low and high interaction levels of the dyad occur both when the collision risk is high and during less critical encounters.","For dyads, the deviation differences between low and high interaction levels are most pronounced when the individual is on course to pass close to the dyad."],"url":"http://arxiv.org/abs/2406.06084v1","category":"physics.soc-ph"}
{"created":"2024-06-10 05:04:33","title":"The Limits of Interval-Regulated Price Discrimination","abstract":"In this paper, we study third-degree price discrimination in a model first presented in Bergemann, Brooks, and Morris [2015]. Since such price discrimination might create market segments with vastly different posted prices, we consider regulating these prices, specifically, via restricting them to lie within an interval. Given a price interval, we consider segmentations of the market where a seller, who is oblivious to the existence of such regulation, still posts prices within the price interval. We show the following surprising result: For any market and price interval where such segmentation is feasible, there is always a different segmentation that optimally transfers all excess surplus to the consumers. In addition, we characterize the entire space of buyer and seller surplus that are achievable by such segmentation, including maximizing seller surplus, and simultaneously minimizing buyer and seller surplus.","sentences":["In this paper, we study third-degree price discrimination in a model first presented in Bergemann, Brooks, and Morris [2015].","Since such price discrimination might create market segments with vastly different posted prices, we consider regulating these prices, specifically, via restricting them to lie within an interval.","Given a price interval, we consider segmentations of the market where a seller, who is oblivious to the existence of such regulation, still posts prices within the price interval.","We show the following surprising result: For any market and price interval where such segmentation is feasible, there is always a different segmentation that optimally transfers all excess surplus to the consumers.","In addition, we characterize the entire space of buyer and seller surplus that are achievable by such segmentation, including maximizing seller surplus, and simultaneously minimizing buyer and seller surplus."],"url":"http://arxiv.org/abs/2406.06023v1","category":"econ.TH"}
{"created":"2024-06-10 03:51:45","title":"Validation of the estimated Effect of Ankle Foot Orthoses on Spinal Cord Injury Gait Using Subject-Adjusted Musculoskeletal Models","abstract":"Simulation of assistive devices on pathological gait through musculoskeletal models offers the potential and advantages of estimating the effect of the device in several biomechanical variables and the device characteristics ahead of manufacturing. In this study, we introduce a novel musculoskeletal modelling approach to simulate the biomechanical impact of ankle foot orthoses (AFO) on gait in individuals with spinal cord injury (SCI). Leveraging data from the Swiss Paraplegic Center, we constructed anatomically and muscularly scaled models for SCI-AFO users, aiming to predict changes in gait kinematics and kinetics. The importance of this work lies in its potential to enhance rehabilitation strategies and improve quality of life by enabling the pre-manufacturing assessment of assistive devices. Despite the application of musculoskeletal models in simulating walking aids effects in other conditions, no predictive model currently exists for SCI gait. Evaluation through RMSE showed similar results compared with other pathologies, simulation errors ranged between 0.23 to 2.3 degrees in kinematics. Moreover, the model was able to capture ankle joint muscular asymmetries and predict symmetry improvements with AFO use. However, the simulation did not reveal all the AFO effects, indicating a need for more personalized model parameters and optimized muscle activation to fully replicate orthosis effects on SCI gait.","sentences":["Simulation of assistive devices on pathological gait through musculoskeletal models offers the potential and advantages of estimating the effect of the device in several biomechanical variables and the device characteristics ahead of manufacturing.","In this study, we introduce a novel musculoskeletal modelling approach to simulate the biomechanical impact of ankle foot orthoses (AFO) on gait in individuals with spinal cord injury (SCI).","Leveraging data from the Swiss Paraplegic Center, we constructed anatomically and muscularly scaled models for SCI-AFO users, aiming to predict changes in gait kinematics and kinetics.","The importance of this work lies in its potential to enhance rehabilitation strategies and improve quality of life by enabling the pre-manufacturing assessment of assistive devices.","Despite the application of musculoskeletal models in simulating walking aids effects in other conditions, no predictive model currently exists for SCI gait.","Evaluation through RMSE showed similar results compared with other pathologies, simulation errors ranged between 0.23 to 2.3 degrees in kinematics.","Moreover, the model was able to capture ankle joint muscular asymmetries and predict symmetry improvements with AFO use.","However, the simulation did not reveal all the AFO effects, indicating a need for more personalized model parameters and optimized muscle activation to fully replicate orthosis effects on SCI gait."],"url":"http://arxiv.org/abs/2406.06003v1","category":"physics.med-ph"}
{"created":"2024-06-10 01:43:15","title":"Data Caching for Enterprise-Grade Petabyte-Scale OLAP","abstract":"With the exponential growth of data and evolving use cases, petabyte-scale OLAP data platforms are increasingly adopting a model that decouples compute from storage. This shift, evident in organizations like Uber and Meta, introduces operational challenges including massive, read-heavy I/O traffic with potential throttling, as well as skewed and fragmented data access patterns. Addressing these challenges, this paper introduces the Alluxio local (edge) cache, a highly effective architectural optimization tailored for such environments. This embeddable cache, optimized for petabyte-scale data analytics, leverages local SSD resources to alleviate network I/O and API call pressures, significantly improving data transfer efficiency. Integrated with OLAP systems like Presto and storage services like HDFS, the Alluxio local cache has demonstrated its effectiveness in handling large-scale, enterprise-grade workloads over three years of deployment at Uber and Meta. We share insights and operational experiences in implementing these optimizations, providing valuable perspectives on managing modern, massive-scale OLAP workloads.","sentences":["With the exponential growth of data and evolving use cases, petabyte-scale OLAP data platforms are increasingly adopting a model that decouples compute from storage.","This shift, evident in organizations like Uber and Meta, introduces operational challenges including massive, read-heavy I/O traffic with potential throttling, as well as skewed and fragmented data access patterns.","Addressing these challenges, this paper introduces the Alluxio local (edge) cache, a highly effective architectural optimization tailored for such environments.","This embeddable cache, optimized for petabyte-scale data analytics, leverages local SSD resources to alleviate network I/O and API call pressures, significantly improving data transfer efficiency.","Integrated with OLAP systems like Presto and storage services like HDFS, the Alluxio local cache has demonstrated its effectiveness in handling large-scale, enterprise-grade workloads over three years of deployment at Uber and Meta.","We share insights and operational experiences in implementing these optimizations, providing valuable perspectives on managing modern, massive-scale OLAP workloads."],"url":"http://arxiv.org/abs/2406.05962v1","category":"cs.DC"}
{"created":"2024-06-10 01:37:18","title":"Speedup of high-order unconstrained binary optimization using quantum Z2 lattice gauge theory","abstract":"How to quickly solve the problem of high-order unconstrained binary optimization (HUBO) has attracted much attention, because of its importance and wide-range applications. Here we implement HUBO using a quantum adiabatic algorithm and achieve algorithmic speedup by introducing gauge symmetry into the algorithm. Gauge symmetry enforces the state to be in the instantaneous ground state, further speeding up the computation. Specifically we map the HUBO problem to quantum Z2 lattice gauge theory defined on the dual graph. The gauge operators are found by using the closed-loop-search algorithm, and subsequently the speedup scheme with gauge symmetry for HUBO problem is developed. As an example demonstrated in the classical computers, we present the mathematical formulation of our speedup scheme and propose the so-called gauged local annealing (gLQA) , which is the local quantum annealing (LQA) protected by the gauge symmetry. We then use gLQA to calculate the ground state energy of the Z2 gauge theory. gLQA reduces the computational time by one order of magnitude from that of LQA.","sentences":["How to quickly solve the problem of high-order unconstrained binary optimization (HUBO) has attracted much attention, because of its importance and wide-range applications.","Here we implement HUBO using a quantum adiabatic algorithm and achieve algorithmic speedup by introducing gauge symmetry into the algorithm.","Gauge symmetry enforces the state to be in the instantaneous ground state, further speeding up the computation.","Specifically we map the HUBO problem to quantum Z2 lattice gauge theory defined on the dual graph.","The gauge operators are found by using the closed-loop-search algorithm, and subsequently the speedup scheme with gauge symmetry for HUBO problem is developed.","As an example demonstrated in the classical computers, we present the mathematical formulation of our speedup scheme and propose the so-called gauged local annealing (gLQA) , which is the local quantum annealing (LQA) protected by the gauge symmetry.","We then use gLQA to calculate the ground state energy of the Z2 gauge theory.","gLQA reduces the computational time by one order of magnitude from that of LQA."],"url":"http://arxiv.org/abs/2406.05958v1","category":"quant-ph"}
{"created":"2024-06-10 00:11:05","title":"Jailbreaking Quantum Computers","abstract":"This work presented the first thorough exploration of the attacks on the interface between gate-level and pulse-level quantum circuits and pulse-level quantum circuits themselves. Typically, quantum circuits and programs that execute on quantum computers, are defined using gate-level primitives. However, to improve the expressivity of quantum circuits and to allow better optimization, pulse-level circuits are now often used. The attacks presented in this work leverage the inconsistency between the gate-level description of the custom gate, and the actual, low-level pulse implementation of this gate. By manipulating the custom gate specification, this work proposes numerous attacks: qubit plunder, qubit block, qubit reorder, timing mismatch, frequency mismatch, phase mismatch, and waveform mismatch. This work demonstrates these attacks on the real quantum computer and simulator, and shows that most current software development kits are vulnerable to these new types of attacks. In the end, this work proposes a defense framework. The exploration of security and privacy issues of the rising pulse-level quantum circuits provides insight into the future development of secure quantum software development kits and quantum computer systems.","sentences":["This work presented the first thorough exploration of the attacks on the interface between gate-level and pulse-level quantum circuits and pulse-level quantum circuits themselves.","Typically, quantum circuits and programs that execute on quantum computers, are defined using gate-level primitives.","However, to improve the expressivity of quantum circuits and to allow better optimization, pulse-level circuits are now often used.","The attacks presented in this work leverage the inconsistency between the gate-level description of the custom gate, and the actual, low-level pulse implementation of this gate.","By manipulating the custom gate specification, this work proposes numerous attacks: qubit plunder, qubit block, qubit reorder, timing mismatch, frequency mismatch, phase mismatch, and waveform mismatch.","This work demonstrates these attacks on the real quantum computer and simulator, and shows that most current software development kits are vulnerable to these new types of attacks.","In the end, this work proposes a defense framework.","The exploration of security and privacy issues of the rising pulse-level quantum circuits provides insight into the future development of secure quantum software development kits and quantum computer systems."],"url":"http://arxiv.org/abs/2406.05941v1","category":"cs.CR"}
{"created":"2024-06-09 23:42:08","title":"Discrete vs. continuous in the semiclassical limit","abstract":"We compare the bottom of the spectrum of discrete and continuous Schr\\\"odinger operators with periodic potentials with barriers at the boundaries of their fundamental domains. Our results show that these energy levels coincide in the semiclassical limit and we provide an explicit rate of convergence. We demonstrate the optimality of our results by using Bohr-Sommerfeld quantization conditions for potentials exhibiting non-degenerate wells, and by numerical experiments for more general potentials. We also investigate the dependence of the spectrum of the discrete semiclassical Schr\\\"odinger operator on the semiclassical parameter $h$ and show that it can be discontinuous.","sentences":["We compare the bottom of the spectrum of discrete and continuous Schr\\\"odinger operators with periodic potentials with barriers at the boundaries of their fundamental domains.","Our results show that these energy levels coincide in the semiclassical limit and we provide an explicit rate of convergence.","We demonstrate the optimality of our results by using Bohr-Sommerfeld quantization conditions for potentials exhibiting non-degenerate wells, and by numerical experiments for more general potentials.","We also investigate the dependence of the spectrum of the discrete semiclassical Schr\\\"odinger operator on the semiclassical parameter $h$ and show that it can be discontinuous."],"url":"http://arxiv.org/abs/2406.05934v1","category":"math.SP"}
{"created":"2024-06-09 21:39:40","title":"Near-field radiative heat transfer between a nanoparticle and a graphene grating","abstract":"We investigate the near-field radiative heat transfer between a normally and/or laterally shifted nanoparticle and a planar fused silica slab coated with a strip graphene grating. For this study we develop and use a scattering matrix approach derived from Fourier modal method augmented with local basis functions. We find that adding a graphene sheet coating on the slab can already enhance the heat flux by about 85%. We show that by patterning the graphene sheet coating into a grating, the heat flux is further increased, and this happens thanks to the a topological transition of the plasmonic modes from circular to hyperbolic one, which allows for more energy transfer. The lateral shift affects the accessible range of high-$k$ modes and thus affects the heat flux, too. By moving the nanoparticle laterally above the graphene grating, we can obtain an optimal heat flux with strong chemical potential dependance above the strips. For a fixed graphene grating period ($D=1\\mu$m) and not too large normal shift (separation $d<800$nm), two different types of lateral shift effects (e.g., enhancement and inhibition) on heat transfer have been observed. As the separation $d$ is further increased, the lateral shift effect becomes less important. We show that the lateral shift effect is sensitive to the geometric factor $d/D$. Two distinct asymptotic regimes are proposed: (1) the inhibition regime ($d/D<0.85$), where the lateral shift reduces the heat transfer, and (2) the neutral regime ($d/D \\geq 0.85$) where the effect of the lateral shift is negligible. In general, we can say that the geometric factor $d/D \\approx 0.85$ is a critical point for the lateral shift effect. Our predictions can have relevant implications to the radiative heat transfer and energy management at the nano/micro scale.","sentences":["We investigate the near-field radiative heat transfer between a normally and/or laterally shifted nanoparticle and a planar fused silica slab coated with a strip graphene grating.","For this study we develop and use a scattering matrix approach derived from Fourier modal method augmented with local basis functions.","We find that adding a graphene sheet coating on the slab can already enhance the heat flux by about 85%.","We show that by patterning the graphene sheet coating into a grating, the heat flux is further increased, and this happens thanks to the a topological transition of the plasmonic modes from circular to hyperbolic one, which allows for more energy transfer.","The lateral shift affects the accessible range of high-$k$ modes and thus affects the heat flux, too.","By moving the nanoparticle laterally above the graphene grating, we can obtain an optimal heat flux with strong chemical potential dependance above the strips.","For a fixed graphene grating period ($D=1\\mu$m) and not too large normal shift (separation $d<800$nm), two different types of lateral shift effects (e.g., enhancement and inhibition) on heat transfer have been observed.","As the separation $d$ is further increased, the lateral shift effect becomes less important.","We show that the lateral shift effect is sensitive to the geometric factor $d/D$. Two distinct asymptotic regimes are proposed: (1) the inhibition regime ($d/D<0.85$), where the lateral shift reduces the heat transfer, and (2) the neutral regime ($d/D \\geq 0.85$) where the effect of the lateral shift is negligible.","In general, we can say that the geometric factor $d/D \\approx 0.85$ is a critical point for the lateral shift effect.","Our predictions can have relevant implications to the radiative heat transfer and energy management at the nano/micro scale."],"url":"http://arxiv.org/abs/2406.05921v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-09 21:07:34","title":"Reforming Quantum Microgrid Formation","abstract":"This letter introduces a novel compact and lossless quantum microgrid formation (qMGF) approach to achieve efficient operational optimization of the power system and improvement of resilience. This is achieved through lossless reformulation to ensure that the results are equivalent to those produced by the classical MGF by exploiting graph-theory-empowered quadratic unconstrained binary optimization (QUBO) that avoids the need for redundant encoding of continuous variables. Additionally, the qMGF approach utilizes a compact formulation that requires significantly fewer qubits compared to other quantum methods thereby enabling a high-accuracy and low-complexity deployment of qMGF on near-term quantum computers. Case studies on real quantum processing units (QPUs) empirically demonstrated that qMGF can achieve the same high accuracy as classic results with a significantly reduced number of qubits.","sentences":["This letter introduces a novel compact and lossless quantum microgrid formation (qMGF) approach to achieve efficient operational optimization of the power system and improvement of resilience.","This is achieved through lossless reformulation to ensure that the results are equivalent to those produced by the classical MGF by exploiting graph-theory-empowered quadratic unconstrained binary optimization (QUBO) that avoids the need for redundant encoding of continuous variables.","Additionally, the qMGF approach utilizes a compact formulation that requires significantly fewer qubits compared to other quantum methods thereby enabling a high-accuracy and low-complexity deployment of qMGF on near-term quantum computers.","Case studies on real quantum processing units (QPUs) empirically demonstrated that qMGF can achieve the same high accuracy as classic results with a significantly reduced number of qubits."],"url":"http://arxiv.org/abs/2406.05916v1","category":"quant-ph"}
{"created":"2024-06-09 20:46:21","title":"Some facts about the optimality of the LSE in the Gaussian sequence model with convex constraint","abstract":"We consider a convex constrained Gaussian sequence model and characterize necessary and sufficient conditions for the least squares estimator (LSE) to be optimal in a minimax sense. For a closed convex set $K\\subset \\mathbb{R}^n$ we observe $Y=\\mu+\\xi$ for $\\xi\\sim N(0,\\sigma^2\\mathbb{I}_n)$ and $\\mu\\in K$ and aim to estimate $\\mu$. We characterize the worst case risk of the LSE in multiple ways by analyzing the behavior of the local Gaussian width on $K$. We demonstrate that optimality is equivalent to a Lipschitz property of the local Gaussian width mapping. We also provide theoretical algorithms that search for the worst case risk. We then provide examples showing optimality or suboptimality of the LSE on various sets, including $\\ell_p$ balls for $p\\in[1,2]$, pyramids, solids of revolution, and multivariate isotonic regression, among others.","sentences":["We consider a convex constrained Gaussian sequence model and characterize necessary and sufficient conditions for the least squares estimator (LSE) to be optimal in a minimax sense.","For a closed convex set $K\\subset \\mathbb{R}^n$ we observe $Y=\\mu+\\xi$ for $\\xi\\sim N(0,\\sigma^2\\mathbb{I}_n)$ and $\\mu\\in K$ and aim to estimate $\\mu$. We characterize the worst case risk of the LSE in multiple ways by analyzing the behavior of the local Gaussian width on $K$. We demonstrate that optimality is equivalent to a Lipschitz property of the local Gaussian width mapping.","We also provide theoretical algorithms that search for the worst case risk.","We then provide examples showing optimality or suboptimality of the LSE on various sets, including $\\ell_p$ balls for $p\\in[1,2]$, pyramids, solids of revolution, and multivariate isotonic regression, among others."],"url":"http://arxiv.org/abs/2406.05911v1","category":"math.ST"}
{"created":"2024-06-09 16:34:03","title":"Fast and Certifiable Trajectory Optimization","abstract":"We propose semidefinite trajectory optimization (STROM), a framework that computes fast and certifiably optimal solutions for nonconvex trajectory optimization problems defined by polynomial objectives and constraints. STROM employs sparse second-order Lasserre's hierarchy to generate semidefinite program (SDP) relaxations of trajectory optimization. Different from existing tools (e.g., YALMIP and SOSTOOLS in Matlab), STROM generates chain-like multiple-block SDPs with only positive semidefinite (PSD) variables. Moreover, STROM does so two orders of magnitude faster. Underpinning STROM is cuADMM, the first ADMM-based SDP solver implemented in CUDA and runs in GPUs. cuADMM builds upon the symmetric Gauss-Seidel ADMM algorithm and leverages GPU parallelization to speedup solving sparse linear systems and projecting onto PSD cones. In five trajectory optimization problems (inverted pendulum, cart-pole, vehicle landing, flying robot, and car back-in), cuADMM computes optimal trajectories (with certified suboptimality below 1%) in minutes (when other solvers take hours or run out of memory) and seconds (when others take minutes). Further, when warmstarted by data-driven initialization in the inverted pendulum problem, cuADMM delivers real-time performance: providing certifiably optimal trajectories in 0.66 seconds despite the SDP has 49,500 variables and 47,351 constraints.","sentences":["We propose semidefinite trajectory optimization (STROM), a framework that computes fast and certifiably optimal solutions for nonconvex trajectory optimization problems defined by polynomial objectives and constraints.","STROM employs sparse second-order Lasserre's hierarchy to generate semidefinite program (SDP) relaxations of trajectory optimization.","Different from existing tools (e.g., YALMIP and SOSTOOLS in Matlab), STROM generates chain-like multiple-block SDPs with only positive semidefinite (PSD) variables.","Moreover, STROM does so two orders of magnitude faster.","Underpinning STROM is cuADMM, the first ADMM-based SDP solver implemented in CUDA and runs in GPUs.","cuADMM builds upon the symmetric Gauss-Seidel ADMM algorithm and leverages GPU parallelization to speedup solving sparse linear systems and projecting onto PSD cones.","In five trajectory optimization problems (inverted pendulum, cart-pole, vehicle landing, flying robot, and car back-in), cuADMM computes optimal trajectories (with certified suboptimality below 1%) in minutes (when other solvers take hours or run out of memory) and seconds (when others take minutes).","Further, when warmstarted by data-driven initialization in the inverted pendulum problem, cuADMM delivers real-time performance: providing certifiably optimal trajectories in 0.66 seconds despite the SDP has 49,500 variables and 47,351 constraints."],"url":"http://arxiv.org/abs/2406.05846v1","category":"math.OC"}
{"created":"2024-06-09 15:37:28","title":"Probabilistic Approach to Black-Box Binary Optimization with Budget Constraints: Application to Sensor Placement","abstract":"We present a fully probabilistic approach for solving binary optimization problems with black-box objective functions and with budget constraints. In the probabilistic approach, the optimization variable is viewed as a random variable and is associated with a parametric probability distribution. The original optimization problem is replaced with an optimization over the expected value of the original objective, which is then optimized over the probability distribution parameters. The resulting optimal parameter (optimal policy) is used to sample the binary space to produce estimates of the optimal solution(s) of the original binary optimization problem. The probability distribution is chosen from the family of Bernoulli models because the optimization variable is binary. The optimization constraints generally restrict the feasibility region. This can be achieved by modeling the random variable with a conditional distribution given satisfiability of the constraints. Thus, in this work we develop conditional Bernoulli distributions to model the random variable conditioned by the total number of nonzero entries, that is, the budget constraint. This approach (a) is generally applicable to binary optimization problems with nonstochastic black-box objective functions and budget constraints; (b) accounts for budget constraints by employing conditional probabilities that sample only the feasible region and thus considerably reduces the computational cost compared with employing soft constraints; and (c) does not employ soft constraints and thus does not require tuning of a regularization parameter, for example to promote sparsity, which is challenging in sensor placement optimization problems. The proposed approach is verified numerically by using an idealized bilinear binary optimization problem and is validated by using a sensor placement experiment in a parameter identification setup.","sentences":["We present a fully probabilistic approach for solving binary optimization problems with black-box objective functions and with budget constraints.","In the probabilistic approach, the optimization variable is viewed as a random variable and is associated with a parametric probability distribution.","The original optimization problem is replaced with an optimization over the expected value of the original objective, which is then optimized over the probability distribution parameters.","The resulting optimal parameter (optimal policy) is used to sample the binary space to produce estimates of the optimal solution(s) of the original binary optimization problem.","The probability distribution is chosen from the family of Bernoulli models because the optimization variable is binary.","The optimization constraints generally restrict the feasibility region.","This can be achieved by modeling the random variable with a conditional distribution given satisfiability of the constraints.","Thus, in this work we develop conditional Bernoulli distributions to model the random variable conditioned by the total number of nonzero entries, that is, the budget constraint.","This approach (a) is generally applicable to binary optimization problems with nonstochastic black-box objective functions and budget constraints; (b) accounts for budget constraints by employing conditional probabilities that sample only the feasible region and thus considerably reduces the computational cost compared with employing soft constraints; and (c) does not employ soft constraints and thus does not require tuning of a regularization parameter, for example to promote sparsity, which is challenging in sensor placement optimization problems.","The proposed approach is verified numerically by using an idealized bilinear binary optimization problem and is validated by using a sensor placement experiment in a parameter identification setup."],"url":"http://arxiv.org/abs/2406.05830v1","category":"math.OC"}
{"created":"2024-06-09 15:14:53","title":"Symmetric Matrix Completion with ReLU Sampling","abstract":"We study the problem of symmetric positive semi-definite low-rank matrix completion (MC) with deterministic entry-dependent sampling. In particular, we consider rectified linear unit (ReLU) sampling, where only positive entries are observed, as well as a generalization to threshold-based sampling. We first empirically demonstrate that the landscape of this MC problem is not globally benign: Gradient descent (GD) with random initialization will generally converge to stationary points that are not globally optimal. Nevertheless, we prove that when the matrix factor with a small rank satisfies mild assumptions, the nonconvex objective function is geodesically strongly convex on the quotient manifold in a neighborhood of a planted low-rank matrix. Moreover, we show that our assumptions are satisfied by a matrix factor with i.i.d. Gaussian entries. Finally, we develop a tailor-designed initialization for GD to solve our studied formulation, which empirically always achieves convergence to the global minima. We also conduct extensive experiments and compare MC methods, investigating convergence and completion performance with respect to initialization, noise level, dimension, and rank.","sentences":["We study the problem of symmetric positive semi-definite low-rank matrix completion (MC) with deterministic entry-dependent sampling.","In particular, we consider rectified linear unit (ReLU) sampling, where only positive entries are observed, as well as a generalization to threshold-based sampling.","We first empirically demonstrate that the landscape of this MC problem is not globally benign: Gradient descent (GD) with random initialization will generally converge to stationary points that are not globally optimal.","Nevertheless, we prove that when the matrix factor with a small rank satisfies mild assumptions, the nonconvex objective function is geodesically strongly convex on the quotient manifold in a neighborhood of a planted low-rank matrix.","Moreover, we show that our assumptions are satisfied by a matrix factor with i.i.d. Gaussian entries.","Finally, we develop a tailor-designed initialization for GD to solve our studied formulation, which empirically always achieves convergence to the global minima.","We also conduct extensive experiments and compare MC methods, investigating convergence and completion performance with respect to initialization, noise level, dimension, and rank."],"url":"http://arxiv.org/abs/2406.05822v1","category":"cs.LG"}
{"created":"2024-06-09 14:20:46","title":"ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations","abstract":"The need for abundant labelled data in supervised Adversarial Training (AT) has prompted the use of Self-Supervised Learning (SSL) techniques with AT. However, the direct application of existing SSL methods to adversarial training has been sub-optimal due to the increased training complexity of combining SSL with AT. A recent approach, DeACL, mitigates this by utilizing supervision from a standard SSL teacher in a distillation setting, to mimic supervised AT. However, we find that there is still a large performance gap when compared to supervised adversarial training, specifically on larger models. In this work, investigate the key reason for this gap and propose Projected Feature Adversarial Training (ProFeAT) to bridge the same. We show that the sub-optimal distillation performance is a result of mismatch in training objectives of the teacher and student, and propose to use a projection head at the student, that allows it to leverage weak supervision from the teacher while also being able to learn adversarially robust representations that are distinct from the teacher. We further propose appropriate attack and defense losses at the feature and projector, alongside a combination of weak and strong augmentations for the teacher and student respectively, to improve the training data diversity without increasing the training complexity. Through extensive experiments on several benchmark datasets and models, we demonstrate significant improvements in both clean and robust accuracy when compared to existing SSL-AT methods, setting a new state-of-the-art. We further report on-par/ improved performance when compared to TRADES, a popular supervised-AT method.","sentences":["The need for abundant labelled data in supervised Adversarial Training (AT) has prompted the use of Self-Supervised Learning (SSL) techniques with AT.","However, the direct application of existing SSL methods to adversarial training has been sub-optimal due to the increased training complexity of combining SSL with AT.","A recent approach, DeACL, mitigates this by utilizing supervision from a standard SSL teacher in a distillation setting, to mimic supervised AT.","However, we find that there is still a large performance gap when compared to supervised adversarial training, specifically on larger models.","In this work, investigate the key reason for this gap and propose Projected Feature Adversarial Training (ProFeAT) to bridge the same.","We show that the sub-optimal distillation performance is a result of mismatch in training objectives of the teacher and student, and propose to use a projection head at the student, that allows it to leverage weak supervision from the teacher while also being able to learn adversarially robust representations that are distinct from the teacher.","We further propose appropriate attack and defense losses at the feature and projector, alongside a combination of weak and strong augmentations for the teacher and student respectively, to improve the training data diversity without increasing the training complexity.","Through extensive experiments on several benchmark datasets and models, we demonstrate significant improvements in both clean and robust accuracy when compared to existing SSL-AT methods, setting a new state-of-the-art.","We further report on-par/ improved performance when compared to TRADES, a popular supervised-AT method."],"url":"http://arxiv.org/abs/2406.05796v1","category":"cs.LG"}
{"created":"2024-06-09 14:07:35","title":"OD-DETR: Online Distillation for Stabilizing Training of Detection Transformer","abstract":"DEtection TRansformer (DETR) becomes a dominant paradigm, mainly due to its common architecture with high accuracy and no post-processing. However, DETR suffers from unstable training dynamics. It consumes more data and epochs to converge compared with CNN-based detectors. This paper aims to stabilize DETR training through the online distillation. It utilizes a teacher model, accumulated by Exponential Moving Average (EMA), and distills its knowledge into the online model in following three aspects. First, the matching relation between object queries and ground truth (GT) boxes in the teacher is employed to guide the student, so queries within the student are not only assigned labels based on their own predictions, but also refer to the matching results from the teacher. Second, the teacher's initial query is given to the online student, and its prediction is directly constrained by the corresponding output from the teacher. Finally, the object queries from teacher's different decoding stages are used to build the auxiliary groups to accelerate the convergence. For each GT, two queries with the least matching costs are selected into this extra group, and they predict the GT box and participate the optimization. Extensive experiments show that the proposed OD-DETR successfully stabilizes the training, and significantly increases the performance without bringing in more parameters.","sentences":["DEtection TRansformer (DETR) becomes a dominant paradigm, mainly due to its common architecture with high accuracy and no post-processing.","However, DETR suffers from unstable training dynamics.","It consumes more data and epochs to converge compared with CNN-based detectors.","This paper aims to stabilize DETR training through the online distillation.","It utilizes a teacher model, accumulated by Exponential Moving Average (EMA), and distills its knowledge into the online model in following three aspects.","First, the matching relation between object queries and ground truth (GT) boxes in the teacher is employed to guide the student, so queries within the student are not only assigned labels based on their own predictions, but also refer to the matching results from the teacher.","Second, the teacher's initial query is given to the online student, and its prediction is directly constrained by the corresponding output from the teacher.","Finally, the object queries from teacher's different decoding stages are used to build the auxiliary groups to accelerate the convergence.","For each GT, two queries with the least matching costs are selected into this extra group, and they predict the GT box and participate the optimization.","Extensive experiments show that the proposed OD-DETR successfully stabilizes the training, and significantly increases the performance without bringing in more parameters."],"url":"http://arxiv.org/abs/2406.05791v1","category":"cs.CV"}
{"created":"2024-06-09 13:54:53","title":"Optical signal recording of cellular activity in optogenetic stimulation of human pulp dental cells using a twin-core fiber-based Mach-Zehnder interferometer biosensor","abstract":"Frazao This paper introduces an innovative two-core fiber (TCF) optic sensor employing a Mach-Zehnder interferometer (MZI) to monitor the optogenetic response of light-sensitive human dental pulp stem cells (hDPSCs). The in-fiber MZI, formed using a segment of TCF optic, detects refractive index (RI) changes in the surrounding medium. The sensor utilizes the evanescent wave of one core as the sensing arm, necessitating a thin cladding achieved through one-sided chemical etching. This design allows the sensor to detect subtle alterations in the RI of the environment by observing displacements in the interference spectrum. The optogenetic stimulation of light-sensitive cells induces variations in ion concentrations, leading to a corresponding change in refractive index. The fabricated sensor, with a peak sensitivity of 675.74 nm/RIU within the RI range of 1.39-1.43, can detect these changes. A computer simulation validated the sensitivity and optimized fabrication parameters, exhibiting satisfactory agreement with experimental results. Spectrum displacements were recorded for both light-sensitive hDPSCs and regular hDPSCs (as a control test). Results from the experiment, analyzed and compared using data analysis software, revealed that 473 nm blue light effectively stimulated light-sensitive hDPSCs. Notably, the proposed sensor, a novel structure, demonstrated its capability to detect RI changes in the cell medium during optogenetic applications.","sentences":["Frazao This paper introduces an innovative two-core fiber (TCF) optic sensor employing a Mach-Zehnder interferometer (MZI) to monitor the optogenetic response of light-sensitive human dental pulp stem cells (hDPSCs).","The in-fiber MZI, formed using a segment of TCF optic, detects refractive index (RI) changes in the surrounding medium.","The sensor utilizes the evanescent wave of one core as the sensing arm, necessitating a thin cladding achieved through one-sided chemical etching.","This design allows the sensor to detect subtle alterations in the RI of the environment by observing displacements in the interference spectrum.","The optogenetic stimulation of light-sensitive cells induces variations in ion concentrations, leading to a corresponding change in refractive index.","The fabricated sensor, with a peak sensitivity of 675.74 nm/RIU within the RI range of 1.39-1.43, can detect these changes.","A computer simulation validated the sensitivity and optimized fabrication parameters, exhibiting satisfactory agreement with experimental results.","Spectrum displacements were recorded for both light-sensitive hDPSCs and regular hDPSCs (as a control test).","Results from the experiment, analyzed and compared using data analysis software, revealed that 473 nm blue light effectively stimulated light-sensitive hDPSCs.","Notably, the proposed sensor, a novel structure, demonstrated its capability to detect RI changes in the cell medium during optogenetic applications."],"url":"http://arxiv.org/abs/2406.05787v1","category":"physics.optics"}
{"created":"2024-06-09 13:16:00","title":"An efficient branch-and-cut approach for large-scale competitive facility location problems with limited choice rule","abstract":"In the paper, we consider the competitive facility location problem with limited choice rule (CFLPLCR), which attempts to open a subset of facilities to maximize the net profit of a newcomer company, requiring customers to patronize only a limited number of opening facilities and an outside option. We propose an efficient branch-and-cut (B&C) approach for the CFLPLCR based on newly proposed mixed integer linear programming (MILP) formulations. Specifically, by establishing the submodularity of the probability function, we develop an MILP formulation for the CFLPLCR using the submodular inequalities. For the special case where each customer patronizes at most one open facility and the outside option, we show that the submodular inequalities can characterize the convex hull of the considered set and provide a compact MILP formulation. Moreover, for the general case, we strengthen the submodular inequalities by sequential lifting, resulting in a class of facet-defining inequalities. The proposed lifted submodular inequalities are shown to be stronger than the classic submodular inequalities, enabling to obtain another MILP formulation with a tighter linear programming (LP) relaxation. By extensive numerical experiments, we show that the proposed B&C approach outperforms the state-of-the-art generalized Benders decomposition approach by at least one order of magnitude. Furthermore, it enables to solve CFLPLCR instances with 10000 customers and 2000 facilities.","sentences":["In the paper, we consider the competitive facility location problem with limited choice rule (CFLPLCR), which attempts to open a subset of facilities to maximize the net profit of a newcomer company, requiring customers to patronize only a limited number of opening facilities and an outside option.","We propose an efficient branch-and-cut (B&C) approach for the CFLPLCR based on newly proposed mixed integer linear programming (MILP) formulations.","Specifically, by establishing the submodularity of the probability function, we develop an MILP formulation for the CFLPLCR using the submodular inequalities.","For the special case where each customer patronizes at most one open facility and the outside option, we show that the submodular inequalities can characterize the convex hull of the considered set and provide a compact MILP formulation.","Moreover, for the general case, we strengthen the submodular inequalities by sequential lifting, resulting in a class of facet-defining inequalities.","The proposed lifted submodular inequalities are shown to be stronger than the classic submodular inequalities, enabling to obtain another MILP formulation with a tighter linear programming (LP) relaxation.","By extensive numerical experiments, we show that the proposed B&C approach outperforms the state-of-the-art generalized Benders decomposition approach by at least one order of magnitude.","Furthermore, it enables to solve CFLPLCR instances with 10000 customers and 2000 facilities."],"url":"http://arxiv.org/abs/2406.05775v1","category":"math.OC"}
{"created":"2024-06-09 12:59:03","title":"Integrative Modeling and Engineering of ZnO Grain Boundaries: A Pathway to Enhanced Thermoelectric Performance","abstract":"Achieving higher performance in zinc oxide (ZnO) at room and mid-temperatures is crucial for applications requiring high energy conversion efficiency. This study focuses on grain boundary engineering techniques and an innovative modeling approach to optimize the thermoelectric properties of ZnO materials. An analysis was conducted to investigate the influence of grain boundaries on ZnO and evaluate its performance at mid-temperatures using the quality factor B as a descriptor.The findings indicate that the thermoelectric performance of ZnO could be significantly enhanced at low and mid-range temperatures if polycrystalline samples with engineered grain boundaries achieve low lattice thermal conductivity. An integrative model was developed to analytically examine the impact of barrier height , mean free path of carriers, and effective mass of carriers on charge mobility. Simulation results show that reducing from 0.5 eV to 0.1 eV resulted in a 45% increase in electron mobility, facilitating the overcoming of energy barriers that impede charge transport. Increasing the mean free path from 5 nm to 25 nm significantly increased charge carrier mobility, indicating fewer scattering events and higher velocities over longer distances.The study also shows that as the effective mass increases from 0.8 to 1.0, electron mobility peaks at 1.0 , suggesting an optimal balance where electron mobility is maximized. This implies an intricate interplay between carrier mass and lattice interaction forces within the material. Drawing on these insights, strategies aimed at improving the figure of merit were proposed, including microstructural engineering, advanced doping techniques, and passivation of grain boundaries. These findings provide a systematic approach to grain boundary engineering, potentially transforming ZnO into efficient materials for thermoelectric applications.","sentences":["Achieving higher performance in zinc oxide (ZnO) at room and mid-temperatures is crucial for applications requiring high energy conversion efficiency.","This study focuses on grain boundary engineering techniques and an innovative modeling approach to optimize the thermoelectric properties of ZnO materials.","An analysis was conducted to investigate the influence of grain boundaries on ZnO and evaluate its performance at mid-temperatures using the quality factor B as a descriptor.","The findings indicate that the thermoelectric performance of ZnO could be significantly enhanced at low and mid-range temperatures if polycrystalline samples with engineered grain boundaries achieve low lattice thermal conductivity.","An integrative model was developed to analytically examine the impact of barrier height , mean free path of carriers, and effective mass of carriers on charge mobility.","Simulation results show that reducing from 0.5 eV to 0.1 eV resulted in a 45% increase in electron mobility, facilitating the overcoming of energy barriers that impede charge transport.","Increasing the mean free path from 5 nm to 25 nm significantly increased charge carrier mobility, indicating fewer scattering events and higher velocities over longer distances.","The study also shows that as the effective mass increases from 0.8 to 1.0, electron mobility peaks at 1.0 , suggesting an optimal balance where electron mobility is maximized.","This implies an intricate interplay between carrier mass and lattice interaction forces within the material.","Drawing on these insights, strategies aimed at improving the figure of merit were proposed, including microstructural engineering, advanced doping techniques, and passivation of grain boundaries.","These findings provide a systematic approach to grain boundary engineering, potentially transforming ZnO into efficient materials for thermoelectric applications."],"url":"http://arxiv.org/abs/2406.05769v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-09 12:46:44","title":"Integrating Quantum Algorithms with Gravitational-Wave Metrology for Enhanced Signal Detection","abstract":"This study explores the integration of quantum algorithms, specifically Grover's algorithm, with quantum metrology to enhance the efficiency and sensitivity of gravitational-wave detection. By combining quantum matched filtering with precise parameter estimation techniques, the research aims to optimize sensor networks for the identification of gravitational waves. This integrated approach leverages the strengths of quantum superposition and entanglement to improve signal detection, reduce noise, and strategically place sensors. The findings demonstrate significant improvements in the sensitivity and accuracy of gravitational wave measurements, highlighting the potential of quantum technologies to revolutionize observational astronomy and enhance our understanding of the universe.","sentences":["This study explores the integration of quantum algorithms, specifically Grover's algorithm, with quantum metrology to enhance the efficiency and sensitivity of gravitational-wave detection.","By combining quantum matched filtering with precise parameter estimation techniques, the research aims to optimize sensor networks for the identification of gravitational waves.","This integrated approach leverages the strengths of quantum superposition and entanglement to improve signal detection, reduce noise, and strategically place sensors.","The findings demonstrate significant improvements in the sensitivity and accuracy of gravitational wave measurements, highlighting the potential of quantum technologies to revolutionize observational astronomy and enhance our understanding of the universe."],"url":"http://arxiv.org/abs/2406.05767v1","category":"gr-qc"}
{"created":"2024-06-09 11:17:15","title":"A Little Aggression Goes a Long Way","abstract":"Aggression is a two-player game of troop placement and attack played on a map (modeled as a graph). Players take turns deploying troops on a territory (a vertex on the graph) until they run out. Once all troops are placed, players take turns attacking enemy territories. A territory can be attacked if it has $k$ troops and there are more than $k$ enemy troops on adjacent territories. At the end of the game, the player who controls the most territories wins. In the case of a tie, the player with more surviving troops wins. The first player to exhaust their troops in the placement phase leads the attack phase.   We study the complexity of the game when the graph along with an assignment of troops and the sequence of attacks planned by the second player. Even in this restrained setting, we show that the problem of determining an optimal sequence of first player moves is NP-complete. We then analyze the game for when the input graph is a matching or a cycle.","sentences":["Aggression is a two-player game of troop placement and attack played on a map (modeled as a graph).","Players take turns deploying troops on a territory (a vertex on the graph) until they run out.","Once all troops are placed, players take turns attacking enemy territories.","A territory can be attacked if it has $k$ troops and there are more than $k$ enemy troops on adjacent territories.","At the end of the game, the player who controls the most territories wins.","In the case of a tie, the player with more surviving troops wins.","The first player to exhaust their troops in the placement phase leads the attack phase.   ","We study the complexity of the game when the graph along with an assignment of troops and the sequence of attacks planned by the second player.","Even in this restrained setting, we show that the problem of determining an optimal sequence of first player moves is NP-complete.","We then analyze the game for when the input graph is a matching or a cycle."],"url":"http://arxiv.org/abs/2406.05742v1","category":"cs.GT"}
{"created":"2024-06-09 11:13:25","title":"Convergence of ZH-type nonmonotone descent method for Kurdyka-\u0141ojasiewicz optimization problems","abstract":"This note concerns a class of nonmonotone descent methods for minimizing a proper lower semicontinuous Kurdyka-{\\L}$\\ddot{o}$jasiewicz (KL) function $\\Phi$, whose iterate sequence obeys the ZH-type nonmonotone decrease condition and a relative error condition. We prove that the iterate sequence converges to a critical point of $\\Phi$, and if $\\Phi$ has the KL property of exponent $\\theta\\in(0,1)$ at this critical point, the convergence has a linear rate for $\\theta\\in(0,1/2]$ and a sublinear rate of exponent $\\frac{1-\\theta}{1-2\\theta}$ for $\\theta\\in(1/2,1)$. Our results first resolve the full convergence of the iterate sequence generated by the ZH-type nonmonotone descent method for nonconvex and nonsmooth optimization problems, and extend the full convergence of monotone descent methods for KL optimization problems to the ZH-type nonmonotone descent method.","sentences":["This note concerns a class of nonmonotone descent methods for minimizing a proper lower semicontinuous Kurdyka-{\\L}$\\ddot{o}$jasiewicz (KL) function $\\Phi$, whose iterate sequence obeys the ZH-type nonmonotone decrease condition and a relative error condition.","We prove that the iterate sequence converges to a critical point of $\\Phi$, and if $\\Phi$ has the KL property of exponent $\\theta\\in(0,1)$ at this critical point, the convergence has a linear rate for $\\theta\\in(0,1/2]$ and a sublinear rate of exponent $\\frac{1-\\theta}{1-2\\theta}$ for $\\theta\\in(1/2,1)$. Our results first resolve the full convergence of the iterate sequence generated by the ZH-type nonmonotone descent method for nonconvex and nonsmooth optimization problems, and extend the full convergence of monotone descent methods for KL optimization problems to the ZH-type nonmonotone descent method."],"url":"http://arxiv.org/abs/2406.05740v1","category":"math.OC"}
{"created":"2024-06-09 10:36:06","title":"Region of Interest Loss for Anonymizing Learned Image Compression","abstract":"The use of AI in public spaces continually raises concerns about privacy and the protection of sensitive data. An example is the deployment of detection and recognition methods on humans, where images are provided by surveillance cameras. This results in the acquisition of great amounts of sensitive data, since the capture and transmission of images taken by such cameras happens unaltered, for them to be received by a server on the network. However, many applications do not explicitly require the identity of a given person in a scene; An anonymized representation containing information of the person's position while preserving the context of them in the scene suffices. We show how using a customized loss function on region of interests (ROI) can achieve sufficient anonymization such that human faces become unrecognizable while persons are kept detectable, by training an end-to-end optimized autoencoder for learned image compression that utilizes the flexibility of the learned analysis and reconstruction transforms for the task of mutating parts of the compression result. This approach enables compression and anonymization in one step on the capture device, instead of transmitting sensitive, nonanonymized data over the network. Additionally, we evaluate how this anonymization impacts the average precision of pre-trained foundation models on detecting faces (MTCNN) and humans (YOLOv8) in comparison to non-ANN based methods, while considering compression rate and latency.","sentences":["The use of AI in public spaces continually raises concerns about privacy and the protection of sensitive data.","An example is the deployment of detection and recognition methods on humans, where images are provided by surveillance cameras.","This results in the acquisition of great amounts of sensitive data, since the capture and transmission of images taken by such cameras happens unaltered, for them to be received by a server on the network.","However, many applications do not explicitly require the identity of a given person in a scene; An anonymized representation containing information of the person's position while preserving the context of them in the scene suffices.","We show how using a customized loss function on region of interests (ROI) can achieve sufficient anonymization such that human faces become unrecognizable while persons are kept detectable, by training an end-to-end optimized autoencoder for learned image compression that utilizes the flexibility of the learned analysis and reconstruction transforms for the task of mutating parts of the compression result.","This approach enables compression and anonymization in one step on the capture device, instead of transmitting sensitive, nonanonymized data over the network.","Additionally, we evaluate how this anonymization impacts the average precision of pre-trained foundation models on detecting faces (MTCNN) and humans (YOLOv8) in comparison to non-ANN based methods, while considering compression rate and latency."],"url":"http://arxiv.org/abs/2406.05726v1","category":"cs.CV"}
{"created":"2024-06-09 10:30:25","title":"Binarized Diffusion Model for Image Super-Resolution","abstract":"Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment. Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs. Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation. In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization. We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection. Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA). The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation alability of the binarized module. Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods. Code is available at https://github.com/zhengchen1999/BI-DiffSR.","sentences":["Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment.","Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs.","Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation.","In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR.","First, for the model structure, we design a UNet architecture optimized for binarization.","We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer.","Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection.","Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA).","The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation alability of the binarized module.","Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods.","Code is available at https://github.com/zhengchen1999/BI-DiffSR."],"url":"http://arxiv.org/abs/2406.05723v1","category":"cs.CV"}
{"created":"2024-06-09 10:12:08","title":"Contextual Continuum Bandits: Static Versus Dynamic Regret","abstract":"We study the contextual continuum bandits problem, where the learner sequentially receives a side information vector and has to choose an action in a convex set, minimizing a function associated to the context. The goal is to minimize all the underlying functions for the received contexts, leading to a dynamic (contextual) notion of regret, which is stronger than the standard static regret. Assuming that the objective functions are H\\\"older with respect to the contexts, we demonstrate that any algorithm achieving a sub-linear static regret can be extended to achieve a sub-linear dynamic regret. We further study the case of strongly convex and smooth functions when the observations are noisy. Inspired by the interior point method and employing self-concordant barriers, we propose an algorithm achieving a sub-linear dynamic regret. Lastly, we present a minimax lower bound, implying two key facts. First, no algorithm can achieve sub-linear dynamic regret over functions that are not continuous with respect to the context. Second, for strongly convex and smooth functions, the algorithm that we propose achieves, up to a logarithmic factor, the minimax optimal rate of dynamic regret as a function of the number of queries.","sentences":["We study the contextual continuum bandits problem, where the learner sequentially receives a side information vector and has to choose an action in a convex set, minimizing a function associated to the context.","The goal is to minimize all the underlying functions for the received contexts, leading to a dynamic (contextual) notion of regret, which is stronger than the standard static regret.","Assuming that the objective functions are H\\\"older with respect to the contexts, we demonstrate that any algorithm achieving a sub-linear static regret can be extended to achieve a sub-linear dynamic regret.","We further study the case of strongly convex and smooth functions when the observations are noisy.","Inspired by the interior point method and employing self-concordant barriers, we propose an algorithm achieving a sub-linear dynamic regret.","Lastly, we present a minimax lower bound, implying two key facts.","First, no algorithm can achieve sub-linear dynamic regret over functions that are not continuous with respect to the context.","Second, for strongly convex and smooth functions, the algorithm that we propose achieves, up to a logarithmic factor, the minimax optimal rate of dynamic regret as a function of the number of queries."],"url":"http://arxiv.org/abs/2406.05714v1","category":"stat.ML"}
{"created":"2024-06-09 09:15:54","title":"Hierarchical Features Matter: A Deep Exploration of GAN Priors for Improved Dataset Distillation","abstract":"Dataset distillation is an emerging dataset reduction method, which condenses large-scale datasets while maintaining task accuracy. Current methods have integrated parameterization techniques to boost synthetic dataset performance by shifting the optimization space from pixel to another informative feature domain. However, they limit themselves to a fixed optimization space for distillation, neglecting the diverse guidance across different informative latent spaces. To overcome this limitation, we propose a novel parameterization method dubbed Hierarchical Generative Latent Distillation (H-GLaD), to systematically explore hierarchical layers within the generative adversarial networks (GANs). This allows us to progressively span from the initial latent space to the final pixel space. In addition, we introduce a novel class-relevant feature distance metric to alleviate the computational burden associated with synthetic dataset evaluation, bridging the gap between synthetic and original datasets. Experimental results demonstrate that the proposed H-GLaD achieves a significant improvement in both same-architecture and cross-architecture performance with equivalent time consumption.","sentences":["Dataset distillation is an emerging dataset reduction method, which condenses large-scale datasets while maintaining task accuracy.","Current methods have integrated parameterization techniques to boost synthetic dataset performance by shifting the optimization space from pixel to another informative feature domain.","However, they limit themselves to a fixed optimization space for distillation, neglecting the diverse guidance across different informative latent spaces.","To overcome this limitation, we propose a novel parameterization method dubbed Hierarchical Generative Latent Distillation (H-GLaD), to systematically explore hierarchical layers within the generative adversarial networks (GANs).","This allows us to progressively span from the initial latent space to the final pixel space.","In addition, we introduce a novel class-relevant feature distance metric to alleviate the computational burden associated with synthetic dataset evaluation, bridging the gap between synthetic and original datasets.","Experimental results demonstrate that the proposed H-GLaD achieves a significant improvement in both same-architecture and cross-architecture performance with equivalent time consumption."],"url":"http://arxiv.org/abs/2406.05704v1","category":"cs.CV"}
{"created":"2024-06-09 08:57:22","title":"The dual of Philo's shortest line segment problem","abstract":"We study the dual of Philo's shortest line segment problem which asks to find the optimal line segments passing through two given points, with a common endpoint, and with the other endpoints on a given line. The provided solution uses multivariable calculus and geometry methods. Interesting connections with the angle bisector of the triangle are explored.","sentences":["We study the dual of Philo's shortest line segment problem which asks to find the optimal line segments passing through two given points, with a common endpoint, and with the other endpoints on a given line.","The provided solution uses multivariable calculus and geometry methods.","Interesting connections with the angle bisector of the triangle are explored."],"url":"http://arxiv.org/abs/2406.05702v1","category":"cs.CG"}
{"created":"2024-06-09 08:17:13","title":"FlightBench: A Comprehensive Benchmark of Spatial Planning Methods for Quadrotors","abstract":"Spatial planning in cluttered environments is crucial for mobile systems, particularly agile quadrotors. Existing methods, both optimization-based and learning-based, often focus only on success rates in specific environments and lack a unified platform with tasks of varying difficulty. To address this, we introduce FlightBench, the first comprehensive open-source benchmark for 3D spatial planning on quadrotors, comparing classical optimization-based methods with emerging learning-based approaches. We also develop a suite of task difficulty metrics and evaluation metrics to quantify the characteristics of tasks and the performance of planning algorithms. Extensive experiments demonstrate the significant advantages of learning-based methods for high-speed flight and real-time planning, while highlighting the need for improvements in complex conditions, such as navigating large corners or dealing with view occlusion. We also conduct analytical experiments to justify the effectiveness of our proposed metrics. Additionally, we show that latency randomization effectively enhances performance in real-world deployments. The source code is available at \\url{https://github.com/thu-uav/FlightBench}.","sentences":["Spatial planning in cluttered environments is crucial for mobile systems, particularly agile quadrotors.","Existing methods, both optimization-based and learning-based, often focus only on success rates in specific environments and lack a unified platform with tasks of varying difficulty.","To address this, we introduce FlightBench, the first comprehensive open-source benchmark for 3D spatial planning on quadrotors, comparing classical optimization-based methods with emerging learning-based approaches.","We also develop a suite of task difficulty metrics and evaluation metrics to quantify the characteristics of tasks and the performance of planning algorithms.","Extensive experiments demonstrate the significant advantages of learning-based methods for high-speed flight and real-time planning, while highlighting the need for improvements in complex conditions, such as navigating large corners or dealing with view occlusion.","We also conduct analytical experiments to justify the effectiveness of our proposed metrics.","Additionally, we show that latency randomization effectively enhances performance in real-world deployments.","The source code is available at \\url{https://github.com/thu-uav/FlightBench}."],"url":"http://arxiv.org/abs/2406.05687v1","category":"cs.RO"}
{"created":"2024-06-09 08:11:12","title":"Provable Optimization for Adversarial Fair Self-supervised Contrastive Learning","abstract":"This paper studies learning fair encoders in a self-supervised learning (SSL) setting, in which all data are unlabeled and only a small portion of them are annotated with sensitive attribute.   Adversarial fair representation learning is well suited for this scenario by minimizing a contrastive loss over unlabeled data while maximizing an adversarial loss of predicting the sensitive attribute over the data with sensitive attribute. Nevertheless, optimizing adversarial fair representation learning presents significant challenges due to solving a non-convex non-concave minimax game. The complexity deepens when incorporating a global contrastive loss that contrasts each anchor data point against all other examples. A central question is ``{\\it can we design a provable yet efficient algorithm for solving adversarial fair self-supervised contrastive learning}?'' Building on advanced optimization techniques, we propose a stochastic algorithm dubbed SoFCLR with a convergence analysis under reasonable conditions without requring a large batch size. We conduct extensive experiments to demonstrate the effectiveness of the proposed approach for downstream classification with eight fairness notions.","sentences":["This paper studies learning fair encoders in a self-supervised learning (SSL) setting, in which all data are unlabeled and only a small portion of them are annotated with sensitive attribute.   ","Adversarial fair representation learning is well suited for this scenario by minimizing a contrastive loss over unlabeled data while maximizing an adversarial loss of predicting the sensitive attribute over the data with sensitive attribute.","Nevertheless, optimizing adversarial fair representation learning presents significant challenges due to solving a non-convex non-concave minimax game.","The complexity deepens when incorporating a global contrastive loss that contrasts each anchor data point against all other examples.","A central question is ``{\\it can we design a provable yet efficient algorithm for solving adversarial fair self-supervised contrastive learning}?''","Building on advanced optimization techniques, we propose a stochastic algorithm dubbed SoFCLR with a convergence analysis under reasonable conditions without requring a large batch size.","We conduct extensive experiments to demonstrate the effectiveness of the proposed approach for downstream classification with eight fairness notions."],"url":"http://arxiv.org/abs/2406.05686v1","category":"cs.LG"}
{"created":"2024-06-09 06:51:38","title":"Achieving High Capacity Transmission With N-Dimensional Quasi-Fractal UCA","abstract":"The vortex electromagnetic wave carried by multiple orthogonal orbital angular momentum (OAM) modes in the same frequency band can be applied to the field of wireless communications, which greatly increases the spectrum efficiency. The uniform circular array (UCA) is widely used to generate and receive vortex electromagnetic waves with multiple OAM-modes. However, the maximum number of orthogonal OAM-modes based on UCA is usually limited to the number of array-elements of the UCA antenna, leaving how to utilize more OAM-modes to achieve higher channel capacity with a fixed number of arrayelements as an intriguing question. In this paper, we propose an N-dimensional quasi-fractal UCA (ND QF-UCA) antenna structure in different fractal geometry layouts to break through the limits of array-elements number on OAM-modes number. We develop the N-dimensional OAM modulation (NOM) and demodulation (NOD) schemes for OAM multiplexing transmission with the OAM-modes number exceeding the array-elements number, which is beyond the traditional concept of multiple antenna based wireless communications. Then, we investigate different dimensional multiplexing transmission schemes based on the corresponding QF-UCA antenna structure with various array-element layouts and evaluate the optimal layout type and dimension to obtain the highest channel capacity with a fixed number of array-elements. Simulation results show that our proposed schemes can obtain a higher spectrum efficiency, surpassing those of alternative array-element layouts of QF-UCA and the traditional multiple antenna systems.","sentences":["The vortex electromagnetic wave carried by multiple orthogonal orbital angular momentum (OAM) modes in the same frequency band can be applied to the field of wireless communications, which greatly increases the spectrum efficiency.","The uniform circular array (UCA) is widely used to generate and receive vortex electromagnetic waves with multiple OAM-modes.","However, the maximum number of orthogonal OAM-modes based on UCA is usually limited to the number of array-elements of the UCA antenna, leaving how to utilize more OAM-modes to achieve higher channel capacity with a fixed number of arrayelements as an intriguing question.","In this paper, we propose an N-dimensional quasi-fractal UCA (ND QF-UCA) antenna structure in different fractal geometry layouts to break through the limits of array-elements number on OAM-modes number.","We develop the N-dimensional OAM modulation (NOM) and demodulation (NOD) schemes for OAM multiplexing transmission with the OAM-modes number exceeding the array-elements number, which is beyond the traditional concept of multiple antenna based wireless communications.","Then, we investigate different dimensional multiplexing transmission schemes based on the corresponding QF-UCA antenna structure with various array-element layouts and evaluate the optimal layout type and dimension to obtain the highest channel capacity with a fixed number of array-elements.","Simulation results show that our proposed schemes can obtain a higher spectrum efficiency, surpassing those of alternative array-element layouts of QF-UCA and the traditional multiple antenna systems."],"url":"http://arxiv.org/abs/2406.05667v1","category":"eess.SP"}
{"created":"2024-06-09 05:50:06","title":"Single channel PICOSEC Micromegas detector with improved time resolution","abstract":"This paper presents design guidelines and experimental verification of a single-channel PICOSEC Micromegas (MM) detector with an improved time resolution. The design encompasses the detector board, vessel, auxiliary mechanical parts, and electrical connectivity for high voltage (HV) and signals, focusing on improving stability, reducing noise, and ensuring signal integrity to optimize timing performance. A notable feature is the simple and fast reassembly procedure, facilitating quick replacement of detector internal components that allows for an efficient measurement strategy involving different detector components. The paper also examines the influence of parasitics on the output signal integrity. To validate the design, a prototype assembly and three interchangeable detector boards with varying readout pad diameters were manufactured. The detectors were initially tested in the laboratory environment. Finally, the timing performance of detectors with different pad sizes was verified using a Minimum Ionizing Particle (MIP) beam test. Notably, a record time resolution for a PICOSEC Micromegas detector technology with a CsI photocathode of 12.5$\\pm$0.8 ps was achieved with a 10 mm diameter readout pad size detector.","sentences":["This paper presents design guidelines and experimental verification of a single-channel PICOSEC Micromegas (MM) detector with an improved time resolution.","The design encompasses the detector board, vessel, auxiliary mechanical parts, and electrical connectivity for high voltage (HV) and signals, focusing on improving stability, reducing noise, and ensuring signal integrity to optimize timing performance.","A notable feature is the simple and fast reassembly procedure, facilitating quick replacement of detector internal components that allows for an efficient measurement strategy involving different detector components.","The paper also examines the influence of parasitics on the output signal integrity.","To validate the design, a prototype assembly and three interchangeable detector boards with varying readout pad diameters were manufactured.","The detectors were initially tested in the laboratory environment.","Finally, the timing performance of detectors with different pad sizes was verified using a Minimum Ionizing Particle (MIP) beam test.","Notably, a record time resolution for a PICOSEC Micromegas detector technology with a CsI photocathode of 12.5$\\pm$0.8 ps was achieved with a 10 mm diameter readout pad size detector."],"url":"http://arxiv.org/abs/2406.05657v1","category":"physics.ins-det"}
{"created":"2024-06-09 05:14:34","title":"Distributionally robust stochastic optimal control","abstract":"The main goal of this paper is to discuss the construction of distributionally robust counterparts of stochastic optimal control problems. Randomized and non-randomized policies are considered. In particular, necessary and sufficient conditions for the existence of non-randomized policies are given.","sentences":["The main goal of this paper is to discuss the construction of distributionally robust counterparts of stochastic optimal control problems.","Randomized and non-randomized policies are considered.","In particular, necessary and sufficient conditions for the existence of non-randomized policies are given."],"url":"http://arxiv.org/abs/2406.05648v1","category":"math.OC"}
{"created":"2024-06-09 04:51:51","title":"PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction","abstract":"Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is challenging as it typically struggles to make an appropriate trade-off between its training data distribution and the target distribution, i.e., learning a novel concept with only a few target images to achieve personalization (aligning with the personalized target) while preserving text editability (aligning with diverse text prompts). In this paper, we propose PaRa, an effective and efficient Parameter Rank Reduction approach for T2I model personalization by explicitly controlling the rank of the diffusion model parameters to restrict its initial diverse generation space into a small and well-balanced target space. Our design is motivated by the fact that taming a T2I model toward a novel concept such as a specific art style implies a small generation space. To this end, by reducing the rank of model parameters during finetuning, we can effectively constrain the space of the denoising sampling trajectories towards the target. With comprehensive experiments, we show that PaRa achieves great advantages over existing finetuning approaches on single/multi-subject generation as well as single-image editing. Notably, compared to the prevailing fine-tuning technique LoRA, PaRa achieves better parameter efficiency (2x fewer learnable parameters) and much better target image alignment.","sentences":["Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is challenging as it typically struggles to make an appropriate trade-off between its training data distribution and the target distribution, i.e., learning a novel concept with only a few target images to achieve personalization (aligning with the personalized target) while preserving text editability (aligning with diverse text prompts).","In this paper, we propose PaRa, an effective and efficient Parameter Rank Reduction approach for T2I model personalization by explicitly controlling the rank of the diffusion model parameters to restrict its initial diverse generation space into a small and well-balanced target space.","Our design is motivated by the fact that taming a T2I model toward a novel concept such as a specific art style implies a small generation space.","To this end, by reducing the rank of model parameters during finetuning, we can effectively constrain the space of the denoising sampling trajectories towards the target.","With comprehensive experiments, we show that PaRa achieves great advantages over existing finetuning approaches on single/multi-subject generation as well as single-image editing.","Notably, compared to the prevailing fine-tuning technique LoRA, PaRa achieves better parameter efficiency (2x fewer learnable parameters) and much better target image alignment."],"url":"http://arxiv.org/abs/2406.05641v1","category":"cs.CV"}
{"created":"2024-06-09 04:42:19","title":"A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Automated Program Repair","abstract":"Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.   To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-INSTRUCTION, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$ improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.   This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are publicly available as open-source resources.","sentences":["Automated Program Repair (APR) aims to fix bugs by generating patches.","And existing work has demonstrated that \"pre-training and fine-tuning\" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR.","However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR.","Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.   ","To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-INSTRUCTION, at first.","Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-INSTRUCTION.","The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques.","The results also show that $(IA)^3$ improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods.","Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT.","Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.   ","This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks.","APR-INSTRUCTION, PEFT weights, and the fine-tuning code are publicly available as open-source resources."],"url":"http://arxiv.org/abs/2406.05639v1","category":"cs.SE"}
{"created":"2024-06-09 04:35:51","title":"Exponential Conic Relaxations for Signomial Geometric Programming","abstract":"Signomial geometric programming (SGP) is a computationally challenging, NP-Hard class of nonconvex nonlinear optimization problems. SGP can be solved iteratively using a sequence of convex relaxations; consequently, the strength of such relaxations is an important factor to this iterative approach. Motivated by recent advances in solving exponential conic programming (ECP) problems, this paper develops a novel convex relaxation for SGP. Unlike existing work on relaxations, the base model in this paper does not assume bounded variables. However, bounded variables or monomial terms can be used to strengthen the relaxation by means of additional valid linear inequalities. We show how to embed the ECP relaxation in an iterative algorithm for SGP; leveraging recent advances in interior point method solvers, our computational experiments demonstrate the practical effectiveness of this approach.","sentences":["Signomial geometric programming (SGP) is a computationally challenging, NP-Hard class of nonconvex nonlinear optimization problems.","SGP can be solved iteratively using a sequence of convex relaxations; consequently, the strength of such relaxations is an important factor to this iterative approach.","Motivated by recent advances in solving exponential conic programming (ECP) problems, this paper develops a novel convex relaxation for SGP.","Unlike existing work on relaxations, the base model in this paper does not assume bounded variables.","However, bounded variables or monomial terms can be used to strengthen the relaxation by means of additional valid linear inequalities.","We show how to embed the ECP relaxation in an iterative algorithm for SGP; leveraging recent advances in interior point method solvers, our computational experiments demonstrate the practical effectiveness of this approach."],"url":"http://arxiv.org/abs/2406.05638v1","category":"math.OC"}
{"created":"2024-06-09 03:32:32","title":"Domain Generalization Guided by Large-Scale Pre-Trained Priors","abstract":"Domain generalization (DG) aims to train a model from limited source domains, allowing it to generalize to unknown target domains. Typically, DG models only employ large-scale pre-trained models during the initialization of fine-tuning. However, large-scale pre-trained models already possess the ability to resist domain shift. If we reference pre-trained models continuously during fine-tuning to maintain this ability, it could further enhance the generalization ability of the DG model. For this purpose, we introduce a new method called Fine-Tune with Large-scale pre-trained Priors (FT-LP), which incorporates the pre-trained model as a prior into the DG fine-tuning process, ensuring that the model refers to its pre-trained model at each optimization step. FT-LP comprises a theoretical framework and a simple implementation strategy. In theory, we verify the rationality of FT-LP by introducing a generalization error bound with the pre-trained priors for DG. In implementation, we utilize an encoder to simulate the model distribution, enabling the use of FT-LP when only pre-trained weights are available. In summary, we offer a new fine-tuning method for DG algorithms to utilize pre-trained models throughout the fine-tuning process. Through experiments on various datasets and DG models, our proposed method exhibits significant improvements, indicating its effectiveness.","sentences":["Domain generalization (DG) aims to train a model from limited source domains, allowing it to generalize to unknown target domains.","Typically, DG models only employ large-scale pre-trained models during the initialization of fine-tuning.","However, large-scale pre-trained models already possess the ability to resist domain shift.","If we reference pre-trained models continuously during fine-tuning to maintain this ability, it could further enhance the generalization ability of the DG model.","For this purpose, we introduce a new method called Fine-Tune with Large-scale pre-trained Priors (FT-LP), which incorporates the pre-trained model as a prior into the DG fine-tuning process, ensuring that the model refers to its pre-trained model at each optimization step.","FT-LP comprises a theoretical framework and a simple implementation strategy.","In theory, we verify the rationality of FT-LP by introducing a generalization error bound with the pre-trained priors for DG.","In implementation, we utilize an encoder to simulate the model distribution, enabling the use of FT-LP when only pre-trained weights are available.","In summary, we offer a new fine-tuning method for DG algorithms to utilize pre-trained models throughout the fine-tuning process.","Through experiments on various datasets and DG models, our proposed method exhibits significant improvements, indicating its effectiveness."],"url":"http://arxiv.org/abs/2406.05628v1","category":"cs.LG"}
{"created":"2024-06-09 03:06:55","title":"Beat: Bi-directional One-to-Many Embedding Alignment for Text-based Person Retrieval","abstract":"Text-based person retrieval (TPR) is a challenging task that involves retrieving a specific individual based on a textual description. Despite considerable efforts to bridge the gap between vision and language, the significant differences between these modalities continue to pose a challenge. Previous methods have attempted to align text and image samples in a modal-shared space, but they face uncertainties in optimization directions due to the movable features of both modalities and the failure to account for one-to-many relationships of image-text pairs in TPR datasets. To address this issue, we propose an effective bi-directional one-to-many embedding paradigm that offers a clear optimization direction for each sample, thus mitigating the optimization problem. Additionally, this embedding scheme generates multiple features for each sample without introducing trainable parameters, making it easier to align with several positive samples. Based on this paradigm, we propose a novel Bi-directional one-to-many Embedding Alignment (Beat) model to address the TPR task. Our experimental results demonstrate that the proposed Beat model achieves state-of-the-art performance on three popular TPR datasets, including CUHK-PEDES (65.61 R@1), ICFG-PEDES (58.25 R@1), and RSTPReID (48.10 R@1). Furthermore, additional experiments on MS-COCO, CUB, and Flowers datasets further demonstrate the potential of Beat to be applied to other image-text retrieval tasks.","sentences":["Text-based person retrieval (TPR) is a challenging task that involves retrieving a specific individual based on a textual description.","Despite considerable efforts to bridge the gap between vision and language, the significant differences between these modalities continue to pose a challenge.","Previous methods have attempted to align text and image samples in a modal-shared space, but they face uncertainties in optimization directions due to the movable features of both modalities and the failure to account for one-to-many relationships of image-text pairs in TPR datasets.","To address this issue, we propose an effective bi-directional one-to-many embedding paradigm that offers a clear optimization direction for each sample, thus mitigating the optimization problem.","Additionally, this embedding scheme generates multiple features for each sample without introducing trainable parameters, making it easier to align with several positive samples.","Based on this paradigm, we propose a novel Bi-directional one-to-many Embedding Alignment (Beat) model to address the TPR task.","Our experimental results demonstrate that the proposed Beat model achieves state-of-the-art performance on three popular TPR datasets, including CUHK-PEDES (65.61 R@1), ICFG-PEDES (58.25 R@1), and RSTPReID (48.10 R@1).","Furthermore, additional experiments on MS-COCO, CUB, and Flowers datasets further demonstrate the potential of Beat to be applied to other image-text retrieval tasks."],"url":"http://arxiv.org/abs/2406.05620v1","category":"cs.CV"}
{"created":"2024-06-09 01:45:36","title":"Janus graphene nanoribbons with a single ferromagnetic zigzag edge","abstract":"Topological design of pi-electrons in zigzag-edged graphene nanoribbons (ZGNRs) leads to a wealth of magnetic quantum phenomena and exotic quantum phases. Symmetric ZGNRs typically exhibit antiferromagnetically coupled spin-ordered edge states. Eliminating cross-edge magnetic coupling in ZGNRs not only enables the realization of a new class of ferromagnetic quantum spin chains, enabling the exploration of quantum spin physics and entanglement of multiple qubits in the 1D limit, but also establishes a long-sought carbon-based ferromagnetic transport channel, pivotal for ultimate scaling of GNR-based quantum electronics. However, designing such GNRs entails overcoming daunting challenges, including simultaneous breaking of structural and spin symmetries, and designing elegant precursors for asymmetric fabrication of reactive zigzag edges. Here, we report a general approach for designing and fabricating such ferromagnetic GNRs in the form of Janus GNRs with two distinct edge configurations. Guided by Lieb's theorem and topological classification theory, we devised two JGNRs by asymmetrically introduced a topological defect array of benzene motifs to one zigzag edge, while keeping the opposing zigzag edge unchanged. This breaks structural symmetry and creates a sublattice imbalance within each unit cell, initiating a spin symmetry breaking. Three Z-shape precursors are designed to fabricate one parent ZGNR and two JGNRs with an optimal lattice spacing of the defect array for a complete quench of the magnetic edge states at the defective edge. Characterization via scanning probe microscopy/spectroscopy and first-principles density functional theory confirms the successful fabrication of Janus GNRs with ferromagnetic ground state delocalised along the pristine zigzag edge.","sentences":["Topological design of pi-electrons in zigzag-edged graphene nanoribbons (ZGNRs) leads to a wealth of magnetic quantum phenomena and exotic quantum phases.","Symmetric ZGNRs typically exhibit antiferromagnetically coupled spin-ordered edge states.","Eliminating cross-edge magnetic coupling in ZGNRs not only enables the realization of a new class of ferromagnetic quantum spin chains, enabling the exploration of quantum spin physics and entanglement of multiple qubits in the 1D limit, but also establishes a long-sought carbon-based ferromagnetic transport channel, pivotal for ultimate scaling of GNR-based quantum electronics.","However, designing such GNRs entails overcoming daunting challenges, including simultaneous breaking of structural and spin symmetries, and designing elegant precursors for asymmetric fabrication of reactive zigzag edges.","Here, we report a general approach for designing and fabricating such ferromagnetic GNRs in the form of Janus GNRs with two distinct edge configurations.","Guided by Lieb's theorem and topological classification theory, we devised two JGNRs by asymmetrically introduced a topological defect array of benzene motifs to one zigzag edge, while keeping the opposing zigzag edge unchanged.","This breaks structural symmetry and creates a sublattice imbalance within each unit cell, initiating a spin symmetry breaking.","Three Z-shape precursors are designed to fabricate one parent ZGNR and two JGNRs with an optimal lattice spacing of the defect array for a complete quench of the magnetic edge states at the defective edge.","Characterization via scanning probe microscopy/spectroscopy and first-principles density functional theory confirms the successful fabrication of Janus GNRs with ferromagnetic ground state delocalised along the pristine zigzag edge."],"url":"http://arxiv.org/abs/2406.05608v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-08 23:26:38","title":"Optimal control of linear Gaussian quantum systems via quantum learning control","abstract":"Efficiently controlling linear Gaussian quantum (LGQ) systems is a significant task in both the study of fundamental quantum theory and the development of modern quantum technology. Here, we propose a general quantum-learning-control method for optimally controlling LGQ systems based on the gradient-descent algorithm. Our approach flexibly designs the loss function for diverse tasks by utilizing first- and second-order moments that completely describe the quantum state of LGQ systems. We demonstrate both deep optomechanical cooling and large optomechanical entanglement using this approach. Our approach enables the fast and deep ground-state cooling of a mechanical resonator within a short time, surpassing the limitations of sideband cooling in the continuous-wave driven strong-coupling regime. Furthermore, optomechanical entanglement could be generated remarkably fast and surpass several times the corresponding steady-state entanglement, even when the thermal phonon occupation reaches one hundred. This work will not only broaden the application of quantum learning control, but also open an avenue for optimal control of LGQ systems.","sentences":["Efficiently controlling linear Gaussian quantum (LGQ) systems is a significant task in both the study of fundamental quantum theory and the development of modern quantum technology.","Here, we propose a general quantum-learning-control method for optimally controlling LGQ systems based on the gradient-descent algorithm.","Our approach flexibly designs the loss function for diverse tasks by utilizing first- and second-order moments that completely describe the quantum state of LGQ systems.","We demonstrate both deep optomechanical cooling and large optomechanical entanglement using this approach.","Our approach enables the fast and deep ground-state cooling of a mechanical resonator within a short time, surpassing the limitations of sideband cooling in the continuous-wave driven strong-coupling regime.","Furthermore, optomechanical entanglement could be generated remarkably fast and surpass several times the corresponding steady-state entanglement, even when the thermal phonon occupation reaches one hundred.","This work will not only broaden the application of quantum learning control, but also open an avenue for optimal control of LGQ systems."],"url":"http://arxiv.org/abs/2406.05597v1","category":"quant-ph"}
{"created":"2024-06-08 21:36:08","title":"Optimizing Gate Decomposition for High-Level Quantum Programming","abstract":"This paper presents novel methods for optimizing multi-controlled quantum gates, which naturally arise in high-level quantum programming. Our primary approach involves rewriting $U(2)$ gates as $SU(2)$ gates, utilizing one auxiliary qubit for phase correction. This reduces the number of CNOT gates required to decompose any multi-controlled quantum gate from $O(n^2)$ to at most $32n$. Additionally, we can reduce the number of CNOTs for multi-controlled Pauli gates from $16n$ to $12n$ and propose an optimization to reduce the number of controlled gates in high-level quantum programming. We have implemented these optimizations in the Ket quantum programming platform and demonstrated significant reductions in the number of gates. For instance, for a Grover's algorithm layer with 114 qubits, we achieved a reduction in the number of CNOTs from 101,245 to 2,684. This reduction in the number of gates significantly impacts the execution time of quantum algorithms, thereby enhancing the feasibility of executing them on NISQ computers.","sentences":["This paper presents novel methods for optimizing multi-controlled quantum gates, which naturally arise in high-level quantum programming.","Our primary approach involves rewriting $U(2)$ gates as $SU(2)$ gates, utilizing one auxiliary qubit for phase correction.","This reduces the number of CNOT gates required to decompose any multi-controlled quantum gate from $O(n^2)$ to at most $32n$. Additionally, we can reduce the number of CNOTs for multi-controlled Pauli gates from $16n$ to $12n$ and propose an optimization to reduce the number of controlled gates in high-level quantum programming.","We have implemented these optimizations in the Ket quantum programming platform and demonstrated significant reductions in the number of gates.","For instance, for a Grover's algorithm layer with 114 qubits, we achieved a reduction in the number of CNOTs from 101,245 to 2,684.","This reduction in the number of gates significantly impacts the execution time of quantum algorithms, thereby enhancing the feasibility of executing them on NISQ computers."],"url":"http://arxiv.org/abs/2406.05581v1","category":"quant-ph"}
{"created":"2024-06-08 21:30:54","title":"Omnidirectional Energetic Electron Fluxes from 150 km to 20,000 km: an ELFIN-Based Model","abstract":"The strong variations of energetic electron fluxes in the Earth's inner magnetosphere are notoriously hard to forecast. Developing accurate empirical models of electron fluxes from low to high altitudes at all latitudes is therefore useful to improve our understanding of flux variations and to assess radiation hazards for spacecraft systems. In the present work, energy- and pitch-angle-resolved precipitating, trapped, and backscattered electron fluxes measured at low altitude by Electron Loss and Fields Investigation (ELFIN) CubeSats are used to infer omnidirectional fluxes at altitudes below and above the spacecraft, from 150 km to 20,000 km, making use of adiabatic transport theory and quasi-linear diffusion theory. The inferred fluxes are fitted as a function of selected parameters using a stepwise multivariate optimization procedure, providing an analytical model of omnidirectional electron flux along each geomagnetic field line, based on measurements from only one spacecraft in low Earth orbit. The modeled electron fluxes are provided as a function of L-shell, altitude, energy, and two different indices of past geomagnetic activity, computed over the preceding 4 hours or 3 days, potentially allowing to disentangle impulsive processes (such as rapid injections) from cumulative processes (such as inward radial diffusion and wave-driven energization). The model is validated through comparisons with equatorial measurements from the Van Allen Probes, demonstrating the broad applicability of the present method. The model indicates that both impulsive and time-integrated geomagnetic activity partly control electron fluxes in the outer radiation belt and in the plasma sheet.","sentences":["The strong variations of energetic electron fluxes in the Earth's inner magnetosphere are notoriously hard to forecast.","Developing accurate empirical models of electron fluxes from low to high altitudes at all latitudes is therefore useful to improve our understanding of flux variations and to assess radiation hazards for spacecraft systems.","In the present work, energy- and pitch-angle-resolved precipitating, trapped, and backscattered electron fluxes measured at low altitude by","Electron Loss and Fields Investigation (ELFIN) CubeSats are used to infer omnidirectional fluxes at altitudes below and above the spacecraft, from 150 km to 20,000 km, making use of adiabatic transport theory and quasi-linear diffusion theory.","The inferred fluxes are fitted as a function of selected parameters using a stepwise multivariate optimization procedure, providing an analytical model of omnidirectional electron flux along each geomagnetic field line, based on measurements from only one spacecraft in low Earth orbit.","The modeled electron fluxes are provided as a function of L-shell, altitude, energy, and two different indices of past geomagnetic activity, computed over the preceding 4 hours or 3 days, potentially allowing to disentangle impulsive processes (such as rapid injections) from cumulative processes (such as inward radial diffusion and wave-driven energization).","The model is validated through comparisons with equatorial measurements from the Van Allen Probes, demonstrating the broad applicability of the present method.","The model indicates that both impulsive and time-integrated geomagnetic activity partly control electron fluxes in the outer radiation belt and in the plasma sheet."],"url":"http://arxiv.org/abs/2406.05579v1","category":"physics.space-ph"}
{"created":"2024-06-08 21:15:22","title":"Uplink resource allocation optimization for user-centric cell-free MIMO networks","abstract":"We examine the problem of optimizing resource allocation in the uplink for a user-centric, cell-free, multi-input multi-output network. We start by modeling and developing resource allocation algorithms for two standard network operation modes. The centralized mode provides high data rates but suffers multiple issues, including scalability. On the other hand, the distributed mode has the opposite problem: relatively low rates, but is scalable. To address these challenges, we combine the strength of the two standard modes, creating a new semi-distributed operation mode. To avoid the need for information exchange between access points, we introduce a new quality of service metric to decentralize the resource allocation algorithms. Our results show that we can eliminate the need for information exchange with a relatively small penalty on data rates.","sentences":["We examine the problem of optimizing resource allocation in the uplink for a user-centric, cell-free, multi-input multi-output network.","We start by modeling and developing resource allocation algorithms for two standard network operation modes.","The centralized mode provides high data rates but suffers multiple issues, including scalability.","On the other hand, the distributed mode has the opposite problem: relatively low rates, but is scalable.","To address these challenges, we combine the strength of the two standard modes, creating a new semi-distributed operation mode.","To avoid the need for information exchange between access points, we introduce a new quality of service metric to decentralize the resource allocation algorithms.","Our results show that we can eliminate the need for information exchange with a relatively small penalty on data rates."],"url":"http://arxiv.org/abs/2406.05576v1","category":"cs.IT"}
{"created":"2024-06-08 21:15:14","title":"A Survey on Hybrid Motion Planning Methods for Automated Driving Systems","abstract":"Motion planning is an essential element of the modular architecture of autonomous vehicles, serving as a bridge between upstream perception modules and downstream low-level control signals. Traditional motion planners were initially designed for specific Automated Driving Functions (ADFs), yet the evolving landscape of highly automated driving systems (ADS) requires motion for a wide range of ADFs, including unforeseen ones. This need has motivated the development of the ``hybrid\" approach in the literature, seeking to enhance motion planning performance by combining diverse techniques, such as data-driven (learning-based) and logic-driven (analytic) methodologies. Recent research endeavours have significantly contributed to the development of more efficient, accurate, and safe hybrid methods for Tactical Decision Making (TDM) and Trajectory Generation (TG), as well as integrating these algorithms into the motion planning module. Owing to the extensive variety and potential of hybrid methods, a timely and comprehensive review of the current literature is undertaken in this survey article. We classify the hybrid motion planners based on the types of components they incorporate, such as combinations of sampling-based with optimization-based/learning-based motion planners. The comparison of different classes is conducted by evaluating the addressed challenges and limitations, as well as assessing whether they focus on TG and/or TDM. We hope this approach will enable the researchers in this field to gain in-depth insights into the identification of current trends in hybrid motion planning and shed light on promising areas for future research.","sentences":["Motion planning is an essential element of the modular architecture of autonomous vehicles, serving as a bridge between upstream perception modules and downstream low-level control signals.","Traditional motion planners were initially designed for specific Automated Driving Functions (ADFs), yet the evolving landscape of highly automated driving systems (ADS) requires motion for a wide range of ADFs, including unforeseen ones.","This need has motivated the development of the ``hybrid\" approach in the literature, seeking to enhance motion planning performance by combining diverse techniques, such as data-driven (learning-based) and logic-driven (analytic) methodologies.","Recent research endeavours have significantly contributed to the development of more efficient, accurate, and safe hybrid methods for Tactical Decision Making (TDM) and Trajectory Generation (TG), as well as integrating these algorithms into the motion planning module.","Owing to the extensive variety and potential of hybrid methods, a timely and comprehensive review of the current literature is undertaken in this survey article.","We classify the hybrid motion planners based on the types of components they incorporate, such as combinations of sampling-based with optimization-based/learning-based motion planners.","The comparison of different classes is conducted by evaluating the addressed challenges and limitations, as well as assessing whether they focus on TG and/or TDM.","We hope this approach will enable the researchers in this field to gain in-depth insights into the identification of current trends in hybrid motion planning and shed light on promising areas for future research."],"url":"http://arxiv.org/abs/2406.05575v1","category":"cs.RO"}
{"created":"2024-06-08 19:29:01","title":"A Shape Change Enhancing Hierarchical Layout for the Pairwise Comparison of Directed Acyclic Graphs","abstract":"Comparing directed acyclic graphs is essential in various fields such as healthcare, social media, finance, biology, and marketing. DAGs often result from contagion processes over networks, including information spreading, retweet activity, disease transmission, financial crisis propagation, malware spread, and gene mutations. For instance, in disease spreading, an infected patient can transmit the disease to contacts, making it crucial to analyze and predict scenarios. Similarly, in finance, understanding the effects of saving or not saving specific banks during a crisis is vital. Experts often need to identify small differences between DAGs, such as changes in a few nodes or edges. Even the presence or absence of a single edge can be significant. Visualization plays a crucial role in facilitating these comparisons. However, standard hierarchical layout algorithms struggle to visualize subtle changes effectively. The typical hierarchical layout, with the root on top, is preferred due to its performance in comparison to other layouts. Nevertheless, these standard algorithms prioritize single-graph aesthetics over comparison suitability, making it challenging for users to spot changes. To address this issue, we propose a layout that enhances shape changes in DAGs while minimizing the impact on aesthetics. Our approach involves outwardly swapping changes, altering the DAG's shape. We introduce new drawing criteria. Our layout builds upon a Sugiyama-like hierarchical layout and implements these criteria through two extensions. We designed it this way to maintain interchangeability and accommodate future optimizations, such as pseudo-nodes for edge crossing minimization. In our evaluations, our layout achieves excellent results, with edge crossing aesthetics averaging around 0.8 (on a scale of 0 to 1). Additionally, our layout outperforms the base implementation by an average of 60-75\\%.","sentences":["Comparing directed acyclic graphs is essential in various fields such as healthcare, social media, finance, biology, and marketing.","DAGs often result from contagion processes over networks, including information spreading, retweet activity, disease transmission, financial crisis propagation, malware spread, and gene mutations.","For instance, in disease spreading, an infected patient can transmit the disease to contacts, making it crucial to analyze and predict scenarios.","Similarly, in finance, understanding the effects of saving or not saving specific banks during a crisis is vital.","Experts often need to identify small differences between DAGs, such as changes in a few nodes or edges.","Even the presence or absence of a single edge can be significant.","Visualization plays a crucial role in facilitating these comparisons.","However, standard hierarchical layout algorithms struggle to visualize subtle changes effectively.","The typical hierarchical layout, with the root on top, is preferred due to its performance in comparison to other layouts.","Nevertheless, these standard algorithms prioritize single-graph aesthetics over comparison suitability, making it challenging for users to spot changes.","To address this issue, we propose a layout that enhances shape changes in DAGs while minimizing the impact on aesthetics.","Our approach involves outwardly swapping changes, altering the DAG's shape.","We introduce new drawing criteria.","Our layout builds upon a Sugiyama-like hierarchical layout and implements these criteria through two extensions.","We designed it this way to maintain interchangeability and accommodate future optimizations, such as pseudo-nodes for edge crossing minimization.","In our evaluations, our layout achieves excellent results, with edge crossing aesthetics averaging around 0.8 (on a scale of 0 to 1).","Additionally, our layout outperforms the base implementation by an average of 60-75\\%."],"url":"http://arxiv.org/abs/2406.05560v1","category":"cs.HC"}
{"created":"2024-06-08 18:14:47","title":"The Development of the Reproductive Healthcare Equity Algorithm (RHEA)","abstract":"After the repeal of Roe vs. Wade in June 2022, women face long-distance travel across state lines to access abortion care. For women who also face socioeconomic hardship, travel for abortion care is a significant burden. To ease this burden, abortion access nonprofits are funding and/or supplying transportation to abortion clinics. However, due to the uneven distribution of demand and supply for abortions, these nonprofits do not have efficient logistical operations. As a result, low-income, underserved women may not have access to adequate reproductive healthcare, thus widening healthcare inequity gaps. Nonprofits may also risk not serving the needs of vulnerable women without access to adequate reproductive healthcare, and in doing so, waste resources, money, and volunteer hours. To address these challenges, we create an interactive, web-based planning tool, the Reproductive Healthcare Equity Algorithm (RHEA), to guide nonprofits in strategically allocating resources and serving demand. RHEA leverages an optimization model to determine the maximum flow and minimum transportation cost to route women across a network of counties and abortion clinics, subject to transportation supply, budget, and time constraints for one day of operations for a nonprofit. In doing so, we collaborate with abortion access nonprofits to cater our model design and interface development to their needs and considerations. Ultimately, we seek to optimize resource allocation for nonprofits providing abortion care logistics and improve abortion access for low-income, underserved women.","sentences":["After the repeal of Roe vs. Wade in June 2022, women face long-distance travel across state lines to access abortion care.","For women who also face socioeconomic hardship, travel for abortion care is a significant burden.","To ease this burden, abortion access nonprofits are funding and/or supplying transportation to abortion clinics.","However, due to the uneven distribution of demand and supply for abortions, these nonprofits do not have efficient logistical operations.","As a result, low-income, underserved women may not have access to adequate reproductive healthcare, thus widening healthcare inequity gaps.","Nonprofits may also risk not serving the needs of vulnerable women without access to adequate reproductive healthcare, and in doing so, waste resources, money, and volunteer hours.","To address these challenges, we create an interactive, web-based planning tool, the Reproductive Healthcare Equity Algorithm (RHEA), to guide nonprofits in strategically allocating resources and serving demand.","RHEA leverages an optimization model to determine the maximum flow and minimum transportation cost to route women across a network of counties and abortion clinics, subject to transportation supply, budget, and time constraints for one day of operations for a nonprofit.","In doing so, we collaborate with abortion access nonprofits to cater our model design and interface development to their needs and considerations.","Ultimately, we seek to optimize resource allocation for nonprofits providing abortion care logistics and improve abortion access for low-income, underserved women."],"url":"http://arxiv.org/abs/2406.05542v1","category":"eess.SY"}
{"created":"2024-06-08 17:35:14","title":"Output-Optimal Algorithms for Join-Aggregate Queries","abstract":"The classic Yannakakis framework proposed in 1981 is still the state-of-the-art approach for tackling acyclic join-aggregate queries defined over commutative semi-rings. It has been shown that the time complexity of the Yannakakis framework is $O(N + \\OUT)$ for any free-connex join-aggregate query, where $N$ is the input size of database and $\\OUT$ is the output size of the query result. This is already output-optimal. However, only a general upper bound $O(N \\cdot \\OUT)$ on the time complexity of the Yannakakis framework is known for the remaining class of acyclic but non-free-connex queries.   We first show a lower bound $\\Omega\\left(N \\cdot \\OUT^{1- \\frac{1}{\\outw}} + \\OUT\\right)$ for computing an acyclic join-aggregate query by {\\em semi-ring algorithms}, where $\\outw$ is identified as the {\\em out-width} of the input query, $N$ is the input size of the database, and $\\OUT$ is the output size of the query result. For example, $\\outw =2$ for the chain matrix multiplication query, and $\\outw=k$ for the star matrix multiplication query with $k$ relations. We give a tighter analysis of the Yannakakis framework and show that Yannakakis framework is already output-optimal on the class of {\\em aggregate-hierarchical} queries. However, for the large remaining class of non-aggregate-hierarchical queries, such as chain matrix multiplication query, Yannakakis framework indeed requires $\\Theta(N \\cdot \\OUT)$ time. We next explore a hybrid version of the Yannakakis framework and present an output-optimal algorithm for computing any general acyclic join-aggregate query within $\\O\\left(N\\cdot \\OUT^{1-\\frac{1}{\\outw}} + \\OUT\\right)$ time, matching the out-width-dependent lower bound up to a poly-logarithmic factor. To the best of our knowledge, this is the first polynomial improvement for computing acyclic join-aggregate queries since 1981.","sentences":["The classic Yannakakis framework proposed in 1981 is still the state-of-the-art approach for tackling acyclic join-aggregate queries defined over commutative semi-rings.","It has been shown that the time complexity of the Yannakakis framework is $O(N + \\OUT)$ for any free-connex join-aggregate query, where $N$ is the input size of database and $\\OUT$ is the output size of the query result.","This is already output-optimal.","However, only a general upper bound $O(N \\cdot \\OUT)$ on the time complexity of the Yannakakis framework is known for the remaining class of acyclic but non-free-connex queries.   ","We first show a lower bound $\\Omega\\left(N \\cdot \\OUT^{1- \\frac{1}{\\outw}} + \\OUT\\right)$ for computing an acyclic join-aggregate query by {\\em semi-ring algorithms}, where $\\outw$ is identified as the {\\em out-width} of the input query, $N$ is the input size of the database, and $\\OUT$ is the output size of the query result.","For example, $\\outw =2$ for the chain matrix multiplication query, and $\\outw=k$ for the star matrix multiplication query with $k$ relations.","We give a tighter analysis of the Yannakakis framework and show that Yannakakis framework is already output-optimal on the class of {\\em aggregate-hierarchical} queries.","However, for the large remaining class of non-aggregate-hierarchical queries, such as chain matrix multiplication query, Yannakakis framework indeed requires $\\Theta(N \\cdot \\OUT)$ time.","We next explore a hybrid version of the Yannakakis framework and present an output-optimal algorithm for computing any general acyclic join-aggregate query within $\\O\\left(N\\cdot \\OUT^{1-\\frac{1}{\\outw}} + \\OUT\\right)$ time, matching the out-width-dependent lower bound up to a poly-logarithmic factor.","To the best of our knowledge, this is the first polynomial improvement for computing acyclic join-aggregate queries since 1981."],"url":"http://arxiv.org/abs/2406.05536v1","category":"cs.DB"}
{"created":"2024-06-08 17:16:05","title":"Optimal Storage Design: An $L^{\\infty}$ infused Inventory Control","abstract":"Inventory control typically considers controlling the price and the production rate. However, such systems have rigidity towards altering the physical storage capacity -- one can not easily alter the physical size after the initial design. The paper focuses on this critical aspect, consideration of which leads to a non-standard control problem. Here, the objective is a weighted combination of the classical integral term (formed by usual inventory costs) and an $L^{\\infty}$ term (the maximum inventory level in the entire planning horizon). Our approach is to consider an additional state component to capture the `instantaneous' $L^{\\infty}$ term (maximum inventory level till that instant) by virtue of which, we could convert the problem to the classical framework. For the direct ($L^{\\infty}$) problem, we first identify a relation between the optimal price and the production rate policy, thereby reducing the dimensionality of the problem. By numerically solving a smooth variant of the converted problem, we obtain an optimal policy that illustrates a significant reduction in the storage capacity requirement. Interestingly, the loss in the revenue is negligible (less than $6\\%$). As the importance of the $L^{\\infty}$ component increases, the variations in the corresponding optimal inventory-level trajectory reduce. In the scenarios with partial/zero information about future demand curves, the above observation provides a guidance -- one should continually tune the policies to maintain instantaneous inventory-levels as close to zero as possible. With such a policy, the reduction in revenue is negligible, while having significant improvements for storage capacity. We theoretically establish certain interesting properties of the optimal policy, which also support the above guidance.","sentences":["Inventory control typically considers controlling the price and the production rate.","However, such systems have rigidity towards altering the physical storage capacity -- one can not easily alter the physical size after the initial design.","The paper focuses on this critical aspect, consideration of which leads to a non-standard control problem.","Here, the objective is a weighted combination of the classical integral term (formed by usual inventory costs) and an $L^{\\infty}$ term (the maximum inventory level in the entire planning horizon).","Our approach is to consider an additional state component to capture the `instantaneous' $L^{\\infty}$ term (maximum inventory level till that instant) by virtue of which, we could convert the problem to the classical framework.","For the direct ($L^{\\infty}$) problem, we first identify a relation between the optimal price and the production rate policy, thereby reducing the dimensionality of the problem.","By numerically solving a smooth variant of the converted problem, we obtain an optimal policy that illustrates a significant reduction in the storage capacity requirement.","Interestingly, the loss in the revenue is negligible (less than $6\\%$).","As the importance of the $L^{\\infty}$ component increases, the variations in the corresponding optimal inventory-level trajectory reduce.","In the scenarios with partial/zero information about future demand curves, the above observation provides a guidance -- one should continually tune the policies to maintain instantaneous inventory-levels as close to zero as possible.","With such a policy, the reduction in revenue is negligible, while having significant improvements for storage capacity.","We theoretically establish certain interesting properties of the optimal policy, which also support the above guidance."],"url":"http://arxiv.org/abs/2406.05526v1","category":"math.OC"}
{"created":"2024-06-08 16:20:41","title":"Optimal k-centers of a graph: a control-theoretic approach","abstract":"In a network consisting of n nodes, our goal is to identify the most central k nodes with respect to the proposed definitions of centrality. Depending on the specific application, there exist several metrics for quantifying k-centrality, and the subset of the best k nodes naturally varies based on the chosen metric. In this paper, we propose two metrics and establish connections to a well-studied metric from the literature (specifically for stochastic matrices). We prove these three notions match for path graphs. We then list a few more control-theoretic notions and compare these various notions for a general randomly generated graph. Our first metric involves maximizing the shift in the smallest eigenvalue of the Laplacian matrix. This shift can be interpreted as an improvement in the time constant when the RC circuit experiences leakage at certain k capacitors. The second metric focuses on minimizing the Perron root of a principal sub-matrix of a stochastic matrix, an idea proposed and interpreted in the literature as manufacturing consent. The third one explores minimizing the Perron root of a perturbed (now super-stochastic) matrix, which can be seen as minimizing the impact of added stubbornness. It is important to emphasize that we consider applications (for example, facility location) when the notions of central ports are such that the set of the best k ports does not necessarily contain the set of the best k-1 ports. We apply our k-port selection metric to various network structures. Notably, we prove the equivalence of three definitions for a path graph and extend the concept of central port linkage beyond Fiedler vectors to other eigenvectors associated with path graphs.","sentences":["In a network consisting of n nodes, our goal is to identify the most central k nodes with respect to the proposed definitions of centrality.","Depending on the specific application, there exist several metrics for quantifying k-centrality, and the subset of the best k nodes naturally varies based on the chosen metric.","In this paper, we propose two metrics and establish connections to a well-studied metric from the literature (specifically for stochastic matrices).","We prove these three notions match for path graphs.","We then list a few more control-theoretic notions and compare these various notions for a general randomly generated graph.","Our first metric involves maximizing the shift in the smallest eigenvalue of the Laplacian matrix.","This shift can be interpreted as an improvement in the time constant when the RC circuit experiences leakage at certain k capacitors.","The second metric focuses on minimizing the Perron root of a principal sub-matrix of a stochastic matrix, an idea proposed and interpreted in the literature as manufacturing consent.","The third one explores minimizing the Perron root of a perturbed (now super-stochastic) matrix, which can be seen as minimizing the impact of added stubbornness.","It is important to emphasize that we consider applications (for example, facility location) when the notions of central ports are such that the set of the best k ports does not necessarily contain the set of the best k-1 ports.","We apply our k-port selection metric to various network structures.","Notably, we prove the equivalence of three definitions for a path graph and extend the concept of central port linkage beyond Fiedler vectors to other eigenvectors associated with path graphs."],"url":"http://arxiv.org/abs/2406.05512v1","category":"math.CO"}
{"created":"2024-06-08 16:04:33","title":"G-Transformer: Counterfactual Outcome Prediction under Dynamic and Time-varying Treatment Regimes","abstract":"In the context of medical decision making, counterfactual prediction enables clinicians to predict treatment outcomes of interest under alternative courses of therapeutic actions given observed patient history. Prior machine learning approaches for counterfactual predictions under time-varying treatments focus on static time-varying treatment regimes where treatments do not depend on previous covariate history. In this work, we present G-Transformer, a Transformer-based framework supporting g-computation for counterfactual prediction under dynamic and time-varying treatment strategies. G-Transfomer captures complex, long-range dependencies in time-varying covariates using a Transformer architecture. G-Transformer estimates the conditional distribution of relevant covariates given covariate and treatment history at each time point using an encoder architecture, then produces Monte Carlo estimates of counterfactual outcomes by simulating forward patient trajectories under treatment strategies of interest. We evaluate G-Transformer extensively using two simulated longitudinal datasets from mechanistic models, and a real-world sepsis ICU dataset from MIMIC-IV. G-Transformer outperforms both classical and state-of-the-art counterfactual prediction models in these settings. To the best of our knowledge, this is the first Transformer-based architecture for counterfactual outcome prediction under dynamic and time-varying treatment strategies. Code will be released upon publication of the paper.","sentences":["In the context of medical decision making, counterfactual prediction enables clinicians to predict treatment outcomes of interest under alternative courses of therapeutic actions given observed patient history.","Prior machine learning approaches for counterfactual predictions under time-varying treatments focus on static time-varying treatment regimes where treatments do not depend on previous covariate history.","In this work, we present G-Transformer, a Transformer-based framework supporting g-computation for counterfactual prediction under dynamic and time-varying treatment strategies.","G-Transfomer captures complex, long-range dependencies in time-varying covariates using a Transformer architecture.","G-Transformer estimates the conditional distribution of relevant covariates given covariate and treatment history at each time point using an encoder architecture, then produces Monte Carlo estimates of counterfactual outcomes by simulating forward patient trajectories under treatment strategies of interest.","We evaluate G-Transformer extensively using two simulated longitudinal datasets from mechanistic models, and a real-world sepsis ICU dataset from MIMIC-IV.","G-Transformer outperforms both classical and state-of-the-art counterfactual prediction models in these settings.","To the best of our knowledge, this is the first Transformer-based architecture for counterfactual outcome prediction under dynamic and time-varying treatment strategies.","Code will be released upon publication of the paper."],"url":"http://arxiv.org/abs/2406.05504v1","category":"cs.LG"}
{"created":"2024-06-08 15:58:55","title":"Characterization of Recirculating Waveguide Meshes Based on an Optimization Method with a Parameter Space Reduction Technology","abstract":"Fabrication imperfections must be considered during configuration to ensure that the setup is suitable for the actual fabricated programmable photonic integrated circuits (PPICs). Therefore, characterization of imperfections is crucial but difficult, especially for PPICs made from recirculating waveguide meshes. The flexibility required by these meshes demands a more complex topology and compact TBU structure, complicating the characterization. In this paper, we propose a characterization method applicable to recirculating waveguide meshes based on an optimization approach, along with a step-by-step procedure to reduce the parameter space of optimization, allowing for characterizing imperfect parameters of each individual component within the waveguide mesh. To the best of our knowledge, this method can greatly broaden the range of characterized parameters compared to currently reported methods. In order to verify the effectiveness of our method, we used the characterized parameters to build a multi-frequency model of a mesh with fabrication errors and successfully demonstrated accurate prediction of its behavior. Furthermore, we applied our method on implementations of 6 different kind of FIR/IRR filters, to further prove the effectiveness of our method in configuring applications on meshes with fabrication errors. At last, our method was carried out under various scenarios considering beam splitter splitting ratio variance, inaccurate measurements of mesh and imprecise TBU insertion loss characterization, to demonstrate its strong robustness under various practical scenarios.","sentences":["Fabrication imperfections must be considered during configuration to ensure that the setup is suitable for the actual fabricated programmable photonic integrated circuits (PPICs).","Therefore, characterization of imperfections is crucial but difficult, especially for PPICs made from recirculating waveguide meshes.","The flexibility required by these meshes demands a more complex topology and compact TBU structure, complicating the characterization.","In this paper, we propose a characterization method applicable to recirculating waveguide meshes based on an optimization approach, along with a step-by-step procedure to reduce the parameter space of optimization, allowing for characterizing imperfect parameters of each individual component within the waveguide mesh.","To the best of our knowledge, this method can greatly broaden the range of characterized parameters compared to currently reported methods.","In order to verify the effectiveness of our method, we used the characterized parameters to build a multi-frequency model of a mesh with fabrication errors and successfully demonstrated accurate prediction of its behavior.","Furthermore, we applied our method on implementations of 6 different kind of FIR/IRR filters, to further prove the effectiveness of our method in configuring applications on meshes with fabrication errors.","At last, our method was carried out under various scenarios considering beam splitter splitting ratio variance, inaccurate measurements of mesh and imprecise TBU insertion loss characterization, to demonstrate its strong robustness under various practical scenarios."],"url":"http://arxiv.org/abs/2406.05502v1","category":"physics.optics"}
{"created":"2024-06-08 15:45:31","title":"SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner","abstract":"Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into four major categories: optimization-based attacks such as Greedy Coordinate Gradient (GCG), jailbreak template-based attacks such as \"Do-Anything-Now\", advanced indirect attacks like DrAttack, and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delay to user prompts, as well as be compatible with both open-source and closed-source LLMs.   Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM defense instance to concurrently protect the target LLM instance in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs (both target and defense LLMs) have the capability to identify harmful prompts or intentions in user queries, which we empirically validate using the commonly used GPT-3.5/4 models across all major jailbreak attacks. Our measurements show that SelfDefend enables GPT-3.5 to suppress the attack success rate (ASR) by 8.97-95.74% (average: 60%) and GPT-4 by even 36.36-100% (average: 83%), while incurring negligible effects on normal queries. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. These models outperform four SOTA defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. We also empirically show that the tuned models are robust to targeted GCG and prompt injection attacks.","sentences":["Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into four major categories: optimization-based attacks such as Greedy Coordinate Gradient (GCG), jailbreak template-based attacks such as \"Do-Anything-Now\", advanced indirect attacks like DrAttack, and multilingual jailbreaks.","However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delay to user prompts, as well as be compatible with both open-source and closed-source LLMs.   ","Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM defense instance to concurrently protect the target LLM instance in the normal stack and collaborate with it for checkpoint-based access control.","The effectiveness of SelfDefend builds upon our observation that existing LLMs (both target and defense LLMs) have the capability to identify harmful prompts or intentions in user queries, which we empirically validate using the commonly used GPT-3.5/4 models across all major jailbreak attacks.","Our measurements show that SelfDefend enables GPT-3.5 to suppress the attack success rate (ASR) by 8.97-95.74% (average: 60%) and GPT-4 by even 36.36-100% (average: 83%), while incurring negligible effects on normal queries.","To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models.","These models outperform four SOTA defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays.","We also empirically show that the tuned models are robust to targeted GCG and prompt injection attacks."],"url":"http://arxiv.org/abs/2406.05498v1","category":"cs.CR"}
{"created":"2024-06-08 13:52:20","title":"Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis","abstract":"The field of image synthesis is currently flourishing due to the advancements in diffusion models. While diffusion models have been successful, their computational intensity has prompted the pursuit of more efficient alternatives. As a representative work, non-autoregressive Transformers (NATs) have been recognized for their rapid generation. However, a major drawback of these models is their inferior performance compared to diffusion models. In this paper, we aim to re-evaluate the full potential of NATs by revisiting the design of their training and inference strategies. Specifically, we identify the complexities in properly configuring these strategies and indicate the possible sub-optimality in existing heuristic-driven designs. Recognizing this, we propose to go beyond existing methods by directly solving the optimal strategies in an automatic framework. The resulting method, named AutoNAT, advances the performance boundaries of NATs notably, and is able to perform comparably with the latest diffusion models at a significantly reduced inference cost. The effectiveness of AutoNAT is validated on four benchmark datasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M. Our code is available at https://github.com/LeapLabTHU/ImprovedNAT.","sentences":["The field of image synthesis is currently flourishing due to the advancements in diffusion models.","While diffusion models have been successful, their computational intensity has prompted the pursuit of more efficient alternatives.","As a representative work, non-autoregressive Transformers (NATs) have been recognized for their rapid generation.","However, a major drawback of these models is their inferior performance compared to diffusion models.","In this paper, we aim to re-evaluate the full potential of NATs by revisiting the design of their training and inference strategies.","Specifically, we identify the complexities in properly configuring these strategies and indicate the possible sub-optimality in existing heuristic-driven designs.","Recognizing this, we propose to go beyond existing methods by directly solving the optimal strategies in an automatic framework.","The resulting method, named AutoNAT, advances the performance boundaries of NATs notably, and is able to perform comparably with the latest diffusion models at a significantly reduced inference cost.","The effectiveness of AutoNAT is validated on four benchmark datasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M. Our code is available at https://github.com/LeapLabTHU/ImprovedNAT."],"url":"http://arxiv.org/abs/2406.05478v1","category":"cs.CV"}
{"created":"2024-06-08 13:48:50","title":"Deformation localisation in stretched liquid crystal elastomers","abstract":"We model within the framework of finite elasticity two inherent instabilities observed in liquid crystal elastomers under uniaxial loads. First is necking which occurs when a material sample suddenly elongates more in a small region where it appears narrower than the rest of the sample. Second is shear striping, which forms when the in-plane director rotates gradually to realign and become parallel with the applied force. These phenomena are due to the liquid crystal molecules rotating freely under mechanical loading. To capture necking, we assume that the uniaxial order parameter increases with tensile stretch, as reported experimentally during polydomain-monodomain transition. To account for shear striping, we maintain the uniaxial order parameter fixed, as suggested by experiments. Our finite element simulations capture well these phenomena. As necking in liquid crystal elastomers has not been satisfactorily modelled before, our theoretical and numerical findings related to this effect can be of wide interest. Shear striping has been well studied, yet our computed examples also show how optimal stripe width increases with the nematic penetration depth measuring the competition between the Frank elasticity of liquid crystals and polymer elasticity. Although known theoretically, this result has not been confirmed numerically by previous nonlinear elastic models.","sentences":["We model within the framework of finite elasticity two inherent instabilities observed in liquid crystal elastomers under uniaxial loads.","First is necking which occurs when a material sample suddenly elongates more in a small region where it appears narrower than the rest of the sample.","Second is shear striping, which forms when the in-plane director rotates gradually to realign and become parallel with the applied force.","These phenomena are due to the liquid crystal molecules rotating freely under mechanical loading.","To capture necking, we assume that the uniaxial order parameter increases with tensile stretch, as reported experimentally during polydomain-monodomain transition.","To account for shear striping, we maintain the uniaxial order parameter fixed, as suggested by experiments.","Our finite element simulations capture well these phenomena.","As necking in liquid crystal elastomers has not been satisfactorily modelled before, our theoretical and numerical findings related to this effect can be of wide interest.","Shear striping has been well studied, yet our computed examples also show how optimal stripe width increases with the nematic penetration depth measuring the competition between the Frank elasticity of liquid crystals and polymer elasticity.","Although known theoretically, this result has not been confirmed numerically by previous nonlinear elastic models."],"url":"http://arxiv.org/abs/2406.05476v1","category":"cond-mat.soft"}
{"created":"2024-06-08 13:32:49","title":"Accelerated Stochastic Gradient Method with Applications to Consensus Problem in Markov-Varying Networks","abstract":"Stochastic optimization is a vital field in the realm of mathematical optimization, finding applications in diverse areas ranging from operations research to machine learning. In this paper, we introduce a novel first-order optimization algorithm designed for scenarios where Markovian noise is present, incorporating Nesterov acceleration for enhanced efficiency. The convergence analysis is performed using an assumption on noise depending on the distance to the solution. We also delve into the consensus problem over Markov-varying networks, exploring how this algorithm can be applied to achieve agreement among multiple agents with differing objectives during changes in the communication system. To show the performance of our method on the problem above, we conduct experiments to demonstrate the superiority over the classic approach.","sentences":["Stochastic optimization is a vital field in the realm of mathematical optimization, finding applications in diverse areas ranging from operations research to machine learning.","In this paper, we introduce a novel first-order optimization algorithm designed for scenarios where Markovian noise is present, incorporating Nesterov acceleration for enhanced efficiency.","The convergence analysis is performed using an assumption on noise depending on the distance to the solution.","We also delve into the consensus problem over Markov-varying networks, exploring how this algorithm can be applied to achieve agreement among multiple agents with differing objectives during changes in the communication system.","To show the performance of our method on the problem above, we conduct experiments to demonstrate the superiority over the classic approach."],"url":"http://arxiv.org/abs/2406.05474v1","category":"math.OC"}
{"created":"2024-06-08 13:20:48","title":"RandONet: Shallow-Networks with Random Projections for learning linear and nonlinear operators","abstract":"Deep Operator Networks (DeepOnets) have revolutionized the domain of scientific machine learning for the solution of the inverse problem for dynamical systems. However, their implementation necessitates optimizing a high-dimensional space of parameters and hyperparameters. This fact, along with the requirement of substantial computational resources, poses a barrier to achieving high numerical accuracy. Here, inpsired by DeepONets and to address the above challenges, we present Random Projection-based Operator Networks (RandONets): shallow networks with random projections that learn linear and nonlinear operators. The implementation of RandONets involves: (a) incorporating random bases, thus enabling the use of shallow neural networks with a single hidden layer, where the only unknowns are the output weights of the network's weighted inner product; this reduces dramatically the dimensionality of the parameter space; and, based on this, (b) using established least-squares solvers (e.g., Tikhonov regularization and preconditioned QR decomposition) that offer superior numerical approximation properties compared to other optimization techniques used in deep-learning. In this work, we prove the universal approximation accuracy of RandONets for approximating nonlinear operators and demonstrate their efficiency in approximating linear nonlinear evolution operators (right-hand-sides (RHS)) with a focus on PDEs. We show, that for this particular task, RandONets outperform, both in terms of numerical approximation accuracy and computational cost, the ``vanilla\" DeepOnets.","sentences":["Deep Operator Networks (DeepOnets) have revolutionized the domain of scientific machine learning for the solution of the inverse problem for dynamical systems.","However, their implementation necessitates optimizing a high-dimensional space of parameters and hyperparameters.","This fact, along with the requirement of substantial computational resources, poses a barrier to achieving high numerical accuracy.","Here, inpsired by DeepONets and to address the above challenges, we present Random Projection-based Operator Networks (RandONets): shallow networks with random projections that learn linear and nonlinear operators.","The implementation of RandONets involves: (a) incorporating random bases, thus enabling the use of shallow neural networks with a single hidden layer, where the only unknowns are the output weights of the network's weighted inner product; this reduces dramatically the dimensionality of the parameter space; and, based on this, (b) using established least-squares solvers (e.g., Tikhonov regularization and preconditioned QR decomposition) that offer superior numerical approximation properties compared to other optimization techniques used in deep-learning.","In this work, we prove the universal approximation accuracy of RandONets for approximating nonlinear operators and demonstrate their efficiency in approximating linear nonlinear evolution operators (right-hand-sides (RHS)) with a focus on PDEs.","We show, that for this particular task, RandONets outperform, both in terms of numerical approximation accuracy and computational cost, the ``vanilla\" DeepOnets."],"url":"http://arxiv.org/abs/2406.05470v1","category":"cs.LG"}
{"created":"2024-06-08 13:19:18","title":"Bayesian vs. PAC-Bayesian Deep Neural Network Ensembles","abstract":"Bayesian neural networks address epistemic uncertainty by learning a posterior distribution over model parameters. Sampling and weighting networks according to this posterior yields an ensemble model referred to as Bayes ensemble. Ensembles of neural networks (deep ensembles) can profit from the cancellation of errors effect: Errors by ensemble members may average out and the deep ensemble achieves better predictive performance than each individual network. We argue that neither the sampling nor the weighting in a Bayes ensemble are particularly well-suited for increasing generalization performance, as they do not support the cancellation of errors effect, which is evident in the limit from the Bernstein-von~Mises theorem for misspecified models. In contrast, a weighted average of models where the weights are optimized by minimizing a PAC-Bayesian generalization bound can improve generalization performance. This requires that the optimization takes correlations between models into account, which can be achieved by minimizing the tandem loss at the cost that hold-out data for estimating error correlations need to be available. The PAC-Bayesian weighting increases the robustness against correlated models and models with lower performance in an ensemble. This allows us to safely add several models from the same learning process to an ensemble, instead of using early-stopping for selecting a single weight configuration. Our study presents empirical results supporting these conceptual considerations on four different classification datasets. We show that state-of-the-art Bayes ensembles from the literature, despite being computationally demanding, do not improve over simple uniformly weighted deep ensembles and cannot match the performance of deep ensembles weighted by optimizing the tandem loss, which additionally come with non-vacuous generalization guarantees.","sentences":["Bayesian neural networks address epistemic uncertainty by learning a posterior distribution over model parameters.","Sampling and weighting networks according to this posterior yields an ensemble model referred to as Bayes ensemble.","Ensembles of neural networks (deep ensembles) can profit from the cancellation of errors effect: Errors by ensemble members may average out and the deep ensemble achieves better predictive performance than each individual network.","We argue that neither the sampling nor the weighting in a Bayes ensemble are particularly well-suited for increasing generalization performance, as they do not support the cancellation of errors effect, which is evident in the limit from the Bernstein-von~Mises theorem for misspecified models.","In contrast, a weighted average of models where the weights are optimized by minimizing a PAC-Bayesian generalization bound can improve generalization performance.","This requires that the optimization takes correlations between models into account, which can be achieved by minimizing the tandem loss at the cost that hold-out data for estimating error correlations need to be available.","The PAC-Bayesian weighting increases the robustness against correlated models and models with lower performance in an ensemble.","This allows us to safely add several models from the same learning process to an ensemble, instead of using early-stopping for selecting a single weight configuration.","Our study presents empirical results supporting these conceptual considerations on four different classification datasets.","We show that state-of-the-art Bayes ensembles from the literature, despite being computationally demanding, do not improve over simple uniformly weighted deep ensembles and cannot match the performance of deep ensembles weighted by optimizing the tandem loss, which additionally come with non-vacuous generalization guarantees."],"url":"http://arxiv.org/abs/2406.05469v1","category":"cs.LG"}
{"created":"2024-06-08 12:25:09","title":"Gradient-based algorithms for multi-objective bi-level optimization","abstract":"Multi-Objective Bi-Level Optimization (MOBLO) addresses nested multi-objective optimization problems common in a range of applications. However, its multi-objective and hierarchical bilevel nature makes it notably complex. Gradient-based MOBLO algorithms have recently grown in popularity, as they effectively solve crucial machine learning problems like meta-learning, neural architecture search, and reinforcement learning. Unfortunately, these algorithms depend on solving a sequence of approximation subproblems with high accuracy, resulting in adverse time and memory complexity that lowers their numerical efficiency. To address this issue, we propose a gradient-based algorithm for MOBLO, called gMOBA, which has fewer hyperparameters to tune, making it both simple and efficient. Additionally, we demonstrate the theoretical validity by accomplishing the desirable Pareto stationarity. Numerical experiments confirm the practical efficiency of the proposed method and verify the theoretical results. To accelerate the convergence of gMOBA, we introduce a beneficial L2O neural network (called L2O-gMOBA) implemented as the initialization phase of our gMOBA algorithm. Comparative results of numerical experiments are presented to illustrate the performance of L2O-gMOBA.","sentences":["Multi-Objective Bi-Level Optimization (MOBLO) addresses nested multi-objective optimization problems common in a range of applications.","However, its multi-objective and hierarchical bilevel nature makes it notably complex.","Gradient-based MOBLO algorithms have recently grown in popularity, as they effectively solve crucial machine learning problems like meta-learning, neural architecture search, and reinforcement learning.","Unfortunately, these algorithms depend on solving a sequence of approximation subproblems with high accuracy, resulting in adverse time and memory complexity that lowers their numerical efficiency.","To address this issue, we propose a gradient-based algorithm for MOBLO, called gMOBA, which has fewer hyperparameters to tune, making it both simple and efficient.","Additionally, we demonstrate the theoretical validity by accomplishing the desirable Pareto stationarity.","Numerical experiments confirm the practical efficiency of the proposed method and verify the theoretical results.","To accelerate the convergence of gMOBA, we introduce a beneficial L2O neural network (called L2O-gMOBA) implemented as the initialization phase of our gMOBA algorithm.","Comparative results of numerical experiments are presented to illustrate the performance of L2O-gMOBA."],"url":"http://arxiv.org/abs/2406.05455v1","category":"math.OC"}
{"created":"2024-06-08 12:11:55","title":"Reconsideration of optimization for reduction of traffic congestion","abstract":"One of the most impressive applications of a quantum annealer was optimizing a group of Volkswagen to reduce traffic congestion using a D-Wave system. A simple formulation of a quadratic term was proposed to reduce traffic congestion. This quadratic term was useful for determining the shortest routes among several candidates. The original formulation produced decreases in the total lengths of car tours and traffic congestion. In this study, we reformulated the cost function with the sole focus on reducing traffic congestion. We then found a unique cost function for expressing the quadratic function with a dead zone and an inequality constraint.","sentences":["One of the most impressive applications of a quantum annealer was optimizing a group of Volkswagen to reduce traffic congestion using a D-Wave system.","A simple formulation of a quadratic term was proposed to reduce traffic congestion.","This quadratic term was useful for determining the shortest routes among several candidates.","The original formulation produced decreases in the total lengths of car tours and traffic congestion.","In this study, we reformulated the cost function with the sole focus on reducing traffic congestion.","We then found a unique cost function for expressing the quadratic function with a dead zone and an inequality constraint."],"url":"http://arxiv.org/abs/2406.05448v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-08 11:28:37","title":"A Generalized Pointing Error Model for FSO Links with Fixed-Wing UAVs for 6G: Analysis and Trajectory Optimization","abstract":"Free-space optical (FSO) communication is a promising solution to support wireless backhaul links in emerging 6G non-terrestrial networks. At the link level, pointing errors in FSO links can significantly impact capacity, making accurate modeling of these errors essential for both assessing and enhancing communication performance. In this paper, we introduce a novel model for FSO pointing errors in unmanned aerial vehicles (UAVs) that incorporates three-dimensional (3D) jitter, including roll, pitch, and yaw angle jittering. We derive a probability density function for the pointing error angle based on the relative position and posture of the UAV to the ground station. This model is then integrated into a trajectory optimization problem designed to maximize energy efficiency while meeting constraints on speed, acceleration, and elevation angle. Our proposed optimization method significantly improves energy efficiency by adjusting the UAV's flight trajectory to minimize exposure to directions highly affected by jitter. The simulation results emphasize the importance of using UAV-specific 3D jitter models in achieving accurate performance measurements and effective system optimization in FSO communication networks. Utilizing our generalized model, the optimized trajectories achieve up to 11.8 percent higher energy efficiency compared to those derived from conventional Gaussian pointing error models.","sentences":["Free-space optical (FSO) communication is a promising solution to support wireless backhaul links in emerging 6G non-terrestrial networks.","At the link level, pointing errors in FSO links can significantly impact capacity, making accurate modeling of these errors essential for both assessing and enhancing communication performance.","In this paper, we introduce a novel model for FSO pointing errors in unmanned aerial vehicles (UAVs) that incorporates three-dimensional (3D) jitter, including roll, pitch, and yaw angle jittering.","We derive a probability density function for the pointing error angle based on the relative position and posture of the UAV to the ground station.","This model is then integrated into a trajectory optimization problem designed to maximize energy efficiency while meeting constraints on speed, acceleration, and elevation angle.","Our proposed optimization method significantly improves energy efficiency by adjusting the UAV's flight trajectory to minimize exposure to directions highly affected by jitter.","The simulation results emphasize the importance of using UAV-specific 3D jitter models in achieving accurate performance measurements and effective system optimization in FSO communication networks.","Utilizing our generalized model, the optimized trajectories achieve up to 11.8 percent higher energy efficiency compared to those derived from conventional Gaussian pointing error models."],"url":"http://arxiv.org/abs/2406.05444v1","category":"eess.SY"}
{"created":"2024-06-10 17:18:07","title":"Searching for Pseudo-Dirac neutrinos from Astrophysical sources in IceCube data","abstract":"We analyze IceCube public data from its IC86 configuration, namely PSTracks event selection, to search for pseudo-Dirac signatures in high-energy neutrinos from astrophysical sources. Neutrino flux from astrophysical sources is reduced in the pseudo-Dirac scenario due to conversion of active-to-sterile neutrinos as compared to the neutrino oscillation scenario of only three active neutrinos over astrophysical distances. We fit IceCube data using astrophysical flux models for three point-like sources in both scenarios and constrain the active-sterile mass-square-difference in the absence of any evidence for pseudo-Dirac scenario. We find that a common mass-squared-difference $\\delta m^2$ for all three flavors can be constrained as $\\delta m^2 \\lesssim 4 \\times 10^{-19}~eV^2$ for the source NGC 1068, $\\delta m^2 \\lesssim 8 \\times 10^{-20}~eV^2$ for the source TXS 0506+056, and $\\delta m^2 \\lesssim 2.3 \\times 10^{-21}~eV^2$ for the source PKS 1424+240 at 90% C.L. A stacking analysis gives a constrain on $\\delta m^2 \\le 2.3 \\times 10^{-21}~eV^2$ at 90% CL which is dominated by the constraint obtained from PKS 1424+240.","sentences":["We analyze IceCube public data from its IC86 configuration, namely PSTracks event selection, to search for pseudo-Dirac signatures in high-energy neutrinos from astrophysical sources.","Neutrino flux from astrophysical sources is reduced in the pseudo-Dirac scenario due to conversion of active-to-sterile neutrinos as compared to the neutrino oscillation scenario of only three active neutrinos over astrophysical distances.","We fit IceCube data using astrophysical flux models for three point-like sources in both scenarios and constrain the active-sterile mass-square-difference in the absence of any evidence for pseudo-Dirac scenario.","We find that a common mass-squared-difference $\\delta m^2$ for all three flavors can be constrained as $\\delta m^2 \\lesssim 4 \\times 10^{-19}~eV^2$ for the source NGC 1068, $\\delta m^2 \\lesssim 8 \\times 10^{-20}~eV^2$ for the source TXS 0506+056, and $\\delta m^2 \\lesssim 2.3 \\times 10^{-21}~eV^2$ for the source PKS 1424+240 at 90%","C.L.","A stacking analysis gives a constrain on $\\delta m^2 \\le 2.3 \\times 10^{-21}~eV^2$ at 90% CL which is dominated by the constraint obtained from PKS 1424+240."],"url":"http://arxiv.org/abs/2406.06476v1","category":"astro-ph.HE"}
{"created":"2024-06-10 16:36:12","title":"Flat space spinning massive amplitudes from momentum space CFT","abstract":"We discuss the flat space limit of AdS using the momentum space representation of CFT correlators. The flat space limit involves sending the AdS radius and the dimensions of operators dual to massive fields to infinity while also scaling appropriately the sources of the dual operators. In this limit, d-dimensional CFT correlators become (d+1)-dimensional scattering amplitudes. We exemplify our discussion with the computation of the flat-space limit of the CFT 3-point function of a conserved current, a non-conserved charged vector operator and its conjugate. The flat-space limit should yield the scattering amplitude of an Abelian gauge field with two massive vector fields. This scattering amplitude computes the electromagnetic form factors of the electromagnetic current in a spin-1 state, and these form factors encode the electromagnetic properties of the massive vector field (charge, magnetic moment and quadruple moment). In terms of the CFT, the flat-space limit amounts to zooming in the infrared region of the triple-K integrals that determine the 3-point function, while also scaling to infinity the order of (some of) the Bessel functions that feature in the triple-K integrals. In this limit the triple-K integral becomes proportional to the energy-preserving delta function, and the flat space limit correctly yields the corresponding flat space scattering amplitude in complete detail.","sentences":["We discuss the flat space limit of AdS using the momentum space representation of CFT correlators.","The flat space limit involves sending the AdS radius and the dimensions of operators dual to massive fields to infinity while also scaling appropriately the sources of the dual operators.","In this limit, d-dimensional CFT correlators become (d+1)-dimensional scattering amplitudes.","We exemplify our discussion with the computation of the flat-space limit of the CFT 3-point function of a conserved current, a non-conserved charged vector operator and its conjugate.","The flat-space limit should yield the scattering amplitude of an Abelian gauge field with two massive vector fields.","This scattering amplitude computes the electromagnetic form factors of the electromagnetic current in a spin-1 state, and these form factors encode the electromagnetic properties of the massive vector field (charge, magnetic moment and quadruple moment).","In terms of the CFT, the flat-space limit amounts to zooming in the infrared region of the triple-K integrals that determine the 3-point function, while also scaling to infinity the order of (some of) the Bessel functions that feature in the triple-K integrals.","In this limit the triple-K integral becomes proportional to the energy-preserving delta function, and the flat space limit correctly yields the corresponding flat space scattering amplitude in complete detail."],"url":"http://arxiv.org/abs/2406.06447v1","category":"hep-th"}
{"created":"2024-06-10 15:48:11","title":"Dilatonic Couplings and the Relic Abundance of Ultralight Dark Matter","abstract":"Models of scalar field dark matter where the scalar is a dilaton have a special behaviour, since non-trivial couplings, $d$, to matter result in a contribution to the potential for the field which is proportional to the trace of the stress-energy tensor. We look in more detail at the dilaton mass, $m_\\phi$, and initial conditions required to yield the correct relic abundance for couplings that are not already excluded by terrestrial experiments. In minimal models with only couplings accessible to terrestrial searches, we find that dilaton dark matter with $m_{\\phi} \\gtrsim 10^{-10}$ eV requires couplings suppressed compared to constraints from equivalence principle (EP) tests and fifth force searches in order to not produce too much dark matter, improving on the strongest current experimental constraints by up to $\\sim {\\cal O}(10)$, with consequences for the proposed mechanical resonator dilaton DM searches. In non-minimal or universally coupled models, the unconstrained couplings of the dilaton to e.g. the top quark can strongly influence the relic abundance at all masses. In particular, this implies that atom interferometry searches at masses $m_\\phi\\approx 10^{-19}\\text{ eV}$ are unable to constrain the early Universe behaviour or UV physics of the dilaton. We also find that dilatonic couplings allow for compatibility of $m_\\phi \\gtrsim 10^{-7}\\text{ eV}$ with an observably large tensor-to-scalar ratio in the cosmic microwave background, which is not possible for a decoupled scalar of the same mass.","sentences":["Models of scalar field dark matter where the scalar is a dilaton have a special behaviour, since non-trivial couplings, $d$, to matter result in a contribution to the potential for the field which is proportional to the trace of the stress-energy tensor.","We look in more detail at the dilaton mass, $m_\\phi$, and initial conditions required to yield the correct relic abundance for couplings that are not already excluded by terrestrial experiments.","In minimal models with only couplings accessible to terrestrial searches, we find that dilaton dark matter with $m_{\\phi} \\gtrsim 10^{-10}$ eV requires couplings suppressed compared to constraints from equivalence principle (EP) tests and fifth force searches in order to not produce too much dark matter, improving on the strongest current experimental constraints by up to $\\sim {\\cal O}(10)$, with consequences for the proposed mechanical resonator dilaton DM searches.","In non-minimal or universally coupled models, the unconstrained couplings of the dilaton to e.g. the top quark can strongly influence the relic abundance at all masses.","In particular, this implies that atom interferometry searches at masses $m_\\phi\\approx 10^{-19}\\text{ eV}$ are unable to constrain the early Universe behaviour or UV physics of the dilaton.","We also find that dilatonic couplings allow for compatibility of $m_\\phi \\gtrsim 10^{-7}\\text{ eV}$ with an observably large tensor-to-scalar ratio in the cosmic microwave background, which is not possible for a decoupled scalar of the same mass."],"url":"http://arxiv.org/abs/2406.06395v1","category":"hep-ph"}
{"created":"2024-06-10 15:27:58","title":"Characteristics and Energy Flux Distributions of Decayless Transverse Oscillations Depending on Coronal Regions","abstract":"Lim et al. (2023) have recently proposed that the slope ($\\delta$) of the power law distribution between the energy flux and oscillation frequency could determine whether high-frequency transverse oscillations give a dominant contribution to the heating ($\\delta<1$). A meta-analysis of decayless transverse oscillations revealed that high-frequency oscillations potentially play a key role in heating the solar corona. We aim to investigate how (whether) the distributions of the energy flux contained in transverse oscillations, and their slopes, depend on the coronal region in which the oscillation occurs. We analyse transverse oscillations from 41 quiet Sun (QS) loops and 22 active region (AR) loops observed by Solar Orbiter/Extreme Ultraviolet Imager (SolO/EUI) HRIEUV. The energy flux and energy are estimated using analysed oscillation parameters and loop properties, such as periods, displacement amplitudes, loop lengths, and minor radii of the loops. It is found that about 71% of QS loops and 86% of AR loops show decayless oscillations. We find that the amplitude does not change depending on different regions, but the difference in the period is more pronounced. Although the power law slope ($\\delta=-1.79$) in AR is steeper than that ($\\delta=-1.59$) in QS, both of them are significantly less than the critical slope of 1. Our statistical study demonstrates that high-frequency transverse oscillations can heat the QS. For ARs, the total energy flux is insufficient unless yet-unobserved oscillations with frequencies up to 0.17 Hz are present. Future EUI campaigns will be planned to confirm this.","sentences":["Lim et al. (2023) have recently proposed that the slope ($\\delta$) of the power law distribution between the energy flux and oscillation frequency could determine whether high-frequency transverse oscillations give a dominant contribution to the heating ($\\delta<1$).","A meta-analysis of decayless transverse oscillations revealed that high-frequency oscillations potentially play a key role in heating the solar corona.","We aim to investigate how (whether) the distributions of the energy flux contained in transverse oscillations, and their slopes, depend on the coronal region in which the oscillation occurs.","We analyse transverse oscillations from 41 quiet Sun (QS) loops and 22 active region (AR) loops observed by Solar Orbiter/Extreme Ultraviolet Imager (SolO/EUI) HRIEUV.","The energy flux and energy are estimated using analysed oscillation parameters and loop properties, such as periods, displacement amplitudes, loop lengths, and minor radii of the loops.","It is found that about 71% of QS loops and 86% of AR loops show decayless oscillations.","We find that the amplitude does not change depending on different regions, but the difference in the period is more pronounced.","Although the power law slope ($\\delta=-1.79$) in AR is steeper than that ($\\delta=-1.59$) in QS, both of them are significantly less than the critical slope of 1.","Our statistical study demonstrates that high-frequency transverse oscillations can heat the QS.","For ARs, the total energy flux is insufficient unless yet-unobserved oscillations with frequencies up to 0.17 Hz are present.","Future EUI campaigns will be planned to confirm this."],"url":"http://arxiv.org/abs/2406.06368v1","category":"astro-ph.SR"}
{"created":"2024-06-10 14:35:59","title":"Vehicle Vectors and Traffic Patterns from Planet Imagery","abstract":"We explore methods to detect automobiles in Planet imagery and build a large scale vector field for moving objects. Planet operates two distinct constellations: high-resolution SkySat satellites as well as medium-resolution SuperDove satellites. We show that both static and moving cars can be identified reliably in high-resolution SkySat imagery. We are able to estimate the speed and heading of moving vehicles by leveraging the inter-band displacement (or \"rainbow\" effect) of moving objects. Identifying cars and trucks in medium-resolution SuperDove imagery is far more difficult, though a similar rainbow effect is observed in these satellites and enables moving vehicles to be detected and vectorized. The frequent revisit of Planet satellites enables the categorization of automobile and truck activity patterns over broad areas of interest and lengthy timeframes.","sentences":["We explore methods to detect automobiles in Planet imagery and build a large scale vector field for moving objects.","Planet operates two distinct constellations: high-resolution SkySat satellites as well as medium-resolution SuperDove satellites.","We show that both static and moving cars can be identified reliably in high-resolution SkySat imagery.","We are able to estimate the speed and heading of moving vehicles by leveraging the inter-band displacement (or \"rainbow\" effect) of moving objects.","Identifying cars and trucks in medium-resolution SuperDove imagery is far more difficult, though a similar rainbow effect is observed in these satellites and enables moving vehicles to be detected and vectorized.","The frequent revisit of Planet satellites enables the categorization of automobile and truck activity patterns over broad areas of interest and lengthy timeframes."],"url":"http://arxiv.org/abs/2406.06320v1","category":"cs.CV"}
{"created":"2024-06-10 14:23:46","title":"Topological structures, dark matter and gravitational waves in $E_6$","abstract":"We discuss the appearance of topological structures from the spontaneous breaking of $E_6$ to the Standard Model via its maximal subgroup $SO(10) \\times U(1)_\\psi$. They include dumbbells, metastable strings, as well as domain walls bounded by necklaces. We provide a novel scenario for producing metastable strings based on the symmetry breaking $U(1)_\\psi \\longrightarrow Z_8 \\longrightarrow Z_4$. The metastable string arises from the merger of $Z_8$ strings that bound a domain wall. An unbroken gauge $Z_2$ symmetry from $SO(10)$ breaking yields viable stable dark matter candidates as well as topologically stable strings. We discuss the gravitational wave emission from two varieties of cosmic strings, namely the superheavy metastable ones and the intermediate scale topologically stable cosmic strings.","sentences":["We discuss the appearance of topological structures from the spontaneous breaking of $E_6$ to the Standard Model via its maximal subgroup $SO(10) \\times U(1)_\\psi$.","They include dumbbells, metastable strings, as well as domain walls bounded by necklaces.","We provide a novel scenario for producing metastable strings based on the symmetry breaking $U(1)_\\psi \\longrightarrow Z_8 \\longrightarrow Z_4$. The metastable string arises from the merger of $Z_8$ strings that bound a domain wall.","An unbroken gauge $Z_2$ symmetry from $SO(10)$ breaking yields viable stable dark matter candidates as well as topologically stable strings.","We discuss the gravitational wave emission from two varieties of cosmic strings, namely the superheavy metastable ones and the intermediate scale topologically stable cosmic strings."],"url":"http://arxiv.org/abs/2406.06308v1","category":"hep-ph"}
{"created":"2024-06-10 14:18:08","title":"Human Gaze and Head Rotation during Navigation, Exploration and Object Manipulation in Shared Environments with Robots","abstract":"The human gaze is an important cue to signal intention, attention, distraction, and the regions of interest in the immediate surroundings. Gaze tracking can transform how robots perceive, understand, and react to people, enabling new modes of robot control, interaction, and collaboration. In this paper, we use gaze tracking data from a rich dataset of human motion (TH\\\"OR-MAGNI) to investigate the coordination between gaze direction and head rotation of humans engaged in various indoor activities involving navigation, interaction with objects, and collaboration with a mobile robot. In particular, we study the spread and central bias of fixations in diverse activities and examine the correlation between gaze direction and head rotation. We introduce various human motion metrics to enhance the understanding of gaze behavior in dynamic interactions. Finally, we apply semantic object labeling to decompose the gaze distribution into activity-relevant regions.","sentences":["The human gaze is an important cue to signal intention, attention, distraction, and the regions of interest in the immediate surroundings.","Gaze tracking can transform how robots perceive, understand, and react to people, enabling new modes of robot control, interaction, and collaboration.","In this paper, we use gaze tracking data from a rich dataset of human motion (TH\\\"OR-MAGNI) to investigate the coordination between gaze direction and head rotation of humans engaged in various indoor activities involving navigation, interaction with objects, and collaboration with a mobile robot.","In particular, we study the spread and central bias of fixations in diverse activities and examine the correlation between gaze direction and head rotation.","We introduce various human motion metrics to enhance the understanding of gaze behavior in dynamic interactions.","Finally, we apply semantic object labeling to decompose the gaze distribution into activity-relevant regions."],"url":"http://arxiv.org/abs/2406.06300v1","category":"cs.RO"}
{"created":"2024-06-10 14:17:34","title":"Large dynamical magnetic effective charges and anti-magnetoelectricity from spin and orbital origin in multiferroic BiCoO$_3$","abstract":"Using first-principles calculations, we explore the magnetoelectric properties of the room-temperature multiferroic crystal BiCoO$_3$. We use both applied magnetic field and finite-difference techniques to show that BiCoO$_3$ is anti-magnetoelectric at the linear level. The calculation of the dynamical effective charges reveals that the total magnetoelectric response is zero due to the compensating non-zero magnetoelectric response of each magnetic sublattice. This calculation also highlights that the the orbital contribution to the response is remarkably larger than the spin one and that each sublattice has a rather large total magnetoelectric response of 85 ps/m. Furthermore, we provide an intuitive recipe to visualize the dynamical magnetic effective charge, allowing to examine its multipolar nature which we confirm by means of ab initio calculations. Given the large value of the local response, we investigate the ferromagnetic phase as well, which gives a giant magnetoelectric response of about 1000 ps/m and coming mainly from the spin contribution this time. Finally, we discuss the possible reasons for such a large magnetoelectric response in BiCoO3 and propose possible strategies to unveil this potentially large response.","sentences":["Using first-principles calculations, we explore the magnetoelectric properties of the room-temperature multiferroic crystal BiCoO$_3$. We use both applied magnetic field and finite-difference techniques to show that BiCoO$_3$ is anti-magnetoelectric at the linear level.","The calculation of the dynamical effective charges reveals that the total magnetoelectric response is zero due to the compensating non-zero magnetoelectric response of each magnetic sublattice.","This calculation also highlights that the the orbital contribution to the response is remarkably larger than the spin one and that each sublattice has a rather large total magnetoelectric response of 85 ps/m. Furthermore, we provide an intuitive recipe to visualize the dynamical magnetic effective charge, allowing to examine its multipolar nature which we confirm by means of ab initio calculations.","Given the large value of the local response, we investigate the ferromagnetic phase as well, which gives a giant magnetoelectric response of about 1000 ps/m and coming mainly from the spin contribution this time.","Finally, we discuss the possible reasons for such a large magnetoelectric response in BiCoO3 and propose possible strategies to unveil this potentially large response."],"url":"http://arxiv.org/abs/2406.06298v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 13:48:39","title":"Large Out-of-Plane Piezoelectric Effect in Janus Ferromagnetic Semiconductor Monolayer of CrOFBr","abstract":"The exploitation of piezoelectric ferromagnetism (PFM) in two-dimensional (2D) materials with large out-of-plane piezoelectric response is motivated not only by technological applications but also scientific interest. In this study, the CrONM monolayer family (N=F, Cl; M=Br, Cl) was investigated using first-principles calculations, revealing that the Janus CrOFBr monolayer exhibits intrinsic ferromagnetic semiconductor behavior along with a significant out-of-plane piezoelectric effect. The calculated out-of-plane piezoelectric strain coefficients d$_{31}$ and d$_{32}$ are up to 1.21 and 0.63 pm/V, respectively. These values are greater than those of the majority of 2D materials. Furthermore, our findings demonstrate that applying tensile strain can enhance the out-of-plane piezoelectric response, leading to a respective 27% and 67% augmentation in the piezoelectric strain coefficients d$_{31}$ and d$_{32}$ compared to the unstrained configurations. This discovery holds great potential for propelling the field of nanoelectronics forward and facilitating the development of multifunctional semiconductor spintronic applications. Finally, by comparing d$_{31}$ and d$_{32}$ of the CrONM monolayer family (N=F, Cl; M=Br, Cl), we find that the magnitudes of d$_{31}$ and d$_{32}$ are correlated with the electronegativity difference between the M and N atoms. These findings provide valuable insights for the design of 2D piezoelectric materials with enhanced vertical piezoelectric responses.","sentences":["The exploitation of piezoelectric ferromagnetism (PFM) in two-dimensional (2D) materials with large out-of-plane piezoelectric response is motivated not only by technological applications but also scientific interest.","In this study, the CrONM monolayer family (N=F, Cl; M=Br, Cl) was investigated using first-principles calculations, revealing that the Janus CrOFBr monolayer exhibits intrinsic ferromagnetic semiconductor behavior along with a significant out-of-plane piezoelectric effect.","The calculated out-of-plane piezoelectric strain coefficients d$_{31}$ and d$_{32}$ are up to 1.21 and 0.63 pm/V, respectively.","These values are greater than those of the majority of 2D materials.","Furthermore, our findings demonstrate that applying tensile strain can enhance the out-of-plane piezoelectric response, leading to a respective 27% and 67% augmentation in the piezoelectric strain coefficients d$_{31}$ and d$_{32}$ compared to the unstrained configurations.","This discovery holds great potential for propelling the field of nanoelectronics forward and facilitating the development of multifunctional semiconductor spintronic applications.","Finally, by comparing d$_{31}$ and d$_{32}$ of the CrONM monolayer family (N=F, Cl; M=Br, Cl), we find that the magnitudes of d$_{31}$ and d$_{32}$ are correlated with the electronegativity difference between the M and N atoms.","These findings provide valuable insights for the design of 2D piezoelectric materials with enhanced vertical piezoelectric responses."],"url":"http://arxiv.org/abs/2406.06265v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 13:46:07","title":"DualAD: Disentangling the Dynamic and Static World for End-to-End Driving","abstract":"State-of-the-art approaches for autonomous driving integrate multiple sub-tasks of the overall driving task into a single pipeline that can be trained in an end-to-end fashion by passing latent representations between the different modules. In contrast to previous approaches that rely on a unified grid to represent the belief state of the scene, we propose dedicated representations to disentangle dynamic agents and static scene elements. This allows us to explicitly compensate for the effect of both ego and object motion between consecutive time steps and to flexibly propagate the belief state through time. Furthermore, dynamic objects can not only attend to the input camera images, but also directly benefit from the inferred static scene structure via a novel dynamic-static cross-attention. Extensive experiments on the challenging nuScenes benchmark demonstrate the benefits of the proposed dual-stream design, especially for modelling highly dynamic agents in the scene, and highlight the improved temporal consistency of our approach. Our method titled DualAD not only outperforms independently trained single-task networks, but also improves over previous state-of-the-art end-to-end models by a large margin on all tasks along the functional chain of driving.","sentences":["State-of-the-art approaches for autonomous driving integrate multiple sub-tasks of the overall driving task into a single pipeline that can be trained in an end-to-end fashion by passing latent representations between the different modules.","In contrast to previous approaches that rely on a unified grid to represent the belief state of the scene, we propose dedicated representations to disentangle dynamic agents and static scene elements.","This allows us to explicitly compensate for the effect of both ego and object motion between consecutive time steps and to flexibly propagate the belief state through time.","Furthermore, dynamic objects can not only attend to the input camera images, but also directly benefit from the inferred static scene structure via a novel dynamic-static cross-attention.","Extensive experiments on the challenging nuScenes benchmark demonstrate the benefits of the proposed dual-stream design, especially for modelling highly dynamic agents in the scene, and highlight the improved temporal consistency of our approach.","Our method titled DualAD not only outperforms independently trained single-task networks, but also improves over previous state-of-the-art end-to-end models by a large margin on all tasks along the functional chain of driving."],"url":"http://arxiv.org/abs/2406.06264v1","category":"cs.CV"}
{"created":"2024-06-10 13:08:10","title":"Significant dependence of the efficiency of energy-saving thermochromic VO$_2$ on slight changes of its properties in the visible due to strain and/or vacancies","abstract":"There are worldwide efforts to maximize the energy saving achieved by VO$_2$-based thermochromic coatings. In particular, there are very different values of the modulation of integral solar energy transmittance, reported by various laboratories on various templates even for seemingly very similar coatings. A detailed analysis reveals that this is largely due to the combination of the intentional and well understood transmittance modulation in the infrared (always beneficial) with not yet understood slight transmittance modulation in the visible (sometimes beneficial, sometimes harmful, and always multiplied by strong solar irradiance). Ab-initio calculations are used to examine the hypothesis that the transmittance modulation in the visible can be controlled by lattice strain and/or slightly off-stoichiometric [O]/[V] ratio. The presented phenomenon opens a pathway which may lead, in a case of reproducible preparation of correctly altered VO$_2$, to significantly enhanced energy saving.","sentences":["There are worldwide efforts to maximize the energy saving achieved by VO$_2$-based thermochromic coatings.","In particular, there are very different values of the modulation of integral solar energy transmittance, reported by various laboratories on various templates even for seemingly very similar coatings.","A detailed analysis reveals that this is largely due to the combination of the intentional and well understood transmittance modulation in the infrared (always beneficial) with not yet understood slight transmittance modulation in the visible (sometimes beneficial, sometimes harmful, and always multiplied by strong solar irradiance).","Ab-initio calculations are used to examine the hypothesis that the transmittance modulation in the visible can be controlled by lattice strain and/or slightly off-stoichiometric [O]/[V] ratio.","The presented phenomenon opens a pathway which may lead, in a case of reproducible preparation of correctly altered VO$_2$, to significantly enhanced energy saving."],"url":"http://arxiv.org/abs/2406.06238v1","category":"physics.app-ph"}
{"created":"2024-06-10 13:04:03","title":"Plasma screening in mid-charged ions observed by K-shell line emission","abstract":"Dense plasma environment affects the electronic structure of ions via variations of the microscopic electrical fields, also known as plasma screening. This effect can be either estimated by simplified analytical models, or by computationally expensive and to date unverified numerical calculations. We have experimentally quantified plasma screening from the energy shifts of the bound-bound transitions in matter driven by the x-ray free electron laser (XFEL). This was enabled by identification of detailed electronic configurations of the observed K{\\alpha}, K\\b{eta} and K{\\gamma} lines. This work paves the way for improving plasma screening models including connected effects like ionization potential depression and continuum lowering, which will advance the understanding of atomic physics in Warm Dense Matter regime.","sentences":["Dense plasma environment affects the electronic structure of ions via variations of the microscopic electrical fields, also known as plasma screening.","This effect can be either estimated by simplified analytical models, or by computationally expensive and to date unverified numerical calculations.","We have experimentally quantified plasma screening from the energy shifts of the bound-bound transitions in matter driven by the x-ray free electron laser (XFEL).","This was enabled by identification of detailed electronic configurations of the observed K{\\alpha}, K\\b{eta} and K{\\gamma} lines.","This work paves the way for improving plasma screening models including connected effects like ionization potential depression and continuum lowering, which will advance the understanding of atomic physics in Warm Dense Matter regime."],"url":"http://arxiv.org/abs/2406.06233v1","category":"physics.plasm-ph"}
{"created":"2024-06-10 12:34:17","title":"Mind the gap: Distinguishing disc substructures and their impact on the composition of the inner disc","abstract":"Improved observational technologies have enabled the resolution of substructures and the measurement of chemical abundances in protoplanetary discs. Understanding the chemical composition of the inner disc allows us to infer the building blocks available for planet formation. Recently, the depletion of water in the inner disc has been suggested to be linked to the presence of substructures like gaps and rings further out in the disc. We investigate this hypothesis further by running 1D semi-analytical models of a protoplanetary disc with a gap to understand the combined effects of disc viscosity, gap depth, gap location and gap formation time on the composition of the inner disc. Our results show that for a specific value of disc viscosity, the simulation outcome can be classified into three regimes: shallow gap, 'traffic jam', and deep gap. While deep gaps may already be distinguishable with moderate resolution, shallow gaps remains a challenge to resolve with current capabilities. On the other hand, discs with traffic jams have a higher chance of being resolved when observed with high resolution but may appear as an intensity enhancement or even featureless when observed with moderate to low angular resolution. In this regard, information on the inner disc composition is useful because it can help to infer the existence of traffic jams or distinguish them from deep gaps: Discs with deep gaps are expected to have a low water content and thus high C/O ratio in the inner disc due to the effective blocking of pebbles, discs with shallow gaps would show the opposite trend, and discs with traffic jam would have a constant -- albeit low -- inward flux of water-rich pebbles resulting in a moderate water content and sub-stellar C/O ratios. Finally, we find that the effectiveness of gaps as pebble barriers diminishes quickly when they form late, as most of the pebbles already drifted inwards.","sentences":["Improved observational technologies have enabled the resolution of substructures and the measurement of chemical abundances in protoplanetary discs.","Understanding the chemical composition of the inner disc allows us to infer the building blocks available for planet formation.","Recently, the depletion of water in the inner disc has been suggested to be linked to the presence of substructures like gaps and rings further out in the disc.","We investigate this hypothesis further by running 1D semi-analytical models of a protoplanetary disc with a gap to understand the combined effects of disc viscosity, gap depth, gap location and gap formation time on the composition of the inner disc.","Our results show that for a specific value of disc viscosity, the simulation outcome can be classified into three regimes: shallow gap, 'traffic jam', and deep gap.","While deep gaps may already be distinguishable with moderate resolution, shallow gaps remains a challenge to resolve with current capabilities.","On the other hand, discs with traffic jams have a higher chance of being resolved when observed with high resolution but may appear as an intensity enhancement or even featureless when observed with moderate to low angular resolution.","In this regard, information on the inner disc composition is useful because it can help to infer the existence of traffic jams or distinguish them from deep gaps: Discs with deep gaps are expected to have a low water content and thus high C/O ratio in the inner disc due to the effective blocking of pebbles, discs with shallow gaps would show the opposite trend, and discs with traffic jam would have a constant -- albeit low -- inward flux of water-rich pebbles resulting in a moderate water content and sub-stellar C/O ratios.","Finally, we find that the effectiveness of gaps as pebble barriers diminishes quickly when they form late, as most of the pebbles already drifted inwards."],"url":"http://arxiv.org/abs/2406.06219v1","category":"astro-ph.EP"}
{"created":"2024-06-10 12:16:59","title":"Precision calculations of the $D_{(s)}D_{(s)}V$ and $B_{(s)}B_{(s)}V$ couplings from light-cone sum rules","abstract":"We present an improved calculation of the $HHV$ ($H=D_{(s)},\\, B_{(s)}$, $V= \\rho$, $K^\\ast$, $\\omega$, and $\\phi$) coupling constants $g_{HHV}$ beyond leading order in $\\alpha_s$ from QCD light-cone sum rules (LCSRs) by means of the light-cone distribution amplitudes (LCDAs) of light vector mesons. Near the light-cone, the next-to-leading order QCD corrections for the vacuum-to-vector-meson correlation function are included at leading power in $\\delta_V = m_V/m_Q$ ($Q=b,c$) within the framework of hard-collinear factorization. The higher-twist corrections from two-particle and three-particle vector meson LCDAs are systematically incorporated at leading order in $\\alpha_s$ by applying the method of background field in LCSRs. Based on these improvements, we perform a systematic computation of the strong coupling constants $g_{H HV}$ and extract the effective coupling $\\beta$ of the heavy meson chiral perturbation theory (HM$\\chi$PT). Furthermore, we accomplish the analysis for the relation between the coupling $g_{HHV}$ and the residue of the $H\\to V$ transition form factor $A_0$ at heavy pseudoscalar pole. Additionally, we provide a detailed investigation of the $SU(3)$ flavour symmetry breaking effects and conduct a comparative analysis with results from previous studies.","sentences":["We present an improved calculation of the $HHV$ ($H=D_{(s)},\\, B_{(s)}$, $V= \\rho$, $K^\\ast$, $\\omega$, and $\\phi$) coupling constants $g_{HHV}$ beyond leading order in $\\alpha_s$ from QCD light-cone sum rules (LCSRs) by means of the light-cone distribution amplitudes (LCDAs) of light vector mesons.","Near the light-cone, the next-to-leading order QCD corrections for the vacuum-to-vector-meson correlation function are included at leading power in $\\delta_V = m_V/m_Q$ ($Q=b,c$) within the framework of hard-collinear factorization.","The higher-twist corrections from two-particle and three-particle vector meson LCDAs are systematically incorporated at leading order in $\\alpha_s$ by applying the method of background field in LCSRs.","Based on these improvements, we perform a systematic computation of the strong coupling constants $g_{H HV}$ and extract the effective coupling $\\beta$ of the heavy meson chiral perturbation theory (HM$\\chi$PT).","Furthermore, we accomplish the analysis for the relation between the coupling $g_{HHV}$ and the residue of the $H\\to V$ transition form factor $A_0$ at heavy pseudoscalar pole.","Additionally, we provide a detailed investigation of the $SU(3)$ flavour symmetry breaking effects and conduct a comparative analysis with results from previous studies."],"url":"http://arxiv.org/abs/2406.06209v1","category":"hep-ph"}
{"created":"2024-06-10 11:53:23","title":"Inequalities of energy release rates in compression of nano-porous materials predict its imminent breakdown","abstract":"We show that the divergent acoustic energy release rate in a quasi-statically compressed nano-porous material can be used as a precursor to failure in such materials. A quantification of the inequality of the energy release rate using social inequality measure indices help constructing a warning signal for large bursts of energy release. We also verify similar behavior for simulations of viscoelastic fiber bundle models that mimic the strain-hardening dynamics of the samples. The results demonstrate experimental applicability of the precursory signal formulation for any diverging response function near a transition point using social inequality indices.","sentences":["We show that the divergent acoustic energy release rate in a quasi-statically compressed nano-porous material can be used as a precursor to failure in such materials.","A quantification of the inequality of the energy release rate using social inequality measure indices help constructing a warning signal for large bursts of energy release.","We also verify similar behavior for simulations of viscoelastic fiber bundle models that mimic the strain-hardening dynamics of the samples.","The results demonstrate experimental applicability of the precursory signal formulation for any diverging response function near a transition point using social inequality indices."],"url":"http://arxiv.org/abs/2406.06200v1","category":"physics.soc-ph"}
{"created":"2024-06-10 11:46:40","title":"The Negative Action Keldysh Spinors","abstract":"We discuss quantization of Dirac spinors with action which is the negative of the standard action. Using the Keldysh QFT formulation, we show that the standard quantization of such Keldysh spinors results in the Hamiltonian bounded from above and in S-matrix for backwards in time scattering. The alternative quantization produces the standard Hamiltonian bounded from below and the standard S-matrix for forwards in time scattering. When the two masses are equal, the sum of their actions describes the standard Keldysh out-of-time-order correlators. When both Keldysh and Dirac spinors are present in a QFT, the standard Fock space is augmented by negative energy states, while its positive and the negative subspaces share the same vacuum state. In absence of gravity the Dirac and the Keldysh spinors do not interact and positive and negative energy states do not exchange quanta with non-zero energy. If Keldysh spinors exist in Nature, then the Universe would consist of bosonic and fermionic particles that are described by both positive and negative Fock energy states with a single vacuum state that cannot be crossed from either side. Such a scenario could explain the non-observation of dark energy/dark matter except via classical gravity.","sentences":["We discuss quantization of Dirac spinors with action which is the negative of the standard action.","Using the Keldysh QFT formulation, we show that the standard quantization of such Keldysh spinors results in the Hamiltonian bounded from above and in S-matrix for backwards in time scattering.","The alternative quantization produces the standard Hamiltonian bounded from below and the standard S-matrix for forwards in time scattering.","When the two masses are equal, the sum of their actions describes the standard Keldysh out-of-time-order correlators.","When both Keldysh and Dirac spinors are present in a QFT, the standard Fock space is augmented by negative energy states, while its positive and the negative subspaces share the same vacuum state.","In absence of gravity the Dirac and the Keldysh spinors do not interact and positive and negative energy states do not exchange quanta with non-zero energy.","If Keldysh spinors exist in Nature, then the Universe would consist of bosonic and fermionic particles that are described by both positive and negative Fock energy states with a single vacuum state that cannot be crossed from either side.","Such a scenario could explain the non-observation of dark energy/dark matter except via classical gravity."],"url":"http://arxiv.org/abs/2406.06194v1","category":"hep-th"}
{"created":"2024-06-10 11:37:16","title":"Tuning the water intrinsic permeability of PEGDA hydrogel membranes by adding free PEG chains of varying molar masses","abstract":"We explore the effect of poly (ethylene glycol) (PEG) molar mass on the intrinsic permeability and structural characteristics of poly (ethylene glycol) diacrylate PEGDA/PEG composite hydrogel membranes. We observe that by varying the PEG content and molar mass, we can finely adjust the water intrinsic permeability over several orders of magnitude. Notably, we show the existence of a maximum water intrinsic permeability, already identified in a previous study to be located at the critical overlap concentration C^* of PEG chains, for the highest PEG molar mass studied. Furthermore, we note that the maximum intrinsic permeability follows a non-monotonic evolution with respect to the PEG molar mass and reaches its peak at 35 000 g.mol-1. Besides our results show that a significant fraction of PEG chains is irreversibly trapped within the PEGDA matrix even for the shortest molar masses down to 600 g.mol-1. This observation suggests the possibility of covalent grafting of PEG chains to the PEGDA matrix. CryoSEM and AFM measurements demonstrate the presence of large micron-sized cavities separated by PEGDA-rich walls whose nanometric structure strongly depends on the PEG content. By combining our permeability and structural measurements, we suggest that the PEG chains trapped inside the PEGDA rich walls induce nanoscale defects in the cross linking density, resulting in an increased permeability below C^*. Conversely, above C^*, we speculate that partially-trapped PEG chains may form a brush-like arrangement on the surface of the PEGDA-rich walls, leading to a reduction in permeability. These two opposing effects are anticipated to exhibit molar-mass-dependent trends, contributing to the non-monotonic variation of the maximum intrinsic permeability at C^*. Overall, our results demonstrate the potential to fine-tune the properties of hydrogel membranes, offering new opportunities in separation applications.","sentences":["We explore the effect of poly (ethylene glycol) (PEG) molar mass on the intrinsic permeability and structural characteristics of poly (ethylene glycol) diacrylate PEGDA/PEG composite hydrogel membranes.","We observe that by varying the PEG content and molar mass, we can finely adjust the water intrinsic permeability over several orders of magnitude.","Notably, we show the existence of a maximum water intrinsic permeability, already identified in a previous study to be located at the critical overlap concentration C^* of PEG chains, for the highest PEG molar mass studied.","Furthermore, we note that the maximum intrinsic permeability follows a non-monotonic evolution with respect to the PEG molar mass and reaches its peak at 35 000 g.mol-1.","Besides our results show that a significant fraction of PEG chains is irreversibly trapped within the PEGDA matrix even for the shortest molar masses down to 600 g.mol-1.","This observation suggests the possibility of covalent grafting of PEG chains to the PEGDA matrix.","CryoSEM and AFM measurements demonstrate the presence of large micron-sized cavities separated by PEGDA-rich walls whose nanometric structure strongly depends on the PEG content.","By combining our permeability and structural measurements, we suggest that the PEG chains trapped inside the PEGDA rich walls induce nanoscale defects in the cross linking density, resulting in an increased permeability below C^*.","Conversely, above C^*, we speculate that partially-trapped PEG chains may form a brush-like arrangement on the surface of the PEGDA-rich walls, leading to a reduction in permeability.","These two opposing effects are anticipated to exhibit molar-mass-dependent trends, contributing to the non-monotonic variation of the maximum intrinsic permeability at C^*.","Overall, our results demonstrate the potential to fine-tune the properties of hydrogel membranes, offering new opportunities in separation applications."],"url":"http://arxiv.org/abs/2406.06190v1","category":"cond-mat.soft"}
{"created":"2024-06-10 11:25:24","title":"Upper Bound Estimate of the Electronic Scattering Potential of a Weakly Interacting Molecular Film on a Metal","abstract":"Thin organic films and two-dimensional (2D) molecular assemblies on solid surfaces yield the potential for applications in molecular electronics, optoelectronics, catalysis, and sensing. These applications rely on the intrinsic electronic properties of the hybrid organic/inorganic interface. Here, we investigate the energy dispersion of 2D electronic states at the interface between an atomically thin self-assembled molecular film, comprised of flat, noncovalently bonded 9,10-dicyanoanthracene (DCA) molecules, and a Ag(111) surface. Using Fourier-transformed scanning tunnelling spectroscopy (FT-STS), we determined that the 2D electronic wave functions with wavevectors within ~80% of the first Brillouin zone (BZ) area close to the Gamma-point are free-electron-like, suggesting a weak electronic interaction between the 2D molecular film and the metal surface. Via a perturbative second-order correction to the free electron energy dispersion, we further established an upper bound for the amplitude of the scattering potential resulting from the self-assembled molecular film that the interface electrons are subject to, on the order of 1.5 eV. Our approach allows for quantifying electronic interactions at hybrid 2D interfaces and heterostructures.","sentences":["Thin organic films and two-dimensional (2D) molecular assemblies on solid surfaces yield the potential for applications in molecular electronics, optoelectronics, catalysis, and sensing.","These applications rely on the intrinsic electronic properties of the hybrid organic/inorganic interface.","Here, we investigate the energy dispersion of 2D electronic states at the interface between an atomically thin self-assembled molecular film, comprised of flat, noncovalently bonded 9,10-dicyanoanthracene (DCA) molecules, and a Ag(111) surface.","Using Fourier-transformed scanning tunnelling spectroscopy (FT-STS), we determined that the 2D electronic wave functions with wavevectors within ~80% of the first Brillouin zone (BZ) area close to the Gamma-point are free-electron-like, suggesting a weak electronic interaction between the 2D molecular film and the metal surface.","Via a perturbative second-order correction to the free electron energy dispersion, we further established an upper bound for the amplitude of the scattering potential resulting from the self-assembled molecular film that the interface electrons are subject to, on the order of 1.5 eV. Our approach allows for quantifying electronic interactions at hybrid 2D interfaces and heterostructures."],"url":"http://arxiv.org/abs/2406.06181v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 10:44:08","title":"A path to Balmer's formula: the Pythagorean search for simplicity and harmony","abstract":"A derivation of Balmer's formula is presented, guided by the principles of simplicity and harmony.","sentences":["A derivation of Balmer's formula is presented, guided by the principles of simplicity and harmony."],"url":"http://arxiv.org/abs/2406.06159v1","category":"physics.hist-ph"}
{"created":"2024-06-10 09:15:27","title":"Hamonically trapped inertial run-and-tumble particle in one-dimension","abstract":"We study the nonequilibrium stationary state of a one-dimensional inertial run-and-tumble particle (IRTP) trapped in a harmonic potential. We find that the presence of inertia leads to two distinct dynamical scenarios, namely, overdamped and underdamped, characterized by the relative strength of the viscous and the trap time-scales. We also find that inertial nature of the active dynamics leads to the particle being confined in specific regions of the phase plane in the overdamped and underdamped cases, which we compute analytically. Moreover, the interplay of the inertial and active time-scales gives rise to several sub-regimes, which are characterized by very different behaviour of position and velocity fluctuations of the IRTP. In particular, in the underdamped regime, both the position and velocity undergoes transitions from a novel multi-peaked structure in the strongly active limit to a single peaked Gaussian-like distribution in the passive limit. On the other hand, in the overdamped scenario, the position distribution shows a transition from a U-shape to a dome-shape, as activity is decreased. Interestingly, the velocity distribution in the overdamped scenario shows two transitions -- from a single-peaked shape with an algebraic divergence at the origin in the strongly active regime to a double peaked one in the moderately active regime to a dome-shaped one in the passive regime.","sentences":["We study the nonequilibrium stationary state of a one-dimensional inertial run-and-tumble particle (IRTP) trapped in a harmonic potential.","We find that the presence of inertia leads to two distinct dynamical scenarios, namely, overdamped and underdamped, characterized by the relative strength of the viscous and the trap time-scales.","We also find that inertial nature of the active dynamics leads to the particle being confined in specific regions of the phase plane in the overdamped and underdamped cases, which we compute analytically.","Moreover, the interplay of the inertial and active time-scales gives rise to several sub-regimes, which are characterized by very different behaviour of position and velocity fluctuations of the IRTP.","In particular, in the underdamped regime, both the position and velocity undergoes transitions from a novel multi-peaked structure in the strongly active limit to a single peaked Gaussian-like distribution in the passive limit.","On the other hand, in the overdamped scenario, the position distribution shows a transition from a U-shape to a dome-shape, as activity is decreased.","Interestingly, the velocity distribution in the overdamped scenario shows two transitions -- from a single-peaked shape with an algebraic divergence at the origin in the strongly active regime to a double peaked one in the moderately active regime to a dome-shaped one in the passive regime."],"url":"http://arxiv.org/abs/2406.06120v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-10 08:59:52","title":"Exploring alignment phenomenon through the transverse momentum disbalance","abstract":"The hypothesis of the relation between the observed alignment of spots in the X-ray films in cosmic ray emulsion experiments and the selection procedure of the highest-energy particles itself together with the transverse momentum conservation is tested in the framework of the HYDJET++ model. It is shown that the high degree of alignment can appear at the reasonable values of transverse momentum disbalance of selected most energetic particles.","sentences":["The hypothesis of the relation between the observed alignment of spots in the X-ray films in cosmic ray emulsion experiments and the selection procedure of the highest-energy particles itself together with the transverse momentum conservation is tested in the framework of the HYDJET++ model.","It is shown that the high degree of alignment can appear at the reasonable values of transverse momentum disbalance of selected most energetic particles."],"url":"http://arxiv.org/abs/2406.06114v1","category":"hep-ph"}
{"created":"2024-06-10 08:13:23","title":"Fiducial-Cosmology-dependent systematics for the DESI 2024 BAO Analysis","abstract":"When measuring the Baryon Acoustic Oscillations (BAO) scale from galaxy surveys, one typically assumes a fiducial cosmology when converting redshift measurements into comoving distances and also when defining input parameters for the reconstruction algorithm. A parameterised template for the model to be fitted is also created based on a (possibly different) fiducial cosmology. This model reliance can be considered a form of data compression, and the data is then analysed allowing that the true answer is different from the fiducial cosmology assumed. In this study, we evaluate the impact of the fiducial cosmology assumed in the BAO analysis of the Dark Energy Spectroscopic Instrument (DESI) survey Data Release 1 (DR1) on the final measurements in DESI 2024 III. We utilise a suite of mock galaxy catalogues with survey realism that mirrors the DESI DR1 tracers: the bright galaxy sample (BGS), the luminous red galaxies (LRG), the emission line galaxies (ELG) and the quasars (QSO), spanning a redshift range from 0.1 to 2.1. We compare the four secondary AbacusSummit cosmologies against DESI's fiducial cosmology (Planck 2018). The secondary cosmologies explored include a lower cold dark matter density, a thawing dark energy universe, a higher number of effective species, and a lower amplitude of matter clustering. The mocks are processed through the BAO pipeline by consistently iterating the grid, template, and reconstruction reference cosmologies. We determine a conservative systematic contribution to the error of $0.1\\%$ for both the isotropic and anisotropic dilation parameters $\\alpha_{\\rm iso}$ and $\\alpha_{\\rm AP}$. We then directly test the impact of the fiducial cosmology on DESI DR1 data.","sentences":["When measuring the Baryon Acoustic Oscillations (BAO) scale from galaxy surveys, one typically assumes a fiducial cosmology when converting redshift measurements into comoving distances and also when defining input parameters for the reconstruction algorithm.","A parameterised template for the model to be fitted is also created based on a (possibly different) fiducial cosmology.","This model reliance can be considered a form of data compression, and the data is then analysed allowing that the true answer is different from the fiducial cosmology assumed.","In this study, we evaluate the impact of the fiducial cosmology assumed in the BAO analysis of the Dark Energy Spectroscopic Instrument (DESI) survey Data Release 1 (DR1) on the final measurements in DESI 2024 III.","We utilise a suite of mock galaxy catalogues with survey realism that mirrors the DESI DR1 tracers: the bright galaxy sample (BGS), the luminous red galaxies (LRG), the emission line galaxies (ELG) and the quasars (QSO), spanning a redshift range from 0.1 to 2.1.","We compare the four secondary AbacusSummit cosmologies against DESI's fiducial cosmology (Planck 2018).","The secondary cosmologies explored include a lower cold dark matter density, a thawing dark energy universe, a higher number of effective species, and a lower amplitude of matter clustering.","The mocks are processed through the BAO pipeline by consistently iterating the grid, template, and reconstruction reference cosmologies.","We determine a conservative systematic contribution to the error of $0.1\\%$ for both the isotropic and anisotropic dilation parameters $\\alpha_{\\rm iso}$ and $\\alpha_{\\rm AP}$. We then directly test the impact of the fiducial cosmology on DESI DR1 data."],"url":"http://arxiv.org/abs/2406.06085v1","category":"astro-ph.CO"}
{"created":"2024-06-10 07:03:31","title":"The matrix model of two-color one-flavor QCD: The ultra-strong coupling regime","abstract":"Using variational methods, we numerically investigate the matrix model for the two-color QCD coupled to a single quark (matrix-QCD$_{2,1}$) in the limit of ultra-strong Yang-Mills coupling ($g =\\infty$). The spectrum of the model has superselection sectors labelled by baryon number $B$ and spin $J$. We study sectors with $B=0,1,2$ and $J=0,1$, which may be organized as mesons, (anti-)diquarks and (anti-)tetraquarks. For each of these sectors, we study the properties of the respective ground states in both chiral and heavy quark limits, and uncover a rich quantum phase transition (QPT) structure. We also investigate the division of the total spin between the glue and the quark and show that glue contribution is significant for several of these sectors. For the $(B,J)=(0,0)$ sector, we find that the dominant glue contribution to the ground state comes from reducible connections. Finally, in the presence of non-trivial baryon chemical potential $\\mu$, we construct the phase diagram of the model. For sufficiently large $\\mu$, we find that the ground state of the theory may have non-zero spin.","sentences":["Using variational methods, we numerically investigate the matrix model for the two-color QCD coupled to a single quark (matrix-QCD$_{2,1}$) in the limit of ultra-strong Yang-Mills coupling ($g =\\infty$).","The spectrum of the model has superselection sectors labelled by baryon number $B$ and spin $J$. We study sectors with $B=0,1,2$ and $J=0,1$, which may be organized as mesons, (anti-)diquarks and (anti-)tetraquarks.","For each of these sectors, we study the properties of the respective ground states in both chiral and heavy quark limits, and uncover a rich quantum phase transition (QPT) structure.","We also investigate the division of the total spin between the glue and the quark and show that glue contribution is significant for several of these sectors.","For the $(B,J)=(0,0)$ sector, we find that the dominant glue contribution to the ground state comes from reducible connections.","Finally, in the presence of non-trivial baryon chemical potential $\\mu$, we construct the phase diagram of the model.","For sufficiently large $\\mu$, we find that the ground state of the theory may have non-zero spin."],"url":"http://arxiv.org/abs/2406.06055v1","category":"hep-th"}
{"created":"2024-06-10 03:10:31","title":"Using $\u039b_b^0(6146)$ and $\u039b_b^0(6152)$ as probes to investigate possible $\\bar{B}^{*}N$ and $\\bar{D}^{*}N$ molecules","abstract":"Heavy quark symmetry can help us identify the internal structure of hadrons and predict new particles. In this study, we examine the strong decay modes of the observed $\\Lambda_b^0(6146)$ and $\\Lambda_b^0(6152)$, assuming these two states are molecular states primarily composed of $\\bar{B}^{*}N$ component. The partial decay widths of the $\\bar{B}^{*}N$ molecular state into the $\\pi\\Sigma_b$ and $\\pi\\Sigma_b^{*}$ final states through hadronic loops are calculated using effective Lagrangians. Our results, when compared with LHCb observations, support the interpretation of $\\Lambda_b^0(6146)$ as a molecule primarily composed of $\\bar{B}^{*}N$ components. However, the decay width of $\\Lambda_b^0(6152)$ cannot be accurately reproduced within the molecular state framework. Based on the above results and heavy quark symmetry, we predict the existence of $\\bar{B}^{*}N$ molecular states with $J^p=5/2^{+}$, which are the heavy quark spin symmetry partners of $\\Lambda_b(6146)$, with masses in the range of 6195-6200 MeV. And the main decay is $\\pi\\Sigma_b^{*}$ channel. Moreover, there must existence of a $\\bar{D}^{*}N$ molecule with $J^p=3/2^{+}$, possible corresponding to the experimentally observed $\\Lambda_c(2860)^{+}$. If $\\Lambda_c(2880)^{+}$ is indeed the heavy quark flavor symmetry partner of $\\Lambda_b(6152)$, it would exhibit a conventional three-quark structure. Therefore, we also propose the search for a $\\bar{D}^{*}N$ molecule with a spin-parity of $J^p=5/2^{+}$, which would be the heavy-quark spin partner state of $\\Lambda_c(2860)^{+}$. It should be noted that these baryons may be mixed states, containing both molecular and three-quark components. These results can aid experiments in exploring the internal structure of these baryons.","sentences":["Heavy quark symmetry can help us identify the internal structure of hadrons and predict new particles.","In this study, we examine the strong decay modes of the observed $\\Lambda_b^0(6146)$ and $\\Lambda_b^0(6152)$, assuming these two states are molecular states primarily composed of $\\bar{B}^{*}N$ component.","The partial decay widths of the $\\bar{B}^{*}N$ molecular state into the $\\pi\\Sigma_b$ and $\\pi\\Sigma_b^{*}$ final states through hadronic loops are calculated using effective Lagrangians.","Our results, when compared with LHCb observations, support the interpretation of $\\Lambda_b^0(6146)$ as a molecule primarily composed of $\\bar{B}^{*}N$ components.","However, the decay width of $\\Lambda_b^0(6152)$ cannot be accurately reproduced within the molecular state framework.","Based on the above results and heavy quark symmetry, we predict the existence of $\\bar{B}^{*}N$ molecular states with $J^p=5/2^{+}$, which are the heavy quark spin symmetry partners of $\\Lambda_b(6146)$, with masses in the range of 6195-6200 MeV.","And the main decay is $\\pi\\Sigma_b^{*}$ channel.","Moreover, there must existence of a $\\bar{D}^{*}N$ molecule with $J^p=3/2^{+}$, possible corresponding to the experimentally observed $\\Lambda_c(2860)^{+}$. If $\\Lambda_c(2880)^{+}$ is indeed the heavy quark flavor symmetry partner of $\\Lambda_b(6152)$, it would exhibit a conventional three-quark structure.","Therefore, we also propose the search for a $\\bar{D}^{*}N$ molecule with a spin-parity of $J^p=5/2^{+}$, which would be the heavy-quark spin partner state of $\\Lambda_c(2860)^{+}$. It should be noted that these baryons may be mixed states, containing both molecular and three-quark components.","These results can aid experiments in exploring the internal structure of these baryons."],"url":"http://arxiv.org/abs/2406.05991v1","category":"hep-ph"}
{"created":"2024-06-10 03:08:24","title":"Enhancing Crustal Velocity Structure in Sedimentary Basin by joint inversion of Teleseismic P-Wave Reverberations and Surface Wave Dispersion","abstract":"Accurately determining the crustal velocity structure within sedimentary basins is crucial for enhancing energy resource evaluation and seismic hazard assessment. Traditional crustal imaging is challenging due to the interference of teleseismic P-wave reverberations (TPR). To address this issue, we propose an inversion strategy that combines multi-frequency TPR and surface wave dispersion (SWD) to constrain the crustal structure. Both theoretical simulations and real data tests from two stations in the Songliao Basin validate our approach. This method shows great promise for improving crustal structure investigations in complex environments, such as sedimentary basins and oceanic regions.","sentences":["Accurately determining the crustal velocity structure within sedimentary basins is crucial for enhancing energy resource evaluation and seismic hazard assessment.","Traditional crustal imaging is challenging due to the interference of teleseismic P-wave reverberations (TPR).","To address this issue, we propose an inversion strategy that combines multi-frequency TPR and surface wave dispersion (SWD) to constrain the crustal structure.","Both theoretical simulations and real data tests from two stations in the Songliao Basin validate our approach.","This method shows great promise for improving crustal structure investigations in complex environments, such as sedimentary basins and oceanic regions."],"url":"http://arxiv.org/abs/2406.05989v1","category":"physics.geo-ph"}
{"created":"2024-06-10 02:28:23","title":"Dynamic Virtual Power Plants With Frequency Regulation Capacity","abstract":"For integrating heterogeneous distributed energy resources to provide fast frequency regulation, this paper proposes a dynamic virtual power plant~(DVPP) with frequency regulation capacity. A parameter anonymity-based approach is established for DVPP aggregating small-scaled inverter-based resources~(IBRs) with privacy concerns. On this basis, a parameter-to-performance mapping is formulated to evaluate how control coefficients impact the DVPP-level power overshoot as well as the IBR-level costs. The objective is to design the best way to provide the frequency response with minimal impacts on grid and the most financial gains. Numerical experiments illustrate the effectiveness of the proposed approach and further analysis validates that our models are able to take dead bands into consideration.","sentences":["For integrating heterogeneous distributed energy resources to provide fast frequency regulation, this paper proposes a dynamic virtual power plant~(DVPP) with frequency regulation capacity.","A parameter anonymity-based approach is established for DVPP aggregating small-scaled inverter-based resources~(IBRs) with privacy concerns.","On this basis, a parameter-to-performance mapping is formulated to evaluate how control coefficients impact the DVPP-level power overshoot as well as the IBR-level costs.","The objective is to design the best way to provide the frequency response with minimal impacts on grid and the most financial gains.","Numerical experiments illustrate the effectiveness of the proposed approach and further analysis validates that our models are able to take dead bands into consideration."],"url":"http://arxiv.org/abs/2406.05976v1","category":"eess.SY"}
{"created":"2024-06-10 01:06:01","title":"Open-Vocabulary Part-Based Grasping","abstract":"Many robotic applications require to grasp objects not arbitrarily but at a very specific object part. This is especially important for manipulation tasks beyond simple pick-and-place scenarios or in robot-human interactions, such as object handovers. We propose AnyPart, a practical system that combines open-vocabulary object detection, open-vocabulary part segmentation and 6DOF grasp pose prediction to infer a grasp pose on a specific part of an object in 800 milliseconds. We contribute two new datasets for the task of open-vocabulary part-based grasping, a hand-segmented dataset containing 1014 object-part segmentations, and a dataset of real-world scenarios gathered during our robot trials for individual objects and table-clearing tasks. We evaluate AnyPart on a mobile manipulator robot using a set of 28 common household objects over 360 grasping trials. AnyPart is capable of producing successful grasps 69.52 %, when ignoring robot-based grasp failures, AnyPart predicts a grasp location on the correct part 88.57 % of the time.","sentences":["Many robotic applications require to grasp objects not arbitrarily but at a very specific object part.","This is especially important for manipulation tasks beyond simple pick-and-place scenarios or in robot-human interactions, such as object handovers.","We propose AnyPart, a practical system that combines open-vocabulary object detection, open-vocabulary part segmentation and 6DOF grasp pose prediction to infer a grasp pose on a specific part of an object in 800 milliseconds.","We contribute two new datasets for the task of open-vocabulary part-based grasping, a hand-segmented dataset containing 1014 object-part segmentations, and a dataset of real-world scenarios gathered during our robot trials for individual objects and table-clearing tasks.","We evaluate AnyPart on a mobile manipulator robot using a set of 28 common household objects over 360 grasping trials.","AnyPart is capable of producing successful grasps 69.52 %, when ignoring robot-based grasp failures, AnyPart predicts a grasp location on the correct part 88.57 % of the time."],"url":"http://arxiv.org/abs/2406.05951v1","category":"cs.RO"}
{"created":"2024-06-10 00:24:49","title":"Embedding Network Autoregression for time series analysis and causal peer effect inference","abstract":"We propose an Embedding Network Autoregressive Model (ENAR) for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related problems, (1) modeling and predicting multivariate time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent factors from the observed network adjacency matrix either through spectral embedding or maximum likelihood estimation, followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normal in asymptotic setups with a growing number of network vertices N while including a growing number of time points T and finite T cases. We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models.","sentences":["We propose an Embedding Network Autoregressive Model (ENAR) for multivariate networked longitudinal data.","We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects.","This approach takes a unified view of two related problems, (1) modeling and predicting multivariate time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data.","Our estimation strategy comprises estimating latent factors from the observed network adjacency matrix either through spectral embedding or maximum likelihood estimation, followed by least squares estimation of the network autoregressive model.","We show that the estimated momentum and peer effect parameters are consistent and asymptotically normal in asymptotic setups with a growing number of network vertices N while including a growing number of time points T and finite T cases.","We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models."],"url":"http://arxiv.org/abs/2406.05944v1","category":"stat.ME"}
{"created":"2024-06-10 00:03:47","title":"Revisiting the magnetic responses of bilayer graphene from the perspective of the quantum distance","abstract":"We study the influence of the quantum geometry on the magnetic responses of quadratic band crossing semimetals. More explicitly, we examine the Landau levels, quantum Hall effect, and magnetic susceptibility of a general two-band Hamiltonian that has fixed isotropic quadratic band dispersion but with tunable quantum geometry, in which the interband coupling is fully characterized by the maximum quantum distance $d_\\mathrm{max}$. By continuously tuning $d_\\mathrm{max}$ in the range of $0\\leq d_\\mathrm{max}\\leq 1$, we investigate how the magnetic properties of the free electron model with $d_\\mathrm{max}=0$ evolve into those of the bilayer graphene with $d_\\mathrm{max}=1$. We demonstrate that despite sharing the same energy dispersion $\\epsilon(p) =\\pm\\frac{p^2}{2m}$, the charge carriers in the free electron model and bilayer graphene exhibit entirely distinct Landau levels and quantum Hall responses due to the nontrivial quantum geometry of the wave functions.","sentences":["We study the influence of the quantum geometry on the magnetic responses of quadratic band crossing semimetals.","More explicitly, we examine the Landau levels, quantum Hall effect, and magnetic susceptibility of a general two-band Hamiltonian that has fixed isotropic quadratic band dispersion but with tunable quantum geometry, in which the interband coupling is fully characterized by the maximum quantum distance $d_\\mathrm{max}$. By continuously tuning $d_\\mathrm{max}$ in the range of $0\\leq d_\\mathrm{max}\\leq 1$, we investigate how the magnetic properties of the free electron model with $d_\\mathrm{max}=0$ evolve into those of the bilayer graphene with $d_\\mathrm{max}=1$. We demonstrate that despite sharing the same energy dispersion $\\epsilon(p) =\\pm\\frac{p^2}{2m}$, the charge carriers in the free electron model and bilayer graphene exhibit entirely distinct Landau levels and quantum Hall responses due to the nontrivial quantum geometry of the wave functions."],"url":"http://arxiv.org/abs/2406.05939v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-09 21:25:09","title":"Possible mixing of a diquark-antidiquark with a $p \\bar p$ hadronic molecule","abstract":"We discuss the possibility that the two nearby resonances observed by BESIII partially below the \\,$p\\bar p$\\, threshold might be due to mixing between two metastable states with the same $J^{PC}=0^{-+}$ quantum numbers, but rather different internal structure. One is a $p \\bar p$ hadronic molecule and the other a bound state of a light-quark diquark and an antidiquark, both with spin 1 and isospin 0, a composite color antitriplet and triplet, respectively. The doubling of resonances, one of which may be interpreted as a hadronic molecule, while the other arises from $q \\bar q$ annihilation in a state with vacuum quantum numbers may be a more general feature than the specific case considered here.","sentences":["We discuss the possibility that the two nearby resonances observed by BESIII partially below the \\,$p\\bar p$\\, threshold might be due to mixing between two metastable states with the same $J^{PC}=0^{-+}$ quantum numbers, but rather different internal structure.","One is a $p \\bar p$ hadronic molecule and the other a bound state of a light-quark diquark and an antidiquark, both with spin 1 and isospin 0, a composite color antitriplet and triplet, respectively.","The doubling of resonances, one of which may be interpreted as a hadronic molecule, while the other arises from $q \\bar q$ annihilation in a state with vacuum quantum numbers may be a more general feature than the specific case considered here."],"url":"http://arxiv.org/abs/2406.05920v1","category":"hep-ph"}
{"created":"2024-06-09 19:43:41","title":"On Kolmogorov Structure Functions","abstract":"All strings with low mutual information with the halting sequence will have flat Kolmogorov Structure Functions, in the context of Algorithmic Statistics. Assuming the Independence Postulate, strings with non-negligible information with the halting sequence are purely mathematical constructions, and cannot be found in nature. Thus Algorithmic Statistics does not study strings in the physical world. This leads to the general thesis that two part codes require limitations as shown in the Minimum Description Length Principle. We also discuss issues with set-restricted Kolmogorov Structure Functions.","sentences":["All strings with low mutual information with the halting sequence will have flat Kolmogorov Structure Functions, in the context of Algorithmic Statistics.","Assuming the Independence Postulate, strings with non-negligible information with the halting sequence are purely mathematical constructions, and cannot be found in nature.","Thus Algorithmic Statistics does not study strings in the physical world.","This leads to the general thesis that two part codes require limitations as shown in the Minimum Description Length Principle.","We also discuss issues with set-restricted Kolmogorov Structure Functions."],"url":"http://arxiv.org/abs/2406.05903v1","category":"cs.CC"}
{"created":"2024-06-09 19:37:41","title":"On the singular position-dependent mass","abstract":"Revisiting the issue associated with Position-Dependent Mass (PDM), we reaffirm that the appropriate framework for addressing a generic PDM is the symmetrization proposed by BenDaniel and Duke. To accomplish this result adopts the effective mass Hamiltonian proposed by von Roos, corrected by a symmetrized kinematic term. After verifying the appropriate ordering to approach the PDM issue, one investigates a crystalline lattice with a defect described by a singular PDM. The singular mass profile proves intriguing as it yields an atom's cluster in the neighborhood of the singularity. Considering that a restoring force acts on the atoms, one notes that the confluent Heun function describes the quantum states. Furthermore, one highlights that when the effective mass distribution tends to a constant profile, we recover a system similar to the harmonic oscillator.","sentences":["Revisiting the issue associated with Position-Dependent Mass (PDM), we reaffirm that the appropriate framework for addressing a generic PDM is the symmetrization proposed by BenDaniel and Duke.","To accomplish this result adopts the effective mass Hamiltonian proposed by von Roos, corrected by a symmetrized kinematic term.","After verifying the appropriate ordering to approach the PDM issue, one investigates a crystalline lattice with a defect described by a singular PDM.","The singular mass profile proves intriguing as it yields an atom's cluster in the neighborhood of the singularity.","Considering that a restoring force acts on the atoms, one notes that the confluent Heun function describes the quantum states.","Furthermore, one highlights that when the effective mass distribution tends to a constant profile, we recover a system similar to the harmonic oscillator."],"url":"http://arxiv.org/abs/2406.05899v1","category":"quant-ph"}
{"created":"2024-06-09 18:39:57","title":"Long Range Azimuthal Correlation, Entanglement and Bell Inequality Violation by Spinning Gluons at the LHC","abstract":"We apply the recently developed concept of the nucleon energy-energy correlator (NEEC) for the gluon sector to investigate the long-range azimuthal angular correlations in proton-proton collisions at the LHC. The spinning gluon in these collisions will introduce a significant nonzero $\\cos(2\\phi)$ asymmetries in both Higgs Boson and top quark pair productions. The genesis of the $\\cos(2\\phi)$ correlation lies in the intricate quantum entanglement. Owing to the substantial $\\cos(2\\phi)$ effect, the NEEC observable in Higgs Boson and $t{\\bar t}$ production emerges as a pivotal avenue for delving into quantum entanglement and scrutinizing the Bell inequality at high-energy colliders.","sentences":["We apply the recently developed concept of the nucleon energy-energy correlator (NEEC) for the gluon sector to investigate the long-range azimuthal angular correlations in proton-proton collisions at the LHC.","The spinning gluon in these collisions will introduce a significant nonzero $\\cos(2\\phi)$ asymmetries in both Higgs Boson and top quark pair productions.","The genesis of the $\\cos(2\\phi)$ correlation lies in the intricate quantum entanglement.","Owing to the substantial $\\cos(2\\phi)$ effect, the NEEC observable in Higgs Boson and $t{\\bar t}$ production emerges as a pivotal avenue for delving into quantum entanglement and scrutinizing the Bell inequality at high-energy colliders."],"url":"http://arxiv.org/abs/2406.05880v1","category":"hep-ph"}
{"created":"2024-06-09 17:04:27","title":"From First-order to Higher-order Interactions: Enhanced Representation of Homotopic Functional Connectivity through Control of Intervening Variables","abstract":"The brain's complex functionality emerges from network interactions that go beyond dyadic connections, with higher-order interactions significantly contributing to this complexity. One method of capturing higher-order interactions is through traversing the brain network using random walks. The efficacy of these random walks depends on the defined mutual interactions between two brain entities. More precise capture of higher-order interactions enables a better reflection of the brain's intrinsic neurophysiological characteristics. One well-established neurophysiological concept is Homotopic Functional Connectivity (HoFC), which illustrates the synchronized spontaneous activity between corresponding regions in the brain's left and right hemispheres. We employ node2vec, a random walk node embedding approach, alongside resting-state fMRI from the Human Connectome Project (HCP) to obtain higher-order feature vectors. We assess the efficacy of different functional connectivity parameterizations using HoFC. The results indicates that the quality of capturing higher-order interactions largely depends on the statistical dependency measure between brain regions. Higher-order interactions defined by partial correlation, better reflects HoFC compare to other statistical associations. In this case of first-order interactions, tangent space embedding more effectively demonstrates HoFC. The findings validate HoFC and underscore the importance of functional connectivity construction method in capturing intrinsic characteristics of the human brain.","sentences":["The brain's complex functionality emerges from network interactions that go beyond dyadic connections, with higher-order interactions significantly contributing to this complexity.","One method of capturing higher-order interactions is through traversing the brain network using random walks.","The efficacy of these random walks depends on the defined mutual interactions between two brain entities.","More precise capture of higher-order interactions enables a better reflection of the brain's intrinsic neurophysiological characteristics.","One well-established neurophysiological concept is Homotopic Functional Connectivity (HoFC), which illustrates the synchronized spontaneous activity between corresponding regions in the brain's left and right hemispheres.","We employ node2vec, a random walk node embedding approach, alongside resting-state fMRI from the Human Connectome Project (HCP) to obtain higher-order feature vectors.","We assess the efficacy of different functional connectivity parameterizations using HoFC.","The results indicates that the quality of capturing higher-order interactions largely depends on the statistical dependency measure between brain regions.","Higher-order interactions defined by partial correlation, better reflects HoFC compare to other statistical associations.","In this case of first-order interactions, tangent space embedding more effectively demonstrates HoFC.","The findings validate HoFC and underscore the importance of functional connectivity construction method in capturing intrinsic characteristics of the human brain."],"url":"http://arxiv.org/abs/2406.05859v1","category":"q-bio.NC"}
{"created":"2024-06-09 16:43:43","title":"Nonlinear Interactions of Planetary-Scale Waves in Mesospheric Winds Observed at 52\u00b0N Latitude and Two Longitudes","abstract":"Nine years of mesospheric wind data from two meteor radars at 52{\\deg}N latitude were analyzed to investigate planetary waves (PWs) and tides by estimating their zonal wavenumber through longitudinal phase differences. Our results reveal that PW normal modes (NMs) primarily drive multi-day oscillations, showing seasonal variability and statistical associations with Sudden Stratospheric Warming (SSW) events. Specifically, a significant 6-day NM emerges in April, followed by predominant 4- and 2-day NMs until June, with peaks of 2-, 4-, and 6-day NMs spanning July to October. Furthermore, our study provides the first observational verification of frequency and zonal wavenumber of over ten secondary waves from nonlinear interactions among planetary-scale waves. One notable finding is the prevalence of non-migrating components in winter 24-hour and summer 8-hour tides, attributed to these nonlinear interactions. Our findings underscore the diverse nonlinear dynamics of planetary-scale waves, triggering a variety of periodic oscillations.","sentences":["Nine years of mesospheric wind data from two meteor radars at 52{\\deg}N latitude were analyzed to investigate planetary waves (PWs) and tides by estimating their zonal wavenumber through longitudinal phase differences.","Our results reveal that PW normal modes (NMs) primarily drive multi-day oscillations, showing seasonal variability and statistical associations with Sudden Stratospheric Warming (SSW) events.","Specifically, a significant 6-day NM emerges in April, followed by predominant 4- and 2-day NMs until June, with peaks of 2-, 4-, and 6-day NMs spanning July to October.","Furthermore, our study provides the first observational verification of frequency and zonal wavenumber of over ten secondary waves from nonlinear interactions among planetary-scale waves.","One notable finding is the prevalence of non-migrating components in winter 24-hour and summer 8-hour tides, attributed to these nonlinear interactions.","Our findings underscore the diverse nonlinear dynamics of planetary-scale waves, triggering a variety of periodic oscillations."],"url":"http://arxiv.org/abs/2406.05848v1","category":"physics.space-ph"}
{"created":"2024-06-09 15:57:29","title":"Bubbles kick off primordial black holes to form more binaries","abstract":"Primordial black holes (PBHs) may form before cosmological first-order phase transitions, leading to inevitable collisions between PBHs and bubble walls. In this Letter, we have simulated for the first time the co-evolution of an expanding scalar wall passing through a black hole with full numerical relativity. This black hole-bubble wall collision yields multiple far-reaching phenomena including the PBH mass growth, gravitational wave radiations, and momentum recoil that endows PBHs with additional velocities, approximately doubling the formation rate for PBH binaries and hence strengthening the observational constraints on the PBH abundances.","sentences":["Primordial black holes (PBHs) may form before cosmological first-order phase transitions, leading to inevitable collisions between PBHs and bubble walls.","In this Letter, we have simulated for the first time the co-evolution of an expanding scalar wall passing through a black hole with full numerical relativity.","This black hole-bubble wall collision yields multiple far-reaching phenomena including the PBH mass growth, gravitational wave radiations, and momentum recoil that endows PBHs with additional velocities, approximately doubling the formation rate for PBH binaries and hence strengthening the observational constraints on the PBH abundances."],"url":"http://arxiv.org/abs/2406.05838v1","category":"gr-qc"}
{"created":"2024-06-09 15:50:35","title":"Improving Antibody Design with Force-Guided Sampling in Diffusion Models","abstract":"Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses. The design of these CDRs determines the antibody's affinity and specificity towards its target. Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions. However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge. Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces. Integrating this foundational information into diffusion models is, therefore, highly desirable. Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback. Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions. Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies.","sentences":["Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses.","The design of these CDRs determines the antibody's affinity and specificity towards its target.","Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions.","However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge.","Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces.","Integrating this foundational information into diffusion models is, therefore, highly desirable.","Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback.","Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions.","Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies."],"url":"http://arxiv.org/abs/2406.05832v1","category":"q-bio.QM"}
{"created":"2024-06-09 14:58:53","title":"Wearable Healthcare Devices for Monitoring Stress and Attention Level in Workplace Environments","abstract":"Wearable devices have revolutionized healthcare monitoring, allowing us to track physiological conditions without disrupting daily routines. Whereas monitoring physical health and physical activities have been widely studied, their application and impact on mental health are significantly understudied. This work reviews the state-of-the-art, focusing on stress and concentration levels. These two can play an important role in workplace humanization. For instance, they can guide breaks in high-pressure workplaces, indicating when and how long to take. Those are important to avoid overwork and burn-out, harming employees and employers. To this end, it is necessary to study which sensors can accurately determine stress and attention levels, considering that they should not interfere with their activities and be comfortable to wear. From the software point of view, it is helpful to know the capabilities and performance of various algorithms, especially for uncontrolled workplace environments. This work aims to research, review, and compare commercially available non-intrusive measurement devices, which can be worn during the day and possibly integrated with healthcare systems for stress and concentration assessment. We analyze the performance of various algorithms used for stress and concentration level assessment and discuss future paths for reliable detection of these two parameters.","sentences":["Wearable devices have revolutionized healthcare monitoring, allowing us to track physiological conditions without disrupting daily routines.","Whereas monitoring physical health and physical activities have been widely studied, their application and impact on mental health are significantly understudied.","This work reviews the state-of-the-art, focusing on stress and concentration levels.","These two can play an important role in workplace humanization.","For instance, they can guide breaks in high-pressure workplaces, indicating when and how long to take.","Those are important to avoid overwork and burn-out, harming employees and employers.","To this end, it is necessary to study which sensors can accurately determine stress and attention levels, considering that they should not interfere with their activities and be comfortable to wear.","From the software point of view, it is helpful to know the capabilities and performance of various algorithms, especially for uncontrolled workplace environments.","This work aims to research, review, and compare commercially available non-intrusive measurement devices, which can be worn during the day and possibly integrated with healthcare systems for stress and concentration assessment.","We analyze the performance of various algorithms used for stress and concentration level assessment and discuss future paths for reliable detection of these two parameters."],"url":"http://arxiv.org/abs/2406.05813v1","category":"cs.HC"}
