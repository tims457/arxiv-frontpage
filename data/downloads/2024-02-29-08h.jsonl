{"created":"2024-02-28 18:59:34","title":"Measuring Tracers of Planet Formation in the Atmosphere of WASP-77A b: Sub-stellar O/H and C/H ratios, with a stellar C/O ratio and a potentially Super-stellar Ti/H ratio","abstract":"We present a comprehensive atmospheric retrieval study of the hot Jupiter WASP-77A\\,b using eclipse observations from the Hubble Space Telescope (HST) and JWST. Using atmospheric retrievals, the spectral features of H$_2$O, CO, and TiO are identified, with volume mixing ratios estimated at log$_{\\rm 10}$(VMR) = -4.40$^{+0.14}_{-0.11}$, -4.44$^{+0.34}_{-0.28}$, and -6.40$^{+0.22}_{-0.23}$, respectively. We derive the atmospheric carbon-to-oxygen ratio -- a key planetary formation tracer -- to be C/O = 0.54$\\pm$0.12, which is consistent with both the stellar host value and previous studies of the planet's atmosphere, suggesting a relatively close-in formation. Computing other elemental ratios (i.e., C/H, O/H, and Ti/H), we conclude that the general enrichment of the atmosphere (i.e., metallicity) is sub-stellar, is depleted in C and O, but that Ti appears slightly super-stellar. A low C and O content could be obtained, in combination with a stellar C/O ratio, if the planet formed outside of the CO$_2$ snow line before migrating inwards. Meanwhile, a super-stellar Ti/H could be obtained by late contamination from refractory rich planetesimals. While broadly in agreement with previous works, we do find some differences and discuss these while also highlighting the need for homogeneous analyses when comparative exoplanetology is conducted.","sentences":["We present a comprehensive atmospheric retrieval study of the hot Jupiter WASP-77A\\,b using eclipse observations from the Hubble Space Telescope (HST) and JWST.","Using atmospheric retrievals, the spectral features of H$_2$O, CO, and TiO are identified, with volume mixing ratios estimated at log$_{\\rm 10}$(VMR) = -4.40$^{+0.14}_{-0.11}$, -4.44$^{+0.34}_{-0.28}$, and -6.40$^{+0.22}_{-0.23}$, respectively.","We derive the atmospheric carbon-to-oxygen ratio -- a key planetary formation tracer -- to be C/O = 0.54$\\pm$0.12, which is consistent with both the stellar host value and previous studies of the planet's atmosphere, suggesting a relatively close-in formation.","Computing other elemental ratios (i.e., C/H, O/H, and Ti/H), we conclude that the general enrichment of the atmosphere (i.e., metallicity) is sub-stellar, is depleted in C and O, but that Ti appears slightly super-stellar.","A low C and O content could be obtained, in combination with a stellar C/O ratio, if the planet formed outside of the CO$_2$ snow line before migrating inwards.","Meanwhile, a super-stellar Ti/H could be obtained by late contamination from refractory rich planetesimals.","While broadly in agreement with previous works, we do find some differences and discuss these while also highlighting the need for homogeneous analyses when comparative exoplanetology is conducted."],"url":"http://arxiv.org/abs/2402.18574v1","category":"astro-ph.EP"}
{"created":"2024-02-28 18:59:31","title":"UniMODE: Unified Monocular 3D Object Detection","abstract":"Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection.","sentences":["Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation.","However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions.","To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors.","Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges.","Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains.","Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection."],"url":"http://arxiv.org/abs/2402.18573v1","category":"cs.CV"}
{"created":"2024-02-28 18:58:25","title":"Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards","abstract":"Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).","sentences":["Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs.","While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications.","To address this limitation, we introduce the Directional Preference Alignment (DPA) framework.","Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles.","Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control.","Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2.","This method enjoys a better performance trade-off across various reward objectives.","In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity).","We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B.","Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO)."],"url":"http://arxiv.org/abs/2402.18571v1","category":"cs.LG"}
{"created":"2024-02-28 18:58:18","title":"Negative Temperature Pressure in Black Holes","abstract":"The concept of negative temperature (T < 0) is unique to quantum physics and describes systems that are hotter than any positive temperature system. For decades negative temperatures have been shown in a number of spin systems, but experiments only recently demonstrated atomic ensembles with negative temperatures in their motional degrees of freedom. An observed behavior of such negative temperature ensembles is that despite highly attractive forces between an arbitrary number of particles, there is a self-stabilization against collapse. Negative temperatures are only possible in quantum systems because there exists upper bounds on the energy of particles -- a property not found in classical physics. Here we consider whether event horizons set up similar upper limits within black holes, giving rise to negative temperature systems just within event horizons. Combining black hole thermodynamics with experimentally observed negative temperature effects could imply a quantum-based outward pressure in black holes.","sentences":["The concept of negative temperature (T < 0) is unique to quantum physics and describes systems that are hotter than any positive temperature system.","For decades negative temperatures have been shown in a number of spin systems, but experiments only recently demonstrated atomic ensembles with negative temperatures in their motional degrees of freedom.","An observed behavior of such negative temperature ensembles is that despite highly attractive forces between an arbitrary number of particles, there is a self-stabilization against collapse.","Negative temperatures are only possible in quantum systems because there exists upper bounds on the energy of particles -- a property not found in classical physics.","Here we consider whether event horizons set up similar upper limits within black holes, giving rise to negative temperature systems just within event horizons.","Combining black hole thermodynamics with experimentally observed negative temperature effects could imply a quantum-based outward pressure in black holes."],"url":"http://arxiv.org/abs/2402.18570v1","category":"gr-qc"}
{"created":"2024-02-28 18:58:02","title":"A New Probe of Cosmic Birefringence Using Galaxy Polarization and Shapes","abstract":"We propose a new method to search for parity-violating new physics via measurements of cosmic birefringence and demonstrate its power in detecting the topological effect originating from an axion string network with an axion-photon coupling as a motivated source of cosmic birefringence. The method, using large galaxy samples, exploits an empirical correlation between the polarization direction of the integrated radio emission from a spiral galaxy and its apparent shape. We devise unbiased minimum-variance quadratic estimators for discrete samples of galaxies with both integrated radio polarization and shape measurements. Assuming a synergy with overlapping optical imaging surveys, we forecast the sensitivity to polarization rotation of the forthcoming SKA radio continuum surveys of spiral galaxies out to $z \\sim 1.5$. The angular noise power spectrum of polarization rotation using our method can be lower than that expected from CMB Stage-IV experiments, when assuming a wide survey covering $\\sim 1000\\,{\\rm deg}^2$ and reaching an RMS flux of $\\sim 1\\,\\mu{\\rm Jy}$. Our method will be complementary to CMB-based methods as it will be subject to different systematics. It can be generalized to probe time-varying or redshift-varying birefringence signals.","sentences":["We propose a new method to search for parity-violating new physics via measurements of cosmic birefringence and demonstrate its power in detecting the topological effect originating from an axion string network with an axion-photon coupling as a motivated source of cosmic birefringence.","The method, using large galaxy samples, exploits an empirical correlation between the polarization direction of the integrated radio emission from a spiral galaxy and its apparent shape.","We devise unbiased minimum-variance quadratic estimators for discrete samples of galaxies with both integrated radio polarization and shape measurements.","Assuming a synergy with overlapping optical imaging surveys, we forecast the sensitivity to polarization rotation of the forthcoming SKA radio continuum surveys of spiral galaxies out to $z \\sim 1.5$.","The angular noise power spectrum of polarization rotation using our method can be lower than that expected from CMB Stage-IV experiments, when assuming a wide survey covering $\\sim 1000\\,{\\rm deg}^2$ and reaching an RMS flux of $\\sim 1\\,\\mu{\\rm Jy}$. Our method will be complementary to CMB-based methods as it will be subject to different systematics.","It can be generalized to probe time-varying or redshift-varying birefringence signals."],"url":"http://arxiv.org/abs/2402.18568v1","category":"astro-ph.CO"}
{"created":"2024-02-28 18:57:56","title":"Diffusion Language Models Are Versatile Protein Learners","abstract":"This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.","sentences":["This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences.","We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way.","After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation.","We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).","Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance."],"url":"http://arxiv.org/abs/2402.18567v1","category":"cs.LG"}
{"created":"2024-02-28 18:54:18","title":"Approaching Human-Level Forecasting with Language Models","abstract":"Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.","sentences":["Forecasting future events is important for policy and decision making.","In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters.","Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions.","To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms.","Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts.","On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it.","Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making."],"url":"http://arxiv.org/abs/2402.18563v1","category":"cs.LG"}
{"created":"2024-02-28 18:51:10","title":"Lossy anharmonic polaritons under periodic driving","abstract":"We report on the anharmonic signatures in dissipative polaritons' stationary energy distribution and thermodynamics under external periodic driving. First, we introduce a dynamic model for the dissipative anharmonic Jaynes-Cummings polariton with a generic time-periodic interaction representing modulations of the polariton's energy due to an external force or field. We characterize the stationary state in terms of the exciton, phonon, and interaction energy dependence on the phonon anharmonicity, exciton-phonon coupling strength, and intensity and form of the external field-polariton coupling. Our model also captures the quantum thermodynamics of the driven polariton, which we analyze in connection with the irreversible heat, maximum power, and efficiency of the process. We find considerable differences in energy distribution and thermodynamics between harmonic, moderate, and strongly anharmonic polaritons. Moreover, comparing the external modulations to the phonon and exciton energy, we conclude that the former enhances the polariton's energy storage capacity and is occasionally limited by interference effects and energy saturation at the exciton.","sentences":["We report on the anharmonic signatures in dissipative polaritons' stationary energy distribution and thermodynamics under external periodic driving.","First, we introduce a dynamic model for the dissipative anharmonic Jaynes-Cummings polariton with a generic time-periodic interaction representing modulations of the polariton's energy due to an external force or field.","We characterize the stationary state in terms of the exciton, phonon, and interaction energy dependence on the phonon anharmonicity, exciton-phonon coupling strength, and intensity and form of the external field-polariton coupling.","Our model also captures the quantum thermodynamics of the driven polariton, which we analyze in connection with the irreversible heat, maximum power, and efficiency of the process.","We find considerable differences in energy distribution and thermodynamics between harmonic, moderate, and strongly anharmonic polaritons.","Moreover, comparing the external modulations to the phonon and exciton energy, we conclude that the former enhances the polariton's energy storage capacity and is occasionally limited by interference effects and energy saturation at the exciton."],"url":"http://arxiv.org/abs/2402.18560v1","category":"quant-ph"}
{"created":"2024-02-28 18:49:47","title":"On the singular abelian rank of ultraproduct II$_1$ factors","abstract":"We prove that, under the continuum hypothesis $\\frak c=\\aleph_1$, any ultraproduct II$_1$ factor $M= \\prod_{\\omega} M_n$ of separable finite factors $M_n$ contains more than $\\frak c$ many mutually disjoint singular MASAs, in other words the {\\it singular abelian rank of} $M$, $\\text{\\rm r}(M)$, is larger than $ \\frak c$. Moreover, if the strong continuum hypothesis $2^{\\frak c}=\\aleph_2$ is assumed, then ${\\text{\\rm r}}(M) = 2^{\\frak c}$. More generally, these results hold true for any II$_1$ factor $M$ with unitary group of cardinality $\\frak c$ that satisfies the bicommutant condition $(A_0'\\cap M)'\\cap M=M$, for all $A_0\\subset M$ separable abelian.","sentences":["We prove that, under the continuum hypothesis $\\frak c=\\aleph_1$, any ultraproduct II$_1$ factor $M= \\prod_{\\omega} M_n$ of separable finite factors $M_n$ contains more than $\\frak c$ many mutually disjoint singular MASAs, in other words the {\\it singular abelian rank of} $M$, $\\text{\\rm r}(M)$, is larger than $ \\frak c$.","Moreover, if the strong continuum hypothesis $2^{\\frak c}=\\aleph_2$ is assumed, then ${\\text{\\rm r}}(M) = 2^{\\frak c}$.","More generally, these results hold true for any II$_1$ factor $M$ with unitary group of cardinality $\\frak c$ that satisfies the bicommutant condition $(A_0'\\cap M)'\\cap M=M$, for all $A_0\\subset M$ separable abelian."],"url":"http://arxiv.org/abs/2402.18559v1","category":"math.OA"}
{"created":"2024-02-28 18:36:07","title":"Extended Kalman filter -- Koopman operator for tractable stochastic optimal control","abstract":"It has been more than seven decades since the introduction of the theory of dual control \\cite{feldbaum1960dual}. Although it has provided rich insights to the fields of control, estimation, and system identification, dual control is generally computationally prohibitive. In recent years, however, the use of Koopman operator theory for control applications has been emerging. The paper presents a new reformulation of the stochastic optimal control problem that, employing the Koopman operator, yields a standard LQR problem with the dual control as its solution. We conclude the paper with a numerical example that demonstrates the effectiveness of the proposed approach, compared to certainty equivalence control, when applied to systems with varying observability.","sentences":["It has been more than seven decades since the introduction of the theory of dual control \\cite{feldbaum1960dual}.","Although it has provided rich insights to the fields of control, estimation, and system identification, dual control is generally computationally prohibitive.","In recent years, however, the use of Koopman operator theory for control applications has been emerging.","The paper presents a new reformulation of the stochastic optimal control problem that, employing the Koopman operator, yields a standard LQR problem with the dual control as its solution.","We conclude the paper with a numerical example that demonstrates the effectiveness of the proposed approach, compared to certainty equivalence control, when applied to systems with varying observability."],"url":"http://arxiv.org/abs/2402.18554v1","category":"eess.SY"}
{"created":"2024-02-28 18:34:53","title":"Implicit Bias of Next-Token Prediction","abstract":"Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \\NTP-separability conditions. Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP.","sentences":["Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence.","Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context.","This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary.","It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)?","Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound.","We also demonstrate that these conditions hold under overparameterization.","Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities.","Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \\NTP-separability conditions.","Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP."],"url":"http://arxiv.org/abs/2402.18551v1","category":"cs.LG"}
{"created":"2024-02-28 18:32:43","title":"Deformed neutron stars","abstract":"We present solutions for non-spherically symmetric neutron stars. We begin by deriving the Tolman-Oppenheimer-Volkoff equations from a parameterized metric that takes into account the deformation of the star due to differences in equatorial and polar pressures, expressed in terms of a parameter D, which is the ratio between polar and equatorial radius. The stellar structure is solved using the GM1 equation of state and the Tolman-Oppenheimer-Volkoff equations for deformed objects are numerically integrated using the fourth-order Runge-Kutta method for different values of the parameter D. We show that larger values of D > 1, that describe prolate neutron stars, yield smaller values of mass and radius, while for smaller values of D < 1, describing oblate neutron stars, larger values for mass and radius are attained. From the confrontation of our model theoretical predictions with recent observational data on pulsars, it is possible to constrain the values of the parameter D.","sentences":["We present solutions for non-spherically symmetric neutron stars.","We begin by deriving the Tolman-Oppenheimer-Volkoff equations from a parameterized metric that takes into account the deformation of the star due to differences in equatorial and polar pressures, expressed in terms of a parameter D, which is the ratio between polar and equatorial radius.","The stellar structure is solved using the GM1 equation of state and the Tolman-Oppenheimer-Volkoff equations for deformed objects are numerically integrated using the fourth-order Runge-Kutta method for different values of the parameter D. We show that larger values of D > 1, that describe prolate neutron stars, yield smaller values of mass and radius, while for smaller values of D < 1, describing oblate neutron stars, larger values for mass and radius are attained.","From the confrontation of our model theoretical predictions with recent observational data on pulsars, it is possible to constrain the values of the parameter D."],"url":"http://arxiv.org/abs/2402.18550v1","category":"gr-qc"}
{"created":"2024-02-28 18:31:16","title":"Stabilizing topological superconductivity in disordered spin-orbit coupled semiconductor-superconductor heterostructures","abstract":"We investigate theoretically a one-dimensional semiconductor-superconductor (SM-SC) heterostructure with Rashba spin-orbit coupling and parallel Zeeman field in the presence of disorder generated by random charged impurities and identify the optimal regimes for realizing topological superconductivity and Majorana zero modes. Using a Green's function approach, we show that upon increasing the disorder strength the stable topological superconducting phase characterized by robust end-to-end Majorana correlations ``migrates'' toward larger values of the Zeeman field and can be stabilized by increasing the effective SM-SC coupling. Based on these findings, we propose a strategy for accessing a regime characterized by well-separated Majorana zero modes that is based on (a) enhancing the strength of the effective SM-SC coupling (e.g., through interface engineering) and (b) expanding the range of accessible Zeeman fields (e.g., by enhancing the gyromagnetic ratio or optimizing the parent superconductor, to enable the application of larger magnetic fields). While this strategy may still require some reduction of the disorder strength, this requirement is significantly less strict than the corresponding requirement in a strategy that focuses exclusively on disorder reduction.","sentences":["We investigate theoretically a one-dimensional semiconductor-superconductor (SM-SC) heterostructure with Rashba spin-orbit coupling and parallel Zeeman field in the presence of disorder generated by random charged impurities and identify the optimal regimes for realizing topological superconductivity and Majorana zero modes.","Using a Green's function approach, we show that upon increasing the disorder strength the stable topological superconducting phase characterized by robust end-to-end Majorana correlations ``migrates'' toward larger values of the Zeeman field and can be stabilized by increasing the effective SM-SC coupling.","Based on these findings, we propose a strategy for accessing a regime characterized by well-separated Majorana zero modes that is based on (a) enhancing the strength of the effective SM-SC coupling (e.g., through interface engineering) and (b) expanding the range of accessible Zeeman fields (e.g., by enhancing the gyromagnetic ratio or optimizing the parent superconductor, to enable the application of larger magnetic fields).","While this strategy may still require some reduction of the disorder strength, this requirement is significantly less strict than the corresponding requirement in a strategy that focuses exclusively on disorder reduction."],"url":"http://arxiv.org/abs/2402.18549v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 18:31:14","title":"Universal Spreading of Conditional Mutual Information in Noisy Random Circuits","abstract":"We study the evolution of conditional mutual information in generic open quantum systems, focusing on one-dimensional random circuits with interspersed local noise. Unlike in noiseless circuits, where conditional mutual information spreads linearly while being bounded by the lightcone, we find that noisy random circuits with an error rate $p$ exhibit superlinear propagation of conditional mutual information, which diverges far beyond the lightcone at a critical circuit depth $t_c \\propto p^{-1}$. We demonstrate that the underlying mechanism for such rapid spreading is the combined effect of local noise and a scrambling unitary, which selectively removes short-range correlations while preserving long-range correlations. To analytically capture the dynamics of conditional mutual information in noisy random circuits, we introduce a coarse-graining method, and we validate our theoretical results through numerical simulations. Furthermore, we identify a universal scaling law governing the spreading of conditional mutual information.","sentences":["We study the evolution of conditional mutual information in generic open quantum systems, focusing on one-dimensional random circuits with interspersed local noise.","Unlike in noiseless circuits, where conditional mutual information spreads linearly while being bounded by the lightcone, we find that noisy random circuits with an error rate $p$ exhibit superlinear propagation of conditional mutual information, which diverges far beyond the lightcone at a critical circuit depth $t_c","\\propto p^{-1}$.","We demonstrate that the underlying mechanism for such rapid spreading is the combined effect of local noise and a scrambling unitary, which selectively removes short-range correlations while preserving long-range correlations.","To analytically capture the dynamics of conditional mutual information in noisy random circuits, we introduce a coarse-graining method, and we validate our theoretical results through numerical simulations.","Furthermore, we identify a universal scaling law governing the spreading of conditional mutual information."],"url":"http://arxiv.org/abs/2402.18548v1","category":"quant-ph"}
{"created":"2024-02-28 18:29:51","title":"Minimal seesaw and leptogenesis with the smallest modular finite group","abstract":"We propose a model for leptons based on the smallest modular finite group $\\Gamma_2\\cong S_3$ that, for the first time, accounts for both the hints of large low-energy CP-violation in the lepton sector and the matter-antimatter asymmetry of the Universe, generated by only two heavy right-handed neutrinos. These same states are also employed in a Minimal seesaw mechanism to generate light neutrino masses. Besides the heavy neutrinos, our particle content is the same as the Standard Model (SM), with the addition of one single modulus $\\tau$, whose vacuum expectation value is responsible for both the modular and CP-symmetry breakings. We show that this minimalistic SM extension is enough to get an excellent fit to low energy neutrino observables and to the required baryon asymmetry $\\eta_B$. Predictions for the neutrino mass ordering, effective masses in neutrinoless double beta decay and tritium decay as well as for the Majorana phases are also provided.","sentences":["We propose a model for leptons based on the smallest modular finite group $\\Gamma_2\\cong S_3$ that, for the first time, accounts for both the hints of large low-energy CP-violation in the lepton sector and the matter-antimatter asymmetry of the Universe, generated by only two heavy right-handed neutrinos.","These same states are also employed in a Minimal seesaw mechanism to generate light neutrino masses.","Besides the heavy neutrinos, our particle content is the same as the Standard Model (SM), with the addition of one single modulus $\\tau$, whose vacuum expectation value is responsible for both the modular and CP-symmetry breakings.","We show that this minimalistic SM extension is enough to get an excellent fit to low energy neutrino observables and to the required baryon asymmetry $\\eta_B$. Predictions for the neutrino mass ordering, effective masses in neutrinoless double beta decay and tritium decay as well as for the Majorana phases are also provided."],"url":"http://arxiv.org/abs/2402.18547v1","category":"hep-ph"}
{"created":"2024-02-28 18:29:25","title":"Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces","abstract":"A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization.","sentences":["A major goal in neuroscience is to discover neural data representations that generalize.","This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others.","Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments.","In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024).","EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model.","We find that TOTEM outperforms or matches EEGNet across all generalizability cases.","Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization."],"url":"http://arxiv.org/abs/2402.18546v1","category":"cs.LG"}
{"created":"2024-02-28 18:29:07","title":"Crowdsourcing Dermatology Images with Google Search Ads: Creating a Real-World Skin Condition Dataset","abstract":"Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development. Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets.   Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic and symptom information. With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023. The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images.   Results: We received a median of 22 submissions/day (IQR 14-30). Female (66.72%) and younger (52% < age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6% of contributors reported a non-White racial or ethnic identity. Over 97.5% of contributions were genuine images of skin conditions. Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weaker correlation with image sharpness (Spearman's P values <0.001 and 0.01 respectively). Most contributions were short-duration (54% with onset < 7 days ago ) and 89% were allergic, infectious, or inflammatory conditions. eFST and eMST distributions reflected the geographical origin of the dataset. The dataset is available at github.com/google-research-datasets/scin .   Conclusion: Search ads are effective at crowdsourcing images of health conditions. The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions.","sentences":["Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development.","Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets.   ","Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic and symptom information.","With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023.","The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images.   ","Results:","We received a median of 22 submissions/day (IQR 14-30).","Female (66.72%) and younger (52% < age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6% of contributors reported a non-White racial or ethnic identity.","Over 97.5% of contributions were genuine images of skin conditions.","Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weaker correlation with image sharpness (Spearman's P values <0.001 and 0.01 respectively).","Most contributions were short-duration (54% with onset < 7 days ago ) and 89% were allergic, infectious, or inflammatory conditions.","eFST and eMST distributions reflected the geographical origin of the dataset.","The dataset is available at github.com/google-research-datasets/scin .   ","Conclusion: Search ads are effective at crowdsourcing images of health conditions.","The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions."],"url":"http://arxiv.org/abs/2402.18545v1","category":"cs.CY"}
{"created":"2024-02-28 18:28:21","title":"Optimizing Beer Glass Shapes to Minimize Heat Transfer During Consumption","abstract":"This paper addresses the problem of determining the optimum shape for a beer glass that minimizes the heat transfer while the liquid is consumed, thereby keeping it cold for as long as possible. The proposed solution avoids the use of insulating materials. The glass is modelled as a body of revolution generated by a smooth curve S, constructed from a material with negligible thermal resistance at the revolution surface but insulated at the bottom. The ordinary differential equation describing the problem is derived from the first law of Thermodynamics applied to a control volume encompassing the liquid. This is an inverse optimization problem, aiming to find the shape of the glass (represented by curve S) that minimizes the heat transfer rate. In contrast, the direct problem aims to determine the heat transfer rate for a given geometry. The solution obtained is analytic, and the resulting expression for S is in closed form, providing a family of optimal glass shapes that can be manufactured using conventional methods.","sentences":["This paper addresses the problem of determining the optimum shape for a beer glass that minimizes the heat transfer while the liquid is consumed, thereby keeping it cold for as long as possible.","The proposed solution avoids the use of insulating materials.","The glass is modelled as a body of revolution generated by a smooth curve S, constructed from a material with negligible thermal resistance at the revolution surface but insulated at the bottom.","The ordinary differential equation describing the problem is derived from the first law of Thermodynamics applied to a control volume encompassing the liquid.","This is an inverse optimization problem, aiming to find the shape of the glass (represented by curve S) that minimizes the heat transfer rate.","In contrast, the direct problem aims to determine the heat transfer rate for a given geometry.","The solution obtained is analytic, and the resulting expression for S is in closed form, providing a family of optimal glass shapes that can be manufactured using conventional methods."],"url":"http://arxiv.org/abs/2402.18544v1","category":"physics.pop-ph"}
{"created":"2024-02-28 18:23:49","title":"Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates","abstract":"Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the \"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases.","sentences":["Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research.","These models underwent alignment training and were considered safe.","Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.","The current paper is about methods and best practices to mitigate such loss of alignment.","Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the \"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time.","Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases."],"url":"http://arxiv.org/abs/2402.18540v1","category":"cs.LG"}
{"created":"2024-02-28 18:22:03","title":"On the enumeration of signatures of XOR-CNF's","abstract":"Given a CNF formula $\\varphi$ with clauses $C_1, \\dots, C_m$ over a set of variables $V$, a truth assignment $\\mathbf{a} : V \\to \\{0, 1\\}$ generates a binary sequence $\\sigma_\\varphi(\\mathbf{a})=(C_1(\\mathbf{a}), \\ldots, C_m(\\mathbf{a}))$, called a signature of $\\varphi$, where $C_i(\\mathbf{a})=1$ if clause $C_i$ evaluates to 1 under assignment $\\mathbf{a}$, and $C_i(\\mathbf{a})=0$ otherwise. Signatures and their associated generation problems have given rise to new yet promising research questions in algorithmic enumeration. In a recent paper, B\\'erczi et al. interestingly proved that generating signatures of a CNF is tractable despite the fact that verifying a solution is hard. They also showed the hardness of finding maximal signatures of an arbitrary CNF due to the intractability of satisfiability in general. Their contribution leaves open the problem of efficiently generating maximal signatures for tractable classes of CNFs, i.e., those for which satisfiability can be solved in polynomial time. Stepping into that direction, we completely characterize the complexity of generating all, minimal, and maximal signatures for XOR-CNFs.","sentences":["Given a CNF formula $\\varphi$ with clauses $C_1, \\dots, C_m$ over a set of variables $V$, a truth assignment $\\mathbf{a} : V \\to \\{0, 1\\}$ generates a binary sequence $\\sigma_\\varphi(\\mathbf{a})=(C_1(\\mathbf{a}), \\ldots, C_m(\\mathbf{a}))$, called a signature of $\\varphi$, where $C_i(\\mathbf{a})=1$ if clause $C_i$ evaluates to 1 under assignment $\\mathbf{a}$, and $C_i(\\mathbf{a})=0$ otherwise.","Signatures and their associated generation problems have given rise to new yet promising research questions in algorithmic enumeration.","In a recent paper, B\\'erczi et al. interestingly proved that generating signatures of a CNF is tractable despite the fact that verifying a solution is hard.","They also showed the hardness of finding maximal signatures of an arbitrary CNF due to the intractability of satisfiability in general.","Their contribution leaves open the problem of efficiently generating maximal signatures for tractable classes of CNFs, i.e., those for which satisfiability can be solved in polynomial time.","Stepping into that direction, we completely characterize the complexity of generating all, minimal, and maximal signatures for XOR-CNFs."],"url":"http://arxiv.org/abs/2402.18537v1","category":"cs.DS"}
{"created":"2024-02-28 18:17:25","title":"Quantum Gravity: are we there yet?","abstract":"The turn of the millennium was a time of optimism about an approach to noncommutative geometry inspired by rich mathematical objects called `quantum groups' and its applications to quantum spacetime. This would model quantum gravity effects as noncommutativity of spacetime coordinates and was arguably going to solve quantum gravity itself. It took a further 20 years from that point to develop a particularly suitable formalism of `quantum Riemannian geometry', but this was largely done and has begun to be used to construct baby quantum gravity models. In this article, we obtain new results for state of the art fuzzy sphere and n-gon models in this approach. We also review what are some elements of quantum gravity that we can already see and what are the critical conceptual and mathematical elements that are still missing to more fully achieve this goal.","sentences":["The turn of the millennium was a time of optimism about an approach to noncommutative geometry inspired by rich mathematical objects called `quantum groups' and its applications to quantum spacetime.","This would model quantum gravity effects as noncommutativity of spacetime coordinates and was arguably going to solve quantum gravity itself.","It took a further 20 years from that point to develop a particularly suitable formalism of `quantum Riemannian geometry', but this was largely done and has begun to be used to construct baby quantum gravity models.","In this article, we obtain new results for state of the art fuzzy sphere and n-gon models in this approach.","We also review what are some elements of quantum gravity that we can already see and what are the critical conceptual and mathematical elements that are still missing to more fully achieve this goal."],"url":"http://arxiv.org/abs/2402.18536v1","category":"gr-qc"}
{"created":"2024-02-28 18:17:19","title":"Dark matter bound-state formation in the Sun","abstract":"The Sun may capture asymmetric dark matter (DM), which can subsequently form bound-states through the radiative emission of a sub-GeV scalar. This process enables generation of scalars without requiring DM annihilation. In addition to DM capture on nucleons, the DM-scalar coupling responsible for bound-state formation also induces capture from self-scatterings of ambient DM particles with DM particles already captured, as well as with DM bound-states formed in-situ within the Sun. This scenario is studied in detail by solving Boltzmann equations numerically and analytically. In particular, we take into consideration that the DM self-capture rates require a treatment beyond the conventional Born approximation. We show that, thanks to DM scatterings on bound-states, the number of DM particles captured increases exponentially, leading to enhanced emission of relativistic scalars through bound-state formation, whose final decay products could be observable. We explore phenomenological signatures with the example that the scalar mediator decays to neutrinos. We find that the neutrino flux emitted can be comparable to atmospheric neutrino fluxes within the range of energies below one hundred MeV. Future facilities like Hyper-K, and direct DM detection experiments can further test such scenario.","sentences":["The Sun may capture asymmetric dark matter (DM), which can subsequently form bound-states through the radiative emission of a sub-GeV scalar.","This process enables generation of scalars without requiring DM annihilation.","In addition to DM capture on nucleons, the DM-scalar coupling responsible for bound-state formation also induces capture from self-scatterings of ambient DM particles with DM particles already captured, as well as with DM bound-states formed in-situ within the Sun.","This scenario is studied in detail by solving Boltzmann equations numerically and analytically.","In particular, we take into consideration that the DM self-capture rates require a treatment beyond the conventional Born approximation.","We show that, thanks to DM scatterings on bound-states, the number of DM particles captured increases exponentially, leading to enhanced emission of relativistic scalars through bound-state formation, whose final decay products could be observable.","We explore phenomenological signatures with the example that the scalar mediator decays to neutrinos.","We find that the neutrino flux emitted can be comparable to atmospheric neutrino fluxes within the range of energies below one hundred MeV. Future facilities like Hyper-K, and direct DM detection experiments can further test such scenario."],"url":"http://arxiv.org/abs/2402.18535v1","category":"hep-ph"}
{"created":"2024-02-28 18:16:56","title":"Enhancing density functional theory using the variational quantum eigensolver","abstract":"Quantum computers open up new avenues for modelling the physical properties of materials and molecules. Density Functional Theory (DFT) is the gold standard classical algorithm for predicting these properties, but relies on approximations of the unknown universal functional, limiting its general applicability for many fundamental and technologically relevant systems. In this work we develop a hybrid quantum/classical algorithm called quantum enhanced DFT (QEDFT) that systematically constructs quantum approximations of the universal functional using data obtained from a quantum computer.   We benchmark the QEDFT algorithm on the Fermi-Hubbard model, both numerically and on data from experiments on real quantum hardware. We find that QEDFT surpasses the quality of groundstate results obtained from Hartree-Fock DFT, as well as from direct application of conventional quantum algorithms such as VQE. Furthermore, we demonstrate that QEDFT works even when only noisy, low-depth quantum computation is available, by benchmarking the algorithm on data obtained from Google's quantum computer.   We further show how QEDFT also captures quintessential properties of strongly correlated Mott physics for large Fermi-Hubbard systems using functionals generated on much smaller system sizes. Our results indicate that QEDFT can be applied to realistic materials and molecular systems, and has the potential to outperform the direct application of either DFT or VQE alone, without the requirement of large scale or fully fault-tolerant quantum computers.","sentences":["Quantum computers open up new avenues for modelling the physical properties of materials and molecules.","Density Functional Theory (DFT) is the gold standard classical algorithm for predicting these properties, but relies on approximations of the unknown universal functional, limiting its general applicability for many fundamental and technologically relevant systems.","In this work we develop a hybrid quantum/classical algorithm called quantum enhanced DFT (QEDFT) that systematically constructs quantum approximations of the universal functional using data obtained from a quantum computer.   ","We benchmark the QEDFT algorithm on the Fermi-Hubbard model, both numerically and on data from experiments on real quantum hardware.","We find that QEDFT surpasses the quality of groundstate results obtained from Hartree-Fock DFT, as well as from direct application of conventional quantum algorithms such as VQE.","Furthermore, we demonstrate that QEDFT works even when only noisy, low-depth quantum computation is available, by benchmarking the algorithm on data obtained from Google's quantum computer.   ","We further show how QEDFT also captures quintessential properties of strongly correlated Mott physics for large Fermi-Hubbard systems using functionals generated on much smaller system sizes.","Our results indicate that QEDFT can be applied to realistic materials and molecular systems, and has the potential to outperform the direct application of either DFT or VQE alone, without the requirement of large scale or fully fault-tolerant quantum computers."],"url":"http://arxiv.org/abs/2402.18534v1","category":"quant-ph"}
{"created":"2024-02-28 18:12:06","title":"Phase transitions beyond criticality: extending Ising universal scaling functions to describe entire phases","abstract":"Universal scaling laws only apply asymptotically near critical phase transitions. We propose a general scheme, based on normal form theory of renormalization group flows, for incorporating corrections to scaling that quantitatively describe the entire neighboring phases. Expanding Onsager's exact solution of the 2D Ising model about the critical point, we identify a special coordinate with radius of convergence covering the entire physical temperature range, $0<T<\\infty$. Without an exact solution, we demonstrate that using solely the critical singularity with low- and high-temperature expansions leads to exponentially converging approximations across all temperatures for both the 2D and 3D Ising free energies and the 3D magnetization. We discuss challenges and opportunities for future work.","sentences":["Universal scaling laws only apply asymptotically near critical phase transitions.","We propose a general scheme, based on normal form theory of renormalization group flows, for incorporating corrections to scaling that quantitatively describe the entire neighboring phases.","Expanding Onsager's exact solution of the 2D Ising model about the critical point, we identify a special coordinate with radius of convergence covering the entire physical temperature range, $0<T<\\infty$. Without an exact solution, we demonstrate that using solely the critical singularity with low- and high-temperature expansions leads to exponentially converging approximations across all temperatures for both the 2D and 3D Ising free energies and the 3D magnetization.","We discuss challenges and opportunities for future work."],"url":"http://arxiv.org/abs/2402.18531v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 18:08:35","title":"High angular momentum hot differentially rotating equilibrium star evolutions in conformally flat spacetime","abstract":"The conformal flatness approximation to the Einstein equations has been successfully used in many astrophysical applications such as initial data constructions and dynamical simulations. Although it has been shown that full general relativistic strongly differentially rotating equilibrium models deviate by at most a few percents from their conformally flat counterparts, whether those solutions share the same dynamical stabilities has not been fully addressed. To further understand the limitations of the conformal flatness approximation, in this work, we construct spatially-conformally-flat hot hypermassive neutron stars with postmerger-like rotation laws, and perform conformally flat evolutions and analysis over dynamical timescales. We found that the stellar profiles of quasi-toroidal models with high angular momentum for $J \\gtrsim 9 \\;G M_{\\odot}^2 / c$ can change significantly over dynamical timescales. In contrast, all the quasi-spherical models considered in this work remain stable even with high angular momentum $J=9\\;G M_{\\odot}^2 / c$. Our investigation suggest that the quasi-spherical models are suitable initial data for long-lived hypermassive neutron star modelings in conformally flat spacetime.","sentences":["The conformal flatness approximation to the Einstein equations has been successfully used in many astrophysical applications such as initial data constructions and dynamical simulations.","Although it has been shown that full general relativistic strongly differentially rotating equilibrium models deviate by at most a few percents from their conformally flat counterparts, whether those solutions share the same dynamical stabilities has not been fully addressed.","To further understand the limitations of the conformal flatness approximation, in this work, we construct spatially-conformally-flat hot hypermassive neutron stars with postmerger-like rotation laws, and perform conformally flat evolutions and analysis over dynamical timescales.","We found that the stellar profiles of quasi-toroidal models with high angular momentum for $J \\gtrsim 9 \\;G M_{\\odot}^2 / c$ can change significantly over dynamical timescales.","In contrast, all the quasi-spherical models considered in this work remain stable even with high angular momentum $J=9\\;G M_{\\odot}^2 / c$. Our investigation suggest that the quasi-spherical models are suitable initial data for long-lived hypermassive neutron star modelings in conformally flat spacetime."],"url":"http://arxiv.org/abs/2402.18529v1","category":"astro-ph.HE"}
{"created":"2024-02-28 17:54:55","title":"Almost device-independent certification of GME states with minimal measurements","abstract":"Device-independent certification of quantum states allows the characterization of quantum states present inside a device by making minimal physical assumptions. A major problem in this regard is to certify quantum states using minimal resources. In this work, we consider the multipartite quantum steering scenario with an arbitrary number of parties but only one of which is trusted in the sense that the measurements performed by the trusted party are known. Consequently, the self-testing scheme is almost device-independent. Importantly, all the parties can only perform two measurements each which is the minimal number of measurements required to observe any form of quantum nonlocality. Then, we propose steering inequalities that are maximally violated by three major classes of genuinely multipartite entangled (GME) states, one, graph states of arbitrary local dimension, two, Schmidt states of arbitrary local dimension, and, three, $N$-qubit generalized W states. Using the proposed inequalities, we then provide an almost device-independent certification of the above GME states.","sentences":["Device-independent certification of quantum states allows the characterization of quantum states present inside a device by making minimal physical assumptions.","A major problem in this regard is to certify quantum states using minimal resources.","In this work, we consider the multipartite quantum steering scenario with an arbitrary number of parties but only one of which is trusted in the sense that the measurements performed by the trusted party are known.","Consequently, the self-testing scheme is almost device-independent.","Importantly, all the parties can only perform two measurements each which is the minimal number of measurements required to observe any form of quantum nonlocality.","Then, we propose steering inequalities that are maximally violated by three major classes of genuinely multipartite entangled (GME) states, one, graph states of arbitrary local dimension, two, Schmidt states of arbitrary local dimension, and, three, $N$-qubit generalized W states.","Using the proposed inequalities, we then provide an almost device-independent certification of the above GME states."],"url":"http://arxiv.org/abs/2402.18522v1","category":"quant-ph"}
{"created":"2024-02-28 17:54:09","title":"Polarized-deuteron scattering by spin-zero target nuclei at intermediate energies","abstract":"General analytical expressions for the observables in $dA$-scattering reaction have been derived in the diffraction approximation. The resulting formulas describe the cross section and polarization states of the deuteron when it scattering by nuclei with zero spin in the ground state. The tabulated distributions of the target nucleus density and the realistic deuteron wave functions calculated on the basis of Nijmegen nucleon-nucleon potentials were used. The nucleon-nucleus phases were calculated in the framework of Glauber formalism and making use of the double-folding potential. The calculated cross sections and analyzing powers in elastic scattering of deuterons by $^{16}\\text{O}$ and $^{40}\\text{Ca}$ nuclei at 700 MeV are compared with the corresponding experimental data.","sentences":["General analytical expressions for the observables in $dA$-scattering reaction have been derived in the diffraction approximation.","The resulting formulas describe the cross section and polarization states of the deuteron when it scattering by nuclei with zero spin in the ground state.","The tabulated distributions of the target nucleus density and the realistic deuteron wave functions calculated on the basis of Nijmegen nucleon-nucleon potentials were used.","The nucleon-nucleus phases were calculated in the framework of Glauber formalism and making use of the double-folding potential.","The calculated cross sections and analyzing powers in elastic scattering of deuterons by $^{16}\\text{O}$ and $^{40}\\text{Ca}$ nuclei at 700 MeV are compared with the corresponding experimental data."],"url":"http://arxiv.org/abs/2402.18521v1","category":"nucl-th"}
{"created":"2024-02-28 17:47:40","title":"BPS chiral vortices in a Maxwell-Higgs electrodynamics","abstract":"We investigate the existence of BPS structures in a Maxwell-Higgs electrodynamics immersed within a chiral medium, whose electromagnetic properties are described by both the Chern-Simons term and a neutral scalar field. The implementation of the Bogomol'nyi-Prasad-Sommerfield's technique provides the BPS potential and the self-dual equations whose solutions saturate the Bogomol'nyi bound. In such a context, we look for vortices in two chiral media: the first one engenders localized vortices with an exponential decay similar to that of the Abrikosov-Nielsen-Olesen solutions, whereas the second medium generates delocalized profiles whose tail follows a power-law decay. Once we have solved the BPS systems, we comment on the effects induced by the presence of the chiral medium on the Maxwell-Higgs vortices.","sentences":["We investigate the existence of BPS structures in a Maxwell-Higgs electrodynamics immersed within a chiral medium, whose electromagnetic properties are described by both the Chern-Simons term and a neutral scalar field.","The implementation of the Bogomol'nyi-Prasad-Sommerfield's technique provides the BPS potential and the self-dual equations whose solutions saturate the Bogomol'nyi bound.","In such a context, we look for vortices in two chiral media: the first one engenders localized vortices with an exponential decay similar to that of the Abrikosov-Nielsen-Olesen solutions, whereas the second medium generates delocalized profiles whose tail follows a power-law decay.","Once we have solved the BPS systems, we comment on the effects induced by the presence of the chiral medium on the Maxwell-Higgs vortices."],"url":"http://arxiv.org/abs/2402.18517v1","category":"hep-th"}
{"created":"2024-02-28 17:40:01","title":"Leveraging Compliant Tactile Perception for Haptic Blind Surface Reconstruction","abstract":"Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces.","sentences":["Non-flat surfaces pose difficulties for robots operating in unstructured environments.","Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions.","This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors.","The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects.","While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points.","These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface.","Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy.","Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces."],"url":"http://arxiv.org/abs/2402.18511v1","category":"cs.RO"}
{"created":"2024-02-28 17:38:06","title":"RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval","abstract":"This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.","sentences":["This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems.","We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.","Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers.","A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease.","Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers."],"url":"http://arxiv.org/abs/2402.18510v1","category":"cs.LG"}
{"created":"2024-02-28 17:37:23","title":"Old Meets New: Connecting Two Infinite Families of Congruences Modulo Powers of 5 for Generalized Frobenius Partition Functions","abstract":"In 2012 Paule and Radu proved a difficult family of congruences modulo powers of 5 for Andrews' 2-colored generalized Frobenius partition function. The family is associated with the classical modular curve of level 20. We demonstrate the existence of a second congruence family for a related generalized Frobenius partition function associated with the same curve. Unlike traditional proofs, we construct an isomorphism between this new family and the original family of congruences via an automorphism on the associated ring of functions. We also give some important insights into the behavior of these congruence families with respect to the Atkin--Lehner involution which proved very important in Paule and Radu's original proof.","sentences":["In 2012 Paule and Radu proved a difficult family of congruences modulo powers of 5 for Andrews' 2-colored generalized Frobenius partition function.","The family is associated with the classical modular curve of level 20.","We demonstrate the existence of a second congruence family for a related generalized Frobenius partition function associated with the same curve.","Unlike traditional proofs, we construct an isomorphism between this new family and the original family of congruences via an automorphism on the associated ring of functions.","We also give some important insights into the behavior of these congruence families with respect to the Atkin--Lehner involution which proved very important in Paule and Radu's original proof."],"url":"http://arxiv.org/abs/2402.18509v1","category":"math.NT"}
{"created":"2024-02-28 17:36:45","title":"Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling","abstract":"In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences. We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality. Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.","sentences":["In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical.","This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism.","Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning.","At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network.","We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation.","The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences.","We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality.","Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers.","This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling."],"url":"http://arxiv.org/abs/2402.18508v1","category":"cs.LG"}
{"created":"2024-02-28 17:29:27","title":"Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification","abstract":"Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness. In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models. This work is one of the early attempts to achieve fairness in prediction tasks by utilizing LLMs through in-context learning.","sentences":["Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model.","Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment.","As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness.","In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction.","We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process.","Experiments conducted with different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models.","This work is one of the early attempts to achieve fairness in prediction tasks by utilizing LLMs through in-context learning."],"url":"http://arxiv.org/abs/2402.18502v1","category":"cs.CL"}
{"created":"2024-02-28 17:29:23","title":"Yangian of the periplectic Lie superalgebra","abstract":"We study in detail the Yangian of the periplectic Lie superalgebra. For this Yangian we verify an analogue of the Poincar\\'e-Birkhoff-Witt Theorem. Moreover we introduce a family of free generators of the centre of this Yangian.","sentences":["We study in detail the Yangian of the periplectic Lie superalgebra.","For this Yangian we verify an analogue of the Poincar\\'e-Birkhoff-Witt Theorem.","Moreover we introduce a family of free generators of the centre of this Yangian."],"url":"http://arxiv.org/abs/2402.18501v1","category":"math.QA"}
{"created":"2024-02-28 17:27:55","title":"On the exact solution for the Schr\u00f6dinger equation","abstract":"For almost 75 years, the general solution for the Schr\\\"odinger equation was assumed to be generated by a time-ordered exponential known as the Dyson series. We discuss under which conditions the unitarity of this solution is broken, and additional singular dynamics emerges. Then, we provide an alternative construction that is manifestly unitary, regardless of the choice of the Hamiltonian, and study various aspects of the implications. The new construction involves an additional self-adjoint operator that might evolve in a non-gradual way. Its corresponding dynamics for gauge theories exhibit the behavior of a collective object governed by a singular Liouville's equation that performs transitions at a measure $0$ set. Our considerations show that Schr\\\"odinger's and Liouville's equations are, in fact, two sides of the same coin, and together they become the unified description of quantum systems.","sentences":["For almost 75 years, the general solution for the Schr\\\"odinger equation was assumed to be generated by a time-ordered exponential known as the Dyson series.","We discuss under which conditions the unitarity of this solution is broken, and additional singular dynamics emerges.","Then, we provide an alternative construction that is manifestly unitary, regardless of the choice of the Hamiltonian, and study various aspects of the implications.","The new construction involves an additional self-adjoint operator that might evolve in a non-gradual way.","Its corresponding dynamics for gauge theories exhibit the behavior of a collective object governed by a singular Liouville's equation that performs transitions at a measure $0$ set.","Our considerations show that Schr\\\"odinger's and Liouville's equations are, in fact, two sides of the same coin, and together they become the unified description of quantum systems."],"url":"http://arxiv.org/abs/2402.18499v1","category":"quant-ph"}
{"created":"2024-02-28 17:26:45","title":"Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration","abstract":"Although recent developments in generative AI have greatly enhanced the capabilities of conversational agents such as Google's Bard or OpenAI's ChatGPT, it's unclear whether the usage of these agents aids users across various contexts. To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to Bard. Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended \"solve\" questions vs. definitive \"search\" questions), and measurement type (demonstrated vs. self-reported). Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on \"solve\"-type questions when using the AI. We discuss common behaviors, design recommendations, and impact considerations to improve collaborations with conversational AI.","sentences":["Although recent developments in generative AI have greatly enhanced the capabilities of conversational agents such as Google's Bard or OpenAI's ChatGPT, it's unclear whether the usage of these agents aids users across various contexts.","To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to Bard.","Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended \"solve\" questions vs. definitive \"search\" questions), and measurement type (demonstrated vs. self-reported).","Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on \"solve\"-type questions when using the AI.","We discuss common behaviors, design recommendations, and impact considerations to improve collaborations with conversational AI."],"url":"http://arxiv.org/abs/2402.18498v1","category":"cs.HC"}
{"created":"2024-02-28 17:26:36","title":"Polarized and unpolarized off-shell $H^\\ast\\to ZZ\\rightarrow 4\\ell$ decay above the $2m_Z$ threshold","abstract":"An analysis of the off-shell $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ decay width is presented for both unpolarized and polarized $Z$ gauge bosons in the scenario with the most general $H^*ZZ$ vertex function, which is given in terms of two $CP$-even ($\\hat b_Z$ and $\\hat c_Z$) and one $CP$-odd ($\\tilde b_Z$) anomalous couplings.   The SM contributions to the $H^*ZZ$ coupling up to the one-loop level are also included. Explicit analytic results for the unpolarized and polarized $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ square amplitudes and the four-body phase space are presented, out of which several observable quantities can be obtained straightforwardly. As far as the numerical analysis is concerned, a cross-check is performed via Madgraph, where our model was implemented with the aid of FeynRules. We then consider the most stringent bounds on anomalous complex $H^*ZZ$ couplings and analyze the effects of the polarizations of the $Z$ gauge bosons through the polarized $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ decay width as well as left-right and forward-backward asymmetries, which are found to be sensitive to new-physics effects. Particular focus is put on the effects of the absorptive parts of the anomalous $H^*ZZ$ couplings, which have been largely overlooked up to now in LHC analyses. It is found that the studied observable quantities, particularly the left-right asymmetries, can be helpful to look for effects of $CP$-violation in the $H^*ZZ$ coupling and set bounds on the absorptive parts at the LHC and future colliders. For completeness we also analyze the case of unpolarized $Z$ gauge bosons.","sentences":["An analysis of the off-shell $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ decay width is presented for both unpolarized and polarized $Z$ gauge bosons in the scenario with the most general $H^*ZZ$ vertex function, which is given in terms of two $CP$-even ($\\hat b_Z$ and $\\hat c_Z$) and one $CP$-odd ($\\tilde b_Z$) anomalous couplings.   ","The SM contributions to the $H^*ZZ$ coupling up to the one-loop level are also included.","Explicit analytic results for the unpolarized and polarized $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ square amplitudes and the four-body phase space are presented, out of which several observable quantities can be obtained straightforwardly.","As far as the numerical analysis is concerned, a cross-check is performed via Madgraph, where our model was implemented with the aid of FeynRules.","We then consider the most stringent bounds on anomalous complex $H^*ZZ$ couplings and analyze the effects of the polarizations of the $Z$ gauge bosons through the polarized $H^\\ast\\rightarrow ZZ \\rightarrow \\overline{\\ell}_1\\ell_1\\overline{\\ell}_2\\ell_2$ decay width as well as left-right and forward-backward asymmetries, which are found to be sensitive to new-physics effects.","Particular focus is put on the effects of the absorptive parts of the anomalous $H^*ZZ$ couplings, which have been largely overlooked up to now in LHC analyses.","It is found that the studied observable quantities, particularly the left-right asymmetries, can be helpful to look for effects of $CP$-violation in the $H^*ZZ$ coupling and set bounds on the absorptive parts at the LHC and future colliders.","For completeness we also analyze the case of unpolarized $Z$ gauge bosons."],"url":"http://arxiv.org/abs/2402.18497v1","category":"hep-ph"}
{"created":"2024-02-28 17:25:59","title":"Language Models Represent Beliefs of Self and Others","abstract":"Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.","sentences":["Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning.","While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive.","In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs.","By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process.","Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations."],"url":"http://arxiv.org/abs/2402.18496v1","category":"cs.AI"}
{"created":"2024-02-28 17:23:56","title":"Thermal Stochastic Inflation","abstract":"Investigating the thermal inflationary model, we introduce stochastic effects, incorporating a cutoff parameter {\\sigma} which distinguishes between quantum and classical modes. Testing the model against Planck 2018 data, we observe a preference for a non-zero {\\sigma} at at least 68% C.L., suggesting the classicalization of most modes and providing a theoretical foundation for the quantum to classical transition. As a result of introducing the stochastic effects, we find that the solution to the large-scale power deficit requires a lower comoving temperature of inflaton.","sentences":["Investigating the thermal inflationary model, we introduce stochastic effects, incorporating a cutoff parameter {\\sigma} which distinguishes between quantum and classical modes.","Testing the model against Planck 2018 data, we observe a preference for a non-zero {\\sigma} at at least 68% C.L., suggesting the classicalization of most modes and providing a theoretical foundation for the quantum to classical transition.","As a result of introducing the stochastic effects, we find that the solution to the large-scale power deficit requires a lower comoving temperature of inflaton."],"url":"http://arxiv.org/abs/2402.18494v1","category":"gr-qc"}
{"created":"2024-02-28 17:19:26","title":"Dynamical Regimes of Diffusion Models","abstract":"Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.","sentences":["Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally.","Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process.","The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions.","It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase.","For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data.","The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models.","Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions."],"url":"http://arxiv.org/abs/2402.18491v1","category":"cs.LG"}
{"created":"2024-02-28 17:15:33","title":"FAST functional connectivity implicates P300 connectivity in working memory deficits in Alzheimer's disease","abstract":"Measuring transient functional connectivity is an important challenge in Electroencephalogram (EEG) research. Here, the rich potential for insightful, discriminative information of brain activity offered by high temporal resolution is confounded by the inherent noise of the medium and the spurious nature of correlations computed over short temporal windows. We propose a novel methodology to overcome these problems called Filter Average Short-Term (FAST) functional connectivity. First, long-term, stable, functional connectivity is averaged across an entire study cohort for a given pair of Visual Short Term Memory (VSTM) tasks. The resulting average connectivity matrix, containing information on the strongest general connections for the tasks, is used as a filter to analyse the transient high temporal resolution functional connectivity of individual subjects. In simulations, we show that this method accurately discriminates differences in noisy Event-Related Potentials (ERPs) between two conditions where standard connectivity and other comparable methods fail. We then apply this to analyse activity related to visual short-term memory binding deficits in two cohorts of familial and sporadic Alzheimer's disease. Reproducible significant differences were found in the binding task with no significant difference in the shape task in the P300 ERP range. This allows new sensitive measurements of transient functional connectivity, which can be implemented to obtain results of clinical significance.","sentences":["Measuring transient functional connectivity is an important challenge in Electroencephalogram (EEG) research.","Here, the rich potential for insightful, discriminative information of brain activity offered by high temporal resolution is confounded by the inherent noise of the medium and the spurious nature of correlations computed over short temporal windows.","We propose a novel methodology to overcome these problems called Filter Average Short-Term (FAST) functional connectivity.","First, long-term, stable, functional connectivity is averaged across an entire study cohort for a given pair of Visual Short Term Memory (VSTM) tasks.","The resulting average connectivity matrix, containing information on the strongest general connections for the tasks, is used as a filter to analyse the transient high temporal resolution functional connectivity of individual subjects.","In simulations, we show that this method accurately discriminates differences in noisy Event-Related Potentials (ERPs) between two conditions where standard connectivity and other comparable methods fail.","We then apply this to analyse activity related to visual short-term memory binding deficits in two cohorts of familial and sporadic Alzheimer's disease.","Reproducible significant differences were found in the binding task with no significant difference in the shape task in the P300 ERP range.","This allows new sensitive measurements of transient functional connectivity, which can be implemented to obtain results of clinical significance."],"url":"http://arxiv.org/abs/2402.18489v1","category":"q-bio.NC"}
{"created":"2024-02-28 17:10:22","title":"Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay","abstract":"The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our contributions include (1) a reinforcement learning framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios. The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values.","sentences":["The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness.","However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground.","This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions.","We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations.","Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR.","Our contributions include (1) a reinforcement learning framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios.","The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values."],"url":"http://arxiv.org/abs/2402.18487v1","category":"cs.RO"}
{"created":"2024-02-28 17:06:54","title":"FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist","abstract":"Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.","sentences":["Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets.","While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks.","To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading.","FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market.","Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes.","The agent's emphasis on reasoning for actions fosters trust in its financial decisions.","Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles.","With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit.","Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset.","Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks."],"url":"http://arxiv.org/abs/2402.18485v1","category":"q-fin.TR"}
{"created":"2024-02-28 17:06:15","title":"A generalised Nehari manifold method for a class of non linear Schr\u00f6dinger systems in $\\mathbb{R}^3$","abstract":"We study the existence of positive solutions of a particular elliptic system in $\\mathbb{R}^3$ composed of two coupled non linear stationary Schr\\\"odinger equations (NLSEs), that is $-\\epsilon^2 \\Delta u + V(x) u= h_v(u,v), - \\epsilon^2 \\Delta v + V(x) v=h_u (u,v)$. Under certain hypotheses on the potential $V$ and the non linearity $h$, we manage to prove that there exists a solution $(u_\\epsilon,v_\\epsilon)$ that decays exponentially with respect to local minima points of the potential and whose energy tends to concentrate around these points, as $\\epsilon \\to 0$. We also estimate this energy in terms of particular ground state energies. This work follows closely what is done in https://doi.org/10.1007/s00526-007-0103-z , although here we consider a more general non linearity and we restrict ourselves to the case where the domain is $\\mathbb{R}^3$.","sentences":["We study the existence of positive solutions of a particular elliptic system in $\\mathbb{R}^3$ composed of two coupled non linear stationary Schr\\\"odinger equations (NLSEs), that is $-\\epsilon^2 \\Delta u + V(x) u= h_v(u,v), - \\epsilon^2 \\Delta v + V(x) v=h_u (u,v)$. Under certain hypotheses on the potential $V$ and the non linearity $h$, we manage to prove that there exists a solution $(u_\\epsilon,v_\\epsilon)$ that decays exponentially with respect to local minima points of the potential and whose energy tends to concentrate around these points, as $\\epsilon \\to 0$.","We also estimate this energy in terms of particular ground state energies.","This work follows closely what is done in https://doi.org/10.1007/s00526-007-0103-z , although here we consider a more general non linearity and we restrict ourselves to the case where the domain is $\\mathbb{R}^3$."],"url":"http://arxiv.org/abs/2402.18483v1","category":"math.AP"}
{"created":"2024-02-28 16:59:35","title":"NewsQs: Multi-Source Question Generation for the Inquiring Mind","abstract":"We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus. We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization.","sentences":["We present NewsQs","(news-cues), a dataset that provides question-answer pairs for multiple news documents.","To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus.","We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation.","We use a QNLI model with high correlation with human annotations to filter our data.","We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization."],"url":"http://arxiv.org/abs/2402.18479v1","category":"cs.CL"}
{"created":"2024-02-28 16:58:31","title":"Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes","abstract":"Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\". In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.","sentences":["Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance.","Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\".","In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent advances in signature kernels.","We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space.","Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph.","Assuming faithfulness and a CI oracle, our algorithm is sound and complete.","We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings."],"url":"http://arxiv.org/abs/2402.18477v1","category":"cs.LG"}
{"created":"2024-02-28 16:57:22","title":"IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding","abstract":"Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.","sentences":["Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations.","An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations.","In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique.","Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text.","We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions.","Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response."],"url":"http://arxiv.org/abs/2402.18476v1","category":"cs.CV"}
{"created":"2024-02-28 16:52:01","title":"Dynamic instabilities and turbulence of merged rotating Bose-Einstein condensates","abstract":"We present the simulation results of merging harmonically confined rotating Bose-Einstein condensates in two dimensions. Merging of the condensate is triggered by positioning the rotation axis at the trap minima and moving both condensates towards each other while slowly ramping their rotation frequency. We analyze the dynamics of the merged condensate by letting them evolve under a single harmonic trap. We systematically investigate the formation of solitonic and vortex structures in the final, unified condensate, considering both non-rotating and rotating initial states. In both cases, merging leads to the formation of solitons that decay into vortex pairs through snake instability and subsequently, these pairs annihilate. Soliton formation and decay-induced phase excitations generate sound waves, more pronounced when the merging time is short. We witness no sound wave generation at sufficiently longer merging times that finally leads to the condensate reaching its ground state. With rotation, we notice off-axis merging (where the rotation axes are not aligned), leading to the distortion and weakening of soliton formation. The incompressible kinetic energy spectrum exhibits a Kolmogorov-like cascade [$E(k) \\sim k^{-5/3}$] in the initial stage for merging condensates rotating above a critical frequency and a Vinen-like cascade [$E(k) \\sim k^{-1}$] at a later time for all cases. Our findings hold potential significance for atomic interferometry, continuous atomic lasers, and cosmic events related to the remnants of binary neutron-star mergers.","sentences":["We present the simulation results of merging harmonically confined rotating Bose-Einstein condensates in two dimensions.","Merging of the condensate is triggered by positioning the rotation axis at the trap minima and moving both condensates towards each other while slowly ramping their rotation frequency.","We analyze the dynamics of the merged condensate by letting them evolve under a single harmonic trap.","We systematically investigate the formation of solitonic and vortex structures in the final, unified condensate, considering both non-rotating and rotating initial states.","In both cases, merging leads to the formation of solitons that decay into vortex pairs through snake instability and subsequently, these pairs annihilate.","Soliton formation and decay-induced phase excitations generate sound waves, more pronounced when the merging time is short.","We witness no sound wave generation at sufficiently longer merging times that finally leads to the condensate reaching its ground state.","With rotation, we notice off-axis merging (where the rotation axes are not aligned), leading to the distortion and weakening of soliton formation.","The incompressible kinetic energy spectrum exhibits a Kolmogorov-like cascade [$E(k) \\sim k^{-5/3}$] in the initial stage for merging condensates rotating above a critical frequency and a Vinen-like cascade [$E(k) \\sim k^{-1}$] at a later time for all cases.","Our findings hold potential significance for atomic interferometry, continuous atomic lasers, and cosmic events related to the remnants of binary neutron-star mergers."],"url":"http://arxiv.org/abs/2402.18474v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-28 16:50:38","title":"Flux Quantization","abstract":"Flux- and charge-quantization laws for higher gauge fields of Maxwell type -- e.g. the common electromagnetic field (the \"A-field\") but also the B-, RR-, and C-fields considered in string/M-theory -- specify non-perturbative completions of these fields by encoding their solitonic behaviour and hence by specifying the discrete charges carried by the individual branes (higher-dimensional monopoles or solitons) that source the field fluxes.   This article surveys the general (rational-)homotopy theoretic understanding of flux- and charge-quantization via the Chern-Dold character map generalized to the non-linear (self-sourcing) Bianchi identities that appear in higher-dimensional supergravity theories, notably for B&RR-fields in D=10, for the C-field in D=11 supergravity, and for the B-field on fivebrane worldvolumes.","sentences":["Flux- and charge-quantization laws for higher gauge fields of Maxwell type -- e.g. the common electromagnetic field (the \"A-field\") but also the B-, RR-, and C-fields considered in string/M-theory -- specify non-perturbative completions of these fields by encoding their solitonic behaviour and hence by specifying the discrete charges carried by the individual branes (higher-dimensional monopoles or solitons) that source the field fluxes.   ","This article surveys the general (rational-)homotopy theoretic understanding of flux- and charge-quantization via the Chern-Dold character map generalized to the non-linear (self-sourcing) Bianchi identities that appear in higher-dimensional supergravity theories, notably for B&RR-fields in D=10, for the C-field in D=11 supergravity, and for the B-field on fivebrane worldvolumes."],"url":"http://arxiv.org/abs/2402.18473v1","category":"hep-th"}
{"created":"2024-02-28 16:50:05","title":"Implementing Online Reinforcement Learning with Clustering Neural Networks","abstract":"An agent employing reinforcement learning takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective. Rewards (positive or negative) guide the agent toward improved future actions. This paper builds on prior clustering neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes). The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition. Simulation results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method.","sentences":["An agent employing reinforcement learning takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective.","Rewards (positive or negative) guide the agent toward improved future actions.","This paper builds on prior clustering neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes).","The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition.","Simulation results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method."],"url":"http://arxiv.org/abs/2402.18472v1","category":"cs.NE"}
{"created":"2024-02-28 16:39:34","title":"Understanding the Impact of AI Generated Content on Social Media: The Pixiv Case","abstract":"In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms. The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design. Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem. To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations. Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023). Importantly, it allows both human and AI generated content to be uploaded. Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv. Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns. Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv.","sentences":["In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms.","The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design.","Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem.","To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations.","Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023).","Importantly, it allows both human and AI generated content to be uploaded.","Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv.","Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns.","Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv."],"url":"http://arxiv.org/abs/2402.18463v1","category":"cs.CY"}
{"created":"2024-02-28 16:38:40","title":"Energy conditions in the $f(R,L,T)$ theory of gravity","abstract":"We construct the energy conditions for the recently proposed $f(R,L,T)$ gravity theory, for which $f$ is a generic function of the Ricci scalar $R$, matter lagrangian density $L$ and trace of the energy-momentum tensor $T$. We analyse two different forms for the $f(R,L,T)$ function within the framework of the Friedmann-Lem\\^aitre-Robertson-Walker universe. We constrain the model parameters from the energy conditions. This approach allows us to assess the feasibility of specific forms of the $f(R,L,T)$ gravity.","sentences":["We construct the energy conditions for the recently proposed $f(R,L,T)$ gravity theory, for which $f$ is a generic function of the Ricci scalar $R$, matter lagrangian density $L$ and trace of the energy-momentum tensor $T$. We analyse two different forms for the $f(R,L,T)$ function within the framework of the Friedmann-Lem\\^aitre-Robertson-Walker universe.","We constrain the model parameters from the energy conditions.","This approach allows us to assess the feasibility of specific forms of the $f(R,L,T)$ gravity."],"url":"http://arxiv.org/abs/2402.18462v1","category":"gr-qc"}
{"created":"2024-02-28 16:36:19","title":"In-Person, Hybrid or Remote? Employers' Perspectives on the Future of Work Post-Pandemic","abstract":"We present an employer-side perspective on remote work through the pandemic using data from top executives of 129 employers in North America. Our analysis suggests that at least some of the pandemic-accelerated changes to the work location landscape will likely stick; with some form of hybrid work being the norm. However, the patterns will vary by department (HR/legal/sales/IT, etc.) and by sector of operations. Top three concerns among employers include: supervision and mentoring, reduction in innovation, and creativity; and the top three benefits include their ability to retain / recruit talent, positive impact on public image and their ability to compete. An Ordered Probit model of the expected April 2024 work location strategy revealed that those in transportation, warehousing, and manufacturing sectors, those with a fully in-person approach to work pre-COVID, and those with a negative outlook towards the impact of remote work are likely to be more in-person-centered, while those with fully remote work approach in April 2020 are likely to be less in-person-centered. Lastly, we present data on resumption of business travel, in-person client interactions and changes in office space reconfigurations that employers have made since the beginning of the pandemic.","sentences":["We present an employer-side perspective on remote work through the pandemic using data from top executives of 129 employers in North America.","Our analysis suggests that at least some of the pandemic-accelerated changes to the work location landscape will likely stick; with some form of hybrid work being the norm.","However, the patterns will vary by department (HR/legal/sales/IT, etc.)","and by sector of operations.","Top three concerns among employers include: supervision and mentoring, reduction in innovation, and creativity; and the top three benefits include their ability to retain / recruit talent, positive impact on public image and their ability to compete.","An Ordered Probit model of the expected April 2024 work location strategy revealed that those in transportation, warehousing, and manufacturing sectors, those with a fully in-person approach to work pre-COVID, and those with a negative outlook towards the impact of remote work are likely to be more in-person-centered, while those with fully remote work approach in April 2020 are likely to be less in-person-centered.","Lastly, we present data on resumption of business travel, in-person client interactions and changes in office space reconfigurations that employers have made since the beginning of the pandemic."],"url":"http://arxiv.org/abs/2402.18459v1","category":"econ.GN"}
{"created":"2024-02-28 16:35:52","title":"Meta-Task Prompting Elicits Embedding from Large Language Models","abstract":"In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.","sentences":["In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering.","Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects.","Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models.","Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios."],"url":"http://arxiv.org/abs/2402.18458v1","category":"cs.CL"}
{"created":"2024-02-28 16:31:04","title":"Dissecting a miniature universe: A multi-wavelength view of galaxy quenching in the Shapley supercluster","abstract":"Multiple-cluster systems, superclusters, contain large numbers of galaxies assembled in clusters inter-connected by multi-scale filamentary networks. As such, superclusters are a smaller version of the cosmic web and can be considered as miniature universes. Superclusters also contain gas, hot in the clusters and warmer in the filaments. Thus, they are ideal laboratories to study the interplay between the galaxies and the gas. In this context, the Shapley supercluster (SSC) stands out since it hosts the highest number of galaxies in the local universe. In addition, it is detected in both X-rays and via the thermal Sunyaev-Zel'dovich (tSZ) effect, making it ideal for a multi-wavelength study. Applying for the first time a filament-finder based on graphs, T-REx, on a spectroscopic galaxy catalogue, we uncovered the 3D filamentary network in and around SSC. Simultaneously, we used a large sample of photometric galaxies with information on their star formation rates (SFR) in order to investigate the quenching of star formation in the SSC environments which we define with the gas distribution in the Planck tSZ map and the ROSAT X-ray map. We confirm filaments already observed in the distribution of galaxies of the SSC, and detect new ones. We observe the quenching of star formation as a function of the gas, and show a general trend of decreasing SFR where the tSZ and X-ray signals are the highest. Within these regions, we also observe a rapid decline of the number of star-forming galaxies, coinciding with an increasing number of transitioning and passive galaxies. Within the filaments, the fraction of passive galaxies is larger than outside filaments, irrespective of the gas pressure. Our results suggest the zone of influence of the SSC, in which galaxies are pre-processed and quenched, is well defined by the tSZ signal that combines the density and temperature of the environments.","sentences":["Multiple-cluster systems, superclusters, contain large numbers of galaxies assembled in clusters inter-connected by multi-scale filamentary networks.","As such, superclusters are a smaller version of the cosmic web and can be considered as miniature universes.","Superclusters also contain gas, hot in the clusters and warmer in the filaments.","Thus, they are ideal laboratories to study the interplay between the galaxies and the gas.","In this context, the Shapley supercluster (SSC) stands out since it hosts the highest number of galaxies in the local universe.","In addition, it is detected in both X-rays and via the thermal Sunyaev-Zel'dovich (tSZ) effect, making it ideal for a multi-wavelength study.","Applying for the first time a filament-finder based on graphs, T-REx, on a spectroscopic galaxy catalogue, we uncovered the 3D filamentary network in and around SSC.","Simultaneously, we used a large sample of photometric galaxies with information on their star formation rates (SFR) in order to investigate the quenching of star formation in the SSC environments which we define with the gas distribution in the Planck tSZ map and the ROSAT X-ray map.","We confirm filaments already observed in the distribution of galaxies of the SSC, and detect new ones.","We observe the quenching of star formation as a function of the gas, and show a general trend of decreasing SFR where the tSZ and X-ray signals are the highest.","Within these regions, we also observe a rapid decline of the number of star-forming galaxies, coinciding with an increasing number of transitioning and passive galaxies.","Within the filaments, the fraction of passive galaxies is larger than outside filaments, irrespective of the gas pressure.","Our results suggest the zone of influence of the SSC, in which galaxies are pre-processed and quenched, is well defined by the tSZ signal that combines the density and temperature of the environments."],"url":"http://arxiv.org/abs/2402.18455v1","category":"astro-ph.CO"}
{"created":"2024-02-28 16:27:38","title":"Prospects for measuring time variation of astrophysical neutrino sources at dark matter detectors","abstract":"We study the prospects for measuring the time variation of solar and atmospheric neutrino fluxes at future large-scale Xenon and Argon dark matter detectors. For solar neutrinos, a yearly time variation arises from the eccentricity of the Earth's orbit, and, for charged current interactions, from a smaller energy-dependent day-night variation to due flavor regeneration as neutrinos travel through the Earth. For a 100-ton Xenon detector running for 10 years with a Xenon-136 fraction of $\\lesssim 0.1\\%$, in the electron recoil channel a time-variation amplitude of about 0.8\\% is detectable with a power of 90\\% and the level of significance of 10\\%. This is sufficient to detect time variation due to eccentricity, which has amplitude of $\\sim 3\\%$. In the nuclear recoil channel, the detectable amplitude is about 10\\% under current detector resolution and efficiency conditions, and this generally reduces to about 1\\% for improved detector resolution and efficiency, the latter of which is sufficient to detect time variation due to eccentricity. Our analysis assumes both known and unknown periods. We provide scalings to determine the sensitivity to an arbitrary time-varying amplitude as a function of detector parameters. Identifying the time variation of the neutrino fluxes will be important for distinguishing neutrinos from dark matter signals and other detector-related backgrounds, and extracting properties of neutrinos that can be uniquely studied in dark matter experiments.","sentences":["We study the prospects for measuring the time variation of solar and atmospheric neutrino fluxes at future large-scale Xenon and Argon dark matter detectors.","For solar neutrinos, a yearly time variation arises from the eccentricity of the Earth's orbit, and, for charged current interactions, from a smaller energy-dependent day-night variation to due flavor regeneration as neutrinos travel through the Earth.","For a 100-ton Xenon detector running for 10 years with a Xenon-136 fraction of $\\lesssim 0.1\\%$, in the electron recoil channel a time-variation amplitude of about 0.8\\% is detectable with a power of 90\\% and the level of significance of 10\\%.","This is sufficient to detect time variation due to eccentricity, which has amplitude of $\\sim 3\\%$. In the nuclear recoil channel, the detectable amplitude is about 10\\% under current detector resolution and efficiency conditions, and this generally reduces to about 1\\% for improved detector resolution and efficiency, the latter of which is sufficient to detect time variation due to eccentricity.","Our analysis assumes both known and unknown periods.","We provide scalings to determine the sensitivity to an arbitrary time-varying amplitude as a function of detector parameters.","Identifying the time variation of the neutrino fluxes will be important for distinguishing neutrinos from dark matter signals and other detector-related backgrounds, and extracting properties of neutrinos that can be uniquely studied in dark matter experiments."],"url":"http://arxiv.org/abs/2402.18454v1","category":"hep-ph"}
{"created":"2024-02-28 16:26:00","title":"Social Learning with Intrinsic Preferences","abstract":"Despite strong evidence for peer effects, little is known about how individuals balance intrinsic preferences and social learning in different choice environments. Using a combination of experiments and discrete choice modeling, we show that intrinsic preferences and social learning jointly influence participants' decisions, but their relative importance varies across choice tasks and environments. Intrinsic preferences guide participants' decisions in a subjective choice task, while social learning determines participants' decisions in a task with an objectively correct solution. A choice environment in which people expect to be rewarded for their choices reinforces the influence of intrinsic preferences, whereas an environment in which people expect to be punished for their choices reinforces conformist social learning. We use simulations to discuss the implications of these findings for the polarization of behavior.","sentences":["Despite strong evidence for peer effects, little is known about how individuals balance intrinsic preferences and social learning in different choice environments.","Using a combination of experiments and discrete choice modeling, we show that intrinsic preferences and social learning jointly influence participants' decisions, but their relative importance varies across choice tasks and environments.","Intrinsic preferences guide participants' decisions in a subjective choice task, while social learning determines participants' decisions in a task with an objectively correct solution.","A choice environment in which people expect to be rewarded for their choices reinforces the influence of intrinsic preferences, whereas an environment in which people expect to be punished for their choices reinforces conformist social learning.","We use simulations to discuss the implications of these findings for the polarization of behavior."],"url":"http://arxiv.org/abs/2402.18452v1","category":"econ.GN"}
{"created":"2024-02-28 16:24:08","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation","abstract":"The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at https://github.com/ayanglab/MambaMIR.","sentences":["The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks.","This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN.","Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model.","The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation.","Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods.","Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality.","The code is publicly available at https://github.com/ayanglab/MambaMIR."],"url":"http://arxiv.org/abs/2402.18451v1","category":"eess.IV"}
{"created":"2024-02-28 16:21:02","title":"HOP to the Next Tasks and Domains for Continual Learning in NLP","abstract":"Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP.","sentences":["Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones.","Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework.","Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem.","Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP."],"url":"http://arxiv.org/abs/2402.18449v1","category":"cs.CL"}
{"created":"2024-02-28 16:16:51","title":"Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization","abstract":"Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.","sentences":["Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains.","Existing works primarily focus on improving the generalization ability of static networks.","However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability.","Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios.","In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity.","Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts.","Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability.","Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection.","The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method."],"url":"http://arxiv.org/abs/2402.18447v1","category":"cs.CV"}
{"created":"2024-02-28 16:16:50","title":"Entanglement cost of discriminating quantum states under locality constraints","abstract":"The unique features of entanglement and non-locality in quantum systems, where there are pairs of bipartite states perfectly distinguishable by general entangled measurements yet indistinguishable by local operations and classical communication, hold significant importance in quantum entanglement theory, distributed quantum information processing, and quantum data hiding. This paper delves into the entanglement cost for discriminating two bipartite quantum states, employing positive operator-valued measures (POVMs) with positive partial transpose (PPT) to achieve optimal success probability through general entangled measurements. First, we introduce an efficiently computable quantity called the spectral PPT-distance of a POVM to quantify the localness of a general measurement. We show that it can be a lower bound for the entanglement cost of optimal discrimination by PPT POVMs. Second, we establish an upper bound on the entanglement cost of optimal discrimination by PPT POVMs for any pair of states. Leveraging this result, we show that a pure state can be optimally discriminated against any other state with the assistance of a single Bell state. This study advances our understanding of the pivotal role played by entanglement in quantum state discrimination, serving as a crucial element in unlocking quantum data hiding against locally constrained measurements.","sentences":["The unique features of entanglement and non-locality in quantum systems, where there are pairs of bipartite states perfectly distinguishable by general entangled measurements yet indistinguishable by local operations and classical communication, hold significant importance in quantum entanglement theory, distributed quantum information processing, and quantum data hiding.","This paper delves into the entanglement cost for discriminating two bipartite quantum states, employing positive operator-valued measures (POVMs) with positive partial transpose (PPT) to achieve optimal success probability through general entangled measurements.","First, we introduce an efficiently computable quantity called the spectral PPT-distance of a POVM to quantify the localness of a general measurement.","We show that it can be a lower bound for the entanglement cost of optimal discrimination by PPT POVMs.","Second, we establish an upper bound on the entanglement cost of optimal discrimination by PPT POVMs for any pair of states.","Leveraging this result, we show that a pure state can be optimally discriminated against any other state with the assistance of a single Bell state.","This study advances our understanding of the pivotal role played by entanglement in quantum state discrimination, serving as a crucial element in unlocking quantum data hiding against locally constrained measurements."],"url":"http://arxiv.org/abs/2402.18446v1","category":"quant-ph"}
{"created":"2024-02-28 16:15:57","title":"HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork","abstract":"There are still many challenges in Federated Learning (FL). First, during the model update process, the model parameters on the local user need to be sent to the server for aggregation. This involves the consumption of network bandwidth, especially when the number of users participating in FL is large. High communication costs may limit the application of FL in certain scenarios. Secondly, since users participating in FL usually have different data distributions, this heterogeneity of data may lead to poor model performance or even failure to converge. Third, privacy and security issues are also challenges that need to be addressed in FL. There is still a risk of information leakage during model aggregation. Malicious users may obtain sensitive information by analyzing communications during model updates or aggregation processes. To address these challenges, we propose HyperFedNet (HFN), an innovative approach that leverages hypernetwork. HFN introduces a paradigm shift in transmission aggregation within FL. Unlike traditional FL methods that transmit a large number of parameters from the main network, HFN reduces the communication burden and improves security by transmitting a compact set of hypernetwork parameters. After the parameters of the hypernetwork are deployed locally to the user, the local database features quantified by the embedding vector can be used as input, and parameters can be dynamically generated for the FL main network through user forward propagation. HFN efficiently reduces communication costs while improving accuracy. Extensive experimentation demonstrates that HFN outperforms traditional FL methods significantly. By seamlessly integrating this concept into the conventional FL algorithm, we achieve even more impressive results compared to the original approach.","sentences":["There are still many challenges in Federated Learning (FL).","First, during the model update process, the model parameters on the local user need to be sent to the server for aggregation.","This involves the consumption of network bandwidth, especially when the number of users participating in FL is large.","High communication costs may limit the application of FL in certain scenarios.","Secondly, since users participating in FL usually have different data distributions, this heterogeneity of data may lead to poor model performance or even failure to converge.","Third, privacy and security issues are also challenges that need to be addressed in FL.","There is still a risk of information leakage during model aggregation.","Malicious users may obtain sensitive information by analyzing communications during model updates or aggregation processes.","To address these challenges, we propose HyperFedNet (HFN), an innovative approach that leverages hypernetwork.","HFN introduces a paradigm shift in transmission aggregation within FL.","Unlike traditional FL methods that transmit a large number of parameters from the main network, HFN reduces the communication burden and improves security by transmitting a compact set of hypernetwork parameters.","After the parameters of the hypernetwork are deployed locally to the user, the local database features quantified by the embedding vector can be used as input, and parameters can be dynamically generated for the FL main network through user forward propagation.","HFN efficiently reduces communication costs while improving accuracy.","Extensive experimentation demonstrates that HFN outperforms traditional FL methods significantly.","By seamlessly integrating this concept into the conventional FL algorithm, we achieve even more impressive results compared to the original approach."],"url":"http://arxiv.org/abs/2402.18445v1","category":"cs.NI"}
{"created":"2024-02-28 16:13:55","title":"Silicon carbide soliton microcomb generation for narrow-grid optical communications","abstract":"A soliton microcomb can play a crucial role in narrow-grid optical communications by replacing many independently operated lasers in wavelength-division multiplexing systems. In this work, we designed and demonstrated power-efficient soliton microcombs with 100-GHz free spectral range in an integrated 4H-SiC platform for the first time. The combination of enabling technologies, including efficient fiber coupling (3 dB insertion loss), high-quality-factor microrings (intrinsic quality factors up to 5.7 million), and the employment of the Raman effect for adiabatic accessing of the soliton state, has enabled the demonstration of soliton pump power as low as 6 mW while supporting comb powers above -20 dBm per line near the pump wavelength.","sentences":["A soliton microcomb can play a crucial role in narrow-grid optical communications by replacing many independently operated lasers in wavelength-division multiplexing systems.","In this work, we designed and demonstrated power-efficient soliton microcombs with 100-GHz free spectral range in an integrated 4H-SiC platform for the first time.","The combination of enabling technologies, including efficient fiber coupling (3 dB insertion loss), high-quality-factor microrings (intrinsic quality factors up to 5.7 million), and the employment of the Raman effect for adiabatic accessing of the soliton state, has enabled the demonstration of soliton pump power as low as 6 mW while supporting comb powers above -20 dBm per line near the pump wavelength."],"url":"http://arxiv.org/abs/2402.18444v1","category":"physics.optics"}
{"created":"2024-02-28 16:13:44","title":"LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs","abstract":"Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user.","sentences":["Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge.","This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions.","In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge.","The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters.","We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component.","We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user."],"url":"http://arxiv.org/abs/2402.18443v1","category":"cs.LG"}
{"created":"2024-02-28 16:12:17","title":"Norming Markushevich bases: recent results and open problems","abstract":"We survey several results concerning norming Markushevich bases (M-bases, for short), focusing in particular on two recent examples of a weakly compactly generated Banach space with no norming M-basis and of an Asplund space with norming M-basis that is not weakly compactly generated. We highlight the context for these problems and state several open problems in different directions that arise from these results.","sentences":["We survey several results concerning norming Markushevich bases (M-bases, for short), focusing in particular on two recent examples of a weakly compactly generated Banach space with no norming M-basis and of an Asplund space with norming M-basis that is not weakly compactly generated.","We highlight the context for these problems and state several open problems in different directions that arise from these results."],"url":"http://arxiv.org/abs/2402.18442v1","category":"math.FA"}
{"created":"2024-02-28 16:07:54","title":"Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication","abstract":"Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \\url{https://github.com/thunlp/AutoForm}.","sentences":["Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs).","Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression.","NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined.","In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts.","We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness.","Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs.","Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication.","Our code is released at \\url{https://github.com/thunlp/AutoForm}."],"url":"http://arxiv.org/abs/2402.18439v1","category":"cs.CL"}
{"created":"2024-02-28 16:06:02","title":"Fast and spurious: a robust determination of our peculiar velocity with future galaxy surveys","abstract":"To date, the most precise measurement of the observer's peculiar velocity comes from the dipole in the Cosmic Microwave Background (CMB). This velocity also generates a dipole in the source number counts, whose amplitude is governed not only by the observer velocity, but also by specific properties of the sources, that are difficult to determine precisely. Quantitative studies of the source number counts currently give dipoles which are reasonably well aligned with the CMB dipole, but with a significantly larger amplitude than that of the CMB dipole. In this work, we explore an alternative way of measuring the observer velocity from the source number counts, using correlations between neighboring spherical harmonic coefficients, induced by the velocity. We show that these correlations contain both a term sensitive to the source properties and another one directly given by the observer velocity. We explore the potential of a Euclid-like survey to directly measure this second contribution, independently of the characteristics of the population of sources. We find that the method can reach a precision of 4%, corresponding to a detection significance of 24 sigma, on the observer velocity. This will settle with precision the present \"dipole tension\".","sentences":["To date, the most precise measurement of the observer's peculiar velocity comes from the dipole in the Cosmic Microwave Background (CMB).","This velocity also generates a dipole in the source number counts, whose amplitude is governed not only by the observer velocity, but also by specific properties of the sources, that are difficult to determine precisely.","Quantitative studies of the source number counts currently give dipoles which are reasonably well aligned with the CMB dipole, but with a significantly larger amplitude than that of the CMB dipole.","In this work, we explore an alternative way of measuring the observer velocity from the source number counts, using correlations between neighboring spherical harmonic coefficients, induced by the velocity.","We show that these correlations contain both a term sensitive to the source properties and another one directly given by the observer velocity.","We explore the potential of a Euclid-like survey to directly measure this second contribution, independently of the characteristics of the population of sources.","We find that the method can reach a precision of 4%, corresponding to a detection significance of 24 sigma, on the observer velocity.","This will settle with precision the present \"dipole tension\"."],"url":"http://arxiv.org/abs/2402.18438v1","category":"astro-ph.CO"}
{"created":"2024-02-28 16:00:47","title":"Stochastic User Equilibrium Model with a Bounded Perceived Travel Time","abstract":"Stochastic User Equilibrium (SUE) models depict the perception differences in traffic assignment problems. According to the assumption of an unbounded perceived travel time distribution, the conventional SUE problems result in a positive choice probability for all available routes, regardless of their unappealing travel time. This study provides an eUnit-SUE model to relax this assumption. The eUnit model is derived from a bounded probability distribution. This closed-form model aligns with an exponentiated random utility maximization (ERUM) paradigm with the exponentiated uniform distributed random error, where the lower and upper bounds endogeneously determine the route usage. Specifically, a Beckmann-type mathematical programming formulation is presented for the eUnit-SUE problem. The equivalency and uniqueness properties are rigorously proven. Numerical examples reveal that the eUnit bound range between the lower and upper bounds greatly affects the SUE assignment results. A larger bound range increases not only the number of routes in the choice set but also the degree of dispersion in the assignment results due to a larger route-specific perception variance. The misperception is contingent upon the disparity between the shortest and longest travel times and the bounds. As the bound range decreases, the shortest route receives significant flow allocation, and the assignment result approaches the deterministic user equilibrium (DUE) flow pattern.","sentences":["Stochastic User Equilibrium (SUE) models depict the perception differences in traffic assignment problems.","According to the assumption of an unbounded perceived travel time distribution, the conventional SUE problems result in a positive choice probability for all available routes, regardless of their unappealing travel time.","This study provides an eUnit-SUE model to relax this assumption.","The eUnit model is derived from a bounded probability distribution.","This closed-form model aligns with an exponentiated random utility maximization (ERUM) paradigm with the exponentiated uniform distributed random error, where the lower and upper bounds endogeneously determine the route usage.","Specifically, a Beckmann-type mathematical programming formulation is presented for the eUnit-SUE problem.","The equivalency and uniqueness properties are rigorously proven.","Numerical examples reveal that the eUnit bound range between the lower and upper bounds greatly affects the SUE assignment results.","A larger bound range increases not only the number of routes in the choice set but also the degree of dispersion in the assignment results due to a larger route-specific perception variance.","The misperception is contingent upon the disparity between the shortest and longest travel times and the bounds.","As the bound range decreases, the shortest route receives significant flow allocation, and the assignment result approaches the deterministic user equilibrium (DUE) flow pattern."],"url":"http://arxiv.org/abs/2402.18435v1","category":"econ.GN"}
{"created":"2024-02-28 15:57:22","title":"Universal neural network potentials as descriptors: Towards scalable chemical property prediction using quantum and classical computers","abstract":"Accurate prediction of diverse chemical properties is crucial for advancing molecular design and materials discovery. Here we present a versatile approach that uses the intermediate information of a universal neural network potential as a general-purpose descriptor for chemical property prediction. Our method is based on the insight that by training a sophisticated neural network architecture for universal force fields, it learns transferable representations of atomic environments. We show that transfer learning with a graph neural network potential M3GNet achieves accuracy comparable to state-of-the-art methods for predicting the NMR chemical shifts of$^1$H, $^{13}$C, $^{15}$N, $^{17}$O, and $^{19}$F using quantum machine learning as well as a standard classical regression model, despite the compactness of its descriptors. This work provides an efficient way to accurately predict properties, potentially accelerating the discovery of new molecules and materials.","sentences":["Accurate prediction of diverse chemical properties is crucial for advancing molecular design and materials discovery.","Here we present a versatile approach that uses the intermediate information of a universal neural network potential as a general-purpose descriptor for chemical property prediction.","Our method is based on the insight that by training a sophisticated neural network architecture for universal force fields, it learns transferable representations of atomic environments.","We show that transfer learning with a graph neural network potential M3GNet achieves accuracy comparable to state-of-the-art methods for predicting the NMR chemical shifts of$^1$H, $^{13}$C, $^{15}$N, $^{17}$O, and $^{19}$F using quantum machine learning as well as a standard classical regression model, despite the compactness of its descriptors.","This work provides an efficient way to accurately predict properties, potentially accelerating the discovery of new molecules and materials."],"url":"http://arxiv.org/abs/2402.18433v1","category":"quant-ph"}
{"created":"2024-02-28 15:56:38","title":"NEC violation in $f(\\bar{R},\\bar{T})$ gravity in the context of a non-canonical theory via modified Raychaudhuri equation","abstract":"In this work, we have developed the Raychaudhuri equation in $f(\\bar{R},\\bar{T})$ gravity in the setting of a non-canonical theory, namely K-essence theory. We solve the modified Raychaudhuri equation for the additive form of $f(\\bar{R},\\bar{T})$, which is $f_{1}(\\bar{R})+f_{2}(\\bar{T})$. For this solution, we employ two different scale factors to give two types of $f(\\bar{R},\\bar{T})$ solutions. By conducting a viability test and analyzing energy conditions, we have determined that in the first scenario, the null energy condition (NEC) is violated between two regions where the NEC is satisfied. Additionally, we have observed that this violation of the NEC exhibits a symmetric property during the phase transition. These observations indicate that bouncing events may occur as a result of the symmetrical violation of the NEC during the expansion of the universe. Once again, this model indicates that resonant-type quantum tunneling may take place during the period where the NEC is violated. In the second scenario, our model indicates that the strong energy condition is violated, but the NEC and weak energy conditions are satisfied. The effective energy density decreases and is positive, while the effective pressure and equation of state parameters are negative. This suggests that the universe is expanding with acceleration and is dominated by dark energy.","sentences":["In this work, we have developed the Raychaudhuri equation in $f(\\bar{R},\\bar{T})$ gravity in the setting of a non-canonical theory, namely K-essence theory.","We solve the modified Raychaudhuri equation for the additive form of $f(\\bar{R},\\bar{T})$, which is $f_{1}(\\bar{R})+f_{2}(\\bar{T})$.","For this solution, we employ two different scale factors to give two types of $f(\\bar{R},\\bar{T})$ solutions.","By conducting a viability test and analyzing energy conditions, we have determined that in the first scenario, the null energy condition (NEC) is violated between two regions where the NEC is satisfied.","Additionally, we have observed that this violation of the NEC exhibits a symmetric property during the phase transition.","These observations indicate that bouncing events may occur as a result of the symmetrical violation of the NEC during the expansion of the universe.","Once again, this model indicates that resonant-type quantum tunneling may take place during the period where the NEC is violated.","In the second scenario, our model indicates that the strong energy condition is violated, but the NEC and weak energy conditions are satisfied.","The effective energy density decreases and is positive, while the effective pressure and equation of state parameters are negative.","This suggests that the universe is expanding with acceleration and is dominated by dark energy."],"url":"http://arxiv.org/abs/2402.18431v1","category":"gr-qc"}
{"created":"2024-02-28 15:55:02","title":"Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation","abstract":"Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students. To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models. Extensive experiments on four widely used benchmarks show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 BLEU scores respectively, and can also outperform the current best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding.","sentences":["Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT).","AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations.","NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation.","Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models.","However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models.","In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students.","To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models.","Extensive experiments on four widely used benchmarks show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 BLEU scores respectively, and can also outperform the current best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding."],"url":"http://arxiv.org/abs/2402.18428v1","category":"cs.CL"}
{"created":"2024-02-28 15:51:05","title":"A Relational Inductive Bias for Dimensional Abstraction in Neural Networks","abstract":"The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility. Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols.","sentences":["The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment.","In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training.","This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing.","We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases.","Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility.","Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols."],"url":"http://arxiv.org/abs/2402.18426v1","category":"cs.AI"}
{"created":"2024-02-28 15:46:09","title":"Emotion Classification in Low and Moderate Resource Languages","abstract":"It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \\textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani. Our results indicate that our approaches outperform random baselines and transfer emotions across languages successfully. For all languages, the direct cross-lingual transfer of emotion yields better results. We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia.","sentences":["It is important to be able to analyze the emotional state of people around the globe.","There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive.","Particularly for low-resource and endangered languages, building emotion classification can be quite challenging.","We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \\textit{English} in our work) and transfer the learning to low and moderate resource languages.","We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language.","One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages.","We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani.","Our results indicate that our approaches outperform random baselines and transfer emotions across languages successfully.","For all languages, the direct cross-lingual transfer of emotion yields better results.","We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia."],"url":"http://arxiv.org/abs/2402.18424v1","category":"cs.CL"}
{"created":"2024-02-28 15:42:23","title":"Generalised Hydrodynamics description of the Page curve-like dynamics of a freely expanding fermionic gas","abstract":"We consider an analytically tractable model that exhibits the main features of the Page curve characterizing the evolution of entanglement entropy during evaporation of a black hole. Our model is a gas of non-interacting fermions on a lattice that is released from a box into the vacuum. More precisely, our Hamiltonian is a tight-binding model with a defect at the junction between the filled box and the vacuum. In addition to the entanglement entropy we consider several other observables, such as the spatial density profile and current, and show that the semiclassical approach of generalized hydrodynamics provides a remarkably accurate description of the quantum dynamics including that of the entanglement entropy at all times. Our hydrodynamic results agree closely with those obtained via exact microscopic numerics. We find that the growth of entanglement is linear and universal, i.e, independent of the details of the defect. The decay shows $1/t$ scaling for conformal defect while for non-conformal defects, it is slower. Our study shows the power of the semiclassical approach and could be relevant for discussions on the resolution of the black hole information paradox.","sentences":["We consider an analytically tractable model that exhibits the main features of the Page curve characterizing the evolution of entanglement entropy during evaporation of a black hole.","Our model is a gas of non-interacting fermions on a lattice that is released from a box into the vacuum.","More precisely, our Hamiltonian is a tight-binding model with a defect at the junction between the filled box and the vacuum.","In addition to the entanglement entropy we consider several other observables, such as the spatial density profile and current, and show that the semiclassical approach of generalized hydrodynamics provides a remarkably accurate description of the quantum dynamics including that of the entanglement entropy at all times.","Our hydrodynamic results agree closely with those obtained via exact microscopic numerics.","We find that the growth of entanglement is linear and universal, i.e, independent of the details of the defect.","The decay shows $1/t$ scaling for conformal defect while for non-conformal defects, it is slower.","Our study shows the power of the semiclassical approach and could be relevant for discussions on the resolution of the black hole information paradox."],"url":"http://arxiv.org/abs/2402.18422v1","category":"quant-ph"}
{"created":"2024-02-28 15:41:12","title":"CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots","abstract":"When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the challenges is kinematic modeling. Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue. Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications. By observing the topology within CDPRs, in this letter, we propose a graph-based representation to model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging Graph Neural Network (GNN). Extensive experiments are conducted on 3D and 2D CDPRs across various configurations, including under-constrained, fully-constrained, and over-constrained cases, in both simulation environments and real-world scenarios. The experimental results showcase that CafkNet can learn the internal topological information of CDPRs and accurately solve the FK problem as an FK solver. Furthermore, training the CafkNet model on partial configurations enables zero-shot generalization to other configurations. Lastly, CafkNet effectively bridges the sim2real gap by using both simulation data and part of real-world data. To the best of our knowledge, it is the first study that employs the GNN to solve the FK problem for CDPRs.","sentences":["When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the challenges is kinematic modeling.","Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue.","Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications.","By observing the topology within CDPRs, in this letter, we propose a graph-based representation to model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging Graph Neural Network (GNN).","Extensive experiments are conducted on 3D and 2D CDPRs across various configurations, including under-constrained, fully-constrained, and over-constrained cases, in both simulation environments and real-world scenarios.","The experimental results showcase that CafkNet can learn the internal topological information of CDPRs and accurately solve the FK problem as an FK solver.","Furthermore, training the CafkNet model on partial configurations enables zero-shot generalization to other configurations.","Lastly, CafkNet effectively bridges the sim2real gap by using both simulation data and part of real-world data.","To the best of our knowledge, it is the first study that employs the GNN to solve the FK problem for CDPRs."],"url":"http://arxiv.org/abs/2402.18420v1","category":"cs.RO"}
{"created":"2024-02-28 15:39:53","title":"Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?","abstract":"Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach. Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts.","sentences":["Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage.","For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task.","One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc.","In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster.","We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record.","We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique.","Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach.","Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts."],"url":"http://arxiv.org/abs/2402.18419v1","category":"cs.CL"}
{"created":"2024-02-28 15:31:45","title":"Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport","abstract":"Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT.","sentences":["Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data.","Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment.","However, these segregated strategies overlook the potential synergies between these tasks.","This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework.","ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR.","By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios.","Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning.","This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness.","ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets.","Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%.","Code is available at https://github.com/HCVLAB/ProtoOT."],"url":"http://arxiv.org/abs/2402.18411v1","category":"cs.CV"}
{"created":"2024-02-28 15:28:36","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models","abstract":"Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.","sentences":["Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities.","Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics.","It defines eight reasoning capabilities and consists of an image description task and a visual question answering task.","Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans."],"url":"http://arxiv.org/abs/2402.18409v1","category":"cs.AI"}
{"created":"2024-02-28 15:26:54","title":"Polarization entanglement by two simultaneous backward phase-matching processes in a single crystal","abstract":"Entanglement enables many promising applications in quantum technology. Devising new generation methods and harnessing entanglement are prerequisites for practical applications. Here we realize a distinct polarization-entangled source by simultaneously achieving type-0 and type-I backward quasi-phase matching (BQPM) through spontaneous parametric down-conversion in a single bulk crystal, which is different from all previous entangled-source configurations. Pumping the crystal with a single polarized beam generates a non-maximally polarization-entangled state, which can be further projected to a maximal Bell state with a pair of Brewster windows. Hong-Ou-Mandel interference experiments are done on polarization-degenerate photon pairs for both type-0 and type-I BQPM processes for the first time. The emitted photons in both processes have a bandwidth as narrow as 15.7 GHz. The high quality of this source is characterized by various methods. The rather simple configuration, narrow bandwidth, and high entanglement quality make the source very promising for many quantum information tasks.","sentences":["Entanglement enables many promising applications in quantum technology.","Devising new generation methods and harnessing entanglement are prerequisites for practical applications.","Here we realize a distinct polarization-entangled source by simultaneously achieving type-0 and type-I backward quasi-phase matching (BQPM) through spontaneous parametric down-conversion in a single bulk crystal, which is different from all previous entangled-source configurations.","Pumping the crystal with a single polarized beam generates a non-maximally polarization-entangled state, which can be further projected to a maximal Bell state with a pair of Brewster windows.","Hong-Ou-Mandel interference experiments are done on polarization-degenerate photon pairs for both type-0 and type-I BQPM processes for the first time.","The emitted photons in both processes have a bandwidth as narrow as 15.7 GHz.","The high quality of this source is characterized by various methods.","The rather simple configuration, narrow bandwidth, and high entanglement quality make the source very promising for many quantum information tasks."],"url":"http://arxiv.org/abs/2402.18404v1","category":"quant-ph"}
{"created":"2024-02-28 15:15:39","title":"Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models","abstract":"Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge.","sentences":["Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities.","This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks.","Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label.","We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs.","Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings.","Further analysis reveals the influence of evaluation methods and the use of instructions in prompts.","Our multilingual investigation shows that English-centric language models perform better on average than multilingual models.","Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge."],"url":"http://arxiv.org/abs/2402.18397v1","category":"cs.CL"}
{"created":"2024-02-28 15:15:23","title":"Deep Confident Steps to New Pockets: Strategies for Docking Generalization","abstract":"Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks. Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.","sentences":["Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome.","Existing benchmarks, however, fail to rigorously assess generalizability.","Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities.","We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks.","Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models.","We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods."],"url":"http://arxiv.org/abs/2402.18396v1","category":"q-bio.BM"}
{"created":"2024-02-28 15:13:42","title":"Dual-IMU State Estimation for Relative Localization of Two Mobile Agents","abstract":"In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world.","sentences":["In this paper, we address the problem of relative localization of two mobile agents.","Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them.","Previous works, however, typically assumed known ego motion and ignored biases of the IMUs.","Instead, we study the most general case of unknown biases for both IMUs.","Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions.","Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency.","Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world."],"url":"http://arxiv.org/abs/2402.18394v1","category":"cs.RO"}
{"created":"2024-02-28 15:13:33","title":"Evaluating Decision Optimality of Autonomous Driving via Metamorphic Testing","abstract":"Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions. In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs. Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions. The MR identifies the property that the ADS should retain optimal decisions when the optimal path remains unaffected by non-invasive changes. Subsequently, we develop a new framework, Decictor, designed to generate NoDSs efficiently. Decictor comprises three main components: Non-invasive Mutation, MR Check, and Feedback. The Non-invasive Mutation ensures that the original optimal path in the mutated scenarios is not affected, while the MR Check is responsible for determining whether non-optimal decisions are made. To enhance the effectiveness of identifying NoDSs, we design a feedback metric that combines both spatial and temporal aspects of the AV's movement. We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS. The experimental results validate the effectiveness of Decictor in detecting non-optimal decisions of ADSs. Our work provides valuable and original insights into evaluating the non-safety-critical performance of ADSs.","sentences":["Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety.","However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs.","Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions.","In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs.","Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions.","The MR identifies the property that the ADS should retain optimal decisions when the optimal path remains unaffected by non-invasive changes.","Subsequently, we develop a new framework, Decictor, designed to generate NoDSs efficiently.","Decictor comprises three main components: Non-invasive Mutation, MR Check, and Feedback.","The Non-invasive Mutation ensures that the original optimal path in the mutated scenarios is not affected, while the MR Check is responsible for determining whether non-optimal decisions are made.","To enhance the effectiveness of identifying NoDSs, we design a feedback metric that combines both spatial and temporal aspects of the AV's movement.","We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS.","The experimental results validate the effectiveness of Decictor in detecting non-optimal decisions of ADSs.","Our work provides valuable and original insights into evaluating the non-safety-critical performance of ADSs."],"url":"http://arxiv.org/abs/2402.18393v1","category":"cs.AI"}
{"created":"2024-02-28 15:12:24","title":"Unveiling the Potential of Robustness in Evaluating Causal Inference Models","abstract":"The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection. The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator. Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones.","sentences":["The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE).","The intersection of machine learning and causal inference has yielded various effective CATE estimators.","However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation.","Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges.","Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners.","Secondly, they lack a specific focus on selecting a robust estimator.","To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection.","The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator.","Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones."],"url":"http://arxiv.org/abs/2402.18392v1","category":"cs.LG"}
{"created":"2024-02-28 15:11:39","title":"Sound Concurrent Traces for Online Monitoring Technical Report","abstract":"Monitoring concurrent programs typically rely on collecting traces to abstract program executions. However, existing approaches targeting general behavioral properties are either not tailored for online monitoring, are no longer maintained, or implement naive instrumentation that often leads to unsound verdicts. We first define the notion of when a trace is representative of a concurrent execution. We then present a non-blocking vector clock algorithm to collect sound concurrent traces on the fly reflecting the partial order between events. Moreover, concurrent events in the representative trace pose a soundness problem for monitors synthesized from total order formalisms. For this, we extract a causal dependence relation from the monitor to check if the trace has the needed orderings and define the conditions to decide at runtime when a collected trace is monitorable. We implement our contributions in a tool, FACTS, which instruments programs compiling to Java bytecode, constructs sound representative traces, and warns the monitor about non-monitorable traces. We evaluate our work and compare it with existing approaches.","sentences":["Monitoring concurrent programs typically rely on collecting traces to abstract program executions.","However, existing approaches targeting general behavioral properties are either not tailored for online monitoring, are no longer maintained, or implement naive instrumentation that often leads to unsound verdicts.","We first define the notion of when a trace is representative of a concurrent execution.","We then present a non-blocking vector clock algorithm to collect sound concurrent traces on the fly reflecting the partial order between events.","Moreover, concurrent events in the representative trace pose a soundness problem for monitors synthesized from total order formalisms.","For this, we extract a causal dependence relation from the monitor to check if the trace has the needed orderings and define the conditions to decide at runtime when a collected trace is monitorable.","We implement our contributions in a tool, FACTS, which instruments programs compiling to Java bytecode, constructs sound representative traces, and warns the monitor about non-monitorable traces.","We evaluate our work and compare it with existing approaches."],"url":"http://arxiv.org/abs/2402.18391v1","category":"cs.SE"}
{"created":"2024-02-28 15:11:02","title":"Neuromorphic Event-Driven Semantic Communication in Microgrids","abstract":"Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes. As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs. Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids.","sentences":["Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids.","On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer.","This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance.","Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes.","As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs.","Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids."],"url":"http://arxiv.org/abs/2402.18390v1","category":"cs.ET"}
{"created":"2024-02-28 15:05:43","title":"The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA","abstract":"Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the \"Conversational Multi-Doc QA\" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.","sentences":["Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations.","In this paper, we introduce our winning approach for the \"Conversational Multi-Doc QA\" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs).","We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data.","Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble.","Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent.","The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024."],"url":"http://arxiv.org/abs/2402.18385v1","category":"cs.CL"}
{"created":"2024-02-28 15:02:17","title":"Large Language Models As Evolution Strategies","abstract":"Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks. Hence, LLMs can act as `plug-in' in-context recombination operators. We provide several comparative studies of the LLM's model size, prompt strategy, and context construction. Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories.","sentences":["Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms.","These include gradient descent, classification, sequence completion, transformation, and improvement.","In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms.","While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization.","We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation.","Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks.","Hence, LLMs can act as `plug-in' in-context recombination operators.","We provide several comparative studies of the LLM's model size, prompt strategy, and context construction.","Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories."],"url":"http://arxiv.org/abs/2402.18381v1","category":"cs.AI"}
{"created":"2024-02-28 15:00:25","title":"Embracing Disorder in Quantum Materials Design","abstract":"Many of the most exciting materials discoveries in fundamental condensed matter physics are made in systems hosting some degree of intrinsic disorder. While disorder has historically been regarded as something to be avoided in materials design, it is often of central importance to correlated and quantum materials. This is largely driven by the conceptual and theoretical ease to handle, predict, and understand highly uniform systems that exhibit complex interactions, symmetries and band structures. In this perspective, we highlight how flipping this paradigm has enabled exciting possibilities in the emerging field of high entropy oxide (HEO) quantum materials. These materials host high levels of cation or anion compositional disorder while maintaining unexpectedly uniform single crystal lattices. The diversity of atomic scale interactions of spin, charge, orbital, and lattice degrees of freedom are found to emerge into coherent properties on much larger length scales. Thus, altering the variance and magnitudes of the atomic scale properties through elemental selection can open new routes to tune global correlated phases such as magnetism, metal-insulator transitions, ferroelectricity, and even emergent topological responses. The strategy of embracing disorder in this way provides a much broader pallet from which functional states can be designed for next-generation microelectronic and quantum information systems.","sentences":["Many of the most exciting materials discoveries in fundamental condensed matter physics are made in systems hosting some degree of intrinsic disorder.","While disorder has historically been regarded as something to be avoided in materials design, it is often of central importance to correlated and quantum materials.","This is largely driven by the conceptual and theoretical ease to handle, predict, and understand highly uniform systems that exhibit complex interactions, symmetries and band structures.","In this perspective, we highlight how flipping this paradigm has enabled exciting possibilities in the emerging field of high entropy oxide (HEO) quantum materials.","These materials host high levels of cation or anion compositional disorder while maintaining unexpectedly uniform single crystal lattices.","The diversity of atomic scale interactions of spin, charge, orbital, and lattice degrees of freedom are found to emerge into coherent properties on much larger length scales.","Thus, altering the variance and magnitudes of the atomic scale properties through elemental selection can open new routes to tune global correlated phases such as magnetism, metal-insulator transitions, ferroelectricity, and even emergent topological responses.","The strategy of embracing disorder in this way provides a much broader pallet from which functional states can be designed for next-generation microelectronic and quantum information systems."],"url":"http://arxiv.org/abs/2402.18379v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 14:57:52","title":"Computation-information gap in high-dimensional clustering","abstract":"We investigate the existence of a fundamental computation-information gap for the problem of clustering a mixture of isotropic Gaussian in the high-dimensional regime, where the ambient dimension $p$ is larger than the number $n$ of points. The existence of a computation-information gap in a specific Bayesian high-dimensional asymptotic regime has been conjectured by arXiv:1610.02918 based on the replica heuristic from statistical physics. We provide evidence of the existence of such a gap generically in the high-dimensional regime $p \\geq n$, by (i) proving a non-asymptotic low-degree polynomials computational barrier for clustering in high-dimension, matching the performance of the best known polynomial time algorithms, and by (ii) establishing that the information barrier for clustering is smaller than the computational barrier, when the number $K$ of clusters is large enough. These results are in contrast with the (moderately) low-dimensional regime $n \\geq poly(p, K)$, where there is no computation-information gap for clustering a mixture of isotropic Gaussian. In order to prove our low-degree computational barrier, we develop sophisticated combinatorial arguments to upper-bound the mixed moments of the signal under a Bernoulli Bayesian model.","sentences":["We investigate the existence of a fundamental computation-information gap for the problem of clustering a mixture of isotropic Gaussian in the high-dimensional regime, where the ambient dimension $p$ is larger than the number $n$ of points.","The existence of a computation-information gap in a specific Bayesian high-dimensional asymptotic regime has been conjectured by arXiv:1610.02918 based on the replica heuristic from statistical physics.","We provide evidence of the existence of such a gap generically in the high-dimensional regime $p \\geq n$, by (i) proving a non-asymptotic low-degree polynomials computational barrier for clustering in high-dimension, matching the performance of the best known polynomial time algorithms, and by (ii) establishing that the information barrier for clustering is smaller than the computational barrier, when the number $K$ of clusters is large enough.","These results are in contrast with the (moderately) low-dimensional regime $n \\geq poly(p, K)$, where there is no computation-information gap for clustering a mixture of isotropic Gaussian.","In order to prove our low-degree computational barrier, we develop sophisticated combinatorial arguments to upper-bound the mixed moments of the signal under a Bernoulli Bayesian model."],"url":"http://arxiv.org/abs/2402.18378v1","category":"math.ST"}
{"created":"2024-02-28 14:52:58","title":"Out-of-Domain Generalization in Dynamical Systems Reconstruction","abstract":"In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.","sentences":["In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena.","While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data.","State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge.","Yet, this is a crucial property we would expect from any viable scientific theory.","In this work, we provide a formal framework that addresses generalization in DSR.","We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning.","We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model.","We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model.","We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space.","Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice."],"url":"http://arxiv.org/abs/2402.18377v1","category":"cs.LG"}
{"created":"2024-02-28 14:52:15","title":"Tokenization Is More Than Compression","abstract":"Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.","sentences":["Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models.","Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens.","We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary.","Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization.","To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers.","Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction.","We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available."],"url":"http://arxiv.org/abs/2402.18376v1","category":"cs.CL"}
{"created":"2024-02-28 14:50:27","title":"Low-Modeling of Software Systems","abstract":"There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.","sentences":["There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems.","New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle.","In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage.","In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems."],"url":"http://arxiv.org/abs/2402.18375v1","category":"cs.SE"}
{"created":"2024-02-28 14:46:38","title":"Intersection the twin dragon with rational lines","abstract":"The Knuth Twin Dragon is a compact subset of the plane with fractal boundary of Hausdorff dimension $s = (\\log \\lambda)/(\\log \\sqrt{2})$, $\\lambda^3 = \\lambda^2 + 2$. Although the intersection with a generic line has Hausdorff dimension $s-1$, we prove that this does not occur for lines with rational parameters. We further describe the intersection of the Twin Dragon with the two diagonals as well as with various axis parallel lines.","sentences":["The Knuth Twin Dragon is a compact subset of the plane with fractal boundary of Hausdorff dimension $s = (\\log \\lambda)/(\\log \\sqrt{2})$, $\\lambda^3 = \\lambda^2 + 2$.","Although the intersection with a generic line has Hausdorff dimension $s-1$, we prove that this does not occur for lines with rational parameters.","We further describe the intersection of the Twin Dragon with the two diagonals as well as with various axis parallel lines."],"url":"http://arxiv.org/abs/2402.18371v1","category":"math.MG"}
{"created":"2024-02-28 14:42:29","title":"TDiff invariant gauge fields in cosmology","abstract":"We study the dynamics of Abelian gauge fields invariant under transverse diffeomorphisms (TDiff) in cosmological contexts. We show that in the geometric optics approximation, very much as for Diff invariant theories, the corresponding massless gauge bosons propagate along null geodesics and particle number is conserved. In addition, the polarization vectors are orthogonal to the propagation direction and the physical (transverse projection) polarization is parallel transported along the geodesics. We also consider TDiff invariant Dirac spinors, study the coupling to the gauge fields and analyze the conditions in order to avoid violations of Einstein's Equivalence Principle. The contributions to the energy-momentum tensor of the gauge field are also analyzed. We find that, in general, the breaking of Diff invariance makes the electric and magnetic parts of the vector field to gravitate in a different way. In the sub-Hubble regime we recover the standard radiation-like behaviour of the energy density, however in the super-Hubble regime the behaviour is totally different to the Diff case, thus opening up a wide range of possibilities for cosmological model building. In particular, possible effects on the evolution of large-scale primordial magnetic fields are discussed.","sentences":["We study the dynamics of Abelian gauge fields invariant under transverse diffeomorphisms (TDiff) in cosmological contexts.","We show that in the geometric optics approximation, very much as for Diff invariant theories, the corresponding massless gauge bosons propagate along null geodesics and particle number is conserved.","In addition, the polarization vectors are orthogonal to the propagation direction and the physical (transverse projection) polarization is parallel transported along the geodesics.","We also consider TDiff invariant Dirac spinors, study the coupling to the gauge fields and analyze the conditions in order to avoid violations of Einstein's Equivalence Principle.","The contributions to the energy-momentum tensor of the gauge field are also analyzed.","We find that, in general, the breaking of Diff invariance makes the electric and magnetic parts of the vector field to gravitate in a different way.","In the sub-Hubble regime we recover the standard radiation-like behaviour of the energy density, however in the super-Hubble regime the behaviour is totally different to the Diff case, thus opening up a wide range of possibilities for cosmological model building.","In particular, possible effects on the evolution of large-scale primordial magnetic fields are discussed."],"url":"http://arxiv.org/abs/2402.18368v1","category":"gr-qc"}
{"created":"2024-02-28 14:39:39","title":"Kernel theorems for operators on co-orbit spaces associated with localised frames","abstract":"Kernel theorems, in general, provide a convenient representation of bounded linear operators. For the operator acting on a concrete function space, this means that its action on any element of the space can be expressed as a generalised integral operator, in a way reminiscent of the matrix representation of linear operators acting on finite dimensional vector spaces. We prove kernel theorems for bounded linear operators acting on co-orbit spaces associated with localised frames. Our two main results consist in characterising the spaces of operators whose generalised integral kernels belong to the co-orbit spaces of test functions and distributions associated with the tensor product of the localised frames respectively. Moreover, using a version of Schur's test, we establish a characterisation of the bounded linear operators between some specific co-orbit spaces.","sentences":["Kernel theorems, in general, provide a convenient representation of bounded linear operators.","For the operator acting on a concrete function space, this means that its action on any element of the space can be expressed as a generalised integral operator, in a way reminiscent of the matrix representation of linear operators acting on finite dimensional vector spaces.","We prove kernel theorems for bounded linear operators acting on co-orbit spaces associated with localised frames.","Our two main results consist in characterising the spaces of operators whose generalised integral kernels belong to the co-orbit spaces of test functions and distributions associated with the tensor product of the localised frames respectively.","Moreover, using a version of Schur's test, we establish a characterisation of the bounded linear operators between some specific co-orbit spaces."],"url":"http://arxiv.org/abs/2402.18367v1","category":"math.FA"}
{"created":"2024-02-28 14:35:24","title":"Extracting the Luttinger parameter from a single wave function","abstract":"The low-energy physics of Tomonaga-Luttinger liquids (TLLs) is controlled by the Luttinger parameter. We demonstrate that this parameter can be extracted from a single wave function for one-component TLLs with periodic boundary condition. This method relies on the fact that TLLs are described by conformal field theory in which crosscap states can be constructed. The overlaps between the crosscap states and the ground state as well as some excited states are proved to be universal numbers that directly reveal the Luttinger parameter. In microscopic lattice models, crosscap states are formed by putting each pair of antipodal sites into a maximally entangled state. Analytical and numerical calculations are performed in a few representative models to substantiate the conformal field theory prediction. The extracted Luttinger parameters are generally quite accurate in finite-size systems with moderate lengths, so there is no need to perform data fitting and/or finite-size scaling.","sentences":["The low-energy physics of Tomonaga-Luttinger liquids (TLLs) is controlled by the Luttinger parameter.","We demonstrate that this parameter can be extracted from a single wave function for one-component TLLs with periodic boundary condition.","This method relies on the fact that TLLs are described by conformal field theory in which crosscap states can be constructed.","The overlaps between the crosscap states and the ground state as well as some excited states are proved to be universal numbers that directly reveal the Luttinger parameter.","In microscopic lattice models, crosscap states are formed by putting each pair of antipodal sites into a maximally entangled state.","Analytical and numerical calculations are performed in a few representative models to substantiate the conformal field theory prediction.","The extracted Luttinger parameters are generally quite accurate in finite-size systems with moderate lengths, so there is no need to perform data fitting and/or finite-size scaling."],"url":"http://arxiv.org/abs/2402.18364v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 14:34:00","title":"Factors influencing the stability of the motor-clutch model on compliant substrates under external load","abstract":"Cellular migration is crucial for biological processes including embryonic development, immune response, and wound healing. The myosin-clutch model is a framework that describes how cells control migration through the interactions between myosin, the clutch mechanism, and the substrate. This model is related to how cells regulate adhesion, generate traction forces, and move on compliant substrates. In this study, we present a five-dimensional nonlinear autonomous system to investigate the influences of myosin, clutches, substrate, and external load on the system's stability. Moreover, we analyze the effects of various parameters on fixed points and explore the frequency and amplitude of the limit cycle associated with oscillations. We discovered that the system demonstrates oscillatory behavior when the velocity of the myosin motor is relatively low, or when the ratio of the motor attachment rate to motor detachment rate is relatively high. The external load shares a fraction of the force exerted by myosin motors, thereby diminishing the force endured by the clutches. Within a specific range, an increase in external load not only diminishes and eventually eliminates the region lacking fixed points but also decelerates clutch detachment, enhancing clutch protein adherence.","sentences":["Cellular migration is crucial for biological processes including embryonic development, immune response, and wound healing.","The myosin-clutch model is a framework that describes how cells control migration through the interactions between myosin, the clutch mechanism, and the substrate.","This model is related to how cells regulate adhesion, generate traction forces, and move on compliant substrates.","In this study, we present a five-dimensional nonlinear autonomous system to investigate the influences of myosin, clutches, substrate, and external load on the system's stability.","Moreover, we analyze the effects of various parameters on fixed points and explore the frequency and amplitude of the limit cycle associated with oscillations.","We discovered that the system demonstrates oscillatory behavior when the velocity of the myosin motor is relatively low, or when the ratio of the motor attachment rate to motor detachment rate is relatively high.","The external load shares a fraction of the force exerted by myosin motors, thereby diminishing the force endured by the clutches.","Within a specific range, an increase in external load not only diminishes and eventually eliminates the region lacking fixed points but also decelerates clutch detachment, enhancing clutch protein adherence."],"url":"http://arxiv.org/abs/2402.18363v1","category":"physics.bio-ph"}
{"created":"2024-02-28 14:33:14","title":"Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model","abstract":"As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.","sentences":["As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life.","However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling.","In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models.","Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions.","By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis.","Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation.","Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation.","Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy.","Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations."],"url":"http://arxiv.org/abs/2402.18362v1","category":"cs.CV"}
{"created":"2024-02-28 14:31:34","title":"Similarity-based analogical proportions","abstract":"The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra. The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former. The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions.","sentences":["The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra.","The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former.","The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions."],"url":"http://arxiv.org/abs/2402.18360v1","category":"cs.LO"}
{"created":"2024-02-28 14:17:32","title":"LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping","abstract":"We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator. Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward. The loss objective consists of only three terms, and can effectively control the face swap results between source and target images. By attaching a pre-trained GAN inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models. We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks. The source code and models can be found at \\url{https://github.com/usingcolor/LatentSwap}.","sentences":["We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator.","Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward.","The loss objective consists of only three terms, and can effectively control the face swap results between source and target images.","By attaching a pre-trained GAN inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models.","We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks.","The source code and models can be found at \\url{https://github.com/usingcolor/LatentSwap}."],"url":"http://arxiv.org/abs/2402.18351v1","category":"cs.CV"}
{"created":"2024-02-28 14:09:02","title":"Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning","abstract":"Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).","sentences":["Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT).","However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem.","To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning.","Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers.","Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives.","Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%)."],"url":"http://arxiv.org/abs/2402.18344v1","category":"cs.CL"}
{"created":"2024-02-28 14:07:32","title":"A class of higher order inverse spectral problems","abstract":"In this paper, we consider the recovery of third-order differential operators from two spectra, as well as fourth-order or fifth-order differential operators from three spectra, where these differential operators are endowed with complex-valued distributional coefficients. For the case of multiple spectra, we first establish the relationship between spectra and the Weyl-Yurko matrix. Secondly, we prove the uniqueness theorem for the solution of the inverse problems. Our approach allows us to obtain results for the general case of complex-valued distributional coefficients.","sentences":["In this paper, we consider the recovery of third-order differential operators from two spectra, as well as fourth-order or fifth-order differential operators from three spectra, where these differential operators are endowed with complex-valued distributional coefficients.","For the case of multiple spectra, we first establish the relationship between spectra and the Weyl-Yurko matrix.","Secondly, we prove the uniqueness theorem for the solution of the inverse problems.","Our approach allows us to obtain results for the general case of complex-valued distributional coefficients."],"url":"http://arxiv.org/abs/2402.18343v1","category":"math.SP"}
{"created":"2024-02-28 14:05:58","title":"Almost diagonalization of $\u03a8$DO's over various generalized function spaces","abstract":"Inductive and projective type sequence spaces of sub- and super-exponential growth, and the corresponding inductive and projective limits of modulation spaces are considered as a framework for almost diagonalization of pseudo-differential operators. Moreover, recent results of the first author and B. Prangoski related to the almost diagonalization of pseudo-differential operators in the context of H\\\"ormander metrics are reviewed.","sentences":["Inductive and projective type sequence spaces of sub- and super-exponential growth, and the corresponding inductive and projective limits of modulation spaces are considered as a framework for almost diagonalization of pseudo-differential operators.","Moreover, recent results of the first author and B. Prangoski related to the almost diagonalization of pseudo-differential operators in the context of H\\\"ormander metrics are reviewed."],"url":"http://arxiv.org/abs/2402.18341v1","category":"math.FA"}
{"created":"2024-02-28 14:03:59","title":"Online Edge Coloring is (Nearly) as Easy as Offline","abstract":"The classic theorem of Vizing (Diskret. Analiz.'64) asserts that any graph of maximum degree $\\Delta$ can be edge colored (offline) using no more than $\\Delta+1$ colors (with $\\Delta$ being a trivial lower bound). In the online setting, Bar-Noy, Motwani and Naor (IPL'92) conjectured that a $(1+o(1))\\Delta$-edge-coloring can be computed online in $n$-vertex graphs of maximum degree $\\Delta=\\omega(\\log n)$. Numerous algorithms made progress on this question, using a higher number of colors or assuming restricted arrival models, such as random-order edge arrivals or vertex arrivals (e.g., AGKM FOCS'03, BMM SODA'10, CPW FOCS'19, BGW SODA'21, KLSST STOC'22). In this work, we resolve this longstanding conjecture in the affirmative in the most general setting of adversarial edge arrivals. We further generalize this result to obtain online counterparts of the list edge coloring result of Kahn (J. Comb. Theory. A'96) and of the recent \"local\" edge coloring result of Christiansen (STOC'23).","sentences":["The classic theorem of Vizing (Diskret.","Analiz.","'64) asserts that any graph of maximum degree $\\Delta$ can be edge colored (offline) using no more than $\\Delta+1$ colors (with $\\Delta$ being a trivial lower bound).","In the online setting, Bar-Noy, Motwani and Naor (IPL'92) conjectured that a $(1+o(1))\\Delta$-edge-coloring can be computed online in $n$-vertex graphs of maximum degree $\\Delta=\\omega(\\log n)$. Numerous algorithms made progress on this question, using a higher number of colors or assuming restricted arrival models, such as random-order edge arrivals or vertex arrivals (e.g., AGKM FOCS'03, BMM SODA'10, CPW FOCS'19, BGW SODA'21, KLSST STOC'22).","In this work, we resolve this longstanding conjecture in the affirmative in the most general setting of adversarial edge arrivals.","We further generalize this result to obtain online counterparts of the list edge coloring result of Kahn (J. Comb.","Theory.","A'96) and of the recent \"local\" edge coloring result of Christiansen (STOC'23)."],"url":"http://arxiv.org/abs/2402.18339v1","category":"cs.DS"}
{"created":"2024-02-28 14:01:09","title":"Generating candidates in global optimization algorithms using complementary energy landscapes","abstract":"Global optimization of atomistic structure rely on the generation of new candidate structures in order to drive the exploration of the potential energy surface (PES) in search for the global minimum energy (GM) structure. In this work, we discuss a type of structure generation, which locally optimizes structures in complementary energy (CE) landscapes. These landscapes are formulated temporarily during the searches as machine learned potentials (MLPs) using local atomistic environments sampled from collected data. The CE landscapes are deliberately incomplete MLPs that rather than mimicking every aspect of the true PES are sought to become much smoother, having only few local minima. This means that local optimization in the CE landscapes may facilitate identification of new funnels in the true PES. We discuss how to construct the CE landscapes and we test their influence on global optimization of a reduced rutile SnO2(110)-(4x1) surface, and an olivine (Mg2SiO4)4 cluster for which we report a new global minimum energy structure.","sentences":["Global optimization of atomistic structure rely on the generation of new candidate structures in order to drive the exploration of the potential energy surface (PES) in search for the global minimum energy (GM) structure.","In this work, we discuss a type of structure generation, which locally optimizes structures in complementary energy (CE) landscapes.","These landscapes are formulated temporarily during the searches as machine learned potentials (MLPs) using local atomistic environments sampled from collected data.","The CE landscapes are deliberately incomplete MLPs that rather than mimicking every aspect of the true PES are sought to become much smoother, having only few local minima.","This means that local optimization in the CE landscapes may facilitate identification of new funnels in the true PES.","We discuss how to construct the CE landscapes and we test their influence on global optimization of a reduced rutile SnO2(110)-(4x1) surface, and an olivine (Mg2SiO4)4 cluster for which we report a new global minimum energy structure."],"url":"http://arxiv.org/abs/2402.18338v1","category":"physics.chem-ph"}
{"created":"2024-02-28 13:58:13","title":"Phase diagrams and polarization reversal in nanosized HfxZr1-xO2-y","abstract":"To describe the polar properties of the nanosized HfxZr1-xO2-y, we evolve the \"effective\" Landau-Ginzburg-Devonshire (LGD) model based on the parametrization of the Landau expansion coefficients for the polar and antipolar orderings. We have shown that the effective LGD model can predict the influence of screening conditions and size effects on phase diagrams, polarization reversal and structural properties of the nanosized HfxZr1-xO2-y of various shape and sizes. To verify the model, we use available experimental results for HfxZr1-xO2 thin films and oxygen-deficient HfO2-y nanoparticles. X-ray diffraction was used to determine the phase composition of the HfO2-y nanoparticles prepared at different annealing conditions and revealed the formation of the ferroelectric orthorhombic phase. Micro-Raman spectroscopy was used to explore the correlation of lattice dynamics and structural changes appearing in dependence on the oxygen vacancies concentration. Obtained results can be promising for creation of next generation Si-compatible ferroelectric nanomaterials based on HfxZr1-xO2-y.","sentences":["To describe the polar properties of the nanosized HfxZr1-xO2-y, we evolve the \"effective\" Landau-Ginzburg-Devonshire (LGD) model based on the parametrization of the Landau expansion coefficients for the polar and antipolar orderings.","We have shown that the effective LGD model can predict the influence of screening conditions and size effects on phase diagrams, polarization reversal and structural properties of the nanosized HfxZr1-xO2-y of various shape and sizes.","To verify the model, we use available experimental results for HfxZr1-xO2 thin films and oxygen-deficient HfO2-y nanoparticles.","X-ray diffraction was used to determine the phase composition of the HfO2-y nanoparticles prepared at different annealing conditions and revealed the formation of the ferroelectric orthorhombic phase.","Micro-Raman spectroscopy was used to explore the correlation of lattice dynamics and structural changes appearing in dependence on the oxygen vacancies concentration.","Obtained results can be promising for creation of next generation Si-compatible ferroelectric nanomaterials based on HfxZr1-xO2-y."],"url":"http://arxiv.org/abs/2402.18336v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 13:54:57","title":"Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation","abstract":"We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.","sentences":["We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning.","Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data.","We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates.","The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response.","We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models.","We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline.","For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points.","We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators.","Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains.","The model, dataset, and code are available at https://github.com/BatsResearch/bonito."],"url":"http://arxiv.org/abs/2402.18334v1","category":"cs.CL"}
{"created":"2024-02-28 13:51:25","title":"Recursive GNNs for Learning Precoding Policies with Size-Generalizability","abstract":"Graph neural networks (GNNs) have been shown promising in optimizing power allocation and link scheduling with good size generalizability and low training complexity. These merits are important for learning wireless policies under dynamic environments, which partially come from the matched permutation equivariance (PE) properties of the GNNs to the policies to be learned. Nonetheless, it has been noticed in literature that only satisfying the PE property of a precoding policy in multi-antenna systems cannot ensure a GNN for learning precoding to be generalizable to the unseen number of users. Incorporating models with GNNs helps improve size generalizability, which however is only applicable to specific problems, settings, and algorithms. In this paper, we propose a framework of size generalizable GNNs for learning precoding policies that are purely data-driven and can learn wireless policies including but not limited to baseband and hybrid precoding in multi-user multi-antenna systems. To this end, we first find a special structure of each iteration of two numerical algorithms for optimizing precoding, from which we identify the key characteristics of a GNN that affect its size generalizability. Then, we design size-generalizable GNNs that are with these key characteristics and satisfy the PE properties of precoding policies in a recursive manner. Simulation results show that the proposed GNNs can be well-generalized to the number of users for learning baseband and hybrid precoding policies and require much fewer samples than existing counterparts to achieve the same performance.","sentences":["Graph neural networks (GNNs) have been shown promising in optimizing power allocation and link scheduling with good size generalizability and low training complexity.","These merits are important for learning wireless policies under dynamic environments, which partially come from the matched permutation equivariance (PE) properties of the GNNs to the policies to be learned.","Nonetheless, it has been noticed in literature that only satisfying the PE property of a precoding policy in multi-antenna systems cannot ensure a GNN for learning precoding to be generalizable to the unseen number of users.","Incorporating models with GNNs helps improve size generalizability, which however is only applicable to specific problems, settings, and algorithms.","In this paper, we propose a framework of size generalizable GNNs for learning precoding policies that are purely data-driven and can learn wireless policies including but not limited to baseband and hybrid precoding in multi-user multi-antenna systems.","To this end, we first find a special structure of each iteration of two numerical algorithms for optimizing precoding, from which we identify the key characteristics of a GNN that affect its size generalizability.","Then, we design size-generalizable GNNs that are with these key characteristics and satisfy the PE properties of precoding policies in a recursive manner.","Simulation results show that the proposed GNNs can be well-generalized to the number of users for learning baseband and hybrid precoding policies and require much fewer samples than existing counterparts to achieve the same performance."],"url":"http://arxiv.org/abs/2402.18332v1","category":"eess.SP"}
{"created":"2024-02-28 13:50:46","title":"FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes","abstract":"The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.","sentences":["The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images.","However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k.","A more challenging task, large-scale fine-grained image generation, remains the boundary to explore.","In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories.","FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters.","To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling.","Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes.","Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods.","The code and more generated results are available at our project website: https://finediffusion.github.io/."],"url":"http://arxiv.org/abs/2402.18331v1","category":"cs.CV"}
{"created":"2024-02-28 13:49:23","title":"Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation","abstract":"The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks. Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like adversarial training, and it is possible to deploy near-real-time models with almost-zero false alarms.","sentences":["The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs.","LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning.","To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs.","Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors.","We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks.","Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like adversarial training, and it is possible to deploy near-real-time models with almost-zero false alarms."],"url":"http://arxiv.org/abs/2402.18329v1","category":"cs.CR"}
{"created":"2024-02-28 13:48:44","title":"When Should Algorithms Resign?","abstract":"This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations. Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems. Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided. We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance. Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement. Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated with AI but also leverages its benefits, ensuring the responsible and effective use of AI systems.","sentences":["This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations.","Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems.","Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided.","We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance.","Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement.","Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated with AI but also leverages its benefits, ensuring the responsible and effective use of AI systems."],"url":"http://arxiv.org/abs/2402.18326v1","category":"cs.CY"}
{"created":"2024-02-28 13:40:40","title":"On the structure of completely useful topologies","abstract":"Let $X$ be an arbitrary set. Then a topology $t$ on $X$ is said to be completely useful if every upper semicontinuous linear (total) preorder $\\precsim$ on $X$ can be represented by an upper semicontinuous real-valued order preserving function. In this paper, appealing, simple and new characterizations of completely useful topologies will be proved, therefore clarifying the structure of such topologies.","sentences":["Let $X$ be an arbitrary set.","Then a topology $t$ on $X$ is said to be completely useful if every upper semicontinuous linear (total) preorder $\\precsim$ on $X$ can be represented by an upper semicontinuous real-valued order preserving function.","In this paper, appealing, simple and new characterizations of completely useful topologies will be proved, therefore clarifying the structure of such topologies."],"url":"http://arxiv.org/abs/2402.18324v1","category":"econ.TH"}
{"created":"2024-02-28 13:38:27","title":"Equivalent Environments and Covering Spaces for Robots","abstract":"This paper formally defines a robot system, including its sensing and actuation components, as a general, topological dynamical system. The focus is on determining general conditions under which various environments in which the robot can be placed are indistinguishable. A key result is that, under very general conditions, covering maps witness such indistinguishability. This formalizes the intuition behind the well studied loop closure problem in robotics. An important special case is where the sensor mapping reports an invariant of the local topological (metric) structure of an environment because such structure is preserved by (metric) covering maps. Whereas coverings provide a sufficient condition for the equivalence of environments, we also give a necessary condition using bisimulation. The overall framework is applied to unify previously identified phenomena in robotics and related fields, in which moving agents with sensors must make inferences about their environments based on limited data. Many open problems are identified.","sentences":["This paper formally defines a robot system, including its sensing and actuation components, as a general, topological dynamical system.","The focus is on determining general conditions under which various environments in which the robot can be placed are indistinguishable.","A key result is that, under very general conditions, covering maps witness such indistinguishability.","This formalizes the intuition behind the well studied loop closure problem in robotics.","An important special case is where the sensor mapping reports an invariant of the local topological (metric) structure of an environment because such structure is preserved by (metric) covering maps.","Whereas coverings provide a sufficient condition for the equivalence of environments, we also give a necessary condition using bisimulation.","The overall framework is applied to unify previously identified phenomena in robotics and related fields, in which moving agents with sensors must make inferences about their environments based on limited data.","Many open problems are identified."],"url":"http://arxiv.org/abs/2402.18323v1","category":"cs.RO"}
{"created":"2024-02-28 13:36:27","title":"Privacy Policies and Consent Management Platforms: Growth and Users' Interactions over Time","abstract":"In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising. The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices. Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features.   This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs. Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide. We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection. For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option. Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent. The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision. Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices.","sentences":["In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising.","The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices.","Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features.   ","This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years.","We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs.","Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe.","Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide.","We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection.","For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option.","Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent.","The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision.","Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices."],"url":"http://arxiv.org/abs/2402.18321v1","category":"cs.CY"}
{"created":"2024-02-28 13:33:43","title":"Location-guided Head Pose Estimation for Fisheye Image","abstract":"Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \\textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \\textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \\textcolor{blue}{a} fisheye-\\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.","sentences":["Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection.","Serious fisheye \\textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \\textcolor{blue}{existing} head pose estimation models trained on undistorted images.","This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion.","We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location.","Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration.","We also created \\textcolor{blue}{a} fisheye-\\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments.","Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods."],"url":"http://arxiv.org/abs/2402.18320v1","category":"cs.CV"}
{"created":"2024-02-28 13:28:54","title":"SD-SLAM: A Semantic SLAM Approach for Dynamic Scenes Based on LiDAR Point Clouds","abstract":"Point cloud maps generated via LiDAR sensors using extensive remotely sensed data are commonly used by autonomous vehicles and robots for localization and navigation. However, dynamic objects contained in point cloud maps not only downgrade localization accuracy and navigation performance but also jeopardize the map quality. In response to this challenge, we propose in this paper a novel semantic SLAM approach for dynamic scenes based on LiDAR point clouds, referred to as SD-SLAM hereafter. The main contributions of this work are in three aspects: 1) introducing a semantic SLAM framework dedicatedly for dynamic scenes based on LiDAR point clouds, 2) Employing semantics and Kalman filtering to effectively differentiate between dynamic and semi-static landmarks, and 3) Making full use of semi-static and pure static landmarks with semantic information in the SD-SLAM process to improve localization and mapping performance. To evaluate the proposed SD-SLAM, tests were conducted using the widely adopted KITTI odometry dataset. Results demonstrate that the proposed SD-SLAM effectively mitigates the adverse effects of dynamic objects on SLAM, improving vehicle localization and mapping performance in dynamic scenes, and simultaneously constructing a static semantic map with multiple semantic classes for enhanced environment understanding.","sentences":["Point cloud maps generated via LiDAR sensors using extensive remotely sensed data are commonly used by autonomous vehicles and robots for localization and navigation.","However, dynamic objects contained in point cloud maps not only downgrade localization accuracy and navigation performance but also jeopardize the map quality.","In response to this challenge, we propose in this paper a novel semantic SLAM approach for dynamic scenes based on LiDAR point clouds, referred to as SD-SLAM hereafter.","The main contributions of this work are in three aspects: 1) introducing a semantic SLAM framework dedicatedly for dynamic scenes based on LiDAR point clouds, 2) Employing semantics and Kalman filtering to effectively differentiate between dynamic and semi-static landmarks, and 3) Making full use of semi-static and pure static landmarks with semantic information in the SD-SLAM process to improve localization and mapping performance.","To evaluate the proposed SD-SLAM, tests were conducted using the widely adopted KITTI odometry dataset.","Results demonstrate that the proposed SD-SLAM effectively mitigates the adverse effects of dynamic objects on SLAM, improving vehicle localization and mapping performance in dynamic scenes, and simultaneously constructing a static semantic map with multiple semantic classes for enhanced environment understanding."],"url":"http://arxiv.org/abs/2402.18318v1","category":"cs.RO"}
{"created":"2024-02-28 13:23:00","title":"Stability and instability of the quasilinear Gross--Pitaevskii dark solitons","abstract":"We study a quasilinear Schr\\\"odinger equation with nonzero conditions at infinity. In previous works, we obtained a continuous branch of traveling waves, given by dark solitons indexed by their speed. Neglecting the quasilinear term, one recovers the Gross--Pitaevskii equation, for which the branch of dark solitons is stable. Moreover, Z.~Lin showed that the Vakhitov--Kolokolov~(VK) stability criterion (in terms of the momentum of solitons) holds for general semilinear equations with nonvanishing conditions at infinity.   In the quasilinear case, we prove that the VK stability criterion still applies, by generalizing Lin's arguments. Therefore, we deduce that the branch of dark solitons is stable for weak quasilinear interactions. For stronger quasilinear interactions, a cusp appears in the energy-momentum diagram, implying the stability of fast waves and the instability of slow waves.","sentences":["We study a quasilinear Schr\\\"odinger equation with nonzero conditions at infinity.","In previous works, we obtained a continuous branch of traveling waves, given by dark solitons indexed by their speed.","Neglecting the quasilinear term, one recovers the Gross--Pitaevskii equation, for which the branch of dark solitons is stable.","Moreover, Z.~Lin showed that the Vakhitov--Kolokolov~(VK) stability criterion (in terms of the momentum of solitons) holds for general semilinear equations with nonvanishing conditions at infinity.   ","In the quasilinear case, we prove that the VK stability criterion still applies, by generalizing Lin's arguments.","Therefore, we deduce that the branch of dark solitons is stable for weak quasilinear interactions.","For stronger quasilinear interactions, a cusp appears in the energy-momentum diagram, implying the stability of fast waves and the instability of slow waves."],"url":"http://arxiv.org/abs/2402.18316v1","category":"math.AP"}
{"created":"2024-02-28 13:16:06","title":"Time Domain Displacement Response to a Concentrated Vertical Force on the Free Surface of An Elastic Half-space","abstract":"This article presents the Huygens method as a novel alternative to the Cagniard-de Hoop method, offering insights into wave generation mechanisms and facilitating wave decomposition. It holds promise for various linear boundary problems and provides results without singularities. The study defines surface waves, including the Rayleigh wave, and identifies a novel wave mode, the Huygens wave. Additionally, it investigates displacements near the axis, providing simplified approximations and examining changes in displacements over time and space, noting their gradual attenuation.","sentences":["This article presents the Huygens method as a novel alternative to the Cagniard-de Hoop method, offering insights into wave generation mechanisms and facilitating wave decomposition.","It holds promise for various linear boundary problems and provides results without singularities.","The study defines surface waves, including the Rayleigh wave, and identifies a novel wave mode, the Huygens wave.","Additionally, it investigates displacements near the axis, providing simplified approximations and examining changes in displacements over time and space, noting their gradual attenuation."],"url":"http://arxiv.org/abs/2402.18314v1","category":"physics.geo-ph"}
{"created":"2024-02-28 13:14:20","title":"How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning","abstract":"Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token predominantly appear in the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.","sentences":["Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation.","This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view.","From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning.","These parallel pathways provide sequential answers from the input question context as well as the generated CoT.","We observe a striking functional rift in the middle layers of the LLM.","Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half.","This internal phase shift manifests in different functional components: attention heads that write the answer token predominantly appear in the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on.","To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs."],"url":"http://arxiv.org/abs/2402.18312v1","category":"cs.CL"}
{"created":"2024-02-28 13:08:46","title":"Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis","abstract":"In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road. Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape. The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences. By automating this process, municipalities can address potential road space constraints, enhancing safety for all. They may also save valuable time by carrying out the inspections more systematically. Our open-source code gives communities inspiration on how to automate the process themselves.","sentences":["In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance.","Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants.","Accurately estimating this space from simple images proves challenging due to a lack of depth information.","This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective.","Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving.","These scans, however, also open up possibilities in urban management.","In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed.","Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road.","Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape.","The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences.","By automating this process, municipalities can address potential road space constraints, enhancing safety for all.","They may also save valuable time by carrying out the inspections more systematically.","Our open-source code gives communities inspiration on how to automate the process themselves."],"url":"http://arxiv.org/abs/2402.18309v1","category":"cs.CV"}
{"created":"2024-02-28 13:07:51","title":"A restricted memory quasi-Newton bundle method for nonsmooth optimization on Riemannian manifolds","abstract":"In this paper, a restricted memory quasi-Newton bundle method for minimizing a locally Lipschitz function over a Riemannian manifold is proposed, which extends the classical one in Euclidean spaces to the manifold setting. The curvature information of the objective function is approximated by applying the Riemannian version of the quasi-Newton updating formulas. The subgradient aggregation technique is used to avoid solving the time-consuming quadratic programming subproblem when calculating the candidate descent direction. Moreover, a new Riemannian line search procedure is proposed to generate the stepsizes, and the process is finitely terminated under a new version of the Riemannian semismooth assumption. Global convergence of the proposed method is established: if the serious iteration steps are finite, then the last serious iterate is stationary; otherwise, every accumulation point of the serious iteration sequence is stationary. Finally, some preliminary numerical results show that the proposed method is efficient.","sentences":["In this paper, a restricted memory quasi-Newton bundle method for minimizing a locally Lipschitz function over a Riemannian manifold is proposed, which extends the classical one in Euclidean spaces to the manifold setting.","The curvature information of the objective function is approximated by applying the Riemannian version of the quasi-Newton updating formulas.","The subgradient aggregation technique is used to avoid solving the time-consuming quadratic programming subproblem when calculating the candidate descent direction.","Moreover, a new Riemannian line search procedure is proposed to generate the stepsizes, and the process is finitely terminated under a new version of the Riemannian semismooth assumption.","Global convergence of the proposed method is established: if the serious iteration steps are finite, then the last serious iterate is stationary; otherwise, every accumulation point of the serious iteration sequence is stationary.","Finally, some preliminary numerical results show that the proposed method is efficient."],"url":"http://arxiv.org/abs/2402.18308v1","category":"math.OC"}
{"created":"2024-02-28 13:00:32","title":"NERV++: An Enhanced Implicit Neural Video Representation","abstract":"Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint. Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability. Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks. We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation. NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs. This achievement narrows the gap to autoencoder-based video coding, marking a significant stride in INR-based video compression research.","sentences":["Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint.","Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability.","Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks.","We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation.","NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs.","We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs.","This achievement narrows the gap to autoencoder-based video coding, marking a significant stride in INR-based video compression research."],"url":"http://arxiv.org/abs/2402.18305v1","category":"eess.IV"}
{"created":"2024-02-28 12:38:49","title":"Whole-body Humanoid Robot Locomotion with Human Reference","abstract":"Recently, humanoid robots have made significant advances in their ability to perform complex tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of planning complex reward functions and training entire complex systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, ''Adam'', whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.","sentences":["Recently, humanoid robots have made significant advances in their ability to perform complex tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of planning complex reward functions and training entire complex systems, still poses a notable challenge.","To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, ''Adam'', whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process.","In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general.","Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks.","Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot."],"url":"http://arxiv.org/abs/2402.18294v1","category":"cs.RO"}
{"created":"2024-02-28 12:38:44","title":"Grid-Based Continuous Normal Representation for Anomaly Detection","abstract":"There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing the input and the directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose GRAD, a novel anomaly detection method for representing normal features within a \"continuous\" feature space, enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that GRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, GRAD effectively handles diverse classes in a single model thanks to the high-granularity global representation. In an evaluation using the MVTec AD dataset, GRAD significantly outperforms the previous state-of-the-art method by reducing 65.0\\% of the error for multi-class unified anomaly detection. The project page is available at https://tae-mo.github.io/grad/.","sentences":["There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training.","Several recent methods aim to detect anomalies based on a memory, comparing the input and the directly stored normal features (or trained features with normal images).","However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively.","Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects.","To tackle all of the above challenges, we propose GRAD, a novel anomaly detection method for representing normal features within a \"continuous\" feature space, enabled by transforming spatial features into coordinates and mapping them to continuous grids.","Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively.","Our extensive experiments demonstrate that GRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, GRAD effectively handles diverse classes in a single model thanks to the high-granularity global representation.","In an evaluation using the MVTec AD dataset, GRAD significantly outperforms the previous state-of-the-art method by reducing 65.0\\% of the error for multi-class unified anomaly detection.","The project page is available at https://tae-mo.github.io/grad/."],"url":"http://arxiv.org/abs/2402.18293v1","category":"cs.CV"}
{"created":"2024-02-28 12:37:30","title":"FSL Model can Score Higher as It Is","abstract":"In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.","sentences":["In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised.","Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training.","Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly.","In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation.","An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples.","Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample.","It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample.","Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset.","According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs.","By augmenting both the support set and the queries, we can achieve even more performance improvement.","Our Github Repository is publicly available."],"url":"http://arxiv.org/abs/2402.18292v1","category":"cs.CV"}
{"created":"2024-02-28 12:28:32","title":"Ekstr\u00f6m-Persson conjecture regarding random covering sets","abstract":"We consider the Hausdorff dimension of random covering sets generated by balls and general measures in Euclidean spaces. We prove, for a certain parameter range, a conjecture by Ekstr\\\"om and Persson concerning the exact value of the dimension in the special case of radii $(n^{-\\alpha})_{n=1}^\\infty$. For generating balls with an arbitrary sequence of radii, we find sharp bounds for the dimension and show that the natural extension of the Ekstr\\\"om-Persson conjecture is not true in this case. Finally, we construct examples demonstrating that there does not exist a dimension formula involving only the lower and upper local dimensions of the measure and a critical parameter determined by the sequence of radii.","sentences":["We consider the Hausdorff dimension of random covering sets generated by balls and general measures in Euclidean spaces.","We prove, for a certain parameter range, a conjecture by Ekstr\\\"om and Persson concerning the exact value of the dimension in the special case of radii $(n^{-\\alpha})_{n=1}^\\infty$. For generating balls with an arbitrary sequence of radii, we find sharp bounds for the dimension and show that the natural extension of the Ekstr\\\"om-Persson conjecture is not true in this case.","Finally, we construct examples demonstrating that there does not exist a dimension formula involving only the lower and upper local dimensions of the measure and a critical parameter determined by the sequence of radii."],"url":"http://arxiv.org/abs/2402.18289v1","category":"math.CA"}
{"created":"2024-02-28 12:27:35","title":"Development of Context-Sensitive Formulas to Obtain Constant Luminance Perception for a Foreground Object in Front of Backgrounds of Varying Luminance","abstract":"In this article, we present a framework for developing context-sensitive luminance correction formulas that can produce constant luminance perception for foreground objects. Our formulas make the foreground object slightly translucent to mix with the blurred version of the background. This mix can quickly produce any desired illusion of luminance in foreground objects based on the luminance of the background. The translucency formula has only one parameter; the relative size of the foreground object, which is a number between zero and one. We have identified the general structure of the translucency formulas as a power function of the relative size of the foreground object. We have implemented a web-based interactive program in Shadertoy. Using this program, we determined the coefficients of the polynomial exponents of the power function. To intuitively control the coefficients of the polynomial functions, we have used a B\\'{e}zier form. Our final translucency formula uses a quadratic polynomial and requires only three coefficients. We also identified a simpler affine formula, which requires only two coefficients. We made our program publicly available in Shadertoy so that anyone can access and improve it. In this article, we also explain how to intuitively change the polynomial part of the formula. Using our explanation, users change the polynomial part of the formula to obtain their own perceptively constant luminance. This can be used as a crowd-sourcing experiment for further improvement of the formula.","sentences":["In this article, we present a framework for developing context-sensitive luminance correction formulas that can produce constant luminance perception for foreground objects.","Our formulas make the foreground object slightly translucent to mix with the blurred version of the background.","This mix can quickly produce any desired illusion of luminance in foreground objects based on the luminance of the background.","The translucency formula has only one parameter; the relative size of the foreground object, which is a number between zero and one.","We have identified the general structure of the translucency formulas as a power function of the relative size of the foreground object.","We have implemented a web-based interactive program in Shadertoy.","Using this program, we determined the coefficients of the polynomial exponents of the power function.","To intuitively control the coefficients of the polynomial functions, we have used a B\\'{e}zier form.","Our final translucency formula uses a quadratic polynomial and requires only three coefficients.","We also identified a simpler affine formula, which requires only two coefficients.","We made our program publicly available in Shadertoy so that anyone can access and improve it.","In this article, we also explain how to intuitively change the polynomial part of the formula.","Using our explanation, users change the polynomial part of the formula to obtain their own perceptively constant luminance.","This can be used as a crowd-sourcing experiment for further improvement of the formula."],"url":"http://arxiv.org/abs/2402.18288v1","category":"cs.GR"}
{"created":"2024-02-28 12:27:28","title":"Windowed-FourierMixer: Enhancing Clutter-Free Room Modeling with Fourier Transform","abstract":"With the growing demand for immersive digital applications, the need to understand and reconstruct 3D scenes has significantly increased. In this context, inpainting indoor environments from a single image plays a crucial role in modeling the internal structure of interior spaces as it enables the creation of textured and clutter-free reconstructions. While recent methods have shown significant progress in room modeling, they rely on constraining layout estimators to guide the reconstruction process. These methods are highly dependent on the performance of the structure estimator and its generative ability in heavily occluded environments. In response to these issues, we propose an innovative approach based on a U-Former architecture and a new Windowed-FourierMixer block, resulting in a unified, single-phase network capable of effectively handle human-made periodic structures such as indoor spaces. This new architecture proves advantageous for tasks involving indoor scenes where symmetry is prevalent, allowing the model to effectively capture features such as horizon/ceiling height lines and cuboid-shaped rooms. Experiments show the proposed approach outperforms current state-of-the-art methods on the Structured3D dataset demonstrating superior performance in both quantitative metrics and qualitative results. Code and models will be made publicly available.","sentences":["With the growing demand for immersive digital applications, the need to understand and reconstruct 3D scenes has significantly increased.","In this context, inpainting indoor environments from a single image plays a crucial role in modeling the internal structure of interior spaces as it enables the creation of textured and clutter-free reconstructions.","While recent methods have shown significant progress in room modeling, they rely on constraining layout estimators to guide the reconstruction process.","These methods are highly dependent on the performance of the structure estimator and its generative ability in heavily occluded environments.","In response to these issues, we propose an innovative approach based on a U-Former architecture and a new Windowed-FourierMixer block, resulting in a unified, single-phase network capable of effectively handle human-made periodic structures such as indoor spaces.","This new architecture proves advantageous for tasks involving indoor scenes where symmetry is prevalent, allowing the model to effectively capture features such as horizon/ceiling height lines and cuboid-shaped rooms.","Experiments show the proposed approach outperforms current state-of-the-art methods on the Structured3D dataset demonstrating superior performance in both quantitative metrics and qualitative results.","Code and models will be made publicly available."],"url":"http://arxiv.org/abs/2402.18287v1","category":"cs.CV"}
{"created":"2024-02-28 12:25:01","title":"Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis","abstract":"In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.","sentences":["In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field.","We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution.","Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization.","We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance.","We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important."],"url":"http://arxiv.org/abs/2402.18286v1","category":"cs.CV"}
{"created":"2024-02-28 12:24:27","title":"PiShield: A NeSy Framework for Learning with Requirements","abstract":"Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.","sentences":["Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs.","In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology.","PiShield guarantees compliance with these requirements, regardless of input.","Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs.","Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains.","Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation."],"url":"http://arxiv.org/abs/2402.18285v1","category":"cs.LG"}
{"created":"2024-02-28 12:24:07","title":"Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization","abstract":"Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.","sentences":["Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback.","However, its training pipeline relies on manual ranking, a resource-intensive process.","To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators.","Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.","We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics.","Subsequently, we construct a reward model to learn the rank and optimize our generative policy.","Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores.","Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans.","This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models."],"url":"http://arxiv.org/abs/2402.18284v1","category":"cs.CL"}
{"created":"2024-02-28 12:15:29","title":"Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing","abstract":"White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.","sentences":["White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene.","Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources.","This often results in unnatural outcomes lacking in overall consistency.","To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants.","This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map.","Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively.","Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity.","This capability allows for illumination editing, an application not feasible with prior methods."],"url":"http://arxiv.org/abs/2402.18277v1","category":"cs.CV"}
{"created":"2024-02-28 12:12:37","title":"Fractional Linear Matroid Matching is in quasi-NC","abstract":"The matching and linear matroid intersection problems are solvable in quasi-NC, meaning that there exist deterministic algorithms that run in polylogarithmic time and use quasi-polynomially many parallel processors. However, such a parallel algorithm is unknown for linear matroid matching, which generalizes both of these problems. In this work, we propose a quasi-NC algorithm for fractional linear matroid matching, which is a relaxation of linear matroid matching and commonly generalizes fractional matching and linear matroid intersection. Our algorithm builds upon the connection of fractional matroid matching to non-commutative Edmonds' problem recently revealed by Oki and Soma~(2023). As a corollary, we also solve black-box non-commutative Edmonds' problem with rank-two skew-symmetric coefficients.","sentences":["The matching and linear matroid intersection problems are solvable in quasi-NC, meaning that there exist deterministic algorithms that run in polylogarithmic time and use quasi-polynomially many parallel processors.","However, such a parallel algorithm is unknown for linear matroid matching, which generalizes both of these problems.","In this work, we propose a quasi-NC algorithm for fractional linear matroid matching, which is a relaxation of linear matroid matching and commonly generalizes fractional matching and linear matroid intersection.","Our algorithm builds upon the connection of fractional matroid matching to non-commutative Edmonds' problem recently revealed by Oki and Soma~(2023).","As a corollary, we also solve black-box non-commutative Edmonds' problem with rank-two skew-symmetric coefficients."],"url":"http://arxiv.org/abs/2402.18276v1","category":"cs.CC"}
{"created":"2024-02-28 12:04:05","title":"Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?","abstract":"Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.","sentences":["Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs.","In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms.","Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs.","We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt.","Further study reveals the common interaction mechanisms of LLMs during the discussion."],"url":"http://arxiv.org/abs/2402.18272v1","category":"cs.CL"}
{"created":"2024-02-28 12:00:48","title":"Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach","abstract":"This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures. Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information. This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective multi-modal sensing performance. Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation. Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world.","sentences":["This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures.","Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information.","This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective multi-modal sensing performance.","Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation.","Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world."],"url":"http://arxiv.org/abs/2402.18271v1","category":"eess.SP"}
{"created":"2024-02-28 11:57:12","title":"A Survey on Neural Question Generation: Methods, Applications, and Prospects","abstract":"In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG.","sentences":["In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images.","The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications.","It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities.","This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations.","The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths.","Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG."],"url":"http://arxiv.org/abs/2402.18267v1","category":"cs.CL"}
{"created":"2024-02-28 11:51:56","title":"Retrieval-based Full-length Wikipedia Generation for Emergent Events","abstract":"In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for generating evidence. Additionally, we design a comprehensive set of systematic evaluation metrics and baseline methods, to evaluate the capability of LLMs in generating factual full-length Wikipedia documents. The data and code are open-sourced at WikiGenBench.","sentences":["In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging.","However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements.","Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus.","In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources.","To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for generating evidence.","Additionally, we design a comprehensive set of systematic evaluation metrics and baseline methods, to evaluate the capability of LLMs in generating factual full-length Wikipedia documents.","The data and code are open-sourced at WikiGenBench."],"url":"http://arxiv.org/abs/2402.18264v1","category":"cs.CL"}
{"created":"2024-02-28 11:39:26","title":"A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames","abstract":"Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.","sentences":["Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent.","This configuration significantly limits the surface form of user utterances and the capacity of output semantics.","In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS.","The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases.","Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network.","Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin."],"url":"http://arxiv.org/abs/2402.18258v1","category":"cs.CL"}
{"created":"2024-02-28 11:29:09","title":"Towards Generalist Prompting for Large Language Models by Mental Models","abstract":"Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs.","sentences":["Large language models (LLMs) have demonstrated impressive performance on many tasks.","However, to achieve optimal performance, specially designed prompting methods are still needed.","These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks.","In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems.","Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting.","MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings.","We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs."],"url":"http://arxiv.org/abs/2402.18252v1","category":"cs.CL"}
{"created":"2024-02-28 11:23:08","title":"String Dimension: VC Dimension for Infinite Shattering","abstract":"In computer science, combinatorics, and model theory, the VC dimension is a central notion underlying far-reaching topics such as error rate for decision rules, combinatorial measurements of classes of finite structures, and neo-stability theory. In all cases, it measures the capacity for a collection of sets $\\mathcal{F}\\subseteq\\mathscr{P}(X)$ to shatter subsets of $X$. The VC dimension of this class then takes values in $\\mathbb{N}\\cup\\{\\infty\\}$. We extend this notion to an infinitary framework and use this to generate ideals on $2^\\kappa$ of families of bounded shattering. We explore the cardinals characteristics of ideals generated by this generalised VC dimension, dubbed string dimension, and present various consistency results.   We also introduce the finality of forcing iteration. A $\\kappa$-final iteration is one for which any sequences of ground model elements of length less than $\\kappa$ in the final model must have been introduced at an intermediate stage. This technique is often used for, say, controlling sets of real numbers when manipulating values of cardinal characteristics, and is often exhibited as a consequence of a chain condition. We demonstrate a precise characterisation of such notions of forcing as a generalisation of distributivity.","sentences":["In computer science, combinatorics, and model theory, the VC dimension is a central notion underlying far-reaching topics such as error rate for decision rules, combinatorial measurements of classes of finite structures, and neo-stability theory.","In all cases, it measures the capacity for a collection of sets $\\mathcal{F}\\subseteq\\mathscr{P}(X)$ to shatter subsets of $X$. The VC dimension of this class then takes values in $\\mathbb{N}\\cup\\{\\infty\\}$. We extend this notion to an infinitary framework and use this to generate ideals on $2^\\kappa$ of families of bounded shattering.","We explore the cardinals characteristics of ideals generated by this generalised VC dimension, dubbed string dimension, and present various consistency results.   ","We also introduce the finality of forcing iteration.","A $\\kappa$-final iteration is one for which any sequences of ground model elements of length less than $\\kappa$ in the final model must have been introduced at an intermediate stage.","This technique is often used for, say, controlling sets of real numbers when manipulating values of cardinal characteristics, and is often exhibited as a consequence of a chain condition.","We demonstrate a precise characterisation of such notions of forcing as a generalisation of distributivity."],"url":"http://arxiv.org/abs/2402.18250v1","category":"math.LO"}
{"created":"2024-02-28 11:23:07","title":"Essential implications of similarities in non-Hermitian systems","abstract":"In this paper, we show that three different generalized similarities enclose all unitary and anti-unitary symmetries that induce exceptional points in lower-dimensional non-Hermitian systems. We prove that the generalized similarity conditions result in a larger class of systems than any class defined by a unitary or anti-unitary symmetry. Further we highlight that the similarities enforce spectral symmetry on the Hamiltonian resulting in a reduction of the codimension of exceptional points. As a consequence we show that the similarities drive the emergence of exceptional points in lower dimensions without the more restrictive need for a unitary and/or anti-unitary symmetry.","sentences":["In this paper, we show that three different generalized similarities enclose all unitary and anti-unitary symmetries that induce exceptional points in lower-dimensional non-Hermitian systems.","We prove that the generalized similarity conditions result in a larger class of systems than any class defined by a unitary or anti-unitary symmetry.","Further we highlight that the similarities enforce spectral symmetry on the Hamiltonian resulting in a reduction of the codimension of exceptional points.","As a consequence we show that the similarities drive the emergence of exceptional points in lower dimensions without the more restrictive need for a unitary and/or anti-unitary symmetry."],"url":"http://arxiv.org/abs/2402.18249v1","category":"quant-ph"}
{"created":"2024-02-28 11:21:50","title":"Lighting Dark Ages with Tomographic ISW Effect","abstract":"The integrated Sachs-Wolfe effect (ISW) describes how CMB photons pick up a net blue or redshift when traversing the time-varying gravitational potentials between the last scattering surface and us. Deviations from its standard amplitude could hint new physics. We show that reconstructing the amplitude of the ISW effect as a function of the redshift may provide a unique tool to probe the gravity sector during the era of dark ages, inaccessible via other cosmological observables. Exploiting Planck CMB temperature, polarization and lensing observations, we find a $2\\sigma$ deviation from the standard ISW amplitude at redshift $z=500$. Barrying a systematic origin, our findings could point to either possibly new physics or a departure from the standard picture of structure formation under the General Relativity framework. Assuming the simplest two-redshift-bin scenario, we ensure $38\\sigma$ and $2\\sigma$ evidences of the early and late ISW effects, respectively, despite a priori possible degeneracy with the CMB lensing amplitude. Using a multiple tomographic method, we present the first complete characterization of the ISW effect over space and time. Future tomographic ISW analyses are therefore crucial to probe the dark ages at redshifts otherwise unreachable via other probes.","sentences":["The integrated Sachs-Wolfe effect (ISW) describes how CMB photons pick up a net blue or redshift when traversing the time-varying gravitational potentials between the last scattering surface and us.","Deviations from its standard amplitude could hint new physics.","We show that reconstructing the amplitude of the ISW effect as a function of the redshift may provide a unique tool to probe the gravity sector during the era of dark ages, inaccessible via other cosmological observables.","Exploiting Planck CMB temperature, polarization and lensing observations, we find a $2\\sigma$ deviation from the standard ISW amplitude at redshift $z=500$. Barrying a systematic origin, our findings could point to either possibly new physics or a departure from the standard picture of structure formation under the General Relativity framework.","Assuming the simplest two-redshift-bin scenario, we ensure $38\\sigma$ and $2\\sigma$ evidences of the early and late ISW effects, respectively, despite a priori possible degeneracy with the CMB lensing amplitude.","Using a multiple tomographic method, we present the first complete characterization of the ISW effect over space and time.","Future tomographic ISW analyses are therefore crucial to probe the dark ages at redshifts otherwise unreachable via other probes."],"url":"http://arxiv.org/abs/2402.18248v1","category":"astro-ph.CO"}
{"created":"2024-02-28 11:21:32","title":"Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment","abstract":"This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of Reinforcement Learning (RL) and Graph Neural Networks (GNNs). The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees. This paper's original idea is to apply RL algorithms to solve a PRA model represented with a graph model. Given enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods. Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems.","sentences":["This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of Reinforcement Learning (RL) and Graph Neural Networks (GNNs).","The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees.","This paper's original idea is to apply RL algorithms to solve a PRA model represented with a graph model.","Given enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods.","Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems."],"url":"http://arxiv.org/abs/2402.18246v1","category":"eess.SY"}
{"created":"2024-02-28 11:16:07","title":"Four-Dimensional Phase-Space Reconstruction of Flat and Magnetized Beams Using Neural Networks and Differentiable Simulations","abstract":"Beams with cross-plane coupling or extreme asymmetries between the two transverse phase spaces are often encountered in particle accelerators. Flat beams with large transverse-emittance ratios are critical for future linear colliders. Similarly, magnetized beams with significant cross-plane coupling are expected to enhance the performance of electron cooling in hadron beams. Preparing these beams requires precise control and characterization of the four-dimensional transverse phase space. In this study, we employ generative phase space reconstruction (GPSR) techniques to rapidly characterize magnetized and flat-beam phase-space distributions using a conventional quadrupole-scan method. The reconstruction technique is experimentally demonstrated on an electron beam produced at the Argonne Wakefield Accelerator and successfully benchmarked against conventional diagnostics techniques. Specifically, we show that predicted beam parameters from the reconstructed phase-space distributions (e.g. as magnetization and flat beam emittances) are in excellent agreement with those measured from the conventional diagnostic methods.","sentences":["Beams with cross-plane coupling or extreme asymmetries between the two transverse phase spaces are often encountered in particle accelerators.","Flat beams with large transverse-emittance ratios are critical for future linear colliders.","Similarly, magnetized beams with significant cross-plane coupling are expected to enhance the performance of electron cooling in hadron beams.","Preparing these beams requires precise control and characterization of the four-dimensional transverse phase space.","In this study, we employ generative phase space reconstruction (GPSR) techniques to rapidly characterize magnetized and flat-beam phase-space distributions using a conventional quadrupole-scan method.","The reconstruction technique is experimentally demonstrated on an electron beam produced at the Argonne Wakefield Accelerator and successfully benchmarked against conventional diagnostics techniques.","Specifically, we show that predicted beam parameters from the reconstructed phase-space distributions (e.g. as magnetization and flat beam emittances) are in excellent agreement with those measured from the conventional diagnostic methods."],"url":"http://arxiv.org/abs/2402.18244v1","category":"physics.acc-ph"}
{"created":"2024-02-28 11:06:45","title":"Quantum state of Yang-Mills fields in $SU(\\infty)$ Quantum Gravity ($SU(\\infty)$-QGR)","abstract":"Our Universe is ruled by quantum mechanics and should be treated as a quantum system. $SU(\\infty)$-QGR is a recently proposed quantum model for the Universe, in which gravity is associated to $SU(\\infty)$ symmetry of its Hilbert space. Clustering of its infinite dimensional state due to random quantum fluctuations divides the Universe to approximately isolated subsystems. In addition to parameters of their internal finite rank symmetries, states and dynamics of subsystems are characterized by 4 continuous parameters, and the perceived classical spacetime is their effective representation, reflecting quantum states of subsystems and theirrelative evolution. At lowest order the effective Lagrangian of $SU(\\infty)$-QGR has the form of Yang-Mills gauge theories for both $SU(\\infty)$ - gravity - and internal symmetries defined on the aforementioned 4D parameter space. In the present work we study more thoroughly some of the fundamental aspects of $SU(\\infty)$-QGR. Specifically, we clarify impact of the degeneracy of $\\mathcal{SU}(\\infty)$; describe mixed states of subsystems and their purification; calculate measures of their entanglement to the rest of the Universe; and discuss their role in the emergence of local gauge symmetries. We also describe the relationship between what is called internal space of $SU(\\infty)$ Yang-Mills with the 4D parameter space, and analytically demonstrate irrelevance of the geometry parameter space for physical observables. Along with these topics, we demonstrate the equivalence of two sets of criteria for compositeness of a quantum system, and show uniqueness of the limit of various algebras leading to $SU(\\infty)$.","sentences":["Our Universe is ruled by quantum mechanics and should be treated as a quantum system.","$SU(\\infty)$-QGR is a recently proposed quantum model for the Universe, in which gravity is associated to $SU(\\infty)$ symmetry of its Hilbert space.","Clustering of its infinite dimensional state due to random quantum fluctuations divides the Universe to approximately isolated subsystems.","In addition to parameters of their internal finite rank symmetries, states and dynamics of subsystems are characterized by 4 continuous parameters, and the perceived classical spacetime is their effective representation, reflecting quantum states of subsystems and theirrelative evolution.","At lowest order the effective Lagrangian of $SU(\\infty)$-QGR has the form of Yang-Mills gauge theories for both $SU(\\infty)$ - gravity - and internal symmetries defined on the aforementioned 4D parameter space.","In the present work we study more thoroughly some of the fundamental aspects of $SU(\\infty)$-QGR.","Specifically, we clarify impact of the degeneracy of $\\mathcal{SU}(\\infty)$; describe mixed states of subsystems and their purification; calculate measures of their entanglement to the rest of the Universe; and discuss their role in the emergence of local gauge symmetries.","We also describe the relationship between what is called internal space of $SU(\\infty)$ Yang-Mills with the 4D parameter space, and analytically demonstrate irrelevance of the geometry parameter space for physical observables.","Along with these topics, we demonstrate the equivalence of two sets of criteria for compositeness of a quantum system, and show uniqueness of the limit of various algebras leading to $SU(\\infty)$."],"url":"http://arxiv.org/abs/2402.18237v1","category":"gr-qc"}
{"created":"2024-02-28 11:01:14","title":"Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data","abstract":"Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics. However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming simulation. This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields.   This study used 135 3D cardiac MRIs from both a public and private dataset. The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes. CFD simulations were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset. The dataset was split 85/10/15 for training, validation and testing. Image2Flow, a hybrid image and graph convolutional neural network, was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values. Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons. Centerline comparisons of Image2Flow and CFD simulations performed using machine learning segmentation were also performed.   Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92). The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively. Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes.   This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation. Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment.","sentences":["Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics.","However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming simulation.","This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields.   ","This study used 135 3D cardiac MRIs from both a public and private dataset.","The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes.","CFD simulations were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset.","The dataset was split 85/10/15 for training, validation and testing.","Image2Flow, a hybrid image and graph convolutional neural network, was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values.","Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons.","Centerline comparisons of Image2Flow and CFD simulations performed using machine learning segmentation were also performed.   ","Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92).","The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively.","Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes.   ","This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation.","Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment."],"url":"http://arxiv.org/abs/2402.18236v1","category":"cs.CV"}
{"created":"2024-02-28 11:00:38","title":"On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks","abstract":"Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.","sentences":["Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM).","These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups.","Until recently, the main focus of research was the structural characteristics of this model.","The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion.","A total of 26 datasets were collected for this work.","It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion.","However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures.","These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses.","Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships."],"url":"http://arxiv.org/abs/2402.18235v1","category":"cs.SI"}
{"created":"2024-02-28 10:59:10","title":"Extreme ultraviolet lithography reaches 5 nm resolution","abstract":"Extreme ultraviolet (EUV) lithography is the leading lithography technique in CMOS mass production, moving towards the sub-10 nm half-pitch (HP) regime with the ongoing development of the next generation high-numerical aperture (high-NA) EUV scanners. Hitherto, EUV interference lithography (EUV-IL) utilizing transmission gratings has been a powerful patterning tool for the early development of EUV resists and related processes, playing a key role in exploring and pushing the boundaries of photon-based lithography. However, achieving pattering with HPs well below 10 nm using this method presents significant challenges. In response, our study introduces a novel EUV-IL setup that employs mirror-based technology and circumvents the limitations of diffraction efficiency towards the diffraction limit that is inherent in conventional grating-based approaches. We present line/space patterning of HSQ resist down to HP 5 nm using the standard EUV wavelength 13.5 nm, and the compatibility of the tool with shorter wavelengths beyond EUV. The mirror-based interference lithography tool paves the way towards the ultimate photon-based resolution at EUV wavelengths and beyond. This advancement is vital for scientific and industrial research, addressing the increasingly challenging needs of nanoscience and technology and future technology nodes of CMOS manufacturing in the few-nanometer HP regime.","sentences":["Extreme ultraviolet (EUV) lithography is the leading lithography technique in CMOS mass production, moving towards the sub-10 nm half-pitch (HP) regime with the ongoing development of the next generation high-numerical aperture (high-NA) EUV scanners.","Hitherto, EUV interference lithography (EUV-IL) utilizing transmission gratings has been a powerful patterning tool for the early development of EUV resists and related processes, playing a key role in exploring and pushing the boundaries of photon-based lithography.","However, achieving pattering with HPs well below 10 nm using this method presents significant challenges.","In response, our study introduces a novel EUV-IL setup that employs mirror-based technology and circumvents the limitations of diffraction efficiency towards the diffraction limit that is inherent in conventional grating-based approaches.","We present line/space patterning of HSQ resist down to HP 5 nm using the standard EUV wavelength 13.5 nm, and the compatibility of the tool with shorter wavelengths beyond EUV.","The mirror-based interference lithography tool paves the way towards the ultimate photon-based resolution at EUV wavelengths and beyond.","This advancement is vital for scientific and industrial research, addressing the increasingly challenging needs of nanoscience and technology and future technology nodes of CMOS manufacturing in the few-nanometer HP regime."],"url":"http://arxiv.org/abs/2402.18234v1","category":"physics.optics"}
{"created":"2024-02-28 10:58:01","title":"Zero-Shot Aerial Object Detection with Visual Description Regularization","abstract":"Existing object detection models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient object detection methods on aerial images. In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture.","sentences":["Existing object detection models are mainly trained on large-scale labeled datasets.","However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge.","Thus, it is desirable to study label-efficient object detection methods on aerial images.","In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg.","Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance.","Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning.","The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space.","We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA.","The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM.","We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture."],"url":"http://arxiv.org/abs/2402.18233v1","category":"cs.CV"}
{"created":"2024-02-28 10:53:17","title":"Practical Derivations of Fermion and Gauge Boson Reduction Formulae in Curved Spacetimes","abstract":"LSZ-type reduction formulae are derived for gauge fields and fermions in curved spacetime. The formulae are derived using a conserved current method applicable also to flat spacetimes. The method generalizes to more general quantum field theories. The formulae are then applied to a few problems to illustrate their use.","sentences":["LSZ-type reduction formulae are derived for gauge fields and fermions in curved spacetime.","The formulae are derived using a conserved current method applicable also to flat spacetimes.","The method generalizes to more general quantum field theories.","The formulae are then applied to a few problems to illustrate their use."],"url":"http://arxiv.org/abs/2402.18230v1","category":"hep-th"}
{"created":"2024-02-28 10:49:53","title":"$E$-theory is compactly assembled","abstract":"We show that the equivariant $E$-theory category $\\mathrm{E}_{\\mathrm{sep}}^{G}$ for separable $C^{*}$-algebras is a compactly assembled stable $\\infty$-category. We derive this result as a consequence of the shape theory for $C^{*}$-algebras developed by Blackadar and Dardarlat and a new construction of $\\mathrm{E}_{\\mathrm{sep}}^{G}$. As an application we investigate a topological enrichment of the homotopy category of a compactly assembled $\\infty$-category in general and argue that the results of Carri\\'on and Schafhauser on the enrichment of the classical $E$-theory category can be derived by specialization.","sentences":["We show that the equivariant $E$-theory category $\\mathrm{E}_{\\mathrm{sep}}^{G}$ for separable $C^{*}$-algebras is a compactly assembled stable $\\infty$-category.","We derive this result as a consequence of the shape theory for $C^{*}$-algebras developed by Blackadar and Dardarlat and a new construction of $\\mathrm{E}_{\\mathrm{sep}}^{G}$. As an application we investigate a topological enrichment of the homotopy category of a compactly assembled $\\infty$-category in general and argue that the results of Carri\\'on and Schafhauser on the enrichment of the classical $E$-theory category can be derived by specialization."],"url":"http://arxiv.org/abs/2402.18228v1","category":"math.KT"}
{"created":"2024-02-28 10:46:39","title":"Drazin Inverses in Categories","abstract":"Drazin inverses are a fundamental algebraic structure which have been extensively deployed in semigroup theory and ring theory. Drazin inverses can also be defined for endomorphisms in any category. However, beyond a paper by Puystjens and Robinson from 1987, not much has been done with Drazin inverses in category theory. As such, here we provide a survey of the theory of Drazin inverses from a categorical perspective. We introduce Drazin categories, in which every endomorphism has a Drazin inverse, and provide various examples including the category of matrices over a field, the category of finite length modules over a ring, and finite set enriched categories. We also introduce the notion of expressive rank and prove that a category with expressive rank is Drazin. Moreover, we study Drazin inverses in mere categories, in additive categories, and in dagger categories. In an arbitrary category, we show how a Drazin inverse corresponds to an isomorphism in the idempotent splitting, as well as explain how Drazin inverses relate to Leinster's notion of eventual image duality. In additive categories, we explore the consequences of the core-nilpotent decomposition and the image-kernel decomposition, which we relate back to Fitting's famous results. We then develop the notion of Drazin inverses for pairs of opposing maps, generalizing the usual notion of Drazin inverse for endomorphisms. As an application of this new kind of Drazin inverse, for dagger categories, we provide a novel characterization of the Moore-Penrose inverse in terms of being a Drazin inverse of the pair of a map and its adjoint.","sentences":["Drazin inverses are a fundamental algebraic structure which have been extensively deployed in semigroup theory and ring theory.","Drazin inverses can also be defined for endomorphisms in any category.","However, beyond a paper by Puystjens and Robinson from 1987, not much has been done with Drazin inverses in category theory.","As such, here we provide a survey of the theory of Drazin inverses from a categorical perspective.","We introduce Drazin categories, in which every endomorphism has a Drazin inverse, and provide various examples including the category of matrices over a field, the category of finite length modules over a ring, and finite set enriched categories.","We also introduce the notion of expressive rank and prove that a category with expressive rank is Drazin.","Moreover, we study Drazin inverses in mere categories, in additive categories, and in dagger categories.","In an arbitrary category, we show how a Drazin inverse corresponds to an isomorphism in the idempotent splitting, as well as explain how Drazin inverses relate to Leinster's notion of eventual image duality.","In additive categories, we explore the consequences of the core-nilpotent decomposition and the image-kernel decomposition, which we relate back to Fitting's famous results.","We then develop the notion of Drazin inverses for pairs of opposing maps, generalizing the usual notion of Drazin inverse for endomorphisms.","As an application of this new kind of Drazin inverse, for dagger categories, we provide a novel characterization of the Moore-Penrose inverse in terms of being a Drazin inverse of the pair of a map and its adjoint."],"url":"http://arxiv.org/abs/2402.18226v1","category":"math.CT"}
{"created":"2024-02-28 10:43:54","title":"CogBench: a large language model walks into a psychology lab","abstract":"Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.","sentences":["Large language models (LLMs) have significantly advanced the field of artificial intelligence.","Yet, evaluating them comprehensively remains challenging.","We argue that this is partly due to the predominant focus on performance metrics in most benchmarks.","This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments.","This novel approach offers a toolkit for phenotyping LLMs' behavior.","We apply CogBench to 35 LLMs, yielding a rich and diverse dataset.","We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs.","Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior.","Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.","Finally, we explore the effects of prompt-engineering techniques.","We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors."],"url":"http://arxiv.org/abs/2402.18225v1","category":"cs.CL"}
{"created":"2024-02-28 10:42:00","title":"Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases","abstract":"In terms of complex radio environments especially in dense urban areas, a very interesting topic is considered - the utilization of reconfigurable intelligent surfaces. Basically, based on simple controls of the angle of reflection of the signal from the surface, it is possible to achieve different effects in a radio communication system. Maximizing or minimizing the received power at specific locations near the reflecting surface is the most important effect. Thanks to this, it is possible to: receive a signal in a place where it was not possible, detect spectrum occupancy in a place where the sensor could not make a correct detection, or minimize interference in a specific receiver. In this paper, all three concepts are presented, and, using a simple ray tracing simulation, the potential profit in each scenario is shown. In addition, a scenario was analyzed in which several of the aforementioned situations are combined.","sentences":["In terms of complex radio environments especially in dense urban areas, a very interesting topic is considered - the utilization of reconfigurable intelligent surfaces.","Basically, based on simple controls of the angle of reflection of the signal from the surface, it is possible to achieve different effects in a radio communication system.","Maximizing or minimizing the received power at specific locations near the reflecting surface is the most important effect.","Thanks to this, it is possible to: receive a signal in a place where it was not possible, detect spectrum occupancy in a place where the sensor could not make a correct detection, or minimize interference in a specific receiver.","In this paper, all three concepts are presented, and, using a simple ray tracing simulation, the potential profit in each scenario is shown.","In addition, a scenario was analyzed in which several of the aforementioned situations are combined."],"url":"http://arxiv.org/abs/2402.18224v1","category":"cs.NI"}
{"created":"2024-02-28 10:38:21","title":"Improving Open-Ended Text Generation via Adaptive Decoding","abstract":"Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.com/zwhong714/adaptive_decoding.","sentences":["Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality.","This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically.","Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process.","The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively.","The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms.","The code is available at https://github.com/zwhong714/adaptive_decoding."],"url":"http://arxiv.org/abs/2402.18223v1","category":"cs.CL"}
{"created":"2024-02-28 10:37:14","title":"HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System","abstract":"Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political classification model, and users can experience diverse perspectives (Hear). Visualization 2 allows users to express their opinions on specific political issues in a comment form and observe the position of their own opinions relative to pro-liberal and pro-conservative comments presented on a map interface (Here). Through a user study with 94 participants, we demonstrate the feasibility of HearHere in supporting the consumption of information from various perspectives. Our findings highlight the importance of providing political stance information and quantifying users' political status as a means to mitigate political polarization. In addition, we propose design implications for system development, including the consideration of demographics such as political interest and providing users with initiatives.","sentences":["Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence.","Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect.","However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information.","In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives.","HearHere facilitates the key processes of news information consumption through two visualizations.","Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political classification model, and users can experience diverse perspectives (Hear).","Visualization 2 allows users to express their opinions on specific political issues in a comment form and observe the position of their own opinions relative to pro-liberal and pro-conservative comments presented on a map interface (Here).","Through a user study with 94 participants, we demonstrate the feasibility of HearHere in supporting the consumption of information from various perspectives.","Our findings highlight the importance of providing political stance information and quantifying users' political status as a means to mitigate political polarization.","In addition, we propose design implications for system development, including the consideration of demographics such as political interest and providing users with initiatives."],"url":"http://arxiv.org/abs/2402.18222v1","category":"cs.HC"}
{"created":"2024-02-28 10:30:31","title":"Thermophysical Properties of Molten FLiNaK: a Moment Tensor Potential Approach","abstract":"Fluoride salts demonstrate significant potential for applications in next-generation nuclear reactors, necessitating a comprehensive understanding of their thermophysical properties for technological advancements. Experimental measurement of these properties poses challenges, due to factors such as high temperatures, impurity control, and corrosion. Consequently, precise computational modeling methods become essential for predicting the thermophysical properties of molten salts. In this work, we performed molecular dynamics (MD) simulations of several thermophysical properties of the eutectic salt mixture LiF-NaF-KF (FLiNaK) melt, including density, self-diffusion coefficients, viscosity, and thermal conductivity. We demonstrated the successful application of moment tensor potentials (MTP) as an accurate model for interatomic interactions in FLiNaK. Our results on thermophysical properties calculations exhibit strong agreement with experimental data. An important aspect of our methodology is the incorporation of an active learning scheme, which enables the generation of a robust and accurate potential, while maintaining a moderate-sized training dataset.","sentences":["Fluoride salts demonstrate significant potential for applications in next-generation nuclear reactors, necessitating a comprehensive understanding of their thermophysical properties for technological advancements.","Experimental measurement of these properties poses challenges, due to factors such as high temperatures, impurity control, and corrosion.","Consequently, precise computational modeling methods become essential for predicting the thermophysical properties of molten salts.","In this work, we performed molecular dynamics (MD) simulations of several thermophysical properties of the eutectic salt mixture LiF-NaF-KF (FLiNaK) melt, including density, self-diffusion coefficients, viscosity, and thermal conductivity.","We demonstrated the successful application of moment tensor potentials (MTP) as an accurate model for interatomic interactions in FLiNaK. Our results on thermophysical properties calculations exhibit strong agreement with experimental data.","An important aspect of our methodology is the incorporation of an active learning scheme, which enables the generation of a robust and accurate potential, while maintaining a moderate-sized training dataset."],"url":"http://arxiv.org/abs/2402.18220v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 10:24:36","title":"Region-Aware Exposure Consistency Network for Mixed Exposure Correction","abstract":"Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet.","sentences":["Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects.","Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions.","The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process.","In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations.","Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space.","Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information.","To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity.","Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method.","The code is released at: https://github.com/kravrolens/RECNet."],"url":"http://arxiv.org/abs/2402.18217v1","category":"cs.CV"}
{"created":"2024-02-28 10:19:05","title":"LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History","abstract":"With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation.","sentences":["With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications.","When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation.","To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history.","Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch.","To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history.","Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation."],"url":"http://arxiv.org/abs/2402.18216v1","category":"cs.CL"}
{"created":"2024-02-28 10:11:23","title":"Weakly toll convexity in graph products","abstract":"The exploration of weakly toll convexity is the focus of this investigation. A weakly toll walk is any walk $W: u, w_1, \\ldots , w_{k-1}, v$ between $u$ and $v$ such that $u$ is adjacent only to the vertex $w_1$, which can appear more than once in the walk, and $v$ is adjacent only to the vertex $w_{k-1}$, which can appear more than once in the walk. Through an examination of general graphs and an analysis of weakly toll intervals in both lexicographic and (generalized) corona product graphs, precise values of the weakly toll number for these product graphs are obtained. Notably, in both instances, the weakly toll number is constrained to either 2 or 3. Additionally, the determination of the weakly toll number for the Cartesian and the strong product graphs is established through previously established findings in toll convexity theory. Lastly for all graph products examined within our scope, the weakly toll hull number is consistently determined to be 2.","sentences":["The exploration of weakly toll convexity is the focus of this investigation.","A weakly toll walk is any walk $W: u, w_1, \\ldots , w_{k-1}, v$ between $u$ and $v$ such that $u$ is adjacent only to the vertex $w_1$, which can appear more than once in the walk, and $v$ is adjacent only to the vertex $w_{k-1}$, which can appear more than once in the walk.","Through an examination of general graphs and an analysis of weakly toll intervals in both lexicographic and (generalized) corona product graphs, precise values of the weakly toll number for these product graphs are obtained.","Notably, in both instances, the weakly toll number is constrained to either 2 or 3.","Additionally, the determination of the weakly toll number for the Cartesian and the strong product graphs is established through previously established findings in toll convexity theory.","Lastly for all graph products examined within our scope, the weakly toll hull number is consistently determined to be 2."],"url":"http://arxiv.org/abs/2402.18214v1","category":"math.CO"}
{"created":"2024-02-28 10:01:14","title":"Pull-back and push-forward functors for holonomic modules over Cherednik algebras","abstract":"In this article we continue the study of holonomic modules over sheaves of Cherednik algebras, initiated by the third author in [Tho18]. Working with arbitrary parameters, we first develop a theory of $b$-functions to prove that push-forward along open embeddings preserves holonomicity. This implies that pull-back along closed embeddings also preserves holonomicity. We use these facts to show that both push-forward and pull-back under any melys morphism preserves holonomicity. Since duality preserves holonomicity, we deduce that extraordinary push-forward and extraordinary pull-back also exist for holonomic modules.   As a consequence, we give a general classification of irreducible holonomic modules similar to the classification of irreducible holonomic $\\mathscr{D}$-modules as minimal extensions of integrable connections on locally closed subsets. Finally, we prove that ext-groups between holonomic modules are finite-dimensional and explore applications of our work to the classification of aspherical parameters and existence of finite-dimensional modules for sheaves of Cherednik algebras.","sentences":["In this article we continue the study of holonomic modules over sheaves of Cherednik algebras, initiated by the third author in [Tho18].","Working with arbitrary parameters, we first develop a theory of $b$-functions to prove that push-forward along open embeddings preserves holonomicity.","This implies that pull-back along closed embeddings also preserves holonomicity.","We use these facts to show that both push-forward and pull-back under any melys morphism preserves holonomicity.","Since duality preserves holonomicity, we deduce that extraordinary push-forward and extraordinary pull-back also exist for holonomic modules.   ","As a consequence, we give a general classification of irreducible holonomic modules similar to the classification of irreducible holonomic $\\mathscr{D}$-modules as minimal extensions of integrable connections on locally closed subsets.","Finally, we prove that ext-groups between holonomic modules are finite-dimensional and explore applications of our work to the classification of aspherical parameters and existence of finite-dimensional modules for sheaves of Cherednik algebras."],"url":"http://arxiv.org/abs/2402.18210v1","category":"math.QA"}
{"created":"2024-02-28 10:01:00","title":"DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition","abstract":"Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish NER is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed. Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work on the generalizability within Danish NER.","sentences":["Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research.","However, Danish NER is inhibited by a lack of available datasets.","As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains.","To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains.","The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field.","Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed.","Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work on the generalizability within Danish NER."],"url":"http://arxiv.org/abs/2402.18209v1","category":"cs.CL"}
{"created":"2024-02-28 09:59:31","title":"Shorts on the Rise: Assessing the Effects of YouTube Shorts on Long-Form Video Content","abstract":"Short form content has permeated into the video creator space over the past few years, led by industry leading products such as TikTok, YouTube Shorts and Instagram Reels. YouTube in particular was previously synonymous with being the main hub for long form video content consumption. The monetization of long form videos was easier as it allowed multiple advertisement placements during the course of the video. This model also facilitated thematic brand partnerships. However, since the introduction of short form content, creators have found it more difficult to generate revenue as advertisement placements have decreased. This leads to a unique situation where people are spending more time watching shorter videos, and yet they generate less revenue for the creators. In this paper, we perform a study of 250 creators with significant audiences to see if the introduction of short form content has affected the view counts and engagement of long form content. Our findings reveal a noteworthy trend: since the advent of short-form content, there has been a significant decrease in both view counts and engagement in long-form videos on these channels.","sentences":["Short form content has permeated into the video creator space over the past few years, led by industry leading products such as TikTok, YouTube Shorts and Instagram Reels.","YouTube in particular was previously synonymous with being the main hub for long form video content consumption.","The monetization of long form videos was easier as it allowed multiple advertisement placements during the course of the video.","This model also facilitated thematic brand partnerships.","However, since the introduction of short form content, creators have found it more difficult to generate revenue as advertisement placements have decreased.","This leads to a unique situation where people are spending more time watching shorter videos, and yet they generate less revenue for the creators.","In this paper, we perform a study of 250 creators with significant audiences to see if the introduction of short form content has affected the view counts and engagement of long form content.","Our findings reveal a noteworthy trend: since the advent of short-form content, there has been a significant decrease in both view counts and engagement in long-form videos on these channels."],"url":"http://arxiv.org/abs/2402.18208v1","category":"cs.SI"}
{"created":"2024-02-28 09:53:17","title":"Balancing Act: Distribution-Guided Debiasing in Diffusion Models","abstract":"Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.","sentences":["Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability.","These models are widely used for data augmentation and creative applications.","However, DMs reflect the biases present in the training datasets.","This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male).","In this work, we present a method for debiasing DMs without relying on additional data or model retraining.","Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution.","To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation.","We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes.","ADP is trained with pseudo labels generated from existing attribute classifiers.","The proposed Distribution Guidance with ADP enables us to do fair generation.","Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models.","Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data."],"url":"http://arxiv.org/abs/2402.18206v1","category":"cs.CV"}
{"created":"2024-02-28 09:51:55","title":"Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging","abstract":"Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \\textbf{L}og parsing framework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs). LLMs exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens. We have conducted experiments on large-scale public datasets. Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency.","sentences":["Logs produced by extensive software systems are integral to monitoring system behaviors.","Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults.","Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics.","Existing log parsers fail to identify the correct templates due to reliance on human-made rules.","Besides, These methods focus on statistical features while ignoring semantic information in log messages.","To address these challenges, we introduce a cutting-edge \\textbf{L}og parsing framework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging (Lemur).","Specifically, to discard the tedious manual rules.","We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs.","Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs).","LLMs exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens.","We have conducted experiments on large-scale public datasets.","Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency."],"url":"http://arxiv.org/abs/2402.18205v1","category":"cs.SE"}
{"created":"2024-02-28 09:46:56","title":"Learning Invariant Inter-pixel Correlations for Superpixel Generation","abstract":"Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel.","sentences":["Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones.","Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset.","Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios.","To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise.","Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations.","Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations.","Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles.","The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency.","Code and pre-trained model are available at https://github.com/rookiie/CDSpixel."],"url":"http://arxiv.org/abs/2402.18201v1","category":"cs.CV"}
{"created":"2024-02-28 09:40:47","title":"Quantitative investigation of quantum emitter yield in drop-casted hexagonal boron nitride nanoflakes","abstract":"Single photon emitters (SPEs) are a key component for their use as pure photon source in quantum technologies. In this study, we investigate the generation of SPEs from drop-casted hexagonal boron nitride (hBN) nanoflakes, examining the influence of the immersion solution and the source of hBN. We show that, depending on the utilized supplier and solution the number and quality of the emitters changes. We perform a comprehensive optical characterization of the deposited nanoflakes to assess the quality of the generated SPEs. We show quantitative data on SPE yields, highlighting significant variations among solvents and different sources of hBN. This holds particular significance for employing drop-casted nanoflakes as SPE sources in quantum communication, sensing, and imaging. Our method is easily expandable to all kinds of surfaces and can be done without requiring complex fabrication steps and equipment, thus providing the necessary scalability required for industrial quantum applications.","sentences":["Single photon emitters (SPEs) are a key component for their use as pure photon source in quantum technologies.","In this study, we investigate the generation of SPEs from drop-casted hexagonal boron nitride (hBN) nanoflakes, examining the influence of the immersion solution and the source of hBN.","We show that, depending on the utilized supplier and solution the number and quality of the emitters changes.","We perform a comprehensive optical characterization of the deposited nanoflakes to assess the quality of the generated SPEs.","We show quantitative data on SPE yields, highlighting significant variations among solvents and different sources of hBN.","This holds particular significance for employing drop-casted nanoflakes as SPE sources in quantum communication, sensing, and imaging.","Our method is easily expandable to all kinds of surfaces and can be done without requiring complex fabrication steps and equipment, thus providing the necessary scalability required for industrial quantum applications."],"url":"http://arxiv.org/abs/2402.18199v1","category":"physics.optics"}
{"created":"2024-02-28 09:40:36","title":"Automated Machine Learning for Multi-Label Classification","abstract":"Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand. For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results. However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far. In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies. While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders.   In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most. This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods. Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification.   In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems.","sentences":["Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand.","For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results.","However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far.","In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies.","While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders.   ","In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most.","This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods.","Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification.   ","In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems."],"url":"http://arxiv.org/abs/2402.18198v1","category":"cs.LG"}
{"created":"2024-02-28 09:36:22","title":"NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images","abstract":"Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain. However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective. Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after finetuning on our training set. A similarly finetuned HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.","sentences":["Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain.","However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations.","Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective.","Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation.","A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after finetuning on our training set.","A similarly finetuned HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set."],"url":"http://arxiv.org/abs/2402.18196v1","category":"cs.CV"}
{"created":"2024-02-28 09:27:07","title":"Generic Global Rigidity in $\\ell_p$-Space and the Identifiability of the $p$-Cayley-Menger Varieties","abstract":"The celebrated result of Gortler-Healy-Thurston (independently, Jackson-Jord\\'an for $d=2$) shows that the global rigidity of graphs realised in the $d$-dimensional Euclidean space is a generic property. Extending this result to the global rigidity problem in $\\ell_p$-spaces remains an open problem. In this paper we affirmatively solve this problem when $d=2$ and $p$ is an even positive integer. A key tool in our proof is a sufficient condition for the $d$-tangentially weakly non-defectiveness of projective varieties due to Bocci, Chiantini, Ottaviani, and Vannieuwenhoven. By specialising the condition to the $p$-Cayley-Menger variety, which is the $\\ell_p$-analogue of the Cayley-Menger variety for Euclidean distance, we provide an $\\ell_p$-extension of the generic global rigidity theory of Connelly. As a by-product of our proof, we also offer a purely graph-theoretical characterisation of the $2$-identifiability of an orthogonal projection of the $p$-Cayley-Menger variety along a coordinate axis of the ambient affine space.","sentences":["The celebrated result of Gortler-Healy-Thurston (independently, Jackson-Jord\\'an for $d=2$) shows that the global rigidity of graphs realised in the $d$-dimensional Euclidean space is a generic property.","Extending this result to the global rigidity problem in $\\ell_p$-spaces remains an open problem.","In this paper we affirmatively solve this problem when $d=2$ and $p$ is an even positive integer.","A key tool in our proof is a sufficient condition for the $d$-tangentially weakly non-defectiveness of projective varieties due to Bocci, Chiantini, Ottaviani, and Vannieuwenhoven.","By specialising the condition to the $p$-Cayley-Menger variety, which is the $\\ell_p$-analogue of the Cayley-Menger variety for Euclidean distance, we provide an $\\ell_p$-extension of the generic global rigidity theory of Connelly.","As a by-product of our proof, we also offer a purely graph-theoretical characterisation of the $2$-identifiability of an orthogonal projection of the $p$-Cayley-Menger variety along a coordinate axis of the ambient affine space."],"url":"http://arxiv.org/abs/2402.18190v1","category":"math.MG"}
{"created":"2024-02-28 09:25:55","title":"VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation","abstract":"In recent years, the rapid development of deep learning technology has brought new prospects to the field of vulnerability detection. Many vulnerability detection methods involve converting source code into images for detection, yet they often overlook the quality of the generated images. Due to the fact that vulnerability images lack clear and continuous contours, unlike images used in object detection, Convolutional Neural Networks (CNNs) tend to lose semantic information during the convolution and pooling processes. Therefore, this paper proposes a pixel row oversampling method based on code line concatenation to generate more continuous code features, addressing the issue of discontinuity in code image coloration.Building upon these contributions, we propose the vulnerability detection system VulMCI and conduct tests on the SARD and NVD datasets. Experimental results demonstrate that VulMCI outperforms seven state-of-the-art vulnerability detectors (namely Checkmarx, FlawFinder, RATS, VulDeePecker, SySeVR, VulCNN, and Devign). Compared to other image-based methods, VulMCI shows improvements in various metrics, including a 2.877\\% increase in True Positive Rate (TPR), a 5.446\\% increase in True Negative Rate (TNR), and a 5.91\\% increase in Accuracy (ACC). On the NVD real-world dataset, VulMCI achieves an average accuracy of 5.162\\%, confirming its value in practical vulnerability detection applications.","sentences":["In recent years, the rapid development of deep learning technology has brought new prospects to the field of vulnerability detection.","Many vulnerability detection methods involve converting source code into images for detection, yet they often overlook the quality of the generated images.","Due to the fact that vulnerability images lack clear and continuous contours, unlike images used in object detection, Convolutional Neural Networks (CNNs) tend to lose semantic information during the convolution and pooling processes.","Therefore, this paper proposes a pixel row oversampling method based on code line concatenation to generate more continuous code features, addressing the issue of discontinuity in code image coloration.","Building upon these contributions, we propose the vulnerability detection system VulMCI and conduct tests on the SARD and NVD datasets.","Experimental results demonstrate that VulMCI outperforms seven state-of-the-art vulnerability detectors (namely Checkmarx, FlawFinder, RATS, VulDeePecker, SySeVR, VulCNN, and Devign).","Compared to other image-based methods, VulMCI shows improvements in various metrics, including a 2.877\\% increase in True Positive Rate (TPR), a 5.446\\% increase in True Negative Rate (TNR), and a 5.91\\% increase in Accuracy (ACC).","On the NVD real-world dataset, VulMCI achieves an average accuracy of 5.162\\%, confirming its value in practical vulnerability detection applications."],"url":"http://arxiv.org/abs/2402.18189v1","category":"cs.CR"}
{"created":"2024-02-28 09:24:20","title":"Mass action systems: two criteria for Hopf bifurcation without Hurwitz","abstract":"We state two sufficient criteria for periodic oscillations in mass action systems. Neither criterion requires a computation of the Hurwitz determinants. The first criterion also applies to general kinetics and it concerns fully-open systems: in essence, it guarantees periodic oscillations whenever the system admits a steady state with a simple pair of complex conjugate eigenvalues with positive real part. The second criterion states an algebraic condition based on the steady-state Jacobian matrix expressed in convex coordinates via Stoichiometric Network Analysis. Both criteria have the potential for extensions and generalizations.","sentences":["We state two sufficient criteria for periodic oscillations in mass action systems.","Neither criterion requires a computation of the Hurwitz determinants.","The first criterion also applies to general kinetics and it concerns fully-open systems: in essence, it guarantees periodic oscillations whenever the system admits a steady state with a simple pair of complex conjugate eigenvalues with positive real part.","The second criterion states an algebraic condition based on the steady-state Jacobian matrix expressed in convex coordinates via Stoichiometric Network Analysis.","Both criteria have the potential for extensions and generalizations."],"url":"http://arxiv.org/abs/2402.18188v1","category":"math.DS"}
{"created":"2024-02-28 09:20:55","title":"Quantile Outcome Adaptive Lasso: Covariate Selection for Inverse Probability Weighting Estimator of Quantile Treatment Effects","abstract":"When using the propensity score method to estimate the treatment effects, it is important to select the covariates to be included in the propensity score model. The inclusion of covariates unrelated to the outcome in the propensity score model led to bias and large variance in the estimator of treatment effects. Many data-driven covariate selection methods have been proposed for selecting covariates related to outcomes. However, most of them assume an average treatment effect estimation and may not be designed to estimate quantile treatment effects (QTE), which is the effect of treatment on the quantiles of outcome distribution. In QTE estimation, we consider two relation types with the outcome as the expected value and quantile point. To achieve this, we propose a data-driven covariate selection method for propensity score models that allows for the selection of covariates related to the expected value and quantile of the outcome for QTE estimation. Assuming the quantile regression model as an outcome regression model, covariate selection was performed using a regularization method with the partial regression coefficients of the quantile regression model as weights. The proposed method was applied to artificial data and a dataset of mothers and children born in King County, Washington, to compare the performance of existing methods and QTE estimators. As a result, the proposed method performs well in the presence of covariates related to both the expected value and quantile of the outcome.","sentences":["When using the propensity score method to estimate the treatment effects, it is important to select the covariates to be included in the propensity score model.","The inclusion of covariates unrelated to the outcome in the propensity score model led to bias and large variance in the estimator of treatment effects.","Many data-driven covariate selection methods have been proposed for selecting covariates related to outcomes.","However, most of them assume an average treatment effect estimation and may not be designed to estimate quantile treatment effects (QTE), which is the effect of treatment on the quantiles of outcome distribution.","In QTE estimation, we consider two relation types with the outcome as the expected value and quantile point.","To achieve this, we propose a data-driven covariate selection method for propensity score models that allows for the selection of covariates related to the expected value and quantile of the outcome for QTE estimation.","Assuming the quantile regression model as an outcome regression model, covariate selection was performed using a regularization method with the partial regression coefficients of the quantile regression model as weights.","The proposed method was applied to artificial data and a dataset of mothers and children born in King County, Washington, to compare the performance of existing methods and QTE estimators.","As a result, the proposed method performs well in the presence of covariates related to both the expected value and quantile of the outcome."],"url":"http://arxiv.org/abs/2402.18185v1","category":"stat.ME"}
{"created":"2024-02-28 09:18:17","title":"Computational Offloading in Semantic-Aware Cloud-Edge-End Collaborative Networks","abstract":"The trend of massive connectivity pushes forward the explosive growth of end devices. The emergence of various applications has prompted a demand for pervasive connectivity and more efficient computing paradigms. On the other hand, the lack of computational capacity of the end devices restricts the implementation of the intelligent applications, and becomes a bottleneck of the multiple access for supporting massive connectivity. Mobile cloud computing (MCC) and mobile edge computing (MEC) techniques enable end devices to offload local computation-intensive tasks to servers by networks. In this paper, we consider the cloud-edge-end collaborative networks to utilize distributed computing resources. Furthermore, we apply task-oriented semantic communications to tackle the fast-varying channel between the end devices and MEC servers and reduce the communication cost. To minimize long-term energy consumption on constraints queue stability and computational delay, a Lyapunov-guided deep reinforcement learning hybrid (DRLH) framework is proposed to solve the mixed integer non-linear programming (MINLP) problem. The long-term energy consumption minimization problem is transformed into the deterministic problem in each time frame. The DRLH framework integrates a model-free deep reinforcement learning algorithm with a model-based mathematical optimization algorithm to mitigate computational complexity and leverage the scenario information, so that improving the convergence performance. Numerical results demonstrate that the proposed DRLH framework achieves near-optimal performance on energy consumption while stabilizing all queues.","sentences":["The trend of massive connectivity pushes forward the explosive growth of end devices.","The emergence of various applications has prompted a demand for pervasive connectivity and more efficient computing paradigms.","On the other hand, the lack of computational capacity of the end devices restricts the implementation of the intelligent applications, and becomes a bottleneck of the multiple access for supporting massive connectivity.","Mobile cloud computing (MCC) and mobile edge computing (MEC) techniques enable end devices to offload local computation-intensive tasks to servers by networks.","In this paper, we consider the cloud-edge-end collaborative networks to utilize distributed computing resources.","Furthermore, we apply task-oriented semantic communications to tackle the fast-varying channel between the end devices and MEC servers and reduce the communication cost.","To minimize long-term energy consumption on constraints queue stability and computational delay, a Lyapunov-guided deep reinforcement learning hybrid (DRLH) framework is proposed to solve the mixed integer non-linear programming (MINLP) problem.","The long-term energy consumption minimization problem is transformed into the deterministic problem in each time frame.","The DRLH framework integrates a model-free deep reinforcement learning algorithm with a model-based mathematical optimization algorithm to mitigate computational complexity and leverage the scenario information, so that improving the convergence performance.","Numerical results demonstrate that the proposed DRLH framework achieves near-optimal performance on energy consumption while stabilizing all queues."],"url":"http://arxiv.org/abs/2402.18183v1","category":"eess.SP"}
{"created":"2024-02-28 09:12:01","title":"CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation","abstract":"Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.","sentences":["Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching.","While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues.","In this work, we introduce a framework based on contrastive feature distillation (CFD).","This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features.","This framework helps to enhance model generalization across both clean and foggy environments.","Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method."],"url":"http://arxiv.org/abs/2402.18181v1","category":"cs.CV"}
{"created":"2024-02-28 09:11:14","title":"Human Simulacra: A Step toward the Personification of Large Language Models","abstract":"Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.","sentences":["Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence.","This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity.","In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives.","Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters.","Our work is a preliminary exploration which offers great potential in practical applications.","All the code and datasets will be released, with the hope of inspiring further investigations."],"url":"http://arxiv.org/abs/2402.18180v1","category":"cs.CY"}
{"created":"2024-02-28 09:04:10","title":"Generation of skill-specific maps from graph world models for robotic systems","abstract":"With the increase in the availability of Building Information Models (BIM) and (semi-) automatic tools to generate BIM from point clouds, we propose a world model architecture and algorithms to allow the use of the semantic and geometric knowledge encoded within these models to generate maps for robot localization and navigation. When heterogeneous robots are deployed within an environment, maps obtained from classical SLAM approaches might not be shared between all agents within a team of robots, e.g. due to a mismatch in sensor type, or a difference in physical robot dimensions. Our approach extracts the 3D geometry and semantic description of building elements (e.g. material, element type, color) from BIM, and represents this knowledge in a graph. Based on queries on the graph and knowledge of the skills of the robot, we can generate skill-specific maps that can be used during the execution of localization or navigation tasks. The approach is validated with data from complex build environments and integrated into existing navigation frameworks.","sentences":["With the increase in the availability of Building Information Models (BIM) and (semi-) automatic tools to generate BIM from point clouds, we propose a world model architecture and algorithms to allow the use of the semantic and geometric knowledge encoded within these models to generate maps for robot localization and navigation.","When heterogeneous robots are deployed within an environment, maps obtained from classical SLAM approaches might not be shared between all agents within a team of robots, e.g. due to a mismatch in sensor type, or a difference in physical robot dimensions.","Our approach extracts the 3D geometry and semantic description of building elements (e.g. material, element type, color) from BIM, and represents this knowledge in a graph.","Based on queries on the graph and knowledge of the skills of the robot, we can generate skill-specific maps that can be used during the execution of localization or navigation tasks.","The approach is validated with data from complex build environments and integrated into existing navigation frameworks."],"url":"http://arxiv.org/abs/2402.18174v1","category":"cs.RO"}
{"created":"2024-02-28 08:59:49","title":"Coherence in cartesian theories using rewriting","abstract":"The celebrated Squier theorem allows to prove coherence properties of algebraic structures, such as MacLane's coherence theorem for monoidal categories, based on rewriting techniques. We are interested here in extending the theory and associated tools simultaneously in two directions. Firstly, we want to take in account situations where coherence is partial, in the sense that it only applies for a subset of structural morphisms (for instance, in the case of the coherence theorem for symmetric monoidal categories, we do not want to strictify the symmetry). Secondly, we are interested in structures where variables can be duplicated or erased. We develop theorems and rewriting techniques in order to achieve this, first in the setting of abstract rewriting systems, and then extend them to term rewriting systems, suitably generalized in order to take coherence in account. As an illustration of our results, we explain how to recover the coherence theorems for monoidal and symmetric monoidal categories.","sentences":["The celebrated Squier theorem allows to prove coherence properties of algebraic structures, such as MacLane's coherence theorem for monoidal categories, based on rewriting techniques.","We are interested here in extending the theory and associated tools simultaneously in two directions.","Firstly, we want to take in account situations where coherence is partial, in the sense that it only applies for a subset of structural morphisms (for instance, in the case of the coherence theorem for symmetric monoidal categories, we do not want to strictify the symmetry).","Secondly, we are interested in structures where variables can be duplicated or erased.","We develop theorems and rewriting techniques in order to achieve this, first in the setting of abstract rewriting systems, and then extend them to term rewriting systems, suitably generalized in order to take coherence in account.","As an illustration of our results, we explain how to recover the coherence theorems for monoidal and symmetric monoidal categories."],"url":"http://arxiv.org/abs/2402.18170v1","category":"math.CT"}
{"created":"2024-02-28 08:57:42","title":"MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery","abstract":"Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention knowledge base featuring 1,372K intentions rooted in 137,287 posts. We conduct a two-stage annotation to verify the quality of the generated knowledge and benchmark the performance of widely used LLMs for intention generation. We further apply MIKO to a sarcasm detection dataset and distill a student model to demonstrate the downstream benefits of applying intention knowledge.","sentences":["Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment.","However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations.","To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions.","Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions.","By applying MIKO to publicly available social media datasets, we construct an intention knowledge base featuring 1,372K intentions rooted in 137,287 posts.","We conduct a two-stage annotation to verify the quality of the generated knowledge and benchmark the performance of widely used LLMs for intention generation.","We further apply MIKO to a sarcasm detection dataset and distill a student model to demonstrate the downstream benefits of applying intention knowledge."],"url":"http://arxiv.org/abs/2402.18169v1","category":"cs.CL"}
{"created":"2024-02-28 08:56:00","title":"Decentralised Traffic Incident Detection via Network Lasso","abstract":"Traffic incident detection plays a key role in intelligent transportation systems, which has gained great attention in transport engineering. In the past, traditional machine learning (ML) based detection methods achieved good performance under a centralised computing paradigm, where all data are transmitted to a central server for building ML models therein. Nowadays, deep neural networks based federated learning (FL) has become a mainstream detection approach to enable the model training in a decentralised manner while warranting local data governance. Such neural networks-centred techniques, however, have overshadowed the utility of well-established ML-based detection methods. In this work, we aim to explore the potential of potent conventional ML-based detection models in modern traffic scenarios featured by distributed data. We leverage an elegant but less explored distributed optimisation framework named Network Lasso, with guaranteed global convergence for convex problem formulations, integrate the potent convex ML model with it, and compare it with centralised learning, local learning, and federated learning methods atop a well-known traffic incident detection dataset. Experimental results show that the proposed network lasso-based approach provides a promising alternative to the FL-based approach in data-decentralised traffic scenarios, with a strong convergence guarantee while rekindling the significance of conventional ML-based detection methods.","sentences":["Traffic incident detection plays a key role in intelligent transportation systems, which has gained great attention in transport engineering.","In the past, traditional machine learning (ML) based detection methods achieved good performance under a centralised computing paradigm, where all data are transmitted to a central server for building ML models therein.","Nowadays, deep neural networks based federated learning (FL) has become a mainstream detection approach to enable the model training in a decentralised manner while warranting local data governance.","Such neural networks-centred techniques, however, have overshadowed the utility of well-established ML-based detection methods.","In this work, we aim to explore the potential of potent conventional ML-based detection models in modern traffic scenarios featured by distributed data.","We leverage an elegant but less explored distributed optimisation framework named Network Lasso, with guaranteed global convergence for convex problem formulations, integrate the potent convex ML model with it, and compare it with centralised learning, local learning, and federated learning methods atop a well-known traffic incident detection dataset.","Experimental results show that the proposed network lasso-based approach provides a promising alternative to the FL-based approach in data-decentralised traffic scenarios, with a strong convergence guarantee while rekindling the significance of conventional ML-based detection methods."],"url":"http://arxiv.org/abs/2402.18167v1","category":"cs.LG"}
{"created":"2024-02-28 08:53:20","title":"Autoencoder-based General Purpose Representation Learning for Customer Embedding","abstract":"In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencoders (CAE) by calculating the Jacobian of the entire encoder leading to a 15% improvement in reconstruction quality when compared to a stacked CAE.","sentences":["In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications.","However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors.","We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data.","We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models.","Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencoders (CAE) by calculating the Jacobian of the entire encoder leading to a 15% improvement in reconstruction quality when compared to a stacked CAE."],"url":"http://arxiv.org/abs/2402.18164v1","category":"cs.LG"}
{"created":"2024-02-28 08:43:18","title":"Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation","abstract":"In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain.","sentences":["In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount.","In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation.","Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity.","We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation.","With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain."],"url":"http://arxiv.org/abs/2402.18159v1","category":"cs.LG"}
{"created":"2024-02-28 08:43:05","title":"Evaluating Quantized Large Language Models","abstract":"Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions.","sentences":["Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs).","Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs.","To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods.","This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B.","The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks.","Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability.","Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions."],"url":"http://arxiv.org/abs/2402.18158v1","category":"cs.CL"}
{"created":"2024-02-28 08:42:23","title":"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs","abstract":"The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.","sentences":["The distinction between humans and animals lies in the unique ability of humans to use and create tools.","Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations.","Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence.","Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs.","The first approach emphasizes the construction of relevant datasets for model fine-tuning.","The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies.","In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs.","This pipeline mirrors the human task-solving process, addressing complicated real-life user queries.","At each step, we guide LLMs to summarize the achieved results and determine the next course of action.","We term this pipeline `from Summary to action', Sum2Act for short.","Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT.","This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks."],"url":"http://arxiv.org/abs/2402.18157v1","category":"cs.AI"}
{"created":"2024-02-28 08:34:41","title":"Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models","abstract":"Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method.","sentences":["Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context.","However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs.","In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point.","We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context.","Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads.","Inspired by the insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters.","PH3 can flexibly control eight LMs to use internal memory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%).","Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks.","We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method."],"url":"http://arxiv.org/abs/2402.18154v1","category":"cs.CL"}
{"created":"2024-02-28 08:34:23","title":"Diffusion-based Neural Network Weights Generation","abstract":"Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance.","sentences":["Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks.","While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets.","Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task.","To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling.","Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets.","By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance."],"url":"http://arxiv.org/abs/2402.18153v1","category":"cs.LG"}
{"created":"2024-02-28 08:32:19","title":"Boosting Neural Representations for Videos with a Conditional Decoder","abstract":"Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs.","sentences":["Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks.","However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding.","This paper introduces a universal boosting framework for current implicit video representation approaches.","Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames.","Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity.","With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results.","Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs.","Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs."],"url":"http://arxiv.org/abs/2402.18152v1","category":"eess.IV"}
{"created":"2024-02-28 08:24:38","title":"Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation","abstract":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.","sentences":["Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval.","However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it.","The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality.","In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts.","To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner.","InFO-RAG is low-cost and general across various tasks.","Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points.","InFO-RAG also shows advantages in in-context learning and robustness of RAG."],"url":"http://arxiv.org/abs/2402.18150v1","category":"cs.CL"}
{"created":"2024-02-28 08:23:29","title":"A metamodel for confined yield stress flows and parameters' estimation","abstract":"With the growing demand of mineral consumption, the management of the mining waste is crucial. Cemented paste backfill (CPB) is one of the techniques developed by the mining industry to fill the voids generated by the excavation of underground spaces. The CPB process is the subject of various studies aimed at optimizing its implementation in the field. In this article, we focus on the modelling of the backfill phase where it has been shown in [Vigneaux et al., Cem. Concr. Res. 164 (2023) 107038] that a viscoplastic lubrication model can be used to describe CPB experiments. The aim here is to propose an accelerated method for performing the parameters' estimation of the properties of the paste (typically its rheological properties), with an inverse problem procedure based on observed height profiles of the paste. The inversion procedure is based on a metamodel built from an initial partial differential equation model, thanks to a Polynomial Chaos Expansion coupled with a Principal Component Analysis.","sentences":["With the growing demand of mineral consumption, the management of the mining waste is crucial.","Cemented paste backfill (CPB) is one of the techniques developed by the mining industry to fill the voids generated by the excavation of underground spaces.","The CPB process is the subject of various studies aimed at optimizing its implementation in the field.","In this article, we focus on the modelling of the backfill phase where it has been shown in [Vigneaux et al., Cem.","Concr.","Res. 164 (2023) 107038] that a viscoplastic lubrication model can be used to describe CPB experiments.","The aim here is to propose an accelerated method for performing the parameters' estimation of the properties of the paste (typically its rheological properties), with an inverse problem procedure based on observed height profiles of the paste.","The inversion procedure is based on a metamodel built from an initial partial differential equation model, thanks to a Polynomial Chaos Expansion coupled with a Principal Component Analysis."],"url":"http://arxiv.org/abs/2402.18148v1","category":"math.NA"}
{"created":"2024-02-28 08:18:20","title":"A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction","abstract":"Human vision relies heavily on available ambient light to perceive objects. Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts. Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario. We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory. This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction. The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria. Furthermore, we utilized knowledge distillation with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time. The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios.","sentences":["Human vision relies heavily on available ambient light to perceive objects.","Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts.","Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario.","We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory.","This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction.","The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria.","Furthermore, we utilized knowledge distillation with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time.","The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios."],"url":"http://arxiv.org/abs/2402.18147v1","category":"eess.IV"}
{"created":"2024-02-28 08:12:31","title":"3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling","abstract":"Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.","sentences":["Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds.","We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds.","Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios.","By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene.","Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion.","By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios.","On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling.","Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error."],"url":"http://arxiv.org/abs/2402.18146v1","category":"cs.CV"}
{"created":"2024-02-28 08:09:14","title":"Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information","abstract":"Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose \"random silicon sampling,\" a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.","sentences":["Large language models exhibit societal biases associated with demographic information, including race, gender, and others.","Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans.","Building on this idea, we propose \"random silicon sampling,\" a method to emulate the opinions of the human population sub-group.","Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions.","Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls.","Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models.","Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations."],"url":"http://arxiv.org/abs/2402.18144v1","category":"cs.AI"}
{"created":"2024-02-28 08:03:34","title":"OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction","abstract":"This technical report presents our solution, \"occTransformer\" for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023. Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques. Firstly, we employed data augmentation to increase the diversity of the training data and improve the model's generalization ability. Secondly, we used a strong image backbone to extract more informative features from the input data. Thirdly, we incorporated a 3D unet head to better capture the spatial information of the scene. Fourthly, we added more loss functions to better optimize the model. Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance. Most importantly, we integrated 3D detection model StreamPETR to enhance the model's ability to detect objects in the scene. Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge.","sentences":["This technical report presents our solution, \"occTransformer\" for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023.","Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques.","Firstly, we employed data augmentation to increase the diversity of the training data and improve the model's generalization ability.","Secondly, we used a strong image backbone to extract more informative features from the input data.","Thirdly, we incorporated a 3D unet head to better capture the spatial information of the scene.","Fourthly, we added more loss functions to better optimize the model.","Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance.","Most importantly, we integrated 3D detection model StreamPETR to enhance the model's ability to detect objects in the scene.","Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge."],"url":"http://arxiv.org/abs/2402.18140v1","category":"cs.CV"}
{"created":"2024-02-28 08:02:14","title":"Cause and Effect: Can Large Language Models Truly Understand Causality?","abstract":"With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.","sentences":["With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails.","Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively.","This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability.","The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs.","Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality.","The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning.","The counterfactual sentences add explicit knowledge of the not caused by scenarios.","By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability.","Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores.","We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain."],"url":"http://arxiv.org/abs/2402.18139v1","category":"cs.CL"}
{"created":"2024-02-28 07:58:24","title":"DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning","abstract":"Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning. Project Page: https://2toinf.github.io/DecisionNCE/","sentences":["Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding.","Most existing methods approach these via separate objectives, which often reach sub-optimal solutions.","In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions.","We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations.","The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding.","Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning.","Project Page: https://2toinf.github.io/DecisionNCE/"],"url":"http://arxiv.org/abs/2402.18137v1","category":"cs.RO"}
{"created":"2024-02-28 07:56:30","title":"Manager Characteristics and SMEs' Restructuring Decisions: In-Court vs. Out-of-Court Restructuring","abstract":"This study aims to empirically investigate the impact of managers' characteristics on their choice between in-court and out-of-court restructuring. Based on the theory of upper echelons, we tested the preferences of 342 managers of financially distressed French firms regarding restructuring decisions. The overall findings of this study provide empirical support for the upper echelons theory. Specifically, managers with a long tenure and those with a high level of education are less likely to restructure before the court and are more likely to restructure privately. The findings also indicate that managers' age and gender do not significantly affect their choice between in-court and out-of-court restructuring. This study contributes to the literature on bankruptcy and corporate restructuring by turning the focus from firm characteristics to manager characteristics to explain restructuring decisions.","sentences":["This study aims to empirically investigate the impact of managers' characteristics on their choice between in-court and out-of-court restructuring.","Based on the theory of upper echelons, we tested the preferences of 342 managers of financially distressed French firms regarding restructuring decisions.","The overall findings of this study provide empirical support for the upper echelons theory.","Specifically, managers with a long tenure and those with a high level of education are less likely to restructure before the court and are more likely to restructure privately.","The findings also indicate that managers' age and gender do not significantly affect their choice between in-court and out-of-court restructuring.","This study contributes to the literature on bankruptcy and corporate restructuring by turning the focus from firm characteristics to manager characteristics to explain restructuring decisions."],"url":"http://arxiv.org/abs/2402.18135v1","category":"q-fin.GN"}
{"created":"2024-02-28 07:53:19","title":"Understanding the Role of Pathways in a Deep Neural Network","abstract":"Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding adversarial attacks, object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.","sentences":["Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application.","The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks.","In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes.","The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories.","We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification.","And the large pathways of images of the same category are more consistent in their trends than those of different categories.","We also apply the pathways to understanding adversarial attacks, object completion, and movement perception.","Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples."],"url":"http://arxiv.org/abs/2402.18132v1","category":"cs.CV"}
{"created":"2024-02-28 07:44:23","title":"Sequential Change-point Detection for Compositional Time Series with Exogenous Variables","abstract":"Sequential change-point detection for time series enables us to sequentially check the hypothesis that the model still holds as more and more data are observed. It is widely used in data monitoring in practice. In this work, we consider sequential change-point detection for compositional time series, time series in which the observations are proportions. For fitting compositional time series, we propose a generalized Beta AR(1) model, which can incorporate exogenous variables upon which the time series observations are dependent. We show the compositional time series are strictly stationary and geometrically ergodic and consider maximum likelihood estimation for model parameters. We show the partial MLEs are consistent and asymptotically normal and propose a parametric sequential change-point detection method for the compositional time series model. The change-point detection method is illustrated using a time series of Covid-19 positivity rates.","sentences":["Sequential change-point detection for time series enables us to sequentially check the hypothesis that the model still holds as more and more data are observed.","It is widely used in data monitoring in practice.","In this work, we consider sequential change-point detection for compositional time series, time series in which the observations are proportions.","For fitting compositional time series, we propose a generalized Beta AR(1) model, which can incorporate exogenous variables upon which the time series observations are dependent.","We show the compositional time series are strictly stationary and geometrically ergodic and consider maximum likelihood estimation for model parameters.","We show the partial MLEs are consistent and asymptotically normal and propose a parametric sequential change-point detection method for the compositional time series model.","The change-point detection method is illustrated using a time series of Covid-19 positivity rates."],"url":"http://arxiv.org/abs/2402.18130v1","category":"stat.ME"}
{"created":"2024-02-28 07:39:58","title":"On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms","abstract":"Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute. Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems. The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method.","sentences":["Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community.","While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms.","In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute.","Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data.","To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute.","Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems.","The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method."],"url":"http://arxiv.org/abs/2402.18129v1","category":"cs.LG"}
{"created":"2024-02-28 16:37:23","title":"Search for baryon number violation in top quark production and decay using proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is presented for baryon number violating interactions in top quark production and decay. The analysis uses data from proton-proton collisions at a center-of-mass energy of 13 TeV, collected with the CMS detector at the LHC with an integrated luminosity of 138 fb$^{-1}$. Candidate events are selected by requiring two oppositely-charged leptons (electrons or muons) and exactly one jet identified as originating from a bottom quark. Multivariate discriminants are used to separate the signal from the background. No significant deviation from the standard model prediction is observed. Upper limits are placed on the strength of baryon number violating couplings. For the first time the production of single top quarks via baryon number violating interactions is studied. This allows the search to set the most stringent constraints to date on the branching fraction of the top quark decay to a lepton, an up-type quark (u or c), and a down-type quark (d, s, or b). The results improve the previous bounds by three to six orders of magnitude based on the fermion flavor combination of the baryon number violating interactions.","sentences":["A search is presented for baryon number violating interactions in top quark production and decay.","The analysis uses data from proton-proton collisions at a center-of-mass energy of 13 TeV, collected with the CMS detector at the LHC with an integrated luminosity of 138 fb$^{-1}$. Candidate events are selected by requiring two oppositely-charged leptons (electrons or muons) and exactly one jet identified as originating from a bottom quark.","Multivariate discriminants are used to separate the signal from the background.","No significant deviation from the standard model prediction is observed.","Upper limits are placed on the strength of baryon number violating couplings.","For the first time the production of single top quarks via baryon number violating interactions is studied.","This allows the search to set the most stringent constraints to date on the branching fraction of the top quark decay to a lepton, an up-type quark (u or c), and a down-type quark (d, s, or b).","The results improve the previous bounds by three to six orders of magnitude based on the fermion flavor combination of the baryon number violating interactions."],"url":"http://arxiv.org/abs/2402.18461v1","category":"hep-ex"}
{"created":"2024-02-28 13:53:02","title":"On the simulation of quantum multimeters","abstract":"In the quest for robust and universal quantum devices, the notion of simulation plays a crucial role, both from a theoretical and from an applied perspective. In this work, we go beyond the simulation of quantum channels and quantum measurements, studying what it means to simulate a collection of measurements, which we call a multimeter. To this end, we first explicitly characterize the completely positive transformations between multimeters. However, not all of these transformations correspond to valid simulations, as evidenced by the existence of maps that always prepare the same multimeter regardless of the input, which we call trash-and-prepare. We give a new definition of multimeter simulations as transformations that are triviality-preserving, i.e., when given a multimeter consisting of trivial measurements they can only produce another trivial multimeter. In the absence of a quantum ancilla, we then characterize the transformations that are triviality-preserving and the transformations that are trash-and-prepare. Finally, we use these characterizations to compare our new definition of multimeter simulation to three existing ones: classical simulations, compression of multimeters, and compatibility-preserving simulations.","sentences":["In the quest for robust and universal quantum devices, the notion of simulation plays a crucial role, both from a theoretical and from an applied perspective.","In this work, we go beyond the simulation of quantum channels and quantum measurements, studying what it means to simulate a collection of measurements, which we call a multimeter.","To this end, we first explicitly characterize the completely positive transformations between multimeters.","However, not all of these transformations correspond to valid simulations, as evidenced by the existence of maps that always prepare the same multimeter regardless of the input, which we call trash-and-prepare.","We give a new definition of multimeter simulations as transformations that are triviality-preserving, i.e., when given a multimeter consisting of trivial measurements they can only produce another trivial multimeter.","In the absence of a quantum ancilla, we then characterize the transformations that are triviality-preserving and the transformations that are trash-and-prepare.","Finally, we use these characterizations to compare our new definition of multimeter simulation to three existing ones: classical simulations, compression of multimeters, and compatibility-preserving simulations."],"url":"http://arxiv.org/abs/2402.18333v1","category":"quant-ph"}
{"created":"2024-02-28 12:41:06","title":"Comparative Analysis of XGBoost and Minirocket Algortihms for Human Activity Recognition","abstract":"Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification. This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors. The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone. The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers. Monte Carlo cross-validation is employed to evaluate the models' robustness. The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classification. XGBoost exhibits a slightly superior performance compared to MiniRocket. Notably, both algorithms surpass the performance of other ML and DL algorithms reported in the literature for HAR tasks. Additionally, the study compares the computational efficiency of the two algorithms, revealing XGBoost's advantage in terms of training time. Furthermore, the performance of MiniRocket, which achieves accuracy and F1 values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one channel from the sensors, highlights the potential of directly leveraging unprocessed signals. It also suggests potential advantages that could be gained by utilizing sensor fusion or channel fusion techniques. Overall, this research sheds light on the effectiveness and computational characteristics of XGBoost and MiniRocket in HAR tasks, providing insights for future studies in activity recognition using smartphone sensor data.","sentences":["Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification.","This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors.","The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone.","The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers.","Monte Carlo cross-validation is employed to evaluate the models' robustness.","The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classification.","XGBoost exhibits a slightly superior performance compared to MiniRocket.","Notably, both algorithms surpass the performance of other ML and DL algorithms reported in the literature for HAR tasks.","Additionally, the study compares the computational efficiency of the two algorithms, revealing XGBoost's advantage in terms of training time.","Furthermore, the performance of MiniRocket, which achieves accuracy and F1 values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one channel from the sensors, highlights the potential of directly leveraging unprocessed signals.","It also suggests potential advantages that could be gained by utilizing sensor fusion or channel fusion techniques.","Overall, this research sheds light on the effectiveness and computational characteristics of XGBoost and MiniRocket in HAR tasks, providing insights for future studies in activity recognition using smartphone sensor data."],"url":"http://arxiv.org/abs/2402.18296v1","category":"cs.LG"}
{"created":"2024-02-28 11:32:31","title":"Photon quantum kinetic equations and collective modes in an axion background","abstract":"We develop a quantum kinetic theory for photons in the presence of an axion background and in the collisioness limit. In deriving the classical regime of our quantum kinetic equations, we observe that they capture well known features of axion electrodynamics. By projecting the Wigner function onto a polarization basis, relating the Wigner matrix function with the Stokes parameters, we establish the dispersion relations and transport equations for each polarization space component. Additionally, we investigate how the axion background affects the dispersion relations of photon collective modes within an electron-positron plasma at equilibrium temperature $T$. While the plasmon remains unaffected, we find that the axion background breaks the degeneracy of transverse collective modes at order $e g_{a\\gamma}T(\\partial a)$, where $e$ represents the electron charge, $ g_{a\\gamma}$ denotes the photon-axion coupling, and $\\partial a$ represents the scale associated with variations in the axion field.","sentences":["We develop a quantum kinetic theory for photons in the presence of an axion background and in the collisioness limit.","In deriving the classical regime of our quantum kinetic equations, we observe that they capture well known features of axion electrodynamics.","By projecting the Wigner function onto a polarization basis, relating the Wigner matrix function with the Stokes parameters, we establish the dispersion relations and transport equations for each polarization space component.","Additionally, we investigate how the axion background affects the dispersion relations of photon collective modes within an electron-positron plasma at equilibrium temperature $T$. While the plasmon remains unaffected, we find that the axion background breaks the degeneracy of transverse collective modes at order $e g_{a\\gamma}T(\\partial a)$, where $e$ represents the electron charge, $ g_{a\\gamma}$ denotes the photon-axion coupling, and $\\partial a$ represents the scale associated with variations in the axion field."],"url":"http://arxiv.org/abs/2402.18254v1","category":"hep-ph"}
{"created":"2024-02-28 11:28:56","title":"On the Accuracy of Edge Detectors in Number Plate Extraction","abstract":"Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system. This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle. As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection. This paper presents a method of number plate extraction using edge detection technique. Edges in number plates are identified with changes in the intensity of pixel values. Therefore, these edges are identified using a single based pixel or collection of pixel-based approach. The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented. Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric","sentences":["Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system.","This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle.","As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection.","This paper presents a method of number plate extraction using edge detection technique.","Edges in number plates are identified with changes in the intensity of pixel values.","Therefore, these edges are identified using a single based pixel or collection of pixel-based approach.","The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented.","Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric"],"url":"http://arxiv.org/abs/2402.18251v1","category":"cs.CV"}
{"created":"2024-02-28 11:12:47","title":"Affective State Detection using fNIRs and Machine Learning","abstract":"Affective states regulate our day to day to function and has a tremendous effect on mental and physical health. Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management. In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy. We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning. A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave one out cross validation. It was found that prediction accuracy for cognitive load was higher (evoked using a pen and paper task) than the other two classes (evoked using computer bases tasks). To verify that this discrepancy was not due to motor skills involved in the pen and paper task, a second experiment was conducted using four participants and the results of that experiment has also been presented in the paper.","sentences":["Affective states regulate our day to day to function and has a tremendous effect on mental and physical health.","Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management.","In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy.","We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning.","A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave one out cross validation.","It was found that prediction accuracy for cognitive load was higher (evoked using a pen and paper task) than the other two classes (evoked using computer bases tasks).","To verify that this discrepancy was not due to motor skills involved in the pen and paper task, a second experiment was conducted using four participants and the results of that experiment has also been presented in the paper."],"url":"http://arxiv.org/abs/2402.18241v1","category":"cs.HC"}
{"created":"2024-02-28 10:04:16","title":"Pach's animal problem within the bounding box","abstract":"A collection of unit cubes with integer coordinates in $\\mathbb R^3$ is an animal if its union is homeomorphic to the 3-ball. Pach's animal problem asks whether any animal can be transformed to a single cube by adding or removing cubes one by one in such a way that any intermediate step is an animal as well. Here we provide an example of an animal that cannot be transformed to a single cube this way within its bounding box.","sentences":["A collection of unit cubes with integer coordinates in $\\mathbb R^3$ is an animal if its union is homeomorphic to the 3-ball.","Pach's animal problem asks whether any animal can be transformed to a single cube by adding or removing cubes one by one in such a way that any intermediate step is an animal as well.","Here we provide an example of an animal that cannot be transformed to a single cube this way within its bounding box."],"url":"http://arxiv.org/abs/2402.18212v1","category":"math.CO"}
{"created":"2024-02-28 09:48:43","title":"ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing","abstract":"This paper proposes a method for Acoustic Constrained Segmentation (ACS) in audio recordings of vehicles driven through a production test track, delimiting the boundaries of surface types in the track. ACS is a variant of classical acoustic segmentation where the sequence of labels is known, contiguous and invariable, which is especially useful in this work as the test track has a standard configuration of surface types. The proposed ConvDTW-ACS method utilizes a Convolutional Neural Network for classifying overlapping image chunks extracted from the full audio spectrogram. Then, our custom Dynamic Time Warping algorithm aligns the sequence of predicted probabilities to the sequence of surface types in the track, from which timestamps of the surface type boundaries can be extracted. The method was evaluated on a real-world dataset collected from the Ford Manufacturing Plant in Valencia (Spain), achieving a mean error of 166 milliseconds when delimiting, within the audio, the boundaries of the surfaces in the track. The results demonstrate the effectiveness of the proposed method in accurately segmenting different surface types, which could enable the development of more specialized AI systems to improve the quality inspection process.","sentences":["This paper proposes a method for Acoustic Constrained Segmentation (ACS) in audio recordings of vehicles driven through a production test track, delimiting the boundaries of surface types in the track.","ACS is a variant of classical acoustic segmentation where the sequence of labels is known, contiguous and invariable, which is especially useful in this work as the test track has a standard configuration of surface types.","The proposed ConvDTW-ACS method utilizes a Convolutional Neural Network for classifying overlapping image chunks extracted from the full audio spectrogram.","Then, our custom Dynamic Time Warping algorithm aligns the sequence of predicted probabilities to the sequence of surface types in the track, from which timestamps of the surface type boundaries can be extracted.","The method was evaluated on a real-world dataset collected from the Ford Manufacturing Plant in Valencia (Spain), achieving a mean error of 166 milliseconds when delimiting, within the audio, the boundaries of the surfaces in the track.","The results demonstrate the effectiveness of the proposed method in accurately segmenting different surface types, which could enable the development of more specialized AI systems to improve the quality inspection process."],"url":"http://arxiv.org/abs/2402.18204v1","category":"cs.SD"}
{"created":"2024-02-28 08:31:33","title":"Concise Spectrotemporal Studies of Magnetar SGR J1935+2154 Bursts","abstract":"SGR J1935+2154 has truly been the most prolific magnetar over the last decade: It has been entering into burst active episodes once every 1-2 years since its discovery in 2014, it emitted the first Galactic fast radio burst associated with an X-ray burst in 2020, and has emitted hundreds of energetic short bursts. Here, we present the time-resolved spectral analysis of 51 bright bursts from SGR J1935+2154. Unlike conventional time-resolved X-ray spectroscopic studies in the literature, we follow a two-step approach to probe true spectral evolution. For each burst, we first extract spectral information from overlapping time segments, fit them with three continuum models, and employ a machine learning based clustering algorithm to identify time segments that provide the largest spectral variations during each burst. We then extract spectra from those non-overlapping (clustered) time segments and fit them again with the three models: the cutoff power-law model, the sum of two blackbody functions, and the model considering the emission of a modified black body undergoing resonant cyclotron scattering, which is applied systematically at this scale for the first time. Our novel technique allowed us to establish the genuine spectral evolution of magnetar bursts. We discuss the implications of our results and compare their collective behavior with the average burst properties of other magnetars.","sentences":["SGR J1935+2154 has truly been the most prolific magnetar over the last decade: It has been entering into burst active episodes once every 1-2 years since its discovery in 2014, it emitted the first Galactic fast radio burst associated with an X-ray burst in 2020, and has emitted hundreds of energetic short bursts.","Here, we present the time-resolved spectral analysis of 51 bright bursts from SGR J1935+2154.","Unlike conventional time-resolved X-ray spectroscopic studies in the literature, we follow a two-step approach to probe true spectral evolution.","For each burst, we first extract spectral information from overlapping time segments, fit them with three continuum models, and employ a machine learning based clustering algorithm to identify time segments that provide the largest spectral variations during each burst.","We then extract spectra from those non-overlapping (clustered) time segments and fit them again with the three models: the cutoff power-law model, the sum of two blackbody functions, and the model considering the emission of a modified black body undergoing resonant cyclotron scattering, which is applied systematically at this scale for the first time.","Our novel technique allowed us to establish the genuine spectral evolution of magnetar bursts.","We discuss the implications of our results and compare their collective behavior with the average burst properties of other magnetars."],"url":"http://arxiv.org/abs/2402.18151v1","category":"astro-ph.HE"}
{"created":"2024-02-28 08:09:12","title":"Invariance principle and McKean-Vlasov limit for randomized load balancing in heavy traffic","abstract":"We consider a load balancing model where a Poisson stream of jobs arrive at a system of many servers whose service time distribution possesses a finite second moment. A small fraction of arrivals pass through the so called power-of-choice algorithm, which assigns a job to the shortest among $\\ell$, $\\ell\\ge 2$, randomly chosen queues, and the remaining jobs are assigned to queues chosen uniformly at random. The system is analyzed at critical load in an asymptotic regime where both the number of servers and the usual heavy traffic parameter associated with individual queue lengths grow to infinity. The first main result is a hydrodynamic limit, where the empirical measure of the diffusively normalized queue lengths is shown to converge to a path in measure space whose density is given by the unique solution of a parabolic PDE with nonlocal coefficients.   Further, two forms of an invariance principle are proved, corresponding to two different assumptions on the initial distribution, where individual normalized queue lengths converge weakly to solutions of SDE. In one of these results, the limit is given by a McKean-Vlasov SDE, and propagation of chaos holds. The McKean-Vlasov limit is closely related to limit results for Brownian particles on $\\mathbb{R}_+$ interacting through their rank (with a specific interaction). However, an entirely different set of tools is required, as the collection of $n$ prelimit particles does not obey a Markovian evolution on $\\mathbb{R}_+^n$.","sentences":["We consider a load balancing model where a Poisson stream of jobs arrive at a system of many servers whose service time distribution possesses a finite second moment.","A small fraction of arrivals pass through the so called power-of-choice algorithm, which assigns a job to the shortest among $\\ell$, $\\ell\\ge 2$, randomly chosen queues, and the remaining jobs are assigned to queues chosen uniformly at random.","The system is analyzed at critical load in an asymptotic regime where both the number of servers and the usual heavy traffic parameter associated with individual queue lengths grow to infinity.","The first main result is a hydrodynamic limit, where the empirical measure of the diffusively normalized queue lengths is shown to converge to a path in measure space whose density is given by the unique solution of a parabolic PDE with nonlocal coefficients.   ","Further, two forms of an invariance principle are proved, corresponding to two different assumptions on the initial distribution, where individual normalized queue lengths converge weakly to solutions of SDE.","In one of these results, the limit is given by a McKean-Vlasov SDE, and propagation of chaos holds.","The McKean-Vlasov limit is closely related to limit results for Brownian particles on $\\mathbb{R}_+$ interacting through their rank (with a specific interaction).","However, an entirely different set of tools is required, as the collection of $n$ prelimit particles does not obey a Markovian evolution on $\\mathbb{R}_+^n$."],"url":"http://arxiv.org/abs/2402.18143v1","category":"math.PR"}
{"created":"2024-02-28 07:22:13","title":"Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian","abstract":"This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.","sentences":["This study assesses four cutting-edge language models in the underexplored Aminoacian language.","Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding.","Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps.","By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology."],"url":"http://arxiv.org/abs/2402.18121v1","category":"cs.CL"}
{"created":"2024-02-28 07:02:38","title":"Small But Funny: A Feedback-Driven Approach to Humor Distillation","abstract":"The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.","sentences":["The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing.","Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs).","While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation.","We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance.","To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance.","Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation.","As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation."],"url":"http://arxiv.org/abs/2402.18113v1","category":"cs.CL"}
{"created":"2024-02-28 06:50:14","title":"Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction","abstract":"In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\\% attack success rate on LLM chatbots GPT-4.","sentences":["In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem.","One specific threat is the potential to generate toxic or harmful responses.","Attackers can craft adversarial prompts that induce harmful responses from LLMs.","In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion.","We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency.","Notably, DRA boasts a 90\\% attack success rate on LLM chatbots GPT-4."],"url":"http://arxiv.org/abs/2402.18104v1","category":"cs.CR"}
{"created":"2024-02-28 06:45:47","title":"Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging","abstract":"Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.","sentences":["Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist.","Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging.","Dual-pixel (DP) sensors are a potential solution to achieve the same.","DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system.","However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views.","This creates a trade-off between disparity estimation vs. deblurring accuracy.","To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor.","In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting.","Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings.","Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor.","Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner."],"url":"http://arxiv.org/abs/2402.18102v1","category":"eess.IV"}
{"created":"2024-02-28 06:40:57","title":"Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models","abstract":"Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited.","sentences":["Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged.","It has been proven effective in resolving hallucination and out-of-date issues in LLMs.","As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable.","In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts.","Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge.","Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing.","It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs.","These adapters are assigned scaling values based on the corresponding specific knowledge.","To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics.","Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited."],"url":"http://arxiv.org/abs/2402.18099v1","category":"cs.CL"}
{"created":"2024-02-28 06:34:54","title":"No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization","abstract":"Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.","sentences":["Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs).","However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself.","Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined.","In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss.","Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation.","On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality.","Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision.","Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines."],"url":"http://arxiv.org/abs/2402.18096v1","category":"cs.LG"}
{"created":"2024-02-28 06:24:39","title":"Polos: Multimodal Metric Learning from Human Feedback for Image Captioning","abstract":"Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.","sentences":["Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models.","Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation.","In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models.","Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning.","To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback.","We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets.","Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness."],"url":"http://arxiv.org/abs/2402.18091v1","category":"cs.CV"}
{"created":"2024-02-28 05:49:08","title":"Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning","abstract":"The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly. Its effectiveness is also demonstrated on real datasets. The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.5--6 within the first 5 samples, and poor hypotheses are quickly identified and rejected, having no adverse effect on planning after around 3 samples.","sentences":["The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment.","Utilization of available a-priori data can be a powerful tool for increasing efficiency.","However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency.","To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function.","Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans.","The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly.","Its effectiveness is also demonstrated on real datasets.","The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.5--6 within the first 5 samples, and poor hypotheses are quickly identified and rejected, having no adverse effect on planning after around 3 samples."],"url":"http://arxiv.org/abs/2402.18064v1","category":"cs.RO"}
{"created":"2024-02-28 05:46:23","title":"Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities","abstract":"With recent advances in artificial intelligence (AI) and robotics, unmanned vehicle swarms have received great attention from both academia and industry due to their potential to provide services that are difficult and dangerous to perform by humans. However, learning and coordinating movements and actions for a large number of unmanned vehicles in complex and dynamic environments introduce significant challenges to conventional AI methods. Generative AI (GAI), with its capabilities in complex data feature extraction, transformation, and enhancement, offers great potential in solving these challenges of unmanned vehicle swarms. For that, this paper aims to provide a comprehensive survey on applications, challenges, and opportunities of GAI in unmanned vehicle swarms. Specifically, we first present an overview of unmanned vehicles and unmanned vehicle swarms as well as their use cases and existing issues. Then, an in-depth background of various GAI techniques together with their capabilities in enhancing unmanned vehicle swarms are provided. After that, we present a comprehensive review on the applications and challenges of GAI in unmanned vehicle swarms with various insights and discussions. Finally, we highlight open issues of GAI in unmanned vehicle swarms and discuss potential research directions.","sentences":["With recent advances in artificial intelligence (AI) and robotics, unmanned vehicle swarms have received great attention from both academia and industry due to their potential to provide services that are difficult and dangerous to perform by humans.","However, learning and coordinating movements and actions for a large number of unmanned vehicles in complex and dynamic environments introduce significant challenges to conventional AI methods.","Generative AI (GAI), with its capabilities in complex data feature extraction, transformation, and enhancement, offers great potential in solving these challenges of unmanned vehicle swarms.","For that, this paper aims to provide a comprehensive survey on applications, challenges, and opportunities of GAI in unmanned vehicle swarms.","Specifically, we first present an overview of unmanned vehicles and unmanned vehicle swarms as well as their use cases and existing issues.","Then, an in-depth background of various GAI techniques together with their capabilities in enhancing unmanned vehicle swarms are provided.","After that, we present a comprehensive review on the applications and challenges of GAI in unmanned vehicle swarms with various insights and discussions.","Finally, we highlight open issues of GAI in unmanned vehicle swarms and discuss potential research directions."],"url":"http://arxiv.org/abs/2402.18062v1","category":"cs.RO"}
{"created":"2024-02-28 05:45:37","title":"On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction","abstract":"The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task. The code is share in https://github.com/wjw136/Clean_LaVe.git.","sentences":["The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data.","Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data.","A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks.","However, there is no further investigation into the use of these data.","In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance.","Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data.","The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task.","The code is share in https://github.com/wjw136/Clean_LaVe.git."],"url":"http://arxiv.org/abs/2402.18061v1","category":"cs.CL"}
{"created":"2024-02-28 04:35:51","title":"Datasets for Large Language Models: A Comprehensive Survey","abstract":"This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.","sentences":["This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs.","The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs.","Consequently, examination of these datasets emerges as a critical topic in research.","In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets.","The survey sheds light on the prevailing challenges and points out potential avenues for future investigation.","Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains.","Information from 20 dimensions is incorporated into the dataset statistics.","The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets.","We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies.","Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets."],"url":"http://arxiv.org/abs/2402.18041v1","category":"cs.CL"}
{"created":"2024-02-28 04:34:15","title":"Automated Discovery of Integral with Deep Learning","abstract":"Recent advancements in the realm of deep learning, particularly in the development of large language models (LLMs), have demonstrated AI's ability to tackle complex mathematical problems or solving programming challenges. However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries. Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.   In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals. By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$ and $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our experiments show that deep learning models can approach the task of inferring integrals either through a sequence-to-sequence model, akin to language translation, or by uncovering the rudimentary principles of integration, such as $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.","sentences":["Recent advancements in the realm of deep learning, particularly in the development of large language models (LLMs), have demonstrated AI's ability to tackle complex mathematical problems or solving programming challenges.","However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries.","Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens.","They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.   ","In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals.","By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$ and $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$.","Our experiments show that deep learning models can approach the task of inferring integrals either through a sequence-to-sequence model, akin to language translation, or by uncovering the rudimentary principles of integration, such as $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$."],"url":"http://arxiv.org/abs/2402.18040v1","category":"cs.AI"}
{"created":"2024-02-28 04:33:20","title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","abstract":"As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .","sentences":["As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs).","However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model.","To address this, we propose ResLoRA, an improved framework of LoRA.","By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA.","The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method.","To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA.","The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora ."],"url":"http://arxiv.org/abs/2402.18039v1","category":"cs.CL"}
{"created":"2024-02-28 04:26:04","title":"Water-Vapor Absorption Database using Dual Comb Spectroscopy from 300-1300 K Part II: Air-Broadened H$_2$O, 6600 to 7650 cm$^{-1}$","abstract":"We present broadband dual frequency comb laser absorption measurements of 2% H$_2$O (natural isotopic abundance of 99.7% H$_2^{16}$O) in air from 6600-7650 cm$^{-1}$ (1307-1515 nm) with a spectral point spacing of 0.0068 cm$^{-1}$. Twenty-nine datasets were collected at temperatures between 300 and 1300 K ($\\pm$0.82% average uncertainty) and pressures ranging from 20 to 600 Torr ($\\pm$0.25%) with an average residual absorbance noise of 8.0E-4 across the spectrum for all measurements. We fit measurements using a quadratic speed-dependent Voigt profile to determine 7088 absorption parameters for 3366 individual transitions found in HITRAN2020. These measurements build on the line strength, line center, self-broadening, and self-shift parameters determined in the Part I companion of this work. Here we measure air-broadened width (with temperature- and speed-dependence) and air pressure shift (with temperature dependence) parameters. Various trends are explored for extrapolation to weak transitions that were not covered in this work. Improvements made in this work are predominantly due to the inclusion of air pressure shift temperature dependence values. In aggregate, these updates improved RMS absorbance error by a factor of 4.2 on average, and the remaining residual is predominantly spectral noise. This updated database improves high temperature spectroscopic knowledge across the 6600 7650 cm$^{-1}$ region of H$_2$O absorption.","sentences":["We present broadband dual frequency comb laser absorption measurements of 2% H$_2$O (natural isotopic abundance of 99.7% H$_2^{16}$O) in air from 6600-7650 cm$^{-1}$ (1307-1515 nm) with a spectral point spacing of 0.0068 cm$^{-1}$. Twenty-nine datasets were collected at temperatures between 300 and 1300 K ($\\pm$0.82% average uncertainty) and pressures ranging from 20 to 600 Torr ($\\pm$0.25%) with an average residual absorbance noise of 8.0E-4 across the spectrum for all measurements.","We fit measurements using a quadratic speed-dependent Voigt profile to determine 7088 absorption parameters for 3366 individual transitions found in HITRAN2020.","These measurements build on the line strength, line center, self-broadening, and self-shift parameters determined in the Part I companion of this work.","Here we measure air-broadened width (with temperature- and speed-dependence) and air pressure shift (with temperature dependence) parameters.","Various trends are explored for extrapolation to weak transitions that were not covered in this work.","Improvements made in this work are predominantly due to the inclusion of air pressure shift temperature dependence values.","In aggregate, these updates improved RMS absorbance error by a factor of 4.2 on average, and the remaining residual is predominantly spectral noise.","This updated database improves high temperature spectroscopic knowledge across the 6600 7650 cm$^{-1}$ region of H$_2$O absorption."],"url":"http://arxiv.org/abs/2402.18036v1","category":"physics.ao-ph"}
{"created":"2024-02-28 03:51:02","title":"OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine","abstract":"The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.","sentences":["The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas.","However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages.","It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data.","The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner.","In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models.","It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data.","Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks.","We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab."],"url":"http://arxiv.org/abs/2402.18028v1","category":"cs.CV"}
{"created":"2024-02-28 03:44:17","title":"Characterization of the Astrophysical Diffuse Neutrino Flux using Starting Track Events in IceCube","abstract":"A measurement of the diffuse astrophysical neutrino spectrum is presented using IceCube data collected from 2011-2022 (10.3 years). We developed novel detection techniques to search for events with a contained vertex and exiting track induced by muon neutrinos undergoing a charged-current interaction. Searching for these starting track events allows us to not only more effectively reject atmospheric muons but also atmospheric neutrino backgrounds in the southern sky, opening a new window to the sub-100 TeV astrophysical neutrino sky. The event selection is constructed using a dynamic starting track veto and machine learning algorithms. We use this data to measure the astrophysical diffuse flux as a single power law flux (SPL) with a best-fit spectral index of $\\gamma = 2.58 ^{+0.10}_{-0.09}$ and per-flavor normalization of $\\phi^{\\mathrm{Astro}}_{\\mathrm{per-flavor}} = 1.68 ^{+0.19}_{-0.22} \\times 10^{-18} \\times \\mathrm{GeV}^{-1} \\mathrm{cm}^{-2} \\mathrm{s}^{-1} \\mathrm{sr}^{-1}$ (at 100 TeV). The sensitive energy range for this dataset is 3 - 550 TeV under the SPL assumption. This data was also used to measure the flux under a broken power law, however we did not find any evidence of a low energy cutoff.","sentences":["A measurement of the diffuse astrophysical neutrino spectrum is presented using IceCube data collected from 2011-2022 (10.3 years).","We developed novel detection techniques to search for events with a contained vertex and exiting track induced by muon neutrinos undergoing a charged-current interaction.","Searching for these starting track events allows us to not only more effectively reject atmospheric muons but also atmospheric neutrino backgrounds in the southern sky, opening a new window to the sub-100 TeV astrophysical neutrino sky.","The event selection is constructed using a dynamic starting track veto and machine learning algorithms.","We use this data to measure the astrophysical diffuse flux as a single power law flux (SPL) with a best-fit spectral index of $\\gamma = 2.58 ^{+0.10}_{-0.09}$","and per-flavor normalization of $\\phi^{\\mathrm{Astro}}_{\\mathrm{per-flavor}} = 1.68 ^{+0.19}_{-0.22} \\times 10^{-18} \\times \\mathrm{GeV}^{-1} \\mathrm{cm}^{-2} \\mathrm{s}^{-1} \\mathrm{sr}^{-1}$ (at 100 TeV).","The sensitive energy range for this dataset is 3 - 550 TeV under the SPL assumption.","This data was also used to measure the flux under a broken power law, however we did not find any evidence of a low energy cutoff."],"url":"http://arxiv.org/abs/2402.18026v1","category":"astro-ph.HE"}
{"created":"2024-02-28 03:38:20","title":"Do Large Language Models Mirror Cognitive Language Processing?","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks.","As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing.","Or to what extend LLMs resemble cognitive language processing?","In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing.","We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain.","We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment.","Experimental results indicate that model scaling is positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity.","Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity."],"url":"http://arxiv.org/abs/2402.18023v1","category":"cs.AI"}
{"created":"2024-02-28 03:21:25","title":"Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI","abstract":"This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisions and improve the performance when AI accuracy is high and a challenge when it is low.","sentences":["This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs).","IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions.","As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions.","This paper proposes X-Selector, a method for dynamically selecting explanations.","X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions.","We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support).","The results suggest the potential of X-Selector to guide users to recommended decisions and improve the performance when AI accuracy is high and a challenge when it is low."],"url":"http://arxiv.org/abs/2402.18016v1","category":"cs.HC"}
{"created":"2024-02-28 03:16:44","title":"A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems","abstract":"This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.","sentences":["This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs).","This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems."],"url":"http://arxiv.org/abs/2402.18013v1","category":"cs.CL"}
{"created":"2024-02-28 03:09:12","title":"Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints","abstract":"Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. Theoretical analysis shows that the initial stage results in a distribution focused on feasible solutions, thereby providing a better initialization for the later stage. Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.","sentences":["Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable.","While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly.","Overlooking these constraints can lead to spurious solutions that are unrealistic in practice.","To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models.","To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model.","To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction.","Theoretical analysis shows that the initial stage results in a distribution focused on feasible solutions, thereby providing a better initialization for the later stage.","Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.18012v1","category":"cs.LG"}
{"created":"2024-02-28 02:45:58","title":"Mixer is more than just a model","abstract":"Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of \"mixing\" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains. Experimental results demonstrate that ASM-RH is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks.","sentences":["Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example.","In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information.","Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information.","The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of \"mixing\" in the realm of neural network architectures.","Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements.","This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains.","Experimental results demonstrate that ASM-RH is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks."],"url":"http://arxiv.org/abs/2402.18007v1","category":"cs.LG"}
{"created":"2024-02-28 02:40:09","title":"Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization","abstract":"Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.","sentences":["Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information.","To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation.","The framework is validated via human annotation.","Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments."],"url":"http://arxiv.org/abs/2402.18005v1","category":"cs.CL"}
{"created":"2024-02-28 02:30:59","title":"Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist","abstract":"This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent. In particular, the sample efficiency also allows us to learn directly on the real robot within 3 hours.","sentences":["This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one.","Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose.","In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals.","Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space.","Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry.","Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent.","In particular, the sample efficiency also allows us to learn directly on the real robot within 3 hours."],"url":"http://arxiv.org/abs/2402.18002v1","category":"cs.RO"}
{"created":"2024-02-28 02:23:16","title":"StaPep: an open-source tool for the structure prediction and feature extraction of hydrocarbon-stapled peptides","abstract":"Many tools exist for extracting structural and physiochemical descriptors from linear peptides to predict their properties, but similar tools for hydrocarbon-stapled peptides are lacking.Here, we present StaPep, a Python-based toolkit designed for generating 2D/3D structures and calculating 21 distinct features for hydrocarbon-stapled peptides.The current version supports hydrocarbon-stapled peptides containing 2 non-standard amino acids (norleucine and 2-aminoisobutyric acid) and 6 nonnatural anchoring residues (S3, S5, S8, R3, R5 and R8).Then we established a hand-curated dataset of 201 hydrocarbon-stapled peptides and 384 linear peptides with sequence information and experimental membrane permeability, to showcase StaPep's application in artificial intelligence projects.A machine learning-based predictor utilizing above calculated features was developed with AUC of 0.85, for identifying cell-penetrating hydrocarbon-stapled peptides.StaPep's pipeline spans data retrieval, cleaning, structure generation, molecular feature calculation, and machine learning model construction for hydrocarbon-stapled peptides.The source codes and dataset are freely available on Github: https://github.com/dahuilangda/stapep_package.","sentences":["Many tools exist for extracting structural and physiochemical descriptors from linear peptides to predict their properties, but similar tools for hydrocarbon-stapled peptides are lacking.","Here, we present StaPep, a Python-based toolkit designed for generating 2D/3D structures and calculating 21 distinct features for hydrocarbon-stapled peptides.","The current version supports hydrocarbon-stapled peptides containing 2 non-standard amino acids (norleucine and 2-aminoisobutyric acid) and 6 nonnatural anchoring residues (S3, S5, S8, R3, R5 and R8).Then we established a hand-curated dataset of 201 hydrocarbon-stapled peptides and 384 linear peptides with sequence information and experimental membrane permeability, to showcase StaPep's application in artificial intelligence projects.","A machine learning-based predictor utilizing above calculated features was developed with AUC of 0.85, for identifying cell-penetrating hydrocarbon-stapled peptides.","StaPep's pipeline spans data retrieval, cleaning, structure generation, molecular feature calculation, and machine learning model construction for hydrocarbon-stapled peptides.","The source codes and dataset are freely available on Github: https://github.com/dahuilangda/stapep_package."],"url":"http://arxiv.org/abs/2402.17997v1","category":"q-bio.BM"}
{"created":"2024-02-28 02:00:34","title":"FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization","abstract":"Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits. The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$ memory reduction for LLMs with negligible loss in accuracy.","sentences":["Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks.","However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance.","Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound.","Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence.","In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss.","Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.","The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.","Our work achieves up to 2$\\times$ speedup and 2.3$\\times$ memory reduction for LLMs with negligible loss in accuracy."],"url":"http://arxiv.org/abs/2402.17985v1","category":"cs.LG"}
{"created":"2024-02-28 01:48:54","title":"Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble","abstract":"In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry. To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations identified in previous studies, with the overarching goal of establishing a novel standard for credit default prediction accuracy. Our experimental findings validate the effectiveness of the ensemble model on the dataset, signifying substantial contributions to the field. This innovative approach not only addresses existing obstacles but also sets a precedent for advancing the accuracy and robustness of credit default prediction models.","sentences":["In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization.","Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions.","This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches.","By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry.","To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization.","By utilizing distinct feature sets, our methodology directly tackles limitations identified in previous studies, with the overarching goal of establishing a novel standard for credit default prediction accuracy.","Our experimental findings validate the effectiveness of the ensemble model on the dataset, signifying substantial contributions to the field.","This innovative approach not only addresses existing obstacles but also sets a precedent for advancing the accuracy and robustness of credit default prediction models."],"url":"http://arxiv.org/abs/2402.17979v1","category":"cs.CE"}
{"created":"2024-02-28 01:45:01","title":"Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning","abstract":"Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.","sentences":["Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks.","Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space.","However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks.","To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios.","IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions.","Then, we initialize the environment at this state using a simulator before the exploration phase.","We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively.","The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation.","By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions.","Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments.","Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models."],"url":"http://arxiv.org/abs/2402.17978v1","category":"cs.LG"}
{"created":"2024-02-28 01:41:34","title":"Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards","abstract":"Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and 21\\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model. Repo: \\texttt{https://github.com/apple/ml-reed}.","sentences":["Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors.","We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude.","In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance.","For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and 21\\%.","The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model.","Repo: \\texttt{https://github.com/apple/ml-reed}."],"url":"http://arxiv.org/abs/2402.17975v1","category":"cs.AI"}
{"created":"2024-02-28 01:32:59","title":"All in a Single Image: Large Multimodal Models are In-Image Learners","abstract":"This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVista and Hallusionbench to test the effectiveness of I$^2$L in complex multimodal reasoning tasks and mitigating language hallucination and visual illusion. Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of I$^2$L. Our code is publicly available at https://github.com/AGI-Edgerunners/IIL.","sentences":["This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities.","This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text.","To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task.","We conducted experiments on MathVista and Hallusionbench to test the effectiveness of I$^2$L in complex multimodal reasoning tasks and mitigating language hallucination and visual illusion.","Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of I$^2$L. Our code is publicly available at https://github.com/AGI-Edgerunners/IIL."],"url":"http://arxiv.org/abs/2402.17971v1","category":"cs.CV"}
{"created":"2024-02-28 01:29:36","title":"Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction","abstract":"Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.","sentences":["Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical.","In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content.","However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement.","This paper presents VisCE$^2$, a vision language model-based caption evaluation method.","Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships.","By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance.","Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment."],"url":"http://arxiv.org/abs/2402.17969v1","category":"cs.CV"}
{"created":"2024-02-28 01:00:49","title":"A Compact Anomaly Detection Solution for Science Instruments","abstract":"Small low-cost instruments enable new and exciting mission opportunities yet their constrained volume and limited budgets make them especially susceptible to suffering anomalies during flight. Radiation effects as well as sensor or actuator failure can all pose a serious threat to the continued collection of scientific data as well as cause the partial or complete loss of a mission science payload. Onboard anomaly detection could allow instruments to recover from such events but its ad hoc development typically falls outside the mission timeline or monetary constraints. Here we describe a compact solution for the implementation of onboard anomaly detection meant for space science missions. The device is designed to be interoperable with a broad range of instruments utilizing easily accessible power and logic signals to monitor the state of peripherals and actuators without disrupting their functionality. By leveraging a commercially available microcontroller with a radiation hardened alternative package the device can be inexpensively sourced and assembled with minimal work enabling instrument characterization on an expedited timeline. The system can then be exchanged for a radiation hardened version ensuring the replicability of observed anomalies in a laboratory environment during instrument operations. We also present currently implemented anomaly detection algorithms which enable the system to detect anomalies in instruments with varying failure modes and allow mission designers to choose which detection approach best fits the specific needs of their instrument. Finally, we showcase an example application of this system in the detection of anomalies during the operation of a lysis motor designed for use in biological space instruments.","sentences":["Small low-cost instruments enable new and exciting mission opportunities yet their constrained volume and limited budgets make them especially susceptible to suffering anomalies during flight.","Radiation effects as well as sensor or actuator failure can all pose a serious threat to the continued collection of scientific data as well as cause the partial or complete loss of a mission science payload.","Onboard anomaly detection could allow instruments to recover from such events but its ad hoc development typically falls outside the mission timeline or monetary constraints.","Here we describe a compact solution for the implementation of onboard anomaly detection meant for space science missions.","The device is designed to be interoperable with a broad range of instruments utilizing easily accessible power and logic signals to monitor the state of peripherals and actuators without disrupting their functionality.","By leveraging a commercially available microcontroller with a radiation hardened alternative package the device can be inexpensively sourced and assembled with minimal work enabling instrument characterization on an expedited timeline.","The system can then be exchanged for a radiation hardened version ensuring the replicability of observed anomalies in a laboratory environment during instrument operations.","We also present currently implemented anomaly detection algorithms which enable the system to detect anomalies in instruments with varying failure modes and allow mission designers to choose which detection approach best fits the specific needs of their instrument.","Finally, we showcase an example application of this system in the detection of anomalies during the operation of a lysis motor designed for use in biological space instruments."],"url":"http://arxiv.org/abs/2402.17961v1","category":"physics.ins-det"}
{"created":"2024-02-28 00:57:35","title":"Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping","abstract":"Ovarian cancer detection has traditionally relied on a multi-step process that includes biopsy, tissue staining, and morphological analysis by experienced pathologists. While widely practiced, this conventional approach suffers from several drawbacks: it is qualitative, time-intensive, and heavily dependent on the quality of staining. Mid-infrared (MIR) hyperspectral photothermal imaging is a label-free, biochemically quantitative technology that, when combined with machine learning algorithms, can eliminate the need for staining and provide quantitative results comparable to traditional histology. However, this technology is slow. This work presents a novel approach to MIR photothermal imaging that enhances its speed by an order of magnitude. Our method significantly accelerates data collection by capturing a combination of high-resolution and interleaved, lower-resolution infrared band images and applying computational techniques for data interpolation. We effectively minimize data collection requirements by leveraging sparse data acquisition and employing curvelet-based reconstruction algorithms. This method enables the reconstruction of high-quality, high-resolution images from undersampled datasets and achieving a 10X improvement in data acquisition time. We assessed the performance of our sparse imaging methodology using a variety of quantitative metrics, including mean squared error (MSE), structural similarity index (SSIM), and tissue subtype classification accuracies, employing both random forest and convolutional neural network (CNN) models, accompanied by ROC curves. Our statistically robust analysis, based on data from 100 ovarian cancer patient samples and over 65 million data points, demonstrates the method's capability to produce superior image quality and accurately distinguish between different gynecological tissue types with segmentation accuracy exceeding 95%.","sentences":["Ovarian cancer detection has traditionally relied on a multi-step process that includes biopsy, tissue staining, and morphological analysis by experienced pathologists.","While widely practiced, this conventional approach suffers from several drawbacks: it is qualitative, time-intensive, and heavily dependent on the quality of staining.","Mid-infrared (MIR) hyperspectral photothermal imaging is a label-free, biochemically quantitative technology that, when combined with machine learning algorithms, can eliminate the need for staining and provide quantitative results comparable to traditional histology.","However, this technology is slow.","This work presents a novel approach to MIR photothermal imaging that enhances its speed by an order of magnitude.","Our method significantly accelerates data collection by capturing a combination of high-resolution and interleaved, lower-resolution infrared band images and applying computational techniques for data interpolation.","We effectively minimize data collection requirements by leveraging sparse data acquisition and employing curvelet-based reconstruction algorithms.","This method enables the reconstruction of high-quality, high-resolution images from undersampled datasets and achieving a 10X improvement in data acquisition time.","We assessed the performance of our sparse imaging methodology using a variety of quantitative metrics, including mean squared error (MSE), structural similarity index (SSIM), and tissue subtype classification accuracies, employing both random forest and convolutional neural network (CNN) models, accompanied by ROC curves.","Our statistically robust analysis, based on data from 100 ovarian cancer patient samples and over 65 million data points, demonstrates the method's capability to produce superior image quality and accurately distinguish between different gynecological tissue types with segmentation accuracy exceeding 95%."],"url":"http://arxiv.org/abs/2402.17960v1","category":"cs.CV"}
{"created":"2024-02-28 00:20:25","title":"QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction","abstract":"Inverse problems span across diverse fields. In medical contexts, computed tomography (CT) plays a crucial role in reconstructing a patient's internal structure, presenting challenges due to artifacts caused by inherently ill-posed inverse problems. Previous research advanced image quality via post-processing and deep unrolling algorithms but faces challenges, such as extended convergence times with ultra-sparse data. Despite enhancements, resulting images often show significant artifacts, limiting their effectiveness for real-world diagnostic applications. We aim to explore deep second-order unrolling algorithms for solving imaging inverse problems, emphasizing their faster convergence and lower time complexity compared to common first-order methods like gradient descent. In this paper, we introduce QN-Mixer, an algorithm based on the quasi-Newton approach. We use learned parameters through the BFGS algorithm and introduce Incept-Mixer, an efficient neural architecture that serves as a non-local regularization term, capturing long-range dependencies within images. To address the computational demands typically associated with quasi-Newton algorithms that require full Hessian matrix computations, we present a memory-efficient alternative. Our approach intelligently downsamples gradient information, significantly reducing computational requirements while maintaining performance. The approach is validated through experiments on the sparse-view CT problem, involving various datasets and scanning protocols, and is compared with post-processing and deep unrolling state-of-the-art approaches. Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, all while reducing the number of unrolling iterations required.","sentences":["Inverse problems span across diverse fields.","In medical contexts, computed tomography (CT) plays a crucial role in reconstructing a patient's internal structure, presenting challenges due to artifacts caused by inherently ill-posed inverse problems.","Previous research advanced image quality via post-processing and deep unrolling algorithms but faces challenges, such as extended convergence times with ultra-sparse data.","Despite enhancements, resulting images often show significant artifacts, limiting their effectiveness for real-world diagnostic applications.","We aim to explore deep second-order unrolling algorithms for solving imaging inverse problems, emphasizing their faster convergence and lower time complexity compared to common first-order methods like gradient descent.","In this paper, we introduce QN-Mixer, an algorithm based on the quasi-Newton approach.","We use learned parameters through the BFGS algorithm and introduce Incept-Mixer, an efficient neural architecture that serves as a non-local regularization term, capturing long-range dependencies within images.","To address the computational demands typically associated with quasi-Newton algorithms that require full Hessian matrix computations, we present a memory-efficient alternative.","Our approach intelligently downsamples gradient information, significantly reducing computational requirements while maintaining performance.","The approach is validated through experiments on the sparse-view CT problem, involving various datasets and scanning protocols, and is compared with post-processing and deep unrolling state-of-the-art approaches.","Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, all while reducing the number of unrolling iterations required."],"url":"http://arxiv.org/abs/2402.17951v1","category":"eess.IV"}
{"created":"2024-02-27 23:12:45","title":"Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures","abstract":"Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures.","sentences":["Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive.","Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters.","However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets.","In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning.","FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters.","By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets.","Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures."],"url":"http://arxiv.org/abs/2402.17934v1","category":"cs.CL"}
{"created":"2024-02-27 23:11:09","title":"ICAT: An Indoor Connected and Autonomous Testbed for Vehicle Computing","abstract":"Indoor autonomous driving testbeds have emerged to complement expensive outdoor testbeds and virtual simulations, offering scalable and cost-effective solutions for research in navigation, traffic optimization, and swarm intelligence. However, they often lack the robust sensing and computing infrastructure for advanced research. Addressing these limitations, we introduce the Indoor Connected Autonomous Testbed (ICAT), a platform that not only tackles the unique challenges of indoor autonomous driving but also innovates vehicle computing and V2X communication. Moreover, ICAT leverages digital twins through CARLA and SUMO simulations, facilitating both centralized and decentralized autonomy deployments.","sentences":["Indoor autonomous driving testbeds have emerged to complement expensive outdoor testbeds and virtual simulations, offering scalable and cost-effective solutions for research in navigation, traffic optimization, and swarm intelligence.","However, they often lack the robust sensing and computing infrastructure for advanced research.","Addressing these limitations, we introduce the Indoor Connected Autonomous Testbed (ICAT), a platform that not only tackles the unique challenges of indoor autonomous driving but also innovates vehicle computing and V2X communication.","Moreover, ICAT leverages digital twins through CARLA and SUMO simulations, facilitating both centralized and decentralized autonomy deployments."],"url":"http://arxiv.org/abs/2402.17933v1","category":"cs.RO"}
{"created":"2024-02-27 23:07:59","title":"Universal energy-speed-accuracy trade-offs in driven nonequilibrium systems","abstract":"Physical systems driven away from equilibrium by an external controller dissipate heat to the environment; the excess entropy production in the thermal reservoir can be interpreted as a \"cost\" to transform the system in a finite time. The connection between measure theoretic optimal transport and dissipative nonequilibrium dynamics provides a language for quantifying this cost and has resulted in a collection of \"thermodynamic speed limits\", which argue that the minimum dissipation of a transformation between two probability distributions is directly proportional to the rate of driving. Thermodynamic speed limits rely on the assumption that the target probability distribution is perfectly realized, which is almost never the case in experiments or numerical simulations. Here, we address the ubiquitous situation in which the external controller is imperfect. As a consequence, we obtain a lower bound for the dissipated work in generic nonequilibrium control problems that 1) is asymptotically tight and 2) matches the thermodynamic speed limit in the case of optimal driving. We illustrate these bounds on analytically solvable examples and also develop a strategy for optimizing minimally dissipative protocols based on optimal transport flow matching, a generative machine learning technique. This latter approach ensures the scalability of both the theoretical and computational framework we put forth. Crucially, we demonstrate that we can compute the terms in our bound numerically using efficient algorithms from the computational optimal transport literature and that the protocols that we learn saturate the bound.","sentences":["Physical systems driven away from equilibrium by an external controller dissipate heat to the environment; the excess entropy production in the thermal reservoir can be interpreted as a \"cost\" to transform the system in a finite time.","The connection between measure theoretic optimal transport and dissipative nonequilibrium dynamics provides a language for quantifying this cost and has resulted in a collection of \"thermodynamic speed limits\", which argue that the minimum dissipation of a transformation between two probability distributions is directly proportional to the rate of driving.","Thermodynamic speed limits rely on the assumption that the target probability distribution is perfectly realized, which is almost never the case in experiments or numerical simulations.","Here, we address the ubiquitous situation in which the external controller is imperfect.","As a consequence, we obtain a lower bound for the dissipated work in generic nonequilibrium control problems that 1) is asymptotically tight and 2) matches the thermodynamic speed limit in the case of optimal driving.","We illustrate these bounds on analytically solvable examples and also develop a strategy for optimizing minimally dissipative protocols based on optimal transport flow matching, a generative machine learning technique.","This latter approach ensures the scalability of both the theoretical and computational framework we put forth.","Crucially, we demonstrate that we can compute the terms in our bound numerically using efficient algorithms from the computational optimal transport literature and that the protocols that we learn saturate the bound."],"url":"http://arxiv.org/abs/2402.17931v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 23:06:53","title":"Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning","abstract":"People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the goal. We evaluate these capabilities in two cooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters.","sentences":["People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions.","How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner?","This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance.","Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan.","Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the goal.","We evaluate these capabilities in two cooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters."],"url":"http://arxiv.org/abs/2402.17930v1","category":"cs.AI"}
{"created":"2024-02-27 22:24:05","title":"Generalized Bayesian Additive Regression Trees for Restricted Mean Survival Time Inference","abstract":"Prediction methods for time-to-event outcomes often utilize survival models that rely on strong assumptions about noninformative censoring or on how individual-level covariates and survival functions are related. When the main interest is in predicting individual-level restricted mean survival times (RMST), reliance on such assumptions can lead to poor predictive performance if these assumptions are not satisfied. We propose a generalized Bayes framework that avoids full probability modeling of all survival outcomes by using an RMST-targeted loss function that depends on a collection of inverse probability of censoring weights (IPCW). In our generalized Bayes formulation, we utilize a flexible additive tree regression model for the RMST function, and the posterior distribution of interest is obtained through model-averaging IPCW-conditional loss function-based pseudo-Bayesian posteriors. Because informative censoring can be captured by the IPCW-dependent loss function, our approach only requires one to specify a model for the censoring distribution, thereby obviating the need for complex joint modeling to handle informative censoring. We evaluate the performance of our method through a series of simulations that compare it with several well-known survival machine learning methods, and we illustrate the application of our method using a multi-site cohort of breast cancer patients with clinical and genomic covariates.","sentences":["Prediction methods for time-to-event outcomes often utilize survival models that rely on strong assumptions about noninformative censoring or on how individual-level covariates and survival functions are related.","When the main interest is in predicting individual-level restricted mean survival times (RMST), reliance on such assumptions can lead to poor predictive performance if these assumptions are not satisfied.","We propose a generalized Bayes framework that avoids full probability modeling of all survival outcomes by using an RMST-targeted loss function that depends on a collection of inverse probability of censoring weights (IPCW).","In our generalized Bayes formulation, we utilize a flexible additive tree regression model for the RMST function, and the posterior distribution of interest is obtained through model-averaging IPCW-conditional loss function-based pseudo-Bayesian posteriors.","Because informative censoring can be captured by the IPCW-dependent loss function, our approach only requires one to specify a model for the censoring distribution, thereby obviating the need for complex joint modeling to handle informative censoring.","We evaluate the performance of our method through a series of simulations that compare it with several well-known survival machine learning methods, and we illustrate the application of our method using a multi-site cohort of breast cancer patients with clinical and genomic covariates."],"url":"http://arxiv.org/abs/2402.17920v1","category":"stat.ME"}
{"created":"2024-02-27 22:14:18","title":"Quanto Option Pricing on a Multivariate Levy Process Model with a Generative Artificial Intelligence","abstract":"In this study, we discuss a machine learning technique to price exotic options with two underlying assets based on a non-Gaussian Levy process model. We introduce a new multivariate Levy process model named the generalized normal tempered stable (gNTS) process, which is defined by time-changed multivariate Brownian motion. Since the probability density function (PDF) of the gNTS process is not given by a simple analytic formula, we use the conditional real-valued non-volume preserving (CRealNVP) model, which is a sort of flow-based generative networks. After that, we discuss the no-arbitrage pricing on the gNTS model for pricing the quanto option whose underlying assets consist of a foreign index and foreign exchange rate. We also present the training of the CRealNVP model to learn the PDF of the gNTS process using a training set generated by Monte Carlo simulation. Next, we estimate the parameters of the gNTS model with the trained CRealNVP model using the empirical data observed in the market. Finally, we provide a method to find an equivalent martingale measure on the gNTS model and to price the quanto option using the CRealNVP model with the risk-neutral parameters of the gNTS model.","sentences":["In this study, we discuss a machine learning technique to price exotic options with two underlying assets based on a non-Gaussian Levy process model.","We introduce a new multivariate Levy process model named the generalized normal tempered stable (gNTS) process, which is defined by time-changed multivariate Brownian motion.","Since the probability density function (PDF) of the gNTS process is not given by a simple analytic formula, we use the conditional real-valued non-volume preserving (CRealNVP) model, which is a sort of flow-based generative networks.","After that, we discuss the no-arbitrage pricing on the gNTS model for pricing the quanto option whose underlying assets consist of a foreign index and foreign exchange rate.","We also present the training of the CRealNVP model to learn the PDF of the gNTS process using a training set generated by Monte Carlo simulation.","Next, we estimate the parameters of the gNTS model with the trained CRealNVP model using the empirical data observed in the market.","Finally, we provide a method to find an equivalent martingale measure on the gNTS model and to price the quanto option using the CRealNVP model with the risk-neutral parameters of the gNTS model."],"url":"http://arxiv.org/abs/2402.17919v1","category":"q-fin.MF"}
{"created":"2024-02-27 22:10:51","title":"Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals","abstract":"In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping. In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data. Our new algorithm integrates Long Short-Term Memory (LSTM) networks with collaborative filtering concepts to identify common physiological states across patients. Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725. Moreover, our algorithm outperforms autoencoders in learning more structured latent representations of the physiological signals. These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collected multivariate time series to improve clinical care practices.","sentences":["In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping.","In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data.","Our new algorithm integrates Long Short-Term Memory (LSTM) networks with collaborative filtering concepts to identify common physiological states across patients.","Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725.","Moreover, our algorithm outperforms autoencoders in learning more structured latent representations of the physiological signals.","These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collected multivariate time series to improve clinical care practices."],"url":"http://arxiv.org/abs/2402.17917v1","category":"cs.LG"}
{"created":"2024-02-27 22:07:52","title":"LLM-Resistant Math Word Problem Generation via Adversarial Attacks","abstract":"Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure to guide future research on LLM's mathematical capability.","sentences":["Large language models (LLMs) have significantly transformed the educational landscape.","As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs.","In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs.","Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems.","We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability.","We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models.","Additionally, we conduct automatic analysis on math problems and investigate the cause of failure to guide future research on LLM's mathematical capability."],"url":"http://arxiv.org/abs/2402.17916v1","category":"cs.CL"}
{"created":"2024-02-27 22:06:55","title":"Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers","abstract":"Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.","sentences":["Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis.","This is largely due to the complexity and nuance involved in studying various dialects.","We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts.","We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations."],"url":"http://arxiv.org/abs/2402.17914v1","category":"cs.CL"}
{"created":"2024-02-27 22:00:50","title":"Using AI libraries for Incompressible Computational Fluid Dynamics","abstract":"Recently, there has been a huge effort focused on developing highly efficient open source libraries to perform Artificial Intelligence (AI) related computations on different computer architectures (for example, CPUs, GPUs and new AI processors). This has not only made the algorithms based on these libraries highly efficient and portable between different architectures, but also has substantially simplified the entry barrier to develop methods using AI. Here, we present a novel methodology to bring the power of both AI software and hardware into the field of numerical modelling by repurposing AI methods, such as Convolutional Neural Networks (CNNs), for the standard operations required in the field of the numerical solution of Partial Differential Equations (PDEs). The aim of this work is to bring the high performance, architecture agnosticism and ease of use into the field of the numerical solution of PDEs. We use the proposed methodology to solve the advection-diffusion equation, the non-linear Burgers equation and incompressible flow past a bluff body. For the latter, a convolutional neural network is used as a multigrid solver in order to enforce the incompressibility constraint. We show that the presented methodology can solve all these problems using repurposed AI libraries in an efficient way, and presents a new avenue to explore in the development of methods to solve PDEs and Computational Fluid Dynamics problems with implicit methods.","sentences":["Recently, there has been a huge effort focused on developing highly efficient open source libraries to perform Artificial Intelligence (AI) related computations on different computer architectures (for example, CPUs, GPUs and new AI processors).","This has not only made the algorithms based on these libraries highly efficient and portable between different architectures, but also has substantially simplified the entry barrier to develop methods using AI.","Here, we present a novel methodology to bring the power of both AI software and hardware into the field of numerical modelling by repurposing AI methods, such as Convolutional Neural Networks (CNNs), for the standard operations required in the field of the numerical solution of Partial Differential Equations (PDEs).","The aim of this work is to bring the high performance, architecture agnosticism and ease of use into the field of the numerical solution of PDEs.","We use the proposed methodology to solve the advection-diffusion equation, the non-linear Burgers equation and incompressible flow past a bluff body.","For the latter, a convolutional neural network is used as a multigrid solver in order to enforce the incompressibility constraint.","We show that the presented methodology can solve all these problems using repurposed AI libraries in an efficient way, and presents a new avenue to explore in the development of methods to solve PDEs and Computational Fluid Dynamics problems with implicit methods."],"url":"http://arxiv.org/abs/2402.17913v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 21:28:08","title":"Exoplanets Prediction in Multi-Planetary Systems and Determining the Correlation Between the Parameters of Planets and Host Stars Using Artificial Intelligence","abstract":"The number of extrasolar planets discovered is increasing, so that more than five thousand exoplanets have been confirmed to date. Now we have an opportunity to test the validity of the laws governing planetary systems and take steps to discover the relationships between the physical parameters of planets and stars. Firstly, we present the results of a search for additional exoplanets in 229 multi-planetary systems that house at least three or more confirmed planets, employing a logarithmic spacing between planets in our Solar System known as the Titius-Bode (TB) relation. We find that the planets in $\\sim53\\%$ of these systems adhere to a logarithmic spacing relation remarkably better than the Solar System planets. We predict the presence of 426 additional exoplanets, 47 of which are located within the habitable zone (HZ), and five of the 47 planets have a maximum mass limit of 0.1-2$M_{\\oplus}$ and a maximum radius lower than 1.25$R_{\\oplus}$. Secondly, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities. We classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\\oplus}$ and $M_{p}=52.48M_{\\oplus}$. Giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements. We highlight that planetary mass, orbital period, and stellar mass play crucial roles in predicting exoplanet radius. Notably, our study reveals a noteworthy result: for giant planets, we observe a strong correlation between planetary radius and the mass of their host stars, which might provide intriguing insights into the relationship between giant planet formation and stellar characteristics.","sentences":["The number of extrasolar planets discovered is increasing, so that more than five thousand exoplanets have been confirmed to date.","Now we have an opportunity to test the validity of the laws governing planetary systems and take steps to discover the relationships between the physical parameters of planets and stars.","Firstly, we present the results of a search for additional exoplanets in 229 multi-planetary systems that house at least three or more confirmed planets, employing a logarithmic spacing between planets in our Solar System known as the Titius-Bode (TB) relation.","We find that the planets in $\\sim53\\%$ of these systems adhere to a logarithmic spacing relation remarkably better than the Solar System planets.","We predict the presence of 426 additional exoplanets, 47 of which are located within the habitable zone (HZ), and five of the 47 planets have a maximum mass limit of 0.1-2$M_{\\oplus}$ and a maximum radius lower than 1.25$R_{\\oplus}$.","Secondly, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities.","We classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\\oplus}$ and $M_{p}=52.48M_{\\oplus}$.","Giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements.","We highlight that planetary mass, orbital period, and stellar mass play crucial roles in predicting exoplanet radius.","Notably, our study reveals a noteworthy result: for giant planets, we observe a strong correlation between planetary radius and the mass of their host stars, which might provide intriguing insights into the relationship between giant planet formation and stellar characteristics."],"url":"http://arxiv.org/abs/2402.17898v1","category":"astro-ph.EP"}
{"created":"2024-02-27 21:27:16","title":"Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents","abstract":"Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective. We show that users spend a lot of ``effort'' on these questions in terms of signals like clicks and session length, and that they are also challenging for GPT-4. We also show that ``slow thinking'' answering techniques, like decomposition into sub-questions shows benefit over answering directly. We release $\\sim$ 100k Researchy Questions, along with the Clueweb22 URLs that were clicked.","sentences":["Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs).","Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question.","Hence, good performance on these benchmarks provides a false sense of security.","A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''.","We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid.","We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective.","We show that users spend a lot of ``effort'' on these questions in terms of signals like clicks and session length, and that they are also challenging for GPT-4.","We also show that ``slow thinking'' answering techniques, like decomposition into sub-questions shows benefit over answering directly.","We release $\\sim$ 100k Researchy Questions, along with the Clueweb22 URLs that were clicked."],"url":"http://arxiv.org/abs/2402.17896v1","category":"cs.CL"}
{"created":"2024-02-27 21:02:47","title":"ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection","abstract":"Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a \\textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tractable estimator of the partition function using the Monte Carlo-based importance sampling technique. Extensive experiments across OOD detection benchmarks empirically demonstrate that our proposed \\textsc{ConjNorm} has established a new state-of-the-art in a variety of OOD detection setups, outperforming the current best method by up to 13.25$\\%$ and 28.19$\\%$ (FPR95) on CIFAR-100 and ImageNet-1K, respectively.","sentences":["Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning.","Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples.","Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints.","To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions.","Leveraging the conjugation constraint revealed in our theorem, we introduce a \\textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset.","In light of the computational challenges of normalization, we devise an unbiased and analytically tractable estimator of the partition function using the Monte Carlo-based importance sampling technique.","Extensive experiments across OOD detection benchmarks empirically demonstrate that our proposed \\textsc{ConjNorm} has established a new state-of-the-art in a variety of OOD detection setups, outperforming the current best method by up to 13.25$\\%$ and 28.19$\\%$ (FPR95) on CIFAR-100 and ImageNet-1K, respectively."],"url":"http://arxiv.org/abs/2402.17888v1","category":"cs.LG"}
{"created":"2024-02-27 21:01:41","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability","abstract":"With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering questions. Our experimental results demonstrate that JMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using conventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3% on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on MedQA) significantly outperforms other public models (Meditron-7B: 50.1%, 47.9%), proving its superiority in terms of cost (our training time: 37 hours, traditional method: 144 hours), efficiency, and effectiveness in medical question-answering tasks. Through this work, we provide a new and efficient knowledge enhancement tool for healthcare, demonstrating the great potential of integrating IR and LLM training in precision medical information retrieval and question-answering systems.","sentences":["With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services.","In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems.","To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase.","This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks.","By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering questions.","Our experimental results demonstrate that JMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using conventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3% on MedQA).","For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on MedQA) significantly outperforms other public models (Meditron-7B: 50.1%, 47.9%), proving its superiority in terms of cost (our training time: 37 hours, traditional method: 144 hours), efficiency, and effectiveness in medical question-answering tasks.","Through this work, we provide a new and efficient knowledge enhancement tool for healthcare, demonstrating the great potential of integrating IR and LLM training in precision medical information retrieval and question-answering systems."],"url":"http://arxiv.org/abs/2402.17887v1","category":"cs.CL"}
{"created":"2024-02-27 19:54:30","title":"REPrune: Channel Pruning via Kernel Representative Selection","abstract":"Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoiding the conventional train-prune-finetune sequence. Experimental results highlight that REPrune performs better in computer vision tasks than existing methods, effectively achieving a balance between acceleration ratio and performance retention.","sentences":["Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs).","The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources.","However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs.","In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity.","REPrune identifies similar kernels within each channel using agglomerative clustering.","Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem.","By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoiding the conventional train-prune-finetune sequence.","Experimental results highlight that REPrune performs better in computer vision tasks than existing methods, effectively achieving a balance between acceleration ratio and performance retention."],"url":"http://arxiv.org/abs/2402.17862v1","category":"cs.CV"}
{"created":"2024-02-27 19:52:54","title":"Towards AI Accountability Infrastructure: Gaps and Opportunities in AI Audit Tooling","abstract":"Audits are critical mechanisms for identifying the risks and limitations of deployed artificial intelligence (AI) systems. However, the effective execution of AI audits remains incredibly difficult. As a result, practitioners make use of various tools to support their efforts. Drawing on interviews with 35 AI audit practitioners and a landscape analysis of 390 tools, we map the current ecosystem of available AI audit tools. While there are many tools designed to assist practitioners with setting standards and evaluating AI systems, these tools often fell short of supporting the accountability goals of AI auditing in practice. We thus highlight areas for future tool development beyond evaluation -- from harms discovery to advocacy -- and outline challenges practitioners faced in their efforts to use AI audit tools. We conclude that resources are lacking to adequately support the full scope of needs for many AI audit practitioners and recommend that the field move beyond tools for just evaluation, towards more comprehensive infrastructure for AI accountability.","sentences":["Audits are critical mechanisms for identifying the risks and limitations of deployed artificial intelligence (AI) systems.","However, the effective execution of AI audits remains incredibly difficult.","As a result, practitioners make use of various tools to support their efforts.","Drawing on interviews with 35 AI audit practitioners and a landscape analysis of 390 tools, we map the current ecosystem of available AI audit tools.","While there are many tools designed to assist practitioners with setting standards and evaluating AI systems, these tools often fell short of supporting the accountability goals of AI auditing in practice.","We thus highlight areas for future tool development beyond evaluation -- from harms discovery to advocacy -- and outline challenges practitioners faced in their efforts to use AI audit tools.","We conclude that resources are lacking to adequately support the full scope of needs for many AI audit practitioners and recommend that the field move beyond tools for just evaluation, towards more comprehensive infrastructure for AI accountability."],"url":"http://arxiv.org/abs/2402.17861v1","category":"cs.CY"}
{"created":"2024-02-27 19:36:27","title":"Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations","abstract":"Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows along with varying system parameters. We showcase that it has competitive accuracy and efficiency compared to the neural PDE solver that operates on full-order space.","sentences":["Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs).","Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations.","In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space.","This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization.","We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows along with varying system parameters.","We showcase that it has competitive accuracy and efficiency compared to the neural PDE solver that operates on full-order space."],"url":"http://arxiv.org/abs/2402.17853v1","category":"cs.LG"}
{"created":"2024-02-27 19:08:05","title":"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems","abstract":"Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.","sentences":["Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation.","We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs).","We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection.","The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up.","Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves."],"url":"http://arxiv.org/abs/2402.17840v1","category":"cs.CL"}
{"created":"2024-02-27 19:00:01","title":"Prediction-Powered Ranking of Large Language Models","abstract":"Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with (the distribution of) human pairwise preferences. Our framework is computationally efficient, easy to use, and does not make any assumption about the distribution of human preferences nor about the degree of alignment between the pairwise comparisons by the humans and the strong large language model.","sentences":["Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans.","One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs.","However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences.","Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings.","In this work, we develop a statistical framework to bridge this gap.","Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison.","Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with (the distribution of) human pairwise preferences.","Our framework is computationally efficient, easy to use, and does not make any assumption about the distribution of human preferences nor about the degree of alignment between the pairwise comparisons by the humans and the strong large language model."],"url":"http://arxiv.org/abs/2402.17826v1","category":"cs.LG"}
{"created":"2024-02-27 19:00:00","title":"scida: scalable analysis for scientific big data","abstract":"scida is a Python package for reading and analyzing large scientific data sets with support for various cosmological and galaxy formation simulations out-of-the-box. Data access is provided through a hierarchical dictionary-like data structure after a simple load() function. Using the dask library for scalable, parallel and out-of-core computation, all computation requests from a user session are first collected in a task graph. Arbitrary custom analysis, as well as all available dask (array) operations, can be performed. The subsequent computation is executed only upon request, on a target resource (e.g. a HPC cluster).","sentences":["scida is a Python package for reading and analyzing large scientific data sets with support for various cosmological and galaxy formation simulations out-of-the-box.","Data access is provided through a hierarchical dictionary-like data structure after a simple load() function.","Using the dask library for scalable, parallel and out-of-core computation, all computation requests from a user session are first collected in a task graph.","Arbitrary custom analysis, as well as all available dask (array) operations, can be performed.","The subsequent computation is executed only upon request, on a target resource (e.g. a HPC cluster)."],"url":"http://arxiv.org/abs/2402.17818v1","category":"astro-ph.IM"}
{"created":"2024-02-27 16:21:28","title":"Compass: A Decentralized Scheduler for Latency-Sensitive ML Workflows","abstract":"We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Compass, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory. Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.","sentences":["We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing.","In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity.","We propose Compass, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory.","Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources.","In one case, just half the servers were needed for processing the same workload."],"url":"http://arxiv.org/abs/2402.17652v2","category":"cs.DC"}
{"created":"2024-02-27 15:53:15","title":"FedUV: Uniformity and Variance for Heterogeneous Federated Learning","abstract":"Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting regardless of the local data distribution, thus offsetting proneness to bias while being flexible to the data. On extensive experiments in both label-shift and feature-shift settings, we verify that our method achieves highest performance by a large margin especially in highly non-IID cases in addition to being scalable to larger models and datasets.","sentences":["Federated learning is a promising framework to train neural networks with widely distributed data.","However, performance degrades heavily with heterogeneously distributed data.","Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier.","We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values.","We find that there are differences when training in IID and non-IID settings.","Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder.","These regularizations promote local models to act as if it were in an IID setting regardless of the local data distribution, thus offsetting proneness to bias while being flexible to the data.","On extensive experiments in both label-shift and feature-shift settings, we verify that our method achieves highest performance by a large margin especially in highly non-IID cases in addition to being scalable to larger models and datasets."],"url":"http://arxiv.org/abs/2402.18372v1","category":"cs.LG"}
{"created":"2024-02-27 14:45:04","title":"TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space acquired by TruthX plays a pivotal role in controlling LLM to produce truthful or hallucinatory responses.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks.","However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge.","In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space.","TruthX","employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space.","During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs.","Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark.","Further analyses suggest that the truthful space acquired by TruthX plays a pivotal role in controlling LLM to produce truthful or hallucinatory responses."],"url":"http://arxiv.org/abs/2402.17811v1","category":"cs.CL"}
{"created":"2024-02-27 12:43:09","title":"BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning","abstract":"Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences. The model is pre-trained and fine-tuned with a large number of experiments, including \\emph{3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets}, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at \\url{https://github.com/QizhiPei/BioT5}.","sentences":["Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins.","However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC).","This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery.","BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data.","These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences.","The model is pre-trained and fine-tuned with a large number of experiments, including \\emph{3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets}, demonstrating the remarkable performance and state-of-the-art results in most cases.","BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology.","Our code is available at \\url{https://github.com/QizhiPei/BioT5}."],"url":"http://arxiv.org/abs/2402.17810v1","category":"q-bio.QM"}
{"created":"2024-02-27 11:29:36","title":"Exploring Gene Regulatory Interaction Networks and predicting therapeutic molecules for Hypopharyngeal Cancer and EGFR-mutated lung adenocarcinoma","abstract":"With the advent of Information technology, the Bioinformatics research field is becoming increasingly attractive to researchers and academicians. The recent development of various Bioinformatics toolkits has facilitated the rapid processing and analysis of vast quantities of biological data for human perception. Most studies focus on locating two connected diseases and making some observations to construct diverse gene regulatory interaction networks, a forerunner to general drug design for curing illness. For instance, Hypopharyngeal cancer is a disease that is associated with EGFR-mutated lung adenocarcinoma. In this study, we select EGFR-mutated lung adenocarcinoma and Hypopharyngeal cancer by finding the Lung metastases in hypopharyngeal cancer. To conduct this study, we collect Mircorarray datasets from GEO (Gene Expression Omnibus), an online database controlled by NCBI. Differentially expressed genes, common genes, and hub genes between the selected two diseases are detected for the succeeding move. Our research findings have suggested common therapeutic molecules for the selected diseases based on 10 hub genes with the highest interactions according to the degree topology method and the maximum clique centrality (MCC). Our suggested therapeutic molecules will be fruitful for patients with those two diseases simultaneously.","sentences":["With the advent of Information technology, the Bioinformatics research field is becoming increasingly attractive to researchers and academicians.","The recent development of various Bioinformatics toolkits has facilitated the rapid processing and analysis of vast quantities of biological data for human perception.","Most studies focus on locating two connected diseases and making some observations to construct diverse gene regulatory interaction networks, a forerunner to general drug design for curing illness.","For instance, Hypopharyngeal cancer is a disease that is associated with EGFR-mutated lung adenocarcinoma.","In this study, we select EGFR-mutated lung adenocarcinoma and Hypopharyngeal cancer by finding the Lung metastases in hypopharyngeal cancer.","To conduct this study, we collect Mircorarray datasets from GEO (Gene Expression Omnibus), an online database controlled by NCBI.","Differentially expressed genes, common genes, and hub genes between the selected two diseases are detected for the succeeding move.","Our research findings have suggested common therapeutic molecules for the selected diseases based on 10 hub genes with the highest interactions according to the degree topology method and the maximum clique centrality (MCC).","Our suggested therapeutic molecules will be fruitful for patients with those two diseases simultaneously."],"url":"http://arxiv.org/abs/2402.17807v1","category":"q-bio.GN"}
{"created":"2024-02-27 11:04:06","title":"Graph Neural Networks and Arithmetic Circuits","abstract":"We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.","sentences":["We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types.","We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers.","In our results the activation function of the network becomes a gate type in the circuit.","Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions."],"url":"http://arxiv.org/abs/2402.17805v1","category":"cs.LG"}
{"created":"2024-02-28 18:56:56","title":"A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic","abstract":"Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. But, different kinds of questions have different answers, and different answers have different complexities. In this paper, we introduce a novel framework for analyzing the complexity of a question answer based on the natural deduction calculus as presented in Prawitz (1965). Our framework is novel both in that no one to our knowledge has used this logic as a basis for complexity classes, and also in that no other existing complexity classes to these have been delineated using any analogous methods either. We identify three decidable fragments in particular called the forward, query and planning fragments, and we compare this to what would be needed to do proofs for the complete first-order calculus, for which theorem-proving is long known to be undecidable.","sentences":["Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex.","Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning.","But, different kinds of questions have different answers, and different answers have different complexities.","In this paper, we introduce a novel framework for analyzing the complexity of a question answer based on the natural deduction calculus as presented in Prawitz (1965).","Our framework is novel both in that no one to our knowledge has used this logic as a basis for complexity classes, and also in that no other existing complexity classes to these have been delineated using any analogous methods either.","We identify three decidable fragments in particular called the forward, query and planning fragments, and we compare this to what would be needed to do proofs for the complete first-order calculus, for which theorem-proving is long known to be undecidable."],"url":"http://arxiv.org/abs/2402.18566v1","category":"cs.IR"}
{"created":"2024-02-28 18:38:34","title":"Probabilistic work extraction on a classical oscillator beyond the second law","abstract":"We demonstrate experimentally that, applying optimal protocols which drive the system between two equilibrium states characterized by a free energy difference $\\Delta F$, we can maximize the probability of performing the transition between the two states with a work $W$ smaller than $\\Delta F$. The second law holds only on average, resulting in the inequality $\\langle W \\rangle \\geq \\Delta F$. The experiment is performed using an underdamped oscillator evolving in a double-well potential. We show that with a suitable choice of parameters the probability of obtaining trajectories with $W \\le \\Delta F$ can be larger than 90 %. Very fast protocols are a key feature to obtain these results which are explained in terms of the Jarzynski equality.","sentences":["We demonstrate experimentally that, applying optimal protocols which drive the system between two equilibrium states characterized by a free energy difference $\\Delta F$, we can maximize the probability of performing the transition between the two states with a work $W$ smaller than $\\Delta F$.","The second law holds only on average, resulting in the inequality","$\\langle W \\rangle \\geq \\Delta F$.","The experiment is performed using an underdamped oscillator evolving in a double-well potential.","We show that with a suitable choice of parameters the probability of obtaining trajectories with $W \\le \\Delta F$ can be larger than 90 %.","Very fast protocols are a key feature to obtain these results which are explained in terms of the Jarzynski equality."],"url":"http://arxiv.org/abs/2402.18556v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 18:35:18","title":"Amplified entanglement witnessed in a quantum critical metal","abstract":"Strong correlations in matter promote a landscape of ground states and the associated quantum critical points. For metallic systems, there is increasing recognition that the quantum criticality goes beyond the standard Landau framework and, thus, novel means are needed to characterize the quantum critical fluid. Here we do so by studying the entanglement properties near a quantum critical point of a strongly correlated metal. We calculate the mutual information and quantum Fisher information of an Anderson/Kondo lattice model across its Kondo destruction quantum critical point, with the former measuring the bipartite entanglement between two subsystems and the latter serving as witness for multipartite entanglement in the entire system. The mutual information between the conduction electrons and local moments reveals a dynamical effect of Kondo correlations across the quantum critical point. Moreover, the quantum Fisher information of the spin degree of freedom peaks at the quantum critical point and indicates a strongly entangled ground state. Our work opens a new avenue to advance the understanding of metallic quantum criticality by entanglement means in a broad range of strongly correlated systems, and reveals a new regime of quantum matter to witness amplified entanglement.","sentences":["Strong correlations in matter promote a landscape of ground states and the associated quantum critical points.","For metallic systems, there is increasing recognition that the quantum criticality goes beyond the standard Landau framework and, thus, novel means are needed to characterize the quantum critical fluid.","Here we do so by studying the entanglement properties near a quantum critical point of a strongly correlated metal.","We calculate the mutual information and quantum Fisher information of an Anderson/Kondo lattice model across its Kondo destruction quantum critical point, with the former measuring the bipartite entanglement between two subsystems and the latter serving as witness for multipartite entanglement in the entire system.","The mutual information between the conduction electrons and local moments reveals a dynamical effect of Kondo correlations across the quantum critical point.","Moreover, the quantum Fisher information of the spin degree of freedom peaks at the quantum critical point and indicates a strongly entangled ground state.","Our work opens a new avenue to advance the understanding of metallic quantum criticality by entanglement means in a broad range of strongly correlated systems, and reveals a new regime of quantum matter to witness amplified entanglement."],"url":"http://arxiv.org/abs/2402.18552v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 18:23:48","title":"Peak Effect and Dynamics of Stripe and Pattern Forming Systems on a Periodic One Dimensional Substrate","abstract":"We examine the ordering, pinning, and dynamics of two-dimensional pattern forming systems interacting with a periodic one-dimensional substrate. In the absence of the substrate, particles with competing long-range repulsion and short-range attraction form anisotropic crystal, stripe, and bubble states. When the system is tuned across the stripe transition in the presence of a substrate, we find that there is a peak effect in the critical depinning force when the stripes align and become commensurate with the substrate. Under an applied drive, the anisotropic crystal and stripe states can exhibit soliton depinning and plastic flow. When the stripes depin plastically, they dynamically reorder into a moving stripe state that is perpendicular to the substrate trough direction. We also find that when the substrate spacing is smaller than the widths of the bubbles or stripes, the system forms pinned stripe states that are perpendicular to the substrate trough direction. The system exhibits multiple reentrant pinning effects as a function of increasing attraction, with the anisotropic crystal and large bubble states experiencing weak pinning but the stripe and smaller bubble states showing stronger pinning. We map out the different dynamic phases as a function of filling, the strength of the attractive interaction term, the substrate strength, and the drive, and demonstrate that the different phases produce identifiable features in the transport curves and particle orderings.","sentences":["We examine the ordering, pinning, and dynamics of two-dimensional pattern forming systems interacting with a periodic one-dimensional substrate.","In the absence of the substrate, particles with competing long-range repulsion and short-range attraction form anisotropic crystal, stripe, and bubble states.","When the system is tuned across the stripe transition in the presence of a substrate, we find that there is a peak effect in the critical depinning force when the stripes align and become commensurate with the substrate.","Under an applied drive, the anisotropic crystal and stripe states can exhibit soliton depinning and plastic flow.","When the stripes depin plastically, they dynamically reorder into a moving stripe state that is perpendicular to the substrate trough direction.","We also find that when the substrate spacing is smaller than the widths of the bubbles or stripes, the system forms pinned stripe states that are perpendicular to the substrate trough direction.","The system exhibits multiple reentrant pinning effects as a function of increasing attraction, with the anisotropic crystal and large bubble states experiencing weak pinning but the stripe and smaller bubble states showing stronger pinning.","We map out the different dynamic phases as a function of filling, the strength of the attractive interaction term, the substrate strength, and the drive, and demonstrate that the different phases produce identifiable features in the transport curves and particle orderings."],"url":"http://arxiv.org/abs/2402.18539v1","category":"cond-mat.soft"}
{"created":"2024-02-28 18:07:47","title":"Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures","abstract":"This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.","sentences":["This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques.","Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems.","By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches.","The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing."],"url":"http://arxiv.org/abs/2402.18527v1","category":"cs.CV"}
{"created":"2024-02-28 18:06:45","title":"Mental Models of Meeting Goals: Supporting Intentionality in Meeting Technologies","abstract":"Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies. To design for intentionality, we need to understand workers' attitudes and practices around goals. We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves. We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees. We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings. Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings.","sentences":["Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies.","To design for intentionality, we need to understand workers' attitudes and practices around goals.","We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves.","We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees.","We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings.","Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings."],"url":"http://arxiv.org/abs/2402.18526v1","category":"cs.HC"}
{"created":"2024-02-28 18:01:16","title":"On properties of effective topological complexity and effective Lusternik-Schnirelmann category","abstract":"The notion of \\textit{effective topological complexity}, introduced by B\\l{}aszczyk and Kaluba \\cite{BlKa2}, deals with using group actions in the configuration space in order to reduce the complexity of the motion planning algorithm. In this article we focus on studying several properties of such notion of topological complexity. We introduce a notion of effective LS-category which mimics the behaviour the usual LS-cat has in the non-effective setting. We use it to investigate the relationship between these effective invariants and the orbit map with respect of the group action, and we give numerous examples. Additionally, we investigate non-vanishing criteria based on a cohomological dimension bound of the saturated diagonal.","sentences":["The notion of \\textit{effective topological complexity}, introduced by B\\l{}aszczyk and Kaluba \\cite{BlKa2}, deals with using group actions in the configuration space in order to reduce the complexity of the motion planning algorithm.","In this article we focus on studying several properties of such notion of topological complexity.","We introduce a notion of effective LS-category which mimics the behaviour the usual LS-cat has in the non-effective setting.","We use it to investigate the relationship between these effective invariants and the orbit map with respect of the group action, and we give numerous examples.","Additionally, we investigate non-vanishing criteria based on a cohomological dimension bound of the saturated diagonal."],"url":"http://arxiv.org/abs/2402.18524v1","category":"math.AT"}
{"created":"2024-02-28 17:56:10","title":"Rigorously proven chaos in chemical kinetics","abstract":"This study addresses a longstanding question regarding the mathematical proof of chaotic behavior in kinetic differential equations. Following the numerous numerical and experimental results in the past 50 years, we introduce two formal chemical reactions that rigorously demonstrate this behavior. Our approach involves transforming chaotic equations into kinetic differential equations and subsequently realizing these equations through formal chemical reactions. The findings present a novel perspective on chaotic dynamics within chemical kinetics, thereby resolving a longstanding open problem.","sentences":["This study addresses a longstanding question regarding the mathematical proof of chaotic behavior in kinetic differential equations.","Following the numerous numerical and experimental results in the past 50 years, we introduce two formal chemical reactions that rigorously demonstrate this behavior.","Our approach involves transforming chaotic equations into kinetic differential equations and subsequently realizing these equations through formal chemical reactions.","The findings present a novel perspective on chaotic dynamics within chemical kinetics, thereby resolving a longstanding open problem."],"url":"http://arxiv.org/abs/2402.18523v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 17:52:23","title":"Nuclear magnetic resonance studies in a model transverse field Ising system","abstract":"The suppression of ferroquadrupolar order in TmVO$_4$ in a magnetic field is well-described by the transverse field Ising model, enabling detailed studies of critical dynamics near the quantum phase transition. We describe nuclear magnetic resonance measurements in pure and Y-doped single crystals. The non-Kramers nature of the ground state doublet leads to a unique form of the hyperfine coupling that exclusively probes the transverse field susceptibility. Our results show that this quantity diverges at the critical field, in contrast to the mean-field prediction. Furthermore, we find evidence for quantum critical fluctuations present near Tm-rich regions in Y-doped crystals at levels beyond which long-range order is suppressed, suggesting the presence of quantum Griffiths phases.","sentences":["The suppression of ferroquadrupolar order in TmVO$_4$ in a magnetic field is well-described by the transverse field Ising model, enabling detailed studies of critical dynamics near the quantum phase transition.","We describe nuclear magnetic resonance measurements in pure and Y-doped single crystals.","The non-Kramers nature of the ground state doublet leads to a unique form of the hyperfine coupling that exclusively probes the transverse field susceptibility.","Our results show that this quantity diverges at the critical field, in contrast to the mean-field prediction.","Furthermore, we find evidence for quantum critical fluctuations present near Tm-rich regions in Y-doped crystals at levels beyond which long-range order is suppressed, suggesting the presence of quantum Griffiths phases."],"url":"http://arxiv.org/abs/2402.18519v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 17:47:27","title":"Model Predictive Control with adaptive resilience for Denial-of-Service Attacks mitigation on a Regulated Dam","abstract":"In recent years, SCADA (Supervisory Control and Data Acquisition) systems have increasingly become the target of cyber attacks. SCADAs are no longer isolated, as web-based applications expose strategic infrastructures to the outside world connection. In a cyber-warfare context, we propose a Model Predictive Control (MPC) architecture with adaptive resilience, capable of guaranteeing control performance in normal operating conditions and driving towards resilience against DoS (controller-actuator) attacks when needed. Since the attackers' goal is typically to maximize the system damage, we assume they solve an adversarial optimal control problem. An adaptive resilience factor is then designed as a function of the intensity function of a Hawkes process, a point process model estimating the occurrence of random events in time, trained on a moving window to estimate the return time of the next attack. We demonstrate the resulting MPC strategy's effectiveness in 2 attack scenarios on a real system with actual data, the regulated Olginate dam of Lake Como.","sentences":["In recent years, SCADA (Supervisory Control and Data Acquisition) systems have increasingly become the target of cyber attacks.","SCADAs are no longer isolated, as web-based applications expose strategic infrastructures to the outside world connection.","In a cyber-warfare context, we propose a Model Predictive Control (MPC) architecture with adaptive resilience, capable of guaranteeing control performance in normal operating conditions and driving towards resilience against DoS (controller-actuator) attacks when needed.","Since the attackers' goal is typically to maximize the system damage, we assume they solve an adversarial optimal control problem.","An adaptive resilience factor is then designed as a function of the intensity function of a Hawkes process, a point process model estimating the occurrence of random events in time, trained on a moving window to estimate the return time of the next attack.","We demonstrate the resulting MPC strategy's effectiveness in 2 attack scenarios on a real system with actual data, the regulated Olginate dam of Lake Como."],"url":"http://arxiv.org/abs/2402.18516v1","category":"eess.SY"}
{"created":"2024-02-28 17:34:41","title":"Optimality conditions for sparse optimal control of viscous Cahn-Hilliard systems with logarithmic potential","abstract":"In this paper we study the optimal control of a parabolic initial-boundary value problem of viscous Cahn-Hilliard type with zero Neumann boundary conditions. Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter. It is assumed that the nonlinear functions driving the physical processes within the spatial domain are double-well potentials of logarithmic type whose derivatives become singular at the boundary of their respective domains of definition. For such systems, optimal control problems have been studied in the past. We focus here on the situation when the cost functional of the optimal control problem contains a nondifferentiable term like the L1-norm, which leads to sparsity of optimal controls. For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls. In the approach to second-order sufficient conditions, the main novelty of this paper, we adapt a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper [SIAM J. Control Optim. 53 (2015), 2168-2202]. In this paper, we show that this method can also be successfully applied to systems of viscous Cahn-Hilliard type with logarithmic nonlinearity. Since the Cahn-Hilliard system corresponds to a fourth-order partial differential equation in contrast to the second-order systems investigated before, additional technical difficulties have to be overcome.","sentences":["In this paper we study the optimal control of a parabolic initial-boundary value problem of viscous Cahn-Hilliard type with zero Neumann boundary conditions.","Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter.","It is assumed that the nonlinear functions driving the physical processes within the spatial domain are double-well potentials of logarithmic type whose derivatives become singular at the boundary of their respective domains of definition.","For such systems, optimal control problems have been studied in the past.","We focus here on the situation when the cost functional of the optimal control problem contains a nondifferentiable term like the L1-norm, which leads to sparsity of optimal controls.","For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls.","In the approach to second-order sufficient conditions, the main novelty of this paper, we adapt a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper","[SIAM J. Control Optim.","53 (2015), 2168-2202].","In this paper, we show that this method can also be successfully applied to systems of viscous Cahn-Hilliard type with logarithmic nonlinearity.","Since the Cahn-Hilliard system corresponds to a fourth-order partial differential equation in contrast to the second-order systems investigated before, additional technical difficulties have to be overcome."],"url":"http://arxiv.org/abs/2402.18506v1","category":"math.OC"}
{"created":"2024-02-28 17:28:01","title":"Conditional Independence of 1D Gibbs States with Applications to Efficient Learning","abstract":"We show that spin chains in thermal equilibrium have a correlation structure in which individual regions are strongly correlated at most with their near vicinity. We quantify this with alternative notions of the conditional mutual information defined through the so-called Belavkin-Staszewski relative entropy. We prove that these measures decay super-exponentially, under the assumption that the spin chain Hamiltonian is translation-invariant. Using a recovery map associated with these measures, we sequentially construct tensor network approximations in terms of marginals of small (sub-logarithmic) size. As a main application, we show that classical representations of the states can be learned efficiently from local measurements with a polynomial sample complexity. We also prove an approximate factorization condition for the purity of the entire Gibbs state, which implies that it can be efficiently estimated to a small multiplicative error from a small number of local measurements. As a technical step of independent interest, we show an upper bound to the decay of the Belavkin-Staszewski relative entropy upon the application of a conditional expectation.","sentences":["We show that spin chains in thermal equilibrium have a correlation structure in which individual regions are strongly correlated at most with their near vicinity.","We quantify this with alternative notions of the conditional mutual information defined through the so-called Belavkin-Staszewski relative entropy.","We prove that these measures decay super-exponentially, under the assumption that the spin chain Hamiltonian is translation-invariant.","Using a recovery map associated with these measures, we sequentially construct tensor network approximations in terms of marginals of small (sub-logarithmic) size.","As a main application, we show that classical representations of the states can be learned efficiently from local measurements with a polynomial sample complexity.","We also prove an approximate factorization condition for the purity of the entire Gibbs state, which implies that it can be efficiently estimated to a small multiplicative error from a small number of local measurements.","As a technical step of independent interest, we show an upper bound to the decay of the Belavkin-Staszewski relative entropy upon the application of a conditional expectation."],"url":"http://arxiv.org/abs/2402.18500v1","category":"quant-ph"}
{"created":"2024-02-28 17:25:06","title":"ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning","abstract":"Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data.To this end, we propose a unified framework named ROG$_{PL}$ to achieve robust open-set learning on complex noisy graph data, by introducing prototype learning. In specific, ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions. The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise. The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem.The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss. To the best of our knowledge, the proposed ROG$_{PL}$ is the first robust open-set node classification method for graph data with complex noise.","sentences":["Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns.","Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise.","OOD data are samples that do not belong to any known classes.","They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing.","IND noise are training samples which are assigned incorrect labels.","The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem.","Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data.","To this end, we propose a unified framework named ROG$_{PL}$ to achieve robust open-set learning on complex noisy graph data, by introducing prototype learning.","In specific, ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions.","The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise.","The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem.","The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss.","To the best of our knowledge, the proposed ROG$_{PL}$ is the first robust open-set node classification method for graph data with complex noise."],"url":"http://arxiv.org/abs/2402.18495v1","category":"cs.LG"}
{"created":"2024-02-28 17:19:47","title":"The Electrochemical Transistor: a device based on the Electrochemical control of a polymer Polaronic state. The PCPDT-BT as a case study","abstract":"This work presents an original concept directed to implement an unconventional methodology where a device is produced by integrating a solid-state circuitry concept and an electrochemistry cell. In our experimental system an organic semiconductor, (PCPDT-BT), serves both as the gate and working electrode. Gating is obtained via electrochemical polarization exploiting a conventional three electrodes electrochemical cell placed on top of a traditional source/drain/gate solid-state device configuration. Source/drain conduction is probed via impedance measurement (electrochemical impedance spectroscopy, EIS) performed as a function of time at constant frequency, under constant potential control. The conductivity of the PCPDT-BT is due to the polaronic state induced via application of a suitable electrochemical potential (in the oxidation regime), as it is proved by infrared (IR) spectra recorded in-situ/in-operando (in attenuated under total reflection, ATR, mode) upon both electrochemical and chemical ionization/doping of the PCPDT-BT.","sentences":["This work presents an original concept directed to implement an unconventional methodology where a device is produced by integrating a solid-state circuitry concept and an electrochemistry cell.","In our experimental system an organic semiconductor, (PCPDT-BT), serves both as the gate and working electrode.","Gating is obtained via electrochemical polarization exploiting a conventional three electrodes electrochemical cell placed on top of a traditional source/drain/gate solid-state device configuration.","Source/drain conduction is probed via impedance measurement (electrochemical impedance spectroscopy, EIS) performed as a function of time at constant frequency, under constant potential control.","The conductivity of the PCPDT-BT is due to the polaronic state induced via application of a suitable electrochemical potential (in the oxidation regime), as it is proved by infrared (IR) spectra recorded in-situ/in-operando (in attenuated under total reflection, ATR, mode) upon both electrochemical and chemical ionization/doping of the PCPDT-BT."],"url":"http://arxiv.org/abs/2402.18492v1","category":"physics.app-ph"}
{"created":"2024-02-28 17:06:19","title":"A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics","abstract":"Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades. As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes. However, coarse models suffer from inherent bias due to the ignored \"sub-grid\" scales. We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators. Previous efforts have attempted to train such operators using loss functions that match statistics. However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged. Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data. The key obstacle is the chaotic nature of the underlying dynamics. To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model simulation nudged towards that reference. The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM). For the former, our method enables the quantification of events that have return period two orders longer than the training data. For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases.","sentences":["Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades.","As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes.","However, coarse models suffer from inherent bias due to the ignored \"sub-grid\" scales.","We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators.","Previous efforts have attempted to train such operators using loss functions that match statistics.","However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged.","Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data.","The key obstacle is the chaotic nature of the underlying dynamics.","To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model simulation nudged towards that reference.","The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM).","For the former, our method enables the quantification of events that have return period two orders longer than the training data.","For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases."],"url":"http://arxiv.org/abs/2402.18484v1","category":"physics.ao-ph"}
{"created":"2024-02-28 16:44:26","title":"A Higher-Order Lens for Social Systems","abstract":"Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional graphs. This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM). These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor. We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles.   To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics. First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily. Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between simulations and theoretical predictions can be explained by considering higher-order joint degree distributions. Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes.   This work pioneers the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains.","sentences":["Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional graphs.","This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM).","These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor.","We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles.   ","To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics.","First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily.","Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between simulations and theoretical predictions can be explained by considering higher-order joint degree distributions.","Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes.   ","This work pioneers the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains."],"url":"http://arxiv.org/abs/2402.18470v1","category":"cs.SI"}
{"created":"2024-02-28 16:44:01","title":"Controllability for a non-local formulation of surface gravity waves","abstract":"In this paper, we study the approximate controllability of a system governed by an evolution problem known as the sloshing problem. This problem involves a spatial, nonlocal differential operator inherent in the dynamics of a two-dimensional, incompressible, non-viscous fluid within a confined domain. Our work establishes unique continuation results that enable the application of source control localized in an interior domain, allowing the aforementioned controllability.","sentences":["In this paper, we study the approximate controllability of a system governed by an evolution problem known as the sloshing problem.","This problem involves a spatial, nonlocal differential operator inherent in the dynamics of a two-dimensional, incompressible, non-viscous fluid within a confined domain.","Our work establishes unique continuation results that enable the application of source control localized in an interior domain, allowing the aforementioned controllability."],"url":"http://arxiv.org/abs/2402.18468v1","category":"math.AP"}
{"created":"2024-02-28 16:43:23","title":"Unraveling the complexity of the Dzyaloshinskii-Moriya interaction in layered magnets: Towards its full magnitude and chirality control","abstract":"Chirality is an inherent characteristics of some objects in nature. In magnetism chiral magnetic textures can be formed in systems with broken inversion symmetry and due to an antisymmetric magnetic interaction, known as Dzyaloshinskii--Moriya interaction (DMI). Here, aiming on a fundamental understanding of this chiral interaction on the atomic scale, we design several synthetic layered structures composed of alternating atomic layers of 3d ferromagnetic metals epitaxially grown on Ir(001). We demonstrate both experimentally and theoretically that the atomistic DMI depends critically not only on the orbital occupancy of the interface magnetic layer but also on the sequence of the atomic layers. The effect is attributed to the complexity of the electronic structure and the contribution of different orbitals to the hybridization and DMI. We anticipate that our results provide guidelines for controlling both the chirality and the magnitude of the atomistic DMI.","sentences":["Chirality is an inherent characteristics of some objects in nature.","In magnetism chiral magnetic textures can be formed in systems with broken inversion symmetry and due to an antisymmetric magnetic interaction, known as Dzyaloshinskii--Moriya interaction (DMI).","Here, aiming on a fundamental understanding of this chiral interaction on the atomic scale, we design several synthetic layered structures composed of alternating atomic layers of 3d ferromagnetic metals epitaxially grown on Ir(001).","We demonstrate both experimentally and theoretically that the atomistic DMI depends critically not only on the orbital occupancy of the interface magnetic layer but also on the sequence of the atomic layers.","The effect is attributed to the complexity of the electronic structure and the contribution of different orbitals to the hybridization and DMI.","We anticipate that our results provide guidelines for controlling both the chirality and the magnitude of the atomistic DMI."],"url":"http://arxiv.org/abs/2402.18466v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 16:41:52","title":"Semantic Information in MC: Chemotaxis Beyond Shannon","abstract":"The recently emerging molecular communication (MC) paradigm intents to leverage communication engineering tools for the design of synthetic chemical communication systems. These systems are envisioned to operate on nanoscale and in biological environments, such as the human body, and catalyze the emergence of revolutionary applications in the context of early disease monitoring and drug targeting. However, while a plethora of theoretical (and more recently also more and more practical) MC system designs have been proposed over the past years, some fundamental questions remain open, hindering the breakthrough of MC in real-world applications. One of these questions is: What is a useful measure of information in the context of MC-based applications? While most existing works in MC build upon the concept of syntactic information as introduced by Shannon, in this paper, we explore the framework of semantic information as introduced by Kolchinsky and Wolpert for the information theoretical analysis of a natural MC system, namely bacterial chemotaxis. Exploiting the computational modeling tool of agent-based modeling (ABM), we are able to demonstrate that the semantic information framework can provide a useful information theoretical framework for quantifying the information exchange of chemotactic bacteria with their environment. In particular, we show that the measured semantic information provides a useful measure of the ability of the bacteria to adapt to and survive in a changing environment. Encouraged by our results, we envision that the semantic information framework can open new avenues for developing theoretical and practical MC system designs and in this way help to unleash the full potential of MC for complex adaptive systems-based nanoscale applications.","sentences":["The recently emerging molecular communication (MC) paradigm intents to leverage communication engineering tools for the design of synthetic chemical communication systems.","These systems are envisioned to operate on nanoscale and in biological environments, such as the human body, and catalyze the emergence of revolutionary applications in the context of early disease monitoring and drug targeting.","However, while a plethora of theoretical (and more recently also more and more practical) MC system designs have been proposed over the past years, some fundamental questions remain open, hindering the breakthrough of MC in real-world applications.","One of these questions is: What is a useful measure of information in the context of MC-based applications?","While most existing works in MC build upon the concept of syntactic information as introduced by Shannon, in this paper, we explore the framework of semantic information as introduced by Kolchinsky and Wolpert for the information theoretical analysis of a natural MC system, namely bacterial chemotaxis.","Exploiting the computational modeling tool of agent-based modeling (ABM), we are able to demonstrate that the semantic information framework can provide a useful information theoretical framework for quantifying the information exchange of chemotactic bacteria with their environment.","In particular, we show that the measured semantic information provides a useful measure of the ability of the bacteria to adapt to and survive in a changing environment.","Encouraged by our results, we envision that the semantic information framework can open new avenues for developing theoretical and practical MC system designs and in this way help to unleash the full potential of MC for complex adaptive systems-based nanoscale applications."],"url":"http://arxiv.org/abs/2402.18465v1","category":"cs.IT"}
{"created":"2024-02-28 16:27:22","title":"A gallery of maximum-entropy distributions: 14 and 21 moments","abstract":"This work explores the different shapes that can be realized by the one-particle velocity distribution functions (VDFs) associated with the fourth-order maximum-entropy moment method. These distributions take the form of an exponential of a polynomial of the particle velocity, with terms up to the fourth-order. The 14- and 21-moment approximations are investigated. Various non-equilibrium gas states are probed throughout moment space. The resulting maximum-entropy distributions deviate strongly from the equilibrium VDF, and show a number of lobes and branches. The Maxwellian and the anisotropic Gaussian distributions are recovered as special cases. The eigenvalues associated with the maximum-entropy system of transport equations are also illustrated for some selected gas states. Anisotropic and/or asymmetric non-equilibrium states are seen to be associated with a non-uniform spacial propagation of perturbations.","sentences":["This work explores the different shapes that can be realized by the one-particle velocity distribution functions (VDFs) associated with the fourth-order maximum-entropy moment method.","These distributions take the form of an exponential of a polynomial of the particle velocity, with terms up to the fourth-order.","The 14- and 21-moment approximations are investigated.","Various non-equilibrium gas states are probed throughout moment space.","The resulting maximum-entropy distributions deviate strongly from the equilibrium VDF, and show a number of lobes and branches.","The Maxwellian and the anisotropic Gaussian distributions are recovered as special cases.","The eigenvalues associated with the maximum-entropy system of transport equations are also illustrated for some selected gas states.","Anisotropic and/or asymmetric non-equilibrium states are seen to be associated with a non-uniform spacial propagation of perturbations."],"url":"http://arxiv.org/abs/2402.18453v1","category":"math-ph"}
{"created":"2024-02-28 16:02:10","title":"Magnetization fluctuations and magnetic aftereffect probed via the anomalous Hall effect","abstract":"Taking advantage of the anomalous Hall effect, we electrically probe low-frequency magnetization fluctuations at room temperature in a thin ferromagnetic Pt/Co/AlO$_x$ layer stack with perpendicular magnetic anisotropy. We observe a strong enhancement of the Hall voltage fluctuations within the hysteretic region of the magnetization loop. Analyzing both the temporal evolution of the anomalous Hall voltage and its frequency-dependent noise power density, we identify two types of magnetic noise: abrupt changes in the magnetic domain configuration, evident as Barkhausen-like steps in the Hall voltage time trace, yield a noise power density spectrum scaling with frequency as $1/f^{\\beta}$ with $\\beta\\approx 1.9$. In contrast, quasi-stationary magnetization configurations are connected with a magnetic noise power density with an exponent $\\beta\\approx 0.9$. The observation of Barkausen steps and relaxation effects shows that the magnetic system is in a non-stationary state in the hysteresis region, such that the fluctuation-dissipation theorem cannot be expected to hold. However, the time-dependent change in the Hall voltage for constant magnetic field strength resembles the integrated noise power.","sentences":["Taking advantage of the anomalous Hall effect, we electrically probe low-frequency magnetization fluctuations at room temperature in a thin ferromagnetic Pt/Co/AlO$_x$ layer stack with perpendicular magnetic anisotropy.","We observe a strong enhancement of the Hall voltage fluctuations within the hysteretic region of the magnetization loop.","Analyzing both the temporal evolution of the anomalous Hall voltage and its frequency-dependent noise power density, we identify two types of magnetic noise: abrupt changes in the magnetic domain configuration, evident as Barkhausen-like steps in the Hall voltage time trace, yield a noise power density spectrum scaling with frequency as $1/f^{\\beta}$ with $\\beta\\approx 1.9$.","In contrast, quasi-stationary magnetization configurations are connected with a magnetic noise power density with an exponent $\\beta\\approx 0.9$. The observation of Barkausen steps and relaxation effects shows that the magnetic system is in a non-stationary state in the hysteresis region, such that the fluctuation-dissipation theorem cannot be expected to hold.","However, the time-dependent change in the Hall voltage for constant magnetic field strength resembles the integrated noise power."],"url":"http://arxiv.org/abs/2402.18436v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 15:46:25","title":"Predicting Phase Transitions in PbTiO$_3$ using Zentropy","abstract":"According to conventional X-ray measurements, PbTiO$_3$ undergoes a phase transition from a tetragonal ferroelectric phase to a cubic paraelectric phase at 763 K. However, x-ray absorption fine-structure (XAFS) measurements indicate that PbTiO$_3$ is tetragonal even after the phase transition has occurred. The difference in these results concerns the length scales accessible to each measurement technique: millimeters for X-ray and Angstroms for XAFS. For both of these measurements to be consistent, PbTiO$_3$ is macroscopically cubic but still locally tetragonal above the phase transition temperature. Despite this, most models, such as the Laundau-Ginsburg-Devonshire theory, are still unable to explain this phenomenon. Moreover, these methods involve model parameters fitted to experimental or theoretical data and do not consider other tetragonal configurations, such as domain walls, to predict the phase transition. In this study, we use our novel zentropy approach to predict the phase transition, which allows us to calculate the total entropy of a system without fitted parameters, taking only inputs from density functional theory calculations through energy-volume curves and phonon calculations. Our approach also considers the tetragonal 90{\\deg} and 180{\\deg} domain walls and the ferroelectric ground state in predicting the phase transition. The predicted phase transition using the metaGGA $r^2\\text{SCAN}$ occurs at 830 K, showing good agreement with the experimental value of 763 K. It is the subject of future work to show that the statistical average of these tetragonal configurations at the phase transition will result in the cubic phase.","sentences":["According to conventional X-ray measurements, PbTiO$_3$ undergoes a phase transition from a tetragonal ferroelectric phase to a cubic paraelectric phase at 763 K. However, x-ray absorption fine-structure (XAFS) measurements indicate that PbTiO$_3$ is tetragonal even after the phase transition has occurred.","The difference in these results concerns the length scales accessible to each measurement technique: millimeters for X-ray and Angstroms for XAFS.","For both of these measurements to be consistent, PbTiO$_3$ is macroscopically cubic but still locally tetragonal above the phase transition temperature.","Despite this, most models, such as the Laundau-Ginsburg-Devonshire theory, are still unable to explain this phenomenon.","Moreover, these methods involve model parameters fitted to experimental or theoretical data and do not consider other tetragonal configurations, such as domain walls, to predict the phase transition.","In this study, we use our novel zentropy approach to predict the phase transition, which allows us to calculate the total entropy of a system without fitted parameters, taking only inputs from density functional theory calculations through energy-volume curves and phonon calculations.","Our approach also considers the tetragonal 90{\\deg} and 180{\\deg} domain walls and the ferroelectric ground state in predicting the phase transition.","The predicted phase transition using the metaGGA $r^2\\text{SCAN}$ occurs at 830 K, showing good agreement with the experimental value of 763 K. It is the subject of future work to show that the statistical average of these tetragonal configurations at the phase transition will result in the cubic phase."],"url":"http://arxiv.org/abs/2402.18425v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 15:32:59","title":"ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization","abstract":"This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control. The main contribution lies in demonstrating how this approach simplifies the black-box optimization problem while ensuring \"always-feasible\" setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints. Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to \"some\" setpoint using simple feedback controllers. Bayesian optimization then computes the optimum setpoint for the constraint controllers. By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme. In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the Gaussian process model used in Bayesian optimization. The effectiveness of the proposed approach is demonstrated on a benchmark Williams-Otto reactor example.","sentences":["This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control.","The main contribution lies in demonstrating how this approach simplifies the black-box optimization problem while ensuring \"always-feasible\" setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints.","Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to \"some\" setpoint using simple feedback controllers.","Bayesian optimization then computes the optimum setpoint for the constraint controllers.","By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme.","In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the Gaussian process model used in Bayesian optimization.","The effectiveness of the proposed approach is demonstrated on a benchmark Williams-Otto reactor example."],"url":"http://arxiv.org/abs/2402.18415v1","category":"math.OC"}
{"created":"2024-02-28 15:32:44","title":"Physics-based block preconditioning for mixed-dimensional beam-solid interaction","abstract":"This paper presents a scalable physics-based block preconditioner for mixed-dimensional models in beam-solid interaction and their application in engineering. In particular, it studies the linear systems arising from a regularized mortar-type approach for embedding geometrically exact beams into solid continua. Due to the lack of block diagonal dominance of the arising 2 x 2 block system, an approximate block factorization preconditioner is used. It exploits the sparsity structure of the beam sub-block to construct a sparse approximate inverse, which is then not only used to explicitly form an approximation of the Schur complement, but also acts as a smoother within the prediction step of the arising SIMPLE-type preconditioner. The correction step utilizes an algebraic multigrid method. Although, for now, the beam sub-block is tackled by a one-level method only, the multi-level nature of the computationally demanding correction step delivers a scalable preconditioner in practice. In numerical test cases, the influence of different algorithmic parameters on the quality of the sparse approximate inverse is studied and the weak scaling behavior of the proposed preconditioner on up to 1000 MPI ranks is demonstrated, before the proposed preconditioner is finally applied for the analysis of steel-reinforced concrete structures in civil engineering.","sentences":["This paper presents a scalable physics-based block preconditioner for mixed-dimensional models in beam-solid interaction and their application in engineering.","In particular, it studies the linear systems arising from a regularized mortar-type approach for embedding geometrically exact beams into solid continua.","Due to the lack of block diagonal dominance of the arising 2 x 2 block system, an approximate block factorization preconditioner is used.","It exploits the sparsity structure of the beam sub-block to construct a sparse approximate inverse, which is then not only used to explicitly form an approximation of the Schur complement, but also acts as a smoother within the prediction step of the arising SIMPLE-type preconditioner.","The correction step utilizes an algebraic multigrid method.","Although, for now, the beam sub-block is tackled by a one-level method only, the multi-level nature of the computationally demanding correction step delivers a scalable preconditioner in practice.","In numerical test cases, the influence of different algorithmic parameters on the quality of the sparse approximate inverse is studied and the weak scaling behavior of the proposed preconditioner on up to 1000 MPI ranks is demonstrated, before the proposed preconditioner is finally applied for the analysis of steel-reinforced concrete structures in civil engineering."],"url":"http://arxiv.org/abs/2402.18414v1","category":"cs.CE"}
{"created":"2024-02-28 15:28:15","title":"Multimode Interferometers: an Analytical Method for Determining the Accumulated Phase Difference Between the Fundamental Mode and One Arbitrary High-Order Mode","abstract":"In multimode interferometers, the interaction between several modes brings a high level of complexity to the interpretation of its light patterns. With recent advances, it is possible to selectively excite only a couple of modes inside the device. This paper presents an analytical method for determining the phase difference between two propagating modes in the multimode interferometer, in an effort to simplify the usage and expand the range of applications for this type of optical devices.","sentences":["In multimode interferometers, the interaction between several modes brings a high level of complexity to the interpretation of its light patterns.","With recent advances, it is possible to selectively excite only a couple of modes inside the device.","This paper presents an analytical method for determining the phase difference between two propagating modes in the multimode interferometer, in an effort to simplify the usage and expand the range of applications for this type of optical devices."],"url":"http://arxiv.org/abs/2402.18408v1","category":"physics.optics"}
{"created":"2024-02-28 15:27:20","title":"Multi-cell Coordinated Joint Sensing and Communications","abstract":"This paper proposes block-level precoder (BLP) designs for a multi-input single-output (MISO) system that performs joint sensing and communication across multiple cells and users. The Cramer-Rao-Bound for estimating a target's azimuth angle is determined for coordinated beamforming (CBF) and coordinated multi-point (CoMP) scenarios while considering inter-cell communication and sensing links. The formulated optimization problems to minimize the CRB and maximize the minimum-signal-to-interference-plus-noise-ratio (SINR) are non-convex and are represented in the semidefinite relaxed (SDR) form to solve using an alternate optimization algorithm. The proposed solutions show improved performance compared to the baseline scenario that neglects the signal component from neighboring cells.","sentences":["This paper proposes block-level precoder (BLP) designs for a multi-input single-output (MISO) system that performs joint sensing and communication across multiple cells and users.","The Cramer-Rao-Bound for estimating a target's azimuth angle is determined for coordinated beamforming (CBF) and coordinated multi-point (CoMP) scenarios while considering inter-cell communication and sensing links.","The formulated optimization problems to minimize the CRB and maximize the minimum-signal-to-interference-plus-noise-ratio (SINR) are non-convex and are represented in the semidefinite relaxed (SDR) form to solve using an alternate optimization algorithm.","The proposed solutions show improved performance compared to the baseline scenario that neglects the signal component from neighboring cells."],"url":"http://arxiv.org/abs/2402.18405v1","category":"cs.IT"}
{"created":"2024-02-28 15:25:23","title":"Preconditioned iterative solvers for constrained high-order implicit shock tracking methods","abstract":"High-order implicit shock tracking (fitting) is a class of high-order numerical methods that use numerical optimization to simultaneously compute a high-order approximation to a conservation law solution and align elements of the computational mesh with non-smooth features. This alignment ensures that non-smooth features are perfectly represented by inter-element jumps and high-order basis functions approximate smooth regions of the solution without nonlinear stabilization, which leads to accurate approximations on traditionally coarse meshes. In this work, we devise a family of preconditioners for the saddle point linear system that defines the step toward optimality at each iteration of the optimization solver so Krylov solvers can be effectively used. Our preconditioners integrate standard preconditioners from constrained optimization with popular preconditioners for discontinuous Galerkin discretizations such as block Jacobi, block incomplete LU factorizations with minimum discarded fill reordering, and p-multigrid. Thorough studies are performed using two inviscid compressible flow problems to evaluate the effectivity of each preconditioner in this family and their sensitivity to critical shock tracking parameters such as the mesh and Hessian regularization, linearization state, and resolution of the solution space.","sentences":["High-order implicit shock tracking (fitting) is a class of high-order numerical methods that use numerical optimization to simultaneously compute a high-order approximation to a conservation law solution and align elements of the computational mesh with non-smooth features.","This alignment ensures that non-smooth features are perfectly represented by inter-element jumps and high-order basis functions approximate smooth regions of the solution without nonlinear stabilization, which leads to accurate approximations on traditionally coarse meshes.","In this work, we devise a family of preconditioners for the saddle point linear system that defines the step toward optimality at each iteration of the optimization solver so Krylov solvers can be effectively used.","Our preconditioners integrate standard preconditioners from constrained optimization with popular preconditioners for discontinuous Galerkin discretizations such as block Jacobi, block incomplete LU factorizations with minimum discarded fill reordering, and p-multigrid.","Thorough studies are performed using two inviscid compressible flow problems to evaluate the effectivity of each preconditioner in this family and their sensitivity to critical shock tracking parameters such as the mesh and Hessian regularization, linearization state, and resolution of the solution space."],"url":"http://arxiv.org/abs/2402.18403v1","category":"math.NA"}
{"created":"2024-02-28 15:24:58","title":"A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation","abstract":"In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.","sentences":["In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation.","In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies.","All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same).","In this paper, we propose SyMPIE to solve these shortcomings.","To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost.","Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples.","Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels.","We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board.","The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication."],"url":"http://arxiv.org/abs/2402.18402v1","category":"cs.CV"}
{"created":"2024-02-28 15:24:43","title":"DevPhish: Exploring Social Engineering in Software Supply Chain Attacks on Developers","abstract":"The Software Supply Chain (SSC) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations. There is evidence indicating that adversaries utilize Social Engineering (SocE) techniques specifically aimed at software developers. That is, they interact with developers at critical steps in the Software Development Life Cycle (SDLC), such as accessing Github repositories, incorporating code dependencies, and obtaining approval for Pull Requests (PR) to introduce malicious code. This paper aims to comprehensively explore the existing and emerging SocE tactics employed by adversaries to trick Software Engineers (SWEs) into delivering malicious software. By analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the SSC. Such insights prove highly beneficial for threat modeling and security gap analysis.","sentences":["The Software Supply Chain (SSC) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations.","There is evidence indicating that adversaries utilize Social Engineering (SocE) techniques specifically aimed at software developers.","That is, they interact with developers at critical steps in the Software Development Life Cycle (SDLC), such as accessing Github repositories, incorporating code dependencies, and obtaining approval for Pull Requests (PR) to introduce malicious code.","This paper aims to comprehensively explore the existing and emerging SocE tactics employed by adversaries to trick Software Engineers (SWEs) into delivering malicious software.","By analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the SSC.","Such insights prove highly beneficial for threat modeling and security gap analysis."],"url":"http://arxiv.org/abs/2402.18401v1","category":"cs.SE"}
{"created":"2024-02-28 15:17:41","title":"Hamiltonian simulation for time-evolving partial differential equation by scalable quantum circuits","abstract":"Solving partial differential equations for extremely large-scale systems within a feasible computation time serves in accelerating engineering developments. Quantum computing algorithm, particularly the Hamiltonian simulation, is a potential and promising approach to achieve this purpose. Actually there are several proposals of Hamiltonian simulation with potential quantum speedup, but their detailed implementation and accordingly the detailed computational complexity are all somewhat unclear. This paper presents a method that enables us to explicitly implement the quantum circuit for Hamiltonian simulation; the key technique is the explicit gate construction of differential operators contained in the target partial differential equation. Moreover, we show that the space and time complexity of the constructed circuit is exponentially smaller than that of all classical algorithms. We also provide numerical experiments and an experiment on a real device for the wave equation to demonstrate the validity of our proposed method.","sentences":["Solving partial differential equations for extremely large-scale systems within a feasible computation time serves in accelerating engineering developments.","Quantum computing algorithm, particularly the Hamiltonian simulation, is a potential and promising approach to achieve this purpose.","Actually there are several proposals of Hamiltonian simulation with potential quantum speedup, but their detailed implementation and accordingly the detailed computational complexity are all somewhat unclear.","This paper presents a method that enables us to explicitly implement the quantum circuit for Hamiltonian simulation; the key technique is the explicit gate construction of differential operators contained in the target partial differential equation.","Moreover, we show that the space and time complexity of the constructed circuit is exponentially smaller than that of all classical algorithms.","We also provide numerical experiments and an experiment on a real device for the wave equation to demonstrate the validity of our proposed method."],"url":"http://arxiv.org/abs/2402.18398v1","category":"quant-ph"}
{"created":"2024-02-28 15:08:58","title":"Discovery of an extended Horizontal Branch in the Large Magellanic Cloud globular cluster NGC1835","abstract":"We present a high angular resolution multi-wavelength study of the massive globular cluster NGC 1835 in the Large Magellanic Cloud. Thanks to a combination of optical and near ultraviolet images acquired with the WFC3 on board the HST, we performed a detailed inspection of the stellar population in this stellar system adopting a ``UV-guided search'' to optimize the detection of relatively hot stars. This allowed us to discover a remarkably extended horizontal branch (HB), spanning more than 4.5 magnitudes in both magnitude and colour from the region redder than the instability strip, up to effective temperatures of 30,000 K, and including a large population of RR Lyrae (67 confirmed variables, and 52 new candidates). This is the first time that such a feature has been detected in an extra-Galactic cluster, demonstrating that the physical conditions responsible for the formation of extended HBs are ubiquitous. The acquired dataset has been also used to redetermine the cluster distance modulus, reddening, and absolute age, yielding $(m-M)_0=18.58$, $E(B-V)=0.08$, and $t=12.5$ Gyr, respectively.","sentences":["We present a high angular resolution multi-wavelength study of the massive globular cluster NGC 1835 in the Large Magellanic Cloud.","Thanks to a combination of optical and near ultraviolet images acquired with the WFC3 on board the HST, we performed a detailed inspection of the stellar population in this stellar system adopting a ``UV-guided search'' to optimize the detection of relatively hot stars.","This allowed us to discover a remarkably extended horizontal branch (HB), spanning more than 4.5 magnitudes in both magnitude and colour from the region redder than the instability strip, up to effective temperatures of 30,000 K, and including a large population of RR Lyrae (67 confirmed variables, and 52 new candidates).","This is the first time that such a feature has been detected in an extra-Galactic cluster, demonstrating that the physical conditions responsible for the formation of extended HBs are ubiquitous.","The acquired dataset has been also used to redetermine the cluster distance modulus, reddening, and absolute age, yielding $(m-M)_0=18.58$, $E(B-V)=0.08$, and $t=12.5$ Gyr, respectively."],"url":"http://arxiv.org/abs/2402.18389v1","category":"astro-ph.GA"}
{"created":"2024-02-28 15:07:36","title":"Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing","abstract":"This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users. We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP). In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing. In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection. We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors. The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex. We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques. Simulations suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system. The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value.","sentences":["This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users.","We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP).","In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing.","In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection.","We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors.","The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex.","We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques.","Simulations suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system.","The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value."],"url":"http://arxiv.org/abs/2402.18387v1","category":"cs.IT"}
{"created":"2024-02-28 15:03:34","title":"Operating semiconductor quantum processors with hopping spins","abstract":"Qubits that can be efficiently controlled are pivotal in the development of scalable quantum hardware. Resonant control is commonly embraced to execute high-fidelity quantum gates but demands integration of high-frequency oscillating signals and results in qubit crosstalk and heating. Establishing quantum control based on discrete signals could therefore result in a paradigm shift. This may be accomplished with single-spin semiconductor qubits, if one can engineer hopping spins between quantum dots with site-dependent spin quantization axis. Here, we introduce hopping-based universal quantum logic and obtain single-qubit gate fidelities of 99.97%, coherent shuttling fidelities of 99.992%, and two-qubit gates fidelities of 99.3%, corresponding to error rates that have been predicted to allow for quantum error correction. We demonstrate that hopping spins also constitute an elegant tuning method by statistically mapping the coherence of a 10-quantum dot system. These results motivate dense quantum dot arrays with sparse occupation for efficient and high-connectivity qubit registers.","sentences":["Qubits that can be efficiently controlled are pivotal in the development of scalable quantum hardware.","Resonant control is commonly embraced to execute high-fidelity quantum gates but demands integration of high-frequency oscillating signals and results in qubit crosstalk and heating.","Establishing quantum control based on discrete signals could therefore result in a paradigm shift.","This may be accomplished with single-spin semiconductor qubits, if one can engineer hopping spins between quantum dots with site-dependent spin quantization axis.","Here, we introduce hopping-based universal quantum logic and obtain single-qubit gate fidelities of 99.97%, coherent shuttling fidelities of 99.992%, and two-qubit gates fidelities of 99.3%, corresponding to error rates that have been predicted to allow for quantum error correction.","We demonstrate that hopping spins also constitute an elegant tuning method by statistically mapping the coherence of a 10-quantum dot system.","These results motivate dense quantum dot arrays with sparse occupation for efficient and high-connectivity qubit registers."],"url":"http://arxiv.org/abs/2402.18382v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 14:44:57","title":"Topological charge and spin Hall effects due to skyrmions in canted antiferromagnets","abstract":"The topological charge Hall effect (TCHE) and the topological spin Hall effect (TSHE), arising from ferromagnetic (FM) and antiferromagnetic (AFM) skyrmions, respectively; can be elucidated through the emergence of spin-dependent Berry gauge fields that affect the adiabatic flow of electrons within the skyrmion texture. TCHE is absent in systems with parity-time (PT) symmetry, such as collinear AFM systems. In this study, we theoretically study TCHE and TSHE in a canted antiferromagnet within the diffusive regime. Spin canting or weak ferromagnetism in canted AFMs that breaks the PT symmetry may arise from strong homogeneous Dzyaloshinskii-Morya interactions. Using a semiclassical Boltzmann approach, we obtain diffusion equations for the spin and charge accumulations in the presence of finite spin-flip and spin-dependent momentum relaxation times. We show that the finite net magnetization, stemming from spin canting and the subsequent breaking of parity-time symmetry, results in the emergence of both finite TCHE and TSHE in AFM systems.","sentences":["The topological charge Hall effect (TCHE) and the topological spin Hall effect (TSHE), arising from ferromagnetic (FM) and antiferromagnetic (AFM) skyrmions, respectively; can be elucidated through the emergence of spin-dependent Berry gauge fields that affect the adiabatic flow of electrons within the skyrmion texture.","TCHE is absent in systems with parity-time (PT) symmetry, such as collinear AFM systems.","In this study, we theoretically study TCHE and TSHE in a canted antiferromagnet within the diffusive regime.","Spin canting or weak ferromagnetism in canted AFMs that breaks the PT symmetry may arise from strong homogeneous Dzyaloshinskii-Morya interactions.","Using a semiclassical Boltzmann approach, we obtain diffusion equations for the spin and charge accumulations in the presence of finite spin-flip and spin-dependent momentum relaxation times.","We show that the finite net magnetization, stemming from spin canting and the subsequent breaking of parity-time symmetry, results in the emergence of both finite TCHE and TSHE in AFM systems."],"url":"http://arxiv.org/abs/2402.18369v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 14:38:04","title":"Estimation of railway vehicle response for track geometry evaluation using branch Fourier neural operator","abstract":"In railway transportation, the evaluation of track geometry is an indispensable requirement to ensure the safety and comfort of railway vehicles. A promising approach is to directly use vehicle dynamic responses to assess the impact of track geometry defects. However, the computational cost of obtaining the dynamic response of the vehicle body using dynamics simulation methods is large. Thus, it is important to obtain the dynamic response of the vehicle-track coupled system efficiently and accurately. In this work, a branch Fourier neural operator (BFNO) model is proposed to obtain the dynamic response of the vehicle-track coupled system. The model takes into account the nonlinear relationship of the vehicle-track coupled system and realizes the fast and accurate estimation of the system dynamic response. The relative loss (rLSE) of BFNO model is 2.04%, which is reduced by 64%, compared with the traditional neural network (CNN-GRU). In the frequency domain, BFNO model achieves the effective estimation of the dynamic response of the system within the primary frequency range. Compared with the existing methods, our proposed model can make predictions at unseen time steps, enabling predictions from low to high time resolutions. Meanwhile, our proposed model is superior to commercial software in terms of efficiency. In the evaluation of track geometry, users can use pre-trained BFNO to obtain the dynamic response with almost no computational cost.","sentences":["In railway transportation, the evaluation of track geometry is an indispensable requirement to ensure the safety and comfort of railway vehicles.","A promising approach is to directly use vehicle dynamic responses to assess the impact of track geometry defects.","However, the computational cost of obtaining the dynamic response of the vehicle body using dynamics simulation methods is large.","Thus, it is important to obtain the dynamic response of the vehicle-track coupled system efficiently and accurately.","In this work, a branch Fourier neural operator (BFNO) model is proposed to obtain the dynamic response of the vehicle-track coupled system.","The model takes into account the nonlinear relationship of the vehicle-track coupled system and realizes the fast and accurate estimation of the system dynamic response.","The relative loss (rLSE) of BFNO model is 2.04%, which is reduced by 64%, compared with the traditional neural network (CNN-GRU).","In the frequency domain, BFNO model achieves the effective estimation of the dynamic response of the system within the primary frequency range.","Compared with the existing methods, our proposed model can make predictions at unseen time steps, enabling predictions from low to high time resolutions.","Meanwhile, our proposed model is superior to commercial software in terms of efficiency.","In the evaluation of track geometry, users can use pre-trained BFNO to obtain the dynamic response with almost no computational cost."],"url":"http://arxiv.org/abs/2402.18366v1","category":"physics.app-ph"}
{"created":"2024-02-28 14:35:52","title":"Token-based Vehicular Security System (TVSS): Scalable, Secure, Low-latency Public Key Infrastructure for Connected Vehicles","abstract":"Connected and Autonomous vehicles stand to drastically improve the safety and efficiency of the transportation system in the near future while also reducing pollution. These systems leverage communication to coordinate among vehicles and infrastructure in service of a number of safety and efficiency driver assist and even fully autonomous applications. Attackers can compromise these systems in a number of ways including by falsifying communication messages, making it critical to support security mechanisms that can operate and scale in dynamic scenarios. Towards this end, we present TVSS, a new VPKI system which improves drastically over prior work in the area (including over SCMS; the US department of transportation standard for VPKI). TVSS leverages the idea of unforgeable tokens to enable rapid verification at the road side units (RSUs), which are part of the road infrastructure at the edge of the network. This edge based solution enables agile authentication by avoiding the need for back-end servers during the potentially short contact time between a moving vehicle and the infrastructure. It also results in several security advantages: (1) Scalable Revocation: it greatly simplifies the revocation problem, a difficult problem in large scale certificate systems; and (2) Faster Refresh: Vehicles interact more frequently with the system to refresh their credentials, improving the privacy of the system. We provide a construction of the system and formally prove its security. Field experiments on a test-bed we develop consisting of on-board units (OBUs) and RSUs shows substantial reduction in the latency of refreshing credentials compared to SCMS, allowing the system to work even with smaller window of connectivity when vehicles are moving at higher speeds. Notably, we are able to execute the bottleneck operation of our scheme with a stationary RSU while traveling at highway speeds .","sentences":["Connected and Autonomous vehicles stand to drastically improve the safety and efficiency of the transportation system in the near future while also reducing pollution.","These systems leverage communication to coordinate among vehicles and infrastructure in service of a number of safety and efficiency driver assist and even fully autonomous applications.","Attackers can compromise these systems in a number of ways including by falsifying communication messages, making it critical to support security mechanisms that can operate and scale in dynamic scenarios.","Towards this end, we present TVSS, a new VPKI system which improves drastically over prior work in the area (including over SCMS; the US department of transportation standard for VPKI).","TVSS leverages the idea of unforgeable tokens to enable rapid verification at the road side units (RSUs), which are part of the road infrastructure at the edge of the network.","This edge based solution enables agile authentication by avoiding the need for back-end servers during the potentially short contact time between a moving vehicle and the infrastructure.","It also results in several security advantages: (1) Scalable Revocation: it greatly simplifies the revocation problem, a difficult problem in large scale certificate systems; and (2) Faster Refresh:","Vehicles interact more frequently with the system to refresh their credentials, improving the privacy of the system.","We provide a construction of the system and formally prove its security.","Field experiments on a test-bed we develop consisting of on-board units (OBUs) and RSUs shows substantial reduction in the latency of refreshing credentials compared to SCMS, allowing the system to work even with smaller window of connectivity when vehicles are moving at higher speeds.","Notably, we are able to execute the bottleneck operation of our scheme with a stationary RSU while traveling at highway speeds ."],"url":"http://arxiv.org/abs/2402.18365v1","category":"cs.CR"}
{"created":"2024-02-28 14:33:09","title":"Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning","abstract":"Diverse studies in systems neuroscience begin with extended periods of training known as 'shaping' procedures. These involve progressively studying component parts of more complex tasks, and can make the difference between learning a task quickly, slowly or not at all. Despite the importance of shaping to the acquisition of complex tasks, there is as yet no theory that can help guide the design of shaping procedures, or more fundamentally, provide insight into its key role in learning. Modern deep reinforcement learning systems might implicitly learn compositional primitives within their multilayer policy networks. Inspired by these models, we propose and analyse a model of deep policy gradient learning of simple compositional reinforcement learning tasks. Using the tools of statistical physics, we solve for exact learning dynamics and characterise different learning strategies including primitives pre-training, in which task primitives are studied individually before learning compositional tasks. We find a complex interplay between task complexity and the efficacy of shaping strategies. Overall, our theory provides an analytical understanding of the benefits of shaping in a class of compositional tasks and a quantitative account of how training protocols can disclose useful task primitives, ultimately yielding faster and more robust learning.","sentences":["Diverse studies in systems neuroscience begin with extended periods of training known as 'shaping' procedures.","These involve progressively studying component parts of more complex tasks, and can make the difference between learning a task quickly, slowly or not at all.","Despite the importance of shaping to the acquisition of complex tasks, there is as yet no theory that can help guide the design of shaping procedures, or more fundamentally, provide insight into its key role in learning.","Modern deep reinforcement learning systems might implicitly learn compositional primitives within their multilayer policy networks.","Inspired by these models, we propose and analyse a model of deep policy gradient learning of simple compositional reinforcement learning tasks.","Using the tools of statistical physics, we solve for exact learning dynamics and characterise different learning strategies including primitives pre-training, in which task primitives are studied individually before learning compositional tasks.","We find a complex interplay between task complexity and the efficacy of shaping strategies.","Overall, our theory provides an analytical understanding of the benefits of shaping in a class of compositional tasks and a quantitative account of how training protocols can disclose useful task primitives, ultimately yielding faster and more robust learning."],"url":"http://arxiv.org/abs/2402.18361v1","category":"q-bio.NC"}
{"created":"2024-02-28 14:30:34","title":"Analysis of double-resonance crossing in adiabatic trapping phenomena for quasi-integrable area-preserving maps with time-dependent exciters","abstract":"In this paper, we analyze the adiabatic crossing of a resonance for Hamiltonian systems when a double-resonance condition is satisfied by the linear frequency at an elliptic fixed point. We discuss in detail the phase-space structure on a class of Hamiltonians and area-preserving maps with an elliptic fixed point in the presence of a time-dependent exciter. Various regimes have been identified and carefully studied. This study extends results obtained recently for the trapping and transport phenomena for periodically perturbed Hamiltonian systems, and it could have relevant applications in the adiabatic beam splitting in accelerator physics.","sentences":["In this paper, we analyze the adiabatic crossing of a resonance for Hamiltonian systems when a double-resonance condition is satisfied by the linear frequency at an elliptic fixed point.","We discuss in detail the phase-space structure on a class of Hamiltonians and area-preserving maps with an elliptic fixed point in the presence of a time-dependent exciter.","Various regimes have been identified and carefully studied.","This study extends results obtained recently for the trapping and transport phenomena for periodically perturbed Hamiltonian systems, and it could have relevant applications in the adiabatic beam splitting in accelerator physics."],"url":"http://arxiv.org/abs/2402.18358v1","category":"math.DS"}
{"created":"2024-02-28 14:28:21","title":"BCS surrogate models for floating superconductor-semiconductor hybrids","abstract":"Superconductor-semiconductor hybrid devices, involving quantum dots interfaced with floating and/or grounded superconductors, have reached a level of complexity which calls for the development of versatile and numerically efficient modelling tools. Here, we propose an extension of the surrogate model solver for sub-gap states [Phys. Rev. B 108, L220506 (2023)], which is able to handle floating superconducting islands with finite charging energy. Upon eliminating all finite-size effects of the computationally demanding Richardson model approach, we achieve a more efficient way of calculating the sub-gap spectra and related observables without compromising their accuracy. We provide a number of benchmarks between the two approaches and showcase the versatility of the extended surrogate model solver by studying the stability of spin-triplet ground states in various tunable devices. The methods introduced here set the stage for reliable microscopic simulations of complex superconducting quantum circuits across all their relevant parameter regimes.","sentences":["Superconductor-semiconductor hybrid devices, involving quantum dots interfaced with floating and/or grounded superconductors, have reached a level of complexity which calls for the development of versatile and numerically efficient modelling tools.","Here, we propose an extension of the surrogate model solver for sub-gap states","[Phys. Rev. B 108, L220506 (2023)], which is able to handle floating superconducting islands with finite charging energy.","Upon eliminating all finite-size effects of the computationally demanding Richardson model approach, we achieve a more efficient way of calculating the sub-gap spectra and related observables without compromising their accuracy.","We provide a number of benchmarks between the two approaches and showcase the versatility of the extended surrogate model solver by studying the stability of spin-triplet ground states in various tunable devices.","The methods introduced here set the stage for reliable microscopic simulations of complex superconducting quantum circuits across all their relevant parameter regimes."],"url":"http://arxiv.org/abs/2402.18357v1","category":"cond-mat.supr-con"}
{"created":"2024-02-28 14:26:52","title":"DynaWarp -- Efficient, large-scale log storage and retrieval","abstract":"Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time. At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use. We present our novel DynaWarp membership sketch, capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data. In our experiments, DynaWarp required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch. Additionally, DynaWarp achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch.","sentences":["Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time.","At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use.","We present our novel DynaWarp membership sketch, capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data.","In our experiments, DynaWarp required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch.","Additionally, DynaWarp achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch."],"url":"http://arxiv.org/abs/2402.18355v1","category":"cs.IR"}
{"created":"2024-02-28 14:15:26","title":"Emergence of rogue-like waves in a reaction-diffusion system: Stochastic output from deterministic dynamics","abstract":"Rogue waves are an intriguing nonlinear phenomenon arising across different scales, ranging from ocean waves through optics to Bose-Einstein condensate. We describe the emergence of rogue wave-like dynamics in a reaction-diffusion system that arise as a result of a subcritical Turing instability. This state is present in the regime where all time-independent states are unstable, and consists of intermittent excitation of spatially localized spikes, followed by collapse to an unstable state and subsequent regrowth. We characterize the spatiotemporal organization of spikes and show that in sufficiently large domains the dynamics are consistent with a memoryless process.","sentences":["Rogue waves are an intriguing nonlinear phenomenon arising across different scales, ranging from ocean waves through optics to Bose-Einstein condensate.","We describe the emergence of rogue wave-like dynamics in a reaction-diffusion system that arise as a result of a subcritical Turing instability.","This state is present in the regime where all time-independent states are unstable, and consists of intermittent excitation of spatially localized spikes, followed by collapse to an unstable state and subsequent regrowth.","We characterize the spatiotemporal organization of spikes and show that in sufficiently large domains the dynamics are consistent with a memoryless process."],"url":"http://arxiv.org/abs/2402.18349v1","category":"nlin.PS"}
{"created":"2024-02-28 14:13:28","title":"Threshold solutions of the energy-critical complex Ginzburg-Landau equation","abstract":"In this article, we consider energy-critical complex Ginzburg-Landau equation in three and four dimensions. We give the dynamics when the energy of the initial data is equal to the energy of the stationary solution.","sentences":["In this article, we consider energy-critical complex Ginzburg-Landau equation in three and four dimensions.","We give the dynamics when the energy of the initial data is equal to the energy of the stationary solution."],"url":"http://arxiv.org/abs/2402.18347v1","category":"math.AP"}
{"created":"2024-02-28 14:10:35","title":"Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks","abstract":"Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals. Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities. Additionally, they often rely on engineered heuristics for assigning entities among agents. We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in simulation. Leveraging permutation invariant neural network architectures and model-free reinforcement learning, our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity. We validate our approach through both simulations and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities. We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems. Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems. The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a zero-shot manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors.","sentences":["Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals.","Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities.","Additionally, they often rely on engineered heuristics for assigning entities among agents.","We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in simulation.","Leveraging permutation invariant neural network architectures and model-free reinforcement learning, our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity.","We validate our approach through both simulations and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities.","We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems.","Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems.","The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a zero-shot manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors."],"url":"http://arxiv.org/abs/2402.18345v1","category":"cs.RO"}
{"created":"2024-02-28 13:59:20","title":"Probabilistic Bayesian optimal experimental design using conditional normalizing flows","abstract":"Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations.","sentences":["Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework.","Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs.","To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach.","This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution.","We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations."],"url":"http://arxiv.org/abs/2402.18337v1","category":"cs.LG"}
{"created":"2024-02-28 13:23:02","title":"Hybrid optomechanical superconducting qubit system","abstract":"We propose an integrated nonlinear superconducting device based on a nanoelectromechanical shuttle. The system can be described as a qubit coupled to a bosonic mode. The topology of the circuit gives rise to an adjustable qubit/mechanical coupling, allowing the experimenter to tune between linear and quadratic coupling in the mechanical degrees of freedom. Owing to its flexibility and potential scalability, the proposed setup represents an important step towards the implementation of bosonic error correction with mechanical elements in large-scale superconducting circuits. We give preliminary evidence of this possibility by discussing a simple state-swapping protocol that uses this device as a quantum memory element.","sentences":["We propose an integrated nonlinear superconducting device based on a nanoelectromechanical shuttle.","The system can be described as a qubit coupled to a bosonic mode.","The topology of the circuit gives rise to an adjustable qubit/mechanical coupling, allowing the experimenter to tune between linear and quadratic coupling in the mechanical degrees of freedom.","Owing to its flexibility and potential scalability, the proposed setup represents an important step towards the implementation of bosonic error correction with mechanical elements in large-scale superconducting circuits.","We give preliminary evidence of this possibility by discussing a simple state-swapping protocol that uses this device as a quantum memory element."],"url":"http://arxiv.org/abs/2402.18317v1","category":"quant-ph"}
{"created":"2024-02-28 13:22:42","title":"Rare events in a stochastic vegetation-water dynamical system based on machine learning","abstract":"Stochastic vegetation-water dynamical systems play a pivotal role in ecological stability, biodiversity, water resource management, and adaptation to climate change. This research proposes a machine learning-based method for analyzing rare events in stochastic vegetation-water dynamical systems with multiplicative Gaussian noise. Utilizing the Freidlin-Wentzell large deviation theory, we derive the asymptotic expressions for the quasipotential and the mean first exit time. Based on the decomposition of vector field, we design a neural network architecture to compute the most probable transition paths and the mean first exit time for both non-characteristic and characteristic boundary scenarios. The results indicate that this method can effectively predict early warnings of vegetation degradation, providing new theoretical foundations and mathematical tools for ecological management and conservation. Moreover, the method offers new possibilities for exploring more complex and higher-dimensional stochastic dynamical systems.","sentences":["Stochastic vegetation-water dynamical systems play a pivotal role in ecological stability, biodiversity, water resource management, and adaptation to climate change.","This research proposes a machine learning-based method for analyzing rare events in stochastic vegetation-water dynamical systems with multiplicative Gaussian noise.","Utilizing the Freidlin-Wentzell large deviation theory, we derive the asymptotic expressions for the quasipotential and the mean first exit time.","Based on the decomposition of vector field, we design a neural network architecture to compute the most probable transition paths and the mean first exit time for both non-characteristic and characteristic boundary scenarios.","The results indicate that this method can effectively predict early warnings of vegetation degradation, providing new theoretical foundations and mathematical tools for ecological management and conservation.","Moreover, the method offers new possibilities for exploring more complex and higher-dimensional stochastic dynamical systems."],"url":"http://arxiv.org/abs/2402.18315v1","category":"math.DS"}
{"created":"2024-02-28 13:08:48","title":"Deformed cluster maps of type $A_{2N}$","abstract":"We extend recent work of the third author and Kouloukas by constructing deformations of integrable cluster maps corresponding to the Dynkin types $A_{2N}$, lifting these to higher-dimensional maps possessing the Laurent property and demonstrating integrality of the deformations for $N\\leq 3$. This provides the first infinite class of examples (in arbitrarily high rank) of such maps and gives information on the associated discrete integrable systems. Key to our approach is a ``local expansion'' operation on quivers which allows us to construct and study mutations in type $A_{2N}$ from those in type $A_{2(N-1)}$.","sentences":["We extend recent work of the third author and Kouloukas by constructing deformations of integrable cluster maps corresponding to the Dynkin types $A_{2N}$, lifting these to higher-dimensional maps possessing the Laurent property and demonstrating integrality of the deformations for $N\\leq 3$.","This provides the first infinite class of examples (in arbitrarily high rank) of such maps and gives information on the associated discrete integrable systems.","Key to our approach is a ``local expansion'' operation on quivers which allows us to construct and study mutations in type $A_{2N}$ from those in type $A_{2(N-1)}$."],"url":"http://arxiv.org/abs/2402.18310v1","category":"nlin.SI"}
{"created":"2024-02-28 13:04:51","title":"Calibration requirement for Epoch of Reionization 21-cm signal observation -- III. Bias and variance in uGMRT ELAIS-N1 field power spectrum","abstract":"Power spectrum of \\HI 21 cm radiation is one of the promising probes to study large scale structure of the universe. Presence of orders of magnitude larger foregrounds in the frequency range for such observations has been one of the major challenge. The foreground contamination also introduce residual calibration errors in the interferometric data. The latter introduce bias in the 21-cm power spectrum estimates and increase systematics. There have been several efforts to understand and improve on the calibration errors. In this work we use an analytical estimate of the bias and variance in redshifted 21-cm power spectrum in presence of time-correlated residual gain errors and foreground. We use the uGMRT Band-3 observations of the ELAIS-N1 field and estimate the bias and variance in the power spectrum from these observation. We first access the statistics of the gain errors and based on the quality of calibration we flag a set of additional antennae. The latter reduce the bias and variance of power spectrum significantly and we found it to be recommended for such analysis. We observe that for the uGMRT baseline configuration and system parameters, the variance is always higher than the bias. The excess variance in the power spectrum reduces with increase of the angular scales and at about $\\ell\\sim6000$ the effects from residual gain errors are negligible. Based on our analysis we observe that for an angular multipole of $\\ell \\sim3000$, $2000$ hours of `on source time' is required with uGMRT to detect redshifted 21-cm signal at $3-\\sigma$ significance from a redshift of $2.55$. In this work we only consider the power spectrum measurement in the plane of the sky, an assessment of residual gain statistics and its effect on multifrequency angular power spectrum estimation will be presented in a companion paper.","sentences":["Power spectrum of \\HI 21 cm radiation is one of the promising probes to study large scale structure of the universe.","Presence of orders of magnitude larger foregrounds in the frequency range for such observations has been one of the major challenge.","The foreground contamination also introduce residual calibration errors in the interferometric data.","The latter introduce bias in the 21-cm power spectrum estimates and increase systematics.","There have been several efforts to understand and improve on the calibration errors.","In this work we use an analytical estimate of the bias and variance in redshifted 21-cm power spectrum in presence of time-correlated residual gain errors and foreground.","We use the uGMRT Band-3 observations of the ELAIS-N1 field and estimate the bias and variance in the power spectrum from these observation.","We first access the statistics of the gain errors and based on the quality of calibration we flag a set of additional antennae.","The latter reduce the bias and variance of power spectrum significantly and we found it to be recommended for such analysis.","We observe that for the uGMRT baseline configuration and system parameters, the variance is always higher than the bias.","The excess variance in the power spectrum reduces with increase of the angular scales and at about $\\ell\\sim6000$ the effects from residual gain errors are negligible.","Based on our analysis we observe that for an angular multipole of $\\ell \\sim3000$, $2000$ hours of `on source time' is required with uGMRT to detect redshifted 21-cm signal at $3-\\sigma$ significance from a redshift of $2.55$. In this work we only consider the power spectrum measurement in the plane of the sky, an assessment of residual gain statistics and its effect on multifrequency angular power spectrum estimation will be presented in a companion paper."],"url":"http://arxiv.org/abs/2402.18306v1","category":"astro-ph.IM"}
{"created":"2024-02-28 12:57:22","title":"Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning","abstract":"In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge. The primary goal is to minimize communication overhead and runtime cost. However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead. Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning. This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable. Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types. In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning. S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets. Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game. Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints. For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines. Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph.","sentences":["In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge.","The primary goal is to minimize communication overhead and runtime cost.","However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead.","Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning.","This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable.","Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types.","In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning.","S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets.","Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game.","Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints.","For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines.","Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph."],"url":"http://arxiv.org/abs/2402.18304v1","category":"cs.DC"}
{"created":"2024-02-28 12:53:20","title":"Grey Two-moment Neutrino Transport: Comprehensive Tests and Improvements for Supernova Simulations","abstract":"In this work, we extended an energy-integrated neutrino transport method to facilitate efficient, yet precise, modeling of compact astrophysical objects. We focus particularly on core-collapse supernovae. We implemented the framework of Foucart et al. (2016) into FLASH and performed a detailed evaluation of its accuracy in core-collapse supernova simulations. Based on comparisons with results from simulations using energy-dependent neutrino transport, we incorporated several improvements to the original scheme. Our analysis shows that our grey neutrino transport method successfully reproduces key aspects from more complex energy-dependent transport across a variety of progenitors and equations of state. We find both qualitative and reasonable quantitative agreement with multi-group M1 transport simulations. However, the grey scheme tends to slightly favor shock revival. In terms of gravitational wave and neutrino signals, there is a good alignment with the energy-dependent transport, although we find 15-30 percent discrepancies in the average energy and luminosity of heavy-lepton neutrinos. Simulations using the grey transport are around four times faster than those using energy-dependent transport.","sentences":["In this work, we extended an energy-integrated neutrino transport method to facilitate efficient, yet precise, modeling of compact astrophysical objects.","We focus particularly on core-collapse supernovae.","We implemented the framework of Foucart et al. (2016) into FLASH and performed a detailed evaluation of its accuracy in core-collapse supernova simulations.","Based on comparisons with results from simulations using energy-dependent neutrino transport, we incorporated several improvements to the original scheme.","Our analysis shows that our grey neutrino transport method successfully reproduces key aspects from more complex energy-dependent transport across a variety of progenitors and equations of state.","We find both qualitative and reasonable quantitative agreement with multi-group M1 transport simulations.","However, the grey scheme tends to slightly favor shock revival.","In terms of gravitational wave and neutrino signals, there is a good alignment with the energy-dependent transport, although we find 15-30 percent discrepancies in the average energy and luminosity of heavy-lepton neutrinos.","Simulations using the grey transport are around four times faster than those using energy-dependent transport."],"url":"http://arxiv.org/abs/2402.18303v1","category":"astro-ph.HE"}
{"created":"2024-02-28 12:50:16","title":"EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving","abstract":"This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack model and its components. The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack.","sentences":["This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving.","Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving.","In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking.","We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.","The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains.","Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively.","Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.","Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack model and its components.","The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack."],"url":"http://arxiv.org/abs/2402.18302v1","category":"cs.CV"}
{"created":"2024-02-28 12:24:05","title":"Cotunneling effects in the geometric statistics of a nonequilibrium spintronic junction","abstract":"In the nonequilibrium steadystate of electronic transport across a spin-resolved quantronic junction, we investigate the role of cotunneling on the emergent statistics under phase-different adiabatic modulation of the reservoirs' chemical potentials. By explicitly identifying the sequential and inelastic cotunneling rates, we numerically evaluate the geometric or Pancharatnam-Berry contributions to the spin exchange flux. We identify the relevant conditions wherein the sequential and cotunneling processes compete and selectively influence the total geometric flux upshot. The Fock space coherences are found to suppress the cotunneling effects when the system reservoir couplings are comparable. The cotunneling contribution to the total geometric flux can be made comparable to the sequential contribution by creating a rightsided asymmetry in the system-reservoir coupling strength. Using a recently proposed geometric thermodynamic uncertainty relationship, we numerically estimate the total rate of minimal entropy production. The geometric flux and the minimum entropy are found to be nonlinear as a function of the interaction energy of the junction's spin orbitals.","sentences":["In the nonequilibrium steadystate of electronic transport across a spin-resolved quantronic junction, we investigate the role of cotunneling on the emergent statistics under phase-different adiabatic modulation of the reservoirs' chemical potentials.","By explicitly identifying the sequential and inelastic cotunneling rates, we numerically evaluate the geometric or Pancharatnam-Berry contributions to the spin exchange flux.","We identify the relevant conditions wherein the sequential and cotunneling processes compete and selectively influence the total geometric flux upshot.","The Fock space coherences are found to suppress the cotunneling effects when the system reservoir couplings are comparable.","The cotunneling contribution to the total geometric flux can be made comparable to the sequential contribution by creating a rightsided asymmetry in the system-reservoir coupling strength.","Using a recently proposed geometric thermodynamic uncertainty relationship, we numerically estimate the total rate of minimal entropy production.","The geometric flux and the minimum entropy are found to be nonlinear as a function of the interaction energy of the junction's spin orbitals."],"url":"http://arxiv.org/abs/2402.18283v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 12:17:21","title":"Indirect Job-Shop coding using rank: application to QAOA (IQAOA)","abstract":"The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling. It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive graph, with earliest starting times computed using a longest path algorithm. The complexity of solving this problem arises in part from the requirement that disjunctive graphs representing solutions must be acyclic. Consequently, enumerating these graphs is feasible for small-scale instances only. A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the 'vector by repetition' (commonly known as Bierwith's vector). Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive graph, thereby enabling the mapping of a vector to a solution. This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive graphs. Our objective is to demonstrate how Bierwith's vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach.","sentences":["The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling.","It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive graph, with earliest starting times computed using a longest path algorithm.","The complexity of solving this problem arises in part from the requirement that disjunctive graphs representing solutions must be acyclic.","Consequently, enumerating these graphs is feasible for small-scale instances only.","A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the 'vector by repetition' (commonly known as Bierwith's vector).","Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive graph, thereby enabling the mapping of a vector to a solution.","This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive graphs.","Our objective is to demonstrate how Bierwith's vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach."],"url":"http://arxiv.org/abs/2402.18280v1","category":"quant-ph"}
{"created":"2024-02-28 12:16:42","title":"EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods","abstract":"High-definition (HD) map is crucial for autonomous driving systems. Most existing works design map elements detection heads based on the DETR decoder. However, the initial queries lack integration with the physical location feature of map elements, and vanilla self-attention entails high computational complexity. Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods. Firstly, we design query units based on the physical location feature of anchor neighborhoods. Non-neighborhood central anchors effectively assist the neighborhood central anchors in fitting to the target points, significantly improving the prediction accuracy. Then, we introduce grouped local self-attention (GL-SA), which innovatively utilizes local queries as the medium for feature interaction, thereby substantially reducing the computational complexity of self-attention while facilitating ample feature interaction among queries. On nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs. Furthermore, it considerably reduces memory consumption by 8198M compared to the baseline.","sentences":["High-definition (HD) map is crucial for autonomous driving systems.","Most existing works design map elements detection heads based on the DETR decoder.","However, the initial queries lack integration with the physical location feature of map elements, and vanilla self-attention entails high computational complexity.","Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods.","Firstly, we design query units based on the physical location feature of anchor neighborhoods.","Non-neighborhood central anchors effectively assist the neighborhood central anchors in fitting to the target points, significantly improving the prediction accuracy.","Then, we introduce grouped local self-attention (GL-SA), which innovatively utilizes local queries as the medium for feature interaction, thereby substantially reducing the computational complexity of self-attention while facilitating ample feature interaction among queries.","On nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs.","Furthermore, it considerably reduces memory consumption by 8198M compared to the baseline."],"url":"http://arxiv.org/abs/2402.18278v1","category":"cs.CV"}
{"created":"2024-02-28 12:06:08","title":"Exploration of Adapter for Noise Robust Automatic Speech Recognition","abstract":"Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This paper thoroughly investigates adapter-based noise-robust ASR adaptation. We conducted the experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. Besides, the simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training remains valid for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.","sentences":["Adapting a robust automatic speech recognition (ASR) system to tackle unseen noise scenarios is crucial.","Integrating adapters into neural networks has emerged as a potent technique for transfer learning.","This paper thoroughly investigates adapter-based noise-robust ASR adaptation.","We conducted the experiments using the CHiME--4 dataset.","The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers.","Besides, the simulated data helps the system to improve its performance under real noise conditions.","Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data.","Multi-condition training remains valid for adapter training.","Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements."],"url":"http://arxiv.org/abs/2402.18275v1","category":"cs.SD"}
{"created":"2024-02-28 12:05:28","title":"Ferroelectrically tunable topological phase transition in In$_2$Se$_3$ thin films","abstract":"Materials with ferroelectrically switchable topological properties are of interest for both fundamental physics and practical applications. Using first-principles calculations, we find that stacking ferroelectric $\\alpha$-In$_2$Se$_3$ monolayers into a bilayer leads to polarization-dependent band structures, which yields polarization-dependent topological properties. Specifically, we find that the states with interlayer ferroelectric couplings are quantum spin Hall insulators, while those with antiferroelectric polarizations are normal insulators. We further find that In$_2$Se$_3$ trilayer and quadlayer exhibit nontrivial band topology as long as in the structure the ferroelectric In$_2$Se$_3$ bilayer is antiferroelectrically coupled to In$_2$Se$_3$ monolayers or other ferroelectric In$_2$Se$_3$ bilayer. Otherwise the system is topologically trivial. The reason is that near the Fermi level the band structure of the ferroelectric In$_2$Se$_3$ bilayer has to be maintained for the nontrivial band topology. This feature can be used to design nontrivial band topology for the thicker films by a proper combination of the interlayer polarization couplings. The topological properties can be ferroelectrically tunable using the dipole locking effect. Our study reveals switchable band topology in a family of natural ferroelectrics, which provide a platform for designing new functional devices.","sentences":["Materials with ferroelectrically switchable topological properties are of interest for both fundamental physics and practical applications.","Using first-principles calculations, we find that stacking ferroelectric $\\alpha$-In$_2$Se$_3$ monolayers into a bilayer leads to polarization-dependent band structures, which yields polarization-dependent topological properties.","Specifically, we find that the states with interlayer ferroelectric couplings are quantum spin Hall insulators, while those with antiferroelectric polarizations are normal insulators.","We further find that In$_2$Se$_3$ trilayer and quadlayer exhibit nontrivial band topology as long as in the structure the ferroelectric In$_2$Se$_3$ bilayer is antiferroelectrically coupled to In$_2$Se$_3$ monolayers or other ferroelectric In$_2$Se$_3$ bilayer.","Otherwise the system is topologically trivial.","The reason is that near the Fermi level the band structure of the ferroelectric In$_2$Se$_3$ bilayer has to be maintained for the nontrivial band topology.","This feature can be used to design nontrivial band topology for the thicker films by a proper combination of the interlayer polarization couplings.","The topological properties can be ferroelectrically tunable using the dipole locking effect.","Our study reveals switchable band topology in a family of natural ferroelectrics, which provide a platform for designing new functional devices."],"url":"http://arxiv.org/abs/2402.18274v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 11:59:12","title":"FPM-WSI: Fourier ptychographic whole slide imaging via feature-domain backdiffraction","abstract":"Fourier ptychographic microscopy (FPM), characterized by high-throughput computational imaging, theoretically provides a cunning solution to the trade-off between spatial resolution and field of view (FOV), which has a promising prospect in the application of digital pathology. However, block reconstruction and then stitching has currently become an unavoidable procedure due to vignetting effects. The stitched image tends to present color inconsistency in different image segments, or even stitching artifacts. In response, we reported a computational framework based on feature-domain backdiffraction to realize full-FOV, stitching-free FPM reconstruction. Different from conventional algorithms that establish the loss function in the image domain, our method formulates it in the feature domain, where effective information of images is extracted by a feature extractor to bypass the vignetting effect. The feature-domain error between predicted images based on estimation of model parameters and practically captured images is then digitally diffracted back through the optical system for complex amplitude reconstruction and aberration compensation. Through massive simulations and experiments, the method presents effective elimination of vignetting artifacts, and reduces the requirement of precise knowledge of illumination positions. We also found its great potential to recover the data with a lower overlapping rate of spectrum and to realize automatic blind-digital refocusing without a prior defocus distance.","sentences":["Fourier ptychographic microscopy (FPM), characterized by high-throughput computational imaging, theoretically provides a cunning solution to the trade-off between spatial resolution and field of view (FOV), which has a promising prospect in the application of digital pathology.","However, block reconstruction and then stitching has currently become an unavoidable procedure due to vignetting effects.","The stitched image tends to present color inconsistency in different image segments, or even stitching artifacts.","In response, we reported a computational framework based on feature-domain backdiffraction to realize full-FOV, stitching-free FPM reconstruction.","Different from conventional algorithms that establish the loss function in the image domain, our method formulates it in the feature domain, where effective information of images is extracted by a feature extractor to bypass the vignetting effect.","The feature-domain error between predicted images based on estimation of model parameters and practically captured images is then digitally diffracted back through the optical system for complex amplitude reconstruction and aberration compensation.","Through massive simulations and experiments, the method presents effective elimination of vignetting artifacts, and reduces the requirement of precise knowledge of illumination positions.","We also found its great potential to recover the data with a lower overlapping rate of spectrum and to realize automatic blind-digital refocusing without a prior defocus distance."],"url":"http://arxiv.org/abs/2402.18270v1","category":"physics.optics"}
{"created":"2024-02-28 11:58:23","title":"Control sets of linear control systems on $\\R^2$. The real case","abstract":"In this paper, we study the dynamical behavior of a linear control system on $\\R^2$ when the associated matrix has real eigenvalues. Different from the complex case, we show that the position of the control zero relative to the control range can have a strong interference in such dynamics if the matrix is not invertible. In the invertible case, we explicitly construct the unique control set with a nonempty interior.","sentences":["In this paper, we study the dynamical behavior of a linear control system on $\\R^2$ when the associated matrix has real eigenvalues.","Different from the complex case, we show that the position of the control zero relative to the control range can have a strong interference in such dynamics if the matrix is not invertible.","In the invertible case, we explicitly construct the unique control set with a nonempty interior."],"url":"http://arxiv.org/abs/2402.18269v1","category":"math.OC"}
{"created":"2024-02-28 11:53:26","title":"Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space","abstract":"A set of vertices in a graph forms a potential maximal clique if there exists a minimal chordal completion in which it is a maximal clique. Potential maximal cliques were first introduced as a key tool to obtain an efficient, though exponential-time algorithm to compute the treewidth of a graph. As a byproduct, this allowed to compute the treewidth of various graph classes in polynomial time.   In recent years, the concept of potential maximal cliques regained interest as it proved to be useful for a handful of graph algorithmic problems. In particular, it turned out to be a key tool to obtain a polynomial time algorithm for computing maximum weight independent sets in $P_5$-free and $P_6$-free graphs (Lokshtanov et al., SODA `14 and Grzeskik et al., SODA `19. In most of their applications, obtaining all the potential maximal cliques constitutes an algorithmic bottleneck, thus motivating the question of how to efficiently enumerate all the potential maximal cliques in a graph $G$.   The state-of-the-art algorithm by Bouchitt\\'e \\& Todinca can enumerate potential maximal cliques in output-polynomial time by using exponential space, a significant limitation for the size of feasible instances. In this paper, we revisit this algorithm and design an enumeration algorithm that preserves an output-polynomial time complexity while only requiring polynomial space.","sentences":["A set of vertices in a graph forms a potential maximal clique if there exists a minimal chordal completion in which it is a maximal clique.","Potential maximal cliques were first introduced as a key tool to obtain an efficient, though exponential-time algorithm to compute the treewidth of a graph.","As a byproduct, this allowed to compute the treewidth of various graph classes in polynomial time.   ","In recent years, the concept of potential maximal cliques regained interest as it proved to be useful for a handful of graph algorithmic problems.","In particular, it turned out to be a key tool to obtain a polynomial time algorithm for computing maximum weight independent sets in $P_5$-free and $P_6$-free graphs (Lokshtanov et al., SODA `14 and Grzeskik et al., SODA `19.","In most of their applications, obtaining all the potential maximal cliques constitutes an algorithmic bottleneck, thus motivating the question of how to efficiently enumerate all the potential maximal cliques in a graph $G$.   The state-of-the-art algorithm by Bouchitt\\'e \\& Todinca can enumerate potential maximal cliques in output-polynomial time by using exponential space, a significant limitation for the size of feasible instances.","In this paper, we revisit this algorithm and design an enumeration algorithm that preserves an output-polynomial time complexity while only requiring polynomial space."],"url":"http://arxiv.org/abs/2402.18265v1","category":"cs.DS"}
{"created":"2024-02-28 11:51:28","title":"Max-Cut with $\u03b5$-Accurate Predictions","abstract":"We study the approximability of the MaxCut problem in the presence of predictions. Specifically, we consider two models: in the noisy predictions model, for each vertex we are given its correct label in $\\{-1,+1\\}$ with some unknown probability $1/2 + \\epsilon$, and the other (incorrect) label otherwise. In the more-informative partial predictions model, for each vertex we are given its correct label with probability $\\epsilon$ and no label otherwise. We assume only pairwise independence between vertices in both models.   We show how these predictions can be used to improve on the worst-case approximation ratios for this problem. Specifically, we give an algorithm that achieves an $\\alpha + \\widetilde{\\Omega}(\\epsilon^4)$-approximation for the noisy predictions model, where $\\alpha \\approx 0.878$ is the MaxCut threshold. While this result also holds for the partial predictions model, we can also give a $\\beta + \\Omega(\\epsilon)$-approximation, where $\\beta \\approx 0.858$ is the approximation ratio for MaxBisection given by Raghavendra and Tan. This answers a question posed by Ola Svensson in his plenary session talk at SODA'23.","sentences":["We study the approximability of the MaxCut problem in the presence of predictions.","Specifically, we consider two models: in the noisy predictions model, for each vertex we are given its correct label in $\\{-1,+1\\}$ with some unknown probability $1/2 + \\epsilon$, and the other (incorrect) label otherwise.","In the more-informative partial predictions model, for each vertex we are given its correct label with probability $\\epsilon$ and no label otherwise.","We assume only pairwise independence between vertices in both models.   ","We show how these predictions can be used to improve on the worst-case approximation ratios for this problem.","Specifically, we give an algorithm that achieves an $\\alpha + \\widetilde{\\Omega}(\\epsilon^4)$-approximation for the noisy predictions model, where $\\alpha \\approx 0.878$ is the MaxCut threshold.","While this result also holds for the partial predictions model, we can also give a $\\beta + \\Omega(\\epsilon)$-approximation, where $\\beta \\approx 0.858$ is the approximation ratio for MaxBisection given by Raghavendra and Tan.","This answers a question posed by Ola Svensson in his plenary session talk at SODA'23."],"url":"http://arxiv.org/abs/2402.18263v1","category":"cs.DS"}
{"created":"2024-02-28 11:47:15","title":"Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning","abstract":"Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example.","sentences":["Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space.","Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose.","In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed.","This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles.","We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP.","Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed.","The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example."],"url":"http://arxiv.org/abs/2402.18260v1","category":"cs.LG"}
{"created":"2024-02-28 11:39:07","title":"Scaling limits of complex and symplectic non-Hermitian Wishart ensembles","abstract":"Non-Hermitian Wishart matrices were introduced in the context of quantum chromodynamics with a baryon chemical potential. These provide chiral extensions of the elliptic Ginibre ensembles as well as non-Hermitian extensions of the classical Wishart/Laguerre ensembles. In this work, we investigate eigenvalues of non-Hermitian Wishart matrices in the symmetry classes of complex and symplectic Ginibre ensembles. We introduce a generalised Christoffel-Darboux formula in the form of a certain second-order differential equation, offering a unified and robust method for analyzing correlation functions across all scaling regimes in the model. By employing this method, we derive universal bulk and edge scaling limits for eigenvalue correlations at both strong and weak non-Hermiticity.","sentences":["Non-Hermitian Wishart matrices were introduced in the context of quantum chromodynamics with a baryon chemical potential.","These provide chiral extensions of the elliptic Ginibre ensembles as well as non-Hermitian extensions of the classical Wishart/Laguerre ensembles.","In this work, we investigate eigenvalues of non-Hermitian Wishart matrices in the symmetry classes of complex and symplectic Ginibre ensembles.","We introduce a generalised Christoffel-Darboux formula in the form of a certain second-order differential equation, offering a unified and robust method for analyzing correlation functions across all scaling regimes in the model.","By employing this method, we derive universal bulk and edge scaling limits for eigenvalue correlations at both strong and weak non-Hermiticity."],"url":"http://arxiv.org/abs/2402.18257v1","category":"math.PR"}
{"created":"2024-02-28 11:12:17","title":"Prospect Personalized Recommendation on Large Language Model-based Agent Platform","abstract":"The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.","sentences":["The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity.","In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender.","Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop.","Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user.","A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application.","Lastly, we discuss potential issues and promising directions for future research."],"url":"http://arxiv.org/abs/2402.18240v1","category":"cs.IR"}
{"created":"2024-02-28 10:53:50","title":"Joint Beamforming Design and Stream Allocation for Non-Coherent Joint Transmission in Cell-Free MIMO Networks","abstract":"We consider joint beamforming and stream allocation to maximize the weighted sum rate (WSR) for non-coherent joint transmission (NCJT) in user-centric cell-free MIMO networks, where distributed access points (APs) are organized in clusters to transmit different signals to serve each user equipment (UE). We for the first time consider the common limits of maximum number of receive streams at UEs in practical networks, and formulate a joint beamforming and transmit stream allocation problem for WSR maximization under per-AP transmit power constraints. Since the integer number of transmit streams determines the dimension of the beamformer, the joint optimization problem is mixed-integer and nonconvex with coupled decision variables that is inherently NP-hard. In this paper, we first propose a distributed low-interaction reduced weighted minimum mean square error (RWMMSE) beamforming algorithm for WSR maximization with fixed streams. Our proposed RWMMSE algorithm requires significantly less interaction across the network and has the current lowest computational complexity that scales linearly with the number of transmit antennas, without any compromise on WSR. We draw insights on the joint beamforming and stream allocation problem to decouple the decision variables and relax the mixed-integer constraints. We then propose a joint beamforming and linear stream allocation algorithm, termed as RWMMSE-LSA, which yields closed-form updates with linear stream allocation complexity and is guaranteed to converge to the stationary points of the original joint optimization problem. Simulation results demonstrate substantial performance gain of our proposed algorithms over the current best alternatives in both WSR performance and convergence time.","sentences":["We consider joint beamforming and stream allocation to maximize the weighted sum rate (WSR) for non-coherent joint transmission (NCJT) in user-centric cell-free MIMO networks, where distributed access points (APs) are organized in clusters to transmit different signals to serve each user equipment (UE).","We for the first time consider the common limits of maximum number of receive streams at UEs in practical networks, and formulate a joint beamforming and transmit stream allocation problem for WSR maximization under per-AP transmit power constraints.","Since the integer number of transmit streams determines the dimension of the beamformer, the joint optimization problem is mixed-integer and nonconvex with coupled decision variables that is inherently NP-hard.","In this paper, we first propose a distributed low-interaction reduced weighted minimum mean square error (RWMMSE) beamforming algorithm for WSR maximization with fixed streams.","Our proposed RWMMSE algorithm requires significantly less interaction across the network and has the current lowest computational complexity that scales linearly with the number of transmit antennas, without any compromise on WSR.","We draw insights on the joint beamforming and stream allocation problem to decouple the decision variables and relax the mixed-integer constraints.","We then propose a joint beamforming and linear stream allocation algorithm, termed as RWMMSE-LSA, which yields closed-form updates with linear stream allocation complexity and is guaranteed to converge to the stationary points of the original joint optimization problem.","Simulation results demonstrate substantial performance gain of our proposed algorithms over the current best alternatives in both WSR performance and convergence time."],"url":"http://arxiv.org/abs/2402.18231v1","category":"eess.SP"}
{"created":"2024-02-28 10:52:15","title":"Linear inviscid damping in the presence of an embedding eigenvalue","abstract":"In this paper, we investigate the long-time dynamics of the linearized 2-D Euler equations around a hyperbolic tangent flow $(\\tanh y,0)$. A key difference compared to previous results is that the linearized operator has an embedding eigenvalue, which has a significant impact on the dynamics of the linearized system. For the first mode, the dynamics consists of there parts: non-decay part related to the eigenspace associated with the embedding eigenvalue, slow decay part due to the resolvent singularity, and fast decay part related to the inviscid damping. For higher modes, the dynamics is similar to the inviscid damping phenomena in the case without embedding eigenvalues.","sentences":["In this paper, we investigate the long-time dynamics of the linearized 2-D Euler equations around a hyperbolic tangent flow $(\\tanh y,0)$. A key difference compared to previous results is that the linearized operator has an embedding eigenvalue, which has a significant impact on the dynamics of the linearized system.","For the first mode, the dynamics consists of there parts: non-decay part related to the eigenspace associated with the embedding eigenvalue, slow decay part due to the resolvent singularity, and fast decay part related to the inviscid damping.","For higher modes, the dynamics is similar to the inviscid damping phenomena in the case without embedding eigenvalues."],"url":"http://arxiv.org/abs/2402.18229v1","category":"math.AP"}
{"created":"2024-02-28 10:29:30","title":"Fluctuations of the Nodal Number in the Two-Energy Planar Berry Random Wave Model","abstract":"We investigate the fluctuations of the nodal number (count of the phase singularities) in a natural extension of the well-known complex planar Berry Random Wave Model - Berry (2002) - obtained by considering two independent real Berry Random Waves, with distinct energies $E_1, E_2 \\to \\infty$ (at possibly $\\neq$ speeds). Our framework relaxes the conditions used in Nourdin, Peccati and Rossi (2019) where the energies were assumed to be identical ($E_1 \\equiv E_2$). We establish the asymptotic equivalence of the nodal number with its 4-th chaotic projection and prove quantitative Central Limit Theorems (CLTs) in the 1-Wasserstein distance for the univariate and multivariate scenarios. We provide a corresponding qualitative theorem on the convergence to the White Noise in a sense of random distributions. We compute the exact formula for the asymptotic variance of the nodal number with exact constants depending on the choice of the subsequence. We provide a simple and complete characterisation of this dependency through introduction of the three asymptotic parameters: $r^{log}$, $r$, $r^{exp}$. The corresponding claims in the one-energy model were established in Nourdin, Peccati and Rossi (2019), Peccati and Vidotto (2020), Notarnicola, Peccati and Vidotto (2023), and we recover them as a special case of our results. Moreover, we establish full-correlations with polyspectra, which are analogues of the full-correlation with tri-spectrum that was previously observed for the nodal length in Vidotto (2021).","sentences":["We investigate the fluctuations of the nodal number (count of the phase singularities) in a natural extension of the well-known complex planar Berry Random Wave Model - Berry (2002) - obtained by considering two independent real Berry Random Waves, with distinct energies $E_1, E_2 \\to \\infty$ (at possibly $\\neq$ speeds).","Our framework relaxes the conditions used in Nourdin, Peccati and Rossi (2019) where the energies were assumed to be identical ($E_1 \\equiv E_2$).","We establish the asymptotic equivalence of the nodal number with its 4-th chaotic projection and prove quantitative Central Limit Theorems (CLTs) in the 1-Wasserstein distance for the univariate and multivariate scenarios.","We provide a corresponding qualitative theorem on the convergence to the White Noise in a sense of random distributions.","We compute the exact formula for the asymptotic variance of the nodal number with exact constants depending on the choice of the subsequence.","We provide a simple and complete characterisation of this dependency through introduction of the three asymptotic parameters: $r^{log}$, $r$, $r^{exp}$. The corresponding claims in the one-energy model were established in Nourdin, Peccati and Rossi (2019), Peccati and Vidotto (2020), Notarnicola, Peccati and Vidotto (2023), and we recover them as a special case of our results.","Moreover, we establish full-correlations with polyspectra, which are analogues of the full-correlation with tri-spectrum that was previously observed for the nodal length in Vidotto (2021)."],"url":"http://arxiv.org/abs/2402.18219v1","category":"math.PR"}
{"created":"2024-02-28 10:26:19","title":"Homogeneity of arithmetic quantum limits for hyperbolic $4$-manifolds","abstract":"We work toward the arithmetic quantum unique ergodicity (AQUE) conjecture for sequences of Hecke--Maass forms on hyperbolic $4$-manifolds. We show that limits of such forms can only scar on totally geodesic $3$-submanifolds, and in fact that all ergodic components of the microlocal lift other than the uniform measure arise from the uniform measures on these submanifolds.","sentences":["We work toward the arithmetic quantum unique ergodicity (AQUE) conjecture for sequences of Hecke--Maass forms on hyperbolic $4$-manifolds.","We show that limits of such forms can only scar on totally geodesic $3$-submanifolds, and in fact that all ergodic components of the microlocal lift other than the uniform measure arise from the uniform measures on these submanifolds."],"url":"http://arxiv.org/abs/2402.18218v1","category":"math.NT"}
{"created":"2024-02-28 09:58:09","title":"Dynamical systems on some elliptic modular surfaces via operators on line arrangements","abstract":"This paper further studies the matroid realization space of a specific deformation of the regular $n$-gon with its lines of symmetry. Recently, we obtained that these particular realization spaces are birational to the elliptic modular surfaces $\\Xi_{1}(n)$ over the modular curve $X_1(n)$. Here, we focus on the peculiar cases when $n=7,8$ in more detail. We obtain concrete quartic surfaces in $\\mathbb{P}^3$ equipped with a dominant rational self-map stemming from an operator on line arrangements, which yields K3 surfaces with a dynamical system that is semi-conjugated to the plane.","sentences":["This paper further studies the matroid realization space of a specific deformation of the regular $n$-gon with its lines of symmetry.","Recently, we obtained that these particular realization spaces are birational to the elliptic modular surfaces $\\Xi_{1}(n)$ over the modular curve $X_1(n)$. Here, we focus on the peculiar cases when $n=7,8$ in more detail.","We obtain concrete quartic surfaces in $\\mathbb{P}^3$ equipped with a dominant rational self-map stemming from an operator on line arrangements, which yields K3 surfaces with a dynamical system that is semi-conjugated to the plane."],"url":"http://arxiv.org/abs/2402.18207v1","category":"math.AG"}
{"created":"2024-02-28 09:29:00","title":"Plasma-induced magnetic phase in 3D $\\mathrm{Mn^{II}-Nb^{IV}}$ octacyanidometalate with magnetic sponge behavior","abstract":"A new magnetic phase with $T_C = 72 \\ \\mathrm K$ was obtained by exposing the three-dimensional $\\mathrm{\\{ [Mn^{II}(H_2O)_2]_2[Nb^{IV}(CN)_8] \\cdot 4H_2O \\} _n}$ coordination ferrimagnet ($T_C = 49 \\ \\mathrm K$) to air, oxygen, nitrogen, and argon-based plasma. The X-ray powder diffraction pattern revealed that the unit cell shrank after plasma treatment, leading to a 20% enhancement of the superexchange couplings, as estimated from the mean-field approximation (MFA) model. Although no stable dehydrated form was found in the thermogravimetric analysis, the observed changes are attributed to the removal of crystallization water molecules. The plasma-induced magnetic phase could not be obtained by exposing the studied material to 0% relative humidity during dynamic vapor sorption. Instead, the material underwent a major structural reorganization after dehydration, necessitating an extended MFA model to reproduce the magnetic susceptibility. These findings demonstrate that plasma-induced changes can create unique magnetic phases in molecule-based systems that are otherwise unobtainable.","sentences":["A new magnetic phase with $T_C = 72 \\ \\mathrm K$ was obtained by exposing the three-dimensional $\\mathrm{\\{ [Mn^{II}(H_2O)_2]_2[Nb^{IV}(CN)_8] \\cdot 4H_2O \\} _n}$ coordination ferrimagnet ($T_C = 49 \\ \\mathrm K$) to air, oxygen, nitrogen, and argon-based plasma.","The X-ray powder diffraction pattern revealed that the unit cell shrank after plasma treatment, leading to a 20% enhancement of the superexchange couplings, as estimated from the mean-field approximation (MFA) model.","Although no stable dehydrated form was found in the thermogravimetric analysis, the observed changes are attributed to the removal of crystallization water molecules.","The plasma-induced magnetic phase could not be obtained by exposing the studied material to 0% relative humidity during dynamic vapor sorption.","Instead, the material underwent a major structural reorganization after dehydration, necessitating an extended MFA model to reproduce the magnetic susceptibility.","These findings demonstrate that plasma-induced changes can create unique magnetic phases in molecule-based systems that are otherwise unobtainable."],"url":"http://arxiv.org/abs/2402.18195v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 09:28:36","title":"Formalized Identification Of Key Factors In Safety-Relevant Failure Scenarios","abstract":"This research article presents a methodical data-based approach to systematically identify key factors in safety-related failure scenarios, with a focus on complex product-environmental systems in the era of Industry 4.0. The study addresses the uncertainty arising from the growing complexity of modern products. The method uses scenario analysis and focuses on failure analysis within technical product development. The approach involves a derivation of influencing factors based on information from failure databases. The failures described here are documented individually in failure sequence diagrams and then related to each other in a relationship matrix. This creates a network of possible failure scenarios from individual failure cases that can be used in product development. To illustrate the application of the methodology, a case study of 41 Rapex safety alerts for a hair dryer is presented. The failure sequence diagrams and influencing factor relationship matrices show 46 influencing factors that lead to safety-related failures. The predominant harm is burns and electric shocks, which are highlighted by the active and passive sum diagrams. The research demonstrates a robust method for identifying key factors in safety-related failure scenarios using information from failure databases. The methodology provides valuable insights into product development and emphasizes the frequency of influencing factors and their interconnectedness.","sentences":["This research article presents a methodical data-based approach to systematically identify key factors in safety-related failure scenarios, with a focus on complex product-environmental systems in the era of Industry 4.0.","The study addresses the uncertainty arising from the growing complexity of modern products.","The method uses scenario analysis and focuses on failure analysis within technical product development.","The approach involves a derivation of influencing factors based on information from failure databases.","The failures described here are documented individually in failure sequence diagrams and then related to each other in a relationship matrix.","This creates a network of possible failure scenarios from individual failure cases that can be used in product development.","To illustrate the application of the methodology, a case study of 41 Rapex safety alerts for a hair dryer is presented.","The failure sequence diagrams and influencing factor relationship matrices show 46 influencing factors that lead to safety-related failures.","The predominant harm is burns and electric shocks, which are highlighted by the active and passive sum diagrams.","The research demonstrates a robust method for identifying key factors in safety-related failure scenarios using information from failure databases.","The methodology provides valuable insights into product development and emphasizes the frequency of influencing factors and their interconnectedness."],"url":"http://arxiv.org/abs/2402.18194v1","category":"cs.SE"}
{"created":"2024-02-28 09:28:29","title":"Counting points with Riemann-Roch formulas","abstract":"We provide an algorithm for computing the number of integral points lying in certain triangles that do not have integral vertices. We use techniques from Algebraic Geometry such as the Riemann-Roch formula for weighted projective planes and resolution of singularities. We analyze the complexity of the method and show that the worst case is given by the Fibonacci sequence. At the end of the manuscript a concrete example is developed in detail where the interplay with other invariants of singularity theory is also treated.","sentences":["We provide an algorithm for computing the number of integral points lying in certain triangles that do not have integral vertices.","We use techniques from Algebraic Geometry such as the Riemann-Roch formula for weighted projective planes and resolution of singularities.","We analyze the complexity of the method and show that the worst case is given by the Fibonacci sequence.","At the end of the manuscript a concrete example is developed in detail where the interplay with other invariants of singularity theory is also treated."],"url":"http://arxiv.org/abs/2402.18193v1","category":"math.AG"}
{"created":"2024-02-28 09:22:52","title":"Reliability of Redundant M-Out-Of-N Architectures With Dependent Components: A Comprehensible Approach With Monte Carlo Simulation","abstract":"Redundant architectures can improve the reliability of complex systems. However, component dependencies can affect the architecture and negate the benefit of redundancy. In this paper, we develop three component dependency models and analyze the reliability of different M-out-of-N configurations using Monte Carlo simulation. The first model assumes a linear component dependency. The second and third models consider common cause failures, in the latter for all components and in the second for random groups of components. As expected, the results show that interdependency degrades the reliability of parallel 1ooN systems while improving it for serial NooN systems. Interestingly, 2oo3 systems produce intermediate results that show an improvement in reliability for certain indicators and a deterioration for some others, depending on the type of dependency models. The results show nonlinear properties of MooN systems with dependent components, which suggest careful handling in applications. An online simulation platform based on Monte Carlo Simulation enables product designers to use the models efficiently and achieve tailored results","sentences":["Redundant architectures can improve the reliability of complex systems.","However, component dependencies can affect the architecture and negate the benefit of redundancy.","In this paper, we develop three component dependency models and analyze the reliability of different M-out-of-N configurations using Monte Carlo simulation.","The first model assumes a linear component dependency.","The second and third models consider common cause failures, in the latter for all components and in the second for random groups of components.","As expected, the results show that interdependency degrades the reliability of parallel 1ooN systems while improving it for serial NooN systems.","Interestingly, 2oo3 systems produce intermediate results that show an improvement in reliability for certain indicators and a deterioration for some others, depending on the type of dependency models.","The results show nonlinear properties of MooN systems with dependent components, which suggest careful handling in applications.","An online simulation platform based on Monte Carlo Simulation enables product designers to use the models efficiently and achieve tailored results"],"url":"http://arxiv.org/abs/2402.18187v1","category":"stat.AP"}
{"created":"2024-02-28 09:19:58","title":"A Tampering Risk of Fiber-Based Frequency Synchronization Networks and Its Countermeasures","abstract":"Fiber optic networks are used worldwide and have been regarded as excellent media for transmitting time-frequency (TF) signals. In the past decades, fiber-based TF synchronization techniques have been extensively studied. Instruments based on these techniques have been successfully applied. With the increasing application of TF synchronization instruments, their security has become an important issue. Unfortunately, the security risks of fiber-based frequency synchronization (FbFS) instruments have been overlooked. This paper proposes a frequency tampering method called \"frequency lens\". On a 200 km fiber link, we demonstrate a frequency tampering scenario using a frequency lens-enabled frequency tampering module (FTM). On the user side, the frequency value of the recovered 100 MHz signal can be stealthily altered within a range of 100 MHz-100 Hz to 100 MHz+100 Hz, while the frequency dissemination stability of the system remains normal. Related to this tampering risk, potential hazards in three different application scenarios, which rely on precise frequency references, are analyzed. Two countermeasures are also proposed to solve this tampering risk.","sentences":["Fiber optic networks are used worldwide and have been regarded as excellent media for transmitting time-frequency (TF) signals.","In the past decades, fiber-based TF synchronization techniques have been extensively studied.","Instruments based on these techniques have been successfully applied.","With the increasing application of TF synchronization instruments, their security has become an important issue.","Unfortunately, the security risks of fiber-based frequency synchronization (FbFS) instruments have been overlooked.","This paper proposes a frequency tampering method called \"frequency lens\".","On a 200 km fiber link, we demonstrate a frequency tampering scenario using a frequency lens-enabled frequency tampering module (FTM).","On the user side, the frequency value of the recovered 100 MHz signal can be stealthily altered within a range of 100 MHz-100 Hz to 100 MHz+100 Hz, while the frequency dissemination stability of the system remains normal.","Related to this tampering risk, potential hazards in three different application scenarios, which rely on precise frequency references, are analyzed.","Two countermeasures are also proposed to solve this tampering risk."],"url":"http://arxiv.org/abs/2402.18184v1","category":"physics.ins-det"}
{"created":"2024-02-28 09:07:59","title":"Lindblad dynamics from spatio-temporal correlation functions in nonintegrable spin-1/2 chains with different boundary conditions","abstract":"We investigate the Lindblad equation in the context of boundary-driven magnetization transport in spin-$1/2$ chains. Our central question is whether the nonequilibrium steady state of the open system, including its buildup in time, can be described on the basis of the dynamics in the closed system. To this end, we rely on a previous work [Phys. Rev. B 108, L201119 (2023)], where a description in terms of spatio-temporal correlation functions has been suggested in the case of weak driving and small system-bath coupling. Because this work has focused on integrable systems and periodic boundary conditions, we here extend the analysis in three directions: We (i) consider nonintegrable systems, (ii) take into account open boundary conditions and other bath-coupling geometries, and (iii) provide a comparison to time-evolving block decimation. While we find that nonintegrability plays a minor role, the choice of the specific boundary conditions can be crucial, due to potentially nondecaying edge modes. Our large-scale numerical simulations suggest that a description based on closed-system correlation functions is an useful alternative to already existing state-of-the-art approaches.","sentences":["We investigate the Lindblad equation in the context of boundary-driven magnetization transport in spin-$1/2$ chains.","Our central question is whether the nonequilibrium steady state of the open system, including its buildup in time, can be described on the basis of the dynamics in the closed system.","To this end, we rely on a previous work [Phys. Rev. B 108, L201119 (2023)], where a description in terms of spatio-temporal correlation functions has been suggested in the case of weak driving and small system-bath coupling.","Because this work has focused on integrable systems and periodic boundary conditions, we here extend the analysis in three directions: We (i) consider nonintegrable systems, (ii) take into account open boundary conditions and other bath-coupling geometries, and (iii) provide a comparison to time-evolving block decimation.","While we find that nonintegrability plays a minor role, the choice of the specific boundary conditions can be crucial, due to potentially nondecaying edge modes.","Our large-scale numerical simulations suggest that a description based on closed-system correlation functions is an useful alternative to already existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.18177v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 09:07:26","title":"Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus","abstract":"In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.","sentences":["In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera.","To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera.","In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network.","We also handle the focus breathing phenomenon that occurs in real DfD situations.","Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation."],"url":"http://arxiv.org/abs/2402.18175v1","category":"cs.CV"}
{"created":"2024-02-28 09:03:33","title":"Harnessing the Duality of Magnetism and Conductivity: A Review of Oxide based Dilute Magnetic Semiconductors","abstract":"Over the last two decades, the new branch of spintronics, i.e., semiconductor spintronics, has gained more attention because it integrates the characteristics of conventional semiconductors, such as optical bandgap and charge carriers, helpful for processing and computing pieces of information combined with magnets for data storage applications in a single device. Likewise, substituting transition metal (TM) ions to induce magnetic qualities into semiconductors or oxides creates dilute magnetic semiconductors (DMSs) or oxides (DMOs) with high electronic, photonic, and magnetic functionality. This review article discusses the historical outline of magnetic semiconductors with their origin and mechanism. It also includes a concise overview of various DMO systems based on their conductivity (p-type and n-type) to elucidate the synthesis, origin, and control mechanisms and further evoke the prepared spintronics devices. The occurrence of RTFM with transparency and conductivity can be helpful in spintronics device fabrications, which was assumed to be governed by the formation of intrinsic defects, charge carriers, morphology, and the induced exchange interactions between ions. The DMOs-based spintronics devices, such as magneto-optical devices, transparent ferromagnets, and spin-based solar cells, exploit both semiconducting and magnetic properties, which have also been discussed in this review article with outlook and perspectives.","sentences":["Over the last two decades, the new branch of spintronics, i.e., semiconductor spintronics, has gained more attention because it integrates the characteristics of conventional semiconductors, such as optical bandgap and charge carriers, helpful for processing and computing pieces of information combined with magnets for data storage applications in a single device.","Likewise, substituting transition metal (TM) ions to induce magnetic qualities into semiconductors or oxides creates dilute magnetic semiconductors (DMSs) or oxides (DMOs) with high electronic, photonic, and magnetic functionality.","This review article discusses the historical outline of magnetic semiconductors with their origin and mechanism.","It also includes a concise overview of various DMO systems based on their conductivity (p-type and n-type) to elucidate the synthesis, origin, and control mechanisms and further evoke the prepared spintronics devices.","The occurrence of RTFM with transparency and conductivity can be helpful in spintronics device fabrications, which was assumed to be governed by the formation of intrinsic defects, charge carriers, morphology, and the induced exchange interactions between ions.","The DMOs-based spintronics devices, such as magneto-optical devices, transparent ferromagnets, and spin-based solar cells, exploit both semiconducting and magnetic properties, which have also been discussed in this review article with outlook and perspectives."],"url":"http://arxiv.org/abs/2402.18173v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 09:02:33","title":"NiteDR: Nighttime Image De-Raining with Cross-View Sensor Cooperative Learning for Dynamic Driving Scenes","abstract":"In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation. Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility. Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios. Additionally, driving assistance systems suffer from reduced capabilities in capturing and discerning the surrounding environment, posing a threat to driving safety. Single-view information captured by single-modal sensors cannot comprehensively depict the entire scene. To address these challenges, we developed an image de-raining framework tailored for rainy nighttime driving scenes. It aims to remove rain artifacts, enrich scene representation, and restore useful information. Specifically, we introduce cooperative learning between visible and infrared images captured by different sensors. By cross-view fusion of these multi-source data, the scene within the images gains richer texture details and enhanced contrast. We constructed an information cleaning module called CleanNet as the first stage of our framework. Moreover, we designed an information fusion module called FusionNet as the second stage to fuse the clean visible images with infrared images. Using this stage-by-stage learning strategy, we obtain de-rained fusion images with higher quality and better visual perception. Extensive experiments demonstrate the effectiveness of our proposed Cross-View Cooperative Learning (CVCL) in adverse driving scenarios in low-light rainy environments. The proposed approach addresses the gap in the utilization of existing rain removal algorithms in specific low-light conditions.","sentences":["In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation.","Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility.","Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios.","Additionally, driving assistance systems suffer from reduced capabilities in capturing and discerning the surrounding environment, posing a threat to driving safety.","Single-view information captured by single-modal sensors cannot comprehensively depict the entire scene.","To address these challenges, we developed an image de-raining framework tailored for rainy nighttime driving scenes.","It aims to remove rain artifacts, enrich scene representation, and restore useful information.","Specifically, we introduce cooperative learning between visible and infrared images captured by different sensors.","By cross-view fusion of these multi-source data, the scene within the images gains richer texture details and enhanced contrast.","We constructed an information cleaning module called CleanNet as the first stage of our framework.","Moreover, we designed an information fusion module called FusionNet as the second stage to fuse the clean visible images with infrared images.","Using this stage-by-stage learning strategy, we obtain de-rained fusion images with higher quality and better visual perception.","Extensive experiments demonstrate the effectiveness of our proposed Cross-View Cooperative Learning (CVCL) in adverse driving scenarios in low-light rainy environments.","The proposed approach addresses the gap in the utilization of existing rain removal algorithms in specific low-light conditions."],"url":"http://arxiv.org/abs/2402.18172v1","category":"cs.CV"}
{"created":"2024-02-28 08:55:20","title":"Sequence-level Semantic Representation Fusion for Recommender Systems","abstract":"With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance. Specially, we focus on the utilization of the associated \\emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation. However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective. To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul \\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation, namely \\textbf{\\our}. The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts. The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency domain}. In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features. Our fusion approach can be proved to have the same effects of contextual convolution, so as to achieving sequence-level semantic fusion. In order to further improve the fusion performance, we propose to enhance the discriminability of the text embeddings from the text encoder, by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method. Our implementation is available at this repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}.","sentences":["With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance.","Specially, we focus on the utilization of the associated \\emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation.","However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective.","To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul \\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation, namely \\textbf{\\our}.","The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts.","The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency domain}.","In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features.","Our fusion approach can be proved to have the same effects of contextual convolution, so as to achieving sequence-level semantic fusion.","In order to further improve the fusion performance, we propose to enhance the discriminability of the text embeddings from the text encoder, by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method.","Our implementation is available at this repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}."],"url":"http://arxiv.org/abs/2402.18166v1","category":"cs.IR"}
{"created":"2024-02-28 08:40:07","title":"Isometric embedding of the n-point spaces into the space of spaces for $n \\leq 4$","abstract":"In [The Space of Spaces: Curvature Bounds and Gradient Flows on the Space of Metric Measure Spaces. Memoirs of the American Mathematical Society. American Mathematical Society, 2023], Sturm studied the space of all metric measure spaces up to isomorphism which he called The space of spaces. He also introduced for a natural number n the space of all n-points metric spaces. The aim of this article is to study if the embedding of this space in the space of spaces is isometric. Using results from [Haggai Maron and Yaron Lipman. (probably) concave graph matching. Advances in Neural Information Processing Systems, 31, 2018] and [Hiroshi Maehara. Euclidean embeddings of finite metric spaces. Discrete Mathematics, 2013], we prove that it is the case for $n \\leq 4$ and for Euclidean metric spaces.","sentences":["In [The Space of Spaces: Curvature Bounds and Gradient Flows on the Space of Metric Measure Spaces.","Memoirs of the American Mathematical Society.","American Mathematical Society, 2023], Sturm studied the space of all metric measure spaces up to isomorphism which he called The space of spaces.","He also introduced for a natural number n the space of all n-points metric spaces.","The aim of this article is to study if the embedding of this space in the space of spaces is isometric.","Using results from [Haggai Maron and Yaron Lipman.","(probably) concave graph matching.","Advances in Neural Information Processing Systems, 31, 2018] and [Hiroshi Maehara.","Euclidean embeddings of finite metric spaces.","Discrete Mathematics, 2013], we prove that it is the case for $n \\leq 4$ and for Euclidean metric spaces."],"url":"http://arxiv.org/abs/2402.18156v1","category":"math.MG"}
{"created":"2024-02-28 08:06:32","title":"Enhanced micromotion compensation using a phase modulated light field","abstract":"We investigate sideband spectroscopy of a trapped ion using a probe laser phase modulated at the trap drive frequency. The enhanced sensitivity of our technique over traditional sideband spectroscopy allows us to detect stray fields of $0.01\\,\\mathrm{V/m}$ on a timescale of a few minutes and detect differential phases of $5\\,\\mu\\mathrm{rad}$ between applied ac potentials. We also demonstrate the ability suppress Doppler shifts from excess motion to well below the limit imposed by the intrinsic motion of the ion in the vibrational ground-state. The technique we introduce can be readily implemented in any ion trap system that utilizes sideband spectroscopy for micromotion compensation and can be seamlessly integrated into experiments in a fully automated way","sentences":["We investigate sideband spectroscopy of a trapped ion using a probe laser phase modulated at the trap drive frequency.","The enhanced sensitivity of our technique over traditional sideband spectroscopy allows us to detect stray fields of $0.01\\,\\mathrm{V/m}$ on a timescale of a few minutes and detect differential phases of $5\\,\\mu\\mathrm{rad}$ between applied ac potentials.","We also demonstrate the ability suppress Doppler shifts from excess motion to well below the limit imposed by the intrinsic motion of the ion in the vibrational ground-state.","The technique we introduce can be readily implemented in any ion trap system that utilizes sideband spectroscopy for micromotion compensation and can be seamlessly integrated into experiments in a fully automated way"],"url":"http://arxiv.org/abs/2402.18142v1","category":"physics.atom-ph"}
{"created":"2024-02-28 06:54:38","title":"Large Deviation Principle for Multi-Scale Fully Local Monotone Stochastic Dynamical Systems with Multiplicative Noise","abstract":"This paper is devoted to proving the small noise asymptotic behaviour, particularly large deviation principle, for multi-scale stochastic dynamical systems with fully local monotone coefficients driven by multiplicative noise. The main techniques are based on a combination of the weak convergence approach, the time discretization technique and the theory of pseudo-monotone operator. The main results derived in this paper have broad applicability to various multi-scale models, where the slow component could be such as stochastic porous medium equations, stochastic Cahn-Hilliard equations and stochastic 2D Liquid crystal equations.","sentences":["This paper is devoted to proving the small noise asymptotic behaviour, particularly large deviation principle, for multi-scale stochastic dynamical systems with fully local monotone coefficients driven by multiplicative noise.","The main techniques are based on a combination of the weak convergence approach, the time discretization technique and the theory of pseudo-monotone operator.","The main results derived in this paper have broad applicability to various multi-scale models, where the slow component could be such as stochastic porous medium equations, stochastic Cahn-Hilliard equations and stochastic 2D Liquid crystal equations."],"url":"http://arxiv.org/abs/2402.18108v1","category":"math.PR"}
{"created":"2024-02-28 06:49:20","title":"Neurological disorders leading to mechanical dysfunction of the esophagus: an emergent behavior of a neuromechanical dynamical system","abstract":"An understanding how neurological disorders lead to mechanical dysfunction of the esophagus requires knowledge of the neural circuit of the enteric nervous system. Historically, this has been elusive. Here, we present an empirically guided neural circuit for the esophagus. It has a chain of unidirectionally coupled relaxation oscillators, receiving excitatory signals from stretch receptors along the esophagus. The resulting neuromechanical model reveals complex patterns and behaviors that emerge from interacting components in the system. A wide variety of clinically observed normal and abnormal esophageal responses to distension are successfully predicted. Specifically, repetitive antegrade contractions (RACs) are conclusively shown to emerge from the coupled neuromechanical dynamics in response to sustained volumetric distension. Normal RACs are shown to have a robust balance between excitatory and inhibitory neuronal populations, and the mechanical input through stretch receptors. When this balance is affected, contraction patterns akin to motility disorders are observed. For example, clinically observed repetitive retrograde contractions emerge due to a hyper stretch sensitive wall. Such neuromechanical insights could be crucial to eventually develop targeted pharmacological interventions.","sentences":["An understanding how neurological disorders lead to mechanical dysfunction of the esophagus requires knowledge of the neural circuit of the enteric nervous system.","Historically, this has been elusive.","Here, we present an empirically guided neural circuit for the esophagus.","It has a chain of unidirectionally coupled relaxation oscillators, receiving excitatory signals from stretch receptors along the esophagus.","The resulting neuromechanical model reveals complex patterns and behaviors that emerge from interacting components in the system.","A wide variety of clinically observed normal and abnormal esophageal responses to distension are successfully predicted.","Specifically, repetitive antegrade contractions (RACs) are conclusively shown to emerge from the coupled neuromechanical dynamics in response to sustained volumetric distension.","Normal RACs are shown to have a robust balance between excitatory and inhibitory neuronal populations, and the mechanical input through stretch receptors.","When this balance is affected, contraction patterns akin to motility disorders are observed.","For example, clinically observed repetitive retrograde contractions emerge due to a hyper stretch sensitive wall.","Such neuromechanical insights could be crucial to eventually develop targeted pharmacological interventions."],"url":"http://arxiv.org/abs/2402.18103v1","category":"q-bio.NC"}
{"created":"2024-02-28 06:43:43","title":"Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context","abstract":"In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students' first language (L1), remained particularly challenging.","sentences":["In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples.","With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark.","Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset.","Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset.","For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness.","Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant.","Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures.","Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students' first language (L1), remained particularly challenging."],"url":"http://arxiv.org/abs/2402.18101v1","category":"cs.CL"}
{"created":"2024-02-28 06:43:20","title":"Direct and retrograde signal propagation in unidirectionally coupled Wilson-Cowan oscillators","abstract":"Certain biological systems exhibit both direct and retrograde propagating wave signals, despite unidirectional neural coupling. However, there is no model to explain this. Therefore, the underlying physics of reversing the signal's direction for one-way coupling remains unclear. Here, we resolve this issue using a Wilson-Cowan oscillators network. By analyzing the limit cycle period of various coupling configurations, we determine that intrinsic frequency differences among oscillators control wave directionality.","sentences":["Certain biological systems exhibit both direct and retrograde propagating wave signals, despite unidirectional neural coupling.","However, there is no model to explain this.","Therefore, the underlying physics of reversing the signal's direction for one-way coupling remains unclear.","Here, we resolve this issue using a Wilson-Cowan oscillators network.","By analyzing the limit cycle period of various coupling configurations, we determine that intrinsic frequency differences among oscillators control wave directionality."],"url":"http://arxiv.org/abs/2402.18100v1","category":"physics.bio-ph"}
{"created":"2024-02-28 06:34:01","title":"Exergetic Port-Hamiltonian Systems for Multibody Dynamics","abstract":"Multibody dynamics simulation plays an important role in various fields, including mechanical engineering, robotics, and biomechanics. Setting up computational models however becomes increasingly challenging as systems grow in size and complexity. Especially the consistent combination of models across different physical domains usually demands a lot of attention. This motivates us to study formal languages for compositional modeling of multiphysical systems. This article shows how multibody systems, or more precisely assemblies of rigid bodies connected by lower kinematic pairs, fit into the framework of Exergetic Port-Hamiltonian Systems (EPHS). This approach is based on the hierarchical decomposition of systems into their ultimately primitive components, using a simple graphical syntax. Thereby, cognitive load can be reduced and communication is facilitated, even with non-experts. Moreover, the encapsulation and reuse of subsystems promotes efficient model development and management. In contrast to established modeling languages such as Modelica, the primitive components of EPHS are not defined by arbitrary equations. Instead, there are four kinds of components, each defined by a particular geometric structure with a clear physical interpretation. This higher-level approach could make the process of building and maintaining large-scale models simpler and also safer.","sentences":["Multibody dynamics simulation plays an important role in various fields, including mechanical engineering, robotics, and biomechanics.","Setting up computational models however becomes increasingly challenging as systems grow in size and complexity.","Especially the consistent combination of models across different physical domains usually demands a lot of attention.","This motivates us to study formal languages for compositional modeling of multiphysical systems.","This article shows how multibody systems, or more precisely assemblies of rigid bodies connected by lower kinematic pairs, fit into the framework of Exergetic Port-Hamiltonian Systems (EPHS).","This approach is based on the hierarchical decomposition of systems into their ultimately primitive components, using a simple graphical syntax.","Thereby, cognitive load can be reduced and communication is facilitated, even with non-experts.","Moreover, the encapsulation and reuse of subsystems promotes efficient model development and management.","In contrast to established modeling languages such as Modelica, the primitive components of EPHS are not defined by arbitrary equations.","Instead, there are four kinds of components, each defined by a particular geometric structure with a clear physical interpretation.","This higher-level approach could make the process of building and maintaining large-scale models simpler and also safer."],"url":"http://arxiv.org/abs/2402.18095v1","category":"eess.SY"}
{"created":"2024-02-28 06:28:15","title":"ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection","abstract":"The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in spam filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as spam, risking the possibility of missing important communications or mistakenly trusting phishing emails.   This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.","sentences":["The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts.","Despite advances in spam filters and email security protocols, problems with oversight and false positives persist.","Users often struggle to understand why emails are flagged as spam, risking the possibility of missing important communications or mistakenly trusting phishing emails.   ","This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails.","By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not.","Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails.","We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems.","We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%.","Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats."],"url":"http://arxiv.org/abs/2402.18093v1","category":"cs.CR"}
{"created":"2024-02-28 06:20:30","title":"Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera Force Control: Cooperative vs. Teleoperation Strategies","abstract":"Performing intricate eye microsurgery, such as retinal vein cannulation (RVC), as a potential treatment for retinal vein occlusion (RVO), without the assistance of a surgical robotic system is very challenging to do safely. The main limitation has to do with the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not address crucial safety considerations, such as minimizing contact force between the surgical instrument and the sclera surface to prevent tissue damage. An adaptive sclera force control algorithm was proposed to address this limitation using an FBG-based force-sensing tool to measure and minimize the tool-sclera interaction force. Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. We implemented a bimanual adaptive teleoperation (BMAT) control mode using SHER 2.0 and SHER 2.1 and compared its performance with a bimanual adaptive cooperative (BMAC) mode. Both BMAT and BMAC modes were tested in sitting and standing postures during a vessel-following experiment under a surgical microscope. It is shown, for the first time to the best of our knowledge in robot-assisted retinal surgery, that integrating the adaptive sclera force control algorithm with the bimanual teleoperation framework enables surgeons to safely perform bimanual telemanipulation of the eye without over-stretching it, even in the absence of registration between the two robots.","sentences":["Performing intricate eye microsurgery, such as retinal vein cannulation (RVC), as a potential treatment for retinal vein occlusion (RVO), without the assistance of a surgical robotic system is very challenging to do safely.","The main limitation has to do with the physiological hand tremor of surgeons.","Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC.","The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively.","However, the admittance-based cooperative control mode does not address crucial safety considerations, such as minimizing contact force between the surgical instrument and the sclera surface to prevent tissue damage.","An adaptive sclera force control algorithm was proposed to address this limitation using an FBG-based force-sensing tool to measure and minimize the tool-sclera interaction force.","Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework.","We implemented a bimanual adaptive teleoperation (BMAT) control mode using SHER 2.0 and SHER 2.1 and compared its performance with a bimanual adaptive cooperative (BMAC) mode.","Both BMAT and BMAC modes were tested in sitting and standing postures during a vessel-following experiment under a surgical microscope.","It is shown, for the first time to the best of our knowledge in robot-assisted retinal surgery, that integrating the adaptive sclera force control algorithm with the bimanual teleoperation framework enables surgeons to safely perform bimanual telemanipulation of the eye without over-stretching it, even in the absence of registration between the two robots."],"url":"http://arxiv.org/abs/2402.18088v1","category":"cs.RO"}
{"created":"2024-02-28 06:17:55","title":"AI-assisted Tagging of Deepfake Audio Calls using Challenge-Response","abstract":"Scammers are aggressively leveraging AI voice-cloning technology for social engineering attacks, a situation significantly worsened by the advent of audio Real-time Deepfakes (RTDFs). RTDFs can clone a target's voice in real-time over phone calls, making these interactions highly interactive and thus far more convincing. Our research confidently addresses the gap in the existing literature on deepfake detection, which has largely been ineffective against RTDF threats. We introduce a robust challenge-response-based method to detect deepfake audio calls, pioneering a comprehensive taxonomy of audio challenges. Our evaluation pitches 20 prospective challenges against a leading voice-cloning system. We have compiled a novel open-source challenge dataset with contributions from 100 smartphone and desktop users, yielding 18,600 original and 1.6 million deepfake samples. Through rigorous machine and human evaluations of this dataset, we achieved a deepfake detection rate of 86% and an 80% AUC score, respectively. Notably, utilizing a set of 11 challenges significantly enhances detection capabilities. Our findings reveal that combining human intuition with machine precision offers complementary advantages. Consequently, we have developed an innovative human-AI collaborative system that melds human discernment with algorithmic accuracy, boosting final joint accuracy to 82.9%. This system highlights the significant advantage of AI-assisted pre-screening in call verification processes. Samples can be heard at https://mittalgovind.github.io/autch-samples/","sentences":["Scammers are aggressively leveraging AI voice-cloning technology for social engineering attacks, a situation significantly worsened by the advent of audio Real-time Deepfakes (RTDFs).","RTDFs can clone a target's voice in real-time over phone calls, making these interactions highly interactive and thus far more convincing.","Our research confidently addresses the gap in the existing literature on deepfake detection, which has largely been ineffective against RTDF threats.","We introduce a robust challenge-response-based method to detect deepfake audio calls, pioneering a comprehensive taxonomy of audio challenges.","Our evaluation pitches 20 prospective challenges against a leading voice-cloning system.","We have compiled a novel open-source challenge dataset with contributions from 100 smartphone and desktop users, yielding 18,600 original and 1.6 million deepfake samples.","Through rigorous machine and human evaluations of this dataset, we achieved a deepfake detection rate of 86% and an 80% AUC score, respectively.","Notably, utilizing a set of 11 challenges significantly enhances detection capabilities.","Our findings reveal that combining human intuition with machine precision offers complementary advantages.","Consequently, we have developed an innovative human-AI collaborative system that melds human discernment with algorithmic accuracy, boosting final joint accuracy to 82.9%.","This system highlights the significant advantage of AI-assisted pre-screening in call verification processes.","Samples can be heard at https://mittalgovind.github.io/autch-samples/"],"url":"http://arxiv.org/abs/2402.18085v1","category":"cs.SD"}
{"created":"2024-02-28 06:17:01","title":"Spannotation: Enhancing Semantic Segmentation for Autonomous Navigation with Efficient Image Annotation","abstract":"Spannotation is an open source user-friendly tool developed for image annotation for semantic segmentation specifically in autonomous navigation tasks. This study provides an evaluation of Spannotation, demonstrating its effectiveness in generating accurate segmentation masks for various environments like agricultural crop rows, off-road terrains and urban roads. Unlike other popular annotation tools that requires about 40 seconds to annotate an image for semantic segmentation in a typical navigation task, Spannotation achieves similar result in about 6.03 seconds. The tools utility was validated through the utilization of its generated masks to train a U-Net model which achieved a validation accuracy of 98.27% and mean Intersection Over Union (mIOU) of 96.66%. The accessibility, simple annotation process and no-cost features have all contributed to the adoption of Spannotation evident from its download count of 2098 (as of February 25, 2024) since its launch. Future enhancements of Spannotation aim to broaden its application to complex navigation scenarios and incorporate additional automation functionalities. Given its increasing popularity and promising potential, Spannotation stands as a valuable resource in autonomous navigation and semantic segmentation. For detailed information and access to Spannotation, readers are encouraged to visit the project's GitHub repository at https://github.com/sof-danny/spannotation","sentences":["Spannotation is an open source user-friendly tool developed for image annotation for semantic segmentation specifically in autonomous navigation tasks.","This study provides an evaluation of Spannotation, demonstrating its effectiveness in generating accurate segmentation masks for various environments like agricultural crop rows, off-road terrains and urban roads.","Unlike other popular annotation tools that requires about 40 seconds to annotate an image for semantic segmentation in a typical navigation task, Spannotation achieves similar result in about 6.03 seconds.","The tools utility was validated through the utilization of its generated masks to train a U-Net model which achieved a validation accuracy of 98.27% and mean Intersection Over Union (mIOU) of 96.66%.","The accessibility, simple annotation process and no-cost features have all contributed to the adoption of Spannotation evident from its download count of 2098 (as of February 25, 2024) since its launch.","Future enhancements of Spannotation aim to broaden its application to complex navigation scenarios and incorporate additional automation functionalities.","Given its increasing popularity and promising potential, Spannotation stands as a valuable resource in autonomous navigation and semantic segmentation.","For detailed information and access to Spannotation, readers are encouraged to visit the project's GitHub repository at https://github.com/sof-danny/spannotation"],"url":"http://arxiv.org/abs/2402.18084v1","category":"cs.CV"}
{"created":"2024-02-28 06:14:49","title":"Study of superconductivity of very thin $\\mathrm{FeSe}_{1-x}\\mathrm{Te}_x$ films investigated by microwave complex conductivity measurements","abstract":"Complex conductivity measurements spanning the entire temperature range, including the vicinity of $T_c$, were conducted on systematically varied FeSe$_{1-x}$Te$_x$ ($x$ = 0 - 0.5) very thin films. By applying a novel cavity measurement technique employing microwave electric fields parallel to FeSe$_{1-x}$Te$_x$ films, we observed distinct temperature-dependent alterations in superfluid fraction and quasiparticle scattering rate at the nematic boundary. These changes in the nematic boundary suggests variations in the superconducting gap structure between samples in the nematic and non-nematic phase. Moreover, fluctuation is visible up to 1.2 $T_c$ irrespective of nematic order, consistent with large superconducting fluctuations in iron chalcogenide superconductors reported previously in [H. Takahashi \\textit{et al}, Phys. Rev. B 99, 060503(R) (1982)] and [F. Nabeshima \\textit{et al}, Phys. Rev. B 97, 024504(R) (1982)].","sentences":["Complex conductivity measurements spanning the entire temperature range, including the vicinity of $T_c$, were conducted on systematically varied FeSe$_{1-x}$Te$_x$ ($x$ = 0 - 0.5) very thin films.","By applying a novel cavity measurement technique employing microwave electric fields parallel to FeSe$_{1-x}$Te$_x$ films, we observed distinct temperature-dependent alterations in superfluid fraction and quasiparticle scattering rate at the nematic boundary.","These changes in the nematic boundary suggests variations in the superconducting gap structure between samples in the nematic and non-nematic phase.","Moreover, fluctuation is visible up to 1.2 $","T_c$ irrespective of nematic order, consistent with large superconducting fluctuations in iron chalcogenide superconductors reported previously in [H. Takahashi \\textit{et al}, Phys.","Rev. B 99, 060503(R) (1982)]","and [F. Nabeshima \\textit{et al}, Phys.","Rev. B 97, 024504(R) (1982)]."],"url":"http://arxiv.org/abs/2402.18082v1","category":"cond-mat.supr-con"}
{"created":"2024-02-28 06:13:20","title":"Dynamics Around the Earth-Moon Triangular Points in the Hill Restricted 4-Body Problem","abstract":"This paper investigates the motion of a small particle moving near the triangular points of the Earth-Moon system. The dynamics are modeled in the Hill restricted 4-body problem (HR4BP), which includes the effect of the Earth and Moon as in the circular restricted 3-body problem (CR3BP), as well as the direct and indirect effect of the Sun as a periodic time-dependent perturbation of the CR3BP. Due to the periodic perturbation, the triangular points of the CR3BP are no longer equilibrium solutions; rather, the triangular points are replaced by periodic orbits with the same period as the perturbation. Additionally, there is a 2:1 resonant periodic orbit that persists from the CR3BP into the HR4BP. In this work, we investigate the dynamics around these invariant objects by computing families of 2-dimensional invariant tori and their linear normal behavior. We identify bifurcations and relationships between families. Mechanisms for transport between Earth, L4, and Moon are discussed. Comparisons are made between the results presented here and in the bicircular problem (BCP).","sentences":["This paper investigates the motion of a small particle moving near the triangular points of the Earth-Moon system.","The dynamics are modeled in the Hill restricted 4-body problem (HR4BP), which includes the effect of the Earth and Moon as in the circular restricted 3-body problem (CR3BP), as well as the direct and indirect effect of the Sun as a periodic time-dependent perturbation of the CR3BP.","Due to the periodic perturbation, the triangular points of the CR3BP are no longer equilibrium solutions; rather, the triangular points are replaced by periodic orbits with the same period as the perturbation.","Additionally, there is a 2:1 resonant periodic orbit that persists from the CR3BP into the HR4BP.","In this work, we investigate the dynamics around these invariant objects by computing families of 2-dimensional invariant tori and their linear normal behavior.","We identify bifurcations and relationships between families.","Mechanisms for transport between Earth, L4, and Moon are discussed.","Comparisons are made between the results presented here and in the bicircular problem (BCP)."],"url":"http://arxiv.org/abs/2402.18081v1","category":"math.DS"}
{"created":"2024-02-28 06:10:34","title":"Challenges in addressing student difficulties with measurement uncertainty of two-state quantum systems using a multiple-choice question sequence in online and in-person classes","abstract":"Research-validated multiple-choice questions comprise an easy-to-implement instructional tool that serves to scaffold student learning and formatively assess students knowledge. We present findings from the implementation, in consecutive years, of research-validated multiple-choice question sequence on measurement uncertainty as it applies to two-state quantum systems. This study was conducted in an advanced undergraduate quantum mechanics course, in an online and in-person learning environments in consecutive years. Student learning was assessed after receiving traditional lecture-based instruction in relevant concepts, and their performance was compared with that on a similar assessment given after engaging with the multiple-choice question sequence. We analyze and discuss the similar and differing trends observed in the two modes of instruction.","sentences":["Research-validated multiple-choice questions comprise an easy-to-implement instructional tool that serves to scaffold student learning and formatively assess students knowledge.","We present findings from the implementation, in consecutive years, of research-validated multiple-choice question sequence on measurement uncertainty as it applies to two-state quantum systems.","This study was conducted in an advanced undergraduate quantum mechanics course, in an online and in-person learning environments in consecutive years.","Student learning was assessed after receiving traditional lecture-based instruction in relevant concepts, and their performance was compared with that on a similar assessment given after engaging with the multiple-choice question sequence.","We analyze and discuss the similar and differing trends observed in the two modes of instruction."],"url":"http://arxiv.org/abs/2402.18080v1","category":"physics.ed-ph"}
{"created":"2024-02-28 06:07:46","title":"The Neumann-Moser dynamical system and the Korteweg-de Vries hierarchy","abstract":"At the focus of the paper are applications of the well-known Moser transformation of the C. Neumann dynamical system. It yields us a new quadratic integrable dynamical system on $\\mathbb{C}^{3n+1}$, which we call the Neumann-Moser dynamical system. We present an explicit formula of the inverse of the Moser transformation. Consequently, we obtain explicitly an invertible transformation of the Uhlenbeck-Devaney integrals of the Neumann system into the integrals of our system. One of the main results of the paper is the recurrent solutions of the Neumann-Moser system. We show that every solution of our system solves the Mumford dynamical system, and vice versa. Every solution of the Neumann-Moser system is proven to solve the stationary Korteweg-de Vries hierarchy. As a corollary, we construct explicit solutions of the Neumann-Moser system in hyperelliptic Kleinian functions.","sentences":["At the focus of the paper are applications of the well-known Moser transformation of the C. Neumann dynamical system.","It yields us a new quadratic integrable dynamical system on $\\mathbb{C}^{3n+1}$, which we call the Neumann-Moser dynamical system.","We present an explicit formula of the inverse of the Moser transformation.","Consequently, we obtain explicitly an invertible transformation of the Uhlenbeck-Devaney integrals of the Neumann system into the integrals of our system.","One of the main results of the paper is the recurrent solutions of the Neumann-Moser system.","We show that every solution of our system solves the Mumford dynamical system, and vice versa.","Every solution of the Neumann-Moser system is proven to solve the stationary Korteweg-de Vries hierarchy.","As a corollary, we construct explicit solutions of the Neumann-Moser system in hyperelliptic Kleinian functions."],"url":"http://arxiv.org/abs/2402.18079v1","category":"nlin.SI"}
{"created":"2024-02-28 06:06:52","title":"Challenges in addressing student difficulties with time-development of two-state quantum systems using a multiple-choice question sequence in virtual and in-person classes","abstract":"Research-validated clicker questions as instructional tools for formative assessment are relatively easy to implement and can provide effective scaffolding when developed and implemented in a sequence. We present findings from the implementation of a research-validated clicker question sequence (CQS) on student understanding of the time-development of two-state quantum systems. This study was conducted in an advanced undergraduate quantum mechanics course for two consecutive years in virtual and in-person classes. The effectiveness of the CQS discussed here in both modes of instruction was determined by evaluating students' performance after traditional lecture-based instruction and comparing it to their performance after engaging with the CQS.","sentences":["Research-validated clicker questions as instructional tools for formative assessment are relatively easy to implement and can provide effective scaffolding when developed and implemented in a sequence.","We present findings from the implementation of a research-validated clicker question sequence (CQS) on student understanding of the time-development of two-state quantum systems.","This study was conducted in an advanced undergraduate quantum mechanics course for two consecutive years in virtual and in-person classes.","The effectiveness of the CQS discussed here in both modes of instruction was determined by evaluating students' performance after traditional lecture-based instruction and comparing it to their performance after engaging with the CQS."],"url":"http://arxiv.org/abs/2402.18075v1","category":"physics.ed-ph"}
{"created":"2024-02-28 06:06:52","title":"Online Ecological Gearshift Strategy via Neural Network with Soft-Argmax Operator","abstract":"This paper presents a neural network optimizer with soft-argmax operator to achieve an ecological gearshift strategy in real-time. The strategy is reformulated as the mixed-integer model predictive control (MIMPC) problem to minimize energy consumption. Then the outer convexification is introduced to transform integer variables into relaxed binary controls. To approximate binary solutions properly within training, the soft-argmax operator is applied to the neural network with the fact that all the operations of this scheme are differentiable. Moreover, this operator can help push the relaxed binary variables close to 0 or 1. To evaluate the strategy effect, we deployed it to a 2-speed electric vehicle (EV). In contrast to the mature solver Bonmin, our proposed method not only achieves similar energy-saving effects but also significantly reduces the solution time to meet real-time requirements. This results in a notable energy savings of 6.02% compared to the rule-based method.","sentences":["This paper presents a neural network optimizer with soft-argmax operator to achieve an ecological gearshift strategy in real-time.","The strategy is reformulated as the mixed-integer model predictive control (MIMPC) problem to minimize energy consumption.","Then the outer convexification is introduced to transform integer variables into relaxed binary controls.","To approximate binary solutions properly within training, the soft-argmax operator is applied to the neural network with the fact that all the operations of this scheme are differentiable.","Moreover, this operator can help push the relaxed binary variables close to 0 or 1.","To evaluate the strategy effect, we deployed it to a 2-speed electric vehicle (EV).","In contrast to the mature solver Bonmin, our proposed method not only achieves similar energy-saving effects but also significantly reduces the solution time to meet real-time requirements.","This results in a notable energy savings of 6.02% compared to the rule-based method."],"url":"http://arxiv.org/abs/2402.18076v1","category":"eess.SY"}
{"created":"2024-02-28 06:03:48","title":"Tensor Network Space-Time Spectral Collocation Method for Time Dependent Convection-Diffusion-Reaction Equations","abstract":"Emerging tensor network techniques for solutions of Partial Differential Equations (PDEs), known for their ability to break the curse of dimensionality, deliver new mathematical methods for ultrafast numerical solutions of high-dimensional problems. Here, we introduce a Tensor Train (TT) Chebyshev spectral collocation method, in both space and time, for solution of the time dependent convection-diffusion-reaction (CDR) equation with inhomogeneous boundary conditions, in Cartesian geometry. Previous methods for numerical solution of time dependent PDEs often use finite difference for time, and a spectral scheme for the spatial dimensions, which leads to slow linear convergence. Spectral collocation space-time methods show exponential convergence, however, for realistic problems they need to solve large four-dimensional systems. We overcome this difficulty by using a TT approach as its complexity only grows linearly with the number of dimensions. We show that our TT space-time Chebyshev spectral collocation method converges exponentially, when the solution of the CDR is smooth, and demonstrate that it leads to very high compression of linear operators from terabytes to kilobytes in TT-format, and tens of thousands times speedup when compared to full grid space-time spectral method. These advantages allow us to obtain the solutions at much higher resolutions.","sentences":["Emerging tensor network techniques for solutions of Partial Differential Equations (PDEs), known for their ability to break the curse of dimensionality, deliver new mathematical methods for ultrafast numerical solutions of high-dimensional problems.","Here, we introduce a Tensor Train (TT)","Chebyshev spectral collocation method, in both space and time, for solution of the time dependent convection-diffusion-reaction (CDR) equation with inhomogeneous boundary conditions, in Cartesian geometry.","Previous methods for numerical solution of time dependent PDEs often use finite difference for time, and a spectral scheme for the spatial dimensions, which leads to slow linear convergence.","Spectral collocation space-time methods show exponential convergence, however, for realistic problems they need to solve large four-dimensional systems.","We overcome this difficulty by using a TT approach as its complexity only grows linearly with the number of dimensions.","We show that our TT space-time Chebyshev spectral collocation method converges exponentially, when the solution of the CDR is smooth, and demonstrate that it leads to very high compression of linear operators from terabytes to kilobytes in TT-format, and tens of thousands times speedup when compared to full grid space-time spectral method.","These advantages allow us to obtain the solutions at much higher resolutions."],"url":"http://arxiv.org/abs/2402.18073v1","category":"math.NA"}
{"created":"2024-02-28 06:03:00","title":"Challenges in addressing student difficulties with quantum measurement of two-state quantum systems using a multiple-choice question sequence in online and in-person classes","abstract":"Research-validated multiple-choice questions comprise an easy-to-implement instructional tool that serves to scaffold student learning and formatively assess students' knowledge. We present findings from the implementation, in consecutive years, of a research-validated multiple-choice question sequence [referred to in this study as a Clicker Question Sequence (CQS)] on quantum measurement as it applies to two-state quantum systems. This study was conducted in an advanced undergraduate quantum mechanics course, in both online and in-person learning environments across three years. Student learning was assessed after traditional lecture-based instruction in relevant concepts, and their performance was compared with that on a similar assessment given after engaging with the CQS. We analyze, compare, and discuss the trends observed in the three implementations.","sentences":["Research-validated multiple-choice questions comprise an easy-to-implement instructional tool that serves to scaffold student learning and formatively assess students' knowledge.","We present findings from the implementation, in consecutive years, of a research-validated multiple-choice question sequence [referred to in this study as a Clicker Question Sequence (CQS)] on quantum measurement as it applies to two-state quantum systems.","This study was conducted in an advanced undergraduate quantum mechanics course, in both online and in-person learning environments across three years.","Student learning was assessed after traditional lecture-based instruction in relevant concepts, and their performance was compared with that on a similar assessment given after engaging with the CQS.","We analyze, compare, and discuss the trends observed in the three implementations."],"url":"http://arxiv.org/abs/2402.18072v1","category":"physics.ed-ph"}
{"created":"2024-02-28 06:01:53","title":"Improved uniform error bounds for long-time dynamics of the high-dimensional nonlinear space fractional sine-Gordon equation with weak nonlinearity","abstract":"In this paper, we derive the improved uniform error bounds for the long-time dynamics of the $d$-dimensional $(d=2,3)$ nonlinear space fractional sine-Gordon equation (NSFSGE). The nonlinearity strength of the NSFSGE is characterized by $\\varepsilon^2$ where $0<\\varepsilon \\le 1$ is a dimensionless parameter. The second-order time-splitting method is applied to the temporal discretization and the Fourier pseudo-spectral method is used for the spatial discretization. To obtain the explicit relation between the numerical errors and the parameter $\\varepsilon$, we introduce the regularity compensation oscillation technique to the convergence analysis of fractional models. Then we establish the improved uniform error bounds $O\\left(\\varepsilon^2 \\tau^2\\right)$ for the semi-discretization scheme and $O\\left(h^m+\\varepsilon^2 \\tau^2\\right)$ for the full-discretization scheme up to the long time at $O(1/\\varepsilon^2)$. Further, we extend the time-splitting Fourier pseudo-spectral method to the complex NSFSGE as well as the oscillatory complex NSFSGE, and the improved uniform error bounds for them are also given. Finally, extensive numerical examples in two-dimension or three-dimension are provided to support the theoretical analysis. The differences in dynamic behaviors between the fractional sine-Gordon equation and classical sine-Gordon equation are also discussed.","sentences":["In this paper, we derive the improved uniform error bounds for the long-time dynamics of the $d$-dimensional $(d=2,3)$ nonlinear space fractional sine-Gordon equation (NSFSGE).","The nonlinearity strength of the NSFSGE is characterized by $\\varepsilon^2$ where $0<\\varepsilon \\le 1$ is a dimensionless parameter.","The second-order time-splitting method is applied to the temporal discretization and the Fourier pseudo-spectral method is used for the spatial discretization.","To obtain the explicit relation between the numerical errors and the parameter $\\varepsilon$, we introduce the regularity compensation oscillation technique to the convergence analysis of fractional models.","Then we establish the improved uniform error bounds $O\\left(\\varepsilon^2 \\tau^2\\right)$ for the semi-discretization scheme and $O\\left(h^m+\\varepsilon^2 \\tau^2\\right)$ for the full-discretization scheme up to the long time at $O(1/\\varepsilon^2)$. Further, we extend the time-splitting Fourier pseudo-spectral method to the complex NSFSGE as well as the oscillatory complex NSFSGE, and the improved uniform error bounds for them are also given.","Finally, extensive numerical examples in two-dimension or three-dimension are provided to support the theoretical analysis.","The differences in dynamic behaviors between the fractional sine-Gordon equation and classical sine-Gordon equation are also discussed."],"url":"http://arxiv.org/abs/2402.18071v1","category":"math.NA"}
{"created":"2024-02-28 05:57:02","title":"Challenges in addressing student difficulties with basics and change of basis for two-state quantum systems using a multiple-choice question sequence in online and in-person classes","abstract":"Research-validated multiple-choice questions comprise an easy-to-implement instructional tool for scaffolding student learning and providing formative assessment of students' knowledge. We present findings from the implementation of a research-validated multiple-choice question sequence on the basics of two-state quantum systems, including inner products, outer products, translation between Dirac notation and matrix representation in a particular basis, and change of basis. This study was conducted in an advanced undergraduate quantum mechanics course, in both online and in-person learning environments, across three years. For each cohort, students had their learning assessed after traditional lecture-based instruction in relevant concepts before engaging with the multiple-choice question sequence. Their performance was evaluated again afterwards with a similar assessment and compared to their earlier performance. We analyze, compare, and discuss the trends observed in the three implementations.","sentences":["Research-validated multiple-choice questions comprise an easy-to-implement instructional tool for scaffolding student learning and providing formative assessment of students' knowledge.","We present findings from the implementation of a research-validated multiple-choice question sequence on the basics of two-state quantum systems, including inner products, outer products, translation between Dirac notation and matrix representation in a particular basis, and change of basis.","This study was conducted in an advanced undergraduate quantum mechanics course, in both online and in-person learning environments, across three years.","For each cohort, students had their learning assessed after traditional lecture-based instruction in relevant concepts before engaging with the multiple-choice question sequence.","Their performance was evaluated again afterwards with a similar assessment and compared to their earlier performance.","We analyze, compare, and discuss the trends observed in the three implementations."],"url":"http://arxiv.org/abs/2402.18069v1","category":"physics.ed-ph"}
{"created":"2024-02-28 05:54:02","title":"SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model","abstract":"In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.","sentences":["In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images.","To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models.","Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%.","To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed.","Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts.","Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved."],"url":"http://arxiv.org/abs/2402.18068v1","category":"cs.CV"}
{"created":"2024-02-28 05:52:25","title":"Six-Point Method for Multi-Camera Systems with Reduced Solution Space","abstract":"Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of a multi-camera system, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated. Extensive experiments demonstrate that our solvers are more accurate than the state-of-the-art six-point methods, while achieving better performance in efficiency.","sentences":["Relative pose estimation using point correspondences (PC) is a widely used technique.","A minimal configuration of six PCs is required for generalized cameras.","In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of a multi-camera system, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs.","The equation construction is based on the decoupling of rotation and translation.","Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique.","Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views.","This is the key to reducing the number of solutions and generating numerically stable solvers.","Moreover, all configurations of six-point problems for multi-camera systems are enumerated.","Extensive experiments demonstrate that our solvers are more accurate than the state-of-the-art six-point methods, while achieving better performance in efficiency."],"url":"http://arxiv.org/abs/2402.18066v1","category":"cs.CV"}
{"created":"2024-02-28 05:44:41","title":"Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions","abstract":"LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. The inconsistency between automatic and human evaluations of model-generated explanations highlights the need to develop new metrics to support future research on explainable medical QA.","sentences":["LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations.","However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases.","Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions.","To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets.","JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions.","Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation.","We evaluate four LLMs on the two datasets using various prompts.","Experiments demonstrate that our datasets are harder than previous benchmarks.","The inconsistency between automatic and human evaluations of model-generated explanations highlights the need to develop new metrics to support future research on explainable medical QA."],"url":"http://arxiv.org/abs/2402.18060v1","category":"cs.CL"}
{"created":"2024-02-28 05:26:32","title":"A scalable cavity-based spin-photon interface in a photonic integrated circuit","abstract":"A central challenge in quantum networking is transferring quantum states between different physical modalities, such as between flying photonic qubits and stationary quantum memories. One implementation entails using spin-photon interfaces that combine solid-state spin qubits, such as color centers in diamond, with photonic nanostructures. However, while high-fidelity spin-photon interactions have been demonstrated on isolated devices, building practical quantum repeaters requires scaling to large numbers of interfaces yet to be realized. Here, we demonstrate integration of nanophotonic cavities containing tin-vacancy (SnV) centers in a photonic integrated circuit (PIC). Out of a six-channel quantum micro-chiplet (QMC), we find four coupled SnV-cavity devices with an average Purcell factor of ~7. Based on system analyses and numerical simulations, we find with near-term improvements this multiplexed architecture can enable high-fidelity quantum state transfer, paving the way towards building large-scale quantum repeaters.","sentences":["A central challenge in quantum networking is transferring quantum states between different physical modalities, such as between flying photonic qubits and stationary quantum memories.","One implementation entails using spin-photon interfaces that combine solid-state spin qubits, such as color centers in diamond, with photonic nanostructures.","However, while high-fidelity spin-photon interactions have been demonstrated on isolated devices, building practical quantum repeaters requires scaling to large numbers of interfaces yet to be realized.","Here, we demonstrate integration of nanophotonic cavities containing tin-vacancy (SnV) centers in a photonic integrated circuit (PIC).","Out of a six-channel quantum micro-chiplet (QMC), we find four coupled SnV-cavity devices with an average Purcell factor of ~7.","Based on system analyses and numerical simulations, we find with near-term improvements this multiplexed architecture can enable high-fidelity quantum state transfer, paving the way towards building large-scale quantum repeaters."],"url":"http://arxiv.org/abs/2402.18057v1","category":"quant-ph"}
{"created":"2024-02-28 05:25:03","title":"Varieties with pseudoeffective canonical divisor and their Kodaira dimension","abstract":"Let X be a smooth, projective variety over the field of complex numbers. Here we focus on a conjecture attributed to Shigefumi Mori, which claims that X is uniruled if and only if the Kodaira dimension of X is negative.","sentences":["Let X be a smooth, projective variety over the field of complex numbers.","Here we focus on a conjecture attributed to Shigefumi Mori, which claims that X is uniruled if and only if the Kodaira dimension of X is negative."],"url":"http://arxiv.org/abs/2402.18055v1","category":"math.AG"}
{"created":"2024-02-28 05:24:21","title":"Contextualizing Generated Citation Texts","abstract":"Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take.","sentences":["Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context.","However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic.","To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation.","This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take."],"url":"http://arxiv.org/abs/2402.18054v1","category":"cs.CL"}
{"created":"2024-02-28 04:58:07","title":"MEGAnno+: A Human-LLM Collaborative Annotation System","abstract":"Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.","sentences":["Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks.","Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations.","Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels.","We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans."],"url":"http://arxiv.org/abs/2402.18050v1","category":"cs.CL"}
{"created":"2024-02-28 04:58:04","title":"Performance modeling of public permissionless blockchains: A survey","abstract":"Public permissionless blockchains facilitate peer-to-peer digital transactions, yet face performance challenges specifically minimizing transaction confirmation time to decrease energy and time consumption per transaction. Performance evaluation and prediction are crucial in achieving this objective, with performance modeling as a key solution despite the complexities involved in assessing these blockchains. This survey examines prior research concerning the performance modeling blockchain systems, specifically focusing on public permissionless blockchains. Initially, it provides foundational knowledge about these blockchains and the crucial performance parameters for their assessment. Additionally, the study delves into research on the performance modeling of public permissionless blockchains, predominantly considering these systems as bulk service queues. It also examines prior studies on workload and traffic modeling, characterization, and analysis within these blockchain networks. By analyzing existing research, our survey aims to provide insights and recommendations for researchers keen on enhancing the performance of public permissionless blockchains or devising novel mechanisms in this domain.","sentences":["Public permissionless blockchains facilitate peer-to-peer digital transactions, yet face performance challenges specifically minimizing transaction confirmation time to decrease energy and time consumption per transaction.","Performance evaluation and prediction are crucial in achieving this objective, with performance modeling as a key solution despite the complexities involved in assessing these blockchains.","This survey examines prior research concerning the performance modeling blockchain systems, specifically focusing on public permissionless blockchains.","Initially, it provides foundational knowledge about these blockchains and the crucial performance parameters for their assessment.","Additionally, the study delves into research on the performance modeling of public permissionless blockchains, predominantly considering these systems as bulk service queues.","It also examines prior studies on workload and traffic modeling, characterization, and analysis within these blockchain networks.","By analyzing existing research, our survey aims to provide insights and recommendations for researchers keen on enhancing the performance of public permissionless blockchains or devising novel mechanisms in this domain."],"url":"http://arxiv.org/abs/2402.18049v1","category":"cs.CR"}
{"created":"2024-02-28 04:43:41","title":"SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation","abstract":"Extrapolating future weather radar echoes from past observations is a complex task vital for precipitation nowcasting. The spatial morphology and temporal evolution of radar echoes exhibit a certain degree of correlation, yet they also possess independent characteristics. {Existing methods learn unified spatial and temporal representations in a highly coupled feature space, emphasizing the correlation between spatial and temporal features but neglecting the explicit modeling of their independent characteristics, which may result in mutual interference between them.} To effectively model the spatiotemporal dynamics of radar echoes, we propose a Spatial-Frequency-Temporal correlation-decoupling Transformer (SFTformer). The model leverages stacked multiple SFT-Blocks to not only mine the correlation of the spatiotemporal dynamics of echo cells but also avoid the mutual interference between the temporal modeling and the spatial morphology refinement by decoupling them. Furthermore, inspired by the practice that weather forecast experts effectively review historical echo evolution to make accurate predictions, SFTfomer incorporates a joint training paradigm for historical echo sequence reconstruction and future echo sequence prediction. Experimental results on the HKO-7 dataset and ChinaNorth-2021 dataset demonstrate the superior performance of SFTfomer in short(1h), mid(2h), and long-term(3h) precipitation nowcasting.","sentences":["Extrapolating future weather radar echoes from past observations is a complex task vital for precipitation nowcasting.","The spatial morphology and temporal evolution of radar echoes exhibit a certain degree of correlation, yet they also possess independent characteristics.","{Existing methods learn unified spatial and temporal representations in a highly coupled feature space, emphasizing the correlation between spatial and temporal features but neglecting the explicit modeling of their independent characteristics, which may result in mutual interference between them.}","To effectively model the spatiotemporal dynamics of radar echoes, we propose a Spatial-Frequency-Temporal correlation-decoupling Transformer (SFTformer).","The model leverages stacked multiple SFT-Blocks to not only mine the correlation of the spatiotemporal dynamics of echo cells but also avoid the mutual interference between the temporal modeling and the spatial morphology refinement by decoupling them.","Furthermore, inspired by the practice that weather forecast experts effectively review historical echo evolution to make accurate predictions, SFTfomer incorporates a joint training paradigm for historical echo sequence reconstruction and future echo sequence prediction.","Experimental results on the HKO-7 dataset and ChinaNorth-2021 dataset demonstrate the superior performance of SFTfomer in short(1h), mid(2h), and long-term(3h) precipitation nowcasting."],"url":"http://arxiv.org/abs/2402.18044v1","category":"cs.CV"}
{"created":"2024-02-28 04:33:18","title":"Infrared fixed point in the massless twelve-flavor SU(3) gauge-fermion system","abstract":"We present strong numerical evidence for the existence of an infrared fixed point in the renormalization group flow of the SU(3) gauge-fermion system with twelve massless fermions in the fundamental representation. Our numerical simulations using nHYP-smeared staggered fermions with Pauli-Villars improvement do not exhibit any first-order bulk phase transition in the investigated parameter region. We utilize an infinite volume renormalization scheme based on the gradient flow transformation to determine the renormalization group $\\beta$ function. We identify an infrared fixed point at $g^2_{\\mathrm{GF}\\star}=6.69(68)$ in the GF scheme and calculate the leading irrelevant critical exponent $\\gamma_{g}^{\\star}=0.204(36)$. Our prediction for $\\gamma_{g}^{\\star}$ is consistent with available literature at the $1\\mbox{-}2\\sigma$ level.","sentences":["We present strong numerical evidence for the existence of an infrared fixed point in the renormalization group flow of the SU(3) gauge-fermion system with twelve massless fermions in the fundamental representation.","Our numerical simulations using nHYP-smeared staggered fermions with Pauli-Villars improvement do not exhibit any first-order bulk phase transition in the investigated parameter region.","We utilize an infinite volume renormalization scheme based on the gradient flow transformation to determine the renormalization group $\\beta$ function.","We identify an infrared fixed point at $g^2_{\\mathrm{GF}\\star}=6.69(68)$ in the GF scheme and calculate the leading irrelevant critical exponent $\\gamma_{g}^{\\star}=0.204(36)$. Our prediction for $\\gamma_{g}^{\\star}$ is consistent with available literature at the $1\\mbox{-}2\\sigma$ level."],"url":"http://arxiv.org/abs/2402.18038v1","category":"hep-lat"}
{"created":"2024-02-28 03:58:58","title":"Corpus-Steered Query Expansion with Large Language Models","abstract":"Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive experiments reveal that CSQE exhibits strong performance without necessitating any training, especially with queries for which LLMs lack knowledge.","sentences":["Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions.","However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.","Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus.","CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents.","These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents.","Extensive experiments reveal that CSQE exhibits strong performance without necessitating any training, especially with queries for which LLMs lack knowledge."],"url":"http://arxiv.org/abs/2402.18031v1","category":"cs.IR"}
{"created":"2024-02-28 03:57:12","title":"Overcoming set imbalance in data driven parameterization: A case study of gravity wave momentum transport","abstract":"Machine learning for the parameterization of subgrid-scale processes in climate models has been widely researched and adopted in a few models. A key challenge in developing data-driven parameterization schemes is how to properly represent rare, but important events that occur in geoscience datasets. We investigate and develop strategies to reduce errors caused by insufficient sampling in the rare data regime, under constraints of no new data and no further expansion of model complexity. Resampling and importance weighting strategies are constructed with user defined parameters that systematically vary the sampling/weighting rates in a linear fashion and curb too much oversampling. Applying this new method to a case study of gravity wave momentum transport reveals that the resampling strategy can successfully improve errors in the rare regime at little to no loss in accuracy overall in the dataset. The success of the strategy, however, depends on the complexity of the model. More complex models can overfit the tails of the distribution when using non-optimal parameters of the resampling strategy.","sentences":["Machine learning for the parameterization of subgrid-scale processes in climate models has been widely researched and adopted in a few models.","A key challenge in developing data-driven parameterization schemes is how to properly represent rare, but important events that occur in geoscience datasets.","We investigate and develop strategies to reduce errors caused by insufficient sampling in the rare data regime, under constraints of no new data and no further expansion of model complexity.","Resampling and importance weighting strategies are constructed with user defined parameters that systematically vary the sampling/weighting rates in a linear fashion and curb too much oversampling.","Applying this new method to a case study of gravity wave momentum transport reveals that the resampling strategy can successfully improve errors in the rare regime at little to no loss in accuracy overall in the dataset.","The success of the strategy, however, depends on the complexity of the model.","More complex models can overfit the tails of the distribution when using non-optimal parameters of the resampling strategy."],"url":"http://arxiv.org/abs/2402.18030v1","category":"physics.ao-ph"}
{"created":"2024-02-28 03:38:54","title":"Synchronization of Complex Dynamical Networks via Event-Triggered Pinning Impulses","abstract":"This article studies the synchronization problem of complex dynamical networks. The impulsive control method is considered with a novel event-triggered pinning algorithm. Sufficient conditions on the network topology are obtained to ensure network synchronization. It is shown that synchronization can be realized with a careful selection of the pinning nodes. Furthermore, an adaptive coupling strength is incorporated into the network to allow network synchronization with an arbitrary selection of the pinning nodes. An example of a network with node dynamics described by the Chen system is studied to demonstrate the theoretical results.","sentences":["This article studies the synchronization problem of complex dynamical networks.","The impulsive control method is considered with a novel event-triggered pinning algorithm.","Sufficient conditions on the network topology are obtained to ensure network synchronization.","It is shown that synchronization can be realized with a careful selection of the pinning nodes.","Furthermore, an adaptive coupling strength is incorporated into the network to allow network synchronization with an arbitrary selection of the pinning nodes.","An example of a network with node dynamics described by the Chen system is studied to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2402.18024v1","category":"math.OC"}
{"created":"2024-02-28 03:27:10","title":"Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach","abstract":"Federated learning (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources. Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability. In this work, we consider a multi-server FL framework, referred to as \\emph{Confederated Learning} (CFL), in order to accommodate a larger number of users. A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users. Decentralized collaboration among servers is leveraged to harness all users' data for model training. Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system. We propose a stochastic gradient method for distributed learning in the CFL framework. The proposed method incorporates a conditionally-triggered user selection (CTUS) mechanism as the central component to effectively reduce communication overhead. Relying on a delicately designed triggering condition, the CTUS mechanism allows each server to select only a small number of users to upload their gradients, without significantly jeopardizing the convergence performance of the algorithm. Our theoretical analysis reveals that the proposed algorithm enjoys a linear convergence rate. Simulation results show that it achieves substantial improvement over state-of-the-art algorithms in terms of communication efficiency.","sentences":["Federated learning (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources.","Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability.","In this work, we consider a multi-server FL framework, referred to as \\emph{Confederated Learning} (CFL), in order to accommodate a larger number of users.","A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users.","Decentralized collaboration among servers is leveraged to harness all users' data for model training.","Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system.","We propose a stochastic gradient method for distributed learning in the CFL framework.","The proposed method incorporates a conditionally-triggered user selection (CTUS) mechanism as the central component to effectively reduce communication overhead.","Relying on a delicately designed triggering condition, the CTUS mechanism allows each server to select only a small number of users to upload their gradients, without significantly jeopardizing the convergence performance of the algorithm.","Our theoretical analysis reveals that the proposed algorithm enjoys a linear convergence rate.","Simulation results show that it achieves substantial improvement over state-of-the-art algorithms in terms of communication efficiency."],"url":"http://arxiv.org/abs/2402.18018v1","category":"cs.LG"}
{"created":"2024-02-28 03:23:10","title":"Hy-DAT: A Tool to Address Hydropower Modelling Gaps Using Interdependency, Efficiency Curves, and Unit Dispatch Models","abstract":"As the power system continues to be flooded with intermittent resources, it becomes more important to accurately assess the role of hydro and its impact on the power grid. While hydropower generation has been studied for decades, dependency of power generation on water availability and constraints in hydro operation are not well represented in power system models used in the planning and operation of large-scale interconnection studies. There are still multiple modeling gaps that need to be addressed; if not, they can lead to inaccurate operation and planning reliability studies, and consequently to unintentional load shedding or even blackouts. As a result, it is very important that hydropower is represented correctly in both steady-state and dynamic power system studies. In this paper, we discuss the development and use of the Hydrological Dispatch and Analysis Tool (Hy-DAT) as an interactive graphical user interface (GUI), that uses a novel methodology to address the hydropower modeling gaps like water availability and inter-dependency using a database, and algorithms, to generate accurate representative models for power system simulation.","sentences":["As the power system continues to be flooded with intermittent resources, it becomes more important to accurately assess the role of hydro and its impact on the power grid.","While hydropower generation has been studied for decades, dependency of power generation on water availability and constraints in hydro operation are not well represented in power system models used in the planning and operation of large-scale interconnection studies.","There are still multiple modeling gaps that need to be addressed; if not, they can lead to inaccurate operation and planning reliability studies, and consequently to unintentional load shedding or even blackouts.","As a result, it is very important that hydropower is represented correctly in both steady-state and dynamic power system studies.","In this paper, we discuss the development and use of the Hydrological Dispatch and Analysis Tool (Hy-DAT) as an interactive graphical user interface (GUI), that uses a novel methodology to address the hydropower modeling gaps like water availability and inter-dependency using a database, and algorithms, to generate accurate representative models for power system simulation."],"url":"http://arxiv.org/abs/2402.18017v1","category":"eess.SP"}
{"created":"2024-02-28 02:30:40","title":"Hilbert Space Fragmentation and Subspace Scar Time-Crystallinity in Driven Homogeneous Central-Spin Models","abstract":"We study the stroboscopic non-equilibrium quantum dynamics of periodically kicked Hamiltonians involving homogeneous central-spin interactions. The system exhibits a strong fragmentation of Hilbert space into four-dimensional Floquet-Krylov subspaces, which oscillate between two disjointed two-dimensional subspaces and thus break the discrete time-translation symmetry of the system. Our analytical and numerical analyses reveal that fully polarized states of the satellite spins exhibit fragmentations that are stable against perturbations and have high overlap with Floquet eigenstates of atypically low bipartite entanglement entropy (scar states). We present evidence of robust time-crystalline behavior in the form of a period doubling of the total magnetization of fully polarized satellite spin states that persists over long time scales. We compute non-equilibrium phase diagrams with respect to a magnetic field, coupling terms, and pulse error for various interaction types, including Heisenberg, Ising, XXZ, and XX. We also discuss possible experimental realizations of scar time crystals in color center, quantum dot, and rare-earth ion platforms.","sentences":["We study the stroboscopic non-equilibrium quantum dynamics of periodically kicked Hamiltonians involving homogeneous central-spin interactions.","The system exhibits a strong fragmentation of Hilbert space into four-dimensional Floquet-Krylov subspaces, which oscillate between two disjointed two-dimensional subspaces and thus break the discrete time-translation symmetry of the system.","Our analytical and numerical analyses reveal that fully polarized states of the satellite spins exhibit fragmentations that are stable against perturbations and have high overlap with Floquet eigenstates of atypically low bipartite entanglement entropy (scar states).","We present evidence of robust time-crystalline behavior in the form of a period doubling of the total magnetization of fully polarized satellite spin states that persists over long time scales.","We compute non-equilibrium phase diagrams with respect to a magnetic field, coupling terms, and pulse error for various interaction types, including Heisenberg, Ising, XXZ, and XX.","We also discuss possible experimental realizations of scar time crystals in color center, quantum dot, and rare-earth ion platforms."],"url":"http://arxiv.org/abs/2402.18001v1","category":"quant-ph"}
{"created":"2024-02-28 02:30:17","title":"Exact solutions for nonlinear trapped lee waves in the $\u03b2$-plane approximation","abstract":"In this paper, we construct exact solutions that character three-dimensional, nonlinear trapped lee waves propagation superimposed on longitudinal atmospheric currents in the $\\beta$-plane approximation. The solutions obtained are presented in Lagrangian coordinates, and are Gerstner-like solutions. In the process, we also derive the dispersion relation and analyze the density, pressure and the vorticity qualitatively.","sentences":["In this paper, we construct exact solutions that character three-dimensional, nonlinear trapped lee waves propagation superimposed on longitudinal atmospheric currents in the $\\beta$-plane approximation.","The solutions obtained are presented in Lagrangian coordinates, and are Gerstner-like solutions.","In the process, we also derive the dispersion relation and analyze the density, pressure and the vorticity qualitatively."],"url":"http://arxiv.org/abs/2402.18000v1","category":"math.DS"}
{"created":"2024-02-28 02:21:18","title":"Joint Activity-Delay Detection and Channel Estimation for Asynchronous Massive Random Access: A Free Probability Theory Approach","abstract":"Grant-free random access (RA) has been recognized as a promising solution to support massive connectivity due to the removal of the uplink grant request procedures. While most endeavours assume perfect synchronization among users and the base station, this paper investigates asynchronous grant-free massive RA, and develop efficient algorithms for joint user activity detection, synchronization delay detection, and channel estimation. Considering the sparsity on user activity, we formulate a sparse signal recovery problem and propose to utilize the framework of orthogonal approximate message passing (OAMP) to deal with the non-independent and identically distributed (i.i.d.) Gaussian pilot matrices caused by the synchronization delays. In particular, an OAMP-based algorithm is developed to fully harness the common sparsity among received pilot signals from multiple base station antennas. To reduce the computational complexity, we further propose a free probability AMP (FPAMP)-based algorithm, which exploits the rectangular free cumulants to make the cost-effective AMP framework compatible to general pilot matrices. Simulation results demonstrate that the two proposed algorithms outperform various baselines, and the FPAMP-based algorithm reduces 40% of the computations while maintaining comparable detection/estimation accuracy with the OAMP-based algorithm.","sentences":["Grant-free random access (RA) has been recognized as a promising solution to support massive connectivity due to the removal of the uplink grant request procedures.","While most endeavours assume perfect synchronization among users and the base station, this paper investigates asynchronous grant-free massive RA, and develop efficient algorithms for joint user activity detection, synchronization delay detection, and channel estimation.","Considering the sparsity on user activity, we formulate a sparse signal recovery problem and propose to utilize the framework of orthogonal approximate message passing (OAMP) to deal with the non-independent and identically distributed (i.i.d.)","Gaussian pilot matrices caused by the synchronization delays.","In particular, an OAMP-based algorithm is developed to fully harness the common sparsity among received pilot signals from multiple base station antennas.","To reduce the computational complexity, we further propose a free probability AMP (FPAMP)-based algorithm, which exploits the rectangular free cumulants to make the cost-effective AMP framework compatible to general pilot matrices.","Simulation results demonstrate that the two proposed algorithms outperform various baselines, and the FPAMP-based algorithm reduces 40% of the computations while maintaining comparable detection/estimation accuracy with the OAMP-based algorithm."],"url":"http://arxiv.org/abs/2402.17996v1","category":"eess.SP"}
{"created":"2024-02-28 02:18:59","title":"Quasipolynomial bounds on the inverse theorem for the Gowers $U^{s+1}[N]$-norm","abstract":"We prove quasipolynomial bounds on the inverse theorem for the Gowers $U^{s+1}[N]$-norm. The proof is modeled after work of Green, Tao, and Ziegler and uses as a crucial input recent work of the first author regarding the equidistribution of nilsequences. In a companion paper, this result will be used to improve the bounds on Szemer\\'{e}di's theorem.","sentences":["We prove quasipolynomial bounds on the inverse theorem for the Gowers $U^{s+1}[N]$-norm.","The proof is modeled after work of Green, Tao, and Ziegler and uses as a crucial input recent work of the first author regarding the equidistribution of nilsequences.","In a companion paper, this result will be used to improve the bounds on Szemer\\'{e}di's theorem."],"url":"http://arxiv.org/abs/2402.17994v1","category":"math.CO"}
{"created":"2024-02-28 02:16:14","title":"Unitary coupled-cluster singles and doubles (UCCSD) calculations in conjunction with fragment molecular orbital (FMO) scheme","abstract":"The fragment molecular orbital (FMO) method is one of the popular methods to efficiently treat macromolecular systems by dividing the system of interest into small fragments based on embedding under the electrostatic potential (ESP). Such a fragmentation method has the potential advantage of making the circuit flat in quantum chemical calculations on quantum computers. In this study, we used a GPU-accelerated quantum simulator (cuQuantum) to perform the electron correlation part of the FMO calculation as a unitary coupled-cluster singles and doubles (UCCSD) with the variational quantum eigensolver (VQE), using hydrogen-bonded systems (FH)$_3$ and (FH)$_2$-H$_2$O as testbeds. From the numerical simulations, we found that the Trotter decomposition of the UCCSD ansatz breaks the orbital-invariance condition, and it can yield different total energies for symmetrically equivalent molecules. We also observed that the Trotterized UCCSD does not satisfy the size-consistency, which is an essential requirement in the FMO scheme, when the molecular orbitals delocalized to dimers are used. The GPU acceleration was substantial for the simulations with larger numbers of qubits, and it was about a factor of 5.8--7.5 for 18 qubit systems.","sentences":["The fragment molecular orbital (FMO) method is one of the popular methods to efficiently treat macromolecular systems by dividing the system of interest into small fragments based on embedding under the electrostatic potential (ESP).","Such a fragmentation method has the potential advantage of making the circuit flat in quantum chemical calculations on quantum computers.","In this study, we used a GPU-accelerated quantum simulator (cuQuantum) to perform the electron correlation part of the FMO calculation as a unitary coupled-cluster singles and doubles (UCCSD) with the variational quantum eigensolver (VQE), using hydrogen-bonded systems (FH)$_3$ and (FH)$_2$-H$_2$O as testbeds.","From the numerical simulations, we found that the Trotter decomposition of the UCCSD ansatz breaks the orbital-invariance condition, and it can yield different total energies for symmetrically equivalent molecules.","We also observed that the Trotterized UCCSD does not satisfy the size-consistency, which is an essential requirement in the FMO scheme, when the molecular orbitals delocalized to dimers are used.","The GPU acceleration was substantial for the simulations with larger numbers of qubits, and it was about a factor of 5.8--7.5 for 18 qubit systems."],"url":"http://arxiv.org/abs/2402.17993v1","category":"quant-ph"}
{"created":"2024-02-28 02:16:03","title":"Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures","abstract":"There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enhancing model efficiency. The LSTM network captures temporal dependencies, enabling accurate prediction of time series responses. The equation of motion is manipulated to learn system nonlinearities and confines the solution space within physically interpretable results. These features enable model training with relatively sparse data and offer benefits in terms of accuracy, interpretability, and robustness. Furthermore, a dataset of seismically designed archetype ductile planar steel moment resistant frames under horizontal seismic loading, available in the DesignSafe-CI Database, is considered for evaluation of the proposed method. The resulting metamodel is capable of handling more complex data compared to existing physics-guided LSTM models and outperforms other non-physics data-driven neural networks.","sentences":["There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations.","The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data.","To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures.","The basic concept is to constrain the solution space of the ML model within known physical bounds.","This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion).","Model order reduction is essential for handling structural systems with inherent redundancy and enhancing model efficiency.","The LSTM network captures temporal dependencies, enabling accurate prediction of time series responses.","The equation of motion is manipulated to learn system nonlinearities and confines the solution space within physically interpretable results.","These features enable model training with relatively sparse data and offer benefits in terms of accuracy, interpretability, and robustness.","Furthermore, a dataset of seismically designed archetype ductile planar steel moment resistant frames under horizontal seismic loading, available in the DesignSafe-CI Database, is considered for evaluation of the proposed method.","The resulting metamodel is capable of handling more complex data compared to existing physics-guided LSTM models and outperforms other non-physics data-driven neural networks."],"url":"http://arxiv.org/abs/2402.17992v1","category":"physics.app-ph"}
{"created":"2024-02-28 02:11:47","title":"Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach","abstract":"Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven drones, correlating target aspect angles to Radar Cross Section (RCS) measurements in an anechoic chamber. Comparing against single radar Automated Target Recognition (ATR) systems and suboptimal fusion methods, our empirical results demonstrate that the OBF method integrated with RBC significantly enhances classification accuracy compared to other fusion methods and single radar configurations.","sentences":["Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications.","Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR.","However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically.","To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars.","OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps.","We evaluate the approach using simulated random walk trajectories for seven drones, correlating target aspect angles to Radar Cross Section (RCS) measurements in an anechoic chamber.","Comparing against single radar Automated Target Recognition (ATR) systems and suboptimal fusion methods, our empirical results demonstrate that the OBF method integrated with RBC significantly enhances classification accuracy compared to other fusion methods and single radar configurations."],"url":"http://arxiv.org/abs/2402.17987v1","category":"eess.SP"}
{"created":"2024-02-28 18:58:11","title":"Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators","abstract":"In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability. In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training. However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness. Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model. However, it has been observed that these techniques are not effective w.r.t. accuracy. Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements. In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices. We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets. Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy.","sentences":["In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability.","In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training.","However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness.","Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model.","However, it has been observed that these techniques are not effective w.r.t. accuracy.","Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements.","In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices.","We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets.","Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy."],"url":"http://arxiv.org/abs/2402.18569v1","category":"cs.AR"}
{"created":"2024-02-28 18:42:46","title":"Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks","abstract":"The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has evolved into a leading research platform. The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms. Consequently, the field is wide and disjointed, hindering direct comparison of methods and making it difficult to assess the state-of-the-art. Therefore, we aim to unify the field by surveying current approaches, describing common methods and providing benchmark results to facilitate clear comparison and establish a baseline for future work. We survey current work in F1TENTH racing in the classical and learning categories, explaining the different solution approaches. We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control (MPCC), follow-the-gap and end-to-end reinforcement learning. We provide an open-source evaluation of benchmark methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods and reward signal and training map for learning methods. The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the MPCC planner. Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain.","sentences":["The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has evolved into a leading research platform.","The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms.","Consequently, the field is wide and disjointed, hindering direct comparison of methods and making it difficult to assess the state-of-the-art.","Therefore, we aim to unify the field by surveying current approaches, describing common methods and providing benchmark results to facilitate clear comparison and establish a baseline for future work.","We survey current work in F1TENTH racing in the classical and learning categories, explaining the different solution approaches.","We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control (MPCC), follow-the-gap and end-to-end reinforcement learning.","We provide an open-source evaluation of benchmark methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods and reward signal and training map for learning methods.","The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the MPCC planner.","Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain."],"url":"http://arxiv.org/abs/2402.18558v1","category":"cs.RO"}
{"created":"2024-02-28 18:39:05","title":"Dark matter from mediator decay in early matter domination","abstract":"We study dark matter production from mediator decays in scenarios with an epoch of early matter domination. Particles that mediate interactions between dark matter and the standard model particles are kinematically accessible to the thermal bath as long as their mass is below the reheating temperature of the Universe after inflation. Decay of on-shell mediators can then lead to copious production of dark matter during early matter domination or a preceding radiation-dominated phase. In particular, for mediators that are charged under the standard model, it can exceed the standard freeze-in channel due to inverse annihilations at much lower temperatures (often by many orders of magnitude). The requirement to obtain the correct relic abundance severely constrains the parameter space for dark matter masses above a few TeV.","sentences":["We study dark matter production from mediator decays in scenarios with an epoch of early matter domination.","Particles that mediate interactions between dark matter and the standard model particles are kinematically accessible to the thermal bath as long as their mass is below the reheating temperature of the Universe after inflation.","Decay of on-shell mediators can then lead to copious production of dark matter during early matter domination or a preceding radiation-dominated phase.","In particular, for mediators that are charged under the standard model, it can exceed the standard freeze-in channel due to inverse annihilations at much lower temperatures (often by many orders of magnitude).","The requirement to obtain the correct relic abundance severely constrains the parameter space for dark matter masses above a few TeV."],"url":"http://arxiv.org/abs/2402.18557v1","category":"hep-ph"}
{"created":"2024-02-28 18:35:59","title":"Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture","abstract":"Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making. The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough. The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings. This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%). The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure. A simulation of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).","sentences":["Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making.","The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough.","The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings.","This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%).","The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic.","An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure.","A simulation of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit.","Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05)."],"url":"http://arxiv.org/abs/2402.18553v1","category":"cs.CV"}
{"created":"2024-02-28 18:08:03","title":"Gradient Reweighting: Towards Imbalanced Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.","sentences":["Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge.","A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance).","We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL.","Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning.","Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases.","To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data.","We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness."],"url":"http://arxiv.org/abs/2402.18528v1","category":"cs.CV"}
{"created":"2024-02-28 17:52:37","title":"Do galaxy mergers prefer under-dense environments?","abstract":"Galaxy mergers play a crucial role in galaxy evolution. However, the correlation between mergers and the local environment of galaxies is not fully understood. We aim to address the question of whether galaxy mergers prefer denser or less dense environments by quantifying the spatial clustering of mergers and non-mergers. We use two different indicators to classify mergers and non-mergers - classification based on a deep learning technique ($f$) and non-parametric measures of galaxy morphology, Gini-$M_{20}$ ($g$). We used a set of galaxy samples in the redshift range 0.1 < z < 0.15 from the Galaxy and Mass Assembly (GAMA) survey with a stellar mass cut of log (M*/Msun) > 9.5. We measured and compared the two-point correlation function (2pCF) of mergers and non-mergers classified using the two merger indicators $f$ and $g$. We measured the marked correlation function (MCF), in which the galaxies are weighted by $f$ to probe the environmental dependence of galaxy mergers. We do not observe a statistically significant difference between the clustering strengths of mergers and non-mergers obtained using 2pCF. However, using the MCF measurements with $f$ as a mark, we observe an anti-correlation between the likelihood of a galaxy being a merger and its environment. Our results emphasise the advantage of MCF over 2pCF in probing the environmental correlations. Based on the MCF measurements, we conclude that the galaxy mergers prefer to occur in the under-dense environments on scales greater than 50 kpc/h of the large-scale structure (LSS). We attribute this observation to the high relative velocities of galaxies in the densest environments that prevent them from merging.","sentences":["Galaxy mergers play a crucial role in galaxy evolution.","However, the correlation between mergers and the local environment of galaxies is not fully understood.","We aim to address the question of whether galaxy mergers prefer denser or less dense environments by quantifying the spatial clustering of mergers and non-mergers.","We use two different indicators to classify mergers and non-mergers - classification based on a deep learning technique ($f$) and non-parametric measures of galaxy morphology, Gini-$M_{20}$ ($g$).","We used a set of galaxy samples in the redshift range 0.1 <","z < 0.15 from the Galaxy and Mass Assembly (GAMA) survey with a stellar mass cut of log (M*/Msun) >","9.5.","We measured and compared the two-point correlation function (2pCF) of mergers and non-mergers classified using the two merger indicators $f$ and $g$. We measured the marked correlation function (MCF), in which the galaxies are weighted by $f$ to probe the environmental dependence of galaxy mergers.","We do not observe a statistically significant difference between the clustering strengths of mergers and non-mergers obtained using 2pCF.","However, using the MCF measurements with $f$ as a mark, we observe an anti-correlation between the likelihood of a galaxy being a merger and its environment.","Our results emphasise the advantage of MCF over 2pCF in probing the environmental correlations.","Based on the MCF measurements, we conclude that the galaxy mergers prefer to occur in the under-dense environments on scales greater than 50 kpc/h of the large-scale structure (LSS).","We attribute this observation to the high relative velocities of galaxies in the densest environments that prevent them from merging."],"url":"http://arxiv.org/abs/2402.18520v1","category":"astro-ph.GA"}
{"created":"2024-02-28 17:40:27","title":"Categorical absorption of a non-isolated singularity","abstract":"We study an example of a projective threefold with a non-isolated singularity and its derived category. The singular locus can be locally described as a line of surface nodes compounded with a threefold node at the origin. We construct a semiorthogonal decomposition where one component absorbs the singularity in the sense of Kuznetsov-Shinder, and the other components are equivalent to the derived categories of smooth projective varieties. The absorbing category is seen to be closely related to the absorbing category constructed for nodal varieties by Kuznetsov-Shinder, reflecting the geometry of the singularity. We further show that the semiorthogonal decomposition is induced by one on a geometric resolution, and briefly consider the properties of the absorbing category under smoothing.","sentences":["We study an example of a projective threefold with a non-isolated singularity and its derived category.","The singular locus can be locally described as a line of surface nodes compounded with a threefold node at the origin.","We construct a semiorthogonal decomposition where one component absorbs the singularity in the sense of Kuznetsov-Shinder, and the other components are equivalent to the derived categories of smooth projective varieties.","The absorbing category is seen to be closely related to the absorbing category constructed for nodal varieties by Kuznetsov-Shinder, reflecting the geometry of the singularity.","We further show that the semiorthogonal decomposition is induced by one on a geometric resolution, and briefly consider the properties of the absorbing category under smoothing."],"url":"http://arxiv.org/abs/2402.18513v1","category":"math.AG"}
{"created":"2024-02-28 17:40:05","title":"Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference","abstract":"The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit.","sentences":["The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path.","Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state.","As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data.","Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs.","The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution.","On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit."],"url":"http://arxiv.org/abs/2402.18512v1","category":"cs.LG"}
{"created":"2024-02-28 17:34:21","title":"Evolving machine learning workflows through interactive AutoML","abstract":"Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters. This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P). Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included. In this paper we present \\ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest. Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML. To evaluate our approach, we present an experimental study in which 20 participants interact with \\ourmethod to evolve workflows according to their preferences. Our results confirm that the collaboration between \\ourmethod and humans allows us to find high-performance workflows in terms of accuracy that require less tuning time than those found without human intervention.","sentences":["Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters.","This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P).","Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included.","In this paper we present \\ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest.","Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML.","To evaluate our approach, we present an experimental study in which 20 participants interact with \\ourmethod to evolve workflows according to their preferences.","Our results confirm that the collaboration between \\ourmethod and humans allows us to find high-performance workflows in terms of accuracy that require less tuning time than those found without human intervention."],"url":"http://arxiv.org/abs/2402.18505v1","category":"cs.LG"}
{"created":"2024-02-28 17:21:02","title":"Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection","abstract":"LiDAR-based 3D object detection models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny","sentences":["LiDAR-based 3D object detection models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals.","Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models.","However, significant disparities exist between simulated and actual rain-impacted data points.","In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training.","Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions.","Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency.","Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny"],"url":"http://arxiv.org/abs/2402.18493v1","category":"cs.CV"}
{"created":"2024-02-28 17:00:49","title":"Towards a precision calculation of $N_{\\rm eff}$ in the Standard Model III: Improved estimate of NLO corrections to the collision integral","abstract":"We compute the dominant QED correction to the neutrino-electron interaction rate in the vicinity of neutrino decoupling in the early universe, and estimate its impact on the effective number of neutrino species $N_{\\rm eff}$ in cosmic microwave background anisotropy observations. We find that the correction to the interaction rate is at the sub-percent level, consistent with a recent estimate by Jackson and Laine. Relative to that work we include the electron mass in our computations, but restrict our analysis to the enhanced $t$-channel contributions. The fractional change in $N_{\\rm eff}^{\\rm SM}$ due to the rate correction is of order $10^{-5}$ or below, i.e., about a factor of 30 smaller than that recently claimed by Cielo {\\it et al.}, and below the nominal computational uncertainties of the current benchmark value of $N_{\\rm eff}^{\\rm SM} = 3.0440 \\pm 0.0002$. We therefore conclude that aforementioned number remains to be the state-of-the-art benchmark for $N_{\\rm eff}^{\\rm SM}$ in the standard model of particle physics.","sentences":["We compute the dominant QED correction to the neutrino-electron interaction rate in the vicinity of neutrino decoupling in the early universe, and estimate its impact on the effective number of neutrino species $N_{\\rm eff}$ in cosmic microwave background anisotropy observations.","We find that the correction to the interaction rate is at the sub-percent level, consistent with a recent estimate by Jackson and Laine.","Relative to that work we include the electron mass in our computations, but restrict our analysis to the enhanced $t$-channel contributions.","The fractional change in $N_{\\rm eff}^{\\rm SM}$ due to the rate correction is of order $10^{-5}$ or below, i.e., about a factor of 30 smaller than that recently claimed by Cielo {\\it et al.}, and below the nominal computational uncertainties of the current benchmark value of $N_{\\rm eff}^{\\rm SM} = 3.0440 \\pm 0.0002$.","We therefore conclude that aforementioned number remains to be the state-of-the-art benchmark for $N_{\\rm eff}^{\\rm SM}$ in the standard model of particle physics."],"url":"http://arxiv.org/abs/2402.18481v1","category":"hep-ph"}
{"created":"2024-02-28 16:59:03","title":"External charge perturbation and electrostatic turbulence","abstract":"Abstract In this work, an 1D electrostatic hybrid-Particle-in-Cell-Monte-Carlo-Collision (h-PIC-MCC) [13, 14] is used to study the response of a plasma to a moving, external, charged perturbation (debris). We show that the so-called pinned solitons can form only under certain specific conditions through a turbulent regime of the ion-ion counter-streaming electrostatic instability. In fact, the pinned solitons are manifestation of the ion phase-space vortices formed around the debris. The simulation shows that the pinned solitons can form only when the debris velocity exceeds a certain critical velocity pushing the instability to a turbulent regime and can then disappear when debris velocity becomes highly supersonic. We further show the existence of a Kolmogorv-type inverse-cascading scaling for this electrostatic turbulence.","sentences":["Abstract In this work, an 1D electrostatic hybrid-Particle-in-Cell-Monte-Carlo-Collision (h-PIC-MCC) [13, 14] is used to study the response of a plasma to a moving, external, charged perturbation (debris).","We show that the so-called pinned solitons can form only under certain specific conditions through a turbulent regime of the ion-ion counter-streaming electrostatic instability.","In fact, the pinned solitons are manifestation of the ion phase-space vortices formed around the debris.","The simulation shows that the pinned solitons can form only when the debris velocity exceeds a certain critical velocity pushing the instability to a turbulent regime and can then disappear when debris velocity becomes highly supersonic.","We further show the existence of a Kolmogorv-type inverse-cascading scaling for this electrostatic turbulence."],"url":"http://arxiv.org/abs/2402.18478v1","category":"physics.plasm-ph"}
{"created":"2024-02-28 16:36:44","title":"Z' boson mass reach and model discrimination at muon colliders","abstract":"We study the discrimination power of future multi-TeV muon colliders for a large set of models with extended gauge symmetries and additional neutral gauge bosons (\"$Z'$-models\"). Our study is carried out using a $\\chi^2$-analysis of leptonic observables of s-channel scattering in effective $Z'$-models. We make use of angular and chiral asymmetries induced in such models to find the discovery reach of a given muon collider setup in terms of the $Z'$ mass and to discriminate between the different scenarios. In this context, we discuss how polarized beams - should they become available at muon colliders - or polarization measurements can help in the discrimination. Our results show that typical muon collider setups which are currently under consideration can give a significantly higher reach compared to existing bounds and projections for high-luminosity LHC.","sentences":["We study the discrimination power of future multi-TeV muon colliders for a large set of models with extended gauge symmetries and additional neutral gauge bosons (\"$Z'$-models\").","Our study is carried out using a $\\chi^2$-analysis of leptonic observables of s-channel scattering in effective $Z'$-models.","We make use of angular and chiral asymmetries induced in such models to find the discovery reach of a given muon collider setup in terms of the $Z'$ mass and to discriminate between the different scenarios.","In this context, we discuss how polarized beams - should they become available at muon colliders - or polarization measurements can help in the discrimination.","Our results show that typical muon collider setups which are currently under consideration can give a significantly higher reach compared to existing bounds and projections for high-luminosity LHC."],"url":"http://arxiv.org/abs/2402.18460v1","category":"hep-ph"}
{"created":"2024-02-28 15:56:57","title":"Phase transitions of Fe$_2$O$_3$ under laser shock compression","abstract":"We present in-situ x-ray diffraction and velocity measurements of Fe$_2$O$_3$ under laser shock compression at pressures between 38-116 GPa. None of the phases reported by static compression studies were observed. Instead, we observed an isostructural phase transition from $\\alpha$-Fe$_2$O$_3$ to a new $\\alpha^\\prime$-Fe$_2$O$_3$ phase at a pressure of 50-62 GPa. The $\\alpha^\\prime$-Fe$_2$O$_3$ phase differs from $\\alpha$-Fe$_2$O$_3$ by an 11% volume drop and a different unit cell compressibility. We further observed a two-wave structure in the velocity profile, which can be related to an intermediate regime where both $\\alpha$ and $\\alpha^\\prime$ phases coexist. Density functional theory calculations with a Hubbard parameter indicate that the observed unit cell volume drop can be associated with a spin transition following a magnetic collapse.","sentences":["We present in-situ x-ray diffraction and velocity measurements of Fe$_2$O$_3$ under laser shock compression at pressures between 38-116 GPa.","None of the phases reported by static compression studies were observed.","Instead, we observed an isostructural phase transition from $\\alpha$-Fe$_2$O$_3$ to a new $\\alpha^\\prime$-Fe$_2$O$_3$ phase at a pressure of 50-62 GPa.","The $\\alpha^\\prime$-Fe$_2$O$_3$ phase differs from $\\alpha$-Fe$_2$O$_3$ by an 11% volume drop and a different unit cell compressibility.","We further observed a two-wave structure in the velocity profile, which can be related to an intermediate regime where both $\\alpha$ and $\\alpha^\\prime$ phases coexist.","Density functional theory calculations with a Hubbard parameter indicate that the observed unit cell volume drop can be associated with a spin transition following a magnetic collapse."],"url":"http://arxiv.org/abs/2402.18432v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-28 15:44:33","title":"An interior-point trust-region method for nonsmooth regularized bound-constrained optimization","abstract":"We develop an interior-point method for nonsmooth regularized bound-constrained optimization problems. Our method consists of iteratively solving a sequence of unconstrained nonsmooth barrier subproblems. We use a variant of the proximal quasi-Newton trust-region algorithm TR of arXiv:2103.15993v3 to solve the barrier subproblems, with additional assumptions inspired from well-known smooth interior-point trust-region methods. We show global convergence of our algorithm with respect to the criticality measure of arXiv:2103.15993v3. Under an additional assumption linked to the convexity of the nonsmooth term in the objective, we present an alternative interior-point algorithm with a slightly modified criticality measure, which performs better in practice. Numerical experiments show that our algorithm performs better than the trust-region method TR, the trust-region method with diagonal hessian approximations TRDH of arXiv:2309.08433, and the quadratic regularization method R2 of arXiv:2103.15993v3 for two out of four tested bound-constrained problems. On those two problems, our algorithm obtains smaller objective values than the other solvers using fewer objective and gradient evaluations. On the two other problems, it performs similarly to TR, R2 and TRDH.","sentences":["We develop an interior-point method for nonsmooth regularized bound-constrained optimization problems.","Our method consists of iteratively solving a sequence of unconstrained nonsmooth barrier subproblems.","We use a variant of the proximal quasi-Newton trust-region algorithm TR of arXiv:2103.15993v3 to solve the barrier subproblems, with additional assumptions inspired from well-known smooth interior-point trust-region methods.","We show global convergence of our algorithm with respect to the criticality measure of arXiv:2103.15993v3.","Under an additional assumption linked to the convexity of the nonsmooth term in the objective, we present an alternative interior-point algorithm with a slightly modified criticality measure, which performs better in practice.","Numerical experiments show that our algorithm performs better than the trust-region method TR, the trust-region method with diagonal hessian approximations TRDH of arXiv:2309.08433, and the quadratic regularization method R2 of arXiv:2103.15993v3 for two out of four tested bound-constrained problems.","On those two problems, our algorithm obtains smaller objective values than the other solvers using fewer objective and gradient evaluations.","On the two other problems, it performs similarly to TR, R2 and TRDH."],"url":"http://arxiv.org/abs/2402.18423v1","category":"math.OC"}
{"created":"2024-02-28 15:41:45","title":"High-speed CMOS compatible plasmonic modulator for non-contact wafer-level testing","abstract":"Wafer-level testing is an important step for process and quality control of electronic chips in integrated circuit (IC) manufacturing which occurs before packaging. The process of wafer probing in its conventional contacting schemes, becomes more complicated as ICs move to smaller technology nodes and more compact designs, greatly increasing testing costs. Non-contact optical wafer probing can overcome physical probing complications, reducing costs, and increasing throughput and reliability. In this paper, a CMOS compatible, broadband (22 GHz), small footprint (5 {\\mu}m dia.) plasmonic electro-optic modulator of low insertion loss (4 dB) and wide optical working bandwidth (100 nm) is proposed and demonstrated as a potential solution for wafer-level optical testing. The device modulates in reflection an incident optical carrier emerging from an optical fiber in a non-contact arrangement, to work as a data output channel from the wafer. A modulation depth of over 2% is achieved which should be sufficient to meet the requirements of wafer-level testing. The device can be placed anywhere on wafer.","sentences":["Wafer-level testing is an important step for process and quality control of electronic chips in integrated circuit (IC) manufacturing which occurs before packaging.","The process of wafer probing in its conventional contacting schemes, becomes more complicated as ICs move to smaller technology nodes and more compact designs, greatly increasing testing costs.","Non-contact optical wafer probing can overcome physical probing complications, reducing costs, and increasing throughput and reliability.","In this paper, a CMOS compatible, broadband (22 GHz), small footprint (5 {\\mu}m dia.)","plasmonic electro-optic modulator of low insertion loss (4 dB) and wide optical working bandwidth (100 nm) is proposed and demonstrated as a potential solution for wafer-level optical testing.","The device modulates in reflection an incident optical carrier emerging from an optical fiber in a non-contact arrangement, to work as a data output channel from the wafer.","A modulation depth of over 2% is achieved which should be sufficient to meet the requirements of wafer-level testing.","The device can be placed anywhere on wafer."],"url":"http://arxiv.org/abs/2402.18421v1","category":"physics.optics"}
{"created":"2024-02-28 15:19:57","title":"Effect of a perpendicular magnetic field on bilayer graphene under dual gating","abstract":"By studying the impact of a perpendicular magnetic field $B$ on AB-bilayer graphene (AB-BLG) under dual gating, we yield several key findings for the ballistic transport of gate $U_\\infty$. Firstly, we discover that the presence of $B$ leads to a decrease in transmission. At a high value of $B$, we notice the occurrence of anti-Klein tunneling over a significant area. Secondly, in contrast to the results reported in the literature, where high peaks were found with an increasing in-plane pseudomagnetic field applied to AB-BLG, we find a decrease in conductivity as $B$ increases. However, it is worth noting that in both cases, the number of oscillations decreases compared to the result in the study where no magnetic field was present $(B = 0)$. Thirdly, at the neutrality point, we demonstrate that the conductivity decreases and eventually reaches zero for a high value of $B$, which contrasts with the result that the conductivity remains unchanged regardless of the value taken by the in-plane field. Finally, we consider the diffusive transport with gate $U_\\infty = 0.2 \\gamma_1$ and observe two scenarios. The amplitude of conductivity oscillations increases with $B$ for energy $E$ less than $U_\\infty$ but decreases in the opposite case $E>U_\\infty$.","sentences":["By studying the impact of a perpendicular magnetic field $B$ on AB-bilayer graphene (AB-BLG) under dual gating, we yield several key findings for the ballistic transport of gate $U_\\infty$. Firstly, we discover that the presence of $B$ leads to a decrease in transmission.","At a high value of $B$, we notice the occurrence of anti-Klein tunneling over a significant area.","Secondly, in contrast to the results reported in the literature, where high peaks were found with an increasing in-plane pseudomagnetic field applied to AB-BLG, we find a decrease in conductivity as $B$ increases.","However, it is worth noting that in both cases, the number of oscillations decreases compared to the result in the study where no magnetic field was present $(B = 0)$. Thirdly, at the neutrality point, we demonstrate that the conductivity decreases and eventually reaches zero for a high value of $B$, which contrasts with the result that the conductivity remains unchanged regardless of the value taken by the in-plane field.","Finally, we consider the diffusive transport with gate $U_\\infty = 0.2 \\gamma_1$ and observe two scenarios.","The amplitude of conductivity oscillations increases with $B$ for energy $E$ less than $U_\\infty$ but decreases in the opposite case $E>U_\\infty$."],"url":"http://arxiv.org/abs/2402.18399v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 15:13:51","title":"Counting rationals and diophantine approximation in missing-digit Cantor sets","abstract":"We establish a new upper bound for the number of rationals up to a given height in a missing-digit set, making progress towards a conjecture of Broderick, Fishman, and Reich. This enables us to make novel progress towards another conjecture of those authors about the corresponding intrinsic diophantine approximation problem. Moreover, we make further progress towards conjectures of Bugeaud--Durand and Levesley--Salp--Velani on the distribution of diophantine exponents in missing-digit sets.   A key tool in our study is Fourier $\\ell^1$ dimension introduced by the last named author in [H. Yu, Rational points near self-similar sets, arXiv:2101.05910]. An important technical contribution of the paper is a method to compute this quantity.","sentences":["We establish a new upper bound for the number of rationals up to a given height in a missing-digit set, making progress towards a conjecture of Broderick, Fishman, and Reich.","This enables us to make novel progress towards another conjecture of those authors about the corresponding intrinsic diophantine approximation problem.","Moreover, we make further progress towards conjectures of Bugeaud--Durand and Levesley--Salp--Velani on the distribution of diophantine exponents in missing-digit sets.   ","A key tool in our study is Fourier $\\ell^1$ dimension introduced by the last named author in [H. Yu, Rational points near self-similar sets, arXiv:2101.05910].","An important technical contribution of the paper is a method to compute this quantity."],"url":"http://arxiv.org/abs/2402.18395v1","category":"math.NT"}
{"created":"2024-02-28 13:50:39","title":"Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting","abstract":"We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\\% reduction of error in an MPJPE metric. Our source code is available in GitHub.","sentences":["We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation.","Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem.","To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process.","We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network.","The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention.","Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints.","Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\\% reduction of error in an MPJPE metric.","Our source code is available in GitHub."],"url":"http://arxiv.org/abs/2402.18330v1","category":"cs.CV"}
{"created":"2024-02-28 13:11:06","title":"Escaping Local Optima in Global Placement","abstract":"Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics. Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement. However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results. This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively. The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks.","sentences":["Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics.","Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement.","However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results.","This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively.","The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks."],"url":"http://arxiv.org/abs/2402.18311v1","category":"cs.LG"}
{"created":"2024-02-28 12:17:40","title":"Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient","abstract":"Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL.","sentences":["Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach.","However, the reasons behind its remarkable effectiveness remain unclear.","Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg).","However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly.","Therefore, two questions arise:","First, what commonalities enable various contrastive losses to achieve superior performance in SRL?","Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective?","To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio.","Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance.","Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL."],"url":"http://arxiv.org/abs/2402.18281v1","category":"cs.CL"}
{"created":"2024-02-28 10:13:30","title":"Gravity effects on a bio-inspired self-burrowing probe in granular soils","abstract":"In recent years, self-burrowing probes have been studied since they can be suitable for soil monitoring in locations with limited access such as outer space bodies and underneath existing structures. We study the performance of a self-burrowing probe under different gravity conditions, from low gravity (i.e., 1/6g, 1/3g and 1g) to high gravity (i.e., 5g, 10g and 15g), specifically in terms of penetration distance and energy consumption. Results show that the probe reaches efficient penetration in all gravity conditions and that it achieves larger penetration distances in high gravity conditions. However, the penetration efficiency, shown as unit energy per meter, is higher in low gravity. Additionally, we prove that a simple dimensional analysis provides reasonable scaling factors for first order effects in forces, velocities and energy. The findings in this study give confidence to the potential use of self-burrowing probes in campaigns of soil testing and sensor deployment in outer space or centrifuges in which the gravity conditions can differ from Earth.","sentences":["In recent years, self-burrowing probes have been studied since they can be suitable for soil monitoring in locations with limited access such as outer space bodies and underneath existing structures.","We study the performance of a self-burrowing probe under different gravity conditions, from low gravity (i.e., 1/6g, 1/3g and 1g) to high gravity (i.e., 5g, 10g and 15g), specifically in terms of penetration distance and energy consumption.","Results show that the probe reaches efficient penetration in all gravity conditions and that it achieves larger penetration distances in high gravity conditions.","However, the penetration efficiency, shown as unit energy per meter, is higher in low gravity.","Additionally, we prove that a simple dimensional analysis provides reasonable scaling factors for first order effects in forces, velocities and energy.","The findings in this study give confidence to the potential use of self-burrowing probes in campaigns of soil testing and sensor deployment in outer space or centrifuges in which the gravity conditions can differ from Earth."],"url":"http://arxiv.org/abs/2402.18215v1","category":"cond-mat.soft"}
{"created":"2024-02-28 09:27:29","title":"Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation","abstract":"With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.","sentences":["With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged.","Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data.","However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset.","In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR).","CaR consists of two steps.","The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%).","The second step involves preserving dataset diversity through a clustering process.","In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations.","Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios."],"url":"http://arxiv.org/abs/2402.18191v1","category":"cs.CL"}
{"created":"2024-02-28 09:21:00","title":"Bayesian Geographically Weighted Regression using Fused Lasso Prior","abstract":"A main purpose of spatial data analysis is to predict the objective variable for the unobserved locations. Although Geographically Weighted Regression (GWR) is often used for this purpose, estimation instability proves to be an issue. To address this issue, Bayesian Geographically Weighted Regression (BGWR) has been proposed. In BGWR, by setting the same prior distribution for all locations, the coefficients' estimation stability is improved. However, when observation locations' density is spatially different, these methods do not sufficiently consider the similarity of coefficients among locations. Moreover, the prediction accuracy of these methods becomes worse. To solve these issues, we propose Bayesian Geographically Weighted Sparse Regression (BGWSR) that uses Bayesian Fused Lasso for the prior distribution of the BGWR coefficients. Constraining the parameters to have the same values at adjacent locations is expected to improve the prediction accuracy at locations with a low number of adjacent locations. Furthermore, from the predictive distribution, it is also possible to evaluate the uncertainty of the predicted value of the objective variable. By examining numerical studies, we confirmed that BGWSR has better prediction performance than the existing methods (GWR and BGWR) when the density of observation locations is spatial difference. Finally, the BGWSR is applied to land price data in Tokyo. Thus, the results suggest that BGWSR has better prediction performance and smaller uncertainty than existing methods.","sentences":["A main purpose of spatial data analysis is to predict the objective variable for the unobserved locations.","Although Geographically Weighted Regression (GWR) is often used for this purpose, estimation instability proves to be an issue.","To address this issue, Bayesian Geographically Weighted Regression (BGWR) has been proposed.","In BGWR, by setting the same prior distribution for all locations, the coefficients' estimation stability is improved.","However, when observation locations' density is spatially different, these methods do not sufficiently consider the similarity of coefficients among locations.","Moreover, the prediction accuracy of these methods becomes worse.","To solve these issues, we propose Bayesian Geographically Weighted Sparse Regression (BGWSR) that uses Bayesian Fused Lasso for the prior distribution of the BGWR coefficients.","Constraining the parameters to have the same values at adjacent locations is expected to improve the prediction accuracy at locations with a low number of adjacent locations.","Furthermore, from the predictive distribution, it is also possible to evaluate the uncertainty of the predicted value of the objective variable.","By examining numerical studies, we confirmed that BGWSR has better prediction performance than the existing methods (GWR and BGWR) when the density of observation locations is spatial difference.","Finally, the BGWSR is applied to land price data in Tokyo.","Thus, the results suggest that BGWSR has better prediction performance and smaller uncertainty than existing methods."],"url":"http://arxiv.org/abs/2402.18186v1","category":"stat.ME"}
{"created":"2024-02-28 08:44:23","title":"Interior pointwise $C^\u03b1$ regularity for elliptic and parabolic equations with divergence-free drifts","abstract":"We investigate the interior pointwise $C^{\\alpha}$ regularity for weak solutions of elliptic and parabolic equations with divergence-free drifts. For such equations, the integrability condition on the drift can be relaxed and the interior $C^{\\alpha}$ regularity for some $0<\\alpha<1$ has been obtained previously with the aid of Harnack inequality. In this paper, we prove the interior pointwise $C^{\\alpha}$ regularity for any $0<\\alpha<1$ provided that the drift is small. We obtain the regularity under three different types conditions on the drift. The proof is based on the energy inequality and the perturbation technique.","sentences":["We investigate the interior pointwise $C^{\\alpha}$ regularity for weak solutions of elliptic and parabolic equations with divergence-free drifts.","For such equations, the integrability condition on the drift can be relaxed and the interior $C^{\\alpha}$ regularity for some $0<\\alpha<1$ has been obtained previously with the aid of Harnack inequality.","In this paper, we prove the interior pointwise $C^{\\alpha}$ regularity for any $0<\\alpha<1$ provided that the drift is small.","We obtain the regularity under three different types conditions on the drift.","The proof is based on the energy inequality and the perturbation technique."],"url":"http://arxiv.org/abs/2402.18161v1","category":"math.AP"}
{"created":"2024-02-28 08:24:06","title":"Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation","abstract":"This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interest to the theoretical study of reinforcement learning.","sentences":["This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration.","We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure.","We develop the first provably efficient RL algorithm tailored for this setting.","We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings.","We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations.","These techniques are of particular interest to the theoretical study of reinforcement learning."],"url":"http://arxiv.org/abs/2402.18149v1","category":"cs.LG"}
{"created":"2024-02-28 07:34:37","title":"27Al-NMR Study on a square-Kagome lattice antiferromagnet","abstract":"NMR study has been performed on an S = 1/2 antiferromagnet KCu6AlBiO4(SO4)5Cl on the square-Kagome lattice, which has three slightly inequivalent nearest-neighbor interactions. Because of the geometrical frustration inherited from triangles within the square kagome lattice and of the low dimensionality, a long range magnetic order is strongly suppressed; its absence has so far been confirmed in low temperatures down to dilution refrigerator region. 27Al-NMR spectra and the longitudinal relaxation time T1 were measured by a conventional pulsed spectrometer on powder sample under several magnetic fields between 3 and 10 T and in low temperatures down to 0.35 K. The NMR line width due to the inhomogeneous broadening increased with lowering temperatures and leveled off below 3 K, where FWHM reached the value as large as 0.1 T, implying that the ground state is magnetic one, consistent with previous reports. On the other hand, the longitudinal nuclear spin relaxation rate 1/T1 obeyed the Arrhenius law with the thermal activation energy {\\Delta} = 2K at low temperatures, suggesting that a small gap is formed in the spin excitation spectrum.","sentences":["NMR study has been performed on an S = 1/2 antiferromagnet KCu6AlBiO4(SO4)5Cl on the square-Kagome lattice, which has three slightly inequivalent nearest-neighbor interactions.","Because of the geometrical frustration inherited from triangles within the square kagome lattice and of the low dimensionality, a long range magnetic order is strongly suppressed; its absence has so far been confirmed in low temperatures down to dilution refrigerator region.","27Al-NMR spectra and the longitudinal relaxation time T1 were measured by a conventional pulsed spectrometer on powder sample under several magnetic fields between 3 and 10 T and in low temperatures down to 0.35","K. The NMR line width due to the inhomogeneous broadening increased with lowering temperatures and leveled off below 3 K, where FWHM reached the value as large as 0.1 T, implying that the ground state is magnetic one, consistent with previous reports.","On the other hand, the longitudinal nuclear spin relaxation rate 1/T1 obeyed the Arrhenius law with the thermal activation energy {\\Delta} = 2K at low temperatures, suggesting that a small gap is formed in the spin excitation spectrum."],"url":"http://arxiv.org/abs/2402.18125v1","category":"cond-mat.str-el"}
{"created":"2024-02-28 07:31:38","title":"From Simulations to Reality: Dark Energy Reconstruction with Simulated SNIa data from the Vera C. Rubin Observatory","abstract":"In this paper, we present an Artificial Neural Network (ANN) based reconstruction analysis of the Supernova Ia (SNIa) distance moduli ($\\mu(z)$), and hence dark energy, using LSST simulated three-year SNIa data. Our ANN reconstruction architecture can model both the distance moduli and their corresponding error estimates. For this we employ astroANN and incorporate Monte Carlo dropout techniques to quantify uncertainties in our predictions. We tune our hyperparameters through advanced genetic algorithms, including elitism, utilizing the DEAP library. We compared the performance of the ANN based reconstruction with two theoretical descriptions of dark energy models, $\\Lambda$CDM and Chevallier-Linder-Polarski (CPL). We perform a Bayesian analysis for these two theoretical models using the LSST simulations and also compare with observations from Pantheon and Pantheon+ SNIa real data. We show that our model-independent reconstruction using ANN is consistent with both of them. We assessed the performance using mean squared error (MSE) and showed that the ANN can produce distance estimates in better agreement with the LSST dataset than either $\\Lambda$CDM or CPL, albeit very small. We included an additional residual analysis and a null test with $F$-scores to show that the reconstructed distances from the ANN model, are in excellent agreement with the $\\Lambda$CDM or CPL model.","sentences":["In this paper, we present an Artificial Neural Network (ANN) based reconstruction analysis of the Supernova Ia (SNIa) distance moduli ($\\mu(z)$), and hence dark energy, using LSST simulated three-year SNIa data.","Our ANN reconstruction architecture can model both the distance moduli and their corresponding error estimates.","For this we employ astroANN and incorporate Monte Carlo dropout techniques to quantify uncertainties in our predictions.","We tune our hyperparameters through advanced genetic algorithms, including elitism, utilizing the DEAP library.","We compared the performance of the ANN based reconstruction with two theoretical descriptions of dark energy models, $\\Lambda$CDM and Chevallier-Linder-Polarski (CPL).","We perform a Bayesian analysis for these two theoretical models using the LSST simulations and also compare with observations from Pantheon and Pantheon+ SNIa real data.","We show that our model-independent reconstruction using ANN is consistent with both of them.","We assessed the performance using mean squared error (MSE) and showed that the ANN can produce distance estimates in better agreement with the LSST dataset than either $\\Lambda$CDM or CPL, albeit very small.","We included an additional residual analysis and a null test with $F$-scores to show that the reconstructed distances from the ANN model, are in excellent agreement with the $\\Lambda$CDM or CPL model."],"url":"http://arxiv.org/abs/2402.18124v1","category":"astro-ph.CO"}
{"created":"2024-02-28 07:27:23","title":"Fixture calibration with guaranteed bounds from a few correspondence-free surface points","abstract":"Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in downstream tasks. Contact-based calibration methods let the user measure points on the fixture's surface with a tool tip attached to the robot's end effector. Most methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture's surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration.","sentences":["Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in downstream tasks.","Contact-based calibration methods let the user measure points on the fixture's surface with a tool tip attached to the robot's end effector.","Most methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience.","We propose a correspondence-free alternative: The user simply measures a few points from the fixture's surface, and our method provides a tight superset of the poses which could explain the measured points.","This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user.","Perhaps more importantly, it provides guaranteed bounds on the pose.","The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3).","Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration."],"url":"http://arxiv.org/abs/2402.18123v1","category":"cs.RO"}
{"created":"2024-02-28 07:05:27","title":"UniVS: Unified and Universal Video Segmentation with Prompts as Queries","abstract":"Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \\url{https://github.com/MinghanLi/UniVS}.","sentences":["Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge.","This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture.","We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries.","UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool.","By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process.","Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios.","UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks.","Code can be found at \\url{https://github.com/MinghanLi/UniVS}."],"url":"http://arxiv.org/abs/2402.18115v1","category":"cs.CV"}
{"created":"2024-02-28 07:02:08","title":"Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input","abstract":"Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability. We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers. This method is simple yet effective. In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability. We will make our experiment data available on GitHub.","sentences":["Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity.","To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data.","Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability.","We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers.","This method is simple yet effective.","In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability.","We will make our experiment data available on GitHub."],"url":"http://arxiv.org/abs/2402.18112v1","category":"eess.SP"}
{"created":"2024-02-28 06:58:26","title":"On global regularity of some bi-rotational Euler flows in $\\mathbb{R}^{4}$","abstract":"In this paper, we consider incompressible Euler flows in $ \\mathbb{R}^{4} $ under bi-rotational symmetry, namely solutions that are invariant under rotations in $\\mathbb{R}^{4}$ fixing either the first two or last two axes. With the additional swirl-free assumption, our first main result gives local wellposedness of Yudovich-type solutions, extending the work of Danchin [Uspekhi Mat. Nauk 62(2007), no.3, 73-94] for axisymmetric flows in $\\mathbb{R}^{3}$. The second main result establishes global wellposedness under additional decay conditions near the axes and at infinity. This in particular gives global regularity of $C^{\\infty}$ smooth and decaying Euler flows in $\\mathbb{R}^{4}$ subject to bi-rotational symmetry without swirl.","sentences":["In this paper, we consider incompressible Euler flows in $ \\mathbb{R}^{4} $ under bi-rotational symmetry, namely solutions that are invariant under rotations in $\\mathbb{R}^{4}$ fixing either the first two or last two axes.","With the additional swirl-free assumption, our first main result gives local wellposedness of Yudovich-type solutions, extending the work of Danchin","[Uspekhi Mat.","Nauk 62(2007), no.3, 73-94] for axisymmetric flows in $\\mathbb{R}^{3}$. The second main result establishes global wellposedness under additional decay conditions near the axes and at infinity.","This in particular gives global regularity of $C^{\\infty}$ smooth and decaying Euler flows in $\\mathbb{R}^{4}$ subject to bi-rotational symmetry without swirl."],"url":"http://arxiv.org/abs/2402.18111v1","category":"math.AP"}
{"created":"2024-02-28 06:54:33","title":"On the Stability of the $s$-Nonlocal $p$-Obstacle Problem and their Coincidence Sets and Free Boundaries","abstract":"We show that the solutions to the nonlocal obstacle problems for the nonlocal $-\\Delta_p^s$ operator, when the fractional parameter $s\\to\\sigma$ for $0<\\sigma\\leq1$, converge to the solution of the corresponding obstacle problem for $-\\Delta_p^\\sigma$, being $\\sigma=1$ the classical obstacle problem for the local $p$-Laplacian. We discuss the weak stability of the quasi-characteristic functions of coincidence sets of the solution with the obstacle, which is a strong convergence of their characteristic functions when $s\\nearrow 1$ under a nondegeneracy condition. This stability can be shown also in terms of the convergence of the free boundaries, as well as of the coincidence sets, in Hausdorff distance when $s\\nearrow 1$, under non-degeneracy local assumptions on the external force and a local topological property of the coincidence set of the limit classical obstacle problem for the local $p$-Laplacian, essentially when the limit coincidence set is the closure of its interior.","sentences":["We show that the solutions to the nonlocal obstacle problems for the nonlocal $-\\Delta_p^s$ operator, when the fractional parameter $s\\to\\sigma$ for $0<\\sigma\\leq1$, converge to the solution of the corresponding obstacle problem for $-\\Delta_p^\\sigma$, being $\\sigma=1$ the classical obstacle problem for the local $p$-Laplacian.","We discuss the weak stability of the quasi-characteristic functions of coincidence sets of the solution with the obstacle, which is a strong convergence of their characteristic functions when $s\\nearrow 1$ under a nondegeneracy condition.","This stability can be shown also in terms of the convergence of the free boundaries, as well as of the coincidence sets, in Hausdorff distance when $s\\nearrow 1$, under non-degeneracy local assumptions on the external force and a local topological property of the coincidence set of the limit classical obstacle problem for the local $p$-Laplacian, essentially when the limit coincidence set is the closure of its interior."],"url":"http://arxiv.org/abs/2402.18106v1","category":"math.AP"}
{"created":"2024-02-28 06:29:46","title":"On the Existence of Cyclic Lattice Codes","abstract":"A coding lattice $\\Lambda_c$ and a shaping lattice $\\Lambda_s$ forms a nested lattice code $\\mathcal{C}$ if $\\Lambda_s \\subseteq \\Lambda_c$. Under some conditions, $\\mathcal{C}$ is a finite cyclic group formed by rectangular encoding. This paper presents the conditions for the existence of such $\\mathcal{C}$ and provides some designs. These designs correspond to solutions to linear Diophantine equations so that a cyclic lattice code $\\mathcal C$ of arbitrary codebook size $M$ can possess group isomorphism, which is an essential property for a nested lattice code to be applied in physical layer network relaying techniques such as compute and forward.","sentences":["A coding lattice $\\Lambda_c$ and a shaping lattice $\\Lambda_s$ forms a nested lattice code $\\mathcal{C}$ if $\\Lambda_s \\subseteq \\Lambda_c$. Under some conditions, $\\mathcal{C}$ is a finite cyclic group formed by rectangular encoding.","This paper presents the conditions for the existence of such $\\mathcal{C}$ and provides some designs.","These designs correspond to solutions to linear Diophantine equations so that a cyclic lattice code $\\mathcal C$ of arbitrary codebook size $M$ can possess group isomorphism, which is an essential property for a nested lattice code to be applied in physical layer network relaying techniques such as compute and forward."],"url":"http://arxiv.org/abs/2402.18094v1","category":"cs.IT"}
{"created":"2024-02-28 06:07:03","title":"Locating heating channels of the solar corona in a plage region with the aid of high-resolution 10830 \u00c5 filtergrams","abstract":"In this paper, with a set of high-resolution He I 10830 \\AA\\ filtergrams, we select an area in a plage, very likely an EUV moss area, as an interface layer to follow the clues of coronal heating channels down to the photosphere. The filtergrams are obtained from the 1-meter aperture New Vacuum Solar Telescope (NVST). We make a distinction between the darker and the brighter regions in the selected area and name the two regions enhanced absorption patches (EAPs) and low absorption patches (LAPs). With well-aligned, nearly simultaneous data from multiple channels of the AIA and the continuum of the HMI on board SDO, we compare the EUV/UV emissions, emission measure, mean temperature, and continuum intensity in the two kinds of regions. The following progress is made: 1) The mean EUV emissions over EAPs are mostly stronger than the corresponding emissions over LAPs except for the emission at 335 \\AA. The UV emissions at 1600 and 1700 \\AA\\ fail to capture the difference between the two regions. 2) In the logarithmic temperature range of 5.6-6.2, EAPs have higher EUV emission measure than LAPs, but they have lower mean coronal temperature. 3) The mean continuum intensity over EAPs is lower. Based on the above progress, we suggest that the energy for coronal heating in the moss region can be traced down to some areas in intergranular lanes with enhanced density of both cool and hot material. The lower temperature over the EAPs is due to the greater fraction of cool material over there.","sentences":["In this paper, with a set of high-resolution He I 10830 \\AA\\ filtergrams, we select an area in a plage, very likely an EUV moss area, as an interface layer to follow the clues of coronal heating channels down to the photosphere.","The filtergrams are obtained from the 1-meter aperture New Vacuum Solar Telescope (NVST).","We make a distinction between the darker and the brighter regions in the selected area and name the two regions enhanced absorption patches (EAPs) and low absorption patches (LAPs).","With well-aligned, nearly simultaneous data from multiple channels of the AIA and the continuum of the HMI on board SDO, we compare the EUV/UV emissions, emission measure, mean temperature, and continuum intensity in the two kinds of regions.","The following progress is made: 1) The mean EUV emissions over EAPs are mostly stronger than the corresponding emissions over LAPs except for the emission at 335 \\AA.","The UV emissions at 1600 and 1700 \\AA\\ fail to capture the difference between the two regions.","2) In the logarithmic temperature range of 5.6-6.2, EAPs have higher EUV emission measure than LAPs, but they have lower mean coronal temperature.","3) The mean continuum intensity over EAPs is lower.","Based on the above progress, we suggest that the energy for coronal heating in the moss region can be traced down to some areas in intergranular lanes with enhanced density of both cool and hot material.","The lower temperature over the EAPs is due to the greater fraction of cool material over there."],"url":"http://arxiv.org/abs/2402.18077v1","category":"astro-ph.SR"}
{"created":"2024-02-28 06:01:44","title":"A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing","abstract":"Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading. Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP. Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency. To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP. A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner. We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources. Experiment results demonstrate that our prototype achieves $2\\times$ and $2.3\\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP benchmarks. Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration.","sentences":["Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading.","Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP.","Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency.","To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP.","A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner.","We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources.","Experiment results demonstrate that our prototype achieves $2\\times$ and $2.3\\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP benchmarks.","Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration."],"url":"http://arxiv.org/abs/2402.18070v1","category":"cs.AR"}
{"created":"2024-02-28 05:50:18","title":"A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains","abstract":"Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.","sentences":["Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications.","When turning at high speeds, these robots tend to undergo significant skidding and slipping.","In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion.","Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models.","By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions.","The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions.","We validate our work on a benchmark real-world multi-terrain SSWMR dataset.","Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions.","As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm."],"url":"http://arxiv.org/abs/2402.18065v1","category":"cs.RO"}
{"created":"2024-02-28 04:56:21","title":"Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension","abstract":"We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.","sentences":["We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs.","Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs.","In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations.","Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method.","Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs."],"url":"http://arxiv.org/abs/2402.18048v1","category":"cs.CL"}
{"created":"2024-02-28 04:29:10","title":"Unveiling NPT bound problem: From Distillability Sets to Inequalities and Multivariable Insights","abstract":"Equivalence between Positive Partial Transpose (PPT) entanglement and bound entanglement is a long-standing open problem in quantum information theory. So far limited progress has been made, even on the seemingly simple case of Werner states bound entanglement. The primary challenge is to give a concise mathematical representation of undistillability. To this end, we propose a decomposition of the N-undistillability verification into $log(N)$ repeated steps of 1-undistillability verification. For Werner state N-undistillability verification, a bound for N-undistillability is given, which is independent of the dimensionality of Werner states. Equivalent forms of inequalities for both rank one and two matrices are presented, before transforming the two-undistillability case into a matrix analysis problem. A new perspective is also attempted by seeing it as a non-convex multi-variable function, proving its critical points and conjecturing Hessian positivity, which would make them local minimums.","sentences":["Equivalence between Positive Partial Transpose (PPT) entanglement and bound entanglement is a long-standing open problem in quantum information theory.","So far limited progress has been made, even on the seemingly simple case of Werner states bound entanglement.","The primary challenge is to give a concise mathematical representation of undistillability.","To this end, we propose a decomposition of the N-undistillability verification into $log(N)$ repeated steps of 1-undistillability verification.","For Werner state N-undistillability verification, a bound for N-undistillability is given, which is independent of the dimensionality of Werner states.","Equivalent forms of inequalities for both rank one and two matrices are presented, before transforming the two-undistillability case into a matrix analysis problem.","A new perspective is also attempted by seeing it as a non-convex multi-variable function, proving its critical points and conjecturing Hessian positivity, which would make them local minimums."],"url":"http://arxiv.org/abs/2402.18037v1","category":"quant-ph"}
{"created":"2024-02-28 03:20:51","title":"Obtaining properly Pareto optimal solutions of multiobjective optimization problems via the branch and bound method","abstract":"In multiobjective optimization, most branch and bound algorithms provide the decision maker with the whole Pareto front, and then decision maker could select a single solution finally. However, if the number of objectives is large, the number of candidate solutions may be also large, and it may be difficult for the decision maker to select the most interesting solution. As we argue in this paper, the most interesting solutions are the ones whose trade-offs are bounded. These solutions are usually known as the properly Pareto optimal solutions. We propose a branch-and-bound-based algorithm to provide the decision maker with so-called $\\epsilon$-properly Pareto optimal solutions. The discarding test of the algorithm adopts a dominance relation induced by a convex polyhedral cone instead of the common used Pareto dominance relation. In this way, the proposed algorithm excludes the subboxes which do not contain $\\epsilon$-properly Pareto optimal solution from further exploration. We establish the global convergence results of the proposed algorithm. Finally, the algorithm is applied to benchmark problems as well as to two real-world optimization problems.","sentences":["In multiobjective optimization, most branch and bound algorithms provide the decision maker with the whole Pareto front, and then decision maker could select a single solution finally.","However, if the number of objectives is large, the number of candidate solutions may be also large, and it may be difficult for the decision maker to select the most interesting solution.","As we argue in this paper, the most interesting solutions are the ones whose trade-offs are bounded.","These solutions are usually known as the properly Pareto optimal solutions.","We propose a branch-and-bound-based algorithm to provide the decision maker with so-called $\\epsilon$-properly Pareto optimal solutions.","The discarding test of the algorithm adopts a dominance relation induced by a convex polyhedral cone instead of the common used Pareto dominance relation.","In this way, the proposed algorithm excludes the subboxes which do not contain $\\epsilon$-properly Pareto optimal solution from further exploration.","We establish the global convergence results of the proposed algorithm.","Finally, the algorithm is applied to benchmark problems as well as to two real-world optimization problems."],"url":"http://arxiv.org/abs/2402.18015v1","category":"math.OC"}
{"created":"2024-02-28 03:05:48","title":"How to coadd images: II. Anti-aliasing and PSF deconvolution","abstract":"We have developed a novel method for co-adding multiple under-sampled images that combines the iteratively reweighted least squares and divide-and-conquer algorithms. Our approach not only allows for the anti-aliasing of the images but also enables PSF deconvolution, resulting in enhanced restoration of extended sources, the highest PSNR, and reduced ringing artefacts. To test our method, we conducted numerical simulations that replicated observation runs of the CSST/VST telescope and compared our results to those obtained using previous algorithms. The simulation showed that our method outperforms previous approaches in several ways, such as restoring the profile of extended sources and minimizing ringing artefacts. Additionally, because our method relies on the inherent advantages of least squares fitting, it is more versatile and does not depend on the local uniformity hypothesis for the PSF. However, the new method consumes much more computation than the other approaches.","sentences":["We have developed a novel method for co-adding multiple under-sampled images that combines the iteratively reweighted least squares and divide-and-conquer algorithms.","Our approach not only allows for the anti-aliasing of the images but also enables PSF deconvolution, resulting in enhanced restoration of extended sources, the highest PSNR, and reduced ringing artefacts.","To test our method, we conducted numerical simulations that replicated observation runs of the CSST/VST telescope and compared our results to those obtained using previous algorithms.","The simulation showed that our method outperforms previous approaches in several ways, such as restoring the profile of extended sources and minimizing ringing artefacts.","Additionally, because our method relies on the inherent advantages of least squares fitting, it is more versatile and does not depend on the local uniformity hypothesis for the PSF.","However, the new method consumes much more computation than the other approaches."],"url":"http://arxiv.org/abs/2402.18010v1","category":"astro-ph.IM"}
{"created":"2024-02-28 02:46:06","title":"Fast and Interpretable 2D Homography Decomposition: Similarity-Kernel-Similarity and Affine-Core-Affine Transformations","abstract":"In this paper, we present two fast and interpretable decomposition methods for 2D homography, which are named Similarity-Kernel-Similarity (SKS) and Affine-Core-Affine (ACA) transformations respectively. Under the minimal $4$-point configuration, the first and the last similarity transformations in SKS are computed by two anchor points on target and source planes, respectively. Then, the other two point correspondences can be exploited to compute the middle kernel transformation with only four parameters. Furthermore, ACA uses three anchor points to compute the first and the last affine transformations, followed by computation of the middle core transformation utilizing the other one point correspondence. ACA can compute a homography up to a scale with only $85$ floating-point operations (FLOPs), without even any division operations. Therefore, as a plug-in module, ACA facilitates the traditional feature-based Random Sample Consensus (RANSAC) pipeline, as well as deep homography pipelines estimating $4$-point offsets. In addition to the advantages of geometric parameterization and computational efficiency, SKS and ACA can express each element of homography by a polynomial of input coordinates ($7$th degree to $9$th degree), extend the existing essential Similarity-Affine-Projective (SAP) decomposition and calculate 2D affine transformations in a unified way. Source codes are released in https://github.com/cscvlab/SKS-Homography.","sentences":["In this paper, we present two fast and interpretable decomposition methods for 2D homography, which are named Similarity-Kernel-Similarity (SKS) and Affine-Core-Affine (ACA) transformations respectively.","Under the minimal $4$-point configuration, the first and the last similarity transformations in SKS are computed by two anchor points on target and source planes, respectively.","Then, the other two point correspondences can be exploited to compute the middle kernel transformation with only four parameters.","Furthermore, ACA uses three anchor points to compute the first and the last affine transformations, followed by computation of the middle core transformation utilizing the other one point correspondence.","ACA can compute a homography up to a scale with only $85$ floating-point operations (FLOPs), without even any division operations.","Therefore, as a plug-in module, ACA facilitates the traditional feature-based Random Sample Consensus (RANSAC) pipeline, as well as deep homography pipelines estimating $4$-point offsets.","In addition to the advantages of geometric parameterization and computational efficiency, SKS and ACA can express each element of homography by a polynomial of input coordinates ($7$th degree to $9$th degree), extend the existing essential Similarity-Affine-Projective (SAP) decomposition and calculate 2D affine transformations in a unified way.","Source codes are released in https://github.com/cscvlab/SKS-Homography."],"url":"http://arxiv.org/abs/2402.18008v1","category":"cs.CV"}
{"created":"2024-02-28 02:42:07","title":"Characterization of birefringence inhomogeneity of KAGRA sapphire mirrors from transmitted wavefront error measurements","abstract":"Future gravitational wave detectors are going to cool down their test masses to cryogenic temperatures to reduce thermal noise. Crystalline materials are considered most promising for making test masses and their coatings due to their excellent thermal and optical properties at low temperatures. However, birefringence due to local impurities and inhomogeneities in the crystal can degrade the performance of the detector. Birefringence measurement, or birefringence mapping over a two-dimensional area has become important. This paper describes a method of fast birefringence measurement for a large sample by simply combining a series of transmission wavefront error measurements using linearly polarized light with Fizeau interferometers. With this method, the birefringence inhomogeneity of two KAGRA's input test masses with diameter of 22~cm is fully reconstructed. The birefringence information is then used to calculate the transverse beam shape of the light fields in orthogonal polarization directions when passing through the substrate. The calculated beam shapes show a good match with in-situ measurements using KAGRA interferometer. This technique is crucial for birefringence characterization of test masses in future detectors where even larger size are used.","sentences":["Future gravitational wave detectors are going to cool down their test masses to cryogenic temperatures to reduce thermal noise.","Crystalline materials are considered most promising for making test masses and their coatings due to their excellent thermal and optical properties at low temperatures.","However, birefringence due to local impurities and inhomogeneities in the crystal can degrade the performance of the detector.","Birefringence measurement, or birefringence mapping over a two-dimensional area has become important.","This paper describes a method of fast birefringence measurement for a large sample by simply combining a series of transmission wavefront error measurements using linearly polarized light with Fizeau interferometers.","With this method, the birefringence inhomogeneity of two KAGRA's input test masses with diameter of 22~cm is fully reconstructed.","The birefringence information is then used to calculate the transverse beam shape of the light fields in orthogonal polarization directions when passing through the substrate.","The calculated beam shapes show a good match with in-situ measurements using KAGRA interferometer.","This technique is crucial for birefringence characterization of test masses in future detectors where even larger size are used."],"url":"http://arxiv.org/abs/2402.18006v1","category":"physics.optics"}
{"created":"2024-02-28 02:13:12","title":"Hydrogen bonding in water under extreme confinement unveiled by nanoscale vibrational spectroscopy and simulations","abstract":"Fluids under extreme confinement exhibit distinctly new properties compared to their bulk analogs. Understanding the structure and intermolecular bonding of confined water lays the foundation for creating and improving applications at the water-energy nexus. However, probing confined water experimentally at the length scale of intermolecular and surface forces has remained a challenge. Here, we report a combined experiment/theory framework to reveal changes in H-bonding environment and the underlying molecular structure of confined water inside individual carbon nanotubes. H-bonding is directly probed through the O-H stretch frequency with vibrational electron energy-loss spectroscopy and compared to spectra from molecular-dynamics simulations based on density-functional-theory. Experimental spectra show that water in larger carbon nanotubes exhibit the bonded O-H vibrations of bulk water, but at smaller diameters, the frequency blueshifts to near the 'free' O-H stretch found in water vapor and hydrophobic surfaces. The matching simulations reveal that, in addition to steric confinement, the tube's vibrations play a key role in breaking up the H-bond network, resulting in an orientationally-dispersed, non-H-bonded phase. Furthermore, the temperature-dependence of the vibrations is investigated, providing insights into phase transitions and the confined-water density. This research demonstrates the potential of the experiment/theory framework to explore unprecedented aspects of structure and bonding in confined fluids.","sentences":["Fluids under extreme confinement exhibit distinctly new properties compared to their bulk analogs.","Understanding the structure and intermolecular bonding of confined water lays the foundation for creating and improving applications at the water-energy nexus.","However, probing confined water experimentally at the length scale of intermolecular and surface forces has remained a challenge.","Here, we report a combined experiment/theory framework to reveal changes in H-bonding environment and the underlying molecular structure of confined water inside individual carbon nanotubes.","H-bonding is directly probed through the O-H stretch frequency with vibrational electron energy-loss spectroscopy and compared to spectra from molecular-dynamics simulations based on density-functional-theory.","Experimental spectra show that water in larger carbon nanotubes exhibit the bonded O-H vibrations of bulk water, but at smaller diameters, the frequency blueshifts to near the 'free' O-H stretch found in water vapor and hydrophobic surfaces.","The matching simulations reveal that, in addition to steric confinement, the tube's vibrations play a key role in breaking up the H-bond network, resulting in an orientationally-dispersed, non-H-bonded phase.","Furthermore, the temperature-dependence of the vibrations is investigated, providing insights into phase transitions and the confined-water density.","This research demonstrates the potential of the experiment/theory framework to explore unprecedented aspects of structure and bonding in confined fluids."],"url":"http://arxiv.org/abs/2402.17989v1","category":"physics.chem-ph"}
{"created":"2024-02-28 02:12:47","title":"Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars","abstract":"Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for Python 3. Our results demonstrate that constrained generation can significantly reduce the incidence of syntax errors in recommended code.","sentences":["Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct.","This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks.","We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages.","The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   ","To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for Python 3.","Our results demonstrate that constrained generation can significantly reduce the incidence of syntax errors in recommended code."],"url":"http://arxiv.org/abs/2402.17988v1","category":"cs.PL"}
{"created":"2024-02-28 01:56:36","title":"Sampling low-fidelity outputs for estimation of high-fidelity density and its tails","abstract":"In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g. computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive. This work studies for which low-fidelity outputs, one should obtain high-fidelity outputs, if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes. It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory. The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations. The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities.","sentences":["In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g. computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive.","This work studies for which low-fidelity outputs, one should obtain high-fidelity outputs, if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes.","It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory.","The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations.","The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities."],"url":"http://arxiv.org/abs/2402.17984v1","category":"stat.ME"}
{"created":"2024-02-28 01:50:31","title":"Emergence of Large-Scale Structures in Holographic Superfluid Turbulence","abstract":"In two-dimensional turbulence systems, the emergence of large-scale structures holds profound physical implications, particularly as it indicates the occurrence of inverse energy cascades, thereby garnering significant attention. In this paper, we report a novel vortex clusters formation in the background of near-extreme Reissner-Nordstr$\\ddot{o}$m black hole holographic model. At temperatures nearing absolute zero, we observe not only the formation of vortex clusters but also the emergence of an inverse energy cascade. Distinct from typical quantum systems, the genesis of holographic vortex clusters is rooted in unique quantum dissipation properties, characterized by the near immobilization of vortex dipoles at low temperatures. Through a comparative analysis with the dynamics of the Gross-Pitaevskii equation, our investigation enhances the understanding of inverse energy cascades under these extreme conditions, thereby broadening our comprehension of quantum turbulence.","sentences":["In two-dimensional turbulence systems, the emergence of large-scale structures holds profound physical implications, particularly as it indicates the occurrence of inverse energy cascades, thereby garnering significant attention.","In this paper, we report a novel vortex clusters formation in the background of near-extreme Reissner-Nordstr$\\ddot{o}$m black hole holographic model.","At temperatures nearing absolute zero, we observe not only the formation of vortex clusters but also the emergence of an inverse energy cascade.","Distinct from typical quantum systems, the genesis of holographic vortex clusters is rooted in unique quantum dissipation properties, characterized by the near immobilization of vortex dipoles at low temperatures.","Through a comparative analysis with the dynamics of the Gross-Pitaevskii equation, our investigation enhances the understanding of inverse energy cascades under these extreme conditions, thereby broadening our comprehension of quantum turbulence."],"url":"http://arxiv.org/abs/2402.17980v1","category":"hep-th"}
{"created":"2024-02-28 01:42:31","title":"Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks","abstract":"Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. These attack methods have garnered considerable attention from researchers in recent years. However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking. To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process. DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments. We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability. Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead. We will make our code publicly available soon.","sentences":["Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images.","These attack methods have garnered considerable attention from researchers in recent years.","However, there is still a lack of research on designing adversarial defense methods specifically for visual object tracking.","To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates adversarial perturbations during the tracking process.","DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images.","Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments.","We train DuaLossDef using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker.","Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that DuaLossDef maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios.","Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability.","Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead.","We will make our code publicly available soon."],"url":"http://arxiv.org/abs/2402.17976v1","category":"cs.CV"}
{"created":"2024-02-28 01:33:49","title":"From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments","abstract":"Purpose: Accurate tool segmentation is essential in computer-aided procedures. However, this task conveys challenges due to artifacts' presence and the limited training data in medical scenarios. Methods that generalize to unseen data represent an interesting venue, where zero-shot segmentation presents an option to account for data limitation. Initial exploratory works with the Segment Anything Model (SAM) show that bounding-box-based prompting presents notable zero-short generalization. However, point-based prompting leads to a degraded performance that further deteriorates under image corruption. We argue that SAM drastically over-segment images with high corruption levels, resulting in degraded performance when only a single segmentation mask is considered, while the combination of the masks overlapping the object of interest generates an accurate prediction. Method: We use SAM to generate the over-segmented prediction of endoscopic frames. Then, we employ the ground-truth tool mask to analyze the results of SAM when the best single mask is selected as prediction and when all the individual masks overlapping the object of interest are combined to obtain the final predicted mask. We analyze the Endovis18 and Endovis17 instrument segmentation datasets using synthetic corruptions of various strengths and an In-House dataset featuring counterfactually created real-world corruptions. Results: Combining the over-segmented masks contributes to improvements in the IoU. Furthermore, selecting the best single segmentation presents a competitive IoU score for clean images. Conclusions: Combined SAM predictions present improved results and robustness up to a certain corruption level. However, appropriate prompting strategies are fundamental for implementing these models in the medical domain.","sentences":["Purpose:","Accurate tool segmentation is essential in computer-aided procedures.","However, this task conveys challenges due to artifacts' presence and the limited training data in medical scenarios.","Methods that generalize to unseen data represent an interesting venue, where zero-shot segmentation presents an option to account for data limitation.","Initial exploratory works with the Segment Anything Model (SAM) show that bounding-box-based prompting presents notable zero-short generalization.","However, point-based prompting leads to a degraded performance that further deteriorates under image corruption.","We argue that SAM drastically over-segment images with high corruption levels, resulting in degraded performance when only a single segmentation mask is considered, while the combination of the masks overlapping the object of interest generates an accurate prediction.","Method: We use SAM to generate the over-segmented prediction of endoscopic frames.","Then, we employ the ground-truth tool mask to analyze the results of SAM when the best single mask is selected as prediction and when all the individual masks overlapping the object of interest are combined to obtain the final predicted mask.","We analyze the Endovis18 and Endovis17 instrument segmentation datasets using synthetic corruptions of various strengths and an In-House dataset featuring counterfactually created real-world corruptions.","Results: Combining the over-segmented masks contributes to improvements in the IoU.","Furthermore, selecting the best single segmentation presents a competitive IoU score for clean images.","Conclusions: Combined SAM predictions present improved results and robustness up to a certain corruption level.","However, appropriate prompting strategies are fundamental for implementing these models in the medical domain."],"url":"http://arxiv.org/abs/2402.17972v1","category":"cs.CV"}
{"created":"2024-02-28 01:08:07","title":"The Design and Implementation of a High-Performance Log-Structured RAID System for ZNS SSDs","abstract":"Zoned Namespace (ZNS) defines a new abstraction for host software to flexibly manage storage in flash-based SSDs as append-only zones. It also provides a Zone Append primitive to further boost the write performance of ZNS SSDs by exploiting intra-zone parallelism. However, making Zone Append effective for reliable and scalable storage, in the form of a RAID array of multiple ZNS SSDs, is non-trivial since Zone Append offloads address management to ZNS SSDs and requires hosts to dedicatedly manage RAID stripes across multiple drives. We propose ZapRAID, a high-performance log-structured RAID system for ZNS SSDs by carefully exploiting Zone Append to achieve high write parallelism and lightweight stripe management. ZapRAID adopts a group-based data layout with a coarse-grained ordering across multiple groups of stripes, such that it can use small-size metadata for stripe management on a per-group basis under Zone Append. It further adopts hybrid data management to simultaneously achieve intra-zone and inter-zone parallelism through a careful combination of both Zone Append and Zone Write primitives. We evaluate ZapRAID using microbenchmarks, trace-driven experiments, and real-application experiments. Our evaluation results show that ZapRAID achieves high write throughput and maintains high performance in normal reads, degraded reads, crash recovery, and full-drive recovery.","sentences":["Zoned Namespace (ZNS) defines a new abstraction for host software to flexibly manage storage in flash-based SSDs as append-only zones.","It also provides a Zone Append primitive to further boost the write performance of ZNS SSDs by exploiting intra-zone parallelism.","However, making Zone Append effective for reliable and scalable storage, in the form of a RAID array of multiple ZNS SSDs, is non-trivial since Zone Append offloads address management to ZNS SSDs and requires hosts to dedicatedly manage RAID stripes across multiple drives.","We propose ZapRAID, a high-performance log-structured RAID system for ZNS SSDs by carefully exploiting Zone Append to achieve high write parallelism and lightweight stripe management.","ZapRAID adopts a group-based data layout with a coarse-grained ordering across multiple groups of stripes, such that it can use small-size metadata for stripe management on a per-group basis under Zone Append.","It further adopts hybrid data management to simultaneously achieve intra-zone and inter-zone parallelism through a careful combination of both Zone Append and Zone Write primitives.","We evaluate ZapRAID using microbenchmarks, trace-driven experiments, and real-application experiments.","Our evaluation results show that ZapRAID achieves high write throughput and maintains high performance in normal reads, degraded reads, crash recovery, and full-drive recovery."],"url":"http://arxiv.org/abs/2402.17963v1","category":"cs.DC"}
{"created":"2024-02-28 00:46:49","title":"Missing Titanium in the Asymmetric Supernova Remnant W49B","abstract":"The progenitor of the W49B supernova remnant is still under debate. One of the candidates is a jet-driven core-collapse supernova. In such a highly asymmetric explosion, a strong $\\alpha$-rich freezeout is expected in local high entropy regions, which should enrich elements synthesized by the capture of $\\alpha$-particles such as $^{44}$Ti and $^{48}$Cr (decaying to $^{44}$Ca and $^{48}$Ti, respectively). In the present work, in order to infer the progenitor of the W49B remnant, we constrain the amount of stable Ti ($^{48}$Ti) synthesized, using the {\\it Suzaku} observation. We found no firm evidence for the Ti line and set the upper limit of $M_{\\rm Ti}/M_{\\rm Fe} < 8.2 \\times$ 10$^{-4}$ (99\\% limit using Xspec) and $M_{\\rm Ti}/M_{\\rm Fe} < 1.9 \\times$ 10$^{-3}$ (99\\% limit using SPEX), and thus excluded almost all hypernova/jet-driven supernova models. Our results, as complemented by some previous studies, suggest that a Type Ia supernova from a near-$M_{\\rm Ch}$ (Chandrasekhar mass) white dwarf is the most favorable candidate for the origin of W49B. Future observations with X-ray calorimeter missions, such as XRISM, will give us a stronger constraint on the progenitor.","sentences":["The progenitor of the W49B supernova remnant is still under debate.","One of the candidates is a jet-driven core-collapse supernova.","In such a highly asymmetric explosion, a strong $\\alpha$-rich freezeout is expected in local high entropy regions, which should enrich elements synthesized by the capture of $\\alpha$-particles such as $^{44}$Ti and $^{48}$Cr (decaying to $^{44}$Ca and $^{48}$Ti, respectively).","In the present work, in order to infer the progenitor of the W49B remnant, we constrain the amount of stable Ti ($^{48}$Ti) synthesized, using the {\\it Suzaku} observation.","We found no firm evidence for the Ti line and set the upper limit of $M_{\\rm Ti}/M_{\\rm Fe} < 8.2 \\times$ 10$^{-4}$ (99\\% limit using Xspec) and $M_{\\rm Ti}/M_{\\rm Fe} < 1.9 \\times$ 10$^{-3}$ (99\\% limit using SPEX), and thus excluded almost all hypernova/jet-driven supernova models.","Our results, as complemented by some previous studies, suggest that a Type Ia supernova from a near-$M_{\\rm Ch}$ (Chandrasekhar mass) white dwarf is the most favorable candidate for the origin of W49B. Future observations with X-ray calorimeter missions, such as XRISM, will give us a stronger constraint on the progenitor."],"url":"http://arxiv.org/abs/2402.17957v1","category":"astro-ph.HE"}
{"created":"2024-02-28 00:29:39","title":"Relating Real and P-adic Kazhdan-Lusztig Polynomials","abstract":"Fix an integral semisimple element $\\lambda$ in the Lie algebra $\\mathfrak{g}$ of a complex reductive algebraic group $G$. Let $L$ denote the centralizer of $\\lambda$ in $G$ and let $\\mathfrak{g}(-1)$ denote the $-1$ eigenspace of $\\mathrm{ad}(\\lambda)$ in $\\mathfrak{g}$. Under a natural hypothesis (which is always satisfied for classical subgroups of $\\mathrm{GL}(n)$), we embed the closure of each $L$ orbit on $\\mathfrak{g}(-1)$ into the closure of an orbit of a symmetric subgroup $K$ containing $L$ on a partial flag variety for $G$. We use this to relate the local intersection homology of the later orbit closures to the former orbit closures. This, in turn, relates multiplicity matrices for split real and $p$-adic groups. We also describe relationships between \"microlocal packets'' of representations of these groups.","sentences":["Fix an integral semisimple element $\\lambda$ in the Lie algebra $\\mathfrak{g}$ of a complex reductive algebraic group $G$. Let $L$ denote the centralizer of $\\lambda$ in $G$ and let $\\mathfrak{g}(-1)$ denote the $-1$ eigenspace of $\\mathrm{ad}(\\lambda)$ in $\\mathfrak{g}$. Under a natural hypothesis (which is always satisfied for classical subgroups of $\\mathrm{GL}(n)$), we embed the closure of each $L$ orbit on $\\mathfrak{g}(-1)$ into the closure of an orbit of a symmetric subgroup $K$ containing $L$ on a partial flag variety for $G$. We use this to relate the local intersection homology of the later orbit closures to the former orbit closures.","This, in turn, relates multiplicity matrices for split real and $p$-adic groups.","We also describe relationships between \"microlocal packets'' of representations of these groups."],"url":"http://arxiv.org/abs/2402.17956v1","category":"math.RT"}
{"created":"2024-02-28 00:24:29","title":"Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps","abstract":"Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group disparities remain unsolved despite great progress on multi-tasking and multilinguality. We provide first valuable insights for evaluating gender gaps in multilingual ASR systems. We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap.","sentences":["Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes.","However, broad language coverage can still mask performance gaps within languages, for example, across genders.","We systematically evaluate multilingual ASR systems on gendered performance gaps.","Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities.","However, the advantaged group varies between languages.","While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap.","I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers.","Our results show that group disparities remain unsolved despite great progress on multi-tasking and multilinguality.","We provide first valuable insights for evaluating gender gaps in multilingual ASR systems.","We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap."],"url":"http://arxiv.org/abs/2402.17954v1","category":"cs.CL"}
{"created":"2024-02-28 00:13:17","title":"The Dirichlet problem for second-order elliptic equations in non-divergence form with continuous coefficients","abstract":"This paper investigates the Dirichlet problem for a non-divergence form elliptic operator $L$ in a bounded domain of $\\mathbb{R}^d$. Under certain conditions on the coefficients of $L$, we first establish the existence of a unique Green's function in a ball and derive two-sided pointwise estimates for it. Utilizing these results, we demonstrate the equivalence of regular points for $L$ and those for the Laplace operator, characterized via the Wiener test. This equivalence facilitates the unique solvability of the Dirichlet problem with continuous boundary data in regular domains. Furthermore, we construct the Green's function for $L$ in regular domains and establish pointwise bounds for it. This advancement is significant, as it extends the scope of existing estimates to domains beyond $C^{1,1}$, contributing to our understanding of elliptic operators in non-divergence form.","sentences":["This paper investigates the Dirichlet problem for a non-divergence form elliptic operator $L$ in a bounded domain of $\\mathbb{R}^d$. Under certain conditions on the coefficients of $L$, we first establish the existence of a unique Green's function in a ball and derive two-sided pointwise estimates for it.","Utilizing these results, we demonstrate the equivalence of regular points for $L$ and those for the Laplace operator, characterized via the Wiener test.","This equivalence facilitates the unique solvability of the Dirichlet problem with continuous boundary data in regular domains.","Furthermore, we construct the Green's function for $L$ in regular domains and establish pointwise bounds for it.","This advancement is significant, as it extends the scope of existing estimates to domains beyond $C^{1,1}$, contributing to our understanding of elliptic operators in non-divergence form."],"url":"http://arxiv.org/abs/2402.17948v1","category":"math.AP"}
{"created":"2024-02-28 00:07:13","title":"Elastocaloric evidence for a multicomponent superconductor stabilized within the nematic state in Ba(Fe$_{1-x}$Co$_x$)$_2$As$_2$","abstract":"The iron-based high-$T_c$ superconductors exhibit rich phase diagrams with intertwined phases, including magnetism, nematicity and superconductivity. The superconducting $T_c$ in many of these materials is maximized in the regime of strong nematic fluctuations, making the role of nematicity in influencing the superconductivity a topic of intense research. Here, we use the AC elastocaloric effect (ECE) to map out the phase diagram of Ba(Fe$_{1-x}$Co$_x$)$_2$As$_2$ near optimal doping. The ECE signature at $T_c$ on the overdoped side, where superconductivity condenses without any nematic order, is quantitatively consistent with other thermodynamic probes that indicate a single-component superconducting state. In contrast, on the slightly underdoped side, where superconductivity condenses within the nematic phase, ECE reveals a second thermodynamic transition proximate to and below $T_c$. We rule out magnetism and re-entrant tetragonality as the origin of this transition, and find that our observations strongly suggest a phase transition into a multicomponent superconducting state. This implies the existence of a sub-dominant pairing instability that competes strongly with the dominant $s^\\pm$ instability. Our results thus motivate a re-examination of the pairing state and its interplay with nematicity in this extensively studied iron-based superconductor, while also demonstrating the power of ECE in uncovering strain-tuned phase diagrams of quantum materials.","sentences":["The iron-based high-$T_c$ superconductors exhibit rich phase diagrams with intertwined phases, including magnetism, nematicity and superconductivity.","The superconducting $T_c$ in many of these materials is maximized in the regime of strong nematic fluctuations, making the role of nematicity in influencing the superconductivity a topic of intense research.","Here, we use the AC elastocaloric effect (ECE) to map out the phase diagram of Ba(Fe$_{1-x}$Co$_x$)$_2$As$_2$ near optimal doping.","The ECE signature at $T_c$ on the overdoped side, where superconductivity condenses without any nematic order, is quantitatively consistent with other thermodynamic probes that indicate a single-component superconducting state.","In contrast, on the slightly underdoped side, where superconductivity condenses within the nematic phase, ECE reveals a second thermodynamic transition proximate to and below $T_c$. We rule out magnetism and re-entrant tetragonality as the origin of this transition, and find that our observations strongly suggest a phase transition into a multicomponent superconducting state.","This implies the existence of a sub-dominant pairing instability that competes strongly with the dominant $s^\\pm$ instability.","Our results thus motivate a re-examination of the pairing state and its interplay with nematicity in this extensively studied iron-based superconductor, while also demonstrating the power of ECE in uncovering strain-tuned phase diagrams of quantum materials."],"url":"http://arxiv.org/abs/2402.17945v1","category":"cond-mat.supr-con"}
{"created":"2024-02-27 23:43:31","title":"Neural Networks for Portfolio-Level Risk Management: Portfolio Compression, Static Hedging, Counterparty Credit Risk Exposures and Impact on Capital Requirement","abstract":"In this paper, we present an artificial neural network framework for portfolio compression of a large portfolio of European options with varying maturities (target portfolio) by a significantly smaller portfolio of European options with shorter or same maturity (compressed portfolio), which also represents a self-replicating static hedge portfolio of the target portfolio. For the proposed machine learning architecture, which is consummately interpretable by choice of design, we also define the algorithm to learn model parameters by providing a parameter initialisation technique and leveraging the optimisation methodology proposed in Lokeshwar and Jain (2024), which was initially introduced to price Bermudan options. We demonstrate the convergence of errors and the iterative evolution of neural network parameters over the course of optimization process, using selected target portfolio samples for illustration. We demonstrate through numerical examples that the Exposure distributions and Exposure profiles (Expected Exposure and Potential Future Exposure) of the target portfolio and compressed portfolio align closely across future risk horizons under risk-neutral and real-world scenarios. Additionally, we benchmark the target portfolio's Financial Greeks (Delta, Gamma, and Vega) against the compressed portfolio at future time horizons across different market scenarios generated by Monte-Carlo simulations. Finally, we compare the regulatory capital requirement under the standardised approach for counterparty credit risk of the target portfolio against the compressed portfolio and highlight that the capital requirement for the compact portfolio substantially reduces.","sentences":["In this paper, we present an artificial neural network framework for portfolio compression of a large portfolio of European options with varying maturities (target portfolio) by a significantly smaller portfolio of European options with shorter or same maturity (compressed portfolio), which also represents a self-replicating static hedge portfolio of the target portfolio.","For the proposed machine learning architecture, which is consummately interpretable by choice of design, we also define the algorithm to learn model parameters by providing a parameter initialisation technique and leveraging the optimisation methodology proposed in Lokeshwar and Jain (2024), which was initially introduced to price Bermudan options.","We demonstrate the convergence of errors and the iterative evolution of neural network parameters over the course of optimization process, using selected target portfolio samples for illustration.","We demonstrate through numerical examples that the Exposure distributions and Exposure profiles (Expected Exposure and Potential Future Exposure) of the target portfolio and compressed portfolio align closely across future risk horizons under risk-neutral and real-world scenarios.","Additionally, we benchmark the target portfolio's Financial Greeks (Delta, Gamma, and Vega) against the compressed portfolio at future time horizons across different market scenarios generated by Monte-Carlo simulations.","Finally, we compare the regulatory capital requirement under the standardised approach for counterparty credit risk of the target portfolio against the compressed portfolio and highlight that the capital requirement for the compact portfolio substantially reduces."],"url":"http://arxiv.org/abs/2402.17941v1","category":"q-fin.PM"}
{"created":"2024-02-27 23:38:49","title":"Weakly Private Information Retrieval from Heterogeneously Trusted Servers","abstract":"We study the problem of weakly private information retrieval (PIR) when there is heterogeneity in servers' trustfulness under the maximal leakage (Max-L) metric and mutual information (MI) metric. A user wishes to retrieve a desired message from N non-colluding servers efficiently, such that the identity of the desired message is not leaked in a significant manner; however, some servers can be more trustworthy than others. We propose a code construction for this setting and optimize the probability distribution for this construction. For the Max-L metric, it is shown that the optimal probability allocation for the proposed scheme essentially separates the delivery patterns into two parts: a completely private part that has the same download overhead as the capacity-achieving PIR code, and a non-private part that allows complete privacy leakage but has no download overhead by downloading only from the most trustful server. The optimal solution is established through a sophisticated analysis of the underlying convex optimization problem, and a reduction between the homogeneous setting and the heterogeneous setting. For the MI metric, the homogeneous case is studied first for which the code can be optimized with an explicit probability assignment, while a closed-form solution becomes intractable for the heterogeneous case. Numerical results are provided for both cases to corroborate the theoretical analysis.","sentences":["We study the problem of weakly private information retrieval (PIR) when there is heterogeneity in servers' trustfulness under the maximal leakage (Max-L) metric and mutual information (MI) metric.","A user wishes to retrieve a desired message from N non-colluding servers efficiently, such that the identity of the desired message is not leaked in a significant manner; however, some servers can be more trustworthy than others.","We propose a code construction for this setting and optimize the probability distribution for this construction.","For the Max-L metric, it is shown that the optimal probability allocation for the proposed scheme essentially separates the delivery patterns into two parts: a completely private part that has the same download overhead as the capacity-achieving PIR code, and a non-private part that allows complete privacy leakage but has no download overhead by downloading only from the most trustful server.","The optimal solution is established through a sophisticated analysis of the underlying convex optimization problem, and a reduction between the homogeneous setting and the heterogeneous setting.","For the MI metric, the homogeneous case is studied first for which the code can be optimized with an explicit probability assignment, while a closed-form solution becomes intractable for the heterogeneous case.","Numerical results are provided for both cases to corroborate the theoretical analysis."],"url":"http://arxiv.org/abs/2402.17940v1","category":"cs.IT"}
{"created":"2024-02-27 23:09:55","title":"A Heterogeneous Agent Model of Mortgage Servicing: An Income-based Relief Analysis","abstract":"Mortgages account for the largest portion of household debt in the United States, totaling around \\$12 trillion nationwide. In times of financial hardship, alleviating mortgage burdens is essential for supporting affected households. The mortgage servicing industry plays a vital role in offering this assistance, yet there has been limited research modelling the complex relationship between households and servicers. To bridge this gap, we developed an agent-based model that explores household behavior and the effectiveness of relief measures during financial distress.   Our model represents households as adaptive learning agents with realistic financial attributes. These households experience exogenous income shocks, which may influence their ability to make mortgage payments. Mortgage servicers provide relief options to these households, who then choose the most suitable relief based on their unique financial circumstances and individual preferences. We analyze the impact of various external shocks and the success of different mortgage relief strategies on specific borrower subgroups.   Through this analysis, we show that our model can not only replicate real-world mortgage studies but also act as a tool for conducting a broad range of what-if scenario analyses. Our approach offers fine-grained insights that can inform the development of more effective and inclusive mortgage relief solutions.","sentences":["Mortgages account for the largest portion of household debt in the United States, totaling around \\$12 trillion nationwide.","In times of financial hardship, alleviating mortgage burdens is essential for supporting affected households.","The mortgage servicing industry plays a vital role in offering this assistance, yet there has been limited research modelling the complex relationship between households and servicers.","To bridge this gap, we developed an agent-based model that explores household behavior and the effectiveness of relief measures during financial distress.   ","Our model represents households as adaptive learning agents with realistic financial attributes.","These households experience exogenous income shocks, which may influence their ability to make mortgage payments.","Mortgage servicers provide relief options to these households, who then choose the most suitable relief based on their unique financial circumstances and individual preferences.","We analyze the impact of various external shocks and the success of different mortgage relief strategies on specific borrower subgroups.   ","Through this analysis, we show that our model can not only replicate real-world mortgage studies but also act as a tool for conducting a broad range of what-if scenario analyses.","Our approach offers fine-grained insights that can inform the development of more effective and inclusive mortgage relief solutions."],"url":"http://arxiv.org/abs/2402.17932v1","category":"cs.MA"}
{"created":"2024-02-27 22:59:48","title":"Electron-Induced Radiolysis Chemistry in Fluid Cell Electron Microscopy: Application on the Reactivity of Aluminum Nanocubes","abstract":"Leveraging the high energy electrons as probe for the atomic landscape of materials, the importance of understanding radiolysis chemistry in the field of transmission electron microscopy (TEM) is exceptionally important. Reactive chemical species generated from electron-material interactions diffuse throughout the sample and may induce \"artifact\" reactions with the specimen of interest. However, in other aspects, this radiolysis can be utilized to intentionally trigger chemical reactions related to the nucleation and growth of colloidal nanoparticles, as well as chemical etching under a priori conditions in fluid condition. Therefore, systematic research and new theories are required to help researchers gain a deeper understanding of these electron-materials interactions either to mitigate damage or rationally use radical chemistry especially in the environmental TEM (E-TEM). Radiolysis in open-cell gas phase TEM (GPTEM) under reduced pressures is usually disregarded considering the low density of molecules and high diffusivity. However, for emerging closed-cell GPTEM, which usually involves the usage of higher pressures, less is known. Here, we utilize an ultrathin (UT) silicon nitride gas cell to investigate and quantify the effects of electron dosage on the oxidation behavior of {100}-faceted aluminum (Al) nanocubes under various conditions. In addition, we develop a generalized computational code to elucidate radiolysis chemistry in varying media with consideration of the diffusion dynamics, while rationalizing the experimental observations. Based on these theoretical electron-fluid interaction models, we propose guidelines to control the radical species of the closed-cell in situ nanoreactors, paving the way to the nanoscale control of chemical reactions in E-TEM.","sentences":["Leveraging the high energy electrons as probe for the atomic landscape of materials, the importance of understanding radiolysis chemistry in the field of transmission electron microscopy (TEM) is exceptionally important.","Reactive chemical species generated from electron-material interactions diffuse throughout the sample and may induce \"artifact\" reactions with the specimen of interest.","However, in other aspects, this radiolysis can be utilized to intentionally trigger chemical reactions related to the nucleation and growth of colloidal nanoparticles, as well as chemical etching under a priori conditions in fluid condition.","Therefore, systematic research and new theories are required to help researchers gain a deeper understanding of these electron-materials interactions either to mitigate damage or rationally use radical chemistry especially in the environmental TEM (E-TEM).","Radiolysis in open-cell gas phase TEM (GPTEM) under reduced pressures is usually disregarded considering the low density of molecules and high diffusivity.","However, for emerging closed-cell GPTEM, which usually involves the usage of higher pressures, less is known.","Here, we utilize an ultrathin (UT) silicon nitride gas cell to investigate and quantify the effects of electron dosage on the oxidation behavior of {100}-faceted aluminum (Al) nanocubes under various conditions.","In addition, we develop a generalized computational code to elucidate radiolysis chemistry in varying media with consideration of the diffusion dynamics, while rationalizing the experimental observations.","Based on these theoretical electron-fluid interaction models, we propose guidelines to control the radical species of the closed-cell in situ nanoreactors, paving the way to the nanoscale control of chemical reactions in E-TEM."],"url":"http://arxiv.org/abs/2402.17928v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 22:52:09","title":"MCSat-based Finite Field Reasoning in the Yices2 SMT Solver","abstract":"This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields. Our reasoning approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields. As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our reasoning approach as a new plugin for finite fields and extended Yices2's frontend to parse finite field problems, making our implementation the first MCSat-based reasoning engine for finite fields. We present its evaluation on finite field benchmarks, comparing it against cvc5. Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further reasoning techniques for this theory.","sentences":["This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields.","Our reasoning approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields.","As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our reasoning approach as a new plugin for finite fields and extended Yices2's frontend to parse finite field problems, making our implementation the first MCSat-based reasoning engine for finite fields.","We present its evaluation on finite field benchmarks, comparing it against cvc5.","Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further reasoning techniques for this theory."],"url":"http://arxiv.org/abs/2402.17927v1","category":"cs.LO"}
{"created":"2024-02-27 22:27:53","title":"Model Structures on Infinity-Categories of Filtrations","abstract":"In 1974, Gugenheim and May showed that the cohomology $\\text{Ext}_A(R,R)$ of a connected augmented algebra over a field $R$ is generated by elements with $s = 1$ under matric Massey products. In particular, this applies to the $E_2$ page of the $H\\mathbb{F}_p$-based Adams spectral sequence. By studying a novel sequence of deformations of a presentably symmetric monoidal stable $\\infty$-category $C$, we show that for a variety of spectral sequences coming from filtered spectra, the set of elements on the $E_2$ page surviving to the $E_k$ page is generated under matric Massey products by elements with degree $s < k.$ This work is the author's PhD thesis, completed under the supervision of Peter May.","sentences":["In 1974, Gugenheim and May showed that the cohomology $\\text{Ext}_A(R,R)$ of a connected augmented algebra over a field $R$ is generated by elements with $s = 1$ under matric Massey products.","In particular, this applies to the $E_2$ page of the $H\\mathbb{F}_p$-based Adams spectral sequence.","By studying a novel sequence of deformations of a presentably symmetric monoidal stable $\\infty$-category $C$, we show that for a variety of spectral sequences coming from filtered spectra, the set of elements on the $E_2$ page surviving to the $E_k$ page is generated under matric Massey products by elements with degree $s < k.$","This work is the author's PhD thesis, completed under the supervision of Peter May."],"url":"http://arxiv.org/abs/2402.17921v1","category":"math.AT"}
{"created":"2024-02-27 22:07:12","title":"Generation and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm","abstract":"Safe and reliable disclosure of information from confidential data is a challenging statistical problem. A common approach considers the generation of synthetic data, to be disclosed instead of the original data. Efficient approaches ought to deal with the trade-off between reliability and confidentiality of the released data. Ultimately, the aim is to be able to reproduce as accurately as possible statistical analysis of the original data using the synthetic one. Bayesian networks is a model-based approach that can be used to parsimoniously estimate the underlying distribution of the original data and generate synthetic datasets. These ought to not only approximate the results of analyses with the original data but also robustly quantify the uncertainty involved in the approximation. This paper proposes a fully Bayesian approach to generate and analyze synthetic data based on the posterior predictive distribution of statistics of the synthetic data, allowing for efficient uncertainty quantification. The methodology makes use of probability properties of the model to devise a computationally efficient algorithm to obtain the target predictive distributions via Monte Carlo. Model parsimony is handled by proposing a general class of penalizing priors for Bayesian network models. Finally, the efficiency and applicability of the proposed methodology is empirically investigated through simulated and real examples.","sentences":["Safe and reliable disclosure of information from confidential data is a challenging statistical problem.","A common approach considers the generation of synthetic data, to be disclosed instead of the original data.","Efficient approaches ought to deal with the trade-off between reliability and confidentiality of the released data.","Ultimately, the aim is to be able to reproduce as accurately as possible statistical analysis of the original data using the synthetic one.","Bayesian networks is a model-based approach that can be used to parsimoniously estimate the underlying distribution of the original data and generate synthetic datasets.","These ought to not only approximate the results of analyses with the original data but also robustly quantify the uncertainty involved in the approximation.","This paper proposes a fully Bayesian approach to generate and analyze synthetic data based on the posterior predictive distribution of statistics of the synthetic data, allowing for efficient uncertainty quantification.","The methodology makes use of probability properties of the model to devise a computationally efficient algorithm to obtain the target predictive distributions via Monte Carlo.","Model parsimony is handled by proposing a general class of penalizing priors for Bayesian network models.","Finally, the efficiency and applicability of the proposed methodology is empirically investigated through simulated and real examples."],"url":"http://arxiv.org/abs/2402.17915v1","category":"stat.ME"}
{"created":"2024-02-27 21:43:14","title":"Using Graph Neural Networks to Predict Local Culture","abstract":"Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.","sentences":["Urban research has long recognized that neighbourhoods are dynamic and relational.","However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics.","To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models.","By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture.","Results are promising from a substantive and methodologically point of view.","Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases.","Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data.","Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking."],"url":"http://arxiv.org/abs/2402.17905v1","category":"cs.LG"}
{"created":"2024-02-27 21:42:58","title":"4CNet: A Confidence-Aware, Contrastive, Conditional, Consistency Model for Robot Map Prediction in Multi-Robot Environments","abstract":"Mobile robots in unknown cluttered environments with irregularly shaped obstacles often face sensing, energy, and communication challenges which directly affect their ability to explore these environments. In this paper, we introduce a novel deep learning method, Confidence-Aware Contrastive Conditional Consistency Model (4CNet), for mobile robot map prediction during resource-limited exploration in multi-robot environments. 4CNet uniquely incorporates: 1) a conditional consistency model for map prediction in irregularly shaped unknown regions, 2) a contrastive map-trajectory pretraining framework for a trajectory encoder that extracts spatial information from the trajectories of nearby robots during map prediction, and 3) a confidence network to measure the uncertainty of map prediction for effective exploration under resource constraints. We incorporate 4CNet within our proposed robot exploration with map prediction architecture, 4CNet-E. We then conduct extensive comparison studies with 4CNet-E and state-of-the-art heuristic and learning methods to investigate both map prediction and exploration performance in environments consisting of uneven terrain and irregularly shaped obstacles. Results showed that 4CNet-E obtained statistically significant higher prediction accuracy and area coverage with varying environment sizes, number of robots, energy budgets, and communication limitations. Real-world mobile robot experiments were performed and validated the feasibility and generalizability of 4CNet-E for mobile robot map prediction and exploration.","sentences":["Mobile robots in unknown cluttered environments with irregularly shaped obstacles often face sensing, energy, and communication challenges which directly affect their ability to explore these environments.","In this paper, we introduce a novel deep learning method, Confidence-Aware Contrastive Conditional Consistency Model (4CNet), for mobile robot map prediction during resource-limited exploration in multi-robot environments.","4CNet uniquely incorporates: 1) a conditional consistency model for map prediction in irregularly shaped unknown regions, 2) a contrastive map-trajectory pretraining framework for a trajectory encoder that extracts spatial information from the trajectories of nearby robots during map prediction, and 3) a confidence network to measure the uncertainty of map prediction for effective exploration under resource constraints.","We incorporate 4CNet within our proposed robot exploration with map prediction architecture, 4CNet-E. We then conduct extensive comparison studies with 4CNet-E and state-of-the-art heuristic and learning methods to investigate both map prediction and exploration performance in environments consisting of uneven terrain and irregularly shaped obstacles.","Results showed that 4CNet-E obtained statistically significant higher prediction accuracy and area coverage with varying environment sizes, number of robots, energy budgets, and communication limitations.","Real-world mobile robot experiments were performed and validated the feasibility and generalizability of 4CNet-E for mobile robot map prediction and exploration."],"url":"http://arxiv.org/abs/2402.17904v1","category":"cs.RO"}
{"created":"2024-02-27 21:29:10","title":"Universal regularity estimates for solutions to fully nonlinear elliptic equations with oblique boundary data","abstract":"In this work, we establish universal moduli of continuity for viscosity solutions to fully nonlinear elliptic equations with oblique boundary conditions, whose general model is given by $$ \\left\\{ \\begin{array}{rcl} F(D^2u,x) &=& f(x) \\quad \\mbox{in} \\,\\, \\Omega\\\\ \\beta(x) \\cdot Du(x) + \\gamma(x) \\, u(x)&=& g(x) \\quad \\mbox{on} \\,\\, \\partial \\Omega. \\end{array} \\right. $$ Such regularity estimates are achieved by exploring the integrability properties of $f$ based on different scenarios, making a $\\text{VMO}$ assumption on the coefficients of $F$, and by considering suitable smoothness properties on the boundary data $\\beta, \\gamma$ and $g$. Particularly, we derive sharp estimates for borderline cases where $f \\in L^n(\\Omega)$ and $f\\in p-\\textrm{BMO}(\\Omega)$. Additionally, for source terms in $L^p(\\Omega)$, for $p \\in (n, \\infty)$, we obtain sharp gradient estimates. Finally, we also address Schauder-type estimates for convex/concave operators and suitable H\\\"{o}lder data.","sentences":["In this work, we establish universal moduli of continuity for viscosity solutions to fully nonlinear elliptic equations with oblique boundary conditions, whose general model is given by $$ \\left\\{ \\begin{array}{rcl} F(D^2u,x) &=& f(x)","\\quad \\mbox{in} \\,\\, \\Omega\\\\ \\beta(x) \\cdot Du(x) + \\gamma(x) \\, u(x)&=& g(x)","\\quad \\mbox{on} \\,\\, \\partial \\Omega.","\\end{array} \\right.","$$ Such regularity estimates are achieved by exploring the integrability properties of $f$ based on different scenarios, making a $\\text{VMO}$ assumption on the coefficients of $F$, and by considering suitable smoothness properties on the boundary data $\\beta, \\gamma$ and $g$. Particularly, we derive sharp estimates for borderline cases where $f \\in L^n(\\Omega)$ and $f\\in p-\\textrm{BMO}(\\Omega)$. Additionally, for source terms in $L^p(\\Omega)$, for $p \\in (n, \\infty)$, we obtain sharp gradient estimates.","Finally, we also address Schauder-type estimates for convex/concave operators and suitable H\\\"{o}lder data."],"url":"http://arxiv.org/abs/2402.17899v1","category":"math.AP"}
{"created":"2024-02-27 21:18:04","title":"Exact Controllability and Stabilization of the Wave Equation","abstract":"These Notes originated from a course I delivered at the Institute of Mathematics of the Universidade Federal do Rio de Janeiro, Brazil (UFRJ) in July-September 1989, were initially published in 1989 in Spanish under the title \"Controlabilidad Exacta y Estabilizaci\\'on de la Ecuaci\\'on de Ondas\" in the Lecture Notes Series of the Institute.   Despite the significant evolution of the topic over the last three decades, I believe that the text, with its synthetic presentation of fundamental tools in the field, remains valuable for researchers in the area, especially for younger generations. It is written from the perspective of the young mathematician I was when I authored the Notes, needing to learn many things in the process and, therefore, taking care to develop details often left to the reader or not readily available elsewhere.   These Notes were written one year after completing my PhD at the Universit\\'e Pierre et Marie Curie in Paris and drafting the lectures of Professor Jacques-Louis Lions at Coll\\`ege de France in the academic year 1986-1987, later published as a book in 1988. Parts of these Notes offer a concise presentation of content developed in more detail in that book, supplemented by work on the decay of dissipative wave equations during my PhD under the supervision of Professor Alain Haraux in Paris.","sentences":["These Notes originated from a course I delivered at the Institute of Mathematics of the Universidade Federal do Rio de Janeiro, Brazil (UFRJ) in July-September 1989, were initially published in 1989 in Spanish under the title \"Controlabilidad Exacta y Estabilizaci\\'on de la Ecuaci\\'on de Ondas\" in the Lecture Notes Series of the Institute.   ","Despite the significant evolution of the topic over the last three decades, I believe that the text, with its synthetic presentation of fundamental tools in the field, remains valuable for researchers in the area, especially for younger generations.","It is written from the perspective of the young mathematician I was when I authored the Notes, needing to learn many things in the process and, therefore, taking care to develop details often left to the reader or not readily available elsewhere.   ","These Notes were written one year after completing my PhD at the Universit\\'e Pierre et Marie Curie in Paris and drafting the lectures of Professor Jacques-Louis Lions at Coll\\`ege de France in the academic year 1986-1987, later published as a book in 1988.","Parts of these Notes offer a concise presentation of content developed in more detail in that book, supplemented by work on the decay of dissipative wave equations during my PhD under the supervision of Professor Alain Haraux in Paris."],"url":"http://arxiv.org/abs/2402.17894v1","category":"math.OC"}
{"created":"2024-02-27 21:12:31","title":"SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking","abstract":"Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate improved tracking performance.","sentences":["Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation.","For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions.","Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time.","The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window.","A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency.","We evaluate our SWTrack implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate improved tracking performance."],"url":"http://arxiv.org/abs/2402.17892v1","category":"cs.RO"}
{"created":"2024-02-27 20:57:35","title":"Independent Learning in Constrained Markov Potential Games","abstract":"Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without a centralized coordination mechanism requiring turn-based agent updates. Under some technical constraint qualification conditions, we establish convergence guarantees towards constrained approximate Nash equilibria. We perform simulations to illustrate our results.","sentences":["Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints.","In this work, we focus on the recently introduced class of constrained Markov Potential Games.","While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question.","We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state.","Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set.","Each proximal step is solved inexactly using a stochastic switching gradient algorithm.","Notably, our algorithm can be implemented independently without a centralized coordination mechanism requiring turn-based agent updates.","Under some technical constraint qualification conditions, we establish convergence guarantees towards constrained approximate Nash equilibria.","We perform simulations to illustrate our results."],"url":"http://arxiv.org/abs/2402.17885v1","category":"cs.LG"}
{"created":"2024-02-27 20:33:22","title":"Automated Statistical Model Discovery with Language Models","abstract":"Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving classic models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method matches the performance of previous systems, identifies models on par with human expert designed models, and extends classic models in interpretable ways. Our results highlight the promise of LM driven model discovery.","sentences":["Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints.","Efficiently searching over this space requires human expertise in modeling and the problem domain.","Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery.","We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert.","By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems.","We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving classic models under natural language constraints (e.g., this model should be interpretable to an ecologist).","Our method matches the performance of previous systems, identifies models on par with human expert designed models, and extends classic models in interpretable ways.","Our results highlight the promise of LM driven model discovery."],"url":"http://arxiv.org/abs/2402.17879v1","category":"cs.LG"}
{"created":"2024-02-27 20:32:28","title":"End-User Development for Human-Robot Interaction","abstract":"End-user development (EUD) represents a key step towards making robotics accessible for experts and nonexperts alike. Within academia, researchers investigate novel ways that EUD tools can capture, represent, visualize, analyze, and test developer intent. At the same time, industry researchers increasingly build and ship programming tools that enable customers to interact with their robots. However, despite this growing interest, the role of EUD within HRI is not well defined. EUD struggles to situate itself within a growing array of alternative approaches to application development, such as robot learning and teleoperation. EUD further struggles due to the wide range of individuals who can be considered end users, such as independent third-party application developers, consumers, hobbyists, or even employees of the robot manufacturer. Key questions remain such as how EUD is justified over alternate approaches to application development, which contexts EUD is most suited for, who the target users of an EUD system are, and where interaction between a human and a robot takes place, amongst many other questions. We seek to address these challenges and questions by organizing the first End-User Development for Human-Robot Interaction (EUD4HRI) workshop at the 2024 International Conference of Human-Robot Interaction. The workshop will bring together researchers with a wide range of expertise across academia and industry, spanning perspectives from multiple subfields of robotics, with the primary goal being a consensus of perspectives about the role that EUD must play within human-robot interaction.","sentences":["End-user development (EUD) represents a key step towards making robotics accessible for experts and nonexperts alike.","Within academia, researchers investigate novel ways that EUD tools can capture, represent, visualize, analyze, and test developer intent.","At the same time, industry researchers increasingly build and ship programming tools that enable customers to interact with their robots.","However, despite this growing interest, the role of EUD within HRI is not well defined.","EUD struggles to situate itself within a growing array of alternative approaches to application development, such as robot learning and teleoperation.","EUD further struggles due to the wide range of individuals who can be considered end users, such as independent third-party application developers, consumers, hobbyists, or even employees of the robot manufacturer.","Key questions remain such as how EUD is justified over alternate approaches to application development, which contexts EUD is most suited for, who the target users of an EUD system are, and where interaction between a human and a robot takes place, amongst many other questions.","We seek to address these challenges and questions by organizing the first End-User Development for Human-Robot Interaction (EUD4HRI) workshop at the 2024 International Conference of Human-Robot Interaction.","The workshop will bring together researchers with a wide range of expertise across academia and industry, spanning perspectives from multiple subfields of robotics, with the primary goal being a consensus of perspectives about the role that EUD must play within human-robot interaction."],"url":"http://arxiv.org/abs/2402.17878v1","category":"cs.RO"}
{"created":"2024-02-27 20:31:44","title":"Accelerated Real-time Cine and Flow under In-magnet Staged Exercise","abstract":"Background: Cardiovascular magnetic resonance imaging (CMR) is a well-established imaging tool for diagnosing and managing cardiac conditions. The integration of exercise stress with CMR (ExCMR) can enhance its diagnostic capacity. Despite recent advances in CMR technology, ExCMR remains technically challenging due to motion artifacts and limited spatial and temporal resolution. Methods: This study investigates the feasibility of biventricular functional and hemodynamic assessment using real-time (RT) ExCMR during a staged exercise protocol in 26 healthy volunteers. We introduce a coil reweighting technique to minimize motion artifacts. In addition, we identify and analyze heartbeats from the end-expiratory phase to enhance the repeatability of cardiac function quantification. To demonstrate clinical feasibility, qualitative results from five patients are also presented. Results: Our findings indicate a consistent decrease in end-systolic volume (ESV) and stable end-diastolic volume (EDV) across exercise intensities, leading to increased stroke volume (SV) and ejection fraction (EF). Coil reweighting effectively reduces motion artifacts, improving image quality in both healthy volunteers and patients. The repeatability of cardiac function parameters, demonstrated by scan-rescan tests in nine volunteers, improves with the selection of end-expiratory beats. Conclusions: The study demonstrates that RT ExCMR with in-magnet exercise is a feasible and effective method for dynamic cardiac function monitoring during exercise. The proposed coil reweighting technique and selection of end-expiratory beats significantly enhance image quality and repeatability.","sentences":["Background: Cardiovascular magnetic resonance imaging (CMR) is a well-established imaging tool for diagnosing and managing cardiac conditions.","The integration of exercise stress with CMR (ExCMR) can enhance its diagnostic capacity.","Despite recent advances in CMR technology, ExCMR remains technically challenging due to motion artifacts and limited spatial and temporal resolution.","Methods: This study investigates the feasibility of biventricular functional and hemodynamic assessment using real-time (RT) ExCMR during a staged exercise protocol in 26 healthy volunteers.","We introduce a coil reweighting technique to minimize motion artifacts.","In addition, we identify and analyze heartbeats from the end-expiratory phase to enhance the repeatability of cardiac function quantification.","To demonstrate clinical feasibility, qualitative results from five patients are also presented.","Results:","Our findings indicate a consistent decrease in end-systolic volume (ESV) and stable end-diastolic volume (EDV) across exercise intensities, leading to increased stroke volume (SV) and ejection fraction (EF).","Coil reweighting effectively reduces motion artifacts, improving image quality in both healthy volunteers and patients.","The repeatability of cardiac function parameters, demonstrated by scan-rescan tests in nine volunteers, improves with the selection of end-expiratory beats.","Conclusions: The study demonstrates that RT ExCMR with in-magnet exercise is a feasible and effective method for dynamic cardiac function monitoring during exercise.","The proposed coil reweighting technique and selection of end-expiratory beats significantly enhance image quality and repeatability."],"url":"http://arxiv.org/abs/2402.17877v1","category":"eess.SP"}
{"created":"2024-02-27 20:06:13","title":"SmartQC: An Extensible DLT-Based Framework for Trusted Data Workflows in Smart Manufacturing","abstract":"Recent developments in Distributed Ledger Technology (DLT), including Blockchain offer new opportunities in the manufacturing domain, by providing mechanisms to automate trust services (digital identity, trusted interactions, and auditable transactions) and when combined with other advanced digital technologies (e.g. machine learning) can provide a secure backbone for trusted data flows between independent entities. This paper presents an DLT-based architectural pattern and technology solution known as SmartQC that aims to provide an extensible and flexible approach to integrating DLT technology into existing workflows and processes. SmartQC offers an opportunity to make processes more time efficient, reliable, and robust by providing two key features i) data integrity through immutable ledgers and ii) automation of business workflows leveraging smart contracts. The paper will present the system architecture, extensible data model and the application of SmartQC in the context of example smart manufacturing applications.","sentences":["Recent developments in Distributed Ledger Technology (DLT), including Blockchain offer new opportunities in the manufacturing domain, by providing mechanisms to automate trust services (digital identity, trusted interactions, and auditable transactions) and when combined with other advanced digital technologies (e.g. machine learning) can provide a secure backbone for trusted data flows between independent entities.","This paper presents an DLT-based architectural pattern and technology solution known as SmartQC that aims to provide an extensible and flexible approach to integrating DLT technology into existing workflows and processes.","SmartQC offers an opportunity to make processes more time efficient, reliable, and robust by providing two key features i) data integrity through immutable ledgers and ii) automation of business workflows leveraging smart contracts.","The paper will present the system architecture, extensible data model and the application of SmartQC in the context of example smart manufacturing applications."],"url":"http://arxiv.org/abs/2402.17868v1","category":"cs.DC"}
{"created":"2024-02-27 20:02:13","title":"Towards spatiotemporal integration of bus transit with data-driven approaches","abstract":"This study aims to propose an approach for spatiotemporal integration of bus transit, which enables users to change bus lines by paying a single fare. This could increase bus transit efficiency and, consequently, help to make this mode of transportation more attractive. Usually, this strategy is allowed for a few hours in a non-restricted area; thus, certain walking distance areas behave like \"virtual terminals.\" For that, two data-driven algorithms are proposed in this work. First, a new algorithm for detecting itineraries based on bus GPS data and the bus stop location. The proposed algorithm's results show that 90% of the database detected valid itineraries by excluding invalid markings and adding times at missing bus stops through temporal interpolation. Second, this study proposes a bus stop clustering algorithm to define suitable areas for these virtual terminals where it would be possible to make bus transfers outside the physical terminals. Using real-world origin-destination trips, the bus network, including clusters, can reduce traveled distances by up to 50%, making twice as many connections on average.","sentences":["This study aims to propose an approach for spatiotemporal integration of bus transit, which enables users to change bus lines by paying a single fare.","This could increase bus transit efficiency and, consequently, help to make this mode of transportation more attractive.","Usually, this strategy is allowed for a few hours in a non-restricted area; thus, certain walking distance areas behave like \"virtual terminals.\"","For that, two data-driven algorithms are proposed in this work.","First, a new algorithm for detecting itineraries based on bus GPS data and the bus stop location.","The proposed algorithm's results show that 90% of the database detected valid itineraries by excluding invalid markings and adding times at missing bus stops through temporal interpolation.","Second, this study proposes a bus stop clustering algorithm to define suitable areas for these virtual terminals where it would be possible to make bus transfers outside the physical terminals.","Using real-world origin-destination trips, the bus network, including clusters, can reduce traveled distances by up to 50%, making twice as many connections on average."],"url":"http://arxiv.org/abs/2402.17866v1","category":"cs.SI"}
{"created":"2024-02-27 20:01:20","title":"The deformed Tanisaki-Garsia-Procesi modules","abstract":"The polynomial ideals studied by A. Garsia and C. Procesi play an important role in the theory of Kostka polynomials. We give multiparameter flat deformations of these ideals and define an action of the extended affine symmetric group on the corresponding quotient algebras multiplied by the sign representation. We show that the images of these modules under the affine Schur-Weyl duality are dual to the local Weyl modules for the loop algebra $\\mathfrak{sl}_{n+1}[t^{\\pm 1}].$","sentences":["The polynomial ideals studied by A. Garsia and C. Procesi play an important role in the theory of Kostka polynomials.","We give multiparameter flat deformations of these ideals and define an action of the extended affine symmetric group on the corresponding quotient algebras multiplied by the sign representation.","We show that the images of these modules under the affine Schur-Weyl duality are dual to the local Weyl modules for the loop algebra $\\mathfrak{sl}_{n+1}[t^{\\pm 1}].$"],"url":"http://arxiv.org/abs/2402.17865v1","category":"math.RT"}
{"created":"2024-02-27 19:54:42","title":"Vision Transformers with Natural Language Semantics","abstract":"Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.","sentences":["Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP).","Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information.","We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies.","sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers.","Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance.","Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic.","Notably, the use of semantic tokens significantly enhances the model's interpretability.","Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities.","Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers."],"url":"http://arxiv.org/abs/2402.17863v1","category":"cs.CV"}
{"created":"2024-02-27 19:48:44","title":"Weak scale supersymmetry emergent from the string landscape","abstract":"Superstring flux compactifications can stabilize all moduli while leading to an enormous number of vacua solutions, each leading to different $4-d$ laws of physics. While the string landscape provides at present the only plausible explanation for the size of the cosmological constant, it may also predict the form of weak scale supersymmetry which is expected to emerge. Rather general arguments suggest a power-law draw to large soft terms, but these are subject to an anthropic selection of not-too-large a value for the weak scale. The combined selection allows one to compute relative probabilities for the emergence of supersymmetric models from the landscape. Models with weak scale naturalness appear most likely to emerge since they have the largest parameter space on the landscape. For finetuned models such as high scale SUSY or split SUSY, the required weak scale finetuning shrinks their parameter space to tiny volumes, making them much less likely to appear compared to natural models. Probability distributions for sparticle and Higgs masses from natural models show a preference for Higgs mass $m_h\\sim 125$ GeV with sparticles typically beyond present LHC limits, in accord with data. From these considerations, we briefly describe how natural SUSY is expected to be revealed at future LHC upgrades. This article is a contribution to the Special Edition of the journal {\\it Entropy} honoring Paul Frampton on his 80th birthday.","sentences":["Superstring flux compactifications can stabilize all moduli while leading to an enormous number of vacua solutions, each leading to different $4-d$ laws of physics.","While the string landscape provides at present the only plausible explanation for the size of the cosmological constant, it may also predict the form of weak scale supersymmetry which is expected to emerge.","Rather general arguments suggest a power-law draw to large soft terms, but these are subject to an anthropic selection of not-too-large a value for the weak scale.","The combined selection allows one to compute relative probabilities for the emergence of supersymmetric models from the landscape.","Models with weak scale naturalness appear most likely to emerge since they have the largest parameter space on the landscape.","For finetuned models such as high scale SUSY or split SUSY, the required weak scale finetuning shrinks their parameter space to tiny volumes, making them much less likely to appear compared to natural models.","Probability distributions for sparticle and Higgs masses from natural models show a preference for Higgs mass $m_h\\sim 125$ GeV with sparticles typically beyond present LHC limits, in accord with data.","From these considerations, we briefly describe how natural SUSY is expected to be revealed at future LHC upgrades.","This article is a contribution to the Special Edition of the journal {\\it Entropy} honoring Paul Frampton on his 80th birthday."],"url":"http://arxiv.org/abs/2402.17859v1","category":"hep-ph"}
{"created":"2024-02-27 19:24:51","title":"On the Parameterized Complexity of Motion Planning for Rectangular Robots","abstract":"We study computationally-hard fundamental motion planning problems where the goal is to translate $k$ axis-aligned rectangular robots from their initial positions to their final positions without collision, and with the minimum number of translation moves. Our aim is to understand the interplay between the number of robots and the geometric complexity of the input instance measured by the input size, which is the number of bits needed to encode the coordinates of the rectangles' vertices. We focus on axis-aligned translations, and more generally, translations restricted to a given set of directions, and we study the two settings where the robots move in the free plane, and where they are confined to a bounding box. We obtain fixed-parameter tractable (FPT) algorithms parameterized by $k$ for all the settings under consideration. In the case where the robots move serially (i.e., one in each time step) and axis-aligned, we prove a structural result stating that every problem instance admits an optimal solution in which the moves are along a grid, whose size is a function of $k$, that can be defined based on the input instance. This structural result implies that the problem is fixed-parameter tractable parameterized by $k$. We also consider the case in which the robots move in parallel (i.e., multiple robots can move during the same time step), and which falls under the category of Coordinated Motion Planning problems. Finally, we show that, when the robots move in the free plane, the FPT results for the serial motion case carry over to the case where the translations are restricted to any given set of directions.","sentences":["We study computationally-hard fundamental motion planning problems where the goal is to translate $k$ axis-aligned rectangular robots from their initial positions to their final positions without collision, and with the minimum number of translation moves.","Our aim is to understand the interplay between the number of robots and the geometric complexity of the input instance measured by the input size, which is the number of bits needed to encode the coordinates of the rectangles' vertices.","We focus on axis-aligned translations, and more generally, translations restricted to a given set of directions, and we study the two settings where the robots move in the free plane, and where they are confined to a bounding box.","We obtain fixed-parameter tractable (FPT) algorithms parameterized by $k$ for all the settings under consideration.","In the case where the robots move serially (i.e., one in each time step) and axis-aligned, we prove a structural result stating that every problem instance admits an optimal solution in which the moves are along a grid, whose size is a function of $k$, that can be defined based on the input instance.","This structural result implies that the problem is fixed-parameter tractable parameterized by $k$. We also consider the case in which the robots move in parallel (i.e., multiple robots can move during the same time step), and which falls under the category of Coordinated Motion Planning problems.","Finally, we show that, when the robots move in the free plane, the FPT results for the serial motion case carry over to the case where the translations are restricted to any given set of directions."],"url":"http://arxiv.org/abs/2402.17846v1","category":"cs.CG"}
{"created":"2024-02-27 19:17:55","title":"The cold interstellar medium of a normal sub-$L^\\star$ galaxy at the end of reionization","abstract":"We present the results of a ~60-hr observational campaign with ALMA targeting a spectroscopically confirmed and lensed sub-$L^\\star$ galaxy at z=6.07, identified during the ALMA Lensing Cluster Survey (ALCS). We sample the dust continuum emission from rest frame 90 to 370 $\\mu$m at six different frequencies and set constraining upper limits on the molecular gas line emission and content via CO(7-6) and [CI](2-1) for two lensed images with $\\mu\\gtrsim20$. Complementing these sub-mm observations with deep optical and near-IR photometry and spectroscopy with JWST, we find this galaxy to form stars at a rate of SFR~7 Msun/yr, ~50-70% of which is obscured by dust. This is consistent with what is expected for a $M_\\star$~7.5$\\times10^{8}$ Msun object by extrapolating the $M_\\star$-obscured SFR fraction relation at z<2.5 and with observations at 5<z<7. The dust temperature of ~50K is similar to that of more massive galaxies at similar redshifts, although with large uncertainties and with possible negative gradients. We measure a dust mass of $M_{\\rm dust}$~1.5$\\times10^6$ Msun and, by combining [CI], [CII], and a dynamical estimate, a gas mass of ~2$\\times10^9$ Msun. Their ratio is in good agreement with the predictions from models in the literature. The $M_{\\rm dust}$/$M_\\star$ fraction of ~0.002 and the young stellar age are consistent with dust production via supernovae. Also, models predict a number density of galaxies with $M_{\\rm dust}\\sim10^{6}$ Msun at z=6 in agreement with our estimate from the parent ALCS survey. The combination of lensing and multiwavelength observations allow us to probe luminosity regimes up to two orders of magnitude lower than what has been explored so far for field galaxies at similar redshifts. Our results serve as a benchmark for future observations of faint sub-$L^\\star$ galaxy population that might have driven the reionization of the Universe. [Abridged]","sentences":["We present the results of a ~60-hr observational campaign with ALMA targeting a spectroscopically confirmed and lensed sub-$L^\\star$ galaxy at z=6.07, identified during the ALMA Lensing Cluster Survey (ALCS).","We sample the dust continuum emission from rest frame 90 to 370 $\\mu$m at six different frequencies and set constraining upper limits on the molecular gas line emission and content via CO(7-6) and","[CI](2-1) for two lensed images with $\\mu\\gtrsim20$. Complementing these sub-mm observations with deep optical and near-IR photometry and spectroscopy with JWST, we find this galaxy to form stars at a rate of SFR~7 Msun/yr, ~50-70% of which is obscured by dust.","This is consistent with what is expected for a $M_\\star$~7.5$\\times10^{8}$ Msun object by extrapolating the $M_\\star$-obscured SFR fraction relation at z<2.5 and with observations at 5<z<7.","The dust temperature of ~50K is similar to that of more massive galaxies at similar redshifts, although with large uncertainties and with possible negative gradients.","We measure a dust mass of $M_{\\rm dust}$~1.5$\\times10^6$ Msun and, by combining [CI], [CII], and a dynamical estimate, a gas mass of ~2$\\times10^9$ Msun.","Their ratio is in good agreement with the predictions from models in the literature.","The $M_{\\rm dust}$/$M_\\star$ fraction of ~0.002 and the young stellar age are consistent with dust production via supernovae.","Also, models predict a number density of galaxies with $M_{\\rm dust}\\sim10^{6}$ Msun at z=6 in agreement with our estimate from the parent ALCS survey.","The combination of lensing and multiwavelength observations allow us to probe luminosity regimes up to two orders of magnitude lower than what has been explored so far for field galaxies at similar redshifts.","Our results serve as a benchmark for future observations of faint sub-$L^\\star$ galaxy population that might have driven the reionization of the Universe.","[Abridged]"],"url":"http://arxiv.org/abs/2402.17845v1","category":"astro-ph.GA"}
{"created":"2024-02-27 19:09:49","title":"Public Goods Games in Disease Evolution and Spread","abstract":"Cooperation arises in nature at every scale, from within cells to entire ecosystems. In the framework of evolutionary game theory, public goods games (PGGs) are used to analyse scenarios where individuals can cooperate or defect, and can predict when and how these behaviours emerge. However, too few examples motivate the transferal of knowledge from one application of PGGs to another. Here, we focus on PGGs arising in disease modelling of cancer evolution and the spread of infectious diseases. We use these two systems as case studies for the development of the theory and applications of PGGs, which we succinctly review and compare. We also posit that applications of evolutionary game theory to decision-making in cancer, such as interactions between a clinician and a tumour, can learn from the PGGs studied in epidemiology, where cooperative behaviours such as quarantine and vaccination compliance have been more thoroughly investigated. Furthermore, instances of cellular-level cooperation observed in cancers point to a corresponding area of potential interest for modellers of other diseases, be they viral, bacterial or otherwise. We aim to demonstrate the breadth of applicability of PGGs in disease modelling while providing a starting point for those interested in quantifying cooperation arising in healthcare.","sentences":["Cooperation arises in nature at every scale, from within cells to entire ecosystems.","In the framework of evolutionary game theory, public goods games (PGGs) are used to analyse scenarios where individuals can cooperate or defect, and can predict when and how these behaviours emerge.","However, too few examples motivate the transferal of knowledge from one application of PGGs to another.","Here, we focus on PGGs arising in disease modelling of cancer evolution and the spread of infectious diseases.","We use these two systems as case studies for the development of the theory and applications of PGGs, which we succinctly review and compare.","We also posit that applications of evolutionary game theory to decision-making in cancer, such as interactions between a clinician and a tumour, can learn from the PGGs studied in epidemiology, where cooperative behaviours such as quarantine and vaccination compliance have been more thoroughly investigated.","Furthermore, instances of cellular-level cooperation observed in cancers point to a corresponding area of potential interest for modellers of other diseases, be they viral, bacterial or otherwise.","We aim to demonstrate the breadth of applicability of PGGs in disease modelling while providing a starting point for those interested in quantifying cooperation arising in healthcare."],"url":"http://arxiv.org/abs/2402.17842v1","category":"q-bio.PE"}
{"created":"2024-02-27 19:03:07","title":"Personalizing Smart Home Privacy Protection With Individuals' Regulatory Focus: Would You Preserve or Enhance Your Information Privacy?","abstract":"In this study, we explore the effectiveness of persuasive messages endorsing the adoption of a privacy protection technology (IoT Inspector) tailored to individuals' regulatory focus (promotion or prevention). We explore if and how regulatory fit (i.e., tuning the goal-pursuit mechanism to individuals' internal regulatory focus) can increase persuasion and adoption. We conducted a between-subject experiment (N = 236) presenting participants with the IoT Inspector in gain (\"Privacy Enhancing Technology\" -- PET) or loss (\"Privacy Preserving Technology\" -- PPT) framing. Results show that the effect of regulatory fit on adoption is mediated by trust and privacy calculus processes: prevention-focused users who read the PPT message trust the tool more. Furthermore, privacy calculus favors using the tool when promotion-focused individuals read the PET message. We discuss the contribution of understanding the cognitive mechanisms behind regulatory fit in privacy decision-making to support privacy protection.","sentences":["In this study, we explore the effectiveness of persuasive messages endorsing the adoption of a privacy protection technology (IoT Inspector) tailored to individuals' regulatory focus (promotion or prevention).","We explore if and how regulatory fit (i.e., tuning the goal-pursuit mechanism to individuals' internal regulatory focus) can increase persuasion and adoption.","We conducted a between-subject experiment (N = 236) presenting participants with the IoT Inspector in gain (\"Privacy Enhancing Technology\" -- PET) or loss (\"Privacy Preserving Technology\" -- PPT) framing.","Results show that the effect of regulatory fit on adoption is mediated by trust and privacy calculus processes: prevention-focused users who read the PPT message trust the tool more.","Furthermore, privacy calculus favors using the tool when promotion-focused individuals read the PET message.","We discuss the contribution of understanding the cognitive mechanisms behind regulatory fit in privacy decision-making to support privacy protection."],"url":"http://arxiv.org/abs/2402.17838v1","category":"cs.HC"}
{"created":"2024-02-27 19:00:09","title":"Black Holes in Multi-Metric Gravity","abstract":"We construct a wide class of black hole solutions to the general theory of ghost free multi-metric gravity in arbitrary spacetime dimension, extending and generalising the known results in 4-dimensional dRGT massive gravity and bigravity. The solutions are split into three generic classes based on whether the metrics can be simultaneously diagonalised - one of which does not exist in dRGT massive gravity nor bigravity, and is only possible when one has more than two interacting metric fields. We also linearise the general multi-metric theory to determine the dynamics of the massive spin-2 modes, including examples where this can be done analytically, and use the linear theory to discuss the stability of the 4-dimensional multi-Schwarzchild and multi-Kerr solutions. We explain how the instabilities that plague these solutions in dRGT massive gravity and bigravity carry across to the general multi-metric theory, touching upon ideas of dimensional deconstruction to make sense of the results.","sentences":["We construct a wide class of black hole solutions to the general theory of ghost free multi-metric gravity in arbitrary spacetime dimension, extending and generalising the known results in 4-dimensional dRGT massive gravity and bigravity.","The solutions are split into three generic classes based on whether the metrics can be simultaneously diagonalised - one of which does not exist in dRGT massive gravity nor bigravity, and is only possible when one has more than two interacting metric fields.","We also linearise the general multi-metric theory to determine the dynamics of the massive spin-2 modes, including examples where this can be done analytically, and use the linear theory to discuss the stability of the 4-dimensional multi-Schwarzchild and multi-Kerr solutions.","We explain how the instabilities that plague these solutions in dRGT massive gravity and bigravity carry across to the general multi-metric theory, touching upon ideas of dimensional deconstruction to make sense of the results."],"url":"http://arxiv.org/abs/2402.17835v1","category":"gr-qc"}
{"created":"2024-02-27 19:00:07","title":"Scaling quantum computing with dynamic circuits","abstract":"Quantum computers process information with the laws of quantum mechanics. Current quantum hardware is noisy, can only store information for a short time, and is limited to a few quantum bits, i.e., qubits, typically arranged in a planar connectivity. However, many applications of quantum computing require more connectivity than the planar lattice offered by the hardware on more qubits than is available on a single quantum processing unit (QPU). Here we overcome these limitations with error mitigated dynamic circuits and circuit-cutting to create quantum states requiring a periodic connectivity employing up to 142 qubits spanning multiple QPUs connected in real-time with a classical link. In a dynamic circuit, quantum gates can be classically controlled by the outcomes of mid-circuit measurements within run-time, i.e., within a fraction of the coherence time of the qubits. Our real-time classical link allows us to apply a quantum gate on one QPU conditioned on the outcome of a measurement on another QPU which enables a modular scaling of quantum hardware. Furthermore, the error mitigated control-flow enhances qubit connectivity and the instruction set of the hardware thus increasing the versatility of our quantum computers. Dynamic circuits and quantum modularity are thus key to scale quantum computers and make them useful.","sentences":["Quantum computers process information with the laws of quantum mechanics.","Current quantum hardware is noisy, can only store information for a short time, and is limited to a few quantum bits, i.e., qubits, typically arranged in a planar connectivity.","However, many applications of quantum computing require more connectivity than the planar lattice offered by the hardware on more qubits than is available on a single quantum processing unit (QPU).","Here we overcome these limitations with error mitigated dynamic circuits and circuit-cutting to create quantum states requiring a periodic connectivity employing up to 142 qubits spanning multiple QPUs connected in real-time with a classical link.","In a dynamic circuit, quantum gates can be classically controlled by the outcomes of mid-circuit measurements within run-time, i.e., within a fraction of the coherence time of the qubits.","Our real-time classical link allows us to apply a quantum gate on one QPU conditioned on the outcome of a measurement on another QPU which enables a modular scaling of quantum hardware.","Furthermore, the error mitigated control-flow enhances qubit connectivity and the instruction set of the hardware thus increasing the versatility of our quantum computers.","Dynamic circuits and quantum modularity are thus key to scale quantum computers and make them useful."],"url":"http://arxiv.org/abs/2402.17833v1","category":"quant-ph"}
{"created":"2024-02-27 19:00:07","title":"Stable LM 2 1.6B Technical Report","abstract":"We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.","sentences":["We introduce StableLM 2 1.6B, the first in a new generation of our language model series.","In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B.","The weights for both models are available via Hugging Face for anyone to download and use.","The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues.","At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin.","Given its appealing small size, we also provide throughput measurements on a number of edge devices.","In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model."],"url":"http://arxiv.org/abs/2402.17834v1","category":"cs.CL"}
{"created":"2024-02-27 19:00:02","title":"The X-ray enhancements of radio-loud quasars at high redshift: New results at $z = 4\\text{ -- }7$","abstract":"Highly radio-loud quasars (HRLQs; $\\log R>2.5$) at $z\\gtrsim 4$ show apparent enhanced X-ray emission compared to matched HRLQs at lower redshifts, perhaps due to a redshift-dependent fractional contribution to the X-ray luminosity from inverse-Compton scattering of cosmic microwave background photons (IC/CMB). Using new {\\it Chandra} observations and archival X-ray data, we investigate this phenomenon with an optically flux-limited sample of 41 HRLQs at $z = 4$--5.5 all with sensitive X-ray coverage, the largest sample utilized to date by a wide margin. X-ray enhancements are assessed using X-ray-to-optical flux ratios and spectral energy distributions. We confirm the presence of X-ray enhancements at a 4.9--5.3$\\sigma$ significance level, finding that the median factor of enhancement is $\\approx 1.8$ at our sample median redshift of $z\\approx 4.4$. Under a fractional IC/CMB model, the expected enhancement at lower redshifts is modest; e.g., $\\approx 4$% at $z\\approx 1.5$. We also investigate a sample of seven radio-loud quasars (RLQs; $\\log R>1$) at even higher redshifts of $z=5.6$--6.8, using new and archival X-ray data. These RLQs also show evidence for X-ray enhancements by a median factor of $\\approx 2.7$ at a 3.7--4.9$\\sigma$ significance level. The X-ray spectral and other properties of these $z=5.6$--6.8 RLQs, however, pose challenges for a straightforward fractional IC/CMB interpretation of their enhancements.","sentences":["Highly radio-loud quasars (HRLQs; $\\log R>2.5$) at $z\\gtrsim 4$ show apparent enhanced X-ray emission compared to matched HRLQs at lower redshifts, perhaps due to a redshift-dependent fractional contribution to the X-ray luminosity from inverse-Compton scattering of cosmic microwave background photons (IC/CMB).","Using new {\\it Chandra} observations and archival X-ray data, we investigate this phenomenon with an optically flux-limited sample of 41 HRLQs at $z = 4$--5.5 all with sensitive X-ray coverage, the largest sample utilized to date by a wide margin.","X-ray enhancements are assessed using X-ray-to-optical flux ratios and spectral energy distributions.","We confirm the presence of X-ray enhancements at a 4.9--5.3$\\sigma$ significance level, finding that the median factor of enhancement is $\\approx 1.8$ at our sample median redshift of $z\\approx 4.4$. Under a fractional IC/CMB model, the expected enhancement at lower redshifts is modest; e.g., $\\approx 4$% at $z\\approx 1.5$. We also investigate a sample of seven radio-loud quasars (RLQs; $\\log R>1$) at even higher redshifts of $z=5.6$--6.8, using new and archival X-ray data.","These RLQs also show evidence for X-ray enhancements by a median factor of $\\approx 2.7$ at a 3.7--4.9$\\sigma$ significance level.","The X-ray spectral and other properties of these $z=5.6$--6.8 RLQs, however, pose challenges for a straightforward fractional IC/CMB interpretation of their enhancements."],"url":"http://arxiv.org/abs/2402.17828v1","category":"astro-ph.HE"}
{"created":"2024-02-27 19:00:01","title":"Particle detectors under chronological hazard","abstract":"We analyze how the presence of closed timelike curves (CTCs) characterizing a time machine can be discerned by placing a local particle detector in a region of spacetime which is causally disconnected from the CTCs. Our study shows that not only can the detector tell if there are CTCs, but also that the detector can separate topological from geometrical information and distinguish periodic spacetimes without CTCs (like the Einstein cylinder), curvature, and spacetimes with topological identifications that enable time-machines.","sentences":["We analyze how the presence of closed timelike curves (CTCs) characterizing a time machine can be discerned by placing a local particle detector in a region of spacetime which is causally disconnected from the CTCs.","Our study shows that not only can the detector tell if there are CTCs, but also that the detector can separate topological from geometrical information and distinguish periodic spacetimes without CTCs (like the Einstein cylinder), curvature, and spacetimes with topological identifications that enable time-machines."],"url":"http://arxiv.org/abs/2402.17825v1","category":"quant-ph"}
{"created":"2024-02-27 19:00:00","title":"Gradient Properties of Perturbative Multiscalar RG Flows to Six Loops","abstract":"The gradient property of the renormalisation group (RG) flow of multiscalar theories is examined perturbatively in $d=4$ and $d=4-\\varepsilon$ dimensions. Such theories undergo RG flows in the space of quartic couplings $\\lambda^I$. Starting at five loops, the relevant vector field that determines the physical RG flow is not the beta function traditionally computed in a minimal subtraction scheme in dimensional regularisation, but a suitable modification of it, the $B$ function. It is found that up to five loops the $B$ vector field is gradient, i.e. $B^I=G^{IJ}\\partial A / \\partial\\lambda^J$ with $A$ a scalar and $G_{IJ}$ a rank-two symmetric tensor of the couplings. Up to five loops the beta function is also gradient, but it fails to be so at six loops. The conditions under which the $B$ function (and hence the RG flow) is gradient at six loops are specified, but their verification rests on a separate six-loop computation that remains to be performed.","sentences":["The gradient property of the renormalisation group (RG) flow of multiscalar theories is examined perturbatively in $d=4$ and $d=4-\\varepsilon$ dimensions.","Such theories undergo RG flows in the space of quartic couplings $\\lambda^I$. Starting at five loops, the relevant vector field that determines the physical RG flow is not the beta function traditionally computed in a minimal subtraction scheme in dimensional regularisation, but a suitable modification of it, the $B$ function.","It is found that up to five loops the $B$ vector field is gradient, i.e. $B^I=G^{IJ}\\partial A / \\partial\\lambda^J$ with $A$ a scalar and $G_{IJ}$ a rank-two symmetric tensor of the couplings.","Up to five loops the beta function is also gradient, but it fails to be so at six loops.","The conditions under which the $B$ function (and hence the RG flow) is gradient at six loops are specified, but their verification rests on a separate six-loop computation that remains to be performed."],"url":"http://arxiv.org/abs/2402.17817v1","category":"hep-th"}
{"created":"2024-02-27 19:00:00","title":"The FLAMINGO simulation view of cluster progenitors observed in the epoch of reionization with JWST","abstract":"Motivated by the recent JWST discovery of galaxy overdensities during the Epoch of Reionzation, we examine the physical properties of high-$z$ protoclusters and their evolution using the FLAMINGO simulation suite. We investigate the impact of the apertures used to define protoclusters, because the heterogeneous apertures used in the literature have limited our understanding of the population. Our results are insensitive to the uncertainties of the subgrid models at a given resolution, whereas further investigation into the dependence on numerical resolution is needed. When considering galaxies more massive than $M_\\ast\\,{\\simeq}\\,10^8\\,{\\rm M_\\odot}$, the FLAMINGO simulations predict a dominant contribution from progenitors similar to those of the Coma cluster to the cosmic star-formation rate density during the reionization epoch. Our results indicate the onset of suppression of star formation in the protocluster environments as early as $z\\,{\\simeq}\\,5$. The galaxy number density profiles are similar to NFW at $z\\,{\\lesssim}\\,1$ while showing a steeper slope at earlier times before the formation of the core. Different from most previous simulations, the predicted star-formation history for individual protoclusters is in good agreement with observations. We demonstrate that, depending on the aperture, the integrated physical properties including the total (dark matter and baryonic) mass can be biased by a factor of 2 to 5 at $z\\,{=}\\,5.5$--$7$, and by an order of magnitude at $z\\,{\\lesssim}\\,4$. This correction suffices to remove the ${\\simeq}\\,3\\,\\sigma$ tensions with the number density of structures found in recent JWST observations.","sentences":["Motivated by the recent JWST discovery of galaxy overdensities during the Epoch of Reionzation, we examine the physical properties of high-$z$ protoclusters and their evolution using the FLAMINGO simulation suite.","We investigate the impact of the apertures used to define protoclusters, because the heterogeneous apertures used in the literature have limited our understanding of the population.","Our results are insensitive to the uncertainties of the subgrid models at a given resolution, whereas further investigation into the dependence on numerical resolution is needed.","When considering galaxies more massive than $M_\\ast\\,{\\simeq}\\,10^8\\,{\\rm M_\\odot}$, the FLAMINGO simulations predict a dominant contribution from progenitors similar to those of the Coma cluster to the cosmic star-formation rate density during the reionization epoch.","Our results indicate the onset of suppression of star formation in the protocluster environments as early as $z\\,{\\simeq}\\,5$.","The galaxy number density profiles are similar to NFW at $z\\,{\\lesssim}\\,1$ while showing a steeper slope at earlier times before the formation of the core.","Different from most previous simulations, the predicted star-formation history for individual protoclusters is in good agreement with observations.","We demonstrate that, depending on the aperture, the integrated physical properties including the total (dark matter and baryonic) mass can be biased by a factor of 2 to 5 at $z\\,{=}\\,5.5$--$7$, and by an order of magnitude at $z\\,{\\lesssim}\\,4$. This correction suffices to remove the ${\\simeq}\\,3\\,\\sigma$ tensions with the number density of structures found in recent JWST observations."],"url":"http://arxiv.org/abs/2402.17819v1","category":"astro-ph.GA"}
{"created":"2024-02-27 19:00:00","title":"An Axion Pulsarscope","abstract":"Electromagnetic fields surrounding pulsars may source coherent ultralight axion signals at the known rotational frequencies of the neutron stars, which can be detected by laboratory experiments (e.g., pulsarscopes). As a promising case study, we model axion emission from the well-studied Crab pulsar, which would yield a prominent signal at $f \\approx 29.6$ Hz regardless of whether the axion contributes to the dark matter abundance. We estimate the relevant sensitivity of future axion dark matter detection experiments such as DMRadio-GUT, Dark SRF, and CASPEr, assuming different magnetosphere models to bracket the uncertainty in astrophysical modeling. For example, depending on final experimental parameters, the Dark SRF experiment could probe axions with any mass $m_a \\ll 10^{-13}$ eV down to $g_{a\\gamma\\gamma} \\sim 3 \\times 10^{-13}$ GeV$^{-1}$ with one year of data and assuming the vacuum magnetosphere model. These projected sensitivities may be degraded depending on the extent to which the magnetosphere is screened by charge-filled plasma. The promise of pulsar-sourced axions as a clean target for direct detection experiments motivates dedicated simulations of axion production in pulsar magnetospheres.","sentences":["Electromagnetic fields surrounding pulsars may source coherent ultralight axion signals at the known rotational frequencies of the neutron stars, which can be detected by laboratory experiments (e.g., pulsarscopes).","As a promising case study, we model axion emission from the well-studied Crab pulsar, which would yield a prominent signal at $f \\approx 29.6$ Hz regardless of whether the axion contributes to the dark matter abundance.","We estimate the relevant sensitivity of future axion dark matter detection experiments such as DMRadio-GUT, Dark SRF, and CASPEr, assuming different magnetosphere models to bracket the uncertainty in astrophysical modeling.","For example, depending on final experimental parameters, the Dark SRF experiment could probe axions with any mass $m_a \\ll 10^{-13}$ eV down to $g_{a\\gamma\\gamma} \\sim 3 \\times 10^{-13}$ GeV$^{-1}$ with one year of data and assuming the vacuum magnetosphere model.","These projected sensitivities may be degraded depending on the extent to which the magnetosphere is screened by charge-filled plasma.","The promise of pulsar-sourced axions as a clean target for direct detection experiments motivates dedicated simulations of axion production in pulsar magnetospheres."],"url":"http://arxiv.org/abs/2402.17820v1","category":"hep-ph"}
{"created":"2024-02-28 17:31:39","title":"Detection of Micromobility Vehicles in Urban Traffic Videos","abstract":"Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects. Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.","sentences":["Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes.","To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks.","This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture.","This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability.","Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects.","Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur."],"url":"http://arxiv.org/abs/2402.18503v1","category":"cs.CV"}
{"created":"2024-02-28 17:18:38","title":"TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding","abstract":"The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \\url{https://alanzhangcs.github.io/tamm-page}.","sentences":["The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes.","However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods.","This is attributed to the domain shift in the 2D images and the distinct focus of each modality.","To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters.","First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs.","Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training.","Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks.","Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0.","Project page: \\url{https://alanzhangcs.github.io/tamm-page}."],"url":"http://arxiv.org/abs/2402.18490v1","category":"cs.CV"}
{"created":"2024-02-28 16:39:40","title":"Transition from weak turbulence to collapse turbulence regimes in MMT model","abstract":"It is well known that wave collapses can emerge from the focusing one-dimensional (1-D) Majda-McLaughlin-Tabak (MMT) model as a result of modulational instability. However, how these wave collapses affect the spectral properties and statistics of the wave field has not been adequately studied. We undertake this task by simulating the forced-dissipated 1-D MMT model over a range of forcing amplitudes. Our results show that when the forcing is weak, the spectrum agrees well with the prediction by wave turbulence theory with few collapses in the field. As the forcing strength increases, we see an increase in the occurrence of collapses, together with a transition from a power-law spectrum to an exponentially decaying spectrum. Through a spectral decomposition, we find that the exponential spectrum is dominated by the wave collapse component in the non-integrable MMT model, which is in analogy to a soliton gas in integrable turbulence.","sentences":["It is well known that wave collapses can emerge from the focusing one-dimensional (1-D) Majda-McLaughlin-Tabak (MMT) model as a result of modulational instability.","However, how these wave collapses affect the spectral properties and statistics of the wave field has not been adequately studied.","We undertake this task by simulating the forced-dissipated 1-D MMT model over a range of forcing amplitudes.","Our results show that when the forcing is weak, the spectrum agrees well with the prediction by wave turbulence theory with few collapses in the field.","As the forcing strength increases, we see an increase in the occurrence of collapses, together with a transition from a power-law spectrum to an exponentially decaying spectrum.","Through a spectral decomposition, we find that the exponential spectrum is dominated by the wave collapse component in the non-integrable MMT model, which is in analogy to a soliton gas in integrable turbulence."],"url":"http://arxiv.org/abs/2402.18464v1","category":"nlin.PS"}
{"created":"2024-02-28 15:04:44","title":"Robust Quantification of Percent Emphysema on CT via Domain Attention: the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study","abstract":"Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans. Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density. Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study. To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework. We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors. We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results.","sentences":["Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans.","Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density.","Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study.","To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework.","We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors.","We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results."],"url":"http://arxiv.org/abs/2402.18383v1","category":"cs.CV"}
{"created":"2024-02-28 13:42:12","title":"Robotising Psychometrics: Validating Wellbeing Assessment Tools in Child-Robot Interactions","abstract":"The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains. However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation. In this work, we conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing. Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI. Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings. Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI. Findings from this work highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations.","sentences":["The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains.","However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation.","In this work, we conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing.","Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI.","Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings.","Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI.","Findings from this work highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations."],"url":"http://arxiv.org/abs/2402.18325v1","category":"cs.HC"}
{"created":"2024-02-28 13:07:16","title":"Feature Denoising For Low-Light Instance Segmentation Using Weighted Non-Local Blocks","abstract":"Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0.","sentences":["Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast.","In this paper, we propose an end-to-end solution to address this challenging task.","Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor.","This integration enables an inherent denoising process at the feature level.","As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets.","We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways.   ","Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0."],"url":"http://arxiv.org/abs/2402.18307v1","category":"cs.CV"}
{"created":"2024-02-28 11:45:35","title":"Phononic Crystals in Superfluid Thin-Film Helium","abstract":"In recent years, nanomechanical oscillators in thin films of superfluid helium have attracted attention in the field of optomechanics due to their exceptionally low mechanical dissipation and optical scattering. Mechanical excitations in superfluid thin films - so-called third sound waves - can interact with the optical mode of an optical microresonator by modulation of its effective refractive index enabling optomechanical coupling. Strong confinement of third sound modes enhances their intrinsic mechanical non-linearity paving the way for strong phonon-phonon interactions with applications in quantum optomechanics. Here, we realize a phononic crystal cavity confining third sound modes in a superfluid helium film to length scales close to the third sound wavelength. A few nanometer thick superfluid film is self-assembled on top of a silicon nanobeam optical resonator. The periodic patterning of the silicon material creates a periodic modulation of the superfluid film leading to the formation of a phononic band gap. By engineering the geometry of the silicon nanobeam, the phononic band gap allows the confinement of a localized phononic mode.","sentences":["In recent years, nanomechanical oscillators in thin films of superfluid helium have attracted attention in the field of optomechanics due to their exceptionally low mechanical dissipation and optical scattering.","Mechanical excitations in superfluid thin films - so-called third sound waves - can interact with the optical mode of an optical microresonator by modulation of its effective refractive index enabling optomechanical coupling.","Strong confinement of third sound modes enhances their intrinsic mechanical non-linearity paving the way for strong phonon-phonon interactions with applications in quantum optomechanics.","Here, we realize a phononic crystal cavity confining third sound modes in a superfluid helium film to length scales close to the third sound wavelength.","A few nanometer thick superfluid film is self-assembled on top of a silicon nanobeam optical resonator.","The periodic patterning of the silicon material creates a periodic modulation of the superfluid film leading to the formation of a phononic band gap.","By engineering the geometry of the silicon nanobeam, the phononic band gap allows the confinement of a localized phononic mode."],"url":"http://arxiv.org/abs/2402.18259v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 07:37:26","title":"Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization","abstract":"Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: https://github.com/Alexiland/MLOMAE","sentences":["Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning.","It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones.","A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask.","To overcome this, some approaches propose masking based on patch informativeness.","However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks.","In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining.","Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning.","Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency.","Our code is available at: https://github.com/Alexiland/MLOMAE"],"url":"http://arxiv.org/abs/2402.18128v1","category":"cs.CV"}
{"created":"2024-02-28 04:43:46","title":"Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore","abstract":"Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.","sentences":["Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge.","While extensive research has addressed this in English, little is known about multilingual LLMs.","This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions.","We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages.","Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts.","Furthermore, multilingual models demonstrate a bias towards factual information from Western continents.","These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation."],"url":"http://arxiv.org/abs/2402.18045v1","category":"cs.CL"}
{"created":"2024-02-28 00:09:07","title":"Gradient-Free Adaptive Global Pruning for Pre-trained Language Models","abstract":"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.","sentences":["The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.","Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency.","Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions.","Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality.","AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17946v1","category":"cs.CL"}
{"created":"2024-02-27 23:04:28","title":"Decremental $(1+\u03b5)$-Approximate Maximum Eigenvector: Dynamic Power Method","abstract":"We present a dynamic algorithm for maintaining $(1+\\epsilon)$-approximate maximum eigenvector and eigenvalue of a positive semi-definite matrix $A$ undergoing \\emph{decreasing} updates, i.e., updates which may only decrease eigenvalues. Given a vector $v$ updating $A\\gets A-vv^{\\top}$, our algorithm takes $\\tilde{O}(\\mathrm{nnz}(v))$ amortized update time, i.e., polylogarithmic per non-zeros in the update vector.   Our technique is based on a novel analysis of the influential power method in the dynamic setting. The two previous sets of techniques have the following drawbacks (1) algebraic techniques can maintain exact solutions but their update time is at least polynomial per non-zeros, and (2) sketching techniques admit polylogarithmic update time but suffer from a crude additive approximation.   Our algorithm exploits an oblivious adversary. Interestingly, we show that any algorithm with polylogarithmic update time per non-zeros that works against an adaptive adversary and satisfies an additional natural property would imply a breakthrough for checking psd-ness of matrices in $\\tilde{O}(n^{2})$ time, instead of $O(n^{\\omega})$ time.","sentences":["We present a dynamic algorithm for maintaining $(1+\\epsilon)$-approximate maximum eigenvector and eigenvalue of a positive semi-definite matrix $A$ undergoing \\emph{decreasing} updates, i.e., updates which may only decrease eigenvalues.","Given a vector $v$ updating $A\\gets A-vv^{\\top}$, our algorithm takes $\\tilde{O}(\\mathrm{nnz}(v))$ amortized update time, i.e., polylogarithmic per non-zeros in the update vector.   ","Our technique is based on a novel analysis of the influential power method in the dynamic setting.","The two previous sets of techniques have the following drawbacks (1) algebraic techniques can maintain exact solutions but their update time is at least polynomial per non-zeros, and (2) sketching techniques admit polylogarithmic update time but suffer from a crude additive approximation.   ","Our algorithm exploits an oblivious adversary.","Interestingly, we show that any algorithm with polylogarithmic update time per non-zeros that works against an adaptive adversary and satisfies an additional natural property would imply a breakthrough for checking psd-ness of matrices in $\\tilde{O}(n^{2})$ time, instead of $O(n^{\\omega})$ time."],"url":"http://arxiv.org/abs/2402.17929v1","category":"cs.DS"}
{"created":"2024-02-27 22:35:55","title":"Band structure and excitonic properties of WSe$_2$ in the isolated monolayer limit in an all-electron approach","abstract":"A study is presented of the electronic band structure and optical absorption spectrum of monolayer WSe$_2$ using an all-electron quasiparticle self-consistent $GW$ approach, QS$G\\hat W$, in which the screened Coulomb interaction $\\hat W$ is calculated including ladder diagrams representing electron-hole interaction. The Bethe-Salpeter Equation is used to calculate both the screened Coulomb interaction $\\hat W$ in the quasiparticle band structure and the imaginary part of the macroscopic dielectric function. The convergence of the quasiparticle band gap and lowest exciton peak position is studied as function of the separation of the monolayers when using periodic boundary conditions. The quasiparticle gap is found to vary as $1/d$ with $d$ the size of the vacuum separation, while the excitonic lowest peak reaches convergence much faster. The nature of the exciton spectrum is analyzed and shows several excitonic peaks below the quasiparticle gap when a sufficient number of $\\textbf{k}$ points is used. They are found to be in good agreement with prior work and experiment after adding spin-orbit coupling corrections and can be explained in the context of the Wannier-Mott theory adapted to 2D.","sentences":["A study is presented of the electronic band structure and optical absorption spectrum of monolayer WSe$_2$ using an all-electron quasiparticle self-consistent $GW$ approach, QS$G\\hat W$, in which the screened Coulomb interaction $\\hat W$ is calculated including ladder diagrams representing electron-hole interaction.","The Bethe-Salpeter Equation is used to calculate both the screened Coulomb interaction $\\hat W$ in the quasiparticle band structure and the imaginary part of the macroscopic dielectric function.","The convergence of the quasiparticle band gap and lowest exciton peak position is studied as function of the separation of the monolayers when using periodic boundary conditions.","The quasiparticle gap is found to vary as $1/d$ with $d$ the size of the vacuum separation, while the excitonic lowest peak reaches convergence much faster.","The nature of the exciton spectrum is analyzed and shows several excitonic peaks below the quasiparticle gap when a sufficient number of $\\textbf{k}$ points is used.","They are found to be in good agreement with prior work and experiment after adding spin-orbit coupling corrections and can be explained in the context of the Wannier-Mott theory adapted to 2D."],"url":"http://arxiv.org/abs/2402.17924v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-27 21:48:41","title":"NIIRF: Neural IIR Filter Field for HRTF Upsampling and Personalization","abstract":"Head-related transfer functions (HRTFs) are important for immersive audio, and their spatial interpolation has been studied to upsample finite measurements. Recently, neural fields (NFs) which map from sound source direction to HRTF have gained attention. Existing NF-based methods focused on estimating the magnitude of the HRTF from a given sound source direction, and the magnitude is converted to a finite impulse response (FIR) filter. We propose the neural infinite impulse response filter field (NIIRF) method that instead estimates the coefficients of cascaded IIR filters. IIR filters mimic the modal nature of HRTFs, thus needing fewer coefficients to approximate them well compared to FIR filters. We find that our method can match the performance of existing NF-based methods on multiple datasets, even outperforming them when measurements are sparse. We also explore approaches to personalize the NF to a subject and experimentally find low-rank adaptation to be effective.","sentences":["Head-related transfer functions (HRTFs) are important for immersive audio, and their spatial interpolation has been studied to upsample finite measurements.","Recently, neural fields (NFs) which map from sound source direction to HRTF have gained attention.","Existing NF-based methods focused on estimating the magnitude of the HRTF from a given sound source direction, and the magnitude is converted to a finite impulse response (FIR) filter.","We propose the neural infinite impulse response filter field (NIIRF) method that instead estimates the coefficients of cascaded IIR filters.","IIR filters mimic the modal nature of HRTFs, thus needing fewer coefficients to approximate them well compared to FIR filters.","We find that our method can match the performance of existing NF-based methods on multiple datasets, even outperforming them when measurements are sparse.","We also explore approaches to personalize the NF to a subject and experimentally find low-rank adaptation to be effective."],"url":"http://arxiv.org/abs/2402.17907v1","category":"eess.AS"}
{"created":"2024-02-27 21:27:35","title":"A Language Model based Framework for New Concept Placement in Ontologies","abstract":"We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we proposed explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.","sentences":["We investigate the task of inserting new concepts extracted from texts into an ontology using language models.","We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into.","In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection.","We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark.","The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection.","Zero-shot prompting of LLMs is still not adequate for the task, and we proposed explainable instruction tuning of LLMs for improved performance.","Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies."],"url":"http://arxiv.org/abs/2402.17897v1","category":"cs.CL"}
{"created":"2024-02-27 19:28:18","title":"Looking for Complexity at Phase Boundaries in Continuous Cellular Automata","abstract":"One key challenge in Artificial Life is designing systems that display an emergence of complex behaviors. Many such systems depend on a high-dimensional parameter space, only a small subset of which displays interesting dynamics. Focusing on the case of continuous systems, we introduce the 'Phase Transition Finder'(PTF) algorithm, which can be used to efficiently generate parameters lying at the border between two phases. We argue that such points are more likely to display complex behaviors, and confirm this by applying PTF to Lenia showing it can increase the frequency of interesting behaviors more than two-fold, while remaining efficient enough for large-scale searches.","sentences":["One key challenge in Artificial Life is designing systems that display an emergence of complex behaviors.","Many such systems depend on a high-dimensional parameter space, only a small subset of which displays interesting dynamics.","Focusing on the case of continuous systems, we introduce the 'Phase Transition Finder'(PTF) algorithm, which can be used to efficiently generate parameters lying at the border between two phases.","We argue that such points are more likely to display complex behaviors, and confirm this by applying PTF to Lenia showing it can increase the frequency of interesting behaviors more than two-fold, while remaining efficient enough for large-scale searches."],"url":"http://arxiv.org/abs/2402.17848v1","category":"nlin.AO"}
{"created":"2024-02-27 11:05:34","title":"DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models","abstract":"In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes \\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, \\textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at https://diffusekrona.github.io/.","sentences":["In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements.","While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis.","Addressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis.","Crucially, \\textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning.","Furthermore, a more controllable decomposition makes \\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\% reduction with results comparable to LoRA-Dreambooth.","Evaluated against diverse and complex input images and text prompts, \\textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling.","Our project page, consisting of links to the code, and pre-trained checkpoints, is available at https://diffusekrona.github.io/."],"url":"http://arxiv.org/abs/2402.17412v2","category":"cs.CV"}
{"created":"2024-02-26 15:11:41","title":"EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams","abstract":"We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals. Emotion recognition is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset. Each game elicits a different predominant emotion: boredom, calmness, horror, or joy. We analyze individual electrodes, time window lengths, and frequency bands to assess the accuracy and interpretability of resulting user-independent neural models. The findings indicate that both brain hemispheres assist classification, especially electrodes on the temporal (T8) and parietal (P7) areas, alongside contributions from frontal and occipital electrodes. While patterns may manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, exhibited higher correspondence with the emotion classes. The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an accuracy of 81.7% and a 0.0029 II interpretability using 10-second time windows, even in face of a highly-stochastic time-varying 4-class classification problem.","sentences":["We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+).","We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts.","The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting.","As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals.","Emotion recognition is crucial for enhancing the realism and interactivity of computer systems.","We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset.","Each game elicits a different predominant emotion: boredom, calmness, horror, or joy.","We analyze individual electrodes, time window lengths, and frequency bands to assess the accuracy and interpretability of resulting user-independent neural models.","The findings indicate that both brain hemispheres assist classification, especially electrodes on the temporal (T8) and parietal (P7) areas, alongside contributions from frontal and occipital electrodes.","While patterns may manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, exhibited higher correspondence with the emotion classes.","The eGNN-C+ demonstrates effectiveness in learning EEG data.","It achieves an accuracy of 81.7% and a 0.0029 II interpretability using 10-second time windows, even in face of a highly-stochastic time-varying 4-class classification problem."],"url":"http://arxiv.org/abs/2402.17792v1","category":"eess.SP"}
{"created":"2024-02-26 11:42:29","title":"Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision","abstract":"Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation.","sentences":["Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language.","Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages.","In this paper, we show that CLQA can be addressed using a single encoder-decoder model.","To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia.","We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation.","Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation."],"url":"http://arxiv.org/abs/2402.16508v1","category":"cs.CL"}
{"created":"2024-02-26 11:08:26","title":"Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification","abstract":"Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.","sentences":["Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification.","This research addresses this problem with a novel, scalable, and AI-driven solution.","The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types.","Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes.","Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft.","It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification.","To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner.","Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936).","The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality.","The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition."],"url":"http://arxiv.org/abs/2402.16486v1","category":"cs.CV"}
{"created":"2024-02-26 09:59:04","title":"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering","abstract":"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}","sentences":["Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information.","However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied.","This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge.","The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly.","This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods.","We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions.","Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.","The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}"],"url":"http://arxiv.org/abs/2402.16457v1","category":"cs.CL"}
{"created":"2024-02-26 09:50:34","title":"Distortion-Controlled Dithering with Reduced Recompression Rate","abstract":"Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts. In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression. We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task. The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE. The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression.","sentences":["Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts.","In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression.","We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task.","The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE.","The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression."],"url":"http://arxiv.org/abs/2402.16447v1","category":"eess.SP"}
{"created":"2024-02-26 09:43:52","title":"ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing","abstract":"Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation. However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks. Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability. Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories. The latter two tasks are beyond the reach of other ProLLMs. Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}.","sentences":["Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks.","Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation.","However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field.","This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands.","To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks.","Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability.","Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously.","Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task.","In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities.","In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories.","The latter two tasks are beyond the reach of other ProLLMs.","Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}."],"url":"http://arxiv.org/abs/2402.16445v1","category":"cs.CE"}
{"created":"2024-02-26 09:19:46","title":"Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models","abstract":"We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.","sentences":["We present our work on predicting United Nations sustainable development goals (SDG) for university courses.","We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input.","We use this data to train several different smaller language models to predict SDGs for university courses.","This work contributes to better university level adaptation of SDGs.","The best performing model in our experiments was BART with an F1-score of 0.786."],"url":"http://arxiv.org/abs/2402.16420v1","category":"cs.CL"}
{"created":"2024-02-26 09:09:24","title":"Direct excitation of Kelvin waves on quantized vortices","abstract":"Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature. A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave. Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids. However, deliberately exciting Kelvin waves has proven to be challenging. Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4. We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex. A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics. Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior. This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems.","sentences":["Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature.","A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave.","Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids.","However, deliberately exciting Kelvin waves has proven to be challenging.","Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4.","We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex.","A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics.","Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior.","This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems."],"url":"http://arxiv.org/abs/2402.16411v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 08:47:35","title":"Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations","abstract":"Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes. The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state. Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras. However, most of the adopted methods have poor real-time performance. To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations. Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3). Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art. Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively.","sentences":["Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes.","The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state.","Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras.","However, most of the adopted methods have poor real-time performance.","To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations.","Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3).","Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression.","Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art.","Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively."],"url":"http://arxiv.org/abs/2402.16398v1","category":"cs.RO"}
{"created":"2024-02-28 18:59:07","title":"Logarithmic Sobolev Inequalities for Bounded Domains and Applications to Drift-Diffusion Equations","abstract":"We prove logarithmic Sobolev inequalities on higher-dimensional bounded smooth domains based on novel Gagliardo-Nirenberg type interpolation inequalities. Moreover, we use them to address the long-time dynamics of some nonlinear nonlocal drift-diffusion models and prove the exponential decay of their solutions to constant steady states.","sentences":["We prove logarithmic Sobolev inequalities on higher-dimensional bounded smooth domains based on novel Gagliardo-Nirenberg type interpolation inequalities.","Moreover, we use them to address the long-time dynamics of some nonlinear nonlocal drift-diffusion models and prove the exponential decay of their solutions to constant steady states."],"url":"http://arxiv.org/abs/2402.18572v1","category":"math.AP"}
{"created":"2024-02-28 17:07:10","title":"Finite skew braces of square-free order and supersolubility","abstract":"The aim of this paper is to study supersoluble skew braces, a class of skew braces that encompasses all finite skew braces of square-free order. It turns out that finite supersoluble skew braces have Sylow towers, and that in an arbitrary supersoluble skew brace $B$ many relevant skew brace-theoretical properties are easier to identify: for example, a centrally nilpotent ideal of $B$ is $B$-centrally nilpotent, a fact that simplifies the computational search for the Fitting ideal; also, $B$ has finite multipermutational level if and only if $(B,+)$ is nilpotent.   Given a finite presentation of the structure skew brace $G(X,r)$ associated with a finite non-degenerate solution of the Yang--Baxter Equation (YBE), there is an algorithm that decides if $G(X,r)$ is supersoluble or not. Moreover, supersoluble skew braces are examples of almost polycyclic skew braces, so they give rise to solutions of the YBE on which one can algorithmically work on.","sentences":["The aim of this paper is to study supersoluble skew braces, a class of skew braces that encompasses all finite skew braces of square-free order.","It turns out that finite supersoluble skew braces have Sylow towers, and that in an arbitrary supersoluble skew brace $B$ many relevant skew brace-theoretical properties are easier to identify: for example, a centrally nilpotent ideal of $B$ is $B$-centrally nilpotent, a fact that simplifies the computational search for the Fitting ideal; also, $B$ has finite multipermutational level if and only if $(B,+)$ is nilpotent.   ","Given a finite presentation of the structure skew brace $G(X,r)$ associated with a finite non-degenerate solution of the Yang--Baxter Equation (YBE), there is an algorithm that decides if $G(X,r)$ is supersoluble or not.","Moreover, supersoluble skew braces are examples of almost polycyclic skew braces, so they give rise to solutions of the YBE on which one can algorithmically work on."],"url":"http://arxiv.org/abs/2402.18486v1","category":"math.GR"}
{"created":"2024-02-28 16:54:46","title":"Three-leg form factor on Coulomb branch","abstract":"We study the form factor of the lowest component of the stress-tensor multiplet away from the origin of the moduli space in the spontaneously broken, aka Coulomb, phase of the maximally supersymmetric Yang-Mills theory for decay into three massive W-bosons. The calculations are done at two-loop order by deriving and solving canonical differential equations in the asymptotical limit of nearly vanishing W-masses. We confirm our previous findings that infrared physics of `off-shell observables' is governed by the octagon anomalous dimension rather than the cusp. In addition, the form factor in question possesses a nontrivial remainder function, which was found to be identical to the massless case, upon a proper subtraction of infrared logarithms (and finite terms). However, the iterative structure of the object is more intricate and is not simply related to the previous orders in coupling as opposed to amplitudes/form factors at the origin of the moduli space.","sentences":["We study the form factor of the lowest component of the stress-tensor multiplet away from the origin of the moduli space in the spontaneously broken, aka Coulomb, phase of the maximally supersymmetric Yang-Mills theory for decay into three massive W-bosons.","The calculations are done at two-loop order by deriving and solving canonical differential equations in the asymptotical limit of nearly vanishing W-masses.","We confirm our previous findings that infrared physics of `off-shell observables' is governed by the octagon anomalous dimension rather than the cusp.","In addition, the form factor in question possesses a nontrivial remainder function, which was found to be identical to the massless case, upon a proper subtraction of infrared logarithms (and finite terms).","However, the iterative structure of the object is more intricate and is not simply related to the previous orders in coupling as opposed to amplitudes/form factors at the origin of the moduli space."],"url":"http://arxiv.org/abs/2402.18475v1","category":"hep-th"}
{"created":"2024-02-28 16:46:00","title":"Introducing cuDisc: a 2D code for protoplanetary disc structure and evolution calculations","abstract":"We present a new 2D axisymmetric code, cuDisc, for studying protoplanetary discs, focusing on the self-consistent calculation of dust dynamics, grain size distribution and disc temperature. Self-consistently studying these physical processes is essential for many disc problems, such as structure formation and dust removal, given that the processes heavily depend on one another. To follow the evolution over substantial fractions of the disc lifetime, cuDisc uses the CUDA language and libraries to speed up the code through GPU acceleration. cuDisc employs a second-order finite-volume Godonuv solver for dust dynamics, solves the Smoluchowski equation for dust growth and calculates radiative transfer using a multi-frequency hybrid ray-tracing/flux-limited-diffusion method. We benchmark our code against current state-of-the-art codes. Through studying steady-state problems, we find that including 2D structure reveals that when collisions are important, the dust vertical structure appears to reach a diffusion-settling-coagulation equilibrium that can differ substantially from standard models that ignore coagulation. For low fragmentation velocities, we find an enhancement of intermediate-sized dust grains at heights of ~ 1 gas scale height due to the variation in collision rates with height, and for large fragmentation velocities, we find an enhancement of small grains around the disc mid-plane due to collisional ''sweeping'' of small grains by large grains. These results could be important for the analysis of disc SEDs or scattered light images, given these observables are sensitive to the vertical grain distribution.","sentences":["We present a new 2D axisymmetric code, cuDisc, for studying protoplanetary discs, focusing on the self-consistent calculation of dust dynamics, grain size distribution and disc temperature.","Self-consistently studying these physical processes is essential for many disc problems, such as structure formation and dust removal, given that the processes heavily depend on one another.","To follow the evolution over substantial fractions of the disc lifetime, cuDisc uses the CUDA language and libraries to speed up the code through GPU acceleration.","cuDisc employs a second-order finite-volume Godonuv solver for dust dynamics, solves the Smoluchowski equation for dust growth and calculates radiative transfer using a multi-frequency hybrid ray-tracing/flux-limited-diffusion method.","We benchmark our code against current state-of-the-art codes.","Through studying steady-state problems, we find that including 2D structure reveals that when collisions are important, the dust vertical structure appears to reach a diffusion-settling-coagulation equilibrium that can differ substantially from standard models that ignore coagulation.","For low fragmentation velocities, we find an enhancement of intermediate-sized dust grains at heights of ~","1 gas scale height due to the variation in collision rates with height, and for large fragmentation velocities, we find an enhancement of small grains around the disc mid-plane due to collisional ''sweeping'' of small grains by large grains.","These results could be important for the analysis of disc SEDs or scattered light images, given these observables are sensitive to the vertical grain distribution."],"url":"http://arxiv.org/abs/2402.18471v1","category":"astro-ph.EP"}
{"created":"2024-02-28 16:02:56","title":"On Affinely Homogeneous Submanifolds: The Power Series Method of Equivalence","abstract":"We determine all affinely homogeneous models for surfaces $S^2 \\subset \\mathbb{R}^4$, including the simply transitive models. We employ an improved power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations. We find several inequivalent terminal branches yielding each to some nonempty moduli space of homogeneous models, sometimes parametrized by a certain invariant algebraic variety. Three main features may be emphasized: 1) Iterated single-pointed jet bundles; 2) Cartan-enhanced power series method of equivalence; 3) Constant ping-pong between normal forms (nf) and vector fields (vf).","sentences":["We determine all affinely homogeneous models for surfaces $S^2 \\subset \\mathbb{R}^4$, including the simply transitive models.","We employ an improved power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations.","We find several inequivalent terminal branches yielding each to some nonempty moduli space of homogeneous models, sometimes parametrized by a certain invariant algebraic variety.","Three main features may be emphasized: 1) Iterated single-pointed jet bundles; 2) Cartan-enhanced power series method of equivalence; 3) Constant ping-pong between normal forms (nf) and vector fields (vf)."],"url":"http://arxiv.org/abs/2402.18437v1","category":"math.DG"}
{"created":"2024-02-28 15:27:41","title":"WKB-based third order method for the highly oscillatory 1D stationary Schr\u00f6dinger equation","abstract":"This paper introduces an efficient high-order numerical method for solving the 1D stationary Schr\\\"odinger equation in the highly oscillatory regime. Building upon the ideas from [2], we first analytically transform the given equation into a smoother (i.e. less oscillatory) equation. By developing sufficiently accurate quadratures for several (iterated) oscillatory integrals occurring in the Picard approximation of the solution, we obtain a one-step method that is third order w.r.t. the step size. The accuracy and efficiency of the method are illustrated through several numerical examples.","sentences":["This paper introduces an efficient high-order numerical method for solving the 1D stationary Schr\\\"odinger equation in the highly oscillatory regime.","Building upon the ideas from [2], we first analytically transform the given equation into a smoother (i.e. less oscillatory) equation.","By developing sufficiently accurate quadratures for several (iterated) oscillatory integrals occurring in the Picard approximation of the solution, we obtain a one-step method that is third order w.r.t.","the step size.","The accuracy and efficiency of the method are illustrated through several numerical examples."],"url":"http://arxiv.org/abs/2402.18406v1","category":"math.NA"}
{"created":"2024-02-28 13:37:38","title":"Unveiling novel insights into Kirchhoff migration for effective object detection using experimental Fresnel dataset","abstract":"This study investigates the applicability of Kirchhoff migration (KM) for a fast identification of unknown objects in a real-world limited-aperture inverse scattering problem. To demonstrate the theoretical basis for the applicability including unique determination of objects, the imaging function of the KM was formulated using a uniformly convergent infinite series of Bessel functions of integer order of the first kind based on the integral equation formula for the scattered field. Numerical simulations performed using the experimental Fresnel dataset are exhibited to achieve the theoretical results.","sentences":["This study investigates the applicability of Kirchhoff migration (KM) for a fast identification of unknown objects in a real-world limited-aperture inverse scattering problem.","To demonstrate the theoretical basis for the applicability including unique determination of objects, the imaging function of the KM was formulated using a uniformly convergent infinite series of Bessel functions of integer order of the first kind based on the integral equation formula for the scattered field.","Numerical simulations performed using the experimental Fresnel dataset are exhibited to achieve the theoretical results."],"url":"http://arxiv.org/abs/2402.18322v1","category":"math.NA"}
{"created":"2024-02-28 12:16:49","title":"On the $p$-adic valuation of third order linear recurrence sequences","abstract":"In a recent paper, Bilu et al. studied a conjecture of Marques and Lengyel on the $p$-adic valuation of the Tribonacci sequence. In this article, we do a similar study on the $p$-adic valuation of third order linear recurrence sequences by considering a generalisation of the conjecture of Marques and Lengyel for third order linear recurrence sequences. We obtain analogues of the results of Bilu et al. for certain third order linear recurrence sequences. Also, as a consequence of the conjecture, we prove that the solution set of the Diophantine equation given by $x_n=m!$ in positive integers $n,m$ is finite for certain third order linear recurrence sequences $(x_n)$.","sentences":["In a recent paper, Bilu et al. studied a conjecture of Marques and Lengyel on the $p$-adic valuation of the Tribonacci sequence.","In this article, we do a similar study on the $p$-adic valuation of third order linear recurrence sequences by considering a generalisation of the conjecture of Marques and Lengyel for third order linear recurrence sequences.","We obtain analogues of the results of Bilu et al. for certain third order linear recurrence sequences.","Also, as a consequence of the conjecture, we prove that the solution set of the Diophantine equation given by $x_n=m!$ in positive integers $n,m$ is finite for certain third order linear recurrence sequences $(x_n)$."],"url":"http://arxiv.org/abs/2402.18279v1","category":"math.NT"}
{"created":"2024-02-28 11:50:36","title":"Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding","abstract":"The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.","sentences":["The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry.","Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks.","In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages.","Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents.","Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively.","Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks.","The pre-trained models and code are available at https://github.com/X-LANCE/weblm."],"url":"http://arxiv.org/abs/2402.18262v1","category":"cs.CL"}
{"created":"2024-02-28 11:21:43","title":"Boundary controllability for degenerate/singular hyperbolic equations in nondivergence form with drift","abstract":"We study the null controllability for a degenerate/singular wave equation with drift in non divergence form. In particular, considering a control localized on the non degenerate boundary point, we provide some conditions for the boundary controllability via energy methods and boundary observability.","sentences":["We study the null controllability for a degenerate/singular wave equation with drift in non divergence form.","In particular, considering a control localized on the non degenerate boundary point, we provide some conditions for the boundary controllability via energy methods and boundary observability."],"url":"http://arxiv.org/abs/2402.18247v1","category":"math.AP"}
{"created":"2024-02-28 10:09:04","title":"Multi-objective Differentiable Neural Architecture Search","abstract":"Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.","sentences":["Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training.","Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices.","Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint.","In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run.","To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices.","Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method.","Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation."],"url":"http://arxiv.org/abs/2402.18213v1","category":"cs.LG"}
{"created":"2024-02-28 09:47:35","title":"Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments","abstract":"The high incidence of oil spills in port areas poses a serious threat to the environment, prompting the need for efficient detection mechanisms. Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection. Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents. Currently, there's a scarcity of datasets employing RGB images for oil spill detection in maritime settings. This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms. The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a convolutional neural network trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection. This underscores the dataset's practicality for real-world applications, offering crucial resources for environmental conservation in port environments.","sentences":["The high incidence of oil spills in port areas poses a serious threat to the environment, prompting the need for efficient detection mechanisms.","Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection.","Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents.","Currently, there's a scarcity of datasets employing RGB images for oil spill detection in maritime settings.","This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms.","The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a convolutional neural network trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection.","This underscores the dataset's practicality for real-world applications, offering crucial resources for environmental conservation in port environments."],"url":"http://arxiv.org/abs/2402.18202v1","category":"cs.CV"}
{"created":"2024-02-28 09:10:25","title":"Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations","abstract":"Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training.","sentences":["Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision.","At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify.","We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection.","Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain.","We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training."],"url":"http://arxiv.org/abs/2402.18179v1","category":"cs.CL"}
{"created":"2024-02-28 08:45:07","title":"Out-of-Distribution Detection using Neural Activation Prior","abstract":"Out-of-distribution detection is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios.In this paper, we propose a simple but effective Neural Activation Prior (NAP) for out-of-distribution detection (OOD). Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few of its neurons being activated with a larger response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is each channel in a model fully trained on ID dataset would play a role in detecting a certain pattern in the samples within the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Thus, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. This approach is plug-and-play and does not lead to any performance degradation on in-distribution data classification and requires no extra training or statistics from training or external datasets. Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator. Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications. Experimental results show that our method achieves the state-of-the-art performance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates the power of the proposed prior.","sentences":["Out-of-distribution detection is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios.","In this paper, we propose a simple but effective Neural Activation Prior (NAP) for out-of-distribution detection (OOD).","Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few of its neurons being activated with a larger response by an in-distribution (ID) sample is significantly higher than that by an OOD sample.","An intuitive explanation is each channel in a model fully trained on ID dataset would play a role in detecting a certain pattern in the samples within the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample.","Thus, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection.","This approach is plug-and-play and does not lead to any performance degradation on in-distribution data classification and requires no extra training or statistics from training or external datasets.","Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator.","Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications.","Experimental results show that our method achieves the state-of-the-art performance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates the power of the proposed prior."],"url":"http://arxiv.org/abs/2402.18162v1","category":"cs.CV"}
{"created":"2024-02-28 08:43:25","title":"Heintze-Karcher Type Inequalities of Fractional GJMS Operators","abstract":"In this paper, we genelize the Heintze-Karcher type inequalities for fractional Q-curvature $Q_{2\\gamma}$ on conformally compact Einstein manifolds. Such inequality holds for all $\\gamma\\in (0,1]$. In particular, for $\\gamma=\\frac{1}{2}$ and $\\gamma=1$, we obtain some rigidity theorems by characterising the equalities. Moreover, we also verify the Heintze-Karcher inequality on Riemannian manifolds with sectional curvature bounded below.","sentences":["In this paper, we genelize the Heintze-Karcher type inequalities for fractional Q-curvature $Q_{2\\gamma}$ on conformally compact Einstein manifolds.","Such inequality holds for all $\\gamma\\in (0,1]$. In particular, for $\\gamma=\\frac{1}{2}$ and $\\gamma=1$, we obtain some rigidity theorems by characterising the equalities.","Moreover, we also verify the Heintze-Karcher inequality on Riemannian manifolds with sectional curvature bounded below."],"url":"http://arxiv.org/abs/2402.18160v1","category":"math.DG"}
{"created":"2024-02-28 08:11:05","title":"Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis","abstract":"Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (\\texttt{IBG}) explanation framework for ABSA. This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our \\texttt{IBG} approach considerably improves both the models' performance and interpretability by identifying sentiment-aware features.","sentences":["Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity.","Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions.","However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent.","To address this, we propose the Information Bottleneck-based Gradient (\\texttt{IBG}) explanation framework for ABSA.","This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information.","Comprehensive tests show that our \\texttt{IBG} approach considerably improves both the models' performance and interpretability by identifying sentiment-aware features."],"url":"http://arxiv.org/abs/2402.18145v1","category":"cs.CL"}
{"created":"2024-02-28 08:03:50","title":"Urysohn 1-width for 4 and 5 manifolds with positive biRicci curvature","abstract":"It was proved by Gromov-Lawson\\cite{gl83} that complete three manifold with positive scalar curvature bounded below has finite Urysohn 1-width only depends on the uniform positive scalar curvature bounds. It is natural to ask the same question for the four manifolds. In this paper, we can show that closed four and five manifolds with positive biRicci curvature has finite Urysohn 1-width only depends on the curvature bounds. During the proof we can also observe that the fundamental group of those manifolds are virtually free. This gives a quick application that $T^{2}\\times S^{2}$ can't admit positive biRicci curvature.","sentences":["It was proved by Gromov-Lawson\\cite{gl83} that complete three manifold with positive scalar curvature bounded below has finite Urysohn 1-width only depends on the uniform positive scalar curvature bounds.","It is natural to ask the same question for the four manifolds.","In this paper, we can show that closed four and five manifolds with positive biRicci curvature has finite Urysohn 1-width only depends on the curvature bounds.","During the proof we can also observe that the fundamental group of those manifolds are virtually free.","This gives a quick application that $T^{2}\\times S^{2}$ can't admit positive biRicci curvature."],"url":"http://arxiv.org/abs/2402.18141v1","category":"math.DG"}
{"created":"2024-02-28 07:56:58","title":"A comparative computational study of different formulations of the compressible Euler equations for mesoscale atmospheric flows in a finite volume framework","abstract":"We consider three conservative forms of the mildly compressible Euler equations, called CE1, CE2 and CE3, with the goal of understanding which leads to the most accurate and robust pressure-based solver in a finite volume environment. Forms CE1 and CE2 are both written in density, momentum, and specific enthalpy, but employ two different treatments of the buoyancy and pressure gradient terms: for CE1 it is the standard pressure splitting implemented in open-source finite volume solvers (e.g., OpenFOAM), while for CE2 it is the typical pressure splitting found in computational atmospheric studies. Form CE3 is written in density, momentum, and potential temperature, with the buoyancy and pressure terms addressed as in CE2. For each formulation, we adopt a computationally efficient splitting approach. The three formulations are thoroughly assessed and compared through six benchmark tests involving dry air flow over a flat terrain or orography. We found that all three models are able to provide accurate results for the tests with a flat terrain, although the solvers based on the CE2 and CE3 forms are more robust. As for the mountain tests, CE1 solutions become unstable, while the CE2 and CE3 models provide results in very good agreement with data in the literature, the CE3 model being the most accurate. Hence, the CE3 model is the most accurate, reliable, and robust for the simulation of mesoscale atmospheric flows when using a pressure-based approach and space discretization by a finite volume method.","sentences":["We consider three conservative forms of the mildly compressible Euler equations, called CE1, CE2 and CE3, with the goal of understanding which leads to the most accurate and robust pressure-based solver in a finite volume environment.","Forms CE1 and CE2 are both written in density, momentum, and specific enthalpy, but employ two different treatments of the buoyancy and pressure gradient terms: for CE1 it is the standard pressure splitting implemented in open-source finite volume solvers (e.g., OpenFOAM), while for CE2 it is the typical pressure splitting found in computational atmospheric studies.","Form CE3 is written in density, momentum, and potential temperature, with the buoyancy and pressure terms addressed as in CE2.","For each formulation, we adopt a computationally efficient splitting approach.","The three formulations are thoroughly assessed and compared through six benchmark tests involving dry air flow over a flat terrain or orography.","We found that all three models are able to provide accurate results for the tests with a flat terrain, although the solvers based on the CE2 and CE3 forms are more robust.","As for the mountain tests, CE1 solutions become unstable, while the CE2 and CE3 models provide results in very good agreement with data in the literature, the CE3 model being the most accurate.","Hence, the CE3 model is the most accurate, reliable, and robust for the simulation of mesoscale atmospheric flows when using a pressure-based approach and space discretization by a finite volume method."],"url":"http://arxiv.org/abs/2402.18136v1","category":"physics.flu-dyn"}
{"created":"2024-02-28 07:56:28","title":"Learning to Deblur Polarized Images","abstract":"A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.","sentences":["A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images.","However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains.","In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively.","Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal."],"url":"http://arxiv.org/abs/2402.18134v1","category":"cs.CV"}
{"created":"2024-02-28 07:36:16","title":"Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions","abstract":"Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions. To address this issue, this paper introduces a hierarchical multi-relational graph representation learning (HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous graphs, where nodes represent drugs and edges denote clear and various associations. The relational graph convolutional network (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous graphs. Additionally, a multi-view differentiable spectral clustering (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs. Within the MVDSC, we utilize multiple DP features to construct graphs, where nodes represent DPs and edges denote different implicit correlations. Subsequently, multiple DP representations are generated through graph cutting, each emphasizing distinct implicit correlations. The graph-cutting strategy enables our HMGRL to identify strongly connected communities of graphs, thereby reducing the fusion of irrelevant features. By combining every representation view of a DP, we create high-level DP representations for predicting DDIs. Two genuine datasets spanning three distinct tasks are adopted to gauge the efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL surpasses several leading-edge methods in performance.","sentences":["Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions.","To address this issue, this paper introduces a hierarchical multi-relational graph representation learning (HMGRL) approach.","Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous graphs, where nodes represent drugs and edges denote clear and various associations.","The relational graph convolutional network (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous graphs.","Additionally, a multi-view differentiable spectral clustering (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs.","Within the MVDSC, we utilize multiple DP features to construct graphs, where nodes represent DPs and edges denote different implicit correlations.","Subsequently, multiple DP representations are generated through graph cutting, each emphasizing distinct implicit correlations.","The graph-cutting strategy enables our HMGRL to identify strongly connected communities of graphs, thereby reducing the fusion of irrelevant features.","By combining every representation view of a DP, we create high-level DP representations for predicting DDIs.","Two genuine datasets spanning three distinct tasks are adopted to gauge the efficacy of our HMGRL.","Experimental outcomes unequivocally indicate that HMGRL surpasses several leading-edge methods in performance."],"url":"http://arxiv.org/abs/2402.18127v1","category":"cs.LG"}
{"created":"2024-02-28 07:23:17","title":"G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment","abstract":"Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G's success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods.","sentences":["Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field.","The shortcomings of published studies continue to confuse many researchers.","This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment.","G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes.","The key to G4G's success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples.","Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio.","A fusion network is then used to further fuse the facial region and the rest.","Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips.","G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.18122v1","category":"cs.CV"}
{"created":"2024-02-28 06:54:35","title":"Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction","abstract":"In line with the latest research, the task of identifying helpful reviews from a vast pool of user-generated textual and visual data has become a prominent area of study. Effective modal representations are expected to possess two key attributes: consistency and differentiation. Current methods designed for Multimodal Review Helpfulness Prediction (MRHP) face limitations in capturing distinctive information due to their reliance on uniform multimodal annotation. The process of adding varied multimodal annotations is not only time-consuming but also labor-intensive. To tackle these challenges, we propose an auto-generated scheme based on multi-task learning to generate pseudo labels. This approach allows us to simultaneously train for the global multimodal interaction task and the separate cross-modal interaction subtasks, enabling us to learn and leverage both consistency and differentiation effectively. Subsequently, experimental results validate the effectiveness of pseudo labels, and our approach surpasses previous textual and multimodal baseline models on two widely accessible benchmark datasets, providing a solution to the MRHP problem.","sentences":["In line with the latest research, the task of identifying helpful reviews from a vast pool of user-generated textual and visual data has become a prominent area of study.","Effective modal representations are expected to possess two key attributes: consistency and differentiation.","Current methods designed for Multimodal Review Helpfulness Prediction (MRHP) face limitations in capturing distinctive information due to their reliance on uniform multimodal annotation.","The process of adding varied multimodal annotations is not only time-consuming but also labor-intensive.","To tackle these challenges, we propose an auto-generated scheme based on multi-task learning to generate pseudo labels.","This approach allows us to simultaneously train for the global multimodal interaction task and the separate cross-modal interaction subtasks, enabling us to learn and leverage both consistency and differentiation effectively.","Subsequently, experimental results validate the effectiveness of pseudo labels, and our approach surpasses previous textual and multimodal baseline models on two widely accessible benchmark datasets, providing a solution to the MRHP problem."],"url":"http://arxiv.org/abs/2402.18107v1","category":"cs.MM"}
{"created":"2024-02-28 06:21:36","title":"PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators","abstract":"Processing-in-memory (PIM) has shown extraordinary potential in accelerating neural networks. To evaluate the performance of PIM accelerators, we present an ISA-based simulation framework including a dedicated ISA targeting neural networks running on PIM architectures, a compiler, and a cycleaccurate configurable simulator. Compared with prior works, this work decouples software algorithms and hardware architectures through the proposed ISA, providing a more convenient way to evaluate the effectiveness of software/hardware optimizations. The simulator adopts an event-driven simulation approach and has better support for hardware parallelism. The framework is open-sourced at https://github.com/wangxy-2000/pimsim-nn.","sentences":["Processing-in-memory (PIM) has shown extraordinary potential in accelerating neural networks.","To evaluate the performance of PIM accelerators, we present an ISA-based simulation framework including a dedicated ISA targeting neural networks running on PIM architectures, a compiler, and a cycleaccurate configurable simulator.","Compared with prior works, this work decouples software algorithms and hardware architectures through the proposed ISA, providing a more convenient way to evaluate the effectiveness of software/hardware optimizations.","The simulator adopts an event-driven simulation approach and has better support for hardware parallelism.","The framework is open-sourced at https://github.com/wangxy-2000/pimsim-nn."],"url":"http://arxiv.org/abs/2402.18089v1","category":"cs.AR"}
{"created":"2024-02-28 06:18:33","title":"Generalizable Two-Branch Framework for Image Class-Incremental Learning","abstract":"Deep neural networks often severely forget previously learned knowledge when learning new knowledge. Various continual learning (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements.In this paper, a novel two-branch continual learning framework is proposed to further enhance most existing CL methods. Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight convolutional network. The output of each main branch block is modulated by the output of the corresponding side branch block. Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells.Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods.","sentences":["Deep neural networks often severely forget previously learned knowledge when learning new knowledge.","Various continual learning (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements.","In this paper, a novel two-branch continual learning framework is proposed to further enhance most existing CL methods.","Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight convolutional network.","The output of each main branch block is modulated by the output of the corresponding side branch block.","Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells.","Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.18086v1","category":"cs.CV"}
{"created":"2024-02-28 06:16:10","title":"Gravitational wave memory signatures in identifying transition redshift","abstract":"Many astrophysical and cosmological observations consistently indicate that the universe is currently accelerating. Despite many possible explanations, the exact cause of this acceleration remains unknown. Therefore, additional observational probes are necessary to pinpoint the cause. Gravitational waves (GWs) have the potential to unravel some of the unresolved mysteries in cosmology. In this work, we highlight the potential utility of gravitational wave memory as a tool to identify the cause of this acceleration. We evaluate cosmological memory as a particular case of the master equation for GW memory in Locally Rotationally Symmetric type II spacetimes. Unlike the previous works, the master equation for GW memory contains non-linear dependence of the background quantities. Hence, even though the successive GWs generated are smaller than their predecessors, we demonstrate that their cumulative effect over cosmological time leads to observable signatures, akin to the growth of density perturbations resulting in large-scale structures. Finally, we show that the GW memory exhibits distinct signatures between accelerated and decelerated universes, potentially enabling the identification of the transition redshift from a matter-dominated to a dark-energy-dominated universe.","sentences":["Many astrophysical and cosmological observations consistently indicate that the universe is currently accelerating.","Despite many possible explanations, the exact cause of this acceleration remains unknown.","Therefore, additional observational probes are necessary to pinpoint the cause.","Gravitational waves (GWs) have the potential to unravel some of the unresolved mysteries in cosmology.","In this work, we highlight the potential utility of gravitational wave memory as a tool to identify the cause of this acceleration.","We evaluate cosmological memory as a particular case of the master equation for GW memory in Locally Rotationally Symmetric type II spacetimes.","Unlike the previous works, the master equation for GW memory contains non-linear dependence of the background quantities.","Hence, even though the successive GWs generated are smaller than their predecessors, we demonstrate that their cumulative effect over cosmological time leads to observable signatures, akin to the growth of density perturbations resulting in large-scale structures.","Finally, we show that the GW memory exhibits distinct signatures between accelerated and decelerated universes, potentially enabling the identification of the transition redshift from a matter-dominated to a dark-energy-dominated universe."],"url":"http://arxiv.org/abs/2402.18083v1","category":"gr-qc"}
{"created":"2024-02-28 05:25:57","title":"Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters","abstract":"With the increasing demand for audiovisual services, telecom service providers and application developers are compelled to ensure that their services provide the best possible user experience. Particularly, services such as videoconferencing are very sensitive to network conditions. Therefore, their performance should be monitored in real time in order to adjust parameters to any network perturbation. In this paper, we developed a parametric model for estimating the perceived audiovisual quality in videoconference services. Our model is developed with the nonlinear autoregressive exogenous (NARX) recurrent neural network and estimates the perceived quality in terms of mean opinion score (MOS). We validate our model using the publicly available INRS bitstream audiovisual quality dataset. This dataset contains bitstream parameters such as loss per frame, bit rate and video duration. We compare the proposed model against state-of-the-art methods based on machine learning and show our model to outperform these methods in terms of mean square error (MSE=0.150) and Pearson correlation coefficient (R=0.931)","sentences":["With the increasing demand for audiovisual services, telecom service providers and application developers are compelled to ensure that their services provide the best possible user experience.","Particularly, services such as videoconferencing are very sensitive to network conditions.","Therefore, their performance should be monitored in real time in order to adjust parameters to any network perturbation.","In this paper, we developed a parametric model for estimating the perceived audiovisual quality in videoconference services.","Our model is developed with the nonlinear autoregressive exogenous (NARX) recurrent neural network and estimates the perceived quality in terms of mean opinion score (MOS).","We validate our model using the publicly available INRS bitstream audiovisual quality dataset.","This dataset contains bitstream parameters such as loss per frame, bit rate and video duration.","We compare the proposed model against state-of-the-art methods based on machine learning and show our model to outperform these methods in terms of mean square error (MSE=0.150) and Pearson correlation coefficient (R=0.931)"],"url":"http://arxiv.org/abs/2402.18056v1","category":"eess.IV"}
{"created":"2024-02-28 03:30:31","title":"Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph","abstract":"Computing the core decomposition of a graph is a fundamental problem that has recently been studied in the differentially private setting, motivated by practical applications in data mining. In particular, Dhulipala et al. [FOCS 2022] gave the first mechanism for approximate core decomposition in the challenging and practically relevant setting of local differential privacy. One of the main open problems left by their work is whether the accuracy, i.e., the approximation ratio and additive error, of their mechanism can be improved. We show the first lower bounds on the additive error of approximate and exact core decomposition mechanisms in the centralized and local model of differential privacy, respectively. We also give mechanisms for exact and approximate core decomposition in the local model, with almost matching additive error bounds. Our mechanisms are based on a black-box application of continual counting. They also yield improved mechanisms for the approximate densest subgraph problem in the local model.","sentences":["Computing the core decomposition of a graph is a fundamental problem that has recently been studied in the differentially private setting, motivated by practical applications in data mining.","In particular, Dhulipala et al.","[FOCS 2022] gave the first mechanism for approximate core decomposition in the challenging and practically relevant setting of local differential privacy.","One of the main open problems left by their work is whether the accuracy, i.e., the approximation ratio and additive error, of their mechanism can be improved.","We show the first lower bounds on the additive error of approximate and exact core decomposition mechanisms in the centralized and local model of differential privacy, respectively.","We also give mechanisms for exact and approximate core decomposition in the local model, with almost matching additive error bounds.","Our mechanisms are based on a black-box application of continual counting.","They also yield improved mechanisms for the approximate densest subgraph problem in the local model."],"url":"http://arxiv.org/abs/2402.18020v1","category":"cs.DS"}
{"created":"2024-02-28 03:07:05","title":"Representing 3D sparse map points and lines for camera relocalization","abstract":"Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/","sentences":["Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features.","However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks.","In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings.","Specifically, we utilize a single transformer block to encode line features, effectively transforming them into distinctive point-like descriptors.","Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets.","Through the integration of self- and cross-attention within several graph layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs.","In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations.","Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies.","The source code and demo videos of this work are publicly available at: https://thpjp.github.io/pl2map/"],"url":"http://arxiv.org/abs/2402.18011v1","category":"cs.CV"}
{"created":"2024-02-28 01:26:08","title":"The Reflection Coefficient of a Reflectionless Kink","abstract":"Classically, reflectionless kinks transmit all incident radiation. Recently, we have used an analyticity argument together with a solution of the Lippmann-Schwinger equation to write down the leading quantum correction to the reflection probability. The argument was fast but rather indirect. In the present paper, we calculate the reflection coefficient and probability by methodically grinding through the Schrodinger picture time evolution. We find the same answer. This answer contains contributions not considered in the traditional calculation of meson-kink scattering in 1991. However, as a result of these contributions, our total result is zero in the case of the Sine-Gordon model, and so is consistent with integrability.","sentences":["Classically, reflectionless kinks transmit all incident radiation.","Recently, we have used an analyticity argument together with a solution of the Lippmann-Schwinger equation to write down the leading quantum correction to the reflection probability.","The argument was fast but rather indirect.","In the present paper, we calculate the reflection coefficient and probability by methodically grinding through the Schrodinger picture time evolution.","We find the same answer.","This answer contains contributions not considered in the traditional calculation of meson-kink scattering in 1991.","However, as a result of these contributions, our total result is zero in the case of the Sine-Gordon model, and so is consistent with integrability."],"url":"http://arxiv.org/abs/2402.17968v1","category":"hep-th"}
{"created":"2024-02-28 01:15:30","title":"Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting","abstract":"Operational weather forecasting system relies on computationally expensive physics-based models. Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while only being trained at lower resolution data.","sentences":["Operational weather forecasting system relies on computationally expensive physics-based models.","Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system.","We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting.","Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism.","The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics.","We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models.","Conformer outperforms some of the existing data-driven models at all lead times while only being trained at lower resolution data."],"url":"http://arxiv.org/abs/2402.17966v1","category":"cs.LG"}
{"created":"2024-02-28 00:18:36","title":"Nucleons and vector mesons in a confining holographic QCD model","abstract":"We present a simple holographic QCD model that provides a unified description of vector mesons and nucleons in a confining background based on Einstein-dilaton gravity. For the confining background we consider analytical solutions of the Einstein-dilaton equations where the dilaton is a quadratic function of the radial coordinate far from the boundary. We build actions for the 5d gauge field and the 5d Dirac field dual to the 4d flavor current and the 4d nucleon interpolator respectively. In order to obtain asymptotically linear Regge trajectories we impose for each sector the condition that the effective Schr\\\"odinger equation has a potential that grows quadratically in the radial coordinate far from the boundary. For the vector mesons we show that this condition is automatically satisfied by a 5d Yang-Mills action minimally coupled to the metric and the dilaton. For the nucleons we find that the mass term of the 5d Dirac action needs to be generalised to include couplings to the metric and the dilaton. Using Sturm-Liouville theory we obtain a spectral decomposition for the hadronic correlators consistent with large $N_c$ QCD. Our setup contains only three parameters: the mass scale associated with confinement, the 5d gauge coupling and the 5d Dirac coupling. The last two are completely fixed by matching the correlators at high energies to perturbative QCD. We calculate masses and decay constants and compare our results against available experimental data. Our model can be thought of as a consistent embedding of soft wall models in Einstein-dilaton gravity.","sentences":["We present a simple holographic QCD model that provides a unified description of vector mesons and nucleons in a confining background based on Einstein-dilaton gravity.","For the confining background we consider analytical solutions of the Einstein-dilaton equations where the dilaton is a quadratic function of the radial coordinate far from the boundary.","We build actions for the 5d gauge field and the 5d Dirac field dual to the 4d flavor current and the 4d nucleon interpolator respectively.","In order to obtain asymptotically linear Regge trajectories we impose for each sector the condition that the effective Schr\\\"odinger equation has a potential that grows quadratically in the radial coordinate far from the boundary.","For the vector mesons we show that this condition is automatically satisfied by a 5d Yang-Mills action minimally coupled to the metric and the dilaton.","For the nucleons we find that the mass term of the 5d Dirac action needs to be generalised to include couplings to the metric and the dilaton.","Using Sturm-Liouville theory we obtain a spectral decomposition for the hadronic correlators consistent with large $N_c$ QCD.","Our setup contains only three parameters: the mass scale associated with confinement, the 5d gauge coupling and the 5d Dirac coupling.","The last two are completely fixed by matching the correlators at high energies to perturbative QCD.","We calculate masses and decay constants and compare our results against available experimental data.","Our model can be thought of as a consistent embedding of soft wall models in Einstein-dilaton gravity."],"url":"http://arxiv.org/abs/2402.17950v1","category":"hep-ph"}
{"created":"2024-02-27 21:49:27","title":"How to Partition a Quantum Observable","abstract":"We present a partition of quantum observables in an open quantum system which is inherited from the division of the underlying Hilbert space or configuration space. It is shown that this partition leads to the definition of an inhomogeneous continuity equation for generic, non-local observables. This formalism is employed to describe the local evolution of the von Neumann entropy of a system of independent quantum particles out of equilibrium. Crucially, we find that all local fluctuations in the entropy are governed by an entropy current operator, implying that the production of entanglement entropy is not measured by this partitioned entropy. For systems linearly perturbed from equilibrium, it is shown that this entropy current is equivalent to a heat current, provided that the system-reservoir coupling is partitioned symmetrically. Finally, we show that any other partition of the coupling leads directly to a divergence of the von Neumann entropy. Thus, we conclude that Hilbert-space partitioning is the only partition of the von Neumann entropy which is consistent with the Laws of Thermodynamics.","sentences":["We present a partition of quantum observables in an open quantum system which is inherited from the division of the underlying Hilbert space or configuration space.","It is shown that this partition leads to the definition of an inhomogeneous continuity equation for generic, non-local observables.","This formalism is employed to describe the local evolution of the von Neumann entropy of a system of independent quantum particles out of equilibrium.","Crucially, we find that all local fluctuations in the entropy are governed by an entropy current operator, implying that the production of entanglement entropy is not measured by this partitioned entropy.","For systems linearly perturbed from equilibrium, it is shown that this entropy current is equivalent to a heat current, provided that the system-reservoir coupling is partitioned symmetrically.","Finally, we show that any other partition of the coupling leads directly to a divergence of the von Neumann entropy.","Thus, we conclude that Hilbert-space partitioning is the only partition of the von Neumann entropy which is consistent with the Laws of Thermodynamics."],"url":"http://arxiv.org/abs/2402.17908v1","category":"quant-ph"}
{"created":"2024-02-27 21:42:18","title":"SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization","abstract":"Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models. Prior work on the subject has developed largely along two orthogonal directions: (1) differentiable pruning for efficiently and accurately scoring the importance of parameters, and (2) combinatorial optimization for efficiently searching over the space of sparse models. We unite the two approaches, both theoretically and empirically, to produce a coherent framework for structured neural network pruning in which differentiable pruning guides combinatorial optimization algorithms to select the most important sparse set of parameters. Theoretically, we show how many existing differentiable pruning techniques can be understood as nonconvex regularization for group sparse optimization, and prove that for a wide class of nonconvex regularizers, the global optimum is unique, group-sparse, and provably yields an approximate solution to a sparse convex optimization problem. The resulting algorithm that we propose, SequentialAttention++, advances the state of the art in large-scale neural network block-wise pruning tasks on the ImageNet and Criteo datasets.","sentences":["Neural network pruning is a key technique towards engineering large yet scalable, interpretable, and generalizable models.","Prior work on the subject has developed largely along two orthogonal directions: (1) differentiable pruning for efficiently and accurately scoring the importance of parameters, and (2) combinatorial optimization for efficiently searching over the space of sparse models.","We unite the two approaches, both theoretically and empirically, to produce a coherent framework for structured neural network pruning in which differentiable pruning guides combinatorial optimization algorithms to select the most important sparse set of parameters.","Theoretically, we show how many existing differentiable pruning techniques can be understood as nonconvex regularization for group sparse optimization, and prove that for a wide class of nonconvex regularizers, the global optimum is unique, group-sparse, and provably yields an approximate solution to a sparse convex optimization problem.","The resulting algorithm that we propose, SequentialAttention++, advances the state of the art in large-scale neural network block-wise pruning tasks on the ImageNet and Criteo datasets."],"url":"http://arxiv.org/abs/2402.17902v1","category":"cs.LG"}
{"created":"2024-02-27 21:34:04","title":"Pathwise Relaxed Optimal Control of Rough Differential Equations","abstract":"This note lays part of the theoretical ground for a definition of differential systems modeling reinforcement learning in continuous time non-Markovian rough environments. Specifically we focus on optimal relaxed control of rough equations (the term relaxed referring to the fact that controls have to be considered as measure valued objects). With reinforcement learning in view, our reward functions encompass forms that involve an entropy-type term favoring exploration. In this context, our contribution focuses on a careful definition of the corresponding relaxed Hamilton-Jacobi-Bellman (HJB)-type equation. A substantial part of our endeavor consists in a precise definition of the notion of test function and viscosity solution for the rough relaxed PDE obtained in this framework. Note that this task is often merely sketched in the rough viscosity literature, in spite of the fact that it gives a proper meaning to the differential system at stake. In the last part of the paper we prove that the natural value function in our context solves a relaxed rough HJB equation in the viscosity sense.","sentences":["This note lays part of the theoretical ground for a definition of differential systems modeling reinforcement learning in continuous time non-Markovian rough environments.","Specifically we focus on optimal relaxed control of rough equations (the term relaxed referring to the fact that controls have to be considered as measure valued objects).","With reinforcement learning in view, our reward functions encompass forms that involve an entropy-type term favoring exploration.","In this context, our contribution focuses on a careful definition of the corresponding relaxed Hamilton-Jacobi-Bellman (HJB)-type equation.","A substantial part of our endeavor consists in a precise definition of the notion of test function and viscosity solution for the rough relaxed PDE obtained in this framework.","Note that this task is often merely sketched in the rough viscosity literature, in spite of the fact that it gives a proper meaning to the differential system at stake.","In the last part of the paper we prove that the natural value function in our context solves a relaxed rough HJB equation in the viscosity sense."],"url":"http://arxiv.org/abs/2402.17900v1","category":"math.OC"}
{"created":"2024-02-27 21:27:04","title":"Global Estimation of Range Resolved Thermodynamic Profiles from MicroPulse Differential Absorption Lidar","abstract":"We demonstrate thermodynamic profile estimation with data obtained using the MicroPulse DIAL such that the retrieval is entirely self contained. The only external input is surface meteorological variables obtained from a weather station installed on the instrument. The estimator provides products of temperature, absolute humidity and backscatter ratio such that cross dependencies between the lidar data products and raw observations are accounted for and the final products are self consistent. The method described here is applied to a combined oxygen DIAL, potassium HSRL, water vapor DIAL system operating at two pairs of wavelengths (nominally centered at 770 and 828 nm). We perform regularized maximum likelihood estimation through the Poisson Total Variation technique to suppress noise and improve the range of the observations. A comparison to 119 radiosondes indicates that this new processing method produces improved temperature retrievals, reducing total errors to less than 2 K below 3 km altitude and extending the maximum altitude of temperature retrievals to 5 km with less than 3 K error. The results of this work definitively demonstrates the potential for measuring temperature through the oxygen DIAL technique and furthermore that this can be accomplished with low-power semiconductor-based lidar sensors.","sentences":["We demonstrate thermodynamic profile estimation with data obtained using the MicroPulse DIAL such that the retrieval is entirely self contained.","The only external input is surface meteorological variables obtained from a weather station installed on the instrument.","The estimator provides products of temperature, absolute humidity and backscatter ratio such that cross dependencies between the lidar data products and raw observations are accounted for and the final products are self consistent.","The method described here is applied to a combined oxygen DIAL, potassium HSRL, water vapor DIAL system operating at two pairs of wavelengths (nominally centered at 770 and 828 nm).","We perform regularized maximum likelihood estimation through the Poisson Total Variation technique to suppress noise and improve the range of the observations.","A comparison to 119 radiosondes indicates that this new processing method produces improved temperature retrievals, reducing total errors to less than 2 K below 3 km altitude and extending the maximum altitude of temperature retrievals to 5 km with less than 3 K error.","The results of this work definitively demonstrates the potential for measuring temperature through the oxygen DIAL technique and furthermore that this can be accomplished with low-power semiconductor-based lidar sensors."],"url":"http://arxiv.org/abs/2402.17895v1","category":"eess.SP"}
{"created":"2024-02-27 21:06:42","title":"From Inverse Optimization to Feasibility to ERM","abstract":"Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions, and is widely used in fields such as transportation, power systems and healthcare. We study the contextual inverse optimization setting that utilizes additional contextual information to better predict the unknown problem parameters. We focus on contextual inverse linear programming (CILP), addressing the challenges posed by the non-differentiable nature of LPs. For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections. The resulting algorithm for CILP is equipped with a linear convergence guarantee without additional assumptions such as degeneracy or interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This reduction enables the use of scalable first-order optimization methods to solve large non-convex problems, while maintaining theoretical guarantees in the convex setting. Finally, we experimentally validate our approach on both synthetic and real-world problems, and demonstrate improved performance compared to existing methods.","sentences":["Inverse optimization involves inferring unknown parameters of an optimization problem from known solutions, and is widely used in fields such as transportation, power systems and healthcare.","We study the contextual inverse optimization setting that utilizes additional contextual information to better predict the unknown problem parameters.","We focus on contextual inverse linear programming (CILP), addressing the challenges posed by the non-differentiable nature of LPs.","For a linear prediction model, we reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections.","The resulting algorithm for CILP is equipped with a linear convergence guarantee without additional assumptions such as degeneracy or interpolation.","Next, we reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition.","This reduction enables the use of scalable first-order optimization methods to solve large non-convex problems, while maintaining theoretical guarantees in the convex setting.","Finally, we experimentally validate our approach on both synthetic and real-world problems, and demonstrate improved performance compared to existing methods."],"url":"http://arxiv.org/abs/2402.17890v1","category":"cs.LG"}
{"created":"2024-02-27 19:32:47","title":"Relation between the minimal Lorentz surfaces in $\\mathbb R^4_2$ and $\\mathbb R^3_1$","abstract":"In this paper we give Weierstrass-type representation formulas for the null curves and for the minimal Lorentz surfaces in the Minkowski 3-space $\\mathbb R^3_1$ using real-valued functions. Applying the Weierstrass-type representations for the null curves, we find a correspondence between the null curves in $\\mathbb R^4_2$ and the pairs of null curves in $\\mathbb R^3_1$. Based on this correspondence, we obtain a relation between the minimal Lorentz surfaces in $\\mathbb R^4_2$ and the pairs of minimal Lorentz surfaces in $\\mathbb R^3_1$.","sentences":["In this paper we give Weierstrass-type representation formulas for the null curves and for the minimal Lorentz surfaces in the Minkowski 3-space $\\mathbb R^3_1$ using real-valued functions.","Applying the Weierstrass-type representations for the null curves, we find a correspondence between the null curves in $\\mathbb R^4_2$ and the pairs of null curves in $\\mathbb R^3_1$. Based on this correspondence, we obtain a relation between the minimal Lorentz surfaces in $\\mathbb R^4_2$ and the pairs of minimal Lorentz surfaces in $\\mathbb R^3_1$."],"url":"http://arxiv.org/abs/2402.17850v1","category":"math.DG"}
{"created":"2024-02-27 19:17:11","title":"Efficient simulations of Hartree--Fock equations by an accelerated gradient descent method","abstract":"We develop convergence acceleration procedures that enable a gradient descent-type iteration method to efficiently simulate Hartree--Fock equations for atoms interacting both with each other and with an external potential. Our development focuses on three aspects: (i) optimization of a parameter in the preconditioning operator; (ii) adoption of a technique that eliminates the slowest-decaying mode to the case of many equations (describing many atoms); and (iii) a novel extension of the above technique that allows one to eliminate multiple modes simultaneously. We illustrate performance of the numerical method for the 2D model of the first layer of helium atoms above a graphene sheet. We demonstrate that incorporation of aspects (i) and (ii) above into the ``plain\" gradient descent method accelerates it by at least two orders of magnitude, and often by much more. Aspect (iii) -- a multiple-mode elimination -- may bring further improvement to the convergence rate compared to aspect (ii), the single-mode elimination. Both single- and multiple-mode elimination techniques are shown to significantly outperform the well-known Anderson Acceleration. We believe that our acceleration techniques can also be gainfully employed by other numerical methods, especially those handling hard-core-type interaction potentials.","sentences":["We develop convergence acceleration procedures that enable a gradient descent-type iteration method to efficiently simulate Hartree--Fock equations for atoms interacting both with each other and with an external potential.","Our development focuses on three aspects: (i) optimization of a parameter in the preconditioning operator; (ii) adoption of a technique that eliminates the slowest-decaying mode to the case of many equations (describing many atoms); and (iii) a novel extension of the above technique that allows one to eliminate multiple modes simultaneously.","We illustrate performance of the numerical method for the 2D model of the first layer of helium atoms above a graphene sheet.","We demonstrate that incorporation of aspects (i) and (ii) above into the ``plain\" gradient descent method accelerates it by at least two orders of magnitude, and often by much more.","Aspect (iii) -- a multiple-mode elimination -- may bring further improvement to the convergence rate compared to aspect (ii), the single-mode elimination.","Both single- and multiple-mode elimination techniques are shown to significantly outperform the well-known Anderson Acceleration.","We believe that our acceleration techniques can also be gainfully employed by other numerical methods, especially those handling hard-core-type interaction potentials."],"url":"http://arxiv.org/abs/2402.17843v1","category":"physics.comp-ph"}
{"created":"2024-02-27 17:30:13","title":"Physics-Informed Machine Learning for the Inverse Design of Wave Scattering Clusters","abstract":"Clusters of wave-scattering oscillators offer the ability to passively control wave energy in elastic continua. However, designing such clusters to achieve a desired wave energy pattern is a highly nontrivial task. While the forward scattering problem may be readily analyzed, the inverse problem is very challenging as it is ill-posed, high-dimensional, and known to admit non-unique solutions. Therefore, the inverse design of multiple scattering fields and remote sensing of scattering elements remains a topic of great interest. Motivated by recent advances in physics-informed machine learning, we develop a deep neural network that is capable of predicting the locations of scatterers by evaluating the patterns of a target wavefield. We present a modeling and training formulation to optimize the multi-functional nature of our network in the context of inverse design, remote sensing, and wavefield engineering. Namely, we develop a multi-stage training routine with customized physics-based loss functions to optimize models to detect the locations of scatterers and predict cluster configurations that are physically consistent with the target wavefield. We demonstrate the efficacy of our model as a remote sensing and inverse design tool for three scattering problem types, and we subsequently applicability for designing clusters that direct waves along preferred paths or localize wave energy. Hence, we present an effective model for multiple scattering inverse design which may have diverse applications such as wavefield imaging or passive wave energy control.","sentences":["Clusters of wave-scattering oscillators offer the ability to passively control wave energy in elastic continua.","However, designing such clusters to achieve a desired wave energy pattern is a highly nontrivial task.","While the forward scattering problem may be readily analyzed, the inverse problem is very challenging as it is ill-posed, high-dimensional, and known to admit non-unique solutions.","Therefore, the inverse design of multiple scattering fields and remote sensing of scattering elements remains a topic of great interest.","Motivated by recent advances in physics-informed machine learning, we develop a deep neural network that is capable of predicting the locations of scatterers by evaluating the patterns of a target wavefield.","We present a modeling and training formulation to optimize the multi-functional nature of our network in the context of inverse design, remote sensing, and wavefield engineering.","Namely, we develop a multi-stage training routine with customized physics-based loss functions to optimize models to detect the locations of scatterers and predict cluster configurations that are physically consistent with the target wavefield.","We demonstrate the efficacy of our model as a remote sensing and inverse design tool for three scattering problem types, and we subsequently applicability for designing clusters that direct waves along preferred paths or localize wave energy.","Hence, we present an effective model for multiple scattering inverse design which may have diverse applications such as wavefield imaging or passive wave energy control."],"url":"http://arxiv.org/abs/2402.17816v1","category":"eess.SP"}
{"created":"2024-02-27 14:51:11","title":"DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation","abstract":"Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation. Specifically, utilizing DropBP in QLoRA reduces training time by 44%, increases the convergence speed to the identical loss level by 1.5$\\times$, and enables training with a 6.2$\\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in LLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.","sentences":["Training deep neural networks typically involves substantial computational costs during both forward and backward propagation.","The conventional layer dropping techniques drop certain layers during training for reducing the computations burden.","However, dropping layers during forward propagation adversely affects the training process by degrading accuracy.","In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy.","DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation.","Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process.","DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation.","Specifically, utilizing DropBP in QLoRA reduces training time by 44%, increases the convergence speed to the identical loss level by 1.5$\\times$, and enables training with a 6.2$\\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in LLaMA2-70B.","The code is available at https://github.com/WooSunghyeon/dropbp."],"url":"http://arxiv.org/abs/2402.17812v1","category":"cs.LG"}
{"created":"2024-02-28 17:34:58","title":"Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images","abstract":"This paper presents a multimodal deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images. More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection. An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs. Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection. Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE.","sentences":["This paper presents a multimodal deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images.","More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection.","An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs.","Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection.","Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE."],"url":"http://arxiv.org/abs/2402.18507v1","category":"cs.CV"}
{"created":"2024-02-28 16:00:25","title":"Graph Regularized Encoder Training for Extreme Classification","abstract":"Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers significant performance boosts with zero increase in inference computational costs. RAMEN scales to datasets with up to 1M labels and offers prediction accuracy up to 15% higher on benchmark datasets than state of the art methods, including those that use graph metadata to train GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a proprietary recommendation dataset sourced from click logs of a popular search engine. Code for RAMEN will be released publicly.","sentences":["Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels.","XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small.","Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings.","This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures.","The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN.","Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers significant performance boosts with zero increase in inference computational costs.","RAMEN scales to datasets with up to 1M labels and offers prediction accuracy up to 15% higher on benchmark datasets than state of the art methods, including those that use graph metadata to train GCNs.","RAMEN also offers 10% higher accuracy over the best baseline on a proprietary recommendation dataset sourced from click logs of a popular search engine.","Code for RAMEN will be released publicly."],"url":"http://arxiv.org/abs/2402.18434v1","category":"cs.LG"}
{"created":"2024-02-28 15:19:59","title":"Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning","abstract":"CLIP has the ability to align texts and images and is nearly the most frequently used foundation model in cross-modal zero-shot learning. However, our experimental findings reveal that CLIP suffers from a bias in text-to-image retrieval, resulting in a decrease in CLIP's zero-shot learning performance. We analytically discover that the bias partly arises from the imbalanced range of similarity scores obtained by CLIP. Accordingly, we propose a Balanced Similarity with Auxiliary Prompts (BSAP) to mitigate the text-to-image retrieval bias of CLIP. Specifically, our BSAP designs auxiliary prompts for CLIP to calculate multiple similarity scores for the retrieval images and then normalizes the scores between each image and the given query text as well as our auxiliary prompts to obtain balanced similarity scores. The balanced similarity score of the given query text is used for the final retrieval. In addition, we attempt to adopt a hybrid similarity that combines our BSAP with the original similarity of CLIP to obtain a more robust outcome. Extensive experiments on two typical zero-shot learning tasks,i.e., Referring Expression Comprehension (REC) and Referring Image Segmentation (RIS), are conducted to demonstrate the effectiveness of our BSAP. Specifically, when using the val dataset of RefCOCO in REC, BSAP increases CLIP's performance by 20.6%.","sentences":["CLIP has the ability to align texts and images and is nearly the most frequently used foundation model in cross-modal zero-shot learning.","However, our experimental findings reveal that CLIP suffers from a bias in text-to-image retrieval, resulting in a decrease in CLIP's zero-shot learning performance.","We analytically discover that the bias partly arises from the imbalanced range of similarity scores obtained by CLIP.","Accordingly, we propose a Balanced Similarity with Auxiliary Prompts (BSAP) to mitigate the text-to-image retrieval bias of CLIP.","Specifically, our BSAP designs auxiliary prompts for CLIP to calculate multiple similarity scores for the retrieval images and then normalizes the scores between each image and the given query text as well as our auxiliary prompts to obtain balanced similarity scores.","The balanced similarity score of the given query text is used for the final retrieval.","In addition, we attempt to adopt a hybrid similarity that combines our BSAP with the original similarity of CLIP to obtain a more robust outcome.","Extensive experiments on two typical zero-shot learning tasks,i.e., Referring Expression Comprehension (REC) and Referring Image Segmentation (RIS), are conducted to demonstrate the effectiveness of our BSAP.","Specifically, when using the val dataset of RefCOCO in REC, BSAP increases CLIP's performance by 20.6%."],"url":"http://arxiv.org/abs/2402.18400v1","category":"cs.MM"}
{"created":"2024-02-28 14:26:16","title":"SuperdropNet: a Stable and Accurate Machine Learning Proxy for Droplet-based Cloud Microphysics","abstract":"Cloud microphysics has important consequences for climate and weather phenomena, and inaccurate representations can limit forecast accuracy. While atmospheric models increasingly resolve storms and clouds, the accuracy of the underlying microphysics remains limited by computationally expedient bulk moment schemes based on simplifying assumptions. Droplet-based Lagrangian schemes are more accurate but are underutilized due to their large computational overhead. Machine learning (ML) based schemes can bridge this gap by learning from vast droplet-based simulation datasets, but have so far struggled to match the accuracy and stability of bulk moment schemes. To address this challenge, we developed SuperdropNet, an ML-based emulator of the Lagrangian superdroplet simulations. To improve accuracy and stability, we employ multi-step autoregressive prediction during training, impose physical constraints, and carefully control stochasticity in the training data. Superdropnet predicted hydrometeor states and cloud-to-rain transition times more accurately than previous ML emulators, and matched or outperformed bulk moment schemes in many cases. We further carried out detailed analyses to reveal how multistep autoregressive training improves performance, and how the performance of SuperdropNet and other microphysical schemes hydrometeors' mass, number and size distribution. Together our results suggest that ML models can effectively emulate cloud microphysics, in a manner consistent with droplet-based simulations.","sentences":["Cloud microphysics has important consequences for climate and weather phenomena, and inaccurate representations can limit forecast accuracy.","While atmospheric models increasingly resolve storms and clouds, the accuracy of the underlying microphysics remains limited by computationally expedient bulk moment schemes based on simplifying assumptions.","Droplet-based Lagrangian schemes are more accurate but are underutilized due to their large computational overhead.","Machine learning (ML) based schemes can bridge this gap by learning from vast droplet-based simulation datasets, but have so far struggled to match the accuracy and stability of bulk moment schemes.","To address this challenge, we developed SuperdropNet, an ML-based emulator of the Lagrangian superdroplet simulations.","To improve accuracy and stability, we employ multi-step autoregressive prediction during training, impose physical constraints, and carefully control stochasticity in the training data.","Superdropnet predicted hydrometeor states and cloud-to-rain transition times more accurately than previous ML emulators, and matched or outperformed bulk moment schemes in many cases.","We further carried out detailed analyses to reveal how multistep autoregressive training improves performance, and how the performance of SuperdropNet and other microphysical schemes hydrometeors' mass, number and size distribution.","Together our results suggest that ML models can effectively emulate cloud microphysics, in a manner consistent with droplet-based simulations."],"url":"http://arxiv.org/abs/2402.18354v1","category":"physics.ao-ph"}
{"created":"2024-02-28 13:15:55","title":"Levelling Up Learning: Exploring the Impact of Gamification in Flipped Classrooms","abstract":"In recent years, the integration of gamification into educational settings has garnered significant attention as a means to enhance student engagement and learning outcomes. By leveraging gamified elements such as points and leaderboards, educators aim to promote active participation, motivation, and deeper understanding among students. This study investigates the effects of gamification on student engagement in a flipped classroom environment. The findings suggest that gamification strategies, when effectively implemented, can have a positive impact on student motivation and engagement. This paper concludes with recommendations for educators, potential challenges such as superficial engagement and demotivation, and future directions for research to address these challenges and further explore the potential of gamification in fostering student success.","sentences":["In recent years, the integration of gamification into educational settings has garnered significant attention as a means to enhance student engagement and learning outcomes.","By leveraging gamified elements such as points and leaderboards, educators aim to promote active participation, motivation, and deeper understanding among students.","This study investigates the effects of gamification on student engagement in a flipped classroom environment.","The findings suggest that gamification strategies, when effectively implemented, can have a positive impact on student motivation and engagement.","This paper concludes with recommendations for educators, potential challenges such as superficial engagement and demotivation, and future directions for research to address these challenges and further explore the potential of gamification in fostering student success."],"url":"http://arxiv.org/abs/2402.18313v1","category":"stat.OT"}
{"created":"2024-02-28 11:16:00","title":"Learning or Self-aligning? Rethinking Instruction Fine-tuning","abstract":"Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.","sentences":["Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs).","Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge.","However, the understanding of the underlying mechanisms of IFT remains significantly limited.","In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors.","Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects.","Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT.","Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works."],"url":"http://arxiv.org/abs/2402.18243v1","category":"cs.CL"}
{"created":"2024-02-28 11:12:53","title":"A network-constrain Weibull AFT model for biomarkers discovery","abstract":"We propose AFTNet, a novel network-constraint survival analysis method based on the Weibull accelerated failure time (AFT) model solved by a penalized likelihood approach for variable selection and estimation. When using the log-linear representation, the inference problem becomes a structured sparse regression problem for which we explicitly incorporate the correlation patterns among predictors using a double penalty that promotes both sparsity and grouping effect. Moreover, we establish the theoretical consistency for the AFTNet estimator and present an efficient iterative computational algorithm based on the proximal gradient descent method. Finally, we evaluate AFTNet performance both on synthetic and real data examples.","sentences":["We propose AFTNet, a novel network-constraint survival analysis method based on the Weibull accelerated failure time (AFT) model solved by a penalized likelihood approach for variable selection and estimation.","When using the log-linear representation, the inference problem becomes a structured sparse regression problem for which we explicitly incorporate the correlation patterns among predictors using a double penalty that promotes both sparsity and grouping effect.","Moreover, we establish the theoretical consistency for the AFTNet estimator and present an efficient iterative computational algorithm based on the proximal gradient descent method.","Finally, we evaluate AFTNet performance both on synthetic and real data examples."],"url":"http://arxiv.org/abs/2402.18242v1","category":"stat.ML"}
{"created":"2024-02-28 10:01:44","title":"Catastrophic Overfitting: A Potential Blessing in Disguise","abstract":"Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness. Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field. Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples. To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO. Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways. By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation. Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work. On this basis, we harness CO to achieve `attack obfuscation', aiming to bolster model performance. Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and adversarial data when adding random noise to inputs during evaluation. We also validate their robustness against transferred adversarial examples and the necessity of inducing CO to improve robustness. Hence, CO may not be a problem that has to be solved.","sentences":["Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness.","Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field.","Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples.","To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO.","Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways.","By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation.","Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work.","On this basis, we harness CO to achieve `attack obfuscation', aiming to bolster model performance.","Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and adversarial data when adding random noise to inputs during evaluation.","We also validate their robustness against transferred adversarial examples and the necessity of inducing CO to improve robustness.","Hence, CO may not be a problem that has to be solved."],"url":"http://arxiv.org/abs/2402.18211v1","category":"cs.LG"}
{"created":"2024-02-28 09:27:41","title":"Misalignment-Robust Frequency Distribution Loss for Image Transformation","abstract":"This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL","sentences":["This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments.","However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data.","To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain.","Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT).","Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function.","Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain.","Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions.","Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data.","Our code is available at: https://github.com/eezkni/FDL"],"url":"http://arxiv.org/abs/2402.18192v1","category":"cs.CV"}
{"created":"2024-02-28 09:01:50","title":"Digging Into Normal Incorporated Stereo Matching","abstract":"Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, disparity estimation in low-texture, occluded, and bordered regions still remains a bottleneck that limits the performance. To tackle these challenges, geometric guidance like plane information is necessary as it provides intuitive guidance about disparity consistency and affinity similarity. In this paper, we propose a normal incorporated joint learning framework consisting of two specific modules named non-local disparity propagation(NDP) and affinity-aware residual learning(ARL). The estimated normal map is first utilized for calculating a non-local affinity matrix and a non-local offset to perform spatial propagation at the disparity level. To enhance geometric consistency, especially in low-texture regions, the estimated normal map is then leveraged to calculate a local affinity matrix, providing the residual learning with information about where the correction should refer and thus improving the residual learning efficiency. Extensive experiments on several public datasets including Scene Flow, KITTI 2015, and Middlebury 2014 validate the effectiveness of our proposed method. By the time we finished this work, our approach ranked 1st for stereo matching across foreground pixels on the KITTI 2015 dataset and 3rd on the Scene Flow dataset among all the published works.","sentences":["Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, disparity estimation in low-texture, occluded, and bordered regions still remains a bottleneck that limits the performance.","To tackle these challenges, geometric guidance like plane information is necessary as it provides intuitive guidance about disparity consistency and affinity similarity.","In this paper, we propose a normal incorporated joint learning framework consisting of two specific modules named non-local disparity propagation(NDP) and affinity-aware residual learning(ARL).","The estimated normal map is first utilized for calculating a non-local affinity matrix and a non-local offset to perform spatial propagation at the disparity level.","To enhance geometric consistency, especially in low-texture regions, the estimated normal map is then leveraged to calculate a local affinity matrix, providing the residual learning with information about where the correction should refer and thus improving the residual learning efficiency.","Extensive experiments on several public datasets including Scene Flow, KITTI 2015, and Middlebury 2014 validate the effectiveness of our proposed method.","By the time we finished this work, our approach ranked 1st for stereo matching across foreground pixels on the KITTI 2015 dataset and 3rd on the Scene Flow dataset among all the published works."],"url":"http://arxiv.org/abs/2402.18171v1","category":"cs.CV"}
{"created":"2024-02-28 07:54:50","title":"Classes Are Not Equal: An Empirical Study on Image Recognition Fairness","abstract":"In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification.","sentences":["In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet.","We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities.","Moreover, several intriguing properties of fairness are identified.","First, the unfairness lies in problematic representation rather than classifier bias.","Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization.","Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize.","It means that more other classes will be confused with harder classes.","Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy.","Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification."],"url":"http://arxiv.org/abs/2402.18133v1","category":"cs.LG"}
{"created":"2024-02-28 07:10:37","title":"PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation","abstract":"Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.","sentences":["Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning.","However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process.","To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process.","We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning.","Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process.","Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations.","In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process.","Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework."],"url":"http://arxiv.org/abs/2402.18117v1","category":"cs.CV"}
{"created":"2024-02-28 05:43:22","title":"Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models","abstract":"Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark .","sentences":["Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts.","Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans.","Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other.","To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios.","By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity.","Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence.","Our code is available at https://github.com/mignonjia/TS_watermark ."],"url":"http://arxiv.org/abs/2402.18059v1","category":"cs.LG"}
{"created":"2024-02-28 04:47:32","title":"Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection","abstract":"We present a novel data augmentation method to address the challenge of data scarcity in modeling longitudinal patterns in Electronic Health Records (EHR) of patients using natural language processing (NLP) algorithms. The proposed method generates augmented data by rearranging the orders of medical records within a visit where the order of elements are not obvious, if any. Applying the proposed method to the clopidogrel treatment failure detection task enabled up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without augmentation to 0.961 with augmentation) when it was used during the pre-training procedure. It was also shown that the augmentation helped to improve performance during fine-tuning procedures, especially when the amount of labeled training data is limited.","sentences":["We present a novel data augmentation method to address the challenge of data scarcity in modeling longitudinal patterns in Electronic Health Records (EHR) of patients using natural language processing (NLP) algorithms.","The proposed method generates augmented data by rearranging the orders of medical records within a visit where the order of elements are not obvious, if any.","Applying the proposed method to the clopidogrel treatment failure detection task enabled up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without augmentation to 0.961 with augmentation) when it was used during the pre-training procedure.","It was also shown that the augmentation helped to improve performance during fine-tuning procedures, especially when the amount of labeled training data is limited."],"url":"http://arxiv.org/abs/2402.18046v1","category":"cs.LG"}
{"created":"2024-02-28 03:44:01","title":"Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions","abstract":"How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LINGOLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM's prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LINGOLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LINGOLLM elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages. Our data, code, and model generations can be found at https://github.com/LLiLab/llm4endangeredlang.","sentences":["How can large language models (LLMs) process and translate endangered languages?","Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages.","On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary.","We propose LINGOLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training.","Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM's prompt, including a dictionary, a grammar book, and morphologically analyzed input text.","We implement LINGOLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages.","Our results show that LINGOLLM elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language directions.","Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages.","Our data, code, and model generations can be found at https://github.com/LLiLab/llm4endangeredlang."],"url":"http://arxiv.org/abs/2402.18025v1","category":"cs.CL"}
{"created":"2024-02-28 02:14:42","title":"Data-driven nonlinear turbulent flow scaling with Buckingham Pi variables","abstract":"Nonlinear machine learning for turbulent flows can exhibit robust performance even outside the range of training data. This is achieved when machine-learning models can accommodate scale-invariant characteristics of turbulent flow structures. This study presents a data-driven approach to reveal scale-invariant vortical structures across Reynolds numbers that provide insights for supporting nonlinear machine-learning-based studies of turbulent flows. To uncover conditions for which nonlinear models are likely to perform well, we use a Buckingham Pi-based sparse nonlinear scaling to find the influence of the Pi groups on the turbulent flow data. We consider nonlinear scalings of the invariants of the velocity gradient tensor for an example of three-dimensional decaying isotropic turbulence. The present scaling not only enables the identification of vortical structures that are interpolatory and extrapolatory for the given flow field data but also captures non-equilibrium effects of the energy cascade. As a demonstration, the present findings are applied to machine-learning-based super-resolution analysis of three-dimensional isotropic turbulence. We show that machine-learning models reconstruct vortical structures well in the interpolatory space with reduced performance in the extrapolatory space revealed by the nonlinearly scaled invariants. The present approach enables us to depart from labeling turbulent flow data with a single parameter of Reynolds number and comprehensively examine the flow field to support training and testing of nonlinear machine-learning techniques.","sentences":["Nonlinear machine learning for turbulent flows can exhibit robust performance even outside the range of training data.","This is achieved when machine-learning models can accommodate scale-invariant characteristics of turbulent flow structures.","This study presents a data-driven approach to reveal scale-invariant vortical structures across Reynolds numbers that provide insights for supporting nonlinear machine-learning-based studies of turbulent flows.","To uncover conditions for which nonlinear models are likely to perform well, we use a Buckingham Pi-based sparse nonlinear scaling to find the influence of the Pi groups on the turbulent flow data.","We consider nonlinear scalings of the invariants of the velocity gradient tensor for an example of three-dimensional decaying isotropic turbulence.","The present scaling not only enables the identification of vortical structures that are interpolatory and extrapolatory for the given flow field data but also captures non-equilibrium effects of the energy cascade.","As a demonstration, the present findings are applied to machine-learning-based super-resolution analysis of three-dimensional isotropic turbulence.","We show that machine-learning models reconstruct vortical structures well in the interpolatory space with reduced performance in the extrapolatory space revealed by the nonlinearly scaled invariants.","The present approach enables us to depart from labeling turbulent flow data with a single parameter of Reynolds number and comprehensively examine the flow field to support training and testing of nonlinear machine-learning techniques."],"url":"http://arxiv.org/abs/2402.17990v1","category":"physics.flu-dyn"}
{"created":"2024-02-28 01:19:42","title":"Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning","abstract":"Network systems form the foundation of modern society, playing a critical role in various applications. However, these systems are at significant risk of being adversely affected by unforeseen circumstances, such as disasters. Considering this, there is a pressing need for research to enhance the robustness of network systems. Recently, in reinforcement learning, the relationship between acquiring robustness and regularizing entropy has been identified. Additionally, imitation learning is used within this framework to reflect experts' behavior. However, there are no comprehensive studies on the use of a similar imitation framework for optimal transport on networks. Therefore, in this study, imitation-regularized optimal transport (I-OT) on networks was investigated. It encodes prior knowledge on the network by imitating a given prior distribution. The I-OT solution demonstrated robustness in terms of the cost defined on the network. Moreover, we applied the I-OT to a logistics planning problem using real data. We also examined the imitation and apriori risk information scenarios to demonstrate the usefulness and implications of the proposed method.","sentences":["Network systems form the foundation of modern society, playing a critical role in various applications.","However, these systems are at significant risk of being adversely affected by unforeseen circumstances, such as disasters.","Considering this, there is a pressing need for research to enhance the robustness of network systems.","Recently, in reinforcement learning, the relationship between acquiring robustness and regularizing entropy has been identified.","Additionally, imitation learning is used within this framework to reflect experts' behavior.","However, there are no comprehensive studies on the use of a similar imitation framework for optimal transport on networks.","Therefore, in this study, imitation-regularized optimal transport (I-OT) on networks was investigated.","It encodes prior knowledge on the network by imitating a given prior distribution.","The I-OT solution demonstrated robustness in terms of the cost defined on the network.","Moreover, we applied the I-OT to a logistics planning problem using real data.","We also examined the imitation and apriori risk information scenarios to demonstrate the usefulness and implications of the proposed method."],"url":"http://arxiv.org/abs/2402.17967v1","category":"cs.LG"}
{"created":"2024-02-27 23:52:58","title":"Sequential transport maps using SoS density estimation and $\u03b1$-divergences","abstract":"Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\\alpha$-divergences is to enable working with unnormalized densities, which provides benefits both numerically and theoretically. In particular, we provide two new convergence analyses of the sequential transport maps: one based on a triangle-like inequality and the second on information geometric properties of $\\alpha$-divergences for unnormalizied densities. The choice of intermediate densities is also crucial for the efficiency of the method. While tempered (or annealed) densities are the state-of-the-art, we introduce diffusion-based intermediate densities which permits to approximate densities known from samples only. Such intermediate densities are well-established in machine learning for generative modeling. Finally we propose and try different low-dimensional maps (or lazy maps) for dealing with high-dimensional problems and numerically demonstrate our methods on several benchmarks, including Bayesian inference problems and unsupervised learning task.","sentences":["Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density.","We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps.","Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density.","In our work, we explore the use of Sum-of-Squares (SoS) densities and $\\alpha$-divergences for approximating the intermediate densities.","Combining SoS densities with $\\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming.","The main advantage of $\\alpha$-divergences is to enable working with unnormalized densities, which provides benefits both numerically and theoretically.","In particular, we provide two new convergence analyses of the sequential transport maps: one based on a triangle-like inequality and the second on information geometric properties of $\\alpha$-divergences for unnormalizied densities.","The choice of intermediate densities is also crucial for the efficiency of the method.","While tempered (or annealed) densities are the state-of-the-art, we introduce diffusion-based intermediate densities which permits to approximate densities known from samples only.","Such intermediate densities are well-established in machine learning for generative modeling.","Finally we propose and try different low-dimensional maps (or lazy maps) for dealing with high-dimensional problems and numerically demonstrate our methods on several benchmarks, including Bayesian inference problems and unsupervised learning task."],"url":"http://arxiv.org/abs/2402.17943v1","category":"stat.ML"}
{"created":"2024-02-27 23:37:11","title":"Fast buffet onset prediction and optimization method based on a pre-trained flowfield prediction model","abstract":"The transonic buffet is a detrimental phenomenon occurs on supercritical airfoils and limits aircraft's operating envelope. Traditional methods for predicting buffet onset rely on multiple computational fluid dynamics simulations to assess a series of airfoil flowfields and then apply criteria to them, which is slow and hinders optimization efforts. This article introduces an innovative approach for rapid buffet onset prediction. A machine-learning flowfield prediction model is pre-trained on a large database and then deployed offline to replace simulations in the buffet prediction process for new airfoil designs. Unlike using a model to directly predict buffet onset, the proposed technique offers better visualization capabilities by providing users with intuitive flowfield outputs. It also demonstrates superior generalization ability, evidenced by a 32.5% reduction in average buffet onset prediction error on the testing dataset. The method is utilized to optimize the buffet performance of 11 distinct airfoils within and outside the training dataset. The optimization results are verified with simulations and proved to yield improved samples across all cases. It is affirmed the pre-trained flowfield prediction model can be applied to accelerate aerodynamic shape optimization, while further work still needs to raise its reliability for this safety-critical task.","sentences":["The transonic buffet is a detrimental phenomenon occurs on supercritical airfoils and limits aircraft's operating envelope.","Traditional methods for predicting buffet onset rely on multiple computational fluid dynamics simulations to assess a series of airfoil flowfields and then apply criteria to them, which is slow and hinders optimization efforts.","This article introduces an innovative approach for rapid buffet onset prediction.","A machine-learning flowfield prediction model is pre-trained on a large database and then deployed offline to replace simulations in the buffet prediction process for new airfoil designs.","Unlike using a model to directly predict buffet onset, the proposed technique offers better visualization capabilities by providing users with intuitive flowfield outputs.","It also demonstrates superior generalization ability, evidenced by a 32.5% reduction in average buffet onset prediction error on the testing dataset.","The method is utilized to optimize the buffet performance of 11 distinct airfoils within and outside the training dataset.","The optimization results are verified with simulations and proved to yield improved samples across all cases.","It is affirmed the pre-trained flowfield prediction model can be applied to accelerate aerodynamic shape optimization, while further work still needs to raise its reliability for this safety-critical task."],"url":"http://arxiv.org/abs/2402.17939v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 23:29:10","title":"Acquiring Linguistic Knowledge from Multimodal Input","abstract":"In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language. In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of multimodal input and grounding in the learning environment of typical language models. Although previous work looking into this question found that multimodal training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to fine-tuning on captions data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a multimodal vision-and-language model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales. We aim to limit catastrophic forgetting through a multitask pretraining regime that includes unimodal text-only tasks and data sampled from WiT, the relatively diverse Wikipedia-based dataset (Srinivasan et al., 2021). Our results are largely negative: Multimodal pretraining does not harm our models' language performance but does not consistently help either. That said, our conclusions are limited by our having been able to conduct only a small number of runs. While we must leave open the possibility that multimodal input explains some of the gap in data efficiency between LMs and humans, positive evidence for this hypothesis will require better architectures and techniques for multimodal training.","sentences":["In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language.","In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of multimodal input and grounding in the learning environment of typical language models.","Although previous work looking into this question found that multimodal training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to fine-tuning on captions data.","To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a multimodal vision-and-language model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales.","We aim to limit catastrophic forgetting through a multitask pretraining regime that includes unimodal text-only tasks and data sampled from WiT, the relatively diverse Wikipedia-based dataset (Srinivasan et al., 2021).","Our results are largely negative: Multimodal pretraining does not harm our models' language performance but does not consistently help either.","That said, our conclusions are limited by our having been able to conduct only a small number of runs.","While we must leave open the possibility that multimodal input explains some of the gap in data efficiency between LMs and humans, positive evidence for this hypothesis will require better architectures and techniques for multimodal training."],"url":"http://arxiv.org/abs/2402.17936v1","category":"cs.CL"}
{"created":"2024-02-27 22:49:33","title":"Certain and Approximately Certain Models for Statistical Learning","abstract":"Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.","sentences":["Real-world data is often incomplete and contains missing values.","To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items.","In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models.","We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms.","We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary.","Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead."],"url":"http://arxiv.org/abs/2402.17926v1","category":"stat.ML"}
{"created":"2024-02-27 22:34:14","title":"A linear photonic swap test circuit for quantum kernel estimation","abstract":"Among supervised learning models, Support Vector Machine stands out as one of the most robust and efficient models for classifying data clusters. At the core of this method, a kernel function is employed to calculate the distance between different elements of the dataset, allowing for their classification. Since every kernel function can be expressed as a scalar product, we can estimate it using Quantum Mechanics, where probability amplitudes and scalar products are fundamental objects. The swap test, indeed, is a quantum algorithm capable of computing the scalar product of two arbitrary wavefunctions, potentially enabling a quantum speed-up. Here, we present an integrated photonic circuit designed to implement the swap test algorithm. Our approach relies solely on linear optical integrated components and qudits, represented by single photons from an attenuated laser beam propagating through a set of waveguides. By utilizing 2$^3$ spatial degrees of freedom for the qudits, we can configure all the necessary arrangements to set any two-qubits state and perform the swap test. This simplifies the requirements on the circuitry elements and eliminates the need for non-linearity, heralding, or post-selection to achieve multi-qubits gates. Our photonic swap test circuit successfully encodes two qubits and estimates their scalar product with a measured root mean square error smaller than 0.05. This result paves the way for the development of integrated photonic architectures capable of performing Quantum Machine Learning tasks with robust devices operating at room temperature.","sentences":["Among supervised learning models, Support Vector Machine stands out as one of the most robust and efficient models for classifying data clusters.","At the core of this method, a kernel function is employed to calculate the distance between different elements of the dataset, allowing for their classification.","Since every kernel function can be expressed as a scalar product, we can estimate it using Quantum Mechanics, where probability amplitudes and scalar products are fundamental objects.","The swap test, indeed, is a quantum algorithm capable of computing the scalar product of two arbitrary wavefunctions, potentially enabling a quantum speed-up.","Here, we present an integrated photonic circuit designed to implement the swap test algorithm.","Our approach relies solely on linear optical integrated components and qudits, represented by single photons from an attenuated laser beam propagating through a set of waveguides.","By utilizing 2$^3$ spatial degrees of freedom for the qudits, we can configure all the necessary arrangements to set any two-qubits state and perform the swap test.","This simplifies the requirements on the circuitry elements and eliminates the need for non-linearity, heralding, or post-selection to achieve multi-qubits gates.","Our photonic swap test circuit successfully encodes two qubits and estimates their scalar product with a measured root mean square error smaller than 0.05.","This result paves the way for the development of integrated photonic architectures capable of performing Quantum Machine Learning tasks with robust devices operating at room temperature."],"url":"http://arxiv.org/abs/2402.17923v1","category":"quant-ph"}
{"created":"2024-02-27 22:14:01","title":"The Seeker's Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection","abstract":"This work focuses on advancing security research in the hardware design space by formally defining the realistic problem of Hardware Trojan (HT) detection. The goal is to model HT detection more closely to the real world, i.e., describing the problem as \"The Seeker's Dilemma\" (an extension of Hide&Seek on a graph), where a detecting agent is unaware of whether circuits are infected by HTs or not. Using this theoretical problem formulation, we create a benchmark that consists of a mixture of HT-free and HT-infected restructured circuits while preserving their original functionalities. The restructured circuits are randomly infected by HTs, causing a situation where the defender is uncertain if a circuit is infected or not. We believe that our innovative dataset will help the community better judge the detection quality of different methods by comparing their success rates in circuit classification. We use our developed benchmark to evaluate three state-of-the-art HT detection tools to show baseline results for this approach. We use Principal Component Analysis to assess the strength of our benchmark, where we observe that some restructured HT-infected circuits are mapped closely to HT-free circuits, leading to significant label misclassification by detectors.","sentences":["This work focuses on advancing security research in the hardware design space by formally defining the realistic problem of Hardware Trojan (HT) detection.","The goal is to model HT detection more closely to the real world, i.e., describing the problem as \"The Seeker's Dilemma\" (an extension of Hide&Seek on a graph), where a detecting agent is unaware of whether circuits are infected by HTs or not.","Using this theoretical problem formulation, we create a benchmark that consists of a mixture of HT-free and HT-infected restructured circuits while preserving their original functionalities.","The restructured circuits are randomly infected by HTs, causing a situation where the defender is uncertain if a circuit is infected or not.","We believe that our innovative dataset will help the community better judge the detection quality of different methods by comparing their success rates in circuit classification.","We use our developed benchmark to evaluate three state-of-the-art HT detection tools to show baseline results for this approach.","We use Principal Component Analysis to assess the strength of our benchmark, where we observe that some restructured HT-infected circuits are mapped closely to HT-free circuits, leading to significant label misclassification by detectors."],"url":"http://arxiv.org/abs/2402.17918v1","category":"cs.CR"}
{"created":"2024-02-27 21:53:32","title":"Demonstration of Robust and Efficient Quantum Property Learning with Shallow Shadows","abstract":"Extracting information efficiently from quantum systems is a major component of quantum information processing tasks. Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements. While random single qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables. Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity. However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results. To address these challenges, we propose the robust shallow shadows protocol. Our protocol uses Bayesian inference to learn the experimentally relevant noise model and mitigate it in postprocessing. This mitigation introduces a bias-variance trade-off: correcting for noise-induced bias comes at the cost of a larger estimator variance. Despite this increased variance, as we demonstrate on a superconducting quantum processor, our protocol correctly recovers state properties such as expectation values, fidelity, and entanglement entropy, while maintaining a lower sample complexity compared to the random single qubit measurement scheme. We also theoretically analyze the effects of noise on sample complexity and show how the optimal choice of the shallow shadow depth varies with noise strength. This combined theoretical and experimental analysis positions the robust shallow shadow protocol as a scalable, robust, and sample-efficient protocol for characterizing quantum states on current quantum computing platforms.","sentences":["Extracting information efficiently from quantum systems is a major component of quantum information processing tasks.","Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements.","While random single qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables.","Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity.","However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results.","To address these challenges, we propose the robust shallow shadows protocol.","Our protocol uses Bayesian inference to learn the experimentally relevant noise model and mitigate it in postprocessing.","This mitigation introduces a bias-variance trade-off: correcting for noise-induced bias comes at the cost of a larger estimator variance.","Despite this increased variance, as we demonstrate on a superconducting quantum processor, our protocol correctly recovers state properties such as expectation values, fidelity, and entanglement entropy, while maintaining a lower sample complexity compared to the random single qubit measurement scheme.","We also theoretically analyze the effects of noise on sample complexity and show how the optimal choice of the shallow shadow depth varies with noise strength.","This combined theoretical and experimental analysis positions the robust shallow shadow protocol as a scalable, robust, and sample-efficient protocol for characterizing quantum states on current quantum computing platforms."],"url":"http://arxiv.org/abs/2402.17911v1","category":"quant-ph"}
{"created":"2024-02-27 21:47:06","title":"Representation learning in multiplex graphs: Where and how to fuse information?","abstract":"In recent years, unsupervised and self-supervised graph representation learning has gained popularity in the research community. However, most proposed methods are focused on homogeneous networks, whereas real-world graphs often contain multiple node and edge types. Multiplex graphs, a special type of heterogeneous graphs, possess richer information, provide better modeling capabilities and integrate more detailed data from potentially different sources. The diverse edge types in multiplex graphs provide more context and insights into the underlying processes of representation learning. In this paper, we tackle the problem of learning representations for nodes in multiplex networks in an unsupervised or self-supervised manner. To that end, we explore diverse information fusion schemes performed at different levels of the graph processing pipeline. The detailed analysis and experimental evaluation of various scenarios inspired us to propose improvements in how to construct GNN architectures that deal with multiplex graphs.","sentences":["In recent years, unsupervised and self-supervised graph representation learning has gained popularity in the research community.","However, most proposed methods are focused on homogeneous networks, whereas real-world graphs often contain multiple node and edge types.","Multiplex graphs, a special type of heterogeneous graphs, possess richer information, provide better modeling capabilities and integrate more detailed data from potentially different sources.","The diverse edge types in multiplex graphs provide more context and insights into the underlying processes of representation learning.","In this paper, we tackle the problem of learning representations for nodes in multiplex networks in an unsupervised or self-supervised manner.","To that end, we explore diverse information fusion schemes performed at different levels of the graph processing pipeline.","The detailed analysis and experimental evaluation of various scenarios inspired us to propose improvements in how to construct GNN architectures that deal with multiplex graphs."],"url":"http://arxiv.org/abs/2402.17906v1","category":"cs.LG"}
{"created":"2024-02-27 21:42:23","title":"Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning","abstract":"Videos are prominent learning materials to prepare surgical trainees before they enter the operating room (OR). In this work, we explore techniques to enrich the video-based surgery learning experience. We propose Surgment, a system that helps expert surgeons create exercises with feedback based on surgery recordings. Surgment is powered by a few-shot-learning-based pipeline (SegGPT+SAM) to segment surgery scenes, achieving an accuracy of 92\\%. The segmentation pipeline enables functionalities to create visual questions and feedback desired by surgeons from a formative study. Surgment enables surgeons to 1) retrieve frames of interest through sketches, and 2) design exercises that target specific anatomical components and offer visual feedback. In an evaluation study with 11 surgeons, participants applauded the search-by-sketch approach for identifying frames of interest and found the resulting image-based questions and feedback to be of high educational value.","sentences":["Videos are prominent learning materials to prepare surgical trainees before they enter the operating room (OR).","In this work, we explore techniques to enrich the video-based surgery learning experience.","We propose Surgment, a system that helps expert surgeons create exercises with feedback based on surgery recordings.","Surgment is powered by a few-shot-learning-based pipeline (SegGPT+SAM) to segment surgery scenes, achieving an accuracy of 92\\%.","The segmentation pipeline enables functionalities to create visual questions and feedback desired by surgeons from a formative study.","Surgment enables surgeons to 1) retrieve frames of interest through sketches, and 2) design exercises that target specific anatomical components and offer visual feedback.","In an evaluation study with 11 surgeons, participants applauded the search-by-sketch approach for identifying frames of interest and found the resulting image-based questions and feedback to be of high educational value."],"url":"http://arxiv.org/abs/2402.17903v1","category":"cs.HC"}
{"created":"2024-02-27 21:08:23","title":"Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation","abstract":"Class activation maps (CAMs) are commonly employed in weakly supervised semantic segmentation (WSSS) to produce pseudo-labels. Due to incomplete or excessive class activation, existing studies often resort to offline CAM refinement, introducing additional stages or proposing offline modules. This can cause optimization difficulties for single-stage methods and limit generalizability. In this study, we aim to reduce the observed CAM inconsistency and error to mitigate reliance on refinement processes. We propose an end-to-end WSSS model incorporating guided CAMs, wherein our segmentation model is trained while concurrently optimizing CAMs online. Our method, Co-training with Swapping Assignments (CoSA), leverages a dual-stream framework, where one sub-network learns from the swapped assignments generated by the other. We introduce three techniques: i) soft perplexity-based regularization to penalize uncertain regions; ii) a threshold-searching approach to dynamically revise the confidence threshold; and iii) contrastive separation to address the coexistence problem. CoSA demonstrates exceptional performance, achieving mIoU of 76.2\\% and 51.0\\% on VOC and COCO validation datasets, respectively, surpassing existing baselines by a substantial margin. Notably, CoSA is the first single-stage approach to outperform all existing multi-stage methods including those with additional supervision. Code is avilable at \\url{https://github.com/youshyee/CoSA}.","sentences":["Class activation maps (CAMs) are commonly employed in weakly supervised semantic segmentation (WSSS) to produce pseudo-labels.","Due to incomplete or excessive class activation, existing studies often resort to offline CAM refinement, introducing additional stages or proposing offline modules.","This can cause optimization difficulties for single-stage methods and limit generalizability.","In this study, we aim to reduce the observed CAM inconsistency and error to mitigate reliance on refinement processes.","We propose an end-to-end WSSS model incorporating guided CAMs, wherein our segmentation model is trained while concurrently optimizing CAMs online.","Our method, Co-training with Swapping Assignments (CoSA), leverages a dual-stream framework, where one sub-network learns from the swapped assignments generated by the other.","We introduce three techniques: i) soft perplexity-based regularization to penalize uncertain regions; ii) a threshold-searching approach to dynamically revise the confidence threshold; and iii) contrastive separation to address the coexistence problem.","CoSA demonstrates exceptional performance, achieving mIoU of 76.2\\% and 51.0\\% on VOC and COCO validation datasets, respectively, surpassing existing baselines by a substantial margin.","Notably, CoSA is the first single-stage approach to outperform all existing multi-stage methods including those with additional supervision.","Code is avilable at \\url{https://github.com/youshyee/CoSA}."],"url":"http://arxiv.org/abs/2402.17891v1","category":"cs.CV"}
{"created":"2024-02-27 21:00:00","title":"Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion","abstract":"This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.","sentences":["This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density.","It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator.","DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator.","Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC).","We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality.","Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality.","Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RS-DMC.","Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential."],"url":"http://arxiv.org/abs/2402.17886v1","category":"stat.ML"}
{"created":"2024-02-27 20:35:31","title":"Challenges in Restructuring Community-based Moderation","abstract":"Content moderation practices and technologies need to change over time as requirements and community expectations shift. However, attempts to restructure existing moderation practices can be difficult, especially for platforms that rely on their communities to conduct moderation activities, because changes can transform the workflow and workload of moderators and contributors' reward systems. Through the study of extensive archival discussions around a prepublication moderation technology on Wikipedia named Flagged Revisions, complemented by seven semi-structured interviews, we identify various challenges in restructuring community-based moderation practices. We learn that while a new system might sound good in theory and perform well in terms of quantitative metrics, it may conflict with existing social norms. Our findings also highlight how the intricate relationship between platforms and self-governed communities can hinder the ability to assess the performance of any new system and introduce considerable costs related to maintaining, overhauling, or scrapping any piece of infrastructure.","sentences":["Content moderation practices and technologies need to change over time as requirements and community expectations shift.","However, attempts to restructure existing moderation practices can be difficult, especially for platforms that rely on their communities to conduct moderation activities, because changes can transform the workflow and workload of moderators and contributors' reward systems.","Through the study of extensive archival discussions around a prepublication moderation technology on Wikipedia named Flagged Revisions, complemented by seven semi-structured interviews, we identify various challenges in restructuring community-based moderation practices.","We learn that while a new system might sound good in theory and perform well in terms of quantitative metrics, it may conflict with existing social norms.","Our findings also highlight how the intricate relationship between platforms and self-governed communities can hinder the ability to assess the performance of any new system and introduce considerable costs related to maintaining, overhauling, or scrapping any piece of infrastructure."],"url":"http://arxiv.org/abs/2402.17880v1","category":"cs.HC"}
{"created":"2024-02-27 20:10:03","title":"Stochastic Approximation with Biased MCMC for Expectation Maximization","abstract":"The expectation maximization (EM) algorithm is a widespread method for empirical Bayesian inference, but its expectation step (E-step) is often intractable. Employing a stochastic approximation scheme with Markov chain Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known as MCMC-SAEM. While theoretical guarantees for MCMC-SAEM have previously been established, these results are restricted to the case where asymptotically unbiased MCMC algorithms are used. In practice, MCMC-SAEM is often run with asymptotically biased MCMC, for which the consequences are theoretically less understood. In this work, we fill this gap by analyzing the asymptotics and non-asymptotics of SAEM with biased MCMC steps, particularly the effect of bias. We also provide numerical experiments comparing the Metropolis-adjusted Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted Langevin algorithm (ULA), which is asymptotically biased, on synthetic and real datasets. Experimental results show that ULA is more stable with respect to the choice of Langevin stepsize and can sometimes result in faster convergence.","sentences":["The expectation maximization (EM) algorithm is a widespread method for empirical Bayesian inference, but its expectation step (E-step) is often intractable.","Employing a stochastic approximation scheme with Markov chain Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known as MCMC-SAEM.","While theoretical guarantees for MCMC-SAEM have previously been established, these results are restricted to the case where asymptotically unbiased MCMC algorithms are used.","In practice, MCMC-SAEM is often run with asymptotically biased MCMC, for which the consequences are theoretically less understood.","In this work, we fill this gap by analyzing the asymptotics and non-asymptotics of SAEM with biased MCMC steps, particularly the effect of bias.","We also provide numerical experiments comparing the Metropolis-adjusted Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted Langevin algorithm (ULA), which is asymptotically biased, on synthetic and real datasets.","Experimental results show that ULA is more stable with respect to the choice of Langevin stepsize and can sometimes result in faster convergence."],"url":"http://arxiv.org/abs/2402.17870v1","category":"stat.CO"}
{"created":"2024-02-28 18:14:06","title":"Constructing Bayesian Optimal Designs for Discrete Choice Experiments by Simulated Annealing","abstract":"Discrete Choice Experiments (DCEs) investigate the attributes that influence individuals' choices when selecting among various options. To enhance the quality of the estimated choice models, researchers opt for Bayesian optimal designs that utilize existing information about the attributes' preferences. Given the nonlinear nature of choice models, the construction of an appropriate design requires efficient algorithms. Among these, the Coordinate-Exchange (CE) algorithm is most commonly employed for constructing designs based on the multinomial logit model. Since this is a hill-climbing algorithm, obtaining better designs necessitates multiple random starting designs. This approach increases the algorithm's run-time, but may not lead to a significant improvement in results. We propose the use of a Simulated Annealing (SA) algorithm to construct Bayesian D-optimal designs. This algorithm accepts both superior and inferior solutions, avoiding premature convergence and allowing a more thorough exploration of potential designs. Consequently, it ultimately obtains higher-quality choice designs within the same time-frame. Our work represents the first application of an SA algorithm in constructing Bayesian optimal designs for DCEs. Through computational experiments and a real-life case study, we demonstrate that the SA designs consistently outperform the CE designs in terms of Bayesian D-efficiency, especially when the prior preference information is highly uncertain.","sentences":["Discrete Choice Experiments (DCEs) investigate the attributes that influence individuals' choices when selecting among various options.","To enhance the quality of the estimated choice models, researchers opt for Bayesian optimal designs that utilize existing information about the attributes' preferences.","Given the nonlinear nature of choice models, the construction of an appropriate design requires efficient algorithms.","Among these, the Coordinate-Exchange (CE) algorithm is most commonly employed for constructing designs based on the multinomial logit model.","Since this is a hill-climbing algorithm, obtaining better designs necessitates multiple random starting designs.","This approach increases the algorithm's run-time, but may not lead to a significant improvement in results.","We propose the use of a Simulated Annealing (SA) algorithm to construct Bayesian D-optimal designs.","This algorithm accepts both superior and inferior solutions, avoiding premature convergence and allowing a more thorough exploration of potential designs.","Consequently, it ultimately obtains higher-quality choice designs within the same time-frame.","Our work represents the first application of an SA algorithm in constructing Bayesian optimal designs for DCEs.","Through computational experiments and a real-life case study, we demonstrate that the SA designs consistently outperform the CE designs in terms of Bayesian D-efficiency, especially when the prior preference information is highly uncertain."],"url":"http://arxiv.org/abs/2402.18533v1","category":"stat.ME"}
{"created":"2024-02-28 18:12:33","title":"All electrical cooling of an optically levitated nanoparticle","abstract":"We implement an all electrical controller for 3D feedback cooling of an optically levitated nanoparticle capable of reaching sub-Kelvin temperatures for the center of mass motion. The controller is based on an optimal policy where state estimation is made by delayed position measurements. The method offers a simplified path for pre-cooling and decoupling the transverse degrees of freedom of the nanoparticle. Numerical simulations show that in an improved setup with quantum limited detection, 3D ground state cooling and all electrical quantum control can be achieved.","sentences":["We implement an all electrical controller for 3D feedback cooling of an optically levitated nanoparticle capable of reaching sub-Kelvin temperatures for the center of mass motion.","The controller is based on an optimal policy where state estimation is made by delayed position measurements.","The method offers a simplified path for pre-cooling and decoupling the transverse degrees of freedom of the nanoparticle.","Numerical simulations show that in an improved setup with quantum limited detection, 3D ground state cooling and all electrical quantum control can be achieved."],"url":"http://arxiv.org/abs/2402.18532v1","category":"quant-ph"}
{"created":"2024-02-28 18:08:52","title":"Reducing the Number of Qubits from $n^2$ to $n\\log_{2} (n)$ to Solve the Traveling Salesman Problem with Quantum Computers: A Proposal for Demonstrating Quantum Supremacy in the NISQ Era","abstract":"In our pursuit of quantum supremacy during the NISQ era, this research introduces a novel approach rooted in the Quantum Approximate Optimization Algorithm (QAOA) framework to address the Traveling Salesman Problem (TSP). By strategically reducing the requisite qubit count from $n^2$ to $n\\log_{2} (n)$, our QAOA-based algorithm not only contributes to the ongoing discourse on qubit efficiency but also demonstrates improved performance based on established metrics, underscoring its potential for achieving NISQ-era supremacy in solving real-world optimization challenges.","sentences":["In our pursuit of quantum supremacy during the NISQ era, this research introduces a novel approach rooted in the Quantum Approximate Optimization Algorithm (QAOA) framework to address the Traveling Salesman Problem (TSP).","By strategically reducing the requisite qubit count from $n^2$ to $n\\log_{2} (n)$, our QAOA-based algorithm not only contributes to the ongoing discourse on qubit efficiency but also demonstrates improved performance based on established metrics, underscoring its potential for achieving NISQ-era supremacy in solving real-world optimization challenges."],"url":"http://arxiv.org/abs/2402.18530v1","category":"quant-ph"}
{"created":"2024-02-28 17:43:02","title":"A Primal-Dual Frank-Wolfe Algorithm for Linear Programming","abstract":"We present two first-order primal-dual algorithms for solving saddle point formulations of linear programs, namely FWLP (Frank-Wolfe Linear Programming) and FWLP-P. The former iteratively applies the Frank-Wolfe algorithm to both the primal and dual of the saddle point formulation of a standard-form LP. The latter is a modification of FWLP in which regularizing perturbations are used in computing the iterates. We show that FWLP-P converges to a primal-dual solution with error $O(1/\\sqrt{k})$ after $k$ iterations, while no convergence guarantees are provided for FWLP. We also discuss the advantages of using FWLP and FWLP-P for solving very large LPs. In particular, we argue that only part of the matrix $A$ is needed at each iteration, in contrast to other first-order methods.","sentences":["We present two first-order primal-dual algorithms for solving saddle point formulations of linear programs, namely FWLP (Frank-Wolfe Linear Programming) and FWLP-P.","The former iteratively applies the Frank-Wolfe algorithm to both the primal and dual of the saddle point formulation of a standard-form LP.","The latter is a modification of FWLP in which regularizing perturbations are used in computing the iterates.","We show that FWLP-P converges to a primal-dual solution with error $O(1/\\sqrt{k})$ after $k$ iterations, while no convergence guarantees are provided for FWLP.","We also discuss the advantages of using FWLP and FWLP-P for solving very large LPs.","In particular, we argue that only part of the matrix $A$ is needed at each iteration, in contrast to other first-order methods."],"url":"http://arxiv.org/abs/2402.18514v1","category":"math.OC"}
{"created":"2024-02-28 17:00:33","title":"Libfork: portable continuation-stealing with stackless coroutines","abstract":"Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.","sentences":["Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling.","The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques.","We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks.","We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks.","Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory.","Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory.","Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers."],"url":"http://arxiv.org/abs/2402.18480v1","category":"cs.DC"}
{"created":"2024-02-28 15:32:35","title":"QAOA with random and subgraph driver Hamiltonians","abstract":"The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems. The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians. In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom driver Hamiltonians, can affect the performance of QAOA. We derive an expected value formula for QAOA with custom driver Hamiltonians at p = 1 and show numerically that some of these custom drivers can achieve higher approximation ratio than the original algorithm implementation. Out of all the graphs tested, 0.036% of the random custom drivers, 75.9% of the subgraph custom drivers, 95.1% of the triangle-removed custom drivers, and 93.9% of the maximal degree edge-removed custom drivers have a higher approximation ratio than the original QAOA implementation. This finding opens up the question of whether better driver Hamiltonians can be designed to further improve the performance of QAOA.","sentences":["The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems.","The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians.","In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom driver Hamiltonians, can affect the performance of QAOA.","We derive an expected value formula for QAOA with custom driver Hamiltonians at p = 1 and show numerically that some of these custom drivers can achieve higher approximation ratio than the original algorithm implementation.","Out of all the graphs tested, 0.036% of the random custom drivers, 75.9% of the subgraph custom drivers, 95.1% of the triangle-removed custom drivers, and 93.9% of the maximal degree edge-removed custom drivers have a higher approximation ratio than the original QAOA implementation.","This finding opens up the question of whether better driver Hamiltonians can be designed to further improve the performance of QAOA."],"url":"http://arxiv.org/abs/2402.18412v1","category":"quant-ph"}
{"created":"2024-02-28 14:27:20","title":"Port-Based State Preparation and Applications","abstract":"We introduce Port-Based State Preparation (PBSP), a teleportation task where Alice holds a complete classical description of the target state and Bob's correction operations are restricted to only tracing out registers. We show a protocol that implements PBSP with error decreasing exponentially in the number of ports, in contrast to the polynomial trade-off for the related task of Port-Based Teleportation, and we prove that this is optimal when a maximally entangled resource state is used.   As an application, we introduce approximate Universal Programmable Hybrid Processors (UPHP). Here the goal is to encode a unitary as a quantum state, and the UPHP can apply this unitary to a quantum state when knowing its classical description. We give a construction that needs strictly less memory in terms of dimension than the optimal approximate Universal Programmable Quantum Processor achieving the same error. Additionally, we provide lower bounds for the optimal trade-off between memory and error of UPHPs.","sentences":["We introduce Port-Based State Preparation (PBSP), a teleportation task where Alice holds a complete classical description of the target state and Bob's correction operations are restricted to only tracing out registers.","We show a protocol that implements PBSP with error decreasing exponentially in the number of ports, in contrast to the polynomial trade-off for the related task of Port-Based Teleportation, and we prove that this is optimal when a maximally entangled resource state is used.   ","As an application, we introduce approximate Universal Programmable Hybrid Processors (UPHP).","Here the goal is to encode a unitary as a quantum state, and the UPHP can apply this unitary to a quantum state when knowing its classical description.","We give a construction that needs strictly less memory in terms of dimension than the optimal approximate Universal Programmable Quantum Processor achieving the same error.","Additionally, we provide lower bounds for the optimal trade-off between memory and error of UPHPs."],"url":"http://arxiv.org/abs/2402.18356v1","category":"quant-ph"}
{"created":"2024-02-28 12:42:34","title":"Mapping between measurement scales in meta-analysis, with application to measures of body mass index in children","abstract":"Quantitative evidence synthesis methods aim to combine data from multiple medical trials to infer relative effects of different interventions. A challenge arises when trials report continuous outcomes on different measurement scales. To include all evidence in one coherent analysis, we require methods to `map' the outcomes onto a single scale. This is particularly challenging when trials report aggregate rather than individual data. We are motivated by a meta-analysis of interventions to prevent obesity in children. Trials report aggregate measurements of body mass index (BMI) either expressed as raw values or standardised for age and sex. We develop three methods for mapping between aggregate BMI data using known relationships between individual measurements on different scales. The first is an analytical method based on the mathematical definitions of z-scores and percentiles. The other two approaches involve sampling individual participant data on which to perform the conversions. One method is a straightforward sampling routine, while the other involves optimization with respect to the reported outcomes. In contrast to the analytical approach, these methods also have wider applicability for mapping between any pair of measurement scales with known or estimable individual-level relationships. We verify and contrast our methods using trials from our data set which report outcomes on multiple scales. We find that all methods recreate mean values with reasonable accuracy, but for standard deviations, optimization outperforms the other methods. However, the optimization method is more likely to underestimate standard deviations and is vulnerable to non-convergence.","sentences":["Quantitative evidence synthesis methods aim to combine data from multiple medical trials to infer relative effects of different interventions.","A challenge arises when trials report continuous outcomes on different measurement scales.","To include all evidence in one coherent analysis, we require methods to `map' the outcomes onto a single scale.","This is particularly challenging when trials report aggregate rather than individual data.","We are motivated by a meta-analysis of interventions to prevent obesity in children.","Trials report aggregate measurements of body mass index (BMI) either expressed as raw values or standardised for age and sex.","We develop three methods for mapping between aggregate BMI data using known relationships between individual measurements on different scales.","The first is an analytical method based on the mathematical definitions of z-scores and percentiles.","The other two approaches involve sampling individual participant data on which to perform the conversions.","One method is a straightforward sampling routine, while the other involves optimization with respect to the reported outcomes.","In contrast to the analytical approach, these methods also have wider applicability for mapping between any pair of measurement scales with known or estimable individual-level relationships.","We verify and contrast our methods using trials from our data set which report outcomes on multiple scales.","We find that all methods recreate mean values with reasonable accuracy, but for standard deviations, optimization outperforms the other methods.","However, the optimization method is more likely to underestimate standard deviations and is vulnerable to non-convergence."],"url":"http://arxiv.org/abs/2402.18298v1","category":"stat.ME"}
{"created":"2024-02-28 12:04:10","title":"Necessary and sufficient conditions of extremum for polynomials and power series in the case of two variables","abstract":"The present paper is a continuation of the author's previous works, in which necessary and sufficient local extrema at a stationary point of a polynomial or a power series (and thus of an analytic function) are given. It is known that for the case of one variable, the necessary and sufficient conditions of the extremum are closing, i.e., they can be formulated as a single condition. The next most complicated case is the case with two variables, which is the one considered in this paper. In this case, many procedures, to which the verification of necessary and sufficient conditions is reduced, are based on the computation of real roots of a polynomial from one variable, as well as on the solution of some other rather simple practically realizable problems. An algorithm based on these procedures is described. Nevertheless, there are still cases where this algorithm \"doesn't work\". For such cases we propose the method of \"substitution of polynomials with uncertain coefficients\", using which, in particular, we have described an algorithm that allows us to unambiguously answer the question about the presence of a local minimum at a stationary point for a polynomial that is the sum of two A-quasi-homogeneous forms, where A - is a two-dimensional vector, whose components are natural numbers.","sentences":["The present paper is a continuation of the author's previous works, in which necessary and sufficient local extrema at a stationary point of a polynomial or a power series (and thus of an analytic function) are given.","It is known that for the case of one variable, the necessary and sufficient conditions of the extremum are closing, i.e., they can be formulated as a single condition.","The next most complicated case is the case with two variables, which is the one considered in this paper.","In this case, many procedures, to which the verification of necessary and sufficient conditions is reduced, are based on the computation of real roots of a polynomial from one variable, as well as on the solution of some other rather simple practically realizable problems.","An algorithm based on these procedures is described.","Nevertheless, there are still cases where this algorithm \"doesn't work\".","For such cases we propose the method of \"substitution of polynomials with uncertain coefficients\", using which, in particular, we have described an algorithm that allows us to unambiguously answer the question about the presence of a local minimum at a stationary point for a polynomial that is the sum of two A-quasi-homogeneous forms, where A - is a two-dimensional vector, whose components are natural numbers."],"url":"http://arxiv.org/abs/2402.18273v1","category":"math.OC"}
{"created":"2024-02-28 11:37:09","title":"The Cost of Permissionless Liquidity Provision in Automated Market Makers","abstract":"Automated market makers (AMMs) allocate fee revenue proportional to the amount of liquidity investors deposit. In this paper, we study the economic consequences of the competition between passive liquidity providers (LPs) caused by this allocation rule. We employ a game-theoretic model in which $N$ strategic agents optimally provide liquidity. In this setting, we find that competition drives LPs to provide excess liquidity. In the limit, the excess liquidity converges to a constant that linearly increases with the amount of base demand, demand that is insensitive to trading costs. Providing excess liquidity is costly as more capital is exposed to adverse selection costs, leading to a loss in welfare. Our main result is that the price of anarchy, defined over the liquidity provider performance, is $O(N)$, implying that the welfare loss scales linearly with the number of liquidity providers. We show that this result is still observable when using richer aggregate demand models.","sentences":["Automated market makers (AMMs) allocate fee revenue proportional to the amount of liquidity investors deposit.","In this paper, we study the economic consequences of the competition between passive liquidity providers (LPs) caused by this allocation rule.","We employ a game-theoretic model in which $N$ strategic agents optimally provide liquidity.","In this setting, we find that competition drives LPs to provide excess liquidity.","In the limit, the excess liquidity converges to a constant that linearly increases with the amount of base demand, demand that is insensitive to trading costs.","Providing excess liquidity is costly as more capital is exposed to adverse selection costs, leading to a loss in welfare.","Our main result is that the price of anarchy, defined over the liquidity provider performance, is $O(N)$, implying that the welfare loss scales linearly with the number of liquidity providers.","We show that this result is still observable when using richer aggregate demand models."],"url":"http://arxiv.org/abs/2402.18256v1","category":"cs.GT"}
{"created":"2024-02-28 08:53:37","title":"Strategies for the alignment of electronic states in quantum-dot tunnel-injection lasers and their influence on the emission dynamics","abstract":"In quantum-dot tunnel-injection lasers, the excited charge carriers are efficiently captured from the bulk states via an injector quantum well and then transferred into the quantum dots via a tunnel barrier. The alignment of the electronic levels is crucial for the high efficiency of these processes and especially for the fast modulation dynamics of these lasers. In particular, the quantum mechanical nature of the tunneling process must be taken into account in the transition from two-dimensional quantum well states to zero-dimensional quantum dot states. This results in hybrid states, from which the scattering into the quantum-dot ground states takes place. We combine electronic state calculations of the tunnel-injection structures with many-body calculations of the scattering processes and insert this into a complete laser simulator. This allows us to study the influence of the level alignment and limitations due to inhomogeneous quantum-dot distributions. We find that the optimal alignment deviates from a simple picture in which the of the quantum-dot ground state energies are one LO-phonon energy below the injector quantum well ground state. \\author{Frank Jahnke","sentences":["In quantum-dot tunnel-injection lasers, the excited charge carriers are efficiently captured from the bulk states via an injector quantum well and then transferred into the quantum dots via a tunnel barrier.","The alignment of the electronic levels is crucial for the high efficiency of these processes and especially for the fast modulation dynamics of these lasers.","In particular, the quantum mechanical nature of the tunneling process must be taken into account in the transition from two-dimensional quantum well states to zero-dimensional quantum dot states.","This results in hybrid states, from which the scattering into the quantum-dot ground states takes place.","We combine electronic state calculations of the tunnel-injection structures with many-body calculations of the scattering processes and insert this into a complete laser simulator.","This allows us to study the influence of the level alignment and limitations due to inhomogeneous quantum-dot distributions.","We find that the optimal alignment deviates from a simple picture in which the of the quantum-dot ground state energies are one LO-phonon energy below the injector quantum well ground state.","\\author{Frank Jahnke"],"url":"http://arxiv.org/abs/2402.18165v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-28 08:53:01","title":"Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision","abstract":"In recent years, model quantization for face recognition has gained prominence. Traditionally, compressing models involved vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential. This paper addresses this by introducing an efficiency-driven approach, fine-tuning the model with just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective quantization is achievable with a smaller dataset, presenting a new paradigm. Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed model training for face recognition. The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach. This paper advances model quantization by highlighting the efficiency and optimal results with small data and training time.","sentences":["In recent years, model quantization for face recognition has gained prominence.","Traditionally, compressing models involved vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential.","This paper addresses this by introducing an efficiency-driven approach, fine-tuning the model with just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective quantization is achievable with a smaller dataset, presenting a new paradigm.","Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed model training for face recognition.","The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach.","This paper advances model quantization by highlighting the efficiency and optimal results with small data and training time."],"url":"http://arxiv.org/abs/2402.18163v1","category":"cs.CV"}
{"created":"2024-02-28 07:05:11","title":"PIMSYN: Synthesizing Processing-in-memory CNN Accelerators","abstract":"Processing-in-memory architectures have been regarded as a promising solution for CNN acceleration. Existing PIM accelerator designs rely heavily on the experience of experts and require significant manual design overhead. Manual design cannot effectively optimize and explore architecture implementations. In this work, we develop an automatic framework PIMSYN for synthesizing PIM-based CNN accelerators, which greatly facilitates architecture design and helps generate energyefficient accelerators. PIMSYN can automatically transform CNN applications into execution workflows and hardware construction of PIM accelerators. To systematically optimize the architecture, we embed an architectural exploration flow into the synthesis framework, providing a more comprehensive design space. Experiments demonstrate that PIMSYN improves the power efficiency by several times compared with existing works. PIMSYN can be obtained from https://github.com/lixixi-jook/PIMSYN-NN.","sentences":["Processing-in-memory architectures have been regarded as a promising solution for CNN acceleration.","Existing PIM accelerator designs rely heavily on the experience of experts and require significant manual design overhead.","Manual design cannot effectively optimize and explore architecture implementations.","In this work, we develop an automatic framework PIMSYN for synthesizing PIM-based CNN accelerators, which greatly facilitates architecture design and helps generate energyefficient accelerators.","PIMSYN can automatically transform CNN applications into execution workflows and hardware construction of PIM accelerators.","To systematically optimize the architecture, we embed an architectural exploration flow into the synthesis framework, providing a more comprehensive design space.","Experiments demonstrate that PIMSYN improves the power efficiency by several times compared with existing works.","PIMSYN can be obtained from https://github.com/lixixi-jook/PIMSYN-NN."],"url":"http://arxiv.org/abs/2402.18114v1","category":"cs.AR"}
{"created":"2024-02-28 05:53:27","title":"A gradient flow method for smooth splines versus least-squares fitting on Riemannian manifolds","abstract":"This article presents a novel resolution to the problem of spline interpolation versus least-squares fitting on smooth Riemannian manifolds utilizing the method of gradient flows of networks. This approach represents a contribution to both geometric control theory and statistical shape data analysis. Our work encompasses a rigorous proof for the existence of global solutions in H\\\"{o}lder spaces for the gradient flow. The asymptotic limits of these solutions establish the existence of the spline interpolation versus least-squares fitting problem on smooth Riemannian manifolds, offering a comprehensive solution. Notably, the constructive nature of the proof suggests potential numerical schemes for finding solutions.","sentences":["This article presents a novel resolution to the problem of spline interpolation versus least-squares fitting on smooth Riemannian manifolds utilizing the method of gradient flows of networks.","This approach represents a contribution to both geometric control theory and statistical shape data analysis.","Our work encompasses a rigorous proof for the existence of global solutions in H\\\"{o}lder spaces for the gradient flow.","The asymptotic limits of these solutions establish the existence of the spline interpolation versus least-squares fitting problem on smooth Riemannian manifolds, offering a comprehensive solution.","Notably, the constructive nature of the proof suggests potential numerical schemes for finding solutions."],"url":"http://arxiv.org/abs/2402.18067v1","category":"math.OC"}
{"created":"2024-02-28 03:47:17","title":"Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift","abstract":"Model inversion attacks (MIAs) seek to infer the private training data of a target classifier by generating synthetic images that reflect the characteristics of the target class through querying the model. However, prior studies have relied on full access to the target model, which is not practical in real-world scenarios. Additionally, existing black-box MIAs assume that the image prior and target model follow the same distribution. However, when confronted with diverse data distribution settings, these methods may result in suboptimal performance in conducting attacks. To address these limitations, this paper proposes a \\textbf{C}onfidence-\\textbf{G}uided \\textbf{M}odel \\textbf{I}nversion attack method called CG-MI, which utilizes the latent space of a pre-trained publicly available generative adversarial network (GAN) as prior information and gradient-free optimizer, enabling high-resolution MIAs across different data distributions in a black-box setting. Our experiments demonstrate that our method significantly \\textbf{outperforms the SOTA black-box MIA by more than 49\\% for Celeba and 58\\% for Facescrub in different distribution settings}. Furthermore, our method exhibits the ability to generate high-quality images \\textbf{comparable to those produced by white-box attacks}. Our method provides a practical and effective solution for black-box model inversion attacks.","sentences":["Model inversion attacks (MIAs) seek to infer the private training data of a target classifier by generating synthetic images that reflect the characteristics of the target class through querying the model.","However, prior studies have relied on full access to the target model, which is not practical in real-world scenarios.","Additionally, existing black-box MIAs assume that the image prior and target model follow the same distribution.","However, when confronted with diverse data distribution settings, these methods may result in suboptimal performance in conducting attacks.","To address these limitations, this paper proposes a \\textbf{C}onfidence-\\textbf{G}uided \\textbf{M}odel \\textbf{I}nversion attack method called CG-MI, which utilizes the latent space of a pre-trained publicly available generative adversarial network (GAN) as prior information and gradient-free optimizer, enabling high-resolution MIAs across different data distributions in a black-box setting.","Our experiments demonstrate that our method significantly \\textbf{outperforms the SOTA black-box MIA by more than 49\\% for Celeba and 58\\% for Facescrub in different distribution settings}.","Furthermore, our method exhibits the ability to generate high-quality images \\textbf{comparable to those produced by white-box attacks}.","Our method provides a practical and effective solution for black-box model inversion attacks."],"url":"http://arxiv.org/abs/2402.18027v1","category":"cs.CV"}
{"created":"2024-02-28 03:33:29","title":"Online Time-Optimal Trajectory Generation for Two Quadrotors with Multi-Waypoints Constraints","abstract":"The autonomous quadrotor's flying speed has kept increasing in the past 5 years, especially in the field of autonomous drone racing. However, the majority of the research mainly focuses on the aggressive flight of a single quadrotor. In this letter, we propose a novel method called Pairwise Model Predictive Control (PMPC) that can guide two quadrotors online to fly through the waypoints with minimum time without collisions. The flight task is first modeled as a nonlinear optimization problem and then an efficient two-step mass point velocity search method is used to provide initial values and references to improve the solving efficiency so that the method can run online with a frequency of 50 Hz and can handle dynamic waypoints. The simulation and real-world experiments validate the feasibility of the proposed method and in the real-world experiments, the two quadrotors can achieve a top speed of 8.1m/s in a 6-waypoint racing track in a compact flying arena of 6m*4m*2m.","sentences":["The autonomous quadrotor's flying speed has kept increasing in the past 5 years, especially in the field of autonomous drone racing.","However, the majority of the research mainly focuses on the aggressive flight of a single quadrotor.","In this letter, we propose a novel method called Pairwise Model Predictive Control (PMPC) that can guide two quadrotors online to fly through the waypoints with minimum time without collisions.","The flight task is first modeled as a nonlinear optimization problem and then an efficient two-step mass point velocity search method is used to provide initial values and references to improve the solving efficiency so that the method can run online with a frequency of 50 Hz and can handle dynamic waypoints.","The simulation and real-world experiments validate the feasibility of the proposed method and in the real-world experiments, the two quadrotors can achieve a top speed of 8.1m/s in a 6-waypoint racing track in a compact flying arena of 6m*4m*2m."],"url":"http://arxiv.org/abs/2402.18021v1","category":"cs.RO"}
{"created":"2024-02-28 02:06:11","title":"PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis","abstract":"This paper considers the problem of generative novel view synthesis (GNVS), generating novel, plausible views of a scene given a limited number of known views. Here, we propose a set-based generative model that can simultaneously generate multiple, self-consistent new views, conditioned on any number of known views. Our approach is not limited to generating a single image at a time and can condition on zero, one, or more views. As a result, when generating a large number of views, our method is not restricted to a low-order autoregressive generation approach and is better able to maintain generated image quality over large sets of images. We evaluate the proposed model on standard NVS datasets and show that it outperforms the state-of-the-art image-based GNVS baselines. Further, we show that the model is capable of generating sets of camera views that have no natural sequential ordering, like loops and binocular trajectories, and significantly outperforms other methods on such tasks.","sentences":["This paper considers the problem of generative novel view synthesis (GNVS), generating novel, plausible views of a scene given a limited number of known views.","Here, we propose a set-based generative model that can simultaneously generate multiple, self-consistent new views, conditioned on any number of known views.","Our approach is not limited to generating a single image at a time and can condition on zero, one, or more views.","As a result, when generating a large number of views, our method is not restricted to a low-order autoregressive generation approach and is better able to maintain generated image quality over large sets of images.","We evaluate the proposed model on standard NVS datasets and show that it outperforms the state-of-the-art image-based GNVS baselines.","Further, we show that the model is capable of generating sets of camera views that have no natural sequential ordering, like loops and binocular trajectories, and significantly outperforms other methods on such tasks."],"url":"http://arxiv.org/abs/2402.17986v1","category":"cs.CV"}
{"created":"2024-02-28 01:52:54","title":"Dynamic laser ablation loading of a linear Paul trap","abstract":"We present a detailed method for accumulating Ca$^{+}$ ions controllably in a linear Paul trap. The ions are generated by pulsed laser ablation and dynamically loaded into the ion trap by switching the trapping potential on and off. The loaded ions are precooled by buffer gas and then laser-cooled to form Coulomb crystals for verifying quantity. The number of ions is controlled by manipulating the trapping potential of the ion trap, partial pressure of buffer gas and turn-on time of the entrance end cap voltage. With single-pulse laser ablation, the number of trapped ions ranges from tens to ten thousand. The kinetic energy of loaded ions can be selected via the optimal turn-on time of the entrance end cap. Using multiple-pulse laser ablation, the number is further increased and reaches about $4 \\times 10^{4}$. The dynamic loading method has wide application for accumulating low-yielding ions via laser ablation in the ion trap.","sentences":["We present a detailed method for accumulating Ca$^{+}$ ions controllably in a linear Paul trap.","The ions are generated by pulsed laser ablation and dynamically loaded into the ion trap by switching the trapping potential on and off.","The loaded ions are precooled by buffer gas and then laser-cooled to form Coulomb crystals for verifying quantity.","The number of ions is controlled by manipulating the trapping potential of the ion trap, partial pressure of buffer gas and turn-on time of the entrance end cap voltage.","With single-pulse laser ablation, the number of trapped ions ranges from tens to ten thousand.","The kinetic energy of loaded ions can be selected via the optimal turn-on time of the entrance end cap.","Using multiple-pulse laser ablation, the number is further increased and reaches about $4 \\times 10^{4}$.","The dynamic loading method has wide application for accumulating low-yielding ions via laser ablation in the ion trap."],"url":"http://arxiv.org/abs/2402.17981v1","category":"physics.atom-ph"}
{"created":"2024-02-28 00:11:08","title":"Quantitative asymptotic regularity of the VAM iteration with error terms for accretive operators in Banach spaces","abstract":"In this paper we obtain, by using proof mining methods, quantitative results on the asymptotic regularity of the viscosity approximation method (VAM) with error terms for accretive operators in Banach spaces. For concrete instances of the parameter sequences, linear rates are computed by applying a lemma due to Sabach and Shtern.","sentences":["In this paper we obtain, by using proof mining methods, quantitative results on the asymptotic regularity of the viscosity approximation method (VAM) with error terms for accretive operators in Banach spaces.","For concrete instances of the parameter sequences, linear rates are computed by applying a lemma due to Sabach and Shtern."],"url":"http://arxiv.org/abs/2402.17947v1","category":"math.OC"}
{"created":"2024-02-27 23:59:01","title":"Large Language Models on Tabular Data -- A Survey","abstract":"Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.","sentences":["Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding.","Each task presents unique challenges and opportunities.","However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain.","This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized.","It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field.","It also provides relevant code and datasets references.","Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field."],"url":"http://arxiv.org/abs/2402.17944v1","category":"cs.CL"}
{"created":"2024-02-27 21:51:15","title":"Simulation of Muon Tomography Projections to Image the Pyramids of Giza","abstract":"Purpose: A geometric simulation of a possible two-plane detector was developed to test the abilities of the detector to generate high-resolution images of the Great Pyramid using muon tomography. Methods and Materials: Trajectory range, angular resolution, and acceptance of the detector were calculated with a simulation. Trajectories and the corresponding sinogram space covered were simulated first with one detector in one location, and then two moving detectors on adjacent sides of the pyramid. The resolution at the center slice of the pyramid was calculated using the angular resolution of the detector. Results: The simulation returned trajectory range encompassing the pyramid and peak angular resolution of .0004sr. Sinogram space covered by one position was inadequate, however two moving detectors on adjacent sides of the pyramid cover a significant portion. Resolution at the center of the pyramid is roughly 3m. Conclusions: The simulation provides a way to calculate the detector positions needed to cover an adequate amount of sinogram space for high-resolution cosmic-ray tomographic reconstruction of the Great Pyramids. Key Words: high-resolution muon tomography, one-sided tomography, sinogram simulation, detector simulation","sentences":["Purpose: A geometric simulation of a possible two-plane detector was developed to test the abilities of the detector to generate high-resolution images of the Great Pyramid using muon tomography.","Methods and Materials: Trajectory range, angular resolution, and acceptance of the detector were calculated with a simulation.","Trajectories and the corresponding sinogram space covered were simulated first with one detector in one location, and then two moving detectors on adjacent sides of the pyramid.","The resolution at the center slice of the pyramid was calculated using the angular resolution of the detector.","Results:","The simulation returned trajectory range encompassing the pyramid and peak angular resolution of .0004sr.","Sinogram space covered by one position was inadequate, however two moving detectors on adjacent sides of the pyramid cover a significant portion.","Resolution at the center of the pyramid is roughly 3m. Conclusions: The simulation provides a way to calculate the detector positions needed to cover an adequate amount of sinogram space for high-resolution cosmic-ray tomographic reconstruction of the Great Pyramids.","Key Words: high-resolution muon tomography, one-sided tomography, sinogram simulation, detector simulation"],"url":"http://arxiv.org/abs/2402.17909v1","category":"eess.IV"}
{"created":"2024-02-27 20:29:56","title":"3D Printing in Microfluidics: Experimental Optimization of Droplet Size and Generation Time through Flow Focusing, Phase, and Geometry Variation","abstract":"Droplet-based microfluidics systems have become widely used in recent years thanks to their advantages, varying from the possibility of handling small fluid volumes to directly synthesizing and encapsulating various living forms for biological-related applications. The effectiveness of such systems mainly depends on the ability to control some of these system's parameters, such as produced droplet size and formation time, which represents a challenging task. This work reports an experimental study on tuning droplet size and generation time in a flow-focusing geometry fabricated with stereolithography 3D printing by exploring the interplay of phase and geometrical parameters. We produced droplets at different low flow rates of continuous and dispersed phases to assess the effect of each of these phases on the droplets' size and formation time. We observed that smaller droplets were produced for high viscosity oil and water phase, along with high flow rates. In addition, changing the microfluidics channels' width, and morphology of the orifice has shown a similar effect on droplet size, as shown in the case of high-viscosity solutions. The variation of the bifurcation angle shows a noticeable variation in terms of the achieved droplet size and formation time. We further investigated the impact of modifying the width ratio of the continuous and dispersed phases on droplet formation","sentences":["Droplet-based microfluidics systems have become widely used in recent years thanks to their advantages, varying from the possibility of handling small fluid volumes to directly synthesizing and encapsulating various living forms for biological-related applications.","The effectiveness of such systems mainly depends on the ability to control some of these system's parameters, such as produced droplet size and formation time, which represents a challenging task.","This work reports an experimental study on tuning droplet size and generation time in a flow-focusing geometry fabricated with stereolithography 3D printing by exploring the interplay of phase and geometrical parameters.","We produced droplets at different low flow rates of continuous and dispersed phases to assess the effect of each of these phases on the droplets' size and formation time.","We observed that smaller droplets were produced for high viscosity oil and water phase, along with high flow rates.","In addition, changing the microfluidics channels' width, and morphology of the orifice has shown a similar effect on droplet size, as shown in the case of high-viscosity solutions.","The variation of the bifurcation angle shows a noticeable variation in terms of the achieved droplet size and formation time.","We further investigated the impact of modifying the width ratio of the continuous and dispersed phases on droplet formation"],"url":"http://arxiv.org/abs/2402.17876v1","category":"physics.flu-dyn"}
{"created":"2024-02-27 20:20:53","title":"Mixed Strategy Constraints in Continuous Games","abstract":"Equilibrium problems representing interaction in physical environments typically require continuous strategies which satisfy opponent-dependent constraints, such as those modeling collision avoidance. However, as with finite games, mixed strategies are often desired, both from an equilibrium existence perspective as well as a competitive perspective. To that end, this work investigates a chance-constraint-based approach to coupled constraints in generalized Nash equilibrium problems which are solved over pure strategies and mixing weights simultaneously. We motivate these constraints in a discrete setting, placing them on tensor games ($n$-player bimatrix games) as a justifiable approach to handling the probabilistic nature of mixing. Then, we describe a numerical solution method for these chance constrained tensor games with simultaneous pure strategy optimization. Finally, using a modified pursuit-evasion game as a motivating examples, we demonstrate the actual behavior of this solution method in terms of its fidelity, parameter sensitivity, and efficiency.","sentences":["Equilibrium problems representing interaction in physical environments typically require continuous strategies which satisfy opponent-dependent constraints, such as those modeling collision avoidance.","However, as with finite games, mixed strategies are often desired, both from an equilibrium existence perspective as well as a competitive perspective.","To that end, this work investigates a chance-constraint-based approach to coupled constraints in generalized Nash equilibrium problems which are solved over pure strategies and mixing weights simultaneously.","We motivate these constraints in a discrete setting, placing them on tensor games ($n$-player bimatrix games) as a justifiable approach to handling the probabilistic nature of mixing.","Then, we describe a numerical solution method for these chance constrained tensor games with simultaneous pure strategy optimization.","Finally, using a modified pursuit-evasion game as a motivating examples, we demonstrate the actual behavior of this solution method in terms of its fidelity, parameter sensitivity, and efficiency."],"url":"http://arxiv.org/abs/2402.17874v1","category":"cs.GT"}
{"created":"2024-02-27 19:51:11","title":"Performance limits of information engines","abstract":"We review recent studies of a colloidal information engine that consists of a bead in water and held by an optical trap. The bead is ratcheted upward without any apparent external work, by taking advantage of favorable thermal fluctuations. Much of the previous work on such engines aimed to show that accounting for information-processing costs can reconcile the observed motion with the second law of thermodynamics. By contrast, we focus on the factors that limit the performance of such engines by optimizing variously the upward velocity, rate of gravitational free-energy extraction, or ability to track a trajectory. We then consider measurement noise, which degrades engine performance. A naive use of noisy measurements in the feedback algorithm leads to a phase transition at finite signal-to-noise ratio: below the transition, the engine no longer functions. A more sophisticated, `Bayesian' algorithm eliminates the phase transition and improves performance. Finally, operating the information engine in a nonequilibrium environment with extra force fluctuations can enhance the performance by orders of magnitude, even to the point where the energy extracted exceeds that needed to run the information processing. Autonomous implementations of an information engine in such environments could be powered entirely by the additional energy of the bath.","sentences":["We review recent studies of a colloidal information engine that consists of a bead in water and held by an optical trap.","The bead is ratcheted upward without any apparent external work, by taking advantage of favorable thermal fluctuations.","Much of the previous work on such engines aimed to show that accounting for information-processing costs can reconcile the observed motion with the second law of thermodynamics.","By contrast, we focus on the factors that limit the performance of such engines by optimizing variously the upward velocity, rate of gravitational free-energy extraction, or ability to track a trajectory.","We then consider measurement noise, which degrades engine performance.","A naive use of noisy measurements in the feedback algorithm leads to a phase transition at finite signal-to-noise ratio: below the transition, the engine no longer functions.","A more sophisticated, `Bayesian' algorithm eliminates the phase transition and improves performance.","Finally, operating the information engine in a nonequilibrium environment with extra force fluctuations can enhance the performance by orders of magnitude, even to the point where the energy extracted exceeds that needed to run the information processing.","Autonomous implementations of an information engine in such environments could be powered entirely by the additional energy of the bath."],"url":"http://arxiv.org/abs/2402.17860v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-27 19:00:02","title":"Optimal control transport of neutral atoms in optical tweezers at finite temperature","abstract":"The transport of neutral atoms in Rydberg quantum computers is a crucial step of the initial arrangement of the grid as well as to the dynamic connectivity, recently successfully demonstrated. We study the application of optimal control and the quantum speed limit for the transport of neutral atoms in optical tweezers at finite temperatures and analyze how laser noise affects transport fidelity. Open-loop optimal control significantly enhances transport fidelity, achieving an improvement up to $89\\%$ for the lowest analyzed temperature of $1\\,\\mu$K for a distance of three micrometers. Furthermore, we simulate how the transport fidelity behaves in release-and-capture measurements, which are realizable in the experiment to estimate transport efficiency and implement closed-loop optimal control.","sentences":["The transport of neutral atoms in Rydberg quantum computers is a crucial step of the initial arrangement of the grid as well as to the dynamic connectivity, recently successfully demonstrated.","We study the application of optimal control and the quantum speed limit for the transport of neutral atoms in optical tweezers at finite temperatures and analyze how laser noise affects transport fidelity.","Open-loop optimal control significantly enhances transport fidelity, achieving an improvement up to $89\\%$ for the lowest analyzed temperature of $1\\,\\mu$K for a distance of three micrometers.","Furthermore, we simulate how the transport fidelity behaves in release-and-capture measurements, which are realizable in the experiment to estimate transport efficiency and implement closed-loop optimal control."],"url":"http://arxiv.org/abs/2402.17831v1","category":"quant-ph"}
{"created":"2024-02-27 11:27:32","title":"Material Microstructure Design Using VAE-Regression with Multimodal Prior","abstract":"We propose a variational autoencoder (VAE)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science. Our model systematically combines VAE with regression, linking the two models through a two-level prior conditioned on the regression variables. The regression loss is optimized jointly with the reconstruction loss of the variational autoencoder, learning microstructure features relevant for property prediction and reconstruction. The resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties. Since the inverse problem is ill-posed (one-to-many), we derive the objective function using a multi-modal Gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties. We show that for forward prediction, our model is as accurate as state-of-the-art forward-only models. Additionally, our method enables direct inverse inference. We show that the microstructures inferred using our model achieve desired properties reasonably accurately, avoiding the need for expensive optimization loops.","sentences":["We propose a variational autoencoder (VAE)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science.","Our model systematically combines VAE with regression, linking the two models through a two-level prior conditioned on the regression variables.","The regression loss is optimized jointly with the reconstruction loss of the variational autoencoder, learning microstructure features relevant for property prediction and reconstruction.","The resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties.","Since the inverse problem is ill-posed (one-to-many), we derive the objective function using a multi-modal Gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties.","We show that for forward prediction, our model is as accurate as state-of-the-art forward-only models.","Additionally, our method enables direct inverse inference.","We show that the microstructures inferred using our model achieve desired properties reasonably accurately, avoiding the need for expensive optimization loops."],"url":"http://arxiv.org/abs/2402.17806v1","category":"cs.LG"}
