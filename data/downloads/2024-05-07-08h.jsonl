{"created":"2024-05-06 17:59:45","title":"Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs","abstract":"Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical imaging, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 9 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, {especially open-source ones,} struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to enhance the performance of existing Video-LMMs. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities. Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.","sentences":["Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks.","These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical imaging, and autonomous vehicles.","The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts.","However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries.","In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions.","We evaluate 9 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, {especially open-source ones,} struggle with robustness and reasoning when dealing with complex videos.","Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to enhance the performance of existing Video-LMMs.","Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities.","Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/."],"url":"http://arxiv.org/abs/2405.03690v1","category":"cs.CV"}
{"created":"2024-05-06 17:59:36","title":"Pose Priors from Language Models","abstract":"We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact.","sentences":["We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans.","Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   ","We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization.","Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions.","We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models.","Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact."],"url":"http://arxiv.org/abs/2405.03689v1","category":"cs.CV"}
{"created":"2024-05-06 17:57:27","title":"Language-Image Models with 3D Understanding","abstract":"Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning. Our project is available at https://janghyuncho.github.io/Cube-LLM.","sentences":["Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks.","We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space.","To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering.","Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D.","We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective.","Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information.","(2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats.","(3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists.","Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively.","Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.","Our project is available at https://janghyuncho.github.io/Cube-LLM."],"url":"http://arxiv.org/abs/2405.03685v1","category":"cs.CV"}
{"created":"2024-05-06 17:54:54","title":"AtomGPT: Atomistic Generative Pre-trained Transformer for Forward and Inverse Materials Design","abstract":"Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored. In this article, we introduce AtomGPT, a model specifically developed for materials design based on transformer architectures, to demonstrate the capability for both atomistic property prediction and structure generation. We show that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods and superconducting transition temperatures. Furthermore, we demonstrate that AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functional theory calculations. This work paves the way for leveraging LLMs in forward and inverse materials design, offering an efficient approach to the discovery and optimization of materials.","sentences":["Large language models (LLMs) such as generative pretrained transformers (GPTs) have shown potential for various commercial applications, but their applicability for materials design remains underexplored.","In this article, we introduce AtomGPT, a model specifically developed for materials design based on transformer architectures, to demonstrate the capability for both atomistic property prediction and structure generation.","We show that a combination of chemical and structural text descriptions can efficiently predict material properties with accuracy comparable to graph neural network models, including formation energies, electronic bandgaps from two different methods and superconducting transition temperatures.","Furthermore, we demonstrate that AtomGPT can generate atomic structures for tasks such as designing new superconductors, with the predictions validated through density functional theory calculations.","This work paves the way for leveraging LLMs in forward and inverse materials design, offering an efficient approach to the discovery and optimization of materials."],"url":"http://arxiv.org/abs/2405.03680v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 17:54:36","title":"Superstrings near the conformal boundary of $\\rm AdS_3$","abstract":"We study worldsheet sphere amplitudes, in the RNS formalism, in superstring theory on $\\text{AdS}_3\\times X$ with pure NS-NS flux using the near-boundary approximation. By computing a number of amplitudes at low-lying spectral flow, we deduce a candidate for a dual CFT for generic $X$. Specialising to $X={\\rm S^3}\\times \\mathbb{T}^4$ and with minimal NS-NS flux, i.e., to the tensionless limit, we explore the effect of the interaction term which we ignore in the near-boundary consideration. We show that amplitudes in the tensionless superstring theory on $\\rm AdS_3\\times S^3\\times\\mathbb{T}^4$ do not receive perturbative contributions from such an interaction term.","sentences":["We study worldsheet sphere amplitudes, in the RNS formalism, in superstring theory on $\\text{AdS}_3\\times X$ with pure NS-NS flux using the near-boundary approximation.","By computing a number of amplitudes at low-lying spectral flow, we deduce a candidate for a dual CFT for generic $X$. Specialising to $X={\\rm S^3}\\times \\mathbb{T}^4$ and with minimal NS-NS flux, i.e., to the tensionless limit, we explore the effect of the interaction term which we ignore in the near-boundary consideration.","We show that amplitudes in the tensionless superstring theory on $\\rm AdS_3\\times S^3\\times\\mathbb{T}^4$ do not receive perturbative contributions from such an interaction term."],"url":"http://arxiv.org/abs/2405.03678v1","category":"hep-th"}
{"created":"2024-05-06 17:53:33","title":"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis","abstract":"LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks. However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse. In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse. Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation.","sentences":["LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks.","However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse.","In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse.","Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation."],"url":"http://arxiv.org/abs/2405.03677v1","category":"cs.CL"}
{"created":"2024-05-06 17:49:31","title":"MemoryMamba: Memory-Augmented State Space Model for Defect Recognition","abstract":"As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows. Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings. These models especially struggle in scenarios involving limited or imbalanced defect data. In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models. MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training. Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection. In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities. The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios.","sentences":["As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows.","Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings.","These models especially struggle in scenarios involving limited or imbalanced defect data.","In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models.","MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training.","Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection.","In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities.","The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios."],"url":"http://arxiv.org/abs/2405.03673v1","category":"cs.CV"}
{"created":"2024-05-06 17:48:10","title":"Prompting Task Trees using Gemini: Methodologies and Insights","abstract":"Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient. The major challenge today is to train the robots exactly and empathetically using knowledge representation. This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them.","sentences":["Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient.","The major challenge today is to train the robots exactly and empathetically using knowledge representation.","This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them."],"url":"http://arxiv.org/abs/2405.03671v1","category":"cs.RO"}
{"created":"2024-05-06 17:43:34","title":"ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection","abstract":"Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms. For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/","sentences":["Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior.","Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play.","In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction.","Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions.","We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning.","Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.","For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/"],"url":"http://arxiv.org/abs/2405.03666v1","category":"cs.RO"}
{"created":"2024-05-06 17:42:18","title":"Distributed Estimation in Blockchain-aided Internet of Things in the Presence of Attacks","abstract":"Distributed estimation in a blockchain-aided Internet of Things (BIoT) is considered, where the integrated blockchain secures data exchanges across the BIoT and the storage of data at BIoT agents. This paper focuses on developing a performance guarantee for the distributed estimation in a BIoT in the presence of malicious attacks which jointly exploits vulnerabilities present in both IoT devices and the employed blockchain within the BIoT. To achieve this, we adopt the Cramer-Rao Bound (CRB) as the performance metric, and maximize the CRB for estimating the parameter of interest over the attack domain. However, the maximization problem is inherently non-convex, making it infeasible to obtain the globally optimal solution in general. To address this issue, we develop a relaxation method capable of transforming the original non-convex optimization problem into a convex optimization problem. Moreover, we derive the analytical expression for the optimal solution to the relaxed optimization problem. The optimal value of the relaxed optimization problem can be used to provide a valid estimation performance guarantee for the BIoT in the presence of attacks.","sentences":["Distributed estimation in a blockchain-aided Internet of Things (BIoT) is considered, where the integrated blockchain secures data exchanges across the BIoT and the storage of data at BIoT agents.","This paper focuses on developing a performance guarantee for the distributed estimation in a BIoT in the presence of malicious attacks which jointly exploits vulnerabilities present in both IoT devices and the employed blockchain within the BIoT. To achieve this, we adopt the Cramer-Rao Bound (CRB) as the performance metric, and maximize the CRB for estimating the parameter of interest over the attack domain.","However, the maximization problem is inherently non-convex, making it infeasible to obtain the globally optimal solution in general.","To address this issue, we develop a relaxation method capable of transforming the original non-convex optimization problem into a convex optimization problem.","Moreover, we derive the analytical expression for the optimal solution to the relaxed optimization problem.","The optimal value of the relaxed optimization problem can be used to provide a valid estimation performance guarantee for the BIoT in the presence of attacks."],"url":"http://arxiv.org/abs/2405.03665v1","category":"eess.SP"}
{"created":"2024-05-06 17:37:23","title":"CICA: Content-Injected Contrastive Alignment for Zero-Shot Document Image Classification","abstract":"Zero-shot learning has been extensively investigated in the broader field of visual recognition, attracting significant interest recently. However, the current work on zero-shot learning in document image classification remains scarce. The existing studies either focus exclusively on zero-shot inference, or their evaluation does not align with the established criteria of zero-shot evaluation in the visual recognition domain. We provide a comprehensive document image classification analysis in Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) settings to address this gap. Our methodology and evaluation align with the established practices of this domain. Additionally, we propose zero-shot splits for the RVL-CDIP dataset. Furthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances the zero-shot learning capabilities of CLIP. CICA consists of a novel 'content module' designed to leverage any generic document-related textual information. The discriminative features extracted by this module are aligned with CLIP's text and image features using a novel 'coupled-contrastive' loss. Our module improves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the RVL-CDIP dataset. Our module is lightweight and adds only 3.3% more parameters to CLIP. Our work sets the direction for future research in zero-shot document classification.","sentences":["Zero-shot learning has been extensively investigated in the broader field of visual recognition, attracting significant interest recently.","However, the current work on zero-shot learning in document image classification remains scarce.","The existing studies either focus exclusively on zero-shot inference, or their evaluation does not align with the established criteria of zero-shot evaluation in the visual recognition domain.","We provide a comprehensive document image classification analysis in Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) settings to address this gap.","Our methodology and evaluation align with the established practices of this domain.","Additionally, we propose zero-shot splits for the RVL-CDIP dataset.","Furthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances the zero-shot learning capabilities of CLIP.","CICA consists of a novel 'content module' designed to leverage any generic document-related textual information.","The discriminative features extracted by this module are aligned with CLIP's text and image features using a novel 'coupled-contrastive' loss.","Our module improves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the RVL-CDIP dataset.","Our module is lightweight and adds only 3.3% more parameters to CLIP.","Our work sets the direction for future research in zero-shot document classification."],"url":"http://arxiv.org/abs/2405.03660v1","category":"cs.CV"}
{"created":"2024-05-06 17:33:58","title":"A review on data-driven constitutive laws for solids","abstract":"This review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids. Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales. Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness. We attempt to provide a road map of how these can be reconciled in a data-availability-aware context. We also touch upon relevant aspects such as data sampling techniques, design of experiments, verification, and validation.","sentences":["This review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids.","Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales.","Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness.","We attempt to provide a road map of how these can be reconciled in a data-availability-aware context.","We also touch upon relevant aspects such as data sampling techniques, design of experiments, verification, and validation."],"url":"http://arxiv.org/abs/2405.03658v1","category":"cs.CE"}
{"created":"2024-05-06 17:28:31","title":"Spin-Hall conductivity and optical characteristics of noncentrosymmetric quantum spin Hall insulators: the case of PbBiI","abstract":"Quantum spin Hall insulators have attracted significant attention in recent years. Understanding the optical properties and spin Hall effect in these materials is crucial for technological advancements. In this study, we present theoretical analyses to explore the optical properties, Berry curvature and spin Hall conductivity of perturbed and pristine PbBiI using the linear combination of atomic orbitals and the Kubo formula. Our calculations reveal that the electronic structure can be modified using staggered exchange fields and electric fields, leading to changes in the optical properties. Additionally, spin Berry curvature and spin Hall conductivity are investigated in terms of various parameters. The results indicate that due to the small dynamical spin Hall conductivity, generating an ac spin current in PbBiI requires the use of external magnetic fields or magnetic materials.","sentences":["Quantum spin Hall insulators have attracted significant attention in recent years.","Understanding the optical properties and spin Hall effect in these materials is crucial for technological advancements.","In this study, we present theoretical analyses to explore the optical properties, Berry curvature and spin Hall conductivity of perturbed and pristine PbBiI using the linear combination of atomic orbitals and the Kubo formula.","Our calculations reveal that the electronic structure can be modified using staggered exchange fields and electric fields, leading to changes in the optical properties.","Additionally, spin Berry curvature and spin Hall conductivity are investigated in terms of various parameters.","The results indicate that due to the small dynamical spin Hall conductivity, generating an ac spin current in PbBiI requires the use of external magnetic fields or magnetic materials."],"url":"http://arxiv.org/abs/2405.03655v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 17:26:34","title":"Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent","abstract":"To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures. We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively. We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%. Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%. We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks.","sentences":["To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.","This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures.","We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively.","We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%.","Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%.","We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks."],"url":"http://arxiv.org/abs/2405.03654v1","category":"cs.CR"}
{"created":"2024-05-06 17:26:30","title":"Stability of backward-in-time semilinear coupled parabolic systems","abstract":"We consider backward problems for semilinear coupled parabolic systems in bounded domains. We prove conditional stability estimates for linear and semilinear systems of strongly coupled parabolic equations involving general semilinearities. The proof of the stability estimates relies on a modified method by Carleman estimates incorporating the simple weight function $e^{\\lambda t}$ with a sufficiently large parameter $\\lambda$.","sentences":["We consider backward problems for semilinear coupled parabolic systems in bounded domains.","We prove conditional stability estimates for linear and semilinear systems of strongly coupled parabolic equations involving general semilinearities.","The proof of the stability estimates relies on a modified method by Carleman estimates incorporating the simple weight function $e^{\\lambda t}$ with a sufficiently large parameter $\\lambda$."],"url":"http://arxiv.org/abs/2405.03653v1","category":"math.AP"}
{"created":"2024-05-06 17:23:42","title":"Field-of-View Extension for Diffusion MRI via Deep Generative Models","abstract":"Purpose: In diffusion MRI (dMRI), the volumetric and bundle analyses of whole-brain tissue microstructure and connectivity can be severely impeded by an incomplete field-of-view (FOV). This work aims to develop a method for imputing the missing slices directly from existing dMRI scans with an incomplete FOV. We hypothesize that the imputed image with complete FOV can improve the whole-brain tractography for corrupted data with incomplete FOV. Therefore, our approach provides a desirable alternative to discarding the valuable dMRI data, enabling subsequent tractography analyses that would otherwise be challenging or unattainable with corrupted data. Approach: We propose a framework based on a deep generative model that estimates the absent brain regions in dMRI scans with incomplete FOV. The model is capable of learning both the diffusion characteristics in diffusion-weighted images (DWI) and the anatomical features evident in the corresponding structural images for efficiently imputing missing slices of DWI outside of incomplete FOV. Results: For evaluating the imputed slices, on the WRAP dataset the proposed framework achieved PSNRb0=22.397, SSIMb0=0.905, PSNRb1300=22.479, SSIMb1300=0.893; on the NACC dataset it achieved PSNRb0=21.304, SSIMb0=0.892, PSNRb1300=21.599, SSIMb1300= 0.877. The proposed framework improved the tractography accuracy, as demonstrated by an increased average Dice score for 72 tracts (p < 0.001) on both the WRAP and NACC datasets. Conclusions: Results suggest that the proposed framework achieved sufficient imputation performance in dMRI data with incomplete FOV for improving whole-brain tractography, thereby repairing the corrupted data. Our approach achieved more accurate whole-brain tractography results with extended and complete FOV and reduced the uncertainty when analyzing bundles associated with Alzheimer's Disease.","sentences":["Purpose: In diffusion MRI (dMRI), the volumetric and bundle analyses of whole-brain tissue microstructure and connectivity can be severely impeded by an incomplete field-of-view (FOV).","This work aims to develop a method for imputing the missing slices directly from existing dMRI scans with an incomplete FOV.","We hypothesize that the imputed image with complete FOV can improve the whole-brain tractography for corrupted data with incomplete FOV.","Therefore, our approach provides a desirable alternative to discarding the valuable dMRI data, enabling subsequent tractography analyses that would otherwise be challenging or unattainable with corrupted data.","Approach:","We propose a framework based on a deep generative model that estimates the absent brain regions in dMRI scans with incomplete FOV.","The model is capable of learning both the diffusion characteristics in diffusion-weighted images (DWI) and the anatomical features evident in the corresponding structural images for efficiently imputing missing slices of DWI outside of incomplete FOV.","Results:","For evaluating the imputed slices, on the WRAP dataset the proposed framework achieved PSNRb0=22.397, SSIMb0=0.905, PSNRb1300=22.479, SSIMb1300=0.893; on the NACC dataset it achieved PSNRb0=21.304, SSIMb0=0.892, PSNRb1300=21.599, SSIMb1300= 0.877.","The proposed framework improved the tractography accuracy, as demonstrated by an increased average Dice score for 72 tracts (p < 0.001) on both the WRAP and NACC datasets.","Conclusions: Results suggest that the proposed framework achieved sufficient imputation performance in dMRI data with incomplete FOV for improving whole-brain tractography, thereby repairing the corrupted data.","Our approach achieved more accurate whole-brain tractography results with extended and complete FOV and reduced the uncertainty when analyzing bundles associated with Alzheimer's Disease."],"url":"http://arxiv.org/abs/2405.03652v1","category":"cs.CV"}
{"created":"2024-05-06 17:14:09","title":"Generated Contents Enrichment","abstract":"In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE). Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant. Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment. Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship. We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects. Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation. Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results.","sentences":["In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE).","Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant.","Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment.","Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship.","We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects.","Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation.","Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results."],"url":"http://arxiv.org/abs/2405.03650v1","category":"cs.CV"}
{"created":"2024-05-06 17:12:21","title":"Learning Robust Classifiers with Self-Guided Spurious Correlation Mitigation","abstract":"Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability. Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get. In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework. Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations. The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space. We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions. We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets.","sentences":["Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability.","Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get.","In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework.","Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations.","The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space.","We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions.","We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets."],"url":"http://arxiv.org/abs/2405.03649v1","category":"cs.LG"}
{"created":"2024-05-06 17:08:35","title":"Calculating HOMFLY-PT polynomials on a photonic processor","abstract":"In this paper we discuss an approach to calculate knot polynomials on a photonic processor. Calculations of knot polynomials is a computationally difficult problem and therefore it is interesting to use new advanced calculation methods to find them. Here we present a proof of concept by calculating the simplest knot polynomial of the trefoil knot in fundamental representation. This approach, however, can easily be generalized to more complex knots and representations. Same operators can also be realized on a quantum computer with the same effect.","sentences":["In this paper we discuss an approach to calculate knot polynomials on a photonic processor.","Calculations of knot polynomials is a computationally difficult problem and therefore it is interesting to use new advanced calculation methods to find them.","Here we present a proof of concept by calculating the simplest knot polynomial of the trefoil knot in fundamental representation.","This approach, however, can easily be generalized to more complex knots and representations.","Same operators can also be realized on a quantum computer with the same effect."],"url":"http://arxiv.org/abs/2405.03645v1","category":"quant-ph"}
{"created":"2024-05-06 17:07:28","title":"When LLMs Meet Cybersecurity: A Systematic Literature Review","abstract":"The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin. We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.","sentences":["The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies.","Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area.","This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios.","Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area.","This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin.","We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity."],"url":"http://arxiv.org/abs/2405.03644v1","category":"cs.CR"}
{"created":"2024-05-06 17:06:32","title":"Collecting Consistently High Quality Object Tracks with Minimal Human Involvement by Using Self-Supervised Learning to Detect Tracker Errors","abstract":"We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input. The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking. Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails. Since labeled data is not needed, our approach can be applied to novel object categories. Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects.","sentences":["We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input.","The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking.","Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails.","Since labeled data is not needed, our approach can be applied to novel object categories.","Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects."],"url":"http://arxiv.org/abs/2405.03643v1","category":"cs.CV"}
{"created":"2024-05-06 16:55:20","title":"Federated Learning Privacy: Attacks, Defenses, Applications, and Policy Landscape - A Survey","abstract":"Deep learning has shown incredible potential across a vast array of tasks and accompanying this growth has been an insatiable appetite for data. However, a large amount of data needed for enabling deep learning is stored on personal devices and recent concerns on privacy have further highlighted challenges for accessing such data. As a result, federated learning (FL) has emerged as an important privacy-preserving technology enabling collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server. However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \"reverse engineered\" to infer information about the private training data. It has been shown under a wide variety of settings that this premise for privacy does {\\em not} hold.   In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL. We identify the current limitations of these attacks and highlight the settings in which FL client privacy can be broken. We dissect some of the successful industry applications of FL and draw lessons for future successful adoption. We survey the emerging landscape of privacy regulation for FL. We conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants.","sentences":["Deep learning has shown incredible potential across a vast array of tasks and accompanying this growth has been an insatiable appetite for data.","However, a large amount of data needed for enabling deep learning is stored on personal devices and recent concerns on privacy have further highlighted challenges for accessing such data.","As a result, federated learning (FL) has emerged as an important privacy-preserving technology enabling collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server.","However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \"reverse engineered\" to infer information about the private training data.","It has been shown under a wide variety of settings that this premise for privacy does {\\em not} hold.   ","In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL.","We identify the current limitations of these attacks and highlight the settings in which FL client privacy can be broken.","We dissect some of the successful industry applications of FL and draw lessons for future successful adoption.","We survey the emerging landscape of privacy regulation for FL.","We conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants."],"url":"http://arxiv.org/abs/2405.03636v1","category":"cs.CR"}
{"created":"2024-05-06 16:45:34","title":"Investigating the Hubble Tension and $\u03c3_8$ Discrepancy in f(Q) Cosmology","abstract":"In this study, we incorporated a three-parameter family, of the metric incompatible modification of standard general relativity $f(Q)$ models into the Boltzmann code MGCLASS at both the background and perturbation levels, in order to conduct a Bayesian study employing probes that include the cosmic microwave background (CMB), baryon acoustic oscillations (BAO), weak lensing (WL), alone or its correlation with galaxy clustering (3$\\times$2pt) and growth measurements $f \\sigma_8$, for each submodel. Our analysis focused on the impact of the Hubble tension in $H_0$ and the discrepancy in $\\sigma_8$ resulting from the inclusion of our model parameters, namely $M$, $\\alpha$ and $\\beta$. We find that none of the sub models, considered alone or combined, were able of alleviating the Hubble tension with only reducing it to 3 $\\sigma$ in the least constraining, highest degree of freedom case while we found that the $\\sigma_8$ discrepancy, already strongly mitigated on WL linear scales, especially when we let all our model's parameters as free, appears again when considering the more constraining 3$\\times$2pt probe. Among the parameters considered, we found that $\\beta$, acting in scaling both the gravitational and the Hubble parameter, had the most impact in reducing the discrepancy, with data preferring far from $\\Lambda$CDM alike values, before the combination with $f \\sigma_8$ constrain it back to its general relativity values.","sentences":["In this study, we incorporated a three-parameter family, of the metric incompatible modification of standard general relativity $f(Q)$ models into the Boltzmann code MGCLASS at both the background and perturbation levels, in order to conduct a Bayesian study employing probes that include the cosmic microwave background (CMB), baryon acoustic oscillations (BAO), weak lensing (WL), alone or its correlation with galaxy clustering (3$\\times$2pt) and growth measurements $f \\sigma_8$, for each submodel.","Our analysis focused on the impact of the Hubble tension in $H_0$ and the discrepancy in $\\sigma_8$ resulting from the inclusion of our model parameters, namely $M$, $\\alpha$ and $\\beta$. We find that none of the sub models, considered alone or combined, were able of alleviating the Hubble tension with only reducing it to 3 $\\sigma$ in the least constraining, highest degree of freedom case while we found that the $\\sigma_8$ discrepancy, already strongly mitigated on WL linear scales, especially when we let all our model's parameters as free, appears again when considering the more constraining 3$\\times$2pt probe.","Among the parameters considered, we found that $\\beta$, acting in scaling both the gravitational and the Hubble parameter, had the most impact in reducing the discrepancy, with data preferring far from $\\Lambda$CDM alike values, before the combination with $f \\sigma_8$ constrain it back to its general relativity values."],"url":"http://arxiv.org/abs/2405.03627v1","category":"astro-ph.CO"}
{"created":"2024-05-06 16:42:36","title":"From counting blocks to the Lebesgue measure, with an application to the Allouche-Hu-Morin limit theorem on block-constrained harmonic series","abstract":"We consider the harmonic series $S(k)=\\sum^{(k)} m^{-1}$ over the integers having $k$ occurrences of a given block of $b$-ary digits, of length $p$, and relate them to certain measures on the interval $[0,1)$. We show that these measures converge weakly to $b^p$ times the Lebesgue measure, a fact which allows a new proof of the theorem of Allouche, Hu, and Morin which says $\\lim S(k)=b^p\\log(b)$. A quantitative error estimate will be given. Combinatorial aspects involve generating series which fall under the scope of the Goulden-Jackson cluster generating function formalism and the work of Guibas-Odlyzko on string overlaps.","sentences":["We consider the harmonic series $S(k)=\\sum^{(k)} m^{-1}$ over the integers having $k$ occurrences of a given block of $b$-ary digits, of length $p$, and relate them to certain measures on the interval $","[0,1)$. We show that these measures converge weakly to $b^p$ times the Lebesgue measure, a fact which allows a new proof of the theorem of Allouche, Hu, and Morin which says $\\lim S(k)=b^p\\log(b)$.","A quantitative error estimate will be given.","Combinatorial aspects involve generating series which fall under the scope of the Goulden-Jackson cluster generating function formalism and the work of Guibas-Odlyzko on string overlaps."],"url":"http://arxiv.org/abs/2405.03625v1","category":"math.NT"}
{"created":"2024-05-06 16:41:13","title":"On Reference Frames and Coordinate Transformations","abstract":"This article explores the differences between frame and coordinate transformations in relativistic theories. We highlight the key role of tetrad fields in connecting spacetime and frame indices. Using Maxwell's electrodynamics as an example, we show that Maxwell's equations are invariant under coordinate transformations but exhibit covariant behavior under frame transformations. We also analyze the energy-momentum of an electromagnetic field in different frames, providing deeper insights into the implications of different frames of reference and coordinate systems.","sentences":["This article explores the differences between frame and coordinate transformations in relativistic theories.","We highlight the key role of tetrad fields in connecting spacetime and frame indices.","Using Maxwell's electrodynamics as an example, we show that Maxwell's equations are invariant under coordinate transformations but exhibit covariant behavior under frame transformations.","We also analyze the energy-momentum of an electromagnetic field in different frames, providing deeper insights into the implications of different frames of reference and coordinate systems."],"url":"http://arxiv.org/abs/2405.03623v1","category":"gr-qc"}
{"created":"2024-05-06 16:36:20","title":"Uniform bounds for fields of definition in projective spaces","abstract":"We give a positive answer to a question of J. Doyle and J. Silverman about fields of definition of dynamical systems on $\\mathbb{P}^{n}$. We prove that, for fixed $n$, there exists a constant $C_{n}$ such that every dynamical system $\\mathbb{P}^{n}\\to\\mathbb{P}^{n}$ is defined over an extension of degree $\\le C_{n}$ of the field of moduli. More generally, the same bound works for any kind of \"algebraic structure\" defined over $\\mathbb{P}^{n}$, such as embedded curves, hypersurfaces, algebraic cycles. As a consequence we prove that, if $x\\in X(k)$ is a rational point of an $n$-dimensional variety with quotient singularities, there exists a field extension $k'/k$ of degree $\\le C_{n-1}$ such that $x$ lifts to a $k'$-rational point of any resolution of singularities.","sentences":["We give a positive answer to a question of J. Doyle and J. Silverman about fields of definition of dynamical systems on $\\mathbb{P}^{n}$. We prove that, for fixed $n$, there exists a constant $C_{n}$ such that every dynamical system $\\mathbb{P}^{n}\\to\\mathbb{P}^{n}$ is defined over an extension of degree $\\le C_{n}$ of the field of moduli.","More generally, the same bound works for any kind of \"algebraic structure\" defined over $\\mathbb{P}^{n}$, such as embedded curves, hypersurfaces, algebraic cycles.","As a consequence we prove that, if $x\\in X(k)$ is a rational point of an $n$-dimensional variety with quotient singularities, there exists a field extension $k'/k$ of degree $\\le C_{n-1}$ such that $x$ lifts to a $k'$-rational point of any resolution of singularities."],"url":"http://arxiv.org/abs/2405.03621v1","category":"math.NT"}
{"created":"2024-05-06 16:35:56","title":"Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid","abstract":"As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies. This has driven a rising interest in automated machine learning solutions. Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success. In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture. Overall, BERTroid emerged as a promising solution for combating Android malware. Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks. Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios. In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems. While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors. This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings.","sentences":["As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies.","This has driven a rising interest in automated machine learning solutions.","Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success.","In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture.","Overall, BERTroid emerged as a promising solution for combating Android malware.","Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks.","Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios.","In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems.","While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors.","This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings."],"url":"http://arxiv.org/abs/2405.03620v1","category":"cs.CR"}
{"created":"2024-05-06 16:34:24","title":"A reduction procedure for determining exact solutions of second order hyperbolic equations","abstract":"In this paper we develop a systematic reduction procedure for determining intermediate integrals of second order hyperbolic equations so that exact solutions of the second order PDEs under interest can be obtained by solving first order PDEs. We give some conditions in order that such a procedure holds and, in particular, we characterize classes of linear second order hyperbolic equations for which the general solution can be found.","sentences":["In this paper we develop a systematic reduction procedure for determining intermediate integrals of second order hyperbolic equations so that exact solutions of the second order PDEs under interest can be obtained by solving first order PDEs.","We give some conditions in order that such a procedure holds and, in particular, we characterize classes of linear second order hyperbolic equations for which the general solution can be found."],"url":"http://arxiv.org/abs/2405.03617v1","category":"math-ph"}
{"created":"2024-05-06 16:32:29","title":"A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama","abstract":"Context. Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code. LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development. Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software. However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools. Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal. In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   Method. We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python. We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures. Therefore, we execute both implementations and profile their energy efficiency.   Results. Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand. Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart. Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   Conclusions. According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so. Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development.","sentences":["Context.","Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code.","LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development.","Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software.","However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools.","Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal.","In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   ","Method.","We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python.","We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures.","Therefore, we execute both implementations and profile their energy efficiency.   Results.","Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand.","Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart.","Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   ","Conclusions.","According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so.","Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development."],"url":"http://arxiv.org/abs/2405.03616v1","category":"cs.SE"}
{"created":"2024-05-06 16:31:19","title":"Dual Relation Mining Network for Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) aims to recognize novel classes through transferring shared semantic knowledge (e.g., attributes) from seen classes to unseen classes. Recently, attention-based methods have exhibited significant progress which align visual features and attributes via a spatial attention mechanism. However, these methods only explore visual-semantic relationship in the spatial dimension, which can lead to classification ambiguity when different attributes share similar attention regions, and semantic relationship between attributes is rarely discussed. To alleviate the above problems, we propose a Dual Relation Mining Network (DRMN) to enable more effective visual-semantic interactions and learn semantic relationship among attributes for knowledge transfer. Specifically, we introduce a Dual Attention Block (DAB) for visual-semantic relationship mining, which enriches visual information by multi-level feature fusion and conducts spatial attention for visual to semantic embedding. Moreover, an attribute-guided channel attention is utilized to decouple entangled semantic features. For semantic relationship modeling, we utilize a Semantic Interaction Transformer (SIT) to enhance the generalization of attribute representations among images. Additionally, a global classification branch is introduced as a complement to human-defined semantic attributes, and we then combine the results with attribute-based classification. Extensive experiments demonstrate that the proposed DRMN leads to new state-of-the-art performances on three standard ZSL benchmarks, i.e., CUB, SUN, and AwA2.","sentences":["Zero-shot learning (ZSL) aims to recognize novel classes through transferring shared semantic knowledge (e.g., attributes) from seen classes to unseen classes.","Recently, attention-based methods have exhibited significant progress which align visual features and attributes via a spatial attention mechanism.","However, these methods only explore visual-semantic relationship in the spatial dimension, which can lead to classification ambiguity when different attributes share similar attention regions, and semantic relationship between attributes is rarely discussed.","To alleviate the above problems, we propose a Dual Relation Mining Network (DRMN) to enable more effective visual-semantic interactions and learn semantic relationship among attributes for knowledge transfer.","Specifically, we introduce a Dual Attention Block (DAB) for visual-semantic relationship mining, which enriches visual information by multi-level feature fusion and conducts spatial attention for visual to semantic embedding.","Moreover, an attribute-guided channel attention is utilized to decouple entangled semantic features.","For semantic relationship modeling, we utilize a Semantic Interaction Transformer (SIT) to enhance the generalization of attribute representations among images.","Additionally, a global classification branch is introduced as a complement to human-defined semantic attributes, and we then combine the results with attribute-based classification.","Extensive experiments demonstrate that the proposed DRMN leads to new state-of-the-art performances on three standard ZSL benchmarks, i.e., CUB, SUN, and AwA2."],"url":"http://arxiv.org/abs/2405.03613v1","category":"cs.CV"}
{"created":"2024-05-06 16:23:12","title":"Decision algorithms for reversibility of one-dimensional non-linear cellular automata under null boundary conditions","abstract":"The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata. For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present. The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm. This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions. We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions. We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions. These decision algorithms work for not only linear rules but also non-linear rules. In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain. Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper.","sentences":["The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata.","For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present.","The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm.","This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions.","We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions.","We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions.","These decision algorithms work for not only linear rules but also non-linear rules.","In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain.","Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper."],"url":"http://arxiv.org/abs/2405.03609v1","category":"cs.CC"}
{"created":"2024-05-06 16:17:53","title":"Observability of spin precession in the presence of a black-hole remnant kick","abstract":"Remnants of binary black-hole mergers can gain significant recoil or kick velocities when the binaries are asymmetric. The kick is the consequence of anisotropic emission of gravitational waves, which may leave a characteristic imprint in the observed signal. So far, only one gravitational-wave event supports a non-zero kick velocity: GW200129_065458. This signal is also the first to show evidence for spin-precession. For most other gravitational-wave observations, spin orientations are poorly constrained as this would require large signal-to-noise ratios, unequal mass ratios or inclined systems. Here we investigate whether the imprint of the kick can help to extract more information about the spins. We perform an injection and recovery study comparing binary black-hole signals with significantly different kick magnitudes, but the same spin magnitudes and spin tilts. To exclude the impact of higher signal harmonics in parameter estimation, we focus on equal-mass binaries that are oriented face-on. We generate signals with PhenomXO4a, which includes mode asymmetries. These asymmetries are the main cause for the kick in precessing binaries. For comparison with an equivalent model without asymmetries, we repeat the same injections with PhenomXPHM. We find that signals with large kicks necessarily include large asymmetries, and these give more structure to the signal, leading to more informative measurements of the spins and mass ratio. Our results also complement previous findings that argued precession in equal-mass, face-on or face-away binaries is nearly impossible to identify. In contrast, we find that in the presence of a remnant kick, even those signals become more informative and allow determining precession with signal-to-noise ratios observable already by current gravitational-wave detectors.","sentences":["Remnants of binary black-hole mergers can gain significant recoil or kick velocities when the binaries are asymmetric.","The kick is the consequence of anisotropic emission of gravitational waves, which may leave a characteristic imprint in the observed signal.","So far, only one gravitational-wave event supports a non-zero kick velocity: GW200129_065458.","This signal is also the first to show evidence for spin-precession.","For most other gravitational-wave observations, spin orientations are poorly constrained as this would require large signal-to-noise ratios, unequal mass ratios or inclined systems.","Here we investigate whether the imprint of the kick can help to extract more information about the spins.","We perform an injection and recovery study comparing binary black-hole signals with significantly different kick magnitudes, but the same spin magnitudes and spin tilts.","To exclude the impact of higher signal harmonics in parameter estimation, we focus on equal-mass binaries that are oriented face-on.","We generate signals with PhenomXO4a, which includes mode asymmetries.","These asymmetries are the main cause for the kick in precessing binaries.","For comparison with an equivalent model without asymmetries, we repeat the same injections with PhenomXPHM.","We find that signals with large kicks necessarily include large asymmetries, and these give more structure to the signal, leading to more informative measurements of the spins and mass ratio.","Our results also complement previous findings that argued precession in equal-mass, face-on or face-away binaries is nearly impossible to identify.","In contrast, we find that in the presence of a remnant kick, even those signals become more informative and allow determining precession with signal-to-noise ratios observable already by current gravitational-wave detectors."],"url":"http://arxiv.org/abs/2405.03607v1","category":"gr-qc"}
{"created":"2024-05-06 16:17:33","title":"Trackable Island-model Genetic Algorithms at Wafer Scale","abstract":"Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation. However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform. We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million. This pace enables quadrillions of evaluations a day. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable. Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community.","sentences":["Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation.","However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform.","We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million.","This pace enables quadrillions of evaluations a day.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable.","Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community."],"url":"http://arxiv.org/abs/2405.03605v1","category":"cs.NE"}
{"created":"2024-05-06 16:17:21","title":"MV-frames","abstract":"Complete MV-algebras are naturally equipped with frame structures. We call them MV-frames and investigate some of their main the properties as frames. We completely characterized algebraic MV-frames as well as regular MV-frames. In addition, we consider nuclei on MV-frames in general and on MV-frames of ideals of Lukasiewicz rings. Finally, we used the Chang-Mundici functor to explore the frame structures of complete unital lattice-ordered groups.","sentences":["Complete MV-algebras are naturally equipped with frame structures.","We call them MV-frames and investigate some of their main the properties as frames.","We completely characterized algebraic MV-frames as well as regular MV-frames.","In addition, we consider nuclei on MV-frames in general and on MV-frames of ideals of Lukasiewicz rings.","Finally, we used the Chang-Mundici functor to explore the frame structures of complete unital lattice-ordered groups."],"url":"http://arxiv.org/abs/2405.03604v1","category":"math.LO"}
{"created":"2024-05-06 16:16:57","title":"Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under the framework of the generalized linear mixed model","abstract":"Publication bias (PB) is one of the serious issues in meta-analysis. Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels. For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly. Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended. However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model. The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model. We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model. An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model.","sentences":["Publication bias (PB) is one of the serious issues in meta-analysis.","Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels.","For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly.","Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended.","However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM.","In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model.","The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model.","We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model.","An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model."],"url":"http://arxiv.org/abs/2405.03603v1","category":"stat.ME"}
{"created":"2024-05-06 16:14:59","title":"One nose but two nostrils: Learn to align with sparse connections between two olfactory cortices","abstract":"The integration of neural representations in the two hemispheres is an important problem in neuroscience. Recent experiments revealed that odor responses in cortical neurons driven by separate stimulation of the two nostrils are highly correlated. This bilateral alignment points to structured inter-hemispheric connections, but detailed mechanism remains unclear. Here, we hypothesized that continuous exposure to environmental odors shapes these projections and modeled it as online learning with local Hebbian rule. We found that Hebbian learning with sparse connections achieves bilateral alignment, exhibiting a linear trade-off between speed and accuracy. We identified an inverse scaling relationship between the number of cortical neurons and the inter-hemispheric projection density required for desired alignment accuracy, i.e., more cortical neurons allow sparser inter-hemispheric projections. We next compared the alignment performance of local Hebbian rule and the global stochastic-gradient-descent (SGD) learning for artificial neural networks. We found that although SGD leads to the same alignment accuracy with modestly sparser connectivity, the same inverse scaling relation holds. We showed that their similar performance originates from the fact that the update vectors of the two learning rules align significantly throughout the learning process. This insight may inspire efficient sparse local learning algorithms for more complex problems.","sentences":["The integration of neural representations in the two hemispheres is an important problem in neuroscience.","Recent experiments revealed that odor responses in cortical neurons driven by separate stimulation of the two nostrils are highly correlated.","This bilateral alignment points to structured inter-hemispheric connections, but detailed mechanism remains unclear.","Here, we hypothesized that continuous exposure to environmental odors shapes these projections and modeled it as online learning with local Hebbian rule.","We found that Hebbian learning with sparse connections achieves bilateral alignment, exhibiting a linear trade-off between speed and accuracy.","We identified an inverse scaling relationship between the number of cortical neurons and the inter-hemispheric projection density required for desired alignment accuracy, i.e., more cortical neurons allow sparser inter-hemispheric projections.","We next compared the alignment performance of local Hebbian rule and the global stochastic-gradient-descent (SGD) learning for artificial neural networks.","We found that although SGD leads to the same alignment accuracy with modestly sparser connectivity, the same inverse scaling relation holds.","We showed that their similar performance originates from the fact that the update vectors of the two learning rules align significantly throughout the learning process.","This insight may inspire efficient sparse local learning algorithms for more complex problems."],"url":"http://arxiv.org/abs/2405.03602v1","category":"q-bio.NC"}
{"created":"2024-05-06 16:14:19","title":"Firing rate model for brain rhythms controlled by astrocytes","abstract":"We propose a new mean-field model of brain rhythms governed by astrocytes. This theoretical framework describes how astrocytes can regulate neuronal activity and contribute to the generation of brain rhythms. The model describes at the population level the interactions between two large groups of excitatory and inhibitory neurons. The excitatory population is governed by astrocytes via a so-called tripartite synapse. This approach allows us to describe how the interactions between different groups of neurons and astrocytes can give rise to various patterns of synchronized activity and transitions between them. Using methods of nonlinear analysis we show that astrocytic modulation can lead to a change in the period and amplitude of oscillations in the populations of neurons.","sentences":["We propose a new mean-field model of brain rhythms governed by astrocytes.","This theoretical framework describes how astrocytes can regulate neuronal activity and contribute to the generation of brain rhythms.","The model describes at the population level the interactions between two large groups of excitatory and inhibitory neurons.","The excitatory population is governed by astrocytes via a so-called tripartite synapse.","This approach allows us to describe how the interactions between different groups of neurons and astrocytes can give rise to various patterns of synchronized activity and transitions between them.","Using methods of nonlinear analysis we show that astrocytic modulation can lead to a change in the period and amplitude of oscillations in the populations of neurons."],"url":"http://arxiv.org/abs/2405.03601v1","category":"q-bio.NC"}
{"created":"2024-05-06 16:13:52","title":"Flexible terahertz metasurface absorbers empowered by bound states in the continuum","abstract":"Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities. It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility. Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view. The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties. Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis. The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands. Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces.","sentences":["Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities.","It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility.","Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view.","The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties.","Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis.","The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands.","Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces."],"url":"http://arxiv.org/abs/2405.03600v1","category":"physics.optics"}
{"created":"2024-05-06 16:04:03","title":"GREEN: Generative Radiology Report Evaluation and Error Notation","abstract":"Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively. Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts. We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts. Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\"","sentences":["Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images.","Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph).","In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively.","Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts.","We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts.","Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\""],"url":"http://arxiv.org/abs/2405.03595v1","category":"cs.CL"}
{"created":"2024-05-06 16:03:32","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment","abstract":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","sentences":["Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks.","We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity.","We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset.","We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling.","In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine.","The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization.","Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x.","We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality.","This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy."],"url":"http://arxiv.org/abs/2405.03594v1","category":"cs.CL"}
{"created":"2024-05-06 16:02:56","title":"Rectifiable Reifenberg and uniform positivity under almost calibrations","abstract":"The Reifenberg theorem \\cite{reif_orig} tells us that if a set $S\\subseteq B_2\\subseteq \\mathbb R^n$ is uniformly close on all points and scales to a $k$-dimensional subspace, then $S$ is H\\\"older homeomorphic to a $k$-dimensional Euclidean ball. In general this is sharp, for instance such an $S$ may have infinite volume, be fractal in nature, and have no rectifiable structure.   The goal of this note is to show that we can improve upon this for an almost calibrated Reifenberg set, or more generally under a positivity condition in the context of an $\\epsilon$-calibration $\\Omega$ . An $\\epsilon$-calibration is very general, the condition holds locally for all continuous $k$-forms such that $\\Omega[L]\\leq 1+\\epsilon$ for all $k$-planes $L$. We say an oriented $k$-plane $L$ is $\\alpha$-positive with respect to $\\Omega$ if $\\Omega[L]>\\alpha>0$. If $\\Omega[L]>\\alpha> 1-\\epsilon$ then we call $L$ an $\\epsilon$-calibrated plane.   The main result of this paper is then the following. Assume at all points and scales $B_r(x)\\subseteq B_2$ that $S$ is $\\delta$-Hausdorff close to a subspace $L_{x,r}$ which is uniformly positive $\\Omega[L_{x,r}]>\\alpha $ with respect to an $\\epsilon$-calibration. Then $S$ is $k$-rectifiable with uniform volume bounds.","sentences":["The Reifenberg theorem \\cite{reif_orig} tells us that if a set $S\\subseteq B_2\\subseteq \\mathbb R^n$ is uniformly close on all points and scales to a $k$-dimensional subspace, then $S$ is H\\\"older homeomorphic to a $k$-dimensional Euclidean ball.","In general this is sharp, for instance such an $S$ may have infinite volume, be fractal in nature, and have no rectifiable structure.   ","The goal of this note is to show that we can improve upon this for an almost calibrated Reifenberg set, or more generally under a positivity condition in the context of an $\\epsilon$-calibration $\\Omega$ .","An $\\epsilon$-calibration is very general, the condition holds locally for all continuous $k$-forms such that $\\Omega[L]\\leq 1+\\epsilon$ for all $k$-planes $L$. We say an oriented $k$-plane $L$ is $\\alpha$-positive with respect to $\\Omega$ if $\\Omega[L]>\\alpha>0$. If $\\Omega[L]>\\alpha> 1-\\epsilon$ then we call $L$ an $\\epsilon$-calibrated plane.   ","The main result of this paper is then the following.","Assume at all points and scales $B_r(x)\\subseteq B_2$ that $S$ is $\\delta$-Hausdorff close to a subspace $L_{x,r}$ which is uniformly positive $\\Omega[L_{x,r}]>\\alpha $ with respect to an $\\epsilon$-calibration.","Then $S$ is $k$-rectifiable with uniform volume bounds."],"url":"http://arxiv.org/abs/2405.03593v1","category":"math.AP"}
{"created":"2024-05-06 16:02:12","title":"Emergence of Cosmic Space and Horizon Thermodynamics from Kaniadakis Entropy","abstract":"Utilizing Kaniadakis entropy associated with the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe and applying the emergence of cosmic space paradigm, we deduce the modified Friedmann equation for a non-flat (n+1)-dimensional universe. Employing the first law of thermodynamics, we arrive at the same modified Friedmann equation, showing the connection between emergence of cosmic space and first law of thermodynamics. We also establish the condition to satisfy the Generalized second law of thermodynamics within the Kaniadakis framework. Our study illuminates the intricate connection between the law of emergence and horizon thermodynamics, offering a deeper insight through the lens of Kaniadakis entropy.","sentences":["Utilizing Kaniadakis entropy associated with the apparent horizon of the Friedmann-Robertson-Walker (FRW) Universe and applying the emergence of cosmic space paradigm, we deduce the modified Friedmann equation for a non-flat (n+1)-dimensional universe.","Employing the first law of thermodynamics, we arrive at the same modified Friedmann equation, showing the connection between emergence of cosmic space and first law of thermodynamics.","We also establish the condition to satisfy the Generalized second law of thermodynamics within the Kaniadakis framework.","Our study illuminates the intricate connection between the law of emergence and horizon thermodynamics, offering a deeper insight through the lens of Kaniadakis entropy."],"url":"http://arxiv.org/abs/2405.03592v1","category":"gr-qc"}
{"created":"2024-05-06 15:59:46","title":"Effective Quadratic Error Bounds for Floating-Point Algorithms Computing the Hypotenuse Function","abstract":"We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers. The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off. Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions. We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms. Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases. An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area. This is particularly important when using low precision formats, which are increasingly common in modern processors. Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging.","sentences":["We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers.","The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off.","Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions.","We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms.","Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases.","An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area.","This is particularly important when using low precision formats, which are increasingly common in modern processors.","Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging."],"url":"http://arxiv.org/abs/2405.03588v1","category":"math.NA"}
{"created":"2024-05-06 15:55:18","title":"A GPU-Accelerated Interior Point Method for Radiation Therapy Optimization","abstract":"Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case. In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints. In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions. The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations. Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world. RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration. GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work. We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases.","sentences":["Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case.","In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints.","In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions.","The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations.","Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   ","The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world.","RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration.","GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work.","We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases."],"url":"http://arxiv.org/abs/2405.03584v1","category":"math.OC"}
{"created":"2024-05-06 15:53:26","title":"From 1 to infinity: The log-correction for the maximum of variable-speed branching Brownian motion","abstract":"We study the extremes of variable speed branching Brownian motion (BBM) where the time-dependent \"speed functions\", which describe the time-inhomogeneous variance, converge to the identity function.   We consider general speed functions lying strictly below their concave hull and piecewise linear, concave speed functions.   In the first case, the log-correction for the order of the maximum depends only on the rate of convergence of the speed function near 0 and 1 and exhibits a smooth interpolation between the correction in the i.i.d. case, $\\frac{1}{2\\sqrt{2}} \\ln t$, and that of standard BBM, $\\frac{3}{2\\sqrt{2}} \\ln t$.   In the second case, we describe the order of the maximum in dependence of the form of speed function and show that any log-correction larger than $\\frac{3}{2\\sqrt{2}} \\ln t$ can be obtained.   In both cases, we prove that the limiting law of the maximum and the extremal process essentially coincide with those of standard BBM, using a first and second moment method which relies on the localisation of extremal particles.   This extends the results of Bovier and Hartung for two-speed BBM.","sentences":["We study the extremes of variable speed branching Brownian motion (BBM) where the time-dependent \"speed functions\", which describe the time-inhomogeneous variance, converge to the identity function.   ","We consider general speed functions lying strictly below their concave hull and piecewise linear, concave speed functions.   ","In the first case, the log-correction for the order of the maximum depends only on the rate of convergence of the speed function near 0 and 1 and exhibits a smooth interpolation between the correction in the i.i.d. case, $\\frac{1}{2\\sqrt{2}} \\ln t$, and that of standard BBM, $\\frac{3}{2\\sqrt{2}} \\ln t$.   ","In the second case, we describe the order of the maximum in dependence of the form of speed function and show that any log-correction larger than $\\frac{3}{2\\sqrt{2}} \\ln t$ can be obtained.   ","In both cases, we prove that the limiting law of the maximum and the extremal process essentially coincide with those of standard BBM, using a first and second moment method which relies on the localisation of extremal particles.   ","This extends the results of Bovier and Hartung for two-speed BBM."],"url":"http://arxiv.org/abs/2405.03580v1","category":"math.PR"}
{"created":"2024-05-06 15:52:17","title":"Equivariant algebraic $\\mathrm{K}$-theory and Artin $L$-functions","abstract":"In this paper, we generalize the Quillen-Lichtenbaum Conjecture relating special values of Dedekind zeta functions to algebraic $\\mathrm{K}$-groups. The former has been settled by Rost-Voevodsky up to the Iwasawa Main Conjecture. Our generalization extends the scope of this conjecture to Artin $L$-functions of Galois representations of finite, function, and totally real number fields. The statement of this conjecture relates norms of the special values of these $L$-functions to sizes of equivariant algebraic $\\mathrm{K}$-groups with coefficients in an equivariant Moore spectrum attached to a Galois representation.   We prove this conjecture in many cases, integrally, except up to a possible factor of powers of $2$ in the non-abelian and totally real number field case. In the finite field case, we further determine the group structures of their equivariant algebraic $\\mathrm{K}$-groups with coefficients in Galois representations. At heart, our method lifts the M\\\"obius inversion formula for factorizations of zeta functions as a product of $L$-functions, to the $E_1$-page of an equivariant spectral sequence converging to equivariant algebraic $\\mathrm{K}$-groups. Additionally, the spectral Mackey functor structure on equivariant $\\mathrm{K}$-theory allows us to incorporate certain ramified extensions that appear in these $L$-functions.","sentences":["In this paper, we generalize the Quillen-Lichtenbaum Conjecture relating special values of Dedekind zeta functions to algebraic $\\mathrm{K}$-groups.","The former has been settled by Rost-Voevodsky up to the Iwasawa Main Conjecture.","Our generalization extends the scope of this conjecture to Artin $L$-functions of Galois representations of finite, function, and totally real number fields.","The statement of this conjecture relates norms of the special values of these $L$-functions to sizes of equivariant algebraic $\\mathrm{K}$-groups with coefficients in an equivariant Moore spectrum attached to a Galois representation.   ","We prove this conjecture in many cases, integrally, except up to a possible factor of powers of $2$ in the non-abelian and totally real number field case.","In the finite field case, we further determine the group structures of their equivariant algebraic $\\mathrm{K}$-groups with coefficients in Galois representations.","At heart, our method lifts the M\\\"obius inversion formula for factorizations of zeta functions as a product of $L$-functions, to the $E_1$-page of an equivariant spectral sequence converging to equivariant algebraic $\\mathrm{K}$-groups.","Additionally, the spectral Mackey functor structure on equivariant $\\mathrm{K}$-theory allows us to incorporate certain ramified extensions that appear in these $L$-functions."],"url":"http://arxiv.org/abs/2405.03578v1","category":"math.KT"}
{"created":"2024-05-06 15:52:11","title":"Toric matroid bundles","abstract":"Recent developments have produced new ways to encode the data of a torus equivariant vector bundle over a toric variety using certain representable matroid(s) labeled by polyhedral data. In this paper we show that this data makes sense for non-representable matroids as well. We call the resulting combinatorial objects toric matorid bundles. We define equivariant $K$-theory and characteristic classes of these bundles. As a particular case, we show that any matroid comes with tautological toric matroid bundles over the permutahedral toric variety and the corresponding equivariant K-classes and Chern classes agree with those in the recent work of Berger-Eur-Spink-Tseng on matroid invariants. Moreover, in analogy with toric vector bundles, we define sheaf of sections and Euler characteristic as well as positivity notions such as global generation, ampleness and nefness for toric matroid bundles. Finally, we study the splitting of toric matroid bundles and, in particular, an analogue of Grothendieck's theorem on splitting of vector bundles on the projective line.","sentences":["Recent developments have produced new ways to encode the data of a torus equivariant vector bundle over a toric variety using certain representable matroid(s) labeled by polyhedral data.","In this paper we show that this data makes sense for non-representable matroids as well.","We call the resulting combinatorial objects toric matorid bundles.","We define equivariant $K$-theory and characteristic classes of these bundles.","As a particular case, we show that any matroid comes with tautological toric matroid bundles over the permutahedral toric variety and the corresponding equivariant K-classes and Chern classes agree with those in the recent work of Berger-Eur-Spink-Tseng on matroid invariants.","Moreover, in analogy with toric vector bundles, we define sheaf of sections and Euler characteristic as well as positivity notions such as global generation, ampleness and nefness for toric matroid bundles.","Finally, we study the splitting of toric matroid bundles and, in particular, an analogue of Grothendieck's theorem on splitting of vector bundles on the projective line."],"url":"http://arxiv.org/abs/2405.03576v1","category":"math.AG"}
{"created":"2024-05-06 15:49:46","title":"ILILT: Implicit Learning of Inverse Lithography Technologies","abstract":"Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow. Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch. Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization. Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions. Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT. This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}. We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model. Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality.","sentences":["Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow.","Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch.","Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization.","Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions.","Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT.","This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}.","We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model.","Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality."],"url":"http://arxiv.org/abs/2405.03574v1","category":"cs.LG"}
{"created":"2024-05-06 15:48:22","title":"Demystifying Anonymity: Uncovering the Structure Underlying \"Read-Write Wait-Free Covering\"","abstract":"The study of particular synchronization problems in anonymous shared-memory models -- be it processor anonymity, memory anonymity, or full anonymity -- has produced ad hoc, so-called covering arguments in which processors overwrite each other's writes. Those arguments give us proverbial fish, but they do not teach us how to fish. In this paper, we take a step back to ask more general questions.   First, what does it mean to solve a task under processor anonymity? With tasks such as renaming, the traditional notion obviously does not apply. Instead of restricting ourselves to colorless tasks, we propose using the notion of group solvability, which allows transferring any task to processor-anonymous models.   Second, we consider solving tasks read-write wait-free under full anonymity, and we ask what we call the eventual-pattern question: if anonymous processors forever read and write in anonymous shared-memory, learning about inputs of other processors, what is the structure of the eventually-stable sets of inputs that processors learn? Solving the eventual-pattern question leads us to a group solution to the snapshot task and to M(M-1)/2-renaming, where M is the number of distinct inputs. Finally, using the snapshot solution, we easily obtain a solution to obstruction-free consensus.","sentences":["The study of particular synchronization problems in anonymous shared-memory models -- be it processor anonymity, memory anonymity, or full anonymity -- has produced ad hoc, so-called covering arguments in which processors overwrite each other's writes.","Those arguments give us proverbial fish, but they do not teach us how to fish.","In this paper, we take a step back to ask more general questions.   ","First, what does it mean to solve a task under processor anonymity?","With tasks such as renaming, the traditional notion obviously does not apply.","Instead of restricting ourselves to colorless tasks, we propose using the notion of group solvability, which allows transferring any task to processor-anonymous models.   ","Second, we consider solving tasks read-write wait-free under full anonymity, and we ask what we call the eventual-pattern question: if anonymous processors forever read and write in anonymous shared-memory, learning about inputs of other processors, what is the structure of the eventually-stable sets of inputs that processors learn?","Solving the eventual-pattern question leads us to a group solution to the snapshot task and to M(M-1)/2-renaming, where M is the number of distinct inputs.","Finally, using the snapshot solution, we easily obtain a solution to obstruction-free consensus."],"url":"http://arxiv.org/abs/2405.03573v1","category":"cs.DC"}
{"created":"2024-05-06 15:41:41","title":"Deep Space Separable Distillation for Lightweight Acoustic Scene Classification","abstract":"Acoustic scene classification (ASC) is highly important in the real world. Recently, deep learning-based methods have been widely employed for acoustic scene classification. However, these methods are currently not lightweight enough as well as their performance is not satisfactory. To solve these problems, we propose a deep space separable distillation network. Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance. Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC). These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks. The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity.","sentences":["Acoustic scene classification (ASC) is highly important in the real world.","Recently, deep learning-based methods have been widely employed for acoustic scene classification.","However, these methods are currently not lightweight enough as well as their performance is not satisfactory.","To solve these problems, we propose a deep space separable distillation network.","Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance.","Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC).","These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks.","The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity."],"url":"http://arxiv.org/abs/2405.03567v1","category":"cs.SD"}
{"created":"2024-05-06 15:39:58","title":"Spin wave vortex as topological probe of magnetic texture","abstract":"A vortex, a circulating flow around a void, is one of the basic topological phenomena in nature. Here we show that vortices generally emerge in spin wave travelling upon topologically nontrivial magnetic texture, due to the transverse precession of spin wave about the background magnetization. The winding number of each spin wave vortex is equivalent to sign of the local topological density of magnetic texture at the vortex core, and all winding numbers add up as twice the topological number of the magnetic texture. Based on the charts of spin wave vortices, the magnetization profile of the magnetic texture is reversely constructed, and a universal relation for the magnon topological Hall angle is theoretically proposed and numerically confirmed in vast types of magnetic textures. The simple connection between dynamic and static magnetizations, promotes spin wave vortex as a powerful tool to reveal the topology of the underlying magnetic texture.","sentences":["A vortex, a circulating flow around a void, is one of the basic topological phenomena in nature.","Here we show that vortices generally emerge in spin wave travelling upon topologically nontrivial magnetic texture, due to the transverse precession of spin wave about the background magnetization.","The winding number of each spin wave vortex is equivalent to sign of the local topological density of magnetic texture at the vortex core, and all winding numbers add up as twice the topological number of the magnetic texture.","Based on the charts of spin wave vortices, the magnetization profile of the magnetic texture is reversely constructed, and a universal relation for the magnon topological Hall angle is theoretically proposed and numerically confirmed in vast types of magnetic textures.","The simple connection between dynamic and static magnetizations, promotes spin wave vortex as a powerful tool to reveal the topology of the underlying magnetic texture."],"url":"http://arxiv.org/abs/2405.03566v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 15:38:32","title":"Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing","abstract":"Few-shot and zero-shot text classification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all. While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient. (2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios. To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shot text classification. We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes. Specifically, for mining more related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors. After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals. Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples.","sentences":["Few-shot and zero-shot text classification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all.","While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient.","(2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios.","To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shot text classification.","We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes.","Specifically, for mining more related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors.","After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals.","Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples."],"url":"http://arxiv.org/abs/2405.03565v1","category":"cs.CV"}
{"created":"2024-05-06 15:36:13","title":"Connection formulae in the Collision Limit I: Case Studies in Lifshitz Geometry","abstract":"The connection formulae provide a systematic way to compute physical quantities, such as the quasinormal modes, Green functions, in blackhole perturbation theories. In this work, we test whether it is possible to consistently take the collision limit, which bring two or more regular singularities into an irregular one, of the connection formulae, and we provide some supportive evidence for it.","sentences":["The connection formulae provide a systematic way to compute physical quantities, such as the quasinormal modes, Green functions, in blackhole perturbation theories.","In this work, we test whether it is possible to consistently take the collision limit, which bring two or more regular singularities into an irregular one, of the connection formulae, and we provide some supportive evidence for it."],"url":"http://arxiv.org/abs/2405.03564v1","category":"hep-th"}
{"created":"2024-05-06 15:35:17","title":"On the source of the Fe K-alpha emission in T Tauri Stars. Radiation induced by relativistic electrons during flares. An application to RY Tau","abstract":"T Tauri Stars (TTSs) are magnetically active stars that accrete matter from the inner border of the surrounding accretion disc; plasma gets trapped into the large scale magnetic structures and falls onto the star, heating the surface through the so-called accretion shocks. The X-ray spectra of the TTSs show prominent Fe II Kalpha fluorescence emission at 6.4keV that cannot be explained in a pure accretion scenario. Neither, it can be produced by the hot coronal plasma.   TTSs display all signs of magnetic activity and magnetic reconnection events are expected to occur frequently. In these events, electrons may get accelerated to relativistic speeds and their interaction with the environmental matter may result in Fe Kalpha emission. It is the aim of this work to evaluate the expected Fe Kalpha emission in the context of the TTS research and compare it with the actual Fe Kalpha measurements obtained during the flare detected while monitoring RY Tau with the XMM-Newton satellite. The propagation of high-energy electrons in dense gas generates a cascade of secondary particles that results in an electron shower of random nature whose evolution and radiative throughput is simulated in this work using the Monte Carlo code PENELOPE. A set of conditions representing the environment of the TTSs where these showers may impinge has been taken into account to generate a grid of models that can aid to the interpretation of the data. The simulations show that the electron beams produce a hot spot at the point of impact; strong Fe Kalpha emission and X-ray continuum radiation are produced by the spot. This emission is compatible with RY Tau observations. The Fe Kalpha emission observed in TTSs could be produced by beams of relativistic electrons accelerated in magnetic reconnection events during flares.","sentences":["T Tauri Stars (TTSs) are magnetically active stars that accrete matter from the inner border of the surrounding accretion disc; plasma gets trapped into the large scale magnetic structures and falls onto the star, heating the surface through the so-called accretion shocks.","The X-ray spectra of the TTSs show prominent Fe II Kalpha fluorescence emission at 6.4keV that cannot be explained in a pure accretion scenario.","Neither, it can be produced by the hot coronal plasma.   ","TTSs display all signs of magnetic activity and magnetic reconnection events are expected to occur frequently.","In these events, electrons may get accelerated to relativistic speeds and their interaction with the environmental matter may result in Fe Kalpha emission.","It is the aim of this work to evaluate the expected Fe Kalpha emission in the context of the TTS research and compare it with the actual Fe Kalpha measurements obtained during the flare detected while monitoring RY Tau with the XMM-Newton satellite.","The propagation of high-energy electrons in dense gas generates a cascade of secondary particles that results in an electron shower of random nature whose evolution and radiative throughput is simulated in this work using the Monte Carlo code PENELOPE.","A set of conditions representing the environment of the TTSs where these showers may impinge has been taken into account to generate a grid of models that can aid to the interpretation of the data.","The simulations show that the electron beams produce a hot spot at the point of impact; strong Fe Kalpha emission and X-ray continuum radiation are produced by the spot.","This emission is compatible with RY Tau observations.","The Fe Kalpha emission observed in TTSs could be produced by beams of relativistic electrons accelerated in magnetic reconnection events during flares."],"url":"http://arxiv.org/abs/2405.03563v1","category":"astro-ph.SR"}
{"created":"2024-05-06 15:34:31","title":"ID-centric Pre-training for Recommendation","abstract":"Classical sequential recommendation models generally adopt ID embeddings to store knowledge learned from user historical behaviors and represent items. However, these unique IDs are challenging to be transferred to new domains. With the thriving of pre-trained language model (PLM), some pioneer works adopt PLM for pre-trained recommendation, where modality information (e.g., text) is considered universal across domains via PLM. Unfortunately, the behavioral information in ID embeddings is still verified to be dominating in PLM-based recommendation models compared to modality information and thus limits these models' performance. In this work, we propose a novel ID-centric recommendation pre-training paradigm (IDP), which directly transfers informative ID embeddings learned in pre-training domains to item representations in new domains. Specifically, in pre-training stage, besides the ID-based sequential model for recommendation, we also build a Cross-domain ID-matcher (CDIM) learned by both behavioral and modality information. In the tuning stage, modality information of new domain items is regarded as a cross-domain bridge built by CDIM. We first leverage the textual information of downstream domain items to retrieve behaviorally and semantically similar items from pre-training domains using CDIM. Next, these retrieved pre-trained ID embeddings, rather than certain textual embeddings, are directly adopted to generate downstream new items' embeddings. Through extensive experiments on real-world datasets, both in cold and warm settings, we demonstrate that our proposed model significantly outperforms all baselines. Codes will be released upon acceptance.","sentences":["Classical sequential recommendation models generally adopt ID embeddings to store knowledge learned from user historical behaviors and represent items.","However, these unique IDs are challenging to be transferred to new domains.","With the thriving of pre-trained language model (PLM), some pioneer works adopt PLM for pre-trained recommendation, where modality information (e.g., text) is considered universal across domains via PLM.","Unfortunately, the behavioral information in ID embeddings is still verified to be dominating in PLM-based recommendation models compared to modality information and thus limits these models' performance.","In this work, we propose a novel ID-centric recommendation pre-training paradigm (IDP), which directly transfers informative ID embeddings learned in pre-training domains to item representations in new domains.","Specifically, in pre-training stage, besides the ID-based sequential model for recommendation, we also build a Cross-domain ID-matcher (CDIM) learned by both behavioral and modality information.","In the tuning stage, modality information of new domain items is regarded as a cross-domain bridge built by CDIM.","We first leverage the textual information of downstream domain items to retrieve behaviorally and semantically similar items from pre-training domains using CDIM.","Next, these retrieved pre-trained ID embeddings, rather than certain textual embeddings, are directly adopted to generate downstream new items' embeddings.","Through extensive experiments on real-world datasets, both in cold and warm settings, we demonstrate that our proposed model significantly outperforms all baselines.","Codes will be released upon acceptance."],"url":"http://arxiv.org/abs/2405.03562v1","category":"cs.IR"}
{"created":"2024-05-06 15:28:21","title":"Homological Quantum Error Correction with Torsion","abstract":"Homological quantum error correction uses tools of algebraic topology and homological algebra to derive Calderbank-Shor-Steane quantum error correcting codes from cellulations of topological spaces. This work is an exploration of the relevant topics, a journey from classical error correction, through homology theory, to CSS codes acting on qudit systems. Qudit codes have torsion in their logical spaces. This is interesting to study because it gives us extra logical qudits, of possibly different dimension.   Apart from examples and comments on the topic, we prove an original result, the Structure Theorem for the Qudit Logical Space, an application of the Universal Coefficient Theorem from homological algebra, which gives us information about the logical space when torsion is involved, and that improves on a previous result in the literature. Furthermore, this work introduces our own abstracted and restricted version of the general notion of a cell complex, suited exactly to our needs.","sentences":["Homological quantum error correction uses tools of algebraic topology and homological algebra to derive Calderbank-Shor-Steane quantum error correcting codes from cellulations of topological spaces.","This work is an exploration of the relevant topics, a journey from classical error correction, through homology theory, to CSS codes acting on qudit systems.","Qudit codes have torsion in their logical spaces.","This is interesting to study because it gives us extra logical qudits, of possibly different dimension.   ","Apart from examples and comments on the topic, we prove an original result, the Structure Theorem for the Qudit Logical Space, an application of the Universal Coefficient Theorem from homological algebra, which gives us information about the logical space when torsion is involved, and that improves on a previous result in the literature.","Furthermore, this work introduces our own abstracted and restricted version of the general notion of a cell complex, suited exactly to our needs."],"url":"http://arxiv.org/abs/2405.03559v1","category":"quant-ph"}
{"created":"2024-05-06 15:25:57","title":"On free bases of Banach spaces","abstract":"We call a closed subset M of a Banach space X a free basis of X if it contains the null vector and every Lipschitz map from M to a Banach space Y, which preserves the null vectors can be uniquely extended to a bounded linear map from X to Y. We then say that two complete metric spaces M and N are Mol-equivalent if they admit bi-Lipschitz copies M' and N', respectively that are free bases of a common Banach space satisfying span M'=span N'.   In this note, we compare Mol-equivalence with some other natural equivalences on the class of complete metric spaces. The main result states that Mol-equivalent spaces must have the same \\v{C}ech-Lebesgue covering dimension. In combination with the work of Godard, this implies that two complete metric spaces with isomorphic Lipschitz-free spaces need not be Mol-equivalent. Also, there exist non-homeomorphic Mol-equivalent metric spaces, and, in contrast with the covering dimension, the metric Assouad dimension is not preserved by Mol-equivalence.","sentences":["We call a closed subset M of a Banach space X a free basis of X if it contains the null vector and every Lipschitz map from M to a Banach space Y, which preserves the null vectors can be uniquely extended to a bounded linear map from X to Y.","We then say that two complete metric spaces M and N are Mol-equivalent if they admit bi-Lipschitz copies M' and N', respectively that are free bases of a common Banach space satisfying span M'=span N'.   ","In this note, we compare Mol-equivalence with some other natural equivalences on the class of complete metric spaces.","The main result states that Mol-equivalent spaces must have the same \\v{C}ech-Lebesgue covering dimension.","In combination with the work of Godard, this implies that two complete metric spaces with isomorphic Lipschitz-free spaces need not be Mol-equivalent.","Also, there exist non-homeomorphic Mol-equivalent metric spaces, and, in contrast with the covering dimension, the metric Assouad dimension is not preserved by Mol-equivalence."],"url":"http://arxiv.org/abs/2405.03556v1","category":"math.FA"}
{"created":"2024-05-06 15:25:48","title":"A Comprehensive Overview and Survey of O-RAN: Exploring Slicing-aware Architecture, Deployment Options, and Use Cases","abstract":"Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces. It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture. This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks. One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing. The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs). In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN. To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions. The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects. Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets...","sentences":["Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces.","It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture.","This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks.","One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing.","The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs).","In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN.","To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions.","The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects.","Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets..."],"url":"http://arxiv.org/abs/2405.03555v1","category":"cs.NI"}
{"created":"2024-05-06 15:21:22","title":"Regular black holes and their relationship to polymerized models and mimetic gravity","abstract":"We present further applications of the formalism introduced by the authors in arXiv:2308.10949, which allows embedding of a broad class of generalized LTB models into effective spherically symmetric spacetimes. We focus on regular black hole models, where a broad class of models can be considered, including for example LQG-inspired models as well as the model with a regular center, e.g. of Bardeen and Hayward. For a certain class of regular black hole models, we can formulate a Birkhoff-like theorem in LTB coordinates. We further show that depending on the properties of the polymerization functions characterizing such regular black hole models in this formalism, the uniqueness of the effective spherically symmetric vacuum solutions might not be given in general in Schwarzschild-like coordinates. Furthermore, we introduce a reconstruction algorithm that allows for a subclass of this models to construct from a given metric in Schwarzschild-like coordinates the corresponding effective spherically symmetric model, its dynamics as an 1+1-dimensional field theory as well as a corresponding covariant Lagrangian of extended mimetic gravity in four dimensions. Such a reconstruction allows us to obtain Lagrangians of extended mimetic gravity models for black holes with a regular center, e.g. the Bardeen and Hayward metric as well as for effective LQG inspired models. Moreover, the reconstruction enables us to extend regular black hole models to general inhomogeneous dust collapse models. For the latter, within this formalism, we can investigate and look at the physical properties of the models such as the existence of weak shell-crossing singularities from a novel perspective.","sentences":["We present further applications of the formalism introduced by the authors in arXiv:2308.10949, which allows embedding of a broad class of generalized LTB models into effective spherically symmetric spacetimes.","We focus on regular black hole models, where a broad class of models can be considered, including for example LQG-inspired models as well as the model with a regular center, e.g. of Bardeen and Hayward.","For a certain class of regular black hole models, we can formulate a Birkhoff-like theorem in LTB coordinates.","We further show that depending on the properties of the polymerization functions characterizing such regular black hole models in this formalism, the uniqueness of the effective spherically symmetric vacuum solutions might not be given in general in Schwarzschild-like coordinates.","Furthermore, we introduce a reconstruction algorithm that allows for a subclass of this models to construct from a given metric in Schwarzschild-like coordinates the corresponding effective spherically symmetric model, its dynamics as an 1+1-dimensional field theory as well as a corresponding covariant Lagrangian of extended mimetic gravity in four dimensions.","Such a reconstruction allows us to obtain Lagrangians of extended mimetic gravity models for black holes with a regular center, e.g. the Bardeen and Hayward metric as well as for effective LQG inspired models.","Moreover, the reconstruction enables us to extend regular black hole models to general inhomogeneous dust collapse models.","For the latter, within this formalism, we can investigate and look at the physical properties of the models such as the existence of weak shell-crossing singularities from a novel perspective."],"url":"http://arxiv.org/abs/2405.03554v1","category":"gr-qc"}
{"created":"2024-05-06 15:20:30","title":"AlphaMath Almost Zero: process Supervision without process","abstract":"Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities. However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors. While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging. Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise. In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically. Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions. We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains. Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks.","sentences":["Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities.","However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors.","While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging.","Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise.","In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically.","Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions.","We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains.","Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks."],"url":"http://arxiv.org/abs/2405.03553v1","category":"cs.CL"}
{"created":"2024-05-06 15:12:51","title":"Bridging discrete and continuous state spaces: Exploring the Ehrenfest process in time-continuous diffusion models","abstract":"Generative modeling via stochastic processes has led to remarkable empirical results as well as to recent advances in their theoretical understanding. In principle, both space and time of the processes can be discrete or continuous. In this work, we study time-continuous Markov jump processes on discrete state spaces and investigate their correspondence to state-continuous diffusion processes given by SDEs. In particular, we revisit the $\\textit{Ehrenfest process}$, which converges to an Ornstein-Uhlenbeck process in the infinite state space limit. Likewise, we can show that the time-reversal of the Ehrenfest process converges to the time-reversed Ornstein-Uhlenbeck process. This observation bridges discrete and continuous state spaces and allows to carry over methods from one to the respective other setting. Additionally, we suggest an algorithm for training the time-reversal of Markov jump processes which relies on conditional expectations and can thus be directly related to denoising score matching. We demonstrate our methods in multiple convincing numerical experiments.","sentences":["Generative modeling via stochastic processes has led to remarkable empirical results as well as to recent advances in their theoretical understanding.","In principle, both space and time of the processes can be discrete or continuous.","In this work, we study time-continuous Markov jump processes on discrete state spaces and investigate their correspondence to state-continuous diffusion processes given by SDEs.","In particular, we revisit the $\\textit{Ehrenfest process}$, which converges to an Ornstein-Uhlenbeck process in the infinite state space limit.","Likewise, we can show that the time-reversal of the Ehrenfest process converges to the time-reversed Ornstein-Uhlenbeck process.","This observation bridges discrete and continuous state spaces and allows to carry over methods from one to the respective other setting.","Additionally, we suggest an algorithm for training the time-reversal of Markov jump processes which relies on conditional expectations and can thus be directly related to denoising score matching.","We demonstrate our methods in multiple convincing numerical experiments."],"url":"http://arxiv.org/abs/2405.03549v1","category":"stat.ML"}
{"created":"2024-05-06 15:10:46","title":"Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions","abstract":"Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.","sentences":["Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision.","Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   ","However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration.","In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature.","We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces."],"url":"http://arxiv.org/abs/2405.03547v1","category":"cs.LG"}
{"created":"2024-05-06 15:10:19","title":"CCDM: Continuous Conditional Diffusion Models for Image Generation","abstract":"Continuous Conditional Generative Modeling (CCGM) aims to estimate the distribution of high-dimensional data, typically images, conditioned on scalar continuous variables known as regression labels. While Continuous conditional Generative Adversarial Networks (CcGANs) were initially designed for this task, their adversarial training mechanism remains vulnerable to extremely sparse or imbalanced data, resulting in suboptimal outcomes. To enhance the quality of generated images, a promising alternative is to replace CcGANs with Conditional Diffusion Models (CDMs), renowned for their stable training process and ability to produce more realistic images. However, existing CDMs encounter challenges when applied to CCGM tasks due to several limitations such as inadequate U-Net architectures and deficient model fitting mechanisms for handling regression labels. In this paper, we introduce Continuous Conditional Diffusion Models (CCDMs), the first CDM designed specifically for the CCGM task. CCDMs address the limitations of existing CDMs by introducing specially designed conditional diffusion processes, a modified denoising U-Net with a custom-made conditioning mechanism, a novel hard vicinal loss for model fitting, and an efficient conditional sampling procedure. With comprehensive experiments on four datasets with varying resolutions ranging from 64x64 to 192x192, we demonstrate the superiority of the proposed CCDM over state-of-the-art CCGM models, establishing new benchmarks in CCGM. Extensive ablation studies validate the model design and implementation configuration of the proposed CCDM. Our code is publicly available at https://github.com/UBCDingXin/CCDM.","sentences":["Continuous Conditional Generative Modeling (CCGM) aims to estimate the distribution of high-dimensional data, typically images, conditioned on scalar continuous variables known as regression labels.","While Continuous conditional Generative Adversarial Networks (CcGANs) were initially designed for this task, their adversarial training mechanism remains vulnerable to extremely sparse or imbalanced data, resulting in suboptimal outcomes.","To enhance the quality of generated images, a promising alternative is to replace CcGANs with Conditional Diffusion Models (CDMs), renowned for their stable training process and ability to produce more realistic images.","However, existing CDMs encounter challenges when applied to CCGM tasks due to several limitations such as inadequate U-Net architectures and deficient model fitting mechanisms for handling regression labels.","In this paper, we introduce Continuous Conditional Diffusion Models (CCDMs), the first CDM designed specifically for the CCGM task.","CCDMs address the limitations of existing CDMs by introducing specially designed conditional diffusion processes, a modified denoising U-Net with a custom-made conditioning mechanism, a novel hard vicinal loss for model fitting, and an efficient conditional sampling procedure.","With comprehensive experiments on four datasets with varying resolutions ranging from 64x64 to 192x192, we demonstrate the superiority of the proposed CCDM over state-of-the-art CCGM models, establishing new benchmarks in CCGM.","Extensive ablation studies validate the model design and implementation configuration of the proposed CCDM.","Our code is publicly available at https://github.com/UBCDingXin/CCDM."],"url":"http://arxiv.org/abs/2405.03546v1","category":"cs.CV"}
{"created":"2024-05-06 15:06:56","title":"A Formal Model of Security Controls' Capabilities and Its Applications to Policy Refinement and Incident Management","abstract":"Enforcing security requirements in networked information systems relies on security controls to mitigate the risks from increasingly dangerous threats. Configuring security controls is challenging; even nowadays, administrators must perform it without adequate tool support. Hence, this process is plagued by errors that translate to insecure postures, security incidents, and a lack of promptness in answering threats. This paper presents the Security Capability Model (SCM), a formal model that abstracts the features that security controls offer for enforcing security policies, which includes an Information Model that depicts the basic concepts related to rules (i.e., conditions, actions, events) and policies (i.e., conditions' evaluation, resolution strategies, default actions), and a Data Model that covers the capabilities needed to describe different types of filtering and channel protection controls. Following state-of-the-art design patterns, the model allows for generating abstract versions of the security controls' languages and a model-driven approach for translating abstract policies into device-specific configuration settings. By validating its effectiveness in real-world scenarios, we show that SCM enables the automation of different and complex security tasks, i.e., accurate and granular security control comparison, policy refinement, and incident response. Lastly, we present opportunities for extensions and integration with other frameworks and models.","sentences":["Enforcing security requirements in networked information systems relies on security controls to mitigate the risks from increasingly dangerous threats.","Configuring security controls is challenging; even nowadays, administrators must perform it without adequate tool support.","Hence, this process is plagued by errors that translate to insecure postures, security incidents, and a lack of promptness in answering threats.","This paper presents the Security Capability Model (SCM), a formal model that abstracts the features that security controls offer for enforcing security policies, which includes an Information Model that depicts the basic concepts related to rules (i.e., conditions, actions, events) and policies (i.e., conditions' evaluation, resolution strategies, default actions), and a Data Model that covers the capabilities needed to describe different types of filtering and channel protection controls.","Following state-of-the-art design patterns, the model allows for generating abstract versions of the security controls' languages and a model-driven approach for translating abstract policies into device-specific configuration settings.","By validating its effectiveness in real-world scenarios, we show that SCM enables the automation of different and complex security tasks, i.e., accurate and granular security control comparison, policy refinement, and incident response.","Lastly, we present opportunities for extensions and integration with other frameworks and models."],"url":"http://arxiv.org/abs/2405.03544v1","category":"cs.CR"}
{"created":"2024-05-06 15:06:02","title":"Axiomatizing the Logic of Ordinary Discourse","abstract":"Most non-classical logics are subclassical, that is, every inference/theorem they validate is also valid classically. A notable exception is the three-valued propositional Logic of Ordinary Discourse (OL) proposed and extensively motivated by W. S. Cooper as a more adequate candidate for formalizing everyday reasoning (in English). OL challenges classical logic not only by rejecting some theses, but also by accepting non-classically valid principles, such as so-called Aristotle's and Boethius' theses. Formally, OL shows a number of unusual features - it is non-structural, connexive, paraconsistent and contradictory - making it all the more interesting for the mathematical logician. We present our recent findings on OL and its structural companion (that we call sOL). We introduce Hilbert-style multiple-conclusion calculi for OL and sOL that are both modular and analytic, and easily allow us to obtain single-conclusion axiomatizations. We prove that sOL is algebraizable and single out its equivalent semantics, which turns out to be a discriminator variety generated by a three-element algebra. Having observed that sOL can express the connectives of other three-valued logics, we prove that it is definitionally equivalent to an expansion of the three-valued logic J3 of D'Ottaviano and da Costa, itself an axiomatic extension of paraconsistent Nelson logic.","sentences":["Most non-classical logics are subclassical, that is, every inference/theorem they validate is also valid classically.","A notable exception is the three-valued propositional Logic of Ordinary Discourse (OL) proposed and extensively motivated by W. S. Cooper as a more adequate candidate for formalizing everyday reasoning (in English).","OL challenges classical logic not only by rejecting some theses, but also by accepting non-classically valid principles, such as so-called Aristotle's and Boethius' theses.","Formally, OL shows a number of unusual features - it is non-structural, connexive, paraconsistent and contradictory - making it all the more interesting for the mathematical logician.","We present our recent findings on OL and its structural companion (that we call sOL).","We introduce Hilbert-style multiple-conclusion calculi for OL and sOL that are both modular and analytic, and easily allow us to obtain single-conclusion axiomatizations.","We prove that sOL is algebraizable and single out its equivalent semantics, which turns out to be a discriminator variety generated by a three-element algebra.","Having observed that sOL can express the connectives of other three-valued logics, we prove that it is definitionally equivalent to an expansion of the three-valued logic J3 of D'Ottaviano and da Costa, itself an axiomatic extension of paraconsistent Nelson logic."],"url":"http://arxiv.org/abs/2405.03543v1","category":"math.LO"}
{"created":"2024-05-06 15:02:16","title":"RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour Detection","abstract":"Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy. However, their application in brain tumour detection remains underexplored. This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images. RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours. Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance. This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG. Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed. Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs. The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images. The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN.","sentences":["Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy.","However, their application in brain tumour detection remains underexplored.","This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images.","RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours.","Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance.","This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG.","Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed.","Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs.","The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images.","The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN."],"url":"http://arxiv.org/abs/2405.03541v1","category":"cs.CV"}
{"created":"2024-05-06 14:58:45","title":"Connecting essential triangulations I: via 2-3 and 0-2 moves","abstract":"Suppose that $M$ is a compact, connected three-manifold with boundary. We show that if the universal cover has infinitely many boundary components then $M$ has an ideal triangulation which is essential: no edge can be homotoped into the boundary. Under the same hypotheses, we show that the set of essential triangulations of $M$ is connected via 2-3, 3-2, 0-2, and 2-0 moves.   The above results are special cases of our general theory. We introduce $L$-essential triangulations: boundary components of the universal cover receive labels and no edge has the same label at both ends. As an application, under mild conditions on a representation, we construct an ideal triangulation for which a solution to Thurston's gluing equations recovers the given representation.   Our results also imply that such triangulations are connected via 2-3, 3-2, 0-2, and 2-0 moves. Together with results of Pandey and Wong, this proves that Dimofte and Garoufalidis' 1-loop invariant is independent of the choice of essential triangulation.","sentences":["Suppose that $M$ is a compact, connected three-manifold with boundary.","We show that if the universal cover has infinitely many boundary components then $M$ has an ideal triangulation which is essential: no edge can be homotoped into the boundary.","Under the same hypotheses, we show that the set of essential triangulations of $M$ is connected via 2-3, 3-2, 0-2, and 2-0 moves.   ","The above results are special cases of our general theory.","We introduce $L$-essential triangulations: boundary components of the universal cover receive labels and no edge has the same label at both ends.","As an application, under mild conditions on a representation, we construct an ideal triangulation for which a solution to Thurston's gluing equations recovers the given representation.   ","Our results also imply that such triangulations are connected via 2-3, 3-2, 0-2, and 2-0 moves.","Together with results of Pandey and Wong, this proves that Dimofte and Garoufalidis' 1-loop invariant is independent of the choice of essential triangulation."],"url":"http://arxiv.org/abs/2405.03539v1","category":"math.GT"}
{"created":"2024-05-06 14:55:37","title":"Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation","abstract":"Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data. These locally adapted models are then aggregated at a central server via federated learning. To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns. We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation. Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge.","sentences":["Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics.","Traditional approaches of accumulating data and periodically retraining models are outpaced.","We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data.","These locally adapted models are then aggregated at a central server via federated learning.","To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns.","We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation.","Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge."],"url":"http://arxiv.org/abs/2405.03537v1","category":"cs.LG"}
{"created":"2024-05-06 14:52:23","title":"Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer","abstract":"We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers.","sentences":["We investigate the problem of transferring an expert policy from a source robot to multiple different robots.","To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences.","The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer.","We present a heuristic approach to determine an optimized robot evolution tree.","Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers."],"url":"http://arxiv.org/abs/2405.03534v1","category":"cs.RO"}
{"created":"2024-05-06 14:49:34","title":"Comomentum sections and Poisson maps in Hamiltonian Lie algebroids","abstract":"In a Hamiltonian Lie algebroid over a pre-symplectic manifold and over a Poisson manifold, we introduce a map corresponding to a comomentum map, called a comomentum section. We show that the comomentum section gives a Lie algebroid morphism among Lie algebroids. Moreover, we prove that a momentum section on a Hamiltonian Lie algebroid is a Poisson map between proper Poisson manifolds, which is a generalization that a momentum map is a Poisson map between the symplectic manifold to dual of the Lie algebra. Finally, a momentum section is reinterpreted as a Dirac morphism on Dirac structures.","sentences":["In a Hamiltonian Lie algebroid over a pre-symplectic manifold and over a Poisson manifold, we introduce a map corresponding to a comomentum map, called a comomentum section.","We show that the comomentum section gives a Lie algebroid morphism among Lie algebroids.","Moreover, we prove that a momentum section on a Hamiltonian Lie algebroid is a Poisson map between proper Poisson manifolds, which is a generalization that a momentum map is a Poisson map between the symplectic manifold to dual of the Lie algebra.","Finally, a momentum section is reinterpreted as a Dirac morphism on Dirac structures."],"url":"http://arxiv.org/abs/2405.03533v1","category":"math.SG"}
{"created":"2024-05-06 14:47:40","title":"Semi-autonomous Robotic Disassembly Enhanced by Mixed Reality","abstract":"In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks. Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy. Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly. The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority. This data guides the MoveIt platform in trajectory planning for the Franka Robot arm. SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy. Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality. Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2. Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment.","sentences":["In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks.","Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy.","Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly.","The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority.","This data guides the MoveIt platform in trajectory planning for the Franka Robot arm.","SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy.","Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality.","Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2.","Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment."],"url":"http://arxiv.org/abs/2405.03530v1","category":"cs.RO"}
{"created":"2024-05-06 14:40:50","title":"Exploring knowledge graph-based neural-symbolic system from application perspective","abstract":"The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing. Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge. The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems. Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object). This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration. It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI.","sentences":["The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing.","Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge.","The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems.","Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object).","This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration.","It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI."],"url":"http://arxiv.org/abs/2405.03524v1","category":"cs.AI"}
{"created":"2024-05-06 14:38:43","title":"Optimisation challenge for superconducting adiabatic neural network implementing XOR and OR boolean functions","abstract":"In this article, we consider designs of simple analog artificial neural networks based on adiabatic Josephson cells with a sigmoid activation function. A new approach based on the gradient descent method is developed to adjust the circuit parameters, allowing efficient signal transmission between the network layers. The proposed solution is demonstrated on the example of the system implementing XOR and OR logical operations.","sentences":["In this article, we consider designs of simple analog artificial neural networks based on adiabatic Josephson cells with a sigmoid activation function.","A new approach based on the gradient descent method is developed to adjust the circuit parameters, allowing efficient signal transmission between the network layers.","The proposed solution is demonstrated on the example of the system implementing XOR and OR logical operations."],"url":"http://arxiv.org/abs/2405.03521v1","category":"cond-mat.supr-con"}
{"created":"2024-05-06 14:37:07","title":"Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond","abstract":"General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.","sentences":["General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems.","Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws.","In this survey, we embark on a comprehensive exploration of the latest advancements in world models.","Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content.","Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility.","Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts.","At last, we examine challenges and limitations of world models, and discuss their potential future directions.","We hope this survey can serve as a foundational reference for the research community and inspire continued innovation.","This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey."],"url":"http://arxiv.org/abs/2405.03520v1","category":"cs.CV"}
{"created":"2024-05-06 14:36:01","title":"Low-light Object Detection","abstract":"In this competition we employed a model fusion approach to achieve object detection results close to those of real images. Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions. We used various enhancement techniques on the test data to generate multiple sets of prediction results. Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results.","sentences":["In this competition we employed a model fusion approach to achieve object detection results close to those of real images.","Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions.","We used various enhancement techniques on the test data to generate multiple sets of prediction results.","Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results."],"url":"http://arxiv.org/abs/2405.03519v1","category":"cs.CV"}
{"created":"2024-05-06 14:33:35","title":"Reinforcement Nash Equilibrium Solver","abstract":"Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities. Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete. Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE. For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short. However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE. Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games. Specifically, our contributions are threefold. i) We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii) We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games. Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games.","sentences":["Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities.","Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete.","Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE.","For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short.","However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE.","Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games.","Specifically, our contributions are threefold.","i)","We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii)","We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games.","Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games."],"url":"http://arxiv.org/abs/2405.03518v1","category":"cs.GT"}
{"created":"2024-05-06 14:23:33","title":"Extremal Separation Problems for Temporal Instance Queries","abstract":"The separation problem for a class Q of database queries is to find a query in Q that distinguishes between a given set of `positive' and `negative' data examples. Separation provides explanations of examples and underpins the query-by-example paradigm to support database users in constructing and refining queries. As the space of all separating queries can be large, it is helpful to succinctly represent this space by means of its most specific (logically strongest) and general (weakest) members. We investigate this extremal separation problem for classes of instance queries formulated in linear temporal logic LTL with the operators conjunction, next, and eventually. Our results range from tight complexity bounds for verifying and counting extremal separators to algorithms computing them.","sentences":["The separation problem for a class Q of database queries is to find a query in Q that distinguishes between a given set of `positive' and `negative' data examples.","Separation provides explanations of examples and underpins the query-by-example paradigm to support database users in constructing and refining queries.","As the space of all separating queries can be large, it is helpful to succinctly represent this space by means of its most specific (logically strongest) and general (weakest) members.","We investigate this extremal separation problem for classes of instance queries formulated in linear temporal logic LTL with the operators conjunction, next, and eventually.","Our results range from tight complexity bounds for verifying and counting extremal separators to algorithms computing them."],"url":"http://arxiv.org/abs/2405.03511v1","category":"cs.DB"}
{"created":"2024-05-06 14:23:29","title":"Prolonging The Inevitable: Maximising survival time of an engine-equipped spacecraft between spatial hypersurfaces, as applied to the Schwarzschild spacetime","abstract":"The fate of an astronaut unfortunate -- or foolish -- enough to find themselves hurtling towards spaghettification after passing the event horizon of a black hole is a common anecdote told by scientists to the regular population. However, despite the fact the Schwarzschild spacetime has been discovered over a century ago, the simple question of how long can such a space traveller live has not been fully elaborated on since. In fact, a few textbooks even give a mistaken or easily misread description of what happens. We address those inconsistencies. We calculate the proper time a space traveller equipped with means of propulsion can expect to live in these circumstances, giving analytical expressions (as elliptic integrals) wherever possible. We prove a principle that explains the best strategy to extend their life, and show its' generalisation for other spacetimes. Finally, we give quantitative answers to what gains due to optimal control can be expected in typical and somewhat `realistic' circumstances.","sentences":["The fate of an astronaut unfortunate -- or foolish -- enough to find themselves hurtling towards spaghettification after passing the event horizon of a black hole is a common anecdote told by scientists to the regular population.","However, despite the fact the Schwarzschild spacetime has been discovered over a century ago, the simple question of how long can such a space traveller live has not been fully elaborated on since.","In fact, a few textbooks even give a mistaken or easily misread description of what happens.","We address those inconsistencies.","We calculate the proper time a space traveller equipped with means of propulsion can expect to live in these circumstances, giving analytical expressions (as elliptic integrals) wherever possible.","We prove a principle that explains the best strategy to extend their life, and show its' generalisation for other spacetimes.","Finally, we give quantitative answers to what gains due to optimal control can be expected in typical and somewhat `realistic' circumstances."],"url":"http://arxiv.org/abs/2405.03510v1","category":"gr-qc"}
{"created":"2024-05-06 14:22:17","title":"Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning","abstract":"Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65%) and return statements (66%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0% and 16.5% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.","sentences":["Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets.","Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools.","Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets.","To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer.","Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65%) and return statements (66%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0% and 16.5% respectively.","Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice.","Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool."],"url":"http://arxiv.org/abs/2405.03509v1","category":"cs.SE"}
{"created":"2024-05-06 14:20:57","title":"Galoisian structure of large steps walks in the quadrant","abstract":"The enumeration of walks in the quarter plane confined in the first quadrant has attracted a lot of attention over the past fifteenth years. The generating functions associated to small steps models satisfy a functional equation in two catalytic variables. For such models, Bousquet-M\\'elou and Mishna defined a group called the group of the walk which turned out to be central in the classification of small steps models. In particular, its action on the catalytic variables yields a set of change of variables compatible with the structure of the functional equation. This particular set called the orbit has been generalized to models with arbitrary large steps by Bostan, Bousquet-M\\'elou and Melczer. However, the orbit had till now no underlying group.   In this article, we endow the orbit with the action of a Galois group, which extends the group of the walk to models with large steps. Within this Galoisian framework, we generalized the notions of invariants and decoupling. This enable us to develop a general strategy to prove the algebraicity of models with small backward steps. Our constructions lead to the first proofs of algebraicity of weighted models with large steps, proving in particular a conjecture of Bostan, Bousquet-M\\'elou and Melczer, and allowing us to find new algebraic models with large steps.","sentences":["The enumeration of walks in the quarter plane confined in the first quadrant has attracted a lot of attention over the past fifteenth years.","The generating functions associated to small steps models satisfy a functional equation in two catalytic variables.","For such models, Bousquet-M\\'elou and Mishna defined a group called the group of the walk which turned out to be central in the classification of small steps models.","In particular, its action on the catalytic variables yields a set of change of variables compatible with the structure of the functional equation.","This particular set called the orbit has been generalized to models with arbitrary large steps by Bostan, Bousquet-M\\'elou and Melczer.","However, the orbit had till now no underlying group.   ","In this article, we endow the orbit with the action of a Galois group, which extends the group of the walk to models with large steps.","Within this Galoisian framework, we generalized the notions of invariants and decoupling.","This enable us to develop a general strategy to prove the algebraicity of models with small backward steps.","Our constructions lead to the first proofs of algebraicity of weighted models with large steps, proving in particular a conjecture of Bostan, Bousquet-M\\'elou and Melczer, and allowing us to find new algebraic models with large steps."],"url":"http://arxiv.org/abs/2405.03508v1","category":"math.CO"}
{"created":"2024-05-06 14:14:32","title":"Classical general relativity effects by magnetars with slow rotation and a magnetic dipole","abstract":"In this contribution, the classical tests of general relativity using the Gutsunaev-Manko metric with slow rotation are obtained. This metric represents the spacetime of an object endowed with mass, magnetic dipole moment and angular moment. These tests are the deflection of light, time delay, precession of the periastron and gravitational redshift. We also provide numerical estimations for real magnetars and magnetar candidates from the McGill magnetar catalogue, and for the Sun in the cycles of low activity. Our results find that the magnetic dipole contribution to the deflection of light is significant when compared to the rotation contribution. The magnetic dipole contributions to the periastron precession and the time delay are 3 and 6 orders of magnitude lower than the rotation contribution respectively. For the gravitational redshift, the contribution is negligible within the approximation presented.","sentences":["In this contribution, the classical tests of general relativity using the Gutsunaev-Manko metric with slow rotation are obtained.","This metric represents the spacetime of an object endowed with mass, magnetic dipole moment and angular moment.","These tests are the deflection of light, time delay, precession of the periastron and gravitational redshift.","We also provide numerical estimations for real magnetars and magnetar candidates from the McGill magnetar catalogue, and for the Sun in the cycles of low activity.","Our results find that the magnetic dipole contribution to the deflection of light is significant when compared to the rotation contribution.","The magnetic dipole contributions to the periastron precession and the time delay are 3 and 6 orders of magnitude lower than the rotation contribution respectively.","For the gravitational redshift, the contribution is negligible within the approximation presented."],"url":"http://arxiv.org/abs/2405.03503v1","category":"gr-qc"}
{"created":"2024-05-06 14:13:38","title":"Boosting Single Positive Multi-label Classification with Generalized Robust Loss","abstract":"Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios. In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label. Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives. To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework. In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples. Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks.","sentences":["Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios.","In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label.","Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives.","To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework.","In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples.","Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks."],"url":"http://arxiv.org/abs/2405.03501v1","category":"cs.LG"}
{"created":"2024-05-06 14:11:36","title":"A Rate-Distortion-Classification Approach for Lossy Image Compression","abstract":"In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate. The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images. To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy. The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset. The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions. This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications.","sentences":["In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate.","The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images.","To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy.","The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset.","The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions.","This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications."],"url":"http://arxiv.org/abs/2405.03500v1","category":"cs.MM"}
{"created":"2024-05-06 14:03:42","title":"Entangled photon pair generation from an epsilon-near-zero metasurface","abstract":"Entangled photon pair sources are essential for diverse quantum technologies, such as quantum communication, computation, and imaging. Currently, most methods for generating and manipulating entangled photon pairs rely on bulk nonlinear crystals, with some requiring sophisticated engineering. In this work, we propose and experimentally demonstrate a 68-nm-thick entangled photon pair source using a plasmonic metasurface strongly coupled to an epsilon-near-zero (ENZ) material. By tailoring dual resonances at both pump and emission wavelengths and utilizing the field enhancement induced by the ENZ effect, the photon pair generation efficiency is boosted. The coincidences-to-accidentals ratio of the generated photon pairs reaches 40, well above the limit of classical light radiation. Moreover, the ENZ metasurface platform enables versatile manipulation of the system's anisotropic second-order nonlinear susceptibility tensor, allowing for direct control over the polarization states of the generated photon pairs. By conducting polarization-resolved second-harmonic generation measurements, we discuss the potential to achieve a polarization-entangled Bell state from the identical ENZ metasurface, based on the quantum-classical correspondence. Our approach opens a new avenue for simultaneously achieving the miniaturization of photon pair sources and quantum state engineering.","sentences":["Entangled photon pair sources are essential for diverse quantum technologies, such as quantum communication, computation, and imaging.","Currently, most methods for generating and manipulating entangled photon pairs rely on bulk nonlinear crystals, with some requiring sophisticated engineering.","In this work, we propose and experimentally demonstrate a 68-nm-thick entangled photon pair source using a plasmonic metasurface strongly coupled to an epsilon-near-zero (ENZ) material.","By tailoring dual resonances at both pump and emission wavelengths and utilizing the field enhancement induced by the ENZ effect, the photon pair generation efficiency is boosted.","The coincidences-to-accidentals ratio of the generated photon pairs reaches 40, well above the limit of classical light radiation.","Moreover, the ENZ metasurface platform enables versatile manipulation of the system's anisotropic second-order nonlinear susceptibility tensor, allowing for direct control over the polarization states of the generated photon pairs.","By conducting polarization-resolved second-harmonic generation measurements, we discuss the potential to achieve a polarization-entangled Bell state from the identical ENZ metasurface, based on the quantum-classical correspondence.","Our approach opens a new avenue for simultaneously achieving the miniaturization of photon pair sources and quantum state engineering."],"url":"http://arxiv.org/abs/2405.03493v1","category":"physics.optics"}
{"created":"2024-05-06 14:02:59","title":"Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation","abstract":"Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.","sentences":["Learning from Demonstration allows robots to mimic human actions.","However, these methods do not model constraints crucial to ensure safety of the learned skill.","Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost.","In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories.","Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations.","Subsequently, a constraint leaning method is used to identify the unknown constraints.","Our approach is validated both on simulated trajectories and a real robotic manipulation task.","Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost."],"url":"http://arxiv.org/abs/2405.03491v1","category":"cs.RO"}
{"created":"2024-05-06 14:02:59","title":"Gravitational Wave Polarization Detection with Pyramid Constellation of Gravitational Wave Observatory","abstract":"In general relativity (GR), gravitational wave (GW) polarization modes are tensor named as plus and cross modes, which have been detected from the LIGO, Virgo, and KAGRA collaborations. The other GW modes beyond GR, called as two scalar polarization modes and two vector modes, have being probed especially with an expected network of Advanced LIGO, Advanced Virgo, and KAGRA. For the space-based GW detection, the LISA - Taiji networks can perform the detection of GW polarizations. For the first time, we have come up with the pyramid constellation of gravitational wave observatory(PCGO) for GW polarization detection. The configuration of PCGO can be simultaneously sensitive to the six polarization modes of GWs. The response intensity of six detector arms changes with the detector positions on orbit in one year period. The newly added three arms have a different response of GWs from the triangle arms, comparing to LISA, Taiji and Tianqin. That indicates that there are more freedoms for the polarization component extraction from the total polarization signals of GWs. Comparing to LISA, Taiji and Tianqin configuration, PCGO has more combinations of optical paths of time delay interferometer (TDI) to suppress the frequency noise. The unequal arm Michelson TDI configuration and the Sagnac TDI configuration are equally effective for elimination of the laser frequency noise.","sentences":["In general relativity (GR), gravitational wave (GW) polarization modes are tensor named as plus and cross modes, which have been detected from the LIGO, Virgo, and KAGRA collaborations.","The other GW modes beyond GR, called as two scalar polarization modes and two vector modes, have being probed especially with an expected network of Advanced LIGO, Advanced Virgo, and KAGRA.","For the space-based GW detection, the LISA - Taiji networks can perform the detection of GW polarizations.","For the first time, we have come up with the pyramid constellation of gravitational wave observatory(PCGO) for GW polarization detection.","The configuration of PCGO can be simultaneously sensitive to the six polarization modes of GWs.","The response intensity of six detector arms changes with the detector positions on orbit in one year period.","The newly added three arms have a different response of GWs from the triangle arms, comparing to LISA, Taiji and Tianqin.","That indicates that there are more freedoms for the polarization component extraction from the total polarization signals of GWs.","Comparing to LISA, Taiji and Tianqin configuration, PCGO has more combinations of optical paths of time delay interferometer (TDI) to suppress the frequency noise.","The unequal arm Michelson TDI configuration and the Sagnac TDI configuration are equally effective for elimination of the laser frequency noise."],"url":"http://arxiv.org/abs/2405.03492v1","category":"gr-qc"}
{"created":"2024-05-06 14:02:45","title":"Robustness of inflation to kinetic inhomogeneities","abstract":"We investigate the effects of large inhomogeneities in both the inflaton field and its momentum. We find that in general, large kinetic perturbations reduce the number of e-folds of inflation. In particular, we observe that inflationary models with sub-Planckian characteristic scales are not robust even to kinetic energy densities that are sub-dominant to the potential energy density. This strengthens the results of our previous work. In inflationary models with super-Planckian characteristic scales, despite a reduction in the number of e-folds, inflation is robust even when the potential energy density is initially sub-dominant. For the cases we study, the robustness of inflation strongly depends on whether the inflaton field is driven into the reheating phase by the inhomogeneous scalar dynamics.","sentences":["We investigate the effects of large inhomogeneities in both the inflaton field and its momentum.","We find that in general, large kinetic perturbations reduce the number of e-folds of inflation.","In particular, we observe that inflationary models with sub-Planckian characteristic scales are not robust even to kinetic energy densities that are sub-dominant to the potential energy density.","This strengthens the results of our previous work.","In inflationary models with super-Planckian characteristic scales, despite a reduction in the number of e-folds, inflation is robust even when the potential energy density is initially sub-dominant.","For the cases we study, the robustness of inflation strongly depends on whether the inflaton field is driven into the reheating phase by the inhomogeneous scalar dynamics."],"url":"http://arxiv.org/abs/2405.03490v1","category":"astro-ph.CO"}
{"created":"2024-05-06 14:01:05","title":"On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations","abstract":"Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection. However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models. This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets. Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances. Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far. This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives. Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods. Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data. Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods. Data resampling on raw data yields superior results compared to data resampling in the feature space. In most cases, certain undersampling and hybrid methods show limited effectiveness. Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling. In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD.","sentences":["Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection.","However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models.","This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets.","Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances.","Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far.","This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives.","Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods.","Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data.","Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods.","Data resampling on raw data yields superior results compared to data resampling in the feature space.","In most cases, certain undersampling and hybrid methods show limited effectiveness.","Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling.","In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD."],"url":"http://arxiv.org/abs/2405.03489v1","category":"cs.SE"}
{"created":"2024-05-06 13:59:54","title":"Accurate and Fast Approximate Graph Pattern Mining at Scale","abstract":"Approximate graph pattern mining (A-GPM) is an important data analysis tool for many graph-based applications. There exist sampling-based A-GPM systems to provide automation and generalization over a wide variety of use cases. However, there are two major obstacles that prevent existing A-GPM systems being adopted in practice. First, the termination mechanism that decides when to end sampling lacks theoretical backup on confidence, and is unstable and slow in practice. Second, they suffer poor performance when dealing with the \"needle-in-the-hay\" cases, because a huge number of samples are required to converge, given the extremely low hit rate of their fixed sampling schemes. We build ScaleGPM, an accurate and fast A-GPM system that removes the two obstacles. First, we propose a novel on-the-fly convergence detection mechanism to achieve stable termination and provide theoretical guarantee on the confidence, with negligible overhead. Second, we propose two techniques to deal with the \"needle-in-the-hay\" problem, eager-verify and hybrid sampling. Our eager-verify method improves sampling hit rate by pruning unpromising candidates as early as possible. Hybrid sampling improves performance by automatically choosing the better scheme between fine-grained and coarse-grained sampling schemes. Experiments show that our online convergence detection mechanism can detect convergence and results in stable and rapid termination with theoretically guaranteed confidence. We show the effectiveness of eager-verify in improving the hit rate, and the scheme-selection mechanism in correctly choosing the better scheme for various cases. Overall, ScaleGPM achieves a geomean average of 565x (up to 610169x) speedup over the state-of-the-art A-GPM system, Arya. In particular, ScaleGPM handles billion-scale graphs in seconds, where existing systems either run out of memory or fail to complete in hours.","sentences":["Approximate graph pattern mining (A-GPM) is an important data analysis tool for many graph-based applications.","There exist sampling-based A-GPM systems to provide automation and generalization over a wide variety of use cases.","However, there are two major obstacles that prevent existing A-GPM systems being adopted in practice.","First, the termination mechanism that decides when to end sampling lacks theoretical backup on confidence, and is unstable and slow in practice.","Second, they suffer poor performance when dealing with the \"needle-in-the-hay\" cases, because a huge number of samples are required to converge, given the extremely low hit rate of their fixed sampling schemes.","We build ScaleGPM, an accurate and fast A-GPM system that removes the two obstacles.","First, we propose a novel on-the-fly convergence detection mechanism to achieve stable termination and provide theoretical guarantee on the confidence, with negligible overhead.","Second, we propose two techniques to deal with the \"needle-in-the-hay\" problem, eager-verify and hybrid sampling.","Our eager-verify method improves sampling hit rate by pruning unpromising candidates as early as possible.","Hybrid sampling improves performance by automatically choosing the better scheme between fine-grained and coarse-grained sampling schemes.","Experiments show that our online convergence detection mechanism can detect convergence and results in stable and rapid termination with theoretically guaranteed confidence.","We show the effectiveness of eager-verify in improving the hit rate, and the scheme-selection mechanism in correctly choosing the better scheme for various cases.","Overall, ScaleGPM achieves a geomean average of 565x (up to 610169x) speedup over the state-of-the-art A-GPM system, Arya.","In particular, ScaleGPM handles billion-scale graphs in seconds, where existing systems either run out of memory or fail to complete in hours."],"url":"http://arxiv.org/abs/2405.03488v1","category":"cs.PF"}
{"created":"2024-05-06 13:57:03","title":"UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images","abstract":"Image safety classifiers play an important role in identifying and mitigating the spread of unsafe images online (e.g., images including violence, hateful rhetoric, etc.). At the same time, with the advent of text-to-image models and increasing concerns about the safety of AI models, developers are increasingly relying on image safety classifiers to safeguard their models. Yet, the performance of current image safety classifiers remains unknown for real-world and AI-generated images. To bridge this research gap, in this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough in mitigating the multifaceted problem of unsafe images. Also, we find that classifiers trained only on real-world images tend to have degraded performance when applied to AI-generated images. Motivated by these findings, we design and implement a comprehensive image moderation tool called PerspectiveVision, which effectively identifies 11 categories of real-world and AI-generated unsafe images. The best PerspectiveVision model achieves an overall F1-Score of 0.810 on six evaluation datasets, which is comparable with closed-source and expensive state-of-the-art models like GPT-4V. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.","sentences":["Image safety classifiers play an important role in identifying and mitigating the spread of unsafe images online (e.g., images including violence, hateful rhetoric, etc.).","At the same time, with the advent of text-to-image models and increasing concerns about the safety of AI models, developers are increasingly relying on image safety classifiers to safeguard their models.","Yet, the performance of current image safety classifiers remains unknown for real-world and AI-generated images.","To bridge this research gap, in this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers.","First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.).","Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models.","Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough in mitigating the multifaceted problem of unsafe images.","Also, we find that classifiers trained only on real-world images tend to have degraded performance when applied to AI-generated images.","Motivated by these findings, we design and implement a comprehensive image moderation tool called PerspectiveVision, which effectively identifies 11 categories of real-world and AI-generated unsafe images.","The best PerspectiveVision model achieves an overall F1-Score of 0.810 on six evaluation datasets, which is comparable with closed-source and expensive state-of-the-art models like GPT-4V. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI."],"url":"http://arxiv.org/abs/2405.03486v1","category":"cs.CR"}
{"created":"2024-05-06 13:56:56","title":"LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model","abstract":"In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM","sentences":["In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation.","LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation.","Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts.","To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment.","Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence.","Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications.","Code and data for this paper are available at https://github.com/L-Sun/LGTM"],"url":"http://arxiv.org/abs/2405.03485v1","category":"cs.CV"}
{"created":"2024-05-06 13:55:39","title":"Whispy: Adapting STT Whisper Models to Real-Time Environments","abstract":"Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection. However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications. In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models. As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost. We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output. Experimental results show how Whispy excels in robustness, promptness, and accuracy.","sentences":["Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis.","In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection.","However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications.","In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models.","As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost.","We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output.","Experimental results show how Whispy excels in robustness, promptness, and accuracy."],"url":"http://arxiv.org/abs/2405.03484v1","category":"cs.SD"}
{"created":"2024-05-06 13:54:59","title":"On certain matrix algebras related to quasi-Toeplitz matrices","abstract":"Let $A_\\alpha$ be the semi-infinite tridiagonal matrix having subdiagonal and superdiagonal unit entries, $(A_\\alpha)_{11}=\\alpha$, where $\\alpha\\in\\mathbb C$, and zero elsewhere. A basis $\\{P_0,P_1,P_2,\\ldots\\}$ of the linear space $\\mathcal P_\\alpha$ spanned by the powers of $A_\\alpha$ is determined, where $P_0=I$, $P_n=T_n+H_n$, $T_n$ is the symmetric Toeplitz matrix having ones in the $n$th super- and sub-diagonal, zeros elsewhere, and $H_n$ is the Hankel matrix with first row $[\\theta\\alpha^{n-2}, \\theta\\alpha^{n-3}, \\ldots, \\theta, \\alpha, 0, \\ldots]$, where $\\theta=\\alpha^2-1$. The set $\\mathcal P_\\alpha$ is an algebra, and for $\\alpha\\in\\{-1,0,1\\}$, $H_n$ has only one nonzero anti-diagonal. This fact is exploited to provide a better representation of symmetric quasi-Toeplitz matrices $\\mathcal {QT}_S$, where, instead of representing a generic matrix $A\\in\\mathcal{QT}_S$ as $A=T+K$, where $T$ is Toeplitz and $K$ is compact, it is represented as $A=P+H$, where $P\\in\\mathcal P_\\alpha$ and $H$ is compact. It is shown experimentally that the matrix arithmetic obtained this way is much more effective than that implemented in the CQT-Toolbox of Numer.~Algo. 81(2):741--769, 2019.","sentences":["Let $A_\\alpha$ be the semi-infinite tridiagonal matrix having subdiagonal and superdiagonal unit entries, $(A_\\alpha)_{11}=\\alpha$, where $\\alpha\\in\\mathbb C$, and zero elsewhere.","A basis $\\{P_0,P_1,P_2,\\ldots\\}$ of the linear space $\\mathcal P_\\alpha$ spanned by the powers of $A_\\alpha$ is determined, where $P_0=I$, $P_n=T_n+H_n$, $T_n$ is the symmetric Toeplitz matrix having ones in the $n$th super- and sub-diagonal, zeros elsewhere, and $H_n$ is the Hankel matrix with first row $[\\theta\\alpha^{n-2}, \\theta\\alpha^{n-3}, \\ldots, \\theta, \\alpha, 0, \\ldots]$, where","$\\theta=\\alpha^2-1$.","The set $\\mathcal P_\\alpha$ is an algebra, and for $\\alpha\\in\\{-1,0,1\\}$, $H_n$ has only one nonzero anti-diagonal.","This fact is exploited to provide a better representation of symmetric quasi-Toeplitz matrices $\\mathcal {QT}_S$, where, instead of representing a generic matrix $A\\in\\mathcal{QT}_S$ as $A=T+K$, where $T$ is Toeplitz and $K$ is compact, it is represented as $A=P+H$, where $P\\in\\mathcal P_\\alpha$ and $H$ is compact.","It is shown experimentally that the matrix arithmetic obtained this way is much more effective than that implemented in the CQT-Toolbox of Numer.~Algo.","81(2):741--769, 2019."],"url":"http://arxiv.org/abs/2405.03483v1","category":"math.NA"}
{"created":"2024-05-06 13:53:03","title":"Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search","abstract":"The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.","sentences":["The future of conversational agents will provide users with personalized information responses.","However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences.","Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks.","Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues.","This method has proven to speed up the creation process and improve quality.","LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences.","When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods.","The collected dataset is suited to train preference extraction and personalized response generation.","Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history.","Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods."],"url":"http://arxiv.org/abs/2405.03480v1","category":"cs.IR"}
{"created":"2024-05-06 13:52:02","title":"Synthetic Datasets for Program Similarity Research","abstract":"Program similarity has become an increasingly popular area of research with various security applications such as plagiarism detection, author identification, and malware analysis. However, program similarity research faces a few unique dataset quality problems in evaluating the effectiveness of novel approaches. First, few high-quality datasets for binary program similarity exist and are widely used in this domain. Second, there are potentially many different, disparate definitions of what makes one program similar to another and in many cases there is often a large semantic gap between the labels provided by a dataset and any useful notion of behavioral or semantic similarity. In this paper, we present HELIX - a framework for generating large, synthetic program similarity datasets. We also introduce Blind HELIX, a tool built on top of HELIX for extracting HELIX components from library code automatically using program slicing. We evaluate HELIX and Blind HELIX by comparing the performance of program similarity tools on a HELIX dataset to a hand-crafted dataset built from multiple, disparate notions of program similarity. Using Blind HELIX, we show that HELIX can generate realistic and useful datasets of virtually infinite size for program similarity research with ground truth labels that embody practical notions of program similarity. Finally, we discuss the results and reason about relative tool ranking.","sentences":["Program similarity has become an increasingly popular area of research with various security applications such as plagiarism detection, author identification, and malware analysis.","However, program similarity research faces a few unique dataset quality problems in evaluating the effectiveness of novel approaches.","First, few high-quality datasets for binary program similarity exist and are widely used in this domain.","Second, there are potentially many different, disparate definitions of what makes one program similar to another and in many cases there is often a large semantic gap between the labels provided by a dataset and any useful notion of behavioral or semantic similarity.","In this paper, we present HELIX - a framework for generating large, synthetic program similarity datasets.","We also introduce Blind HELIX, a tool built on top of HELIX for extracting HELIX components from library code automatically using program slicing.","We evaluate HELIX and Blind HELIX by comparing the performance of program similarity tools on a HELIX dataset to a hand-crafted dataset built from multiple, disparate notions of program similarity.","Using Blind HELIX, we show that HELIX can generate realistic and useful datasets of virtually infinite size for program similarity research with ground truth labels that embody practical notions of program similarity.","Finally, we discuss the results and reason about relative tool ranking."],"url":"http://arxiv.org/abs/2405.03478v1","category":"cs.CR"}
{"created":"2024-05-06 13:51:14","title":"Even and odd compositions with restricted parts","abstract":"A result of Legendre asserts that the difference between the numbers of (length) even and odd partitions of $n$ into distinct parts is $0$, $1$, or $-1$; this also follows from Euler's pentagonal number theorem. We establish an analogous result for compositions and obtain some generalizations that are related to various entries in the On-Line Encyclopedia of Integer Sequences.","sentences":["A result of Legendre asserts that the difference between the numbers of (length) even and odd partitions of $n$ into distinct parts is $0$, $1$, or $-1$; this also follows from Euler's pentagonal number theorem.","We establish an analogous result for compositions and obtain some generalizations that are related to various entries in the On-Line Encyclopedia of Integer Sequences."],"url":"http://arxiv.org/abs/2405.03477v1","category":"math.CO"}
{"created":"2024-05-06 13:47:09","title":"A Symplectic Analysis of Alternating Mirror Descent","abstract":"Motivated by understanding the behavior of the Alternating Mirror Descent (AMD) algorithm for bilinear zero-sum games, we study the discretization of continuous-time Hamiltonian flow via the symplectic Euler method. We provide a framework for analysis using results from Hamiltonian dynamics, Lie algebra, and symplectic numerical integrators, with an emphasis on the existence and properties of a conserved quantity, the modified Hamiltonian (MH), for the symplectic Euler method. We compute the MH in closed-form when the original Hamiltonian is a quadratic function, and show that it generally differs from the other conserved quantity known previously in that case. We derive new error bounds on the MH when truncated at orders in the stepsize in terms of the number of iterations, $K$, and utilize this bound to show an improved $\\mathcal{O}(K^{1/5})$ total regret bound and an $\\mathcal{O}(K^{-4/5})$ duality gap of the average iterates for AMD. Finally, we propose a conjecture which, if true, would imply that the total regret for AMD goes as $\\mathcal{O}\\left(K^{\\varepsilon}\\right)$ and the duality gap of the average iterates as $\\mathcal{O}\\left(K^{-1+\\varepsilon}\\right)$ for any $\\varepsilon>0$, and we can take $\\varepsilon=0$ upon certain convergence conditions for the MH.","sentences":["Motivated by understanding the behavior of the Alternating Mirror Descent (AMD) algorithm for bilinear zero-sum games, we study the discretization of continuous-time Hamiltonian flow via the symplectic Euler method.","We provide a framework for analysis using results from Hamiltonian dynamics, Lie algebra, and symplectic numerical integrators, with an emphasis on the existence and properties of a conserved quantity, the modified Hamiltonian (MH), for the symplectic Euler method.","We compute the MH in closed-form when the original Hamiltonian is a quadratic function, and show that it generally differs from the other conserved quantity known previously in that case.","We derive new error bounds on the MH when truncated at orders in the stepsize in terms of the number of iterations, $K$, and utilize this bound to show an improved $\\mathcal{O}(K^{1/5})$ total regret bound and an $\\mathcal{O}(K^{-4/5})$ duality gap of the average iterates for AMD.","Finally, we propose a conjecture which, if true, would imply that the total regret for AMD goes as $\\mathcal{O}\\left(K^{\\varepsilon}\\right)$ and the duality gap of the average iterates as $\\mathcal{O}\\left(K^{-1+\\varepsilon}\\right)$ for any $\\varepsilon>0$, and we can take $\\varepsilon=0$ upon certain convergence conditions for the MH."],"url":"http://arxiv.org/abs/2405.03472v1","category":"math.OC"}
{"created":"2024-05-06 13:46:09","title":"Off-equatorial deflections and gravitational lensing. II. In general stationary and axisymmetric spacetimes","abstract":"In this work, we develop a general perturbative procedure to find the off-equatorial plane deflections in the weak deflection limit in general stationary and axisymmetric spacetimes satisfying some common variable separation conditions. Deflections of both null and timelike rays with the finite distance effect of the source and detector taken into account are obtained as dual series of $M/r_0$ and $r_0/r_{s,d}$. These deflections allow a set of exact gravitational lensing equations from which the images' apparent angular positions are solved. The method and general results are then applied to the Kerr-Newmann, Kerr-Sen, and rotating Simpson-Visser spacetimes to study the effect of the spin and characteristic (effective) charge of the spacetimes and the source altitude on the deflection angles and image apparent angles. It is found that in general, both the spacetime spin and charge only affect the deflections from the second non-trivial order, while the source altitude influences the deflection from the leading order. Because of these, it is found that in gravitational lensing in realistic situations, it is hard to measure the effects of the spacetime spin and charge from the images' apparent locations. We also presented the off-equatorial deflections in the rotating Bardeen, Hayward, Ghosh, and Tinchev black hole spacetimes.","sentences":["In this work, we develop a general perturbative procedure to find the off-equatorial plane deflections in the weak deflection limit in general stationary and axisymmetric spacetimes satisfying some common variable separation conditions.","Deflections of both null and timelike rays with the finite distance effect of the source and detector taken into account are obtained as dual series of $M/r_0$ and $r_0/r_{s,d}$. These deflections allow a set of exact gravitational lensing equations from which the images' apparent angular positions are solved.","The method and general results are then applied to the Kerr-Newmann, Kerr-Sen, and rotating Simpson-Visser spacetimes to study the effect of the spin and characteristic (effective) charge of the spacetimes and the source altitude on the deflection angles and image apparent angles.","It is found that in general, both the spacetime spin and charge only affect the deflections from the second non-trivial order, while the source altitude influences the deflection from the leading order.","Because of these, it is found that in gravitational lensing in realistic situations, it is hard to measure the effects of the spacetime spin and charge from the images' apparent locations.","We also presented the off-equatorial deflections in the rotating Bardeen, Hayward, Ghosh, and Tinchev black hole spacetimes."],"url":"http://arxiv.org/abs/2405.03471v1","category":"gr-qc"}
{"created":"2024-05-06 13:44:51","title":"Hierarchic Flows to Estimate and Sample High-dimensional Probabilities","abstract":"Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov. Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality. It may be avoided by following a hierarchic probability flow from coarse to fine scales. This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis. For a $\\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition. An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales. We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies. They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales. We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities.","sentences":["Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov.","Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality.","It may be avoided by following a hierarchic probability flow from coarse to fine scales.","This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis.","For a $\\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition.","An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales.","We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies.","They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales.","We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities."],"url":"http://arxiv.org/abs/2405.03468v1","category":"stat.ML"}
{"created":"2024-05-06 13:44:42","title":"Welfare Loss in Connected Resource Allocation","abstract":"We study the allocation of indivisible goods that form an undirected graph and investigate the worst-case welfare loss when requiring that each agent must receive a connected subgraph. Our focus is on both egalitarian and utilitarian welfare. Specifically, we introduce the concept of egalitarian (resp., utilitarian) price of connectivity, which captures the worst-case ratio between the optimal egalitarian (resp., utilitarian) welfare among all allocations and that among the connected allocations. We provide tight or asymptotically tight bounds on the price of connectivity for various large classes of graphs when there are two agents as well as for paths, stars and cycles in the general case. Many of our results are supplemented with algorithms which find connected allocations with a welfare guarantee corresponding to the price of connectivity.","sentences":["We study the allocation of indivisible goods that form an undirected graph and investigate the worst-case welfare loss when requiring that each agent must receive a connected subgraph.","Our focus is on both egalitarian and utilitarian welfare.","Specifically, we introduce the concept of egalitarian (resp., utilitarian) price of connectivity, which captures the worst-case ratio between the optimal egalitarian (resp., utilitarian) welfare among all allocations and that among the connected allocations.","We provide tight or asymptotically tight bounds on the price of connectivity for various large classes of graphs when there are two agents as well as for paths, stars and cycles in the general case.","Many of our results are supplemented with algorithms which find connected allocations with a welfare guarantee corresponding to the price of connectivity."],"url":"http://arxiv.org/abs/2405.03467v1","category":"cs.GT"}
{"created":"2024-05-06 13:38:25","title":"Geometric formulation of generalized root-$T\\bar{T}$ deformations","abstract":"We develop a generic geometric formalism that incorporates both $T\\bar{T}$-like and root-$T\\bar{T}$-like deformations in arbitrary dimensions. This framework applies to a wide family of stress-energy tensor perturbations and encompasses various well-known field theories. Building upon the recently proposed correspondence between Ricci-based gravity and $T\\bar{T}$-like deformations, we further extend this duality to include root-$T\\bar{T}$-like perturbations. This refinement extends the potential applications of our approach and contributes to a deeper exploration of the interplay between stress tensor perturbations and gravitational dynamics. Among the various original outcomes detailed in this article, we have also obtained a deformation of the flat Jackiw-Teitelboim gravity action.","sentences":["We develop a generic geometric formalism that incorporates both $T\\bar{T}$-like and root-$T\\bar{T}$-like deformations in arbitrary dimensions.","This framework applies to a wide family of stress-energy tensor perturbations and encompasses various well-known field theories.","Building upon the recently proposed correspondence between Ricci-based gravity and $T\\bar{T}$-like deformations, we further extend this duality to include root-$T\\bar{T}$-like perturbations.","This refinement extends the potential applications of our approach and contributes to a deeper exploration of the interplay between stress tensor perturbations and gravitational dynamics.","Among the various original outcomes detailed in this article, we have also obtained a deformation of the flat Jackiw-Teitelboim gravity action."],"url":"http://arxiv.org/abs/2405.03465v1","category":"hep-th"}
{"created":"2024-05-06 13:33:38","title":"A Lightweight Neural Architecture Search Model for Medical Image Classification","abstract":"Accurate classification of medical images is essential for modern diagnostics. Deep learning advancements led clinicians to increasingly use sophisticated models to make faster and more accurate decisions, sometimes replacing human judgment. However, model development is costly and repetitive. Neural Architecture Search (NAS) provides solutions by automating the design of deep learning architectures. This paper presents ZO-DARTS+, a differentiable NAS algorithm that improves search efficiency through a novel method of generating sparse probabilities by bi-level optimization. Experiments on five public medical datasets show that ZO-DARTS+ matches the accuracy of state-of-the-art solutions while reducing search times by up to three times.","sentences":["Accurate classification of medical images is essential for modern diagnostics.","Deep learning advancements led clinicians to increasingly use sophisticated models to make faster and more accurate decisions, sometimes replacing human judgment.","However, model development is costly and repetitive.","Neural Architecture Search (NAS) provides solutions by automating the design of deep learning architectures.","This paper presents ZO-DARTS+, a differentiable NAS algorithm that improves search efficiency through a novel method of generating sparse probabilities by bi-level optimization.","Experiments on five public medical datasets show that ZO-DARTS+ matches the accuracy of state-of-the-art solutions while reducing search times by up to three times."],"url":"http://arxiv.org/abs/2405.03462v1","category":"cs.CV"}
{"created":"2024-05-06 13:32:53","title":"Variations on a Theme of Makowski","abstract":"We distinguish the axiomatic study of proofs in geometry from study about geometry from general axioms for mathematics. We briefly report on an abuse of that distinction and its unfortunate effect on US high school education. We review a number of 20th century approaches to synthetic geometry. In doing so, we disambiguate (in the Wikipedia sense) the terms: metric, orthogonal, isotropic and hyperbolic. With some of these systems we are able to axiomatize `affine geometry' over the complex field (The argument is trivial from [Wu94] or [Szm78], but not remarked by either of them.). We examine the general question of the connections between axioms for Affine geometries and the stability classification of associated complete first order theories of fields. We conclude with reminiscences of a half-century friendship with Jan\\'{o}s.","sentences":["We distinguish the axiomatic study of proofs in geometry from study about geometry from general axioms for mathematics.","We briefly report on an abuse of that distinction and its unfortunate effect on US high school education.","We review a number of 20th century approaches to synthetic geometry.","In doing so, we disambiguate (in the Wikipedia sense) the terms: metric, orthogonal, isotropic and hyperbolic.","With some of these systems we are able to axiomatize `affine geometry' over the complex field (The argument is trivial from [Wu94] or [Szm78], but not remarked by either of them.).","We examine the general question of the connections between axioms for Affine geometries and the stability classification of associated complete first order theories of fields.","We conclude with reminiscences of a half-century friendship with Jan\\'{o}s."],"url":"http://arxiv.org/abs/2405.03461v1","category":"math.MG"}
{"created":"2024-05-06 13:27:19","title":"Flux-Tunable Regimes and Supersymmetry in Twisted Cuprate Heterostructures","abstract":"Van der Waals assembly allows for the creation of Josephson junctions in an atomically sharp interface between two exfoliated Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (Bi-2212) flakes that are twisted relative to each other. In a narrow range of angles close to $45^\\circ$, the junction exhibits a regime where time-reversal symmetry can be spontaneously broken and it can be used to encode an inherently protected qubit called flowermon. In this work we investigate the physics emerging when two such junctions are integrated in a SQuID circuit threaded by a magnetic flux. We show that the flowermon qubit regime is maintained up to a finite critical value of the magnetic field and, under appropriate conditions, it is protected against both charge and flux noise. For larger external fluxes, the interplay between the inherent twisted d-wave nature of the order parameter and the external magnetic flux enables the implementation of different artificial atoms, including a flux-biased protected qubit and a supersymmetric quantum circuit.","sentences":["Van der Waals assembly allows for the creation of Josephson junctions in an atomically sharp interface between two exfoliated Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (Bi-2212) flakes that are twisted relative to each other.","In a narrow range of angles close to $45^\\circ$, the junction exhibits a regime where time-reversal symmetry can be spontaneously broken and it can be used to encode an inherently protected qubit called flowermon.","In this work we investigate the physics emerging when two such junctions are integrated in a SQuID circuit threaded by a magnetic flux.","We show that the flowermon qubit regime is maintained up to a finite critical value of the magnetic field and, under appropriate conditions, it is protected against both charge and flux noise.","For larger external fluxes, the interplay between the inherent twisted d-wave nature of the order parameter and the external magnetic flux enables the implementation of different artificial atoms, including a flux-biased protected qubit and a supersymmetric quantum circuit."],"url":"http://arxiv.org/abs/2405.03454v1","category":"cond-mat.supr-con"}
{"created":"2024-05-06 13:25:02","title":"A weighted multilevel Monte Carlo method","abstract":"The Multilevel Monte Carlo (MLMC) method has been applied successfully in a wide range of settings since its first introduction by Giles (2008). When using only two levels, the method can be viewed as a kind of control-variate approach to reduce variance, as earlier proposed by Kebaier (2005). We introduce a generalization of the MLMC formulation by extending this control variate approach to any number of levels and deriving a recursive formula for computing the weights associated with the control variates and the optimal numbers of samples at the various levels.   We also show how the generalisation can also be applied to the \\emph{multi-index} MLMC method of Haji-Ali, Nobile, Tempone (2015), at the cost of solving a $(2^d-1)$-dimensional minimisation problem at each node when $d$ index dimensions are used.   The comparative performance of the weighted MLMC method is illustrated in a range of numerical settings. While the addition of weights does not change the \\emph{asymptotic} complexity of the method, the results show that significant efficiency improvements over the standard MLMC formulation are possible, particularly when the coarse level approximations are poorly correlated.","sentences":["The Multilevel Monte Carlo (MLMC) method has been applied successfully in a wide range of settings since its first introduction by Giles (2008).","When using only two levels, the method can be viewed as a kind of control-variate approach to reduce variance, as earlier proposed by Kebaier (2005).","We introduce a generalization of the MLMC formulation by extending this control variate approach to any number of levels and deriving a recursive formula for computing the weights associated with the control variates and the optimal numbers of samples at the various levels.   ","We also show how the generalisation can also be applied to the \\emph{multi-index} MLMC method of Haji-Ali, Nobile, Tempone (2015), at the cost of solving a $(2^d-1)$-dimensional minimisation problem at each node when $d$ index dimensions are used.   ","The comparative performance of the weighted MLMC method is illustrated in a range of numerical settings.","While the addition of weights does not change the \\emph{asymptotic} complexity of the method, the results show that significant efficiency improvements over the standard MLMC formulation are possible, particularly when the coarse level approximations are poorly correlated."],"url":"http://arxiv.org/abs/2405.03453v1","category":"q-fin.CP"}
{"created":"2024-05-06 13:23:57","title":"Large Language Models (LLMs) as Agents for Augmented Democracy","abstract":"We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.","sentences":["We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections.","We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants.","At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants.","At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs.","We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population.","These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy."],"url":"http://arxiv.org/abs/2405.03452v1","category":"cs.CY"}
{"created":"2024-05-06 13:23:40","title":"Uncertainty of Supply Chains: Risk and Ambiguity","abstract":"Motivated by the recently experienced systemic shocks (the COVID-19 pandemic and the full-fledged Russia's war of aggression against Ukraine) - that have created new forms of uncertainties to our supplies - this paper explores the supply chain robustness under risk aversion and ambiguity aversion. We aim to understand the potential consequences of deeply uncertain systemic events on the supply chain resilience and how does the information precision affect individual agents' choices and the chain-level preparedness to aggregate shocks. Augmenting a parsimonious supply chain model with uncertainty, we analyse the relationship between the upstream sourcing decisions and the supply chain survival probability. Both risk-averse and ambiguity-averse individually-optimising agents' upstream sourcing paths are efficient but can become vulnerable to aggregate shocks. In contrast, a chain-level coordination of downstream firm sourcing decisions can qualitatively improve the robustness of the entire supply chain compared to the individual decision-making baseline. Such a robust decision making ensures that in the presence of an aggregate shock - independently of its realisation - part of upstream suppliers will survive and the final goods' supply will be ensured even under the most demanding circumstances. Our results also indicate that an input source diversification extracts a cost in foregone efficiency.","sentences":["Motivated by the recently experienced systemic shocks (the COVID-19 pandemic and the full-fledged Russia's war of aggression against Ukraine) - that have created new forms of uncertainties to our supplies - this paper explores the supply chain robustness under risk aversion and ambiguity aversion.","We aim to understand the potential consequences of deeply uncertain systemic events on the supply chain resilience and how does the information precision affect individual agents' choices and the chain-level preparedness to aggregate shocks.","Augmenting a parsimonious supply chain model with uncertainty, we analyse the relationship between the upstream sourcing decisions and the supply chain survival probability.","Both risk-averse and ambiguity-averse individually-optimising agents' upstream sourcing paths are efficient but can become vulnerable to aggregate shocks.","In contrast, a chain-level coordination of downstream firm sourcing decisions can qualitatively improve the robustness of the entire supply chain compared to the individual decision-making baseline.","Such a robust decision making ensures that in the presence of an aggregate shock - independently of its realisation - part of upstream suppliers will survive and the final goods' supply will be ensured even under the most demanding circumstances.","Our results also indicate that an input source diversification extracts a cost in foregone efficiency."],"url":"http://arxiv.org/abs/2405.03451v1","category":"econ.GN"}
{"created":"2024-05-06 13:22:54","title":"Byzantine-Robust Gossip: Insights from a Dual Approach","abstract":"Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information. This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another. We leverage the so-called dual approach to design a general robust decentralized optimization method. We provide both global and local clipping rules in the special case of average consensus, with tight convergence guarantees. These clipping rules are practical, and yield results that finely characterize the impact of Byzantine nodes, highlighting for instance a qualitative difference in convergence between global and local clipping thresholds. Lastly, we demonstrate that they can serve as a basis for designing efficient attacks.","sentences":["Distributed approaches have many computational benefits, but they are vulnerable to attacks from a subset of devices transmitting incorrect information.","This paper investigates Byzantine-resilient algorithms in a decentralized setting, where devices communicate directly with one another.","We leverage the so-called dual approach to design a general robust decentralized optimization method.","We provide both global and local clipping rules in the special case of average consensus, with tight convergence guarantees.","These clipping rules are practical, and yield results that finely characterize the impact of Byzantine nodes, highlighting for instance a qualitative difference in convergence between global and local clipping thresholds.","Lastly, we demonstrate that they can serve as a basis for designing efficient attacks."],"url":"http://arxiv.org/abs/2405.03449v1","category":"cs.LG"}
{"created":"2024-05-06 13:20:49","title":"Fluctuation-Driven Morphological Patterning: A Novel Approach to Morphogenesis","abstract":"Recent experimental investigations into Hydra regeneration revealed a remarkable phenomenon: the morphological transformation of a tissue fragment from the incipient spherical configuration to a tube-like structure - the hallmark of a mature Hydra - has the dynamical characteristics of a first-order phase-transition, with calcium field fluctuations within the tissue playing an essential role. This morphological transition was shown to be generated by activation over an energy barrier within an effective potential that underlies morphogenesis. Inspired by this intriguing insight, we propose a novel mechanism where stochastic fluctuations drive the emergence of morphological patterns. Thus, the inherent fluctuations determine the nature of the dynamics and are not incidental noise in the background of the otherwise deterministic dynamics. Instead, they play an important role as a driving force that defines the attributes of the pattern formation dynamics and the nature of the transition itself. Here, we present a simple model that captures the essence of this novel mechanism for morphological pattern formation. Specifically, we consider a one-dimensional tissue arranged as a closed contour embedded in a two-dimensional space, where the local curvature of the contour is coupled to a non-negative scalar field. An effective temperature parameter regulates the strength of the fluctuations in the system. The tissue exhibits fluctuations near a circular shape at sufficiently low coupling strengths, but as the coupling strength exceeds some critical value, the circular state becomes unstable. The nature of the transition, namely whether it is a first or a second-order-like transition, depends on the temperature and the effective cutoff on the wavelength of the spatial variations in the system. It is also found that entropic barriers separate the various metastable states of the system.","sentences":["Recent experimental investigations into Hydra regeneration revealed a remarkable phenomenon: the morphological transformation of a tissue fragment from the incipient spherical configuration to a tube-like structure - the hallmark of a mature Hydra - has the dynamical characteristics of a first-order phase-transition, with calcium field fluctuations within the tissue playing an essential role.","This morphological transition was shown to be generated by activation over an energy barrier within an effective potential that underlies morphogenesis.","Inspired by this intriguing insight, we propose a novel mechanism where stochastic fluctuations drive the emergence of morphological patterns.","Thus, the inherent fluctuations determine the nature of the dynamics and are not incidental noise in the background of the otherwise deterministic dynamics.","Instead, they play an important role as a driving force that defines the attributes of the pattern formation dynamics and the nature of the transition itself.","Here, we present a simple model that captures the essence of this novel mechanism for morphological pattern formation.","Specifically, we consider a one-dimensional tissue arranged as a closed contour embedded in a two-dimensional space, where the local curvature of the contour is coupled to a non-negative scalar field.","An effective temperature parameter regulates the strength of the fluctuations in the system.","The tissue exhibits fluctuations near a circular shape at sufficiently low coupling strengths, but as the coupling strength exceeds some critical value, the circular state becomes unstable.","The nature of the transition, namely whether it is a first or a second-order-like transition, depends on the temperature and the effective cutoff on the wavelength of the spatial variations in the system.","It is also found that entropic barriers separate the various metastable states of the system."],"url":"http://arxiv.org/abs/2405.03448v1","category":"physics.bio-ph"}
{"created":"2024-05-06 13:17:43","title":"SEvenLLM: Benchmarking, Eliciting, and Enhancing Abilities of Large Language Models in Cyber Threat Intelligence","abstract":"To address the increasing complexity and frequency of cybersecurity incidents emphasized by the recent cybersecurity threat reports with over 10 billion instances, cyber threat intelligence (CTI) plays a critical role in the modern cybersecurity landscape by offering the insights required to understand and combat the constantly evolving nature of cyber threats. Inspired by the powerful capability of large language models (LLMs) in handling complex tasks, in this paper, we introduce a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events (SEvenLLM). Specifically, we create a high-quality bilingual instruction corpus by crawling cybersecurity raw text from cybersecurity websites to overcome the lack of effective data for information extraction. Then, we design a pipeline to auto-select tasks from the tasks pool and convert the raw text into supervised corpora comprised of question and response. The instruction dataset SEvenLLM-Instruct is used to train cybersecurity LLMs with the multi-task learning objective (27 well-designed tasks) for augmenting the analysis of cybersecurity events. Extensive experiments in our curated benchmark (SEvenLLM-bench) demonstrate that SEvenLLM performs more sophisticated threat analysis and fortifies defenses against the evolving landscape of cyber threats.","sentences":["To address the increasing complexity and frequency of cybersecurity incidents emphasized by the recent cybersecurity threat reports with over 10 billion instances, cyber threat intelligence (CTI) plays a critical role in the modern cybersecurity landscape by offering the insights required to understand and combat the constantly evolving nature of cyber threats.","Inspired by the powerful capability of large language models (LLMs) in handling complex tasks, in this paper, we introduce a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events (SEvenLLM).","Specifically, we create a high-quality bilingual instruction corpus by crawling cybersecurity raw text from cybersecurity websites to overcome the lack of effective data for information extraction.","Then, we design a pipeline to auto-select tasks from the tasks pool and convert the raw text into supervised corpora comprised of question and response.","The instruction dataset SEvenLLM-Instruct is used to train cybersecurity LLMs with the multi-task learning objective (27 well-designed tasks) for augmenting the analysis of cybersecurity events.","Extensive experiments in our curated benchmark (SEvenLLM-bench) demonstrate that SEvenLLM performs more sophisticated threat analysis and fortifies defenses against the evolving landscape of cyber threats."],"url":"http://arxiv.org/abs/2405.03446v1","category":"cs.CR"}
{"created":"2024-05-06 13:16:42","title":"An elementary proof of the Benjamini-Nekrashevych-Pete conjecture for the semi-direct products $\\mathbb{Z}^n\\rtimes \\mathbb{Z}$","abstract":"A finitely generated group $G$ is called strongly scale-invariant if there exists an injective homomorphism $f:G\\to G$ such that $f(G)$ is a finite index subgroup of $G$ and such that $\\cap_{n\\geq 0} f^n(G)$ is finite. Nekrashevych and Pete conjectured that all strongly scale-invariant groups are virtually nilpotent, after disproving a stronger conjecture by Benjamini.   This conjecture is known to be true in some situations. Der\\'e proved it for virtually polycyclic groups. In this paper, we provide an elementary proof for those polycyclic groups that can be written as a semi-direct product $\\mathbb{Z}^n\\rtimes \\mathbb{Z}$.","sentences":["A finitely generated group $G$ is called strongly scale-invariant if there exists an injective homomorphism $f:G\\to G$ such that $f(G)$ is a finite index subgroup of $G$ and such that $\\cap_{n\\geq 0} f^n(G)$ is finite.","Nekrashevych and Pete conjectured that all strongly scale-invariant groups are virtually nilpotent, after disproving a stronger conjecture by Benjamini.   ","This conjecture is known to be true in some situations.","Der\\'e proved it for virtually polycyclic groups.","In this paper, we provide an elementary proof for those polycyclic groups that can be written as a semi-direct product $\\mathbb{Z}^n\\rtimes \\mathbb{Z}$."],"url":"http://arxiv.org/abs/2405.03445v1","category":"math.GR"}
{"created":"2024-05-06 13:12:25","title":"Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery","abstract":"In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery. Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor. Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on. Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data. We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness.","sentences":["In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery.","Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor.","Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on.","Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data.","We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness."],"url":"http://arxiv.org/abs/2405.03440v1","category":"cs.RO"}
{"created":"2024-05-06 13:02:47","title":"pyCFS-data: Data Processing Framework in Python for openCFS","abstract":"Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics. Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method. Since 2000, the software has been developed continuously. The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++). In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS.","sentences":["Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics.","Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method.","Since 2000, the software has been developed continuously.","The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++).","In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS."],"url":"http://arxiv.org/abs/2405.03437v1","category":"cs.CE"}
{"created":"2024-05-06 12:58:48","title":"A method for quantifying the generalization capabilities of generative models for solving Ising models","abstract":"For Ising models with complex energy landscapes, whether the ground state can be found by neural networks depends heavily on the Hamming distance between the training datasets and the ground state. Despite the fact that various recently proposed generative models have shown good performance in solving Ising models, there is no adequate discussion on how to quantify their generalization capabilities. Here we design a Hamming distance regularizer in the framework of a class of generative models, variational autoregressive networks (VAN), to quantify the generalization capabilities of various network architectures combined with VAN. The regularizer can control the size of the overlaps between the ground state and the training datasets generated by networks, which, together with the success rates of finding the ground state, form a quantitative metric to quantify their generalization capabilities. We conduct numerical experiments on several prototypical network architectures combined with VAN, including feed-forward neural networks, recurrent neural networks, and graph neural networks, to quantify their generalization capabilities when solving Ising models. Moreover, considering the fact that the quantification of the generalization capabilities of networks on small-scale problems can be used to predict their relative performance on large-scale problems, our method is of great significance for assisting in the Neural Architecture Search field of searching for the optimal network architectures when solving large-scale Ising models.","sentences":["For Ising models with complex energy landscapes, whether the ground state can be found by neural networks depends heavily on the Hamming distance between the training datasets and the ground state.","Despite the fact that various recently proposed generative models have shown good performance in solving Ising models, there is no adequate discussion on how to quantify their generalization capabilities.","Here we design a Hamming distance regularizer in the framework of a class of generative models, variational autoregressive networks (VAN), to quantify the generalization capabilities of various network architectures combined with VAN.","The regularizer can control the size of the overlaps between the ground state and the training datasets generated by networks, which, together with the success rates of finding the ground state, form a quantitative metric to quantify their generalization capabilities.","We conduct numerical experiments on several prototypical network architectures combined with VAN, including feed-forward neural networks, recurrent neural networks, and graph neural networks, to quantify their generalization capabilities when solving Ising models.","Moreover, considering the fact that the quantification of the generalization capabilities of networks on small-scale problems can be used to predict their relative performance on large-scale problems, our method is of great significance for assisting in the Neural Architecture Search field of searching for the optimal network architectures when solving large-scale Ising models."],"url":"http://arxiv.org/abs/2405.03435v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-06 12:53:02","title":"Phase transition between shock formation and stability in cosmological fluids","abstract":"We demonstrate a novel phase transition from stable to unstable fluid behaviour for fluid-filled cosmological spacetimes undergoing decelerated expansion. This transition occurs when the fluid speed of sound $c_S$ exceeds a critical value relative to the expansion rate $a(t) = t^\\alpha$ of spacetime. We present an explicit relationship between $\\alpha$ and $c_S$ , which subdivides the $(\\alpha,c_S)$-parameter space into two regions. Using rigorous techniques, we establish stability of quiet fluid solutions in the first stable region. Numerical experiments reveal that the complement of the stable region consists of unstable solutions, implying sharpness of our stability result. We provide a definitive analytical bound and high-precision numerical evidence for the exact location of the critical line separating the stable from the unstable region.","sentences":["We demonstrate a novel phase transition from stable to unstable fluid behaviour for fluid-filled cosmological spacetimes undergoing decelerated expansion.","This transition occurs when the fluid speed of sound $c_S$ exceeds a critical value relative to the expansion rate $a(t) = t^\\alpha$ of spacetime.","We present an explicit relationship between $\\alpha$ and $c_S$ , which subdivides the $(\\alpha,c_S)$-parameter space into two regions.","Using rigorous techniques, we establish stability of quiet fluid solutions in the first stable region.","Numerical experiments reveal that the complement of the stable region consists of unstable solutions, implying sharpness of our stability result.","We provide a definitive analytical bound and high-precision numerical evidence for the exact location of the critical line separating the stable from the unstable region."],"url":"http://arxiv.org/abs/2405.03431v1","category":"gr-qc"}
{"created":"2024-05-06 12:48:34","title":"ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers","abstract":"Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases. Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous, leading to models with computational demand infeasible for most practical applications. To bridge the gap between high method complexity and realistic computational resources, we introduce the Residual Cyclic Transformer, ReCycle. ReCycle utilizes primary cycle compression to address the computational complexity of the attention mechanism in long time series. By learning residuals from refined smoothing average techniques, ReCycle surpasses state-of-the-art accuracy in a variety of application use cases. The reliable and explainable fallback behavior ensured by simple, yet robust, smoothing average techniques additionally lowers the barrier for user acceptance. At the same time, our approach reduces the run time and energy consumption by more than an order of magnitude, making both training and inference feasible on low-performance, low-power and edge computing devices. Code is available at https://github.com/Helmholtz-AI-Energy/ReCycle","sentences":["Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases.","Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous, leading to models with computational demand infeasible for most practical applications.","To bridge the gap between high method complexity and realistic computational resources, we introduce the Residual Cyclic Transformer, ReCycle.","ReCycle utilizes primary cycle compression to address the computational complexity of the attention mechanism in long time series.","By learning residuals from refined smoothing average techniques, ReCycle surpasses state-of-the-art accuracy in a variety of application use cases.","The reliable and explainable fallback behavior ensured by simple, yet robust, smoothing average techniques additionally lowers the barrier for user acceptance.","At the same time, our approach reduces the run time and energy consumption by more than an order of magnitude, making both training and inference feasible on low-performance, low-power and edge computing devices.","Code is available at https://github.com/Helmholtz-AI-Energy/ReCycle"],"url":"http://arxiv.org/abs/2405.03429v1","category":"cs.LG"}
{"created":"2024-05-06 12:47:54","title":"On the categoricity of complete second order theories","abstract":"We show, assuming PD, that every complete finitely axiomatized second order theory with a countable model is categorical, but that there is, assuming again PD, a complete recursively axiomatized second order theory with a countable model which is non-categorical. We show that the existence of even very large (e.g. supercompact) cardinals does not imply the categoricity of all finite complete second order theories. More exactly, we show that a non-categorical complete finitely axiomatized second order theory can always be obtained by (set) forcing. We also show that the categoricity of all finite complete second order theories with a model of a certain singular cardinality kappa of uncountable cofinality can be forced over any model of set theory. Previously, Solovay had proved, assuming V=L, that every complete finitely axiomatized second order theory (with or without a countable model) is categorical, and that in a generic extension of L there is a complete finitely axiomatized second order theory with a countable model which is non-categorical.","sentences":["We show, assuming PD, that every complete finitely axiomatized second order theory with a countable model is categorical, but that there is, assuming again PD, a complete recursively axiomatized second order theory with a countable model which is non-categorical.","We show that the existence of even very large (e.g. supercompact) cardinals does not imply the categoricity of all finite complete second order theories.","More exactly, we show that a non-categorical complete finitely axiomatized second order theory can always be obtained by (set) forcing.","We also show that the categoricity of all finite complete second order theories with a model of a certain singular cardinality kappa of uncountable cofinality can be forced over any model of set theory.","Previously, Solovay had proved, assuming V=L, that every complete finitely axiomatized second order theory (with or without a countable model) is categorical, and that in a generic extension of L there is a complete finitely axiomatized second order theory with a countable model which is non-categorical."],"url":"http://arxiv.org/abs/2405.03428v1","category":"math.LO"}
{"created":"2024-05-06 12:46:43","title":"EdgeAlpha: Bringing Process Discovery to the Data Sources","abstract":"Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT). The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs. Further, privacy considerations often prevent data collection at a central location in the first place. To address this challenge, this paper introduces EdgeAlpha, a distributed algorithm for process discovery operating directly on sensor nodes and edge devices on a stream of real-time event data. Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded. From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model. EdgeAlpha enables (a) scalable mining, as a node, for each event, only interacts with its predecessors and, when queried, only exchanges aggregates, i.e., partial footprint matrices, with the central location and (b) privacy preserving process mining, as nodes only store their own as well as predecessor and successor events. On the Sepsis Cases event log, for example, a node queries on average 18.7% of all nodes. For the Hospital Log, we can even reduce the overall querying to 3.87% of the nodes.","sentences":["Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT).","The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs.","Further, privacy considerations often prevent data collection at a central location in the first place.","To address this challenge, this paper introduces EdgeAlpha, a distributed algorithm for process discovery operating directly on sensor nodes and edge devices on a stream of real-time event data.","Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded.","From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model.","EdgeAlpha enables (a) scalable mining, as a node, for each event, only interacts with its predecessors and, when queried, only exchanges aggregates, i.e., partial footprint matrices, with the central location and (b) privacy preserving process mining, as nodes only store their own as well as predecessor and successor events.","On the Sepsis Cases event log, for example, a node queries on average 18.7% of all nodes.","For the Hospital Log, we can even reduce the overall querying to 3.87% of the nodes."],"url":"http://arxiv.org/abs/2405.03426v1","category":"cs.DB"}
{"created":"2024-05-06 12:44:37","title":"Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models","abstract":"Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets. To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs. Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration. We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks.","sentences":["Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets.","To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs.","Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration.","We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks."],"url":"http://arxiv.org/abs/2405.03425v1","category":"cs.CL"}
{"created":"2024-05-06 12:44:03","title":"Generalized Baer and Generalized Quasi-Baer Rings of Skew Generalized Power Series","abstract":"Let $R$ be a ring with identity, $(S,\\leq)$ an ordered monoid, $\\omega:S \\to End(R)$ a monoid homomorphism, and $A= R\\left[\\left[S,\\omega \\right]\\right]$ the ring of skew generalized power series. The concepts of generalized Baer and generalized quasi-Baer rings are generalization of Baer and quasi-Baer rings, respectively. A ring $R$ is called generalized right Baer (generalized right quasi-Baer) if for any non-empty subset $S$ (right ideal $I$) of $R$, the right annihilator of $S^n \\hspace{0.1cm}(I^n)$ is generated by an idempotent for some positive integer $n$. Left cases may be defined analogously. A ring $R$ is called generalized Baer (generalized quasi-Baer) if it is both generalized right and left Baer (generalized right and left quasi-Baer) ring. In this paper, we examine the behavior of a skew generalized power series ring over a generalized right Baer (generalized right quasi-Baer) ring and prove that, under specific conditions, the ring $A$ is generalized right Baer (generalized right quasi-Baer) if and only if $R$ is a generalized right Baer (generalized right quasi-Baer) ring.","sentences":["Let $R$ be a ring with identity, $(S,\\leq)$ an ordered monoid, $\\omega:S \\to End(R)$ a monoid homomorphism, and $A= R\\left[\\left[S,\\omega \\right]\\right]$ the ring of skew generalized power series.","The concepts of generalized Baer and generalized quasi-Baer rings are generalization of Baer and quasi-Baer rings, respectively.","A ring $R$ is called generalized right Baer (generalized right quasi-Baer) if for any non-empty subset $S$ (right ideal $I$) of $R$, the right annihilator of $S^n \\hspace{0.1cm}(I^n)$ is generated by an idempotent for some positive integer $n$. Left cases may be defined analogously.","A ring $R$ is called generalized Baer (generalized quasi-Baer) if it is both generalized right and left Baer (generalized right and left quasi-Baer) ring.","In this paper, we examine the behavior of a skew generalized power series ring over a generalized right Baer (generalized right quasi-Baer) ring and prove that, under specific conditions, the ring $A$ is generalized right Baer (generalized right quasi-Baer) if and only if $R$ is a generalized right Baer (generalized right quasi-Baer) ring."],"url":"http://arxiv.org/abs/2405.03423v1","category":"math.RA"}
{"created":"2024-05-06 12:40:25","title":"Homotopy methods for higher order shape optimization: A globalized shape-Newton method and Pareto-front tracing","abstract":"First order shape optimization methods, in general, require a large number of iterations until they reach a locally optimal design. While higher order methods can significantly reduce the number of iterations, they exhibit only local convergence properties, necessitating a sufficiently close initial guess. In this work, we present an unregularized shape-Newton method and combine shape optimization with homotopy (or continuation) methods in order to allow for the use of higher order methods even if the initial design is far from a solution. The idea of homotopy methods is to continuously connect the problem of interest with a simpler problem and to follow the corresponding solution path by a predictor-corrector scheme. We use a shape-Newton method as a corrector and arbitrary order shape derivatives for the predictor. Moreover, we apply homotopy methods also to the case of multi-objective shape optimization to efficiently obtain well-distributed points on a Pareto front. Finally, our results are substantiated with a set of numerical experiments.","sentences":["First order shape optimization methods, in general, require a large number of iterations until they reach a locally optimal design.","While higher order methods can significantly reduce the number of iterations, they exhibit only local convergence properties, necessitating a sufficiently close initial guess.","In this work, we present an unregularized shape-Newton method and combine shape optimization with homotopy (or continuation) methods in order to allow for the use of higher order methods even if the initial design is far from a solution.","The idea of homotopy methods is to continuously connect the problem of interest with a simpler problem and to follow the corresponding solution path by a predictor-corrector scheme.","We use a shape-Newton method as a corrector and arbitrary order shape derivatives for the predictor.","Moreover, we apply homotopy methods also to the case of multi-objective shape optimization to efficiently obtain well-distributed points on a Pareto front.","Finally, our results are substantiated with a set of numerical experiments."],"url":"http://arxiv.org/abs/2405.03421v1","category":"math.NA"}
{"created":"2024-05-06 12:36:17","title":"Automated Metaheuristic Algorithm Design with Autoregressive Learning","abstract":"Automated design of metaheuristic algorithms offers an attractive avenue to reduce human effort and gain enhanced performance beyond human intuition. Current automated methods design algorithms within a fixed structure and operate from scratch. This poses a clear gap towards fully discovering potentials over the metaheuristic family and fertilizing from prior design experience. To bridge the gap, this paper proposes an autoregressive learning-based designer for automated design of metaheuristic algorithms. Our designer formulates metaheuristic algorithm design as a sequence generation task, and harnesses an autoregressive generative network to handle the task. This offers two advances. First, through autoregressive inference, the designer generates algorithms with diverse lengths and structures, enabling to fully discover potentials over the metaheuristic family. Second, prior design knowledge learned and accumulated in neurons of the designer can be retrieved for designing algorithms for future problems, paving the way to continual design of algorithms for open-ended problem-solving. Extensive experiments on numeral benchmarks and real-world problems reveal that the proposed designer generates algorithms that outperform all human-created baselines on 24 out of 25 test problems. The generated algorithms display various structures and behaviors, reasonably fitting for different problem-solving contexts. Code will be released after paper publication.","sentences":["Automated design of metaheuristic algorithms offers an attractive avenue to reduce human effort and gain enhanced performance beyond human intuition.","Current automated methods design algorithms within a fixed structure and operate from scratch.","This poses a clear gap towards fully discovering potentials over the metaheuristic family and fertilizing from prior design experience.","To bridge the gap, this paper proposes an autoregressive learning-based designer for automated design of metaheuristic algorithms.","Our designer formulates metaheuristic algorithm design as a sequence generation task, and harnesses an autoregressive generative network to handle the task.","This offers two advances.","First, through autoregressive inference, the designer generates algorithms with diverse lengths and structures, enabling to fully discover potentials over the metaheuristic family.","Second, prior design knowledge learned and accumulated in neurons of the designer can be retrieved for designing algorithms for future problems, paving the way to continual design of algorithms for open-ended problem-solving.","Extensive experiments on numeral benchmarks and real-world problems reveal that the proposed designer generates algorithms that outperform all human-created baselines on 24 out of 25 test problems.","The generated algorithms display various structures and behaviors, reasonably fitting for different problem-solving contexts.","Code will be released after paper publication."],"url":"http://arxiv.org/abs/2405.03419v1","category":"cs.NE"}
{"created":"2024-05-06 12:34:49","title":"The Decoherent Arrow of Time and the Entanglement Past Hypothesis","abstract":"If an asymmetry in time does not arise from the fundamental dynamical laws of physics, it may be found in special boundary conditions. The argument normally goes that since thermodynamic entropy in the past is lower than in the future according to the Second Law of Thermodynamics, then tracing this back to the time around the Big Bang means the universe must have started off in a state of very low thermodynamic entropy: the Thermodynamic Past Hypothesis. In this paper, we consider another boundary condition that plays a similar role, but for the decoherent arrow of time, i.e. the quantum state of the universe is more mixed in the future than in the past. According to what we call the Entanglement Past Hypothesis, the initial quantum state of the universe had very low entanglement entropy. We clarify the content of the Entanglement Past Hypothesis, compare it with the Thermodynamic Past Hypothesis, and identify some challenges and open questions for future research.","sentences":["If an asymmetry in time does not arise from the fundamental dynamical laws of physics, it may be found in special boundary conditions.","The argument normally goes that since thermodynamic entropy in the past is lower than in the future according to the Second Law of Thermodynamics, then tracing this back to the time around the Big Bang means the universe must have started off in a state of very low thermodynamic entropy: the Thermodynamic Past Hypothesis.","In this paper, we consider another boundary condition that plays a similar role, but for the decoherent arrow of time, i.e. the quantum state of the universe is more mixed in the future than in the past.","According to what we call the Entanglement Past Hypothesis, the initial quantum state of the universe had very low entanglement entropy.","We clarify the content of the Entanglement Past Hypothesis, compare it with the Thermodynamic Past Hypothesis, and identify some challenges and open questions for future research."],"url":"http://arxiv.org/abs/2405.03418v1","category":"quant-ph"}
{"created":"2024-05-06 12:32:38","title":"Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review","abstract":"Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.","sentences":["Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images.","Learning-based methods have gained attention for their ability to directly estimate 3D shapes.","This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views.","An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies.","Unresolved challenges and future directions are also discussed.","Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential.","Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting."],"url":"http://arxiv.org/abs/2405.03417v1","category":"cs.CV"}
{"created":"2024-05-06 12:20:55","title":"LightTR: A Lightweight Framework for Federated Trajectory Recovery","abstract":"With the proliferation of GPS-equipped edge devices, huge trajectory data is generated and accumulated in various domains, motivating a variety of urban applications. Due to the limited acquisition capabilities of edge devices, a lot of trajectories are recorded at a low sampling rate, which may lead to the effectiveness drop of urban applications. We aim to recover a high-sampled trajectory based on the low-sampled trajectory in free space, i.e., without road network information, to enhance the usability of trajectory data and support urban applications more effectively. Recent proposals targeting trajectory recovery often assume that trajectories are available at a central location, which fail to handle the decentralized trajectories and hurt privacy. To bridge the gap between decentralized training and trajectory recovery, we propose a lightweight framework, LightTR, for federated trajectory recovery based on a client-server architecture, while keeping the data decentralized and private in each client/platform center (e.g., each data center of a company). Specifically, considering the limited processing capabilities of edge devices, LightTR encompasses a light local trajectory embedding module that offers improved computational efficiency without compromising its feature extraction capabilities. LightTR also features a meta-knowledge enhanced local-global training scheme to reduce communication costs between the server and clients and thus further offer efficiency improvement. Extensive experiments demonstrate the effectiveness and efficiency of the proposed framework.","sentences":["With the proliferation of GPS-equipped edge devices, huge trajectory data is generated and accumulated in various domains, motivating a variety of urban applications.","Due to the limited acquisition capabilities of edge devices, a lot of trajectories are recorded at a low sampling rate, which may lead to the effectiveness drop of urban applications.","We aim to recover a high-sampled trajectory based on the low-sampled trajectory in free space, i.e., without road network information, to enhance the usability of trajectory data and support urban applications more effectively.","Recent proposals targeting trajectory recovery often assume that trajectories are available at a central location, which fail to handle the decentralized trajectories and hurt privacy.","To bridge the gap between decentralized training and trajectory recovery, we propose a lightweight framework, LightTR, for federated trajectory recovery based on a client-server architecture, while keeping the data decentralized and private in each client/platform center (e.g., each data center of a company).","Specifically, considering the limited processing capabilities of edge devices, LightTR encompasses a light local trajectory embedding module that offers improved computational efficiency without compromising its feature extraction capabilities.","LightTR also features a meta-knowledge enhanced local-global training scheme to reduce communication costs between the server and clients and thus further offer efficiency improvement.","Extensive experiments demonstrate the effectiveness and efficiency of the proposed framework."],"url":"http://arxiv.org/abs/2405.03409v1","category":"cs.LG"}
{"created":"2024-05-06 12:16:53","title":"Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain","abstract":"Failure mode and effects analysis (FMEA) is a systematic approach to identify and analyse potential failures and their effects in a system or process. The FMEA approach, however, requires domain experts to manually analyse the FMEA model to derive risk-reducing actions that should be applied. In this paper, we provide a formal framework to allow for automatic planning and acting in FMEA models. More specifically, we cast the FMEA model into a Markov decision process which can then be solved by existing solvers. We show that the FMEA approach can not only be used to support medical experts during the modelling process but also to automatically derive optimal therapies for the treatment of patients.","sentences":["Failure mode and effects analysis (FMEA) is a systematic approach to identify and analyse potential failures and their effects in a system or process.","The FMEA approach, however, requires domain experts to manually analyse the FMEA model to derive risk-reducing actions that should be applied.","In this paper, we provide a formal framework to allow for automatic planning and acting in FMEA models.","More specifically, we cast the FMEA model into a Markov decision process which can then be solved by existing solvers.","We show that the FMEA approach can not only be used to support medical experts during the modelling process but also to automatically derive optimal therapies for the treatment of patients."],"url":"http://arxiv.org/abs/2405.03406v1","category":"cs.AI"}
{"created":"2024-05-06 12:11:46","title":"E2GNN: Efficient Graph Neural Network Ensembles for Semi-Supervised Classification","abstract":"This work studies ensemble learning for graph neural networks (GNNs) under the popular semi-supervised setting. Ensemble learning has shown superiority in improving the accuracy and robustness of traditional machine learning by combining the outputs of multiple weak learners. However, adopting a similar idea to integrate different GNN models is challenging because of two reasons. First, GNN is notorious for its poor inference ability, so naively assembling multiple GNN models would deteriorate the inference efficiency. Second, when GNN models are trained with few labeled nodes, their performance are limited. In this case, the vanilla ensemble approach, e.g., majority vote, may be sub-optimal since most base models, i.e., GNNs, may make the wrong predictions. To this end, in this paper, we propose an efficient ensemble learner--E2GNN to assemble multiple GNNs in a learnable way by leveraging both labeled and unlabeled nodes. Specifically, we first pre-train different GNN models on a given data scenario according to the labeled nodes. Next, instead of directly combing their outputs for label inference, we train a simple multi-layer perceptron--MLP model to mimic their predictions on both labeled and unlabeled nodes. Then the unified MLP model is deployed to infer labels for unlabeled or new nodes. Since the predictions of unlabeled nodes from different GNN models may be incorrect, we develop a reinforced discriminator to effectively filter out those wrongly predicted nodes to boost the performance of MLP. By doing this, we suggest a principled approach to tackle the inference issues of GNN ensembles and maintain the merit of ensemble learning: improved performance. Comprehensive experiments over both transductive and inductive settings, across different GNN backbones and 8 benchmark datasets, demonstrate the superiority of E2GNN.","sentences":["This work studies ensemble learning for graph neural networks (GNNs) under the popular semi-supervised setting.","Ensemble learning has shown superiority in improving the accuracy and robustness of traditional machine learning by combining the outputs of multiple weak learners.","However, adopting a similar idea to integrate different GNN models is challenging because of two reasons.","First, GNN is notorious for its poor inference ability, so naively assembling multiple GNN models would deteriorate the inference efficiency.","Second, when GNN models are trained with few labeled nodes, their performance are limited.","In this case, the vanilla ensemble approach, e.g., majority vote, may be sub-optimal since most base models, i.e., GNNs, may make the wrong predictions.","To this end, in this paper, we propose an efficient ensemble learner--E2GNN to assemble multiple GNNs in a learnable way by leveraging both labeled and unlabeled nodes.","Specifically, we first pre-train different GNN models on a given data scenario according to the labeled nodes.","Next, instead of directly combing their outputs for label inference, we train a simple multi-layer perceptron--MLP model to mimic their predictions on both labeled and unlabeled nodes.","Then the unified MLP model is deployed to infer labels for unlabeled or new nodes.","Since the predictions of unlabeled nodes from different GNN models may be incorrect, we develop a reinforced discriminator to effectively filter out those wrongly predicted nodes to boost the performance of MLP.","By doing this, we suggest a principled approach to tackle the inference issues of GNN ensembles and maintain the merit of ensemble learning: improved performance.","Comprehensive experiments over both transductive and inductive settings, across different GNN backbones and 8 benchmark datasets, demonstrate the superiority of E2GNN."],"url":"http://arxiv.org/abs/2405.03401v1","category":"cs.LG"}
{"created":"2024-05-06 12:09:08","title":"Modulating trap properties by Cr3+-doping in Zn2SiO4: Mn2+ nano phosphor for optical information storage","abstract":"Photo stimulated luminescent materials are one of the most attractive alternatives for next generation optical information storage technologies. However, there are still some challenges in regulating appropriate energy levels in luminescent materials for optical information storage. Herein, a green emission nanophosphor Zn2SiO4: Cr3+, Mn2+ with the trap depth of 1.05 eV, fulfilling the requirements for optical information storage, was fabricated for the first time through the solution combustion method and subsequent heat treatment at 1000 degree centigrade for 2h. The crystal structure, micromorphology, photoluminescence (PL), photoluminescence excitation (PLE), and afterglow properties of Zn2SiO4: xCr3+, yMn2+ were studied systematically. By applying the strategy of trap depth engineering, high trap density with proper trap depth was observed when Cr3+ ions were introduced into Zn2SiO4: Mn2+. Thermoluminescence (TL) glow curve analysis through the initial rise (IR) method was conducted to gain some insight into the information of traps. As proof of application, information storage was experimentally achieved by choosing 275 nm illumination for information writing and 980 nm NIR excitation for information reading. The results indicate that Zn2SiO4: Cr3+, Mn2+ phosphor holds promise for potential applications in the field of optical information storage.","sentences":["Photo stimulated luminescent materials are one of the most attractive alternatives for next generation optical information storage technologies.","However, there are still some challenges in regulating appropriate energy levels in luminescent materials for optical information storage.","Herein, a green emission nanophosphor Zn2SiO4: Cr3+, Mn2+ with the trap depth of 1.05 eV, fulfilling the requirements for optical information storage, was fabricated for the first time through the solution combustion method and subsequent heat treatment at 1000 degree centigrade for 2h.","The crystal structure, micromorphology, photoluminescence (PL), photoluminescence excitation (PLE), and afterglow properties of Zn2SiO4: xCr3+, yMn2+ were studied systematically.","By applying the strategy of trap depth engineering, high trap density with proper trap depth was observed when Cr3+ ions were introduced into Zn2SiO4: Mn2+.","Thermoluminescence (TL) glow curve analysis through the initial rise (IR) method was conducted to gain some insight into the information of traps.","As proof of application, information storage was experimentally achieved by choosing 275 nm illumination for information writing and 980 nm NIR excitation for information reading.","The results indicate that Zn2SiO4: Cr3+, Mn2+ phosphor holds promise for potential applications in the field of optical information storage."],"url":"http://arxiv.org/abs/2405.03400v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 12:03:58","title":"A geometric realization for maximal almost pre-rigid representations over type $\\mathbb{D}$ quivers","abstract":"We focus on a class of special representations over a type $\\mathbb{D}$ quiver $Q_{D}$ with $n$ vertices and directional symmetry, namely, maximal almost pre-rigid representations. By using the equivariant theory of group actions, we give a geometric model for the category of finite dimensional representations over $Q_{D}$ via centrally-symmetric polygon $P(Q_{D})$ with a puncture, and show that the dimension of extension group between indecomposable representations can be interpreted as the crossing number on $P(Q_{D})$. Furthermore, we provide a geometric realization for maximal almost pre-rigid representations over $Q_{D}$. As an application, we illustrate their general form and prove that each maximal almost pre-rigid representation will determine two or four tilting objects over the path algebra $\\Bbbk Q_{\\overline{D}}$, where $Q_{\\overline{D}}$ is a quiver obtained by adding $n-2$ new vertices and $n-2$ arrows to the quiver $Q_{D}$.","sentences":["We focus on a class of special representations over a type $\\mathbb{D}$ quiver $Q_{D}$ with $n$ vertices and directional symmetry, namely, maximal almost pre-rigid representations.","By using the equivariant theory of group actions, we give a geometric model for the category of finite dimensional representations over $Q_{D}$ via centrally-symmetric polygon $P(Q_{D})$ with a puncture, and show that the dimension of extension group between indecomposable representations can be interpreted as the crossing number on $P(Q_{D})$.","Furthermore, we provide a geometric realization for maximal almost pre-rigid representations over $Q_{D}$. As an application, we illustrate their general form and prove that each maximal almost pre-rigid representation will determine two or four tilting objects over the path algebra $\\Bbbk Q_{\\overline{D}}$, where $Q_{\\overline{D}}$ is a quiver obtained by adding $n-2$ new vertices and $n-2$ arrows to the quiver $Q_{D}$."],"url":"http://arxiv.org/abs/2405.03395v1","category":"math.RT"}
{"created":"2024-05-06 11:51:09","title":"Don't Waste Your Time: Early Stopping Cross-Validation","abstract":"State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit. However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration. While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget. We aim to make model selection with cross-validation more effective. Therefore, we study early stopping the process of cross-validation during model selection. We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets. We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds. In addition, we investigate the impact of early stopping with Bayesian optimization instead of random search and also repeated cross-validation. Our exploratory study shows that even a simple-to-understand and easy-to-implement method consistently allows model selection to converge faster; in ~94% of all datasets, on average by ~214%. Moreover, stopping cross-validation enables model selection to explore the search space more exhaustively by considering +167% configurations on average within one hour, while also obtaining better overall performance.","sentences":["State-of-the-art automated machine learning systems for tabular data often employ cross-validation; ensuring that measured performances generalize to unseen data, or that subsequent ensembling does not overfit.","However, using k-fold cross-validation instead of holdout validation drastically increases the computational cost of validating a single configuration.","While ensuring better generalization and, by extension, better performance, the additional cost is often prohibitive for effective model selection within a time budget.","We aim to make model selection with cross-validation more effective.","Therefore, we study early stopping the process of cross-validation during model selection.","We investigate the impact of early stopping on random search for two algorithms, MLP and random forest, across 36 classification datasets.","We further analyze the impact of the number of folds by considering 3-, 5-, and 10-folds.","In addition, we investigate the impact of early stopping with Bayesian optimization instead of random search and also repeated cross-validation.","Our exploratory study shows that even a simple-to-understand and easy-to-implement method consistently allows model selection to converge faster; in ~94% of all datasets, on average by ~214%.","Moreover, stopping cross-validation enables model selection to explore the search space more exhaustively by considering +167% configurations on average within one hour, while also obtaining better overall performance."],"url":"http://arxiv.org/abs/2405.03389v1","category":"cs.LG"}
{"created":"2024-05-06 11:45:59","title":"The high dimensional psychological profile and cultural bias of ChatGPT","abstract":"Given the rapid advancement of large-scale language models, artificial intelligence (AI) models, like ChatGPT, are playing an increasingly prominent role in human society. However, to ensure that artificial intelligence models benefit human society, we must first fully understand the similarities and differences between the human-like characteristics exhibited by artificial intelligence models and real humans, as well as the cultural stereotypes and biases that artificial intelligence models may exhibit in the process of interacting with humans. This study first measured ChatGPT in 84 dimensions of psychological characteristics, revealing differences between ChatGPT and human norms in most dimensions as well as in high-dimensional psychological representations. Additionally, through the measurement of ChatGPT in 13 dimensions of cultural values, it was revealed that ChatGPT's cultural value patterns are dissimilar to those of various countries/regions worldwide. Finally, an analysis of ChatGPT's performance in eight decision-making tasks involving interactions with humans from different countries/regions revealed that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks and shows significant cultural bias in third-party punishment and ultimatum games. The findings indicate that, compared to humans, ChatGPT exhibits a distinct psychological profile and cultural value orientation, and it also shows cultural biases and stereotypes in interpersonal decision-making. Future research endeavors should emphasize enhanced technical oversight and augmented transparency in the database and algorithmic training procedures to foster more efficient cross-cultural communication and mitigate social disparities.","sentences":["Given the rapid advancement of large-scale language models, artificial intelligence (AI) models, like ChatGPT, are playing an increasingly prominent role in human society.","However, to ensure that artificial intelligence models benefit human society, we must first fully understand the similarities and differences between the human-like characteristics exhibited by artificial intelligence models and real humans, as well as the cultural stereotypes and biases that artificial intelligence models may exhibit in the process of interacting with humans.","This study first measured ChatGPT in 84 dimensions of psychological characteristics, revealing differences between ChatGPT and human norms in most dimensions as well as in high-dimensional psychological representations.","Additionally, through the measurement of ChatGPT in 13 dimensions of cultural values, it was revealed that ChatGPT's cultural value patterns are dissimilar to those of various countries/regions worldwide.","Finally, an analysis of ChatGPT's performance in eight decision-making tasks involving interactions with humans from different countries/regions revealed that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks and shows significant cultural bias in third-party punishment and ultimatum games.","The findings indicate that, compared to humans, ChatGPT exhibits a distinct psychological profile and cultural value orientation, and it also shows cultural biases and stereotypes in interpersonal decision-making.","Future research endeavors should emphasize enhanced technical oversight and augmented transparency in the database and algorithmic training procedures to foster more efficient cross-cultural communication and mitigate social disparities."],"url":"http://arxiv.org/abs/2405.03387v1","category":"cs.CL"}
{"created":"2024-05-06 11:44:54","title":"Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via a Mixup Extension","abstract":"Training with noisy class labels impairs neural networks' generalization performance. In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult. However, mixup neglects that, typically, multiple annotators, e.g., crowdworkers, provide class labels. Therefore, we propose an extension of mixup, which handles multiple class labels per instance while considering which class label originates from which annotator. Integrated into our multi-annotator classification framework annot-mix, it performs superiorly to eight state-of-the-art approaches on eleven datasets with noisy class labels provided either by human or simulated annotators. Our code is publicly available through our repository at https://github.com/ies-research/annot-mix.","sentences":["Training with noisy class labels impairs neural networks' generalization performance.","In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult.","However, mixup neglects that, typically, multiple annotators, e.g., crowdworkers, provide class labels.","Therefore, we propose an extension of mixup, which handles multiple class labels per instance while considering which class label originates from which annotator.","Integrated into our multi-annotator classification framework annot-mix, it performs superiorly to eight state-of-the-art approaches on eleven datasets with noisy class labels provided either by human or simulated annotators.","Our code is publicly available through our repository at https://github.com/ies-research/annot-mix."],"url":"http://arxiv.org/abs/2405.03386v1","category":"cs.LG"}
{"created":"2024-05-06 11:43:49","title":"Fully Reversing the Shoebox Image Source Method: From Impulse Responses to Room Parameters","abstract":"We present an algorithm that fully reverses the shoebox image source method (ISM), a popular and widely used room impulse response (RIR) simulator for cuboid rooms introduced by Allen and Berkley in 1979. More precisely, given a discrete multichannel RIR generated by the shoebox ISM for a microphone array of known geometry, the algorithm reliably recovers the 18 input parameters. These are the 3D source position, the 3 dimensions of the room, the 6-degrees-of-freedom room translation and orientation, and an absorption coefficient for each of the 6 room boundaries. The approach builds on a recently proposed gridless image source localization technique combined with new procedures for room axes recovery and first-order-reflection identification. Extensive simulated experiments reveal that near-exact recovery of all parameters is achieved for a 32-element, 8.4-cm-wide spherical microphone array and a sampling rate of 16~kHz using fully randomized input parameters within rooms of size 2X2X2 to 10X10X5 meters. Estimation errors decay towards zero when increasing the array size and sampling rate. The method is also shown to strongly outperform a known baseline, and its ability to extrapolate RIRs at new positions is demonstrated. Crucially, the approach is strictly limited to low-passed discrete RIRs simulated using the vanilla shoebox ISM. Nonetheless, it represents to our knowledge the first algorithmic demonstration that this difficult inverse problem is in-principle fully solvable over a wide range of configurations.","sentences":["We present an algorithm that fully reverses the shoebox image source method (ISM), a popular and widely used room impulse response (RIR) simulator for cuboid rooms introduced by Allen and Berkley in 1979.","More precisely, given a discrete multichannel RIR generated by the shoebox ISM for a microphone array of known geometry, the algorithm reliably recovers the 18 input parameters.","These are the 3D source position, the 3 dimensions of the room, the 6-degrees-of-freedom room translation and orientation, and an absorption coefficient for each of the 6 room boundaries.","The approach builds on a recently proposed gridless image source localization technique combined with new procedures for room axes recovery and first-order-reflection identification.","Extensive simulated experiments reveal that near-exact recovery of all parameters is achieved for a 32-element, 8.4-cm-wide spherical microphone array and a sampling rate of 16~kHz using fully randomized input parameters within rooms of size 2X2X2 to 10X10X5 meters.","Estimation errors decay towards zero when increasing the array size and sampling rate.","The method is also shown to strongly outperform a known baseline, and its ability to extrapolate RIRs at new positions is demonstrated.","Crucially, the approach is strictly limited to low-passed discrete RIRs simulated using the vanilla shoebox ISM.","Nonetheless, it represents to our knowledge the first algorithmic demonstration that this difficult inverse problem is in-principle fully solvable over a wide range of configurations."],"url":"http://arxiv.org/abs/2405.03385v1","category":"cs.SD"}
{"created":"2024-05-06 11:43:01","title":"GLIP: Electromagnetic Field Exposure Map Completion by Deep Generative Networks","abstract":"In Spectrum cartography (SC), the generation of exposure maps for radio frequency electromagnetic fields (RF-EMF) spans dimensions of frequency, space, and time, which relies on a sparse collection of sensor data, posing a challenging ill-posed inverse problem. Cartography methods based on models integrate designed priors, such as sparsity and low-rank structures, to refine the solution of this inverse problem. In our previous work, EMF exposure map reconstruction was achieved by Generative Adversarial Networks (GANs) where physical laws or structural constraints were employed as a prior, but they require a large amount of labeled data or simulated full maps for training to produce efficient results. In this paper, we present a method to reconstruct EMF exposure maps using only the generator network in GANs which does not require explicit training, thus overcoming the limitations of GANs, such as using reference full exposure maps. This approach uses a prior from sensor data as Local Image Prior (LIP) captured by deep convolutional generative networks independent of learning the network parameters from images in an urban environment. Experimental results show that, even when only sparse sensor data are available, our method can produce accurate estimates.","sentences":["In Spectrum cartography (SC), the generation of exposure maps for radio frequency electromagnetic fields (RF-EMF) spans dimensions of frequency, space, and time, which relies on a sparse collection of sensor data, posing a challenging ill-posed inverse problem.","Cartography methods based on models integrate designed priors, such as sparsity and low-rank structures, to refine the solution of this inverse problem.","In our previous work, EMF exposure map reconstruction was achieved by Generative Adversarial Networks (GANs) where physical laws or structural constraints were employed as a prior, but they require a large amount of labeled data or simulated full maps for training to produce efficient results.","In this paper, we present a method to reconstruct EMF exposure maps using only the generator network in GANs which does not require explicit training, thus overcoming the limitations of GANs, such as using reference full exposure maps.","This approach uses a prior from sensor data as Local Image Prior (LIP) captured by deep convolutional generative networks independent of learning the network parameters from images in an urban environment.","Experimental results show that, even when only sparse sensor data are available, our method can produce accurate estimates."],"url":"http://arxiv.org/abs/2405.03384v1","category":"cs.LG"}
{"created":"2024-05-06 11:41:54","title":"On Wave-Like Differential Equations in General Hilbert Space. The Functional Analytic Investigation of Euler-Bernoulli Bending Vibrations of a Beam as an Application in Engineering Science","abstract":"Wave-like partial differential equations occur in many engineering applications. Here the engineering setup is embedded into the Hilbert space framework of functional analysis of modern mathematical physics. The notion wave-like is a generalization of the primary wave (partial) differential equation.   A short overview over three wave-like problems in physics and engineering is presented. The mathematical procedure for achieving positive, selfadjoint differential operators in an $\\mathrm{L}^2$-Hilbert space is described, operators which then may be taken for wave-like differential equations. Also some general results from the functional analytic literature are summarized.   The main part concerns the investigation of the free Euler--Bernoulli bending vibrations of a slender, straight, elastic beam in one spatial dimension in the $\\mathrm{L}^2$-Hilbert space setup. Taking suitable Sobolev spaces we perform the mathematically exact introduction and analysis of the corresponding (spatial) positive, selfadjoint differential operators of $4$-th order, which belong to the different boundary conditions arising as supports in statics. A comparison with free wave swinging of a string is added, using a Laplacian as differential operator.","sentences":["Wave-like partial differential equations occur in many engineering applications.","Here the engineering setup is embedded into the Hilbert space framework of functional analysis of modern mathematical physics.","The notion wave-like is a generalization of the primary wave (partial) differential equation.   ","A short overview over three wave-like problems in physics and engineering is presented.","The mathematical procedure for achieving positive, selfadjoint differential operators in an $\\mathrm{L}^2$-Hilbert space is described, operators which then may be taken for wave-like differential equations.","Also some general results from the functional analytic literature are summarized.   ","The main part concerns the investigation of the free Euler--Bernoulli bending vibrations of a slender, straight, elastic beam in one spatial dimension in the $\\mathrm{L}^2$-Hilbert space setup.","Taking suitable Sobolev spaces we perform the mathematically exact introduction and analysis of the corresponding (spatial) positive, selfadjoint differential operators of $4$-th order, which belong to the different boundary conditions arising as supports in statics.","A comparison with free wave swinging of a string is added, using a Laplacian as differential operator."],"url":"http://arxiv.org/abs/2405.03383v1","category":"math-ph"}
{"created":"2024-05-06 11:33:12","title":"Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in Reinforcement Learning","abstract":"Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics. Our approach consists of a reverse curriculum followed by a forward curriculum. Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets. The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems. A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency. We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control.","sentences":["Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards.","One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics.","Our approach consists of a reverse curriculum followed by a forward curriculum.","Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets.","The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems.","A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency.","We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control."],"url":"http://arxiv.org/abs/2405.03379v1","category":"cs.LG"}
{"created":"2024-05-06 11:25:59","title":"Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G","abstract":"In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization. Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes. These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes. To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework. Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes. This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions.","sentences":["In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization.","Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes.","These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes.","To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework.","Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes.","This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions."],"url":"http://arxiv.org/abs/2405.03372v1","category":"cs.NI"}
{"created":"2024-05-06 11:24:13","title":"Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom","abstract":"Most fake news detection methods learn latent feature representations based on neural networks, which makes them black boxes to classify a piece of news without giving any justification. Existing explainable systems generate veracity justifications from investigative journalism, which suffer from debunking delayed and low efficiency. Recent studies simply assume that the justification is equivalent to the majority opinions expressed in the wisdom of crowds. However, the opinions typically contain some inaccurate or biased information since the wisdom of crowds is uncensored. To detect fake news from a sea of diverse, crowded and even competing narratives, in this paper, we propose a novel defense-based explainable fake news detection framework. Specifically, we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences. To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities. Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications. Extensive experiments conducted on two real-world benchmarks demonstrate that our proposed method outperforms state-of-the-art baselines in terms of fake news detection and provides high-quality justifications.","sentences":["Most fake news detection methods learn latent feature representations based on neural networks, which makes them black boxes to classify a piece of news without giving any justification.","Existing explainable systems generate veracity justifications from investigative journalism, which suffer from debunking delayed and low efficiency.","Recent studies simply assume that the justification is equivalent to the majority opinions expressed in the wisdom of crowds.","However, the opinions typically contain some inaccurate or biased information since the wisdom of crowds is uncensored.","To detect fake news from a sea of diverse, crowded and even competing narratives, in this paper, we propose a novel defense-based explainable fake news detection framework.","Specifically, we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences.","To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities.","Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications.","Extensive experiments conducted on two real-world benchmarks demonstrate that our proposed method outperforms state-of-the-art baselines in terms of fake news detection and provides high-quality justifications."],"url":"http://arxiv.org/abs/2405.03371v1","category":"cs.CL"}
{"created":"2024-05-06 11:23:47","title":"AntiFold: Improved antibody structure-based design using inverse folding","abstract":"The design and optimization of antibodies requires an intricate balance across multiple properties. Protein inverse folding models, capable of generating diverse sequences folding into the same structure, are promising tools for maintaining structural integrity during antibody design. Here, we present AntiFold, an antibody-specific inverse folding model, fine-tuned from ESM-IF1 on solved and predicted antibody structures. AntiFold outperforms existing inverse folding tools on sequence recovery across complementarity-determining regions, with designed sequences showing high structural similarity to their solved counterpart. It additionally achieves stronger correlations when predicting antibody-antigen binding affinity in a zero-shot manner, while performance is augmented further when including antigen information. AntiFold assigns low probabilities to mutations that disrupt antigen binding, synergizing with protein language model residue probabilities, and demonstrates promise for guiding antibody optimization while retaining structure-related properties. AntiFold is freely available under the BSD 3-Clause as a web server at https://opig.stats.ox.ac.uk/webapps/antifold/ and and pip installable package at https://github.com/oxpig/AntiFold","sentences":["The design and optimization of antibodies requires an intricate balance across multiple properties.","Protein inverse folding models, capable of generating diverse sequences folding into the same structure, are promising tools for maintaining structural integrity during antibody design.","Here, we present AntiFold, an antibody-specific inverse folding model, fine-tuned from ESM-IF1 on solved and predicted antibody structures.","AntiFold outperforms existing inverse folding tools on sequence recovery across complementarity-determining regions, with designed sequences showing high structural similarity to their solved counterpart.","It additionally achieves stronger correlations when predicting antibody-antigen binding affinity in a zero-shot manner, while performance is augmented further when including antigen information.","AntiFold assigns low probabilities to mutations that disrupt antigen binding, synergizing with protein language model residue probabilities, and demonstrates promise for guiding antibody optimization while retaining structure-related properties.","AntiFold is freely available under the BSD 3-Clause as a web server at https://opig.stats.ox.ac.uk/webapps/antifold/ and and pip installable package at https://github.com/oxpig/AntiFold"],"url":"http://arxiv.org/abs/2405.03370v1","category":"q-bio.BM"}
{"created":"2024-05-06 11:23:38","title":"Speckle pattern analysis of PVK:rGO composite based memristor device","abstract":"The memristors are expected to be fundamental devices for neuromorphic systems and switching applications. For example, the device made of a sandwiched layer of poly(N-vinylcarbazole) and reduced graphene composite between asymmetric electrodes (ITO/PVK:rGO/Al) exhibits bistable resistive switching behavior. Depending on the resistance state of the (ON-state or OFF-state) at a constant applied voltage, it may show two different resistivities. The performance of the memristor can be optimized by controlling the doping amount of graphene oxide in the PVK polymer. To assess the performance of the device, when it switches between ON and OFF states, optical characterization approaches are highly promising due to their non-destructive and remote nature. Here, we characterize the memristor device by the use of speckle pattern (SP) analysis. The speckle pattern is the interference of multiple light waves with random relative phases, which is generated via different mechanisms such as scattering from diffusive materials. Therefore, SPs can be used to investigate such samples as they include a huge amount of information to be statistically elaborated. The experimental paradigm includes \\textit{in situ} acquisition of SPs of the PVK:rGO in different states followed by statistical post-processing toward examining its conduction mechanism. The variations in these statistical parameters are attributed to the resistance state of the PVK:rGO samples under the applied voltage with regard to the physical switching mechanism of the device. The resistance/conduction state, in turn, depends on the activity and properties of PVK:rGO memristors as well as the additional non-uniformities induced through the variations of density of carriers. The present optical methodology can be potentially served as a bench-top device for characterization purposes of similar devices while they are operating.","sentences":["The memristors are expected to be fundamental devices for neuromorphic systems and switching applications.","For example, the device made of a sandwiched layer of poly(N-vinylcarbazole) and reduced graphene composite between asymmetric electrodes (ITO/PVK:rGO/Al) exhibits bistable resistive switching behavior.","Depending on the resistance state of the (ON-state or OFF-state) at a constant applied voltage, it may show two different resistivities.","The performance of the memristor can be optimized by controlling the doping amount of graphene oxide in the PVK polymer.","To assess the performance of the device, when it switches between ON and OFF states, optical characterization approaches are highly promising due to their non-destructive and remote nature.","Here, we characterize the memristor device by the use of speckle pattern (SP) analysis.","The speckle pattern is the interference of multiple light waves with random relative phases, which is generated via different mechanisms such as scattering from diffusive materials.","Therefore, SPs can be used to investigate such samples as they include a huge amount of information to be statistically elaborated.","The experimental paradigm includes \\textit{in situ} acquisition of SPs of the PVK:rGO in different states followed by statistical post-processing toward examining its conduction mechanism.","The variations in these statistical parameters are attributed to the resistance state of the PVK:rGO samples under the applied voltage with regard to the physical switching mechanism of the device.","The resistance/conduction state, in turn, depends on the activity and properties of PVK:rGO memristors as well as the additional non-uniformities induced through the variations of density of carriers.","The present optical methodology can be potentially served as a bench-top device for characterization purposes of similar devices while they are operating."],"url":"http://arxiv.org/abs/2405.03369v1","category":"physics.optics"}
{"created":"2024-05-06 11:23:31","title":"Updating neutrino mass constraints with Background measurements","abstract":"Low-redshift probes, such as Baryon Acoustic Oscillations (BAO) and Supernovae Ia luminosity distances, have been shown to be crucial for improving the bounds on the total neutrino mass from cosmological observations, due to their ability to break degeneracies among the different parameters. Here, we expand background observations to include $H(z)$ measurements from cosmic chronometers, distance moduli from Gamma Ray Bursts (GRBs), and angular diameter distances from galaxy clusters. For the very first time, we find neutrino mass limits below the minimal expectations from neutrino oscillation probes, suggesting non-standard neutrino and/or cosmological scenarios. The tightening of the neutrino mass bound is due to the slightly higher value of the Hubble constant $H_0$ preferred by the former three background probes, and also due to the improved errors on $H_0$ and the matter mass-energy density $\\Omega_{\\rm m}$. All values of $H_0$ are however in agreement at the $1-2\\sigma$ level. Interestingly, it is not only the combination of the three background probes that is responsible for the $\\sum m_\\nu <0.06$~eV limits, but also each of them independently. The tightest bound we find here is $\\sum m_\\nu<0.043$~eV at $2\\sigma$ after combining Cosmic Microwave Background Planck data with DESI BAO, Supernovae Ia, GRBs, cosmic chronometers, and galaxy clusters, showing a clear tension between neutrino oscillation results and cosmological analyses. In general, removing either one of the two background probes still provides a limit $\\sum m_\\nu \\lesssim 0.06$~eV, reassuring the enormous potential of these low-redshift observations in constraining the neutrino mass.","sentences":["Low-redshift probes, such as Baryon Acoustic Oscillations (BAO) and Supernovae Ia luminosity distances, have been shown to be crucial for improving the bounds on the total neutrino mass from cosmological observations, due to their ability to break degeneracies among the different parameters.","Here, we expand background observations to include $H(z)$ measurements from cosmic chronometers, distance moduli from Gamma Ray Bursts (GRBs), and angular diameter distances from galaxy clusters.","For the very first time, we find neutrino mass limits below the minimal expectations from neutrino oscillation probes, suggesting non-standard neutrino and/or cosmological scenarios.","The tightening of the neutrino mass bound is due to the slightly higher value of the Hubble constant $H_0$ preferred by the former three background probes, and also due to the improved errors on $H_0$ and the matter mass-energy density $\\Omega_{\\rm m}$. All values of $H_0$ are however in agreement at the $1-2\\sigma$ level.","Interestingly, it is not only the combination of the three background probes that is responsible for the $\\sum m_\\nu <0.06$~eV limits, but also each of them independently.","The tightest bound we find here is $\\sum m_\\nu<0.043$~eV at $2\\sigma$ after combining Cosmic Microwave Background Planck data with DESI BAO, Supernovae Ia, GRBs, cosmic chronometers, and galaxy clusters, showing a clear tension between neutrino oscillation results and cosmological analyses.","In general, removing either one of the two background probes still provides a limit $\\sum m_\\nu \\lesssim 0.06$~eV, reassuring the enormous potential of these low-redshift observations in constraining the neutrino mass."],"url":"http://arxiv.org/abs/2405.03368v1","category":"astro-ph.CO"}
{"created":"2024-05-06 11:12:28","title":"Secure Semantic Communication over Wiretap Channel","abstract":"Semantic communication, an emerging feature for future networks like 6G, emphasizes message meaning. Yet, the open nature of a wireless channel poses security risks for semantic communications. In this paper we derive information-theoretic limits, considering the semantic source model within a wiretap channel framework. Under separate equivocation and distortion conditions for semantics and observed data, we present the general outer and inner bounds of the region. We also reduce the general region to a case of Gaussian source and channel and provide numerical evaluation.","sentences":["Semantic communication, an emerging feature for future networks like 6G, emphasizes message meaning.","Yet, the open nature of a wireless channel poses security risks for semantic communications.","In this paper we derive information-theoretic limits, considering the semantic source model within a wiretap channel framework.","Under separate equivocation and distortion conditions for semantics and observed data, we present the general outer and inner bounds of the region.","We also reduce the general region to a case of Gaussian source and channel and provide numerical evaluation."],"url":"http://arxiv.org/abs/2405.03361v1","category":"cs.IT"}
{"created":"2024-05-06 11:12:19","title":"Embedded Distributed Inference of Deep Neural Networks: A Systematic Review","abstract":"Embedded distributed inference of Neural Networks has emerged as a promising approach for deploying machine-learning models on resource-constrained devices in an efficient and scalable manner. The inference task is distributed across a network of embedded devices, with each device contributing to the overall computation by performing a portion of the workload. In some cases, more powerful devices such as edge or cloud servers can be part of the system to be responsible of the most demanding layers of the network. As the demand for intelligent systems and the complexity of the deployed neural network models increases, this approach is becoming more relevant in a variety of applications such as robotics, autonomous vehicles, smart cities, Industry 4.0 and smart health. We present a systematic review of papers published during the last six years which describe techniques and methods to distribute Neural Networks across these kind of systems. We provide an overview of the current state-of-the-art by analysing more than 100 papers, present a new taxonomy to characterize them, and discuss trends and challenges in the field.","sentences":["Embedded distributed inference of Neural Networks has emerged as a promising approach for deploying machine-learning models on resource-constrained devices in an efficient and scalable manner.","The inference task is distributed across a network of embedded devices, with each device contributing to the overall computation by performing a portion of the workload.","In some cases, more powerful devices such as edge or cloud servers can be part of the system to be responsible of the most demanding layers of the network.","As the demand for intelligent systems and the complexity of the deployed neural network models increases, this approach is becoming more relevant in a variety of applications such as robotics, autonomous vehicles, smart cities, Industry 4.0 and smart health.","We present a systematic review of papers published during the last six years which describe techniques and methods to distribute Neural Networks across these kind of systems.","We provide an overview of the current state-of-the-art by analysing more than 100 papers, present a new taxonomy to characterize them, and discuss trends and challenges in the field."],"url":"http://arxiv.org/abs/2405.03360v1","category":"cs.DC"}
{"created":"2024-05-06 11:11:23","title":"MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline","abstract":"This research focuses on evaluating the non-commercial open-source large language models (LLMs) Meditron, MedAlpaca, Mistral, and Llama-2 for their efficacy in interpreting medical guidelines saved in PDF format. As a specific test scenario, we applied these models to the guidelines for hypertension in children and adolescents provided by the European Society of Cardiology (ESC). Leveraging Streamlit, a Python library, we developed a user-friendly medical document chatbot tool (MedDoc-Bot). This tool enables authorized users to upload PDF files and pose questions, generating interpretive responses from four locally stored LLMs. A pediatric expert provides a benchmark for evaluation by formulating questions and responses extracted from the ESC guidelines. The expert rates the model-generated responses based on their fidelity and relevance. Additionally, we evaluated the METEOR and chrF metric scores to assess the similarity of model responses to reference answers. Our study found that Llama-2 and Mistral performed well in metrics evaluation. However, Llama-2 was slower when dealing with text and tabular data. In our human evaluation, we observed that responses created by Mistral, Meditron, and Llama-2 exhibited reasonable fidelity and relevance. This study provides valuable insights into the strengths and limitations of LLMs for future developments in medical document interpretation. Open-Source Code: https://github.com/yaseen28/MedDoc-Bot","sentences":["This research focuses on evaluating the non-commercial open-source large language models (LLMs) Meditron, MedAlpaca, Mistral, and Llama-2 for their efficacy in interpreting medical guidelines saved in PDF format.","As a specific test scenario, we applied these models to the guidelines for hypertension in children and adolescents provided by the European Society of Cardiology (ESC).","Leveraging Streamlit, a Python library, we developed a user-friendly medical document chatbot tool (MedDoc-Bot).","This tool enables authorized users to upload PDF files and pose questions, generating interpretive responses from four locally stored LLMs.","A pediatric expert provides a benchmark for evaluation by formulating questions and responses extracted from the ESC guidelines.","The expert rates the model-generated responses based on their fidelity and relevance.","Additionally, we evaluated the METEOR and chrF metric scores to assess the similarity of model responses to reference answers.","Our study found that Llama-2 and Mistral performed well in metrics evaluation.","However, Llama-2 was slower when dealing with text and tabular data.","In our human evaluation, we observed that responses created by Mistral, Meditron, and Llama-2 exhibited reasonable fidelity and relevance.","This study provides valuable insights into the strengths and limitations of LLMs for future developments in medical document interpretation.","Open-Source Code: https://github.com/yaseen28/MedDoc-Bot"],"url":"http://arxiv.org/abs/2405.03359v1","category":"cs.CL"}
{"created":"2024-05-06 11:07:51","title":"Pinching Tactile Display: A Cloth that Changes Tactile Sensation by Electrostatic Adsorption","abstract":"Haptic displays play an important role in enhancing the sense of presence in VR and telepresence. Displaying the tactile properties of fabrics has potential in the fashion industry, but there are difficulties in dynamically displaying different types of tactile sensations while maintaining their flexible properties. The vibrotactile stimulation of fabrics is an important element in the tactile properties of fabrics, as it greatly affects the way a garment feels when rubbed against the skin. To dynamically change the vibrotactile stimuli, many studies have used mechanical actuators. However, when combined with fabric, the soft properties of the fabric are compromised by the stiffness of the actuator. In addition, because the vibration generated by such actuators is applied to a single point, it is not possible to provide a uniform tactile sensation over the entire surface of the fabric, resulting in an uneven tactile sensation. In this study, we propose a Pinching Tactile Display: a conductive cloth that changes the tactile sensation by controlling electrostatic adsorption. By controlling the voltage and frequency applied to the conductive cloth, different tactile sensations can be dynamically generated. This makes it possible to create a tactile device in which tactile sensations are applied to the entire fabric while maintaining the thin and soft characteristics of the fabric. As a result, users could experiment with tactile sensations by picking up and rubbing the fabric in the same way they normally touch it. This mechanism has the potential for dynamic tactile transformation of soft materials.","sentences":["Haptic displays play an important role in enhancing the sense of presence in VR and telepresence.","Displaying the tactile properties of fabrics has potential in the fashion industry, but there are difficulties in dynamically displaying different types of tactile sensations while maintaining their flexible properties.","The vibrotactile stimulation of fabrics is an important element in the tactile properties of fabrics, as it greatly affects the way a garment feels when rubbed against the skin.","To dynamically change the vibrotactile stimuli, many studies have used mechanical actuators.","However, when combined with fabric, the soft properties of the fabric are compromised by the stiffness of the actuator.","In addition, because the vibration generated by such actuators is applied to a single point, it is not possible to provide a uniform tactile sensation over the entire surface of the fabric, resulting in an uneven tactile sensation.","In this study, we propose a Pinching Tactile Display: a conductive cloth that changes the tactile sensation by controlling electrostatic adsorption.","By controlling the voltage and frequency applied to the conductive cloth, different tactile sensations can be dynamically generated.","This makes it possible to create a tactile device in which tactile sensations are applied to the entire fabric while maintaining the thin and soft characteristics of the fabric.","As a result, users could experiment with tactile sensations by picking up and rubbing the fabric in the same way they normally touch it.","This mechanism has the potential for dynamic tactile transformation of soft materials."],"url":"http://arxiv.org/abs/2405.03358v1","category":"cs.HC"}
{"created":"2024-05-06 11:05:45","title":"An Overview of Intelligent Meta-surfaces for 6G and Beyond: Opportunities, Trends, and Challenges","abstract":"With the impending arrival of the sixth generation (6G) of wireless communication technology, the telecommunications landscape is poised for another revolutionary transformation. At the forefront of this evolution are intelligent meta-surfaces (IS), emerging as a disruptive physical layer technology with the potential to redefine the capabilities and performance metrics of future wireless networks. As 6G evolves from concept to reality, industry stakeholders, standards organizations, and regulatory bodies are collaborating to define the specifications, protocols, and interoperability standards governing IS deployment. Against this background, this article delves into the ongoing standardization efforts, emerging trends, potential opportunities, and prevailing challenges surrounding the integration of IS into the framework of 6G and beyond networks. Specifically, it provides a tutorial-style overview of recent advancements in IS and explores their potential applications within future networks beyond 6G. Additionally, the article identifies key challenges in the design and implementation of various types of intelligent surfaces, along with considerations for their practical standardization. Finally, it highlights potential future prospects in this evolving field.","sentences":["With the impending arrival of the sixth generation (6G) of wireless communication technology, the telecommunications landscape is poised for another revolutionary transformation.","At the forefront of this evolution are intelligent meta-surfaces (IS), emerging as a disruptive physical layer technology with the potential to redefine the capabilities and performance metrics of future wireless networks.","As 6G evolves from concept to reality, industry stakeholders, standards organizations, and regulatory bodies are collaborating to define the specifications, protocols, and interoperability standards governing IS deployment.","Against this background, this article delves into the ongoing standardization efforts, emerging trends, potential opportunities, and prevailing challenges surrounding the integration of IS into the framework of 6G and beyond networks.","Specifically, it provides a tutorial-style overview of recent advancements in IS and explores their potential applications within future networks beyond 6G. Additionally, the article identifies key challenges in the design and implementation of various types of intelligent surfaces, along with considerations for their practical standardization.","Finally, it highlights potential future prospects in this evolving field."],"url":"http://arxiv.org/abs/2405.03356v1","category":"cs.NI"}
{"created":"2024-05-06 11:05:13","title":"On the Theory of Cross-Modality Distillation with Contrastive Learning","abstract":"Cross-modality distillation arises as an important topic for data modalities containing limited knowledge such as depth maps and high-quality sketches. Such techniques are of great importance, especially for memory and privacy-restricted scenarios where labeled training data is generally unavailable. To solve the problem, existing label-free methods leverage a few pairwise unlabeled data to distill the knowledge by aligning features or statistics between the source and target modalities. For instance, one typically aims to minimize the L2 distance or contrastive loss between the learned features of pairs of samples in the source (e.g. image) and the target (e.g. sketch) modalities. However, most algorithms in this domain only focus on the experimental results but lack theoretical insight. To bridge the gap between the theory and practical method of cross-modality distillation, we first formulate a general framework of cross-modality contrastive distillation (CMCD), built upon contrastive learning that leverages both positive and negative correspondence, towards a better distillation of generalizable features. Furthermore, we establish a thorough convergence analysis that reveals that the distance between source and target modalities significantly impacts the test error on downstream tasks within the target modality which is also validated by the empirical results. Extensive experimental results show that our algorithm outperforms existing algorithms consistently by a margin of 2-3\\% across diverse modalities and tasks, covering modalities of image, sketch, depth map, and audio and tasks of recognition and segmentation.","sentences":["Cross-modality distillation arises as an important topic for data modalities containing limited knowledge such as depth maps and high-quality sketches.","Such techniques are of great importance, especially for memory and privacy-restricted scenarios where labeled training data is generally unavailable.","To solve the problem, existing label-free methods leverage a few pairwise unlabeled data to distill the knowledge by aligning features or statistics between the source and target modalities.","For instance, one typically aims to minimize the L2 distance or contrastive loss between the learned features of pairs of samples in the source (e.g. image) and the target (e.g. sketch) modalities.","However, most algorithms in this domain only focus on the experimental results but lack theoretical insight.","To bridge the gap between the theory and practical method of cross-modality distillation, we first formulate a general framework of cross-modality contrastive distillation (CMCD), built upon contrastive learning that leverages both positive and negative correspondence, towards a better distillation of generalizable features.","Furthermore, we establish a thorough convergence analysis that reveals that the distance between source and target modalities significantly impacts the test error on downstream tasks within the target modality which is also validated by the empirical results.","Extensive experimental results show that our algorithm outperforms existing algorithms consistently by a margin of 2-3\\% across diverse modalities and tasks, covering modalities of image, sketch, depth map, and audio and tasks of recognition and segmentation."],"url":"http://arxiv.org/abs/2405.03355v1","category":"cs.LG"}
{"created":"2024-05-06 11:04:32","title":"VACO: a Multi-perspective Development of a Therapeutic and Motivational Virtual Robotic Agent for Concentration for children with ADHD","abstract":"In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives. Therefore, we present three participative approaches to include the perspectives of different stakeholders. An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet. About half of the parents would be willing to use software to promote attention. To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible. Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments. A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it. Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user. This development process requires a lot of time and close interdisciplinary collaboration.","sentences":["In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives.","Therefore, we present three participative approaches to include the perspectives of different stakeholders.","An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet.","About half of the parents would be willing to use software to promote attention.","To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible.","Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments.","A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it.","Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user.","This development process requires a lot of time and close interdisciplinary collaboration."],"url":"http://arxiv.org/abs/2405.03354v1","category":"cs.HC"}
{"created":"2024-05-06 11:02:26","title":"Salient Object Detection From Arbitrary Modalities","abstract":"Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, \\i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection.","sentences":["Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications.","However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs.","Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs.","Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD).","The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed.","The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them.","While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images.","Accordingly, a preliminary solution to the above challenges, \\i.e.","a modality switch network (MSN), is proposed in this paper.","In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching.","Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure.","Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD.","Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection."],"url":"http://arxiv.org/abs/2405.03352v1","category":"cs.CV"}
{"created":"2024-05-06 10:58:09","title":"Evolution of the 5G New Radio Two-Step Random Access towards 6G Unsourced MAC","abstract":"This report summarizes some considerations on possible evolutions of grant-free random access in the next generation of the 3GPP wireless cellular standard. The analysis is carried out by mapping the problem to the recently-introduced unsourced multiple access channel (UMAC) setup. By doing so, the performance of existing solutions can be benchmarked with information-theoretic bounds, assessing the potential gains that can be achieved over legacy 3GPP schemes. The study focuses on the two-step random access (2SRA) protocol introduced by Release 16 of the 5G New Radio standard, investigating its applicability to support large MTC / IoT terminal populations in a grant-free fashion. The analysis shows that the existing 2SRA scheme may not succeed in providing energy-efficient support to large user populations. Modifications to the protocol are proposed that enable remarkable gains in both energy and spectral efficiency while retaining a strong resemblance to the legacy protocol.","sentences":["This report summarizes some considerations on possible evolutions of grant-free random access in the next generation of the 3GPP wireless cellular standard.","The analysis is carried out by mapping the problem to the recently-introduced unsourced multiple access channel (UMAC) setup.","By doing so, the performance of existing solutions can be benchmarked with information-theoretic bounds, assessing the potential gains that can be achieved over legacy 3GPP schemes.","The study focuses on the two-step random access (2SRA) protocol introduced by Release 16 of the 5G New Radio standard, investigating its applicability to support large MTC / IoT terminal populations in a grant-free fashion.","The analysis shows that the existing 2SRA scheme may not succeed in providing energy-efficient support to large user populations.","Modifications to the protocol are proposed that enable remarkable gains in both energy and spectral efficiency while retaining a strong resemblance to the legacy protocol."],"url":"http://arxiv.org/abs/2405.03348v1","category":"cs.IT"}
{"created":"2024-05-06 10:52:19","title":"Population dynamics and games of variable size","abstract":"This work introduces the concept of Variable Size Game Theory (VSGT), in which the number of players in a game is a strategic decision made by the players themselves. We start by discussing the main examples in game theory: dominance, coexistence, and coordination. We show that the same set of pay-offs can result in coordination-like or coexistence-like games depending on the strategic decision of each player type. We also solve an inverse problem to find a $d$-player game that reproduces the same fixation pattern of the VSGT. In the sequel, we consider a game involving prosocial and antisocial players, i.e., individuals who tend to play with large groups and small groups, respectively. In this game, a certain task should be performed, that will benefit one of the participants at the expense of the other players. We show that individuals able to gather large groups to perform the task may prevail, even if this task is costly, providing a possible scenario for the evolution of eusociality. The next example shows that different strategies regarding game size may lead to spontaneous separation of different types, a possible scenario for speciation without physical separation (sympatric speciation). In the last example, we generalize to three types of populations from the previous analysis and study compartmental epidemic models: in particular, we recast the SIRS model into the VSGT framework: Susceptibles play 2-player games, while Infectious and Removed play a 1-player game. The SIRS epidemic model is then obtained as the replicator equation of the VSGT. We finish with possible applications of VSGT to be addressed in the future.","sentences":["This work introduces the concept of Variable Size Game Theory (VSGT), in which the number of players in a game is a strategic decision made by the players themselves.","We start by discussing the main examples in game theory: dominance, coexistence, and coordination.","We show that the same set of pay-offs can result in coordination-like or coexistence-like games depending on the strategic decision of each player type.","We also solve an inverse problem to find a $d$-player game that reproduces the same fixation pattern of the VSGT.","In the sequel, we consider a game involving prosocial and antisocial players, i.e., individuals who tend to play with large groups and small groups, respectively.","In this game, a certain task should be performed, that will benefit one of the participants at the expense of the other players.","We show that individuals able to gather large groups to perform the task may prevail, even if this task is costly, providing a possible scenario for the evolution of eusociality.","The next example shows that different strategies regarding game size may lead to spontaneous separation of different types, a possible scenario for speciation without physical separation (sympatric speciation).","In the last example, we generalize to three types of populations from the previous analysis and study compartmental epidemic models: in particular, we recast the SIRS model into the VSGT framework: Susceptibles play 2-player games, while Infectious and Removed play a 1-player game.","The SIRS epidemic model is then obtained as the replicator equation of the VSGT.","We finish with possible applications of VSGT to be addressed in the future."],"url":"http://arxiv.org/abs/2405.03346v1","category":"q-bio.PE"}
{"created":"2024-05-06 10:49:51","title":"Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning","abstract":"Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.","sentences":["Causal effect estimation under networked interference is an important but challenging problem.","Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process.","To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks.","Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness.","Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss.","Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model.","Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators."],"url":"http://arxiv.org/abs/2405.03342v1","category":"cs.LG"}
{"created":"2024-05-06 10:42:28","title":"Enhancing Q-Learning with Large Language Model Heuristics","abstract":"Q-learning excels in learning from feedback within sequential decision-making tasks but requires extensive sampling for significant improvements. Although reward shaping is a powerful technique for enhancing learning efficiency, it can introduce biases that affect agent performance. Furthermore, potential-based reward shaping is constrained as it does not allow for reward modifications based on actions or terminal states, potentially limiting its effectiveness in complex environments. Additionally, large language models (LLMs) can achieve zero-shot learning, but this is generally limited to simpler tasks. They also exhibit low inference speeds and occasionally produce hallucinations. To address these issues, we propose \\textbf{LLM-guided Q-learning} that employs LLMs as heuristic to aid in learning the Q-function for reinforcement learning. It combines the advantages of both technologies without introducing performance bias. Our theoretical analysis demonstrates that the LLM heuristic provides action-level guidance. Additionally, our architecture has the capability to convert the impact of hallucinations into exploration costs. Moreover, the converged Q function corresponds to the MDP optimal Q function. Experiment results demonstrated that our algorithm enables agents to avoid ineffective exploration, enhances sampling efficiency, and is well-suited for complex control tasks.","sentences":["Q-learning excels in learning from feedback within sequential decision-making tasks but requires extensive sampling for significant improvements.","Although reward shaping is a powerful technique for enhancing learning efficiency, it can introduce biases that affect agent performance.","Furthermore, potential-based reward shaping is constrained as it does not allow for reward modifications based on actions or terminal states, potentially limiting its effectiveness in complex environments.","Additionally, large language models (LLMs) can achieve zero-shot learning, but this is generally limited to simpler tasks.","They also exhibit low inference speeds and occasionally produce hallucinations.","To address these issues, we propose \\textbf{LLM-guided Q-learning} that employs LLMs as heuristic to aid in learning the Q-function for reinforcement learning.","It combines the advantages of both technologies without introducing performance bias.","Our theoretical analysis demonstrates that the LLM heuristic provides action-level guidance.","Additionally, our architecture has the capability to convert the impact of hallucinations into exploration costs.","Moreover, the converged Q function corresponds to the MDP optimal Q function.","Experiment results demonstrated that our algorithm enables agents to avoid ineffective exploration, enhances sampling efficiency, and is well-suited for complex control tasks."],"url":"http://arxiv.org/abs/2405.03341v1","category":"cs.LG"}
{"created":"2024-05-06 10:40:34","title":"Functional Equivalence with NARS","abstract":"This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA). Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability. In this study, ONA was modified to allow the derivation of functional equivalence. This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making. An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words. The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI.","sentences":["This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA).","Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability.","In this study, ONA was modified to allow the derivation of functional equivalence.","This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making.","An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words.","The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI."],"url":"http://arxiv.org/abs/2405.03340v1","category":"cs.AI"}
{"created":"2024-05-06 10:39:25","title":"Delayed Electron-Ion Entanglement Revealed with Zero Area Pulses","abstract":"The Grobe--Eberly doublet phenomenon occurs in photoelectron distributions when the remaining ion is dressed by a field. As was recently shown, the doublet can be interpreted as a signature of quantum entanglement between photoelectrons and strongly coupled ions. However, the dressed state nature of the ion prevents detection of the entanglement by straightforward coincidence detection. Here, we find that odd (zero-area) envelopes can substantially delay the generation of entanglement, but also modify the dynamics such that the doublet transforms into unique channel-resolved photoelectron distributions. Because these distributions can be used to correlate with the internal state of the ion, our proposed scheme opens up for detection of quantum entanglement, between photoelectrons and stongly-coupled ions, without a need for quantum phase measurements.","sentences":["The Grobe--Eberly doublet phenomenon occurs in photoelectron distributions when the remaining ion is dressed by a field.","As was recently shown, the doublet can be interpreted as a signature of quantum entanglement between photoelectrons and strongly coupled ions.","However, the dressed state nature of the ion prevents detection of the entanglement by straightforward coincidence detection.","Here, we find that odd (zero-area) envelopes can substantially delay the generation of entanglement, but also modify the dynamics such that the doublet transforms into unique channel-resolved photoelectron distributions.","Because these distributions can be used to correlate with the internal state of the ion, our proposed scheme opens up for detection of quantum entanglement, between photoelectrons and stongly-coupled ions, without a need for quantum phase measurements."],"url":"http://arxiv.org/abs/2405.03339v1","category":"quant-ph"}
{"created":"2024-05-06 10:30:27","title":"Spectral properties of the resolvent difference for singularly perturbed operators","abstract":"We obtain order sharp spectral estimates for the difference of resolvents of singularly perturbed elliptic operators $\\mathbf{A}+\\mathbf{V}_1$ and $\\mathbf{A}+\\mathbf{V}_2$ in a domain $\\Omega\\subseteq \\mathbb{R}^\\mathbf{N}$ with perturbations $\\mathbf{V}_1, \\mathbf{V}_2$ generated by $V_1\\mu,V_2\\mu,$ where $\\mu$ is a measure singular with respect to the Lebesgue measure and satisfying two-sided or one-sided conditions of Ahlfors type, while $V_1,V_2$ are weight functions subject to some integral conditions. As an important special case, spectral estimates for the difference of resolvents of two Robin realizations of the operator $\\mathbf{A}$ with different weight functions are obtained. For the case when the support of the measure is a compact Lipschitz hypersurface in $\\Omega$ or, more generally, a rectifiable set of Hau{\\ss}dorff dimension $d=\\mathbf{N}-1$, the Weyl type asymptotics for eigenvalues is justified.","sentences":["We obtain order sharp spectral estimates for the difference of resolvents of singularly perturbed elliptic operators $\\mathbf{A}+\\mathbf{V}_1$ and $\\mathbf{A}+\\mathbf{V}_2$ in a domain $\\Omega\\subseteq \\mathbb{R}^\\mathbf{N}$ with perturbations $\\mathbf{V}_1, \\mathbf{V}_2$ generated by $V_1\\mu,V_2\\mu,$ where $\\mu$ is a measure singular with respect to the Lebesgue measure and satisfying two-sided or one-sided conditions of Ahlfors type, while $V_1,V_2$ are weight functions subject to some integral conditions.","As an important special case, spectral estimates for the difference of resolvents of two Robin realizations of the operator $\\mathbf{A}$ with different weight functions are obtained.","For the case when the support of the measure is a compact Lipschitz hypersurface in $\\Omega$ or, more generally, a rectifiable set of Hau{\\ss}dorff dimension $d=\\mathbf{N}-1$, the Weyl type asymptotics for eigenvalues is justified."],"url":"http://arxiv.org/abs/2405.03335v1","category":"math.SP"}
{"created":"2024-05-06 10:27:31","title":"On the constrained feedback linearization control based on the MILP representation of a ReLU-ANN","abstract":"In this work, we explore the efficacy of rectified linear unit artificial neural networks in addressing the intricate challenges of convoluted constraints arising from feedback linearization mapping. Our approach involves a comprehensive procedure, encompassing the approximation of constraints through a regression process. Subsequently, we transform these constraints into an equivalent representation of mixed-integer linear constraints, seamlessly integrating them into other stabilizing control architectures. The advantage resides in the compatibility with the linear control design and the constraint satisfaction in the model predictive control setup, even for forecasted trajectories. Simulations are provided to validate the proposed constraint reformulation.","sentences":["In this work, we explore the efficacy of rectified linear unit artificial neural networks in addressing the intricate challenges of convoluted constraints arising from feedback linearization mapping.","Our approach involves a comprehensive procedure, encompassing the approximation of constraints through a regression process.","Subsequently, we transform these constraints into an equivalent representation of mixed-integer linear constraints, seamlessly integrating them into other stabilizing control architectures.","The advantage resides in the compatibility with the linear control design and the constraint satisfaction in the model predictive control setup, even for forecasted trajectories.","Simulations are provided to validate the proposed constraint reformulation."],"url":"http://arxiv.org/abs/2405.03334v1","category":"eess.SY"}
{"created":"2024-05-06 10:26:06","title":"Light-VQA+: A Video Quality Assessment Model for Exposure Correction with Vision-Language Guidance","abstract":"Recently, User-Generated Content (UGC) videos have gained popularity in our daily lives. However, UGC videos often suffer from poor exposure due to the limitations of photographic equipment and techniques. Therefore, Video Exposure Correction (VEC) algorithms have been proposed, Low-Light Video Enhancement (LLVE) and Over-Exposed Video Recovery (OEVR) included. Equally important to the VEC is the Video Quality Assessment (VQA). Unfortunately, almost all existing VQA models are built generally, measuring the quality of a video from a comprehensive perspective. As a result, Light-VQA, trained on LLVE-QA, is proposed for assessing LLVE. We extend the work of Light-VQA by expanding the LLVE-QA dataset into Video Exposure Correction Quality Assessment (VEC-QA) dataset with over-exposed videos and their corresponding corrected versions. In addition, we propose Light-VQA+, a VQA model specialized in assessing VEC. Light-VQA+ differs from Light-VQA mainly from the usage of the CLIP model and the vision-language guidance during the feature extraction, followed by a new module referring to the Human Visual System (HVS) for more accurate assessment. Extensive experimental results show that our model achieves the best performance against the current State-Of-The-Art (SOTA) VQA models on the VEC-QA dataset and other public datasets.","sentences":["Recently, User-Generated Content (UGC) videos have gained popularity in our daily lives.","However, UGC videos often suffer from poor exposure due to the limitations of photographic equipment and techniques.","Therefore, Video Exposure Correction (VEC) algorithms have been proposed, Low-Light Video Enhancement (LLVE) and Over-Exposed Video Recovery (OEVR) included.","Equally important to the VEC is the Video Quality Assessment (VQA).","Unfortunately, almost all existing VQA models are built generally, measuring the quality of a video from a comprehensive perspective.","As a result, Light-VQA, trained on LLVE-QA, is proposed for assessing LLVE.","We extend the work of Light-VQA by expanding the LLVE-QA dataset into Video Exposure Correction Quality Assessment (VEC-QA) dataset with over-exposed videos and their corresponding corrected versions.","In addition, we propose Light-VQA+, a VQA model specialized in assessing VEC.","Light-VQA+ differs from Light-VQA mainly from the usage of the CLIP model and the vision-language guidance during the feature extraction, followed by a new module referring to the Human Visual System (HVS) for more accurate assessment.","Extensive experimental results show that our model achieves the best performance against the current State-Of-The-Art (SOTA) VQA models on the VEC-QA dataset and other public datasets."],"url":"http://arxiv.org/abs/2405.03333v1","category":"cs.CV"}
{"created":"2024-05-06 10:20:26","title":"Molecular dynamics simulations of neutron induced collision cascades in Zr - statistical modelling of irradiation damage and potential applications","abstract":"Understanding the nature of irradiation damage often requires a multi-scale and multi-physics approach, i.e. it requires a significant amount of information from experiments, simulations and phenomenological models. This paper focuses on the initial stages of irradiation damage, namely neutron-induced displacement cascades in zirconium, as nuclear-grade zirconium alloys are widely used in fuel assemblies. We provide results of large-scale molecular dynamics (MD) simulations based on existing inter-atomic potentials and the two-temperature model to include the effect of electron-phonon coupling. Our data can be used directly in higher scale methods. Furthermore, we analysed summary statistics associated with defect production, such as the number of defects produced, their distribution and the size of clusters. As a result, we have developed a generative model of collision cascades. The model is hierarchical, as well as stochastic, i.e. it includes the variance of the considered features. This development had three main objectives: to establish a sufficient descriptor of a cascade, to develop an interpolator of data obtained from high-fidelity simulations, and to demonstrate that the statistical model of the data can generate representative distributions of primary irradiation defects. The results can be used to generate synthetic inputs for longer length- and time-scale models, as well as to build fast approximations relating dose, damage and irradiation conditions.","sentences":["Understanding the nature of irradiation damage often requires a multi-scale and multi-physics approach, i.e. it requires a significant amount of information from experiments, simulations and phenomenological models.","This paper focuses on the initial stages of irradiation damage, namely neutron-induced displacement cascades in zirconium, as nuclear-grade zirconium alloys are widely used in fuel assemblies.","We provide results of large-scale molecular dynamics (MD) simulations based on existing inter-atomic potentials and the two-temperature model to include the effect of electron-phonon coupling.","Our data can be used directly in higher scale methods.","Furthermore, we analysed summary statistics associated with defect production, such as the number of defects produced, their distribution and the size of clusters.","As a result, we have developed a generative model of collision cascades.","The model is hierarchical, as well as stochastic, i.e. it includes the variance of the considered features.","This development had three main objectives: to establish a sufficient descriptor of a cascade, to develop an interpolator of data obtained from high-fidelity simulations, and to demonstrate that the statistical model of the data can generate representative distributions of primary irradiation defects.","The results can be used to generate synthetic inputs for longer length- and time-scale models, as well as to build fast approximations relating dose, damage and irradiation conditions."],"url":"http://arxiv.org/abs/2405.03332v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 10:20:22","title":"A second-generation URANS model (STRUCT$-\u03b5$) applied to a Generic Side Mirror and its Impact on Sound Generation","abstract":"A generic side mirror can be approximated to the combination of a half cylinder topped with a quarter of sphere. The flow structure in the wake of the side mirror is highly transient and the turbulence plays an important role affecting aeroacoustics through pressure fluctuation. Thus, this geometry is one of the test cases object of several numerical studies in recent years to assess the aerodynamic and aeroacoustic capabilities of the turbulence models. In this context, this study presents how the second-generation URANS close STRUCT$-\\epsilon$ is able to properly predict the expected stagnation, flow separation and vortex shedding phenomena. Besides, the predictive accuracy for the noise generation mechanism is evaluated by comparing the spectra of the sound pressure level measured at several static pressure sensors with the numerical results obtained with the STRUCT$-\\epsilon$. The response of this turbulence model has overachieved other hybrid methods and is in good agreement with the results from Large-Eddy Simulations or the experiments. To conclude the paper, the applicability of STRUCT$-\\epsilon$ to construct a Spectral Proper Orthogonal Decomposition method that helps identifying the most energetic modes to appropriately capture the dominant flow structures is also introduced.","sentences":["A generic side mirror can be approximated to the combination of a half cylinder topped with a quarter of sphere.","The flow structure in the wake of the side mirror is highly transient and the turbulence plays an important role affecting aeroacoustics through pressure fluctuation.","Thus, this geometry is one of the test cases object of several numerical studies in recent years to assess the aerodynamic and aeroacoustic capabilities of the turbulence models.","In this context, this study presents how the second-generation URANS close STRUCT$-\\epsilon$ is able to properly predict the expected stagnation, flow separation and vortex shedding phenomena.","Besides, the predictive accuracy for the noise generation mechanism is evaluated by comparing the spectra of the sound pressure level measured at several static pressure sensors with the numerical results obtained with the STRUCT$-\\epsilon$. The response of this turbulence model has overachieved other hybrid methods and is in good agreement with the results from Large-Eddy Simulations or the experiments.","To conclude the paper, the applicability of STRUCT$-\\epsilon$ to construct a Spectral Proper Orthogonal Decomposition method that helps identifying the most energetic modes to appropriately capture the dominant flow structures is also introduced."],"url":"http://arxiv.org/abs/2405.03331v1","category":"physics.flu-dyn"}
{"created":"2024-05-06 10:13:18","title":"Swarm intelligence for full Stokes dynamic imaging reconstruction of interferometric data","abstract":"In very long baseline interferometry (VLBI) the combination of multiple antennas permits the synthesis of a virtual telescope with a larger diameter and consequently higher resolution than the individual antennae. Yet, due to the sparse nature of the array, recovering an image from the observed data is a challenging ill-posed inverse problem. The VLBI community is interested in not only recovering an image in total intensity from interferometric data, but also to obtain results in the polarimetric and the temporal domain. Only a few algorithms are able to work in all these domains simultaneously. In particular, the algorithms based on optimization that consider various penalty terms specific to static total intensity imaging, time-variability and polarimetry are restricted to grids the domain of the objective function. In this work we present a novel algorithm, multiobjective particle swarm optimization, that is able to recover the optimal weights without any space-gridding, and to obtain the marginal contribution of each the playing terms. To this end, we utilize multiobjective optimization together with particle swarm metaheuristics. We let the swarm of weights to converge together to the best position. We evaluate our algorithm with representative synthetic data sets focused on the instrumental configuration of the Event Horizon Telescope Collaboration and its planned successors. We successfully recover the polarimetric, static and time-dynamic signature of the ground truth movie, even with relative sparsity, and a set of realistic data corruptions. This is a novel, fast, weighting space gridding-free algorithm that successfully recovers static and dynamic polarimetric reconstructions. Compared to Regularized Maximum Likelihood methods, it avoids the need for parameter surveys, and it is not limited to the number of pixels such as recently proposed multiobjective imaging algorithms.","sentences":["In very long baseline interferometry (VLBI) the combination of multiple antennas permits the synthesis of a virtual telescope with a larger diameter and consequently higher resolution than the individual antennae.","Yet, due to the sparse nature of the array, recovering an image from the observed data is a challenging ill-posed inverse problem.","The VLBI community is interested in not only recovering an image in total intensity from interferometric data, but also to obtain results in the polarimetric and the temporal domain.","Only a few algorithms are able to work in all these domains simultaneously.","In particular, the algorithms based on optimization that consider various penalty terms specific to static total intensity imaging, time-variability and polarimetry are restricted to grids the domain of the objective function.","In this work we present a novel algorithm, multiobjective particle swarm optimization, that is able to recover the optimal weights without any space-gridding, and to obtain the marginal contribution of each the playing terms.","To this end, we utilize multiobjective optimization together with particle swarm metaheuristics.","We let the swarm of weights to converge together to the best position.","We evaluate our algorithm with representative synthetic data sets focused on the instrumental configuration of the Event Horizon Telescope Collaboration and its planned successors.","We successfully recover the polarimetric, static and time-dynamic signature of the ground truth movie, even with relative sparsity, and a set of realistic data corruptions.","This is a novel, fast, weighting space gridding-free algorithm that successfully recovers static and dynamic polarimetric reconstructions.","Compared to Regularized Maximum Likelihood methods, it avoids the need for parameter surveys, and it is not limited to the number of pixels such as recently proposed multiobjective imaging algorithms."],"url":"http://arxiv.org/abs/2405.03330v1","category":"astro-ph.IM"}
{"created":"2024-05-06 10:07:16","title":"Enhancing Spatiotemporal Disease Progression Models via Latent Diffusion and Prior Knowledge","abstract":"In this work, we introduce Brain Latent Progression (BrLP), a novel spatiotemporal disease progression model based on latent diffusion. BrLP is designed to predict the evolution of diseases at the individual level on 3D brain MRIs. Existing deep generative models developed for this task are primarily data-driven and face challenges in learning disease progressions. BrLP addresses these challenges by incorporating prior knowledge from disease models to enhance the accuracy of predictions. To implement this, we propose to integrate an auxiliary model that infers volumetric changes in various brain regions. Additionally, we introduce Latent Average Stabilization (LAS), a novel technique to improve spatiotemporal consistency of the predicted progression. BrLP is trained and evaluated on a large dataset comprising 11,730 T1-weighted brain MRIs from 2,805 subjects, collected from three publicly available, longitudinal Alzheimer's Disease (AD) studies. In our experiments, we compare the MRI scans generated by BrLP with the actual follow-up MRIs available from the subjects, in both cross-sectional and longitudinal settings. BrLP demonstrates significant improvements over existing methods, with an increase of 22% in volumetric accuracy across AD-related brain regions and 43% in image similarity to the ground-truth scans. The ability of BrLP to generate conditioned 3D scans at the subject level, along with the novelty of integrating prior knowledge to enhance accuracy, represents a significant advancement in disease progression modeling, opening new avenues for precision medicine. The code of BrLP is available at the following link: https://github.com/LemuelPuglisi/BrLP.","sentences":["In this work, we introduce Brain Latent Progression (BrLP), a novel spatiotemporal disease progression model based on latent diffusion.","BrLP is designed to predict the evolution of diseases at the individual level on 3D brain MRIs.","Existing deep generative models developed for this task are primarily data-driven and face challenges in learning disease progressions.","BrLP addresses these challenges by incorporating prior knowledge from disease models to enhance the accuracy of predictions.","To implement this, we propose to integrate an auxiliary model that infers volumetric changes in various brain regions.","Additionally, we introduce Latent Average Stabilization (LAS), a novel technique to improve spatiotemporal consistency of the predicted progression.","BrLP is trained and evaluated on a large dataset comprising 11,730 T1-weighted brain MRIs from 2,805 subjects, collected from three publicly available, longitudinal Alzheimer's Disease (AD) studies.","In our experiments, we compare the MRI scans generated by BrLP with the actual follow-up MRIs available from the subjects, in both cross-sectional and longitudinal settings.","BrLP demonstrates significant improvements over existing methods, with an increase of 22% in volumetric accuracy across AD-related brain regions and 43% in image similarity to the ground-truth scans.","The ability of BrLP to generate conditioned 3D scans at the subject level, along with the novelty of integrating prior knowledge to enhance accuracy, represents a significant advancement in disease progression modeling, opening new avenues for precision medicine.","The code of BrLP is available at the following link: https://github.com/LemuelPuglisi/BrLP."],"url":"http://arxiv.org/abs/2405.03328v1","category":"cs.CV"}
{"created":"2024-05-06 10:05:46","title":"Clustering of Disease Trajectories with Explainable Machine Learning: A Case Study on Postoperative Delirium Phenotypes","abstract":"The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics. Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology. We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice. Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies. In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes. We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features. We aim to mimic any clinical disease with our synthetic data generation method. By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space. We then present a case study using real-world data from a cohort of elderly surgical patients. The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies.","sentences":["The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics.","Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology.","We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice.","Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies.","In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes.","We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features.","We aim to mimic any clinical disease with our synthetic data generation method.","By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space.","We then present a case study using real-world data from a cohort of elderly surgical patients.","The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies."],"url":"http://arxiv.org/abs/2405.03327v1","category":"cs.LG"}
{"created":"2024-05-06 10:04:40","title":"PAFOT: A Position-Based Approach for Finding Optimal Tests of Autonomous Vehicles","abstract":"Autonomous Vehicles (AVs) are prone to revolutionise the transportation industry. However, they must be thoroughly tested to avoid safety violations. Simulation testing plays a crucial role in finding safety violations of Automated Driving Systems (ADSs). This paper proposes PAFOT, a position-based approach testing framework, which generates adversarial driving scenarios to expose safety violations of ADSs. We introduce a 9-position grid which is virtually drawn around the Ego Vehicle (EV) and modify the driving behaviours of Non-Playable Characters (NPCs) to move within this grid. PAFOT utilises a single-objective genetic algorithm to search for adversarial test scenarios. We demonstrate PAFOT on a well-known high-fidelity simulator, CARLA. The experimental results show that PAFOT can effectively generate safety-critical scenarios to crash ADSs and is able to find collisions in a short simulation time. Furthermore, it outperforms other search-based testing techniques by finding more safety-critical scenarios under the same driving conditions within less effective simulation time.","sentences":["Autonomous Vehicles (AVs) are prone to revolutionise the transportation industry.","However, they must be thoroughly tested to avoid safety violations.","Simulation testing plays a crucial role in finding safety violations of Automated Driving Systems (ADSs).","This paper proposes PAFOT, a position-based approach testing framework, which generates adversarial driving scenarios to expose safety violations of ADSs.","We introduce a 9-position grid which is virtually drawn around the Ego Vehicle (EV) and modify the driving behaviours of Non-Playable Characters (NPCs) to move within this grid.","PAFOT utilises a single-objective genetic algorithm to search for adversarial test scenarios.","We demonstrate PAFOT on a well-known high-fidelity simulator, CARLA.","The experimental results show that PAFOT can effectively generate safety-critical scenarios to crash ADSs and is able to find collisions in a short simulation time.","Furthermore, it outperforms other search-based testing techniques by finding more safety-critical scenarios under the same driving conditions within less effective simulation time."],"url":"http://arxiv.org/abs/2405.03326v1","category":"cs.SE"}
{"created":"2024-05-06 10:04:02","title":"Dark matter models with suppressed dark matter nuclei elastic cross section","abstract":"We propose two generalizations of the dark photon model which predict the suppressed elastic dark matter nuclei cross section in comparison with the corresponding prediction of the dark photon model. In the first model the main difference from dark photon model is that the mixing parameter $\\epsilon$ is nonlocal formfactor $\\epsilon(q^2)= \\frac{q^2}{\\Lambda^2}V(\\frac{q^2}{\\Lambda^2}) $ depending on the square of the momentum transfer $q^2$. Here $V(\\frac{q^2}{\\Lambda^2})$ is an entire function of the growth $\\rho \\geq \\frac{1}{2}$ and $\\Lambda$ is nonlocal scale. In this model our world and dark world are described by renormalizable field theories while the communication between them is performed by nonlocal interaction. The second model is renormalizable model where besides dark photon field $A'$ additional vector boson $Z'$ interacts with $B - L$ current. The communication between our world and dark world is performed due to nonzero kinetic mixing between $Z'$ and $A'$ fields. The predictions of the models for the search for dark matter at the accelerators don't contain additional suppression factors.","sentences":["We propose two generalizations of the dark photon model which predict the suppressed elastic dark matter nuclei cross section in comparison with the corresponding prediction of the dark photon model.","In the first model the main difference from dark photon model is that the mixing parameter $\\epsilon$ is nonlocal formfactor $\\epsilon(q^2)= \\frac{q^2}{\\Lambda^2}V(\\frac{q^2}{\\Lambda^2}) $ depending on the square of the momentum transfer $q^2$. Here $V(\\frac{q^2}{\\Lambda^2})$ is an entire function of the growth $\\rho \\geq \\frac{1}{2}$ and $\\Lambda$ is nonlocal scale.","In this model our world and dark world are described by renormalizable field theories while the communication between them is performed by nonlocal interaction.","The second model is renormalizable model where besides dark photon field $A'$ additional vector boson $Z'$ interacts with $B - L$ current.","The communication between our world and dark world is performed due to nonzero kinetic mixing between $Z'$ and $A'$ fields.","The predictions of the models for the search for dark matter at the accelerators don't contain additional suppression factors."],"url":"http://arxiv.org/abs/2405.03325v1","category":"hep-ph"}
{"created":"2024-05-06 10:00:38","title":"Distance between two manifolds, topological phase transitions and scaling laws","abstract":"Topological phases are generally characterized by topological invariants denoted by integer numbers. However, different topological systems often require different topological invariants to measure, such as geometric phases, topological orders, winding numbers, etc. Moreover, geometric phases and its associated definitions usually fail at critical points. Therefore, it's challenging to predict what would occur during the transformation between two different topological phases. To address these issues, in this work, we propose a general definition based on fidelity and trace distance from quantum information theory: manifold distance. This definition does not rely on the berry connection of the manifolds but rather on the information of the two manifolds - their ground state wave functions. Thus, it can measure different topological systems (including traditional band topology models, non-Hermitian systems, and topological order models, etc.) and exhibit some universal laws during the transformation between two topological phases. Our research demonstrates that when the properties of two manifolds are identical, the distance and associated higher-order derivatives between them can smoothly transition to each other. However, for two different topological manifolds, the higher-order derivatives exhibit various divergent behaviors near the critical points. For subsequent studies, we expect the method to be generalized to real-space or non-lattice models, in order to facilitate the study of a wider range of physical platforms such as open systems and many-body localization.","sentences":["Topological phases are generally characterized by topological invariants denoted by integer numbers.","However, different topological systems often require different topological invariants to measure, such as geometric phases, topological orders, winding numbers, etc.","Moreover, geometric phases and its associated definitions usually fail at critical points.","Therefore, it's challenging to predict what would occur during the transformation between two different topological phases.","To address these issues, in this work, we propose a general definition based on fidelity and trace distance from quantum information theory: manifold distance.","This definition does not rely on the berry connection of the manifolds but rather on the information of the two manifolds - their ground state wave functions.","Thus, it can measure different topological systems (including traditional band topology models, non-Hermitian systems, and topological order models, etc.) and exhibit some universal laws during the transformation between two topological phases.","Our research demonstrates that when the properties of two manifolds are identical, the distance and associated higher-order derivatives between them can smoothly transition to each other.","However, for two different topological manifolds, the higher-order derivatives exhibit various divergent behaviors near the critical points.","For subsequent studies, we expect the method to be generalized to real-space or non-lattice models, in order to facilitate the study of a wider range of physical platforms such as open systems and many-body localization."],"url":"http://arxiv.org/abs/2405.03323v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 09:55:11","title":"Denoising of Geodetic Time Series Using Spatiotemporal Graph Neural Networks: Application to Slow Slip Event Extraction","abstract":"Geospatial data has been transformative for the monitoring of the Earth, yet, as in the case of (geo)physical monitoring, the measurements can have variable spatial and temporal sampling and may be associated with a significant level of perturbations degrading the signal quality. Denoising geospatial data is, therefore, essential, yet often challenging because the observations may comprise noise coming from different origins, including both environmental signals and instrumental artifacts, which are spatially and temporally correlated, thus hard to disentangle. This study addresses the denoising of multivariate time series acquired by irregularly distributed networks of sensors, requiring specific methods to handle the spatiotemporal correlation of the noise and the signal of interest. Specifically, our method focuses on the denoising of geodetic position time series, used to monitor ground displacement worldwide with centimeter- to-millimeter precision. Among the signals affecting GNSS data, slow slip events (SSEs) are of interest to seismologists. These are transients of deformation that are weakly emerging compared to other signals. Here, we design SSEdenoiser, a multi-station spatiotemporal graph-based attentive denoiser that learns latent characteristics of GNSS noise to reveal SSE-related displacement with sub-millimeter precision. It is based on the key combination of graph recurrent networks and spatiotemporal Transformers. The proposed method is applied to the Cascadia subduction zone, where SSEs occur along with bursts of tectonic tremors, a seismic rumbling identified from independent seismic recordings. The extracted events match the spatiotemporal evolution of tremors. This good space-time correlation of the denoised GNSS signals with the tremors validates the proposed denoising procedure.","sentences":["Geospatial data has been transformative for the monitoring of the Earth, yet, as in the case of (geo)physical monitoring, the measurements can have variable spatial and temporal sampling and may be associated with a significant level of perturbations degrading the signal quality.","Denoising geospatial data is, therefore, essential, yet often challenging because the observations may comprise noise coming from different origins, including both environmental signals and instrumental artifacts, which are spatially and temporally correlated, thus hard to disentangle.","This study addresses the denoising of multivariate time series acquired by irregularly distributed networks of sensors, requiring specific methods to handle the spatiotemporal correlation of the noise and the signal of interest.","Specifically, our method focuses on the denoising of geodetic position time series, used to monitor ground displacement worldwide with centimeter- to-millimeter precision.","Among the signals affecting GNSS data, slow slip events (SSEs) are of interest to seismologists.","These are transients of deformation that are weakly emerging compared to other signals.","Here, we design SSEdenoiser, a multi-station spatiotemporal graph-based attentive denoiser that learns latent characteristics of GNSS noise to reveal SSE-related displacement with sub-millimeter precision.","It is based on the key combination of graph recurrent networks and spatiotemporal Transformers.","The proposed method is applied to the Cascadia subduction zone, where SSEs occur along with bursts of tectonic tremors, a seismic rumbling identified from independent seismic recordings.","The extracted events match the spatiotemporal evolution of tremors.","This good space-time correlation of the denoised GNSS signals with the tremors validates the proposed denoising procedure."],"url":"http://arxiv.org/abs/2405.03320v1","category":"cs.LG"}
{"created":"2024-05-06 09:53:52","title":"Characterization of locally standard, $\\mathbb{Z}$-equivariantly formal manifolds in general position","abstract":"We give a characterization of locally standard, $\\mathbb{Z}$-equivariantly formal manifolds in general position. In particular, we show that for dimension $2n$ at least $10$, to every such manifold with labeled GKM graph $\\Gamma$ there is an equivariantly formal torus manifold such that the restriction of the $T^n$-action to a certain $T^{n-1}$-action yields the same labeled graph $\\Gamma$, thus showing that the (equivariant) cohomology with $\\mathbb{Z}$-coefficients of those manifolds has the same description as that of equivariantly formal torus manifolds.","sentences":["We give a characterization of locally standard, $\\mathbb{Z}$-equivariantly formal manifolds in general position.","In particular, we show that for dimension $2n$ at least $10$, to every such manifold with labeled GKM graph $\\Gamma$ there is an equivariantly formal torus manifold such that the restriction of the $T^n$-action to a certain $T^{n-1}$-action yields the same labeled graph $\\Gamma$, thus showing that the (equivariant) cohomology with $\\mathbb{Z}$-coefficients of those manifolds has the same description as that of equivariantly formal torus manifolds."],"url":"http://arxiv.org/abs/2405.03319v1","category":"math.AT"}
{"created":"2024-05-06 09:50:04","title":"Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation","abstract":"The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.","sentences":["The design of the query is crucial for the performance of DETR and its variants.","Each query consists of two components: a content part and a positional one.","Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance.","In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation.","The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling.","This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects.","However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones.","To overcome this, we propose a query aggregation strategy to cooperate with SACQ.","It merges similar predicted candidates from different queries, easing the optimization.","Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP."],"url":"http://arxiv.org/abs/2405.03318v1","category":"cs.CV"}
{"created":"2024-05-06 09:48:47","title":"Provably Unlearnable Examples","abstract":"The exploitation of publicly accessible data has led to escalating concerns regarding data privacy and intellectual property (IP) breaches in the age of artificial intelligence. As a strategy to safeguard both data privacy and IP-related domain knowledge, efforts have been undertaken to render shared data unlearnable for unauthorized models in the wild. Existing methods apply empirically optimized perturbations to the data in the hope of disrupting the correlation between the inputs and the corresponding labels such that the data samples are converted into Unlearnable Examples (UEs). Nevertheless, the absence of mechanisms that can verify how robust the UEs are against unknown unauthorized models and train-time techniques engenders several problems. First, the empirically optimized perturbations may suffer from the problem of cross-model generalization, which echoes the fact that the unauthorized models are usually unknown to the defender. Second, UEs can be mitigated by train-time techniques such as data augmentation and adversarial training. Furthermore, we find that a simple recovery attack can restore the clean-task performance of the classifiers trained on UEs by slightly perturbing the learned weights. To mitigate the aforementioned problems, in this paper, we propose a mechanism for certifying the so-called $(q, \\eta)$-Learnability of an unlearnable dataset via parametric smoothing. A lower certified $(q, \\eta)$-Learnability indicates a more robust protection over the dataset. Finally, we try to 1) improve the tightness of certified $(q, \\eta)$-Learnability and 2) design Provably Unlearnable Examples (PUEs) which have reduced $(q, \\eta)$-Learnability. According to experimental results, PUEs demonstrate both decreased certified $(q, \\eta)$-Learnability and enhanced empirical robustness compared to existing UEs.","sentences":["The exploitation of publicly accessible data has led to escalating concerns regarding data privacy and intellectual property (IP) breaches in the age of artificial intelligence.","As a strategy to safeguard both data privacy and IP-related domain knowledge, efforts have been undertaken to render shared data unlearnable for unauthorized models in the wild.","Existing methods apply empirically optimized perturbations to the data in the hope of disrupting the correlation between the inputs and the corresponding labels such that the data samples are converted into Unlearnable Examples (UEs).","Nevertheless, the absence of mechanisms that can verify how robust the UEs are against unknown unauthorized models and train-time techniques engenders several problems.","First, the empirically optimized perturbations may suffer from the problem of cross-model generalization, which echoes the fact that the unauthorized models are usually unknown to the defender.","Second, UEs can be mitigated by train-time techniques such as data augmentation and adversarial training.","Furthermore, we find that a simple recovery attack can restore the clean-task performance of the classifiers trained on UEs by slightly perturbing the learned weights.","To mitigate the aforementioned problems, in this paper, we propose a mechanism for certifying the so-called $(q, \\eta)$-Learnability of an unlearnable dataset via parametric smoothing.","A lower certified $(q, \\eta)$-Learnability indicates a more robust protection over the dataset.","Finally, we try to 1) improve the tightness of certified $(q, \\eta)$-Learnability and 2) design Provably Unlearnable Examples (PUEs) which have reduced $(q, \\eta)$-Learnability.","According to experimental results, PUEs demonstrate both decreased certified $(q, \\eta)$-Learnability and enhanced empirical robustness compared to existing UEs."],"url":"http://arxiv.org/abs/2405.03316v1","category":"cs.LG"}
{"created":"2024-05-06 09:44:17","title":"The period-index conjecture for abelian threefolds and Donaldson-Thomas theory","abstract":"We prove the period-index conjecture for unramified Brauer classes on abelian threefolds. To do so, we develop a theory of reduced Donaldson-Thomas invariants for 3-dimensional Calabi-Yau categories, with the feature that the noncommutative variational integral Hodge conjecture holds for classes with nonvanishing invariant. The period-index result is then proved by interpreting it as the algebraicity of a Hodge class on the twisted derived category, and specializing within the Hodge locus to an untwisted abelian threefold with nonvanishing invariant. As a consequence, we also deduce the integral Hodge conjecture for generically twisted abelian threefolds.","sentences":["We prove the period-index conjecture for unramified Brauer classes on abelian threefolds.","To do so, we develop a theory of reduced Donaldson-Thomas invariants for 3-dimensional Calabi-Yau categories, with the feature that the noncommutative variational integral Hodge conjecture holds for classes with nonvanishing invariant.","The period-index result is then proved by interpreting it as the algebraicity of a Hodge class on the twisted derived category, and specializing within the Hodge locus to an untwisted abelian threefold with nonvanishing invariant.","As a consequence, we also deduce the integral Hodge conjecture for generically twisted abelian threefolds."],"url":"http://arxiv.org/abs/2405.03315v1","category":"math.AG"}
{"created":"2024-05-06 09:41:28","title":"On the normal stability of the 4-harmonic and the ES-4-harmonic hypersphere","abstract":"Both 4-harmonic and ES-4-harmonic maps are two higher order generalizations of the well-studied harmonic map equation given by a nonlinear elliptic partial differential equation of order eight. Due to the large number of derivatives it is very difficult to find any difference in the qualitative behavior of these two variational problems. It is well known that the small hypersphere $\\iota\\colon\\mathbb{S}^m(\\frac{1}{\\sqrt{4}})\\to\\mathbb{S}^{m+1}$ is a critical point of both the 4-energy as well as the ES-4-energy but up to now it has not been investigated if there is a difference concerning its stability.   The main contribution of this article is to show that the small hypersphere is unstable with respect to normal variations both as 4-harmonic hypersphere as well as ES-4-harmonic hypersphere and that its normal index equals one in both cases.","sentences":["Both 4-harmonic and ES-4-harmonic maps are two higher order generalizations of the well-studied harmonic map equation given by a nonlinear elliptic partial differential equation of order eight.","Due to the large number of derivatives it is very difficult to find any difference in the qualitative behavior of these two variational problems.","It is well known that the small hypersphere $\\iota\\colon\\mathbb{S}^m(\\frac{1}{\\sqrt{4}})\\to\\mathbb{S}^{m+1}$ is a critical point of both the 4-energy as well as the ES-4-energy but up to now it has not been investigated if there is a difference concerning its stability.   ","The main contribution of this article is to show that the small hypersphere is unstable with respect to normal variations both as 4-harmonic hypersphere as well as ES-4-harmonic hypersphere and that its normal index equals one in both cases."],"url":"http://arxiv.org/abs/2405.03313v1","category":"math.DG"}
{"created":"2024-05-06 09:39:13","title":"Federated Learning for Drowsiness Detection in Connected Vehicles","abstract":"Ensuring driver readiness poses challenges, yet driver monitoring systems can assist in determining the driver's state. By observing visual cues, such systems recognize various behaviors and associate them with specific conditions. For instance, yawning or eye blinking can indicate driver drowsiness. Consequently, an abundance of distributed data is generated for driver monitoring. Employing machine learning techniques, such as driver drowsiness detection, presents a potential solution. However, transmitting the data to a central machine for model training is impractical due to the large data size and privacy concerns. Conversely, training on a single vehicle would limit the available data and likely result in inferior performance. To address these issues, we propose a federated learning framework for drowsiness detection within a vehicular network, leveraging the YawDD dataset. Our approach achieves an accuracy of 99.2%, demonstrating its promise and comparability to conventional deep learning techniques. Lastly, we show how our model scales using various number of federated clients","sentences":["Ensuring driver readiness poses challenges, yet driver monitoring systems can assist in determining the driver's state.","By observing visual cues, such systems recognize various behaviors and associate them with specific conditions.","For instance, yawning or eye blinking can indicate driver drowsiness.","Consequently, an abundance of distributed data is generated for driver monitoring.","Employing machine learning techniques, such as driver drowsiness detection, presents a potential solution.","However, transmitting the data to a central machine for model training is impractical due to the large data size and privacy concerns.","Conversely, training on a single vehicle would limit the available data and likely result in inferior performance.","To address these issues, we propose a federated learning framework for drowsiness detection within a vehicular network, leveraging the YawDD dataset.","Our approach achieves an accuracy of 99.2%, demonstrating its promise and comparability to conventional deep learning techniques.","Lastly, we show how our model scales using various number of federated clients"],"url":"http://arxiv.org/abs/2405.03311v1","category":"cs.CV"}
{"created":"2024-05-06 09:34:26","title":"Efficient Symbolic Planning with Views","abstract":"Robotic planning systems model spatial relations in detail as these are needed for manipulation tasks. In contrast to this, other physical attributes of objects and the effect of devices are usually oversimplified and expressed by abstract compound attributes. This limits the ability of planners to find alternative solutions. We propose to break these compound attributes down into a shared set of elementary attributes. This strongly facilitates generalization between different tasks and environments and thus helps to find innovative solutions. On the down-side, this generalization comes with an increased complexity of the solution space. Therefore, as the main contribution of the paper, we propose a method that splits the planning problem into a sequence of views, where in each view only an increasing subset of attributes is considered. We show that this view-based strategy offers a good compromise between planning speed and quality of the found plan, and discuss its general applicability and limitations.","sentences":["Robotic planning systems model spatial relations in detail as these are needed for manipulation tasks.","In contrast to this, other physical attributes of objects and the effect of devices are usually oversimplified and expressed by abstract compound attributes.","This limits the ability of planners to find alternative solutions.","We propose to break these compound attributes down into a shared set of elementary attributes.","This strongly facilitates generalization between different tasks and environments and thus helps to find innovative solutions.","On the down-side, this generalization comes with an increased complexity of the solution space.","Therefore, as the main contribution of the paper, we propose a method that splits the planning problem into a sequence of views, where in each view only an increasing subset of attributes is considered.","We show that this view-based strategy offers a good compromise between planning speed and quality of the found plan, and discuss its general applicability and limitations."],"url":"http://arxiv.org/abs/2405.03307v1","category":"cs.RO"}
{"created":"2024-05-06 09:28:30","title":"Artificial Intelligence in the Autonomous Navigation of Endovascular Interventions: A Systematic Review","abstract":"Purpose: Autonomous navigation of devices in endovascular interventions can decrease operation times, improve decision-making during surgery, and reduce operator radiation exposure while increasing access to treatment. This systematic review explores recent literature to assess the impact, challenges, and opportunities artificial intelligence (AI) has for the autonomous endovascular intervention navigation.   Methods: PubMed and IEEEXplore databases were queried. Eligibility criteria included studies investigating the use of AI in enabling the autonomous navigation of catheters/guidewires in endovascular interventions. Following PRISMA, articles were assessed using QUADAS-2. PROSPERO: CRD42023392259.   Results: Among 462 studies, fourteen met inclusion criteria. Reinforcement learning (9/14, 64%) and learning from demonstration (7/14, 50%) were used as data-driven models for autonomous navigation. Studies predominantly utilised physical phantoms (10/14, 71%) and in silico (4/14, 29%) models. Experiments within or around the blood vessels of the heart were reported by the majority of studies (10/14, 71%), while simple non-anatomical vessel platforms were used in three studies (3/14, 21%), and the porcine liver venous system in one study. We observed that risk of bias and poor generalisability were present across studies. No procedures were performed on patients in any of the studies reviewed. Studies lacked patient selection criteria, reference standards, and reproducibility, resulting in low clinical evidence levels.   Conclusions: AI's potential in autonomous endovascular navigation is promising, but in an experimental proof-of-concept stage, with a technology readiness level of 3. We highlight that reference standards with well-identified performance metrics are crucial to allow for comparisons of data-driven algorithms proposed in the years to come.","sentences":["Purpose: Autonomous navigation of devices in endovascular interventions can decrease operation times, improve decision-making during surgery, and reduce operator radiation exposure while increasing access to treatment.","This systematic review explores recent literature to assess the impact, challenges, and opportunities artificial intelligence (AI) has for the autonomous endovascular intervention navigation.   ","Methods: PubMed and IEEEXplore databases were queried.","Eligibility criteria included studies investigating the use of AI in enabling the autonomous navigation of catheters/guidewires in endovascular interventions.","Following PRISMA, articles were assessed using QUADAS-2.","PROSPERO: CRD42023392259.   ","Results:","Among 462 studies, fourteen met inclusion criteria.","Reinforcement learning (9/14, 64%) and learning from demonstration (7/14, 50%) were used as data-driven models for autonomous navigation.","Studies predominantly utilised physical phantoms (10/14, 71%) and in silico (4/14, 29%) models.","Experiments within or around the blood vessels of the heart were reported by the majority of studies (10/14, 71%), while simple non-anatomical vessel platforms were used in three studies (3/14, 21%), and the porcine liver venous system in one study.","We observed that risk of bias and poor generalisability were present across studies.","No procedures were performed on patients in any of the studies reviewed.","Studies lacked patient selection criteria, reference standards, and reproducibility, resulting in low clinical evidence levels.   ","Conclusions: AI's potential in autonomous endovascular navigation is promising, but in an experimental proof-of-concept stage, with a technology readiness level of 3.","We highlight that reference standards with well-identified performance metrics are crucial to allow for comparisons of data-driven algorithms proposed in the years to come."],"url":"http://arxiv.org/abs/2405.03305v1","category":"cs.AI"}
{"created":"2024-05-06 09:25:14","title":"Explainability for Transparent Conversational Information-Seeking","abstract":"The increasing reliance on digital information necessitates advancements in conversational search systems, particularly in terms of information transparency. While prior research in conversational information-seeking has concentrated on improving retrieval techniques, the challenge remains in generating responses useful from a user perspective. This study explores different methods of explaining the responses, hypothesizing that transparency about the source of the information, system confidence, and limitations can enhance users' ability to objectively assess the response. By exploring transparency across explanation type, quality, and presentation mode, this research aims to bridge the gap between system-generated responses and responses verifiable by the user. We design a user study to answer questions concerning the impact of (1) the quality of explanations enhancing the response on its usefulness and (2) ways of presenting explanations to users. The analysis of the collected data reveals lower user ratings for noisy explanations, although these scores seem insensitive to the quality of the response. Inconclusive results on the explanations presentation format suggest that it may not be a critical factor in this setting.","sentences":["The increasing reliance on digital information necessitates advancements in conversational search systems, particularly in terms of information transparency.","While prior research in conversational information-seeking has concentrated on improving retrieval techniques, the challenge remains in generating responses useful from a user perspective.","This study explores different methods of explaining the responses, hypothesizing that transparency about the source of the information, system confidence, and limitations can enhance users' ability to objectively assess the response.","By exploring transparency across explanation type, quality, and presentation mode, this research aims to bridge the gap between system-generated responses and responses verifiable by the user.","We design a user study to answer questions concerning the impact of (1) the quality of explanations enhancing the response on its usefulness and (2) ways of presenting explanations to users.","The analysis of the collected data reveals lower user ratings for noisy explanations, although these scores seem insensitive to the quality of the response.","Inconclusive results on the explanations presentation format suggest that it may not be a critical factor in this setting."],"url":"http://arxiv.org/abs/2405.03303v1","category":"cs.IR"}
{"created":"2024-05-06 09:21:35","title":"Interpretable Network Visualizations: A Human-in-the-Loop Approach for Post-hoc Explainability of CNN-based Image Classification","abstract":"Transparency and explainability in image classification are essential for establishing trust in machine learning models and detecting biases and errors. State-of-the-art explainability methods generate saliency maps to show where a specific class is identified, without providing a detailed explanation of the model's decision process. Striving to address such a need, we introduce a post-hoc method that explains the entire feature extraction process of a Convolutional Neural Network. These explanations include a layer-wise representation of the features the model extracts from the input. Such features are represented as saliency maps generated by clustering and merging similar feature maps, to which we associate a weight derived by generalizing Grad-CAM for the proposed methodology. To further enhance these explanations, we include a set of textual labels collected through a gamified crowdsourcing activity and processed using NLP techniques and Sentence-BERT. Finally, we show an approach to generate global explanations by aggregating labels across multiple images.","sentences":["Transparency and explainability in image classification are essential for establishing trust in machine learning models and detecting biases and errors.","State-of-the-art explainability methods generate saliency maps to show where a specific class is identified, without providing a detailed explanation of the model's decision process.","Striving to address such a need, we introduce a post-hoc method that explains the entire feature extraction process of a Convolutional Neural Network.","These explanations include a layer-wise representation of the features the model extracts from the input.","Such features are represented as saliency maps generated by clustering and merging similar feature maps, to which we associate a weight derived by generalizing Grad-CAM for the proposed methodology.","To further enhance these explanations, we include a set of textual labels collected through a gamified crowdsourcing activity and processed using NLP techniques and Sentence-BERT.","Finally, we show an approach to generate global explanations by aggregating labels across multiple images."],"url":"http://arxiv.org/abs/2405.03301v1","category":"cs.LG"}
{"created":"2024-05-06 09:21:33","title":"Active RIS-Aided Massive MIMO With Imperfect CSI and Phase Noise","abstract":"Active reconfigurable intelligent surface (RIS) has attracted significant attention as a recently proposed RIS architecture. Owing to its capability to amplify the incident signals, active RIS can mitigate the multiplicative fading effect inherent in the passive RIS-aided system. In this paper, we consider an active RIS-aided uplink multi-user massive multiple-input multiple-output (MIMO) system in the presence of phase noise at the active RIS. Specifically, we employ a two-timescale scheme, where the beamforming at the base station (BS) is adjusted based on the instantaneous aggregated channel state information (CSI) and the statistical CSI serves as the basis for designing the phase shifts at the active RIS, so that the feedback overhead and computational complexity can be significantly reduced. The aggregated channel composed of the cascaded and direct channels is estimated by utilizing the linear minimum mean square error (LMMSE) technique. Based on the estimated channel, we derive the analytical closed-form expression of a lower bound of the achievable rate. The power scaling laws in the active RIS-aided system are investigated based on the theoretical expressions. When the transmit power of each user is scaled down by the number of BS antennas M or reflecting elements N, we find that the thermal noise will cause the lower bound of the achievable rate to approach zero, as the number of M or N increases to infinity. Moreover, an optimization approach based on genetic algorithms (GA) is introduced to tackle the phase shift optimization problem. Numerical results reveal that the active RIS can greatly enhance the performance of the considered system under various settings.","sentences":["Active reconfigurable intelligent surface (RIS) has attracted significant attention as a recently proposed RIS architecture.","Owing to its capability to amplify the incident signals, active RIS can mitigate the multiplicative fading effect inherent in the passive RIS-aided system.","In this paper, we consider an active RIS-aided uplink multi-user massive multiple-input multiple-output (MIMO) system in the presence of phase noise at the active RIS.","Specifically, we employ a two-timescale scheme, where the beamforming at the base station (BS) is adjusted based on the instantaneous aggregated channel state information (CSI) and the statistical CSI serves as the basis for designing the phase shifts at the active RIS, so that the feedback overhead and computational complexity can be significantly reduced.","The aggregated channel composed of the cascaded and direct channels is estimated by utilizing the linear minimum mean square error (LMMSE) technique.","Based on the estimated channel, we derive the analytical closed-form expression of a lower bound of the achievable rate.","The power scaling laws in the active RIS-aided system are investigated based on the theoretical expressions.","When the transmit power of each user is scaled down by the number of BS antennas M or reflecting elements N, we find that the thermal noise will cause the lower bound of the achievable rate to approach zero, as the number of M or N increases to infinity.","Moreover, an optimization approach based on genetic algorithms (GA) is introduced to tackle the phase shift optimization problem.","Numerical results reveal that the active RIS can greatly enhance the performance of the considered system under various settings."],"url":"http://arxiv.org/abs/2405.03300v1","category":"cs.IT"}
{"created":"2024-05-06 09:17:23","title":"Coefficient Decomposition for Spectral Graph Convolution","abstract":"Spectral graph convolutional network (SGCN) is a kind of graph neural networks (GNN) based on graph signal filters, and has shown compelling expressivity for modeling graph-structured data. Most SGCNs adopt polynomial filters and learn the coefficients from the training data. Many of them focus on which polynomial basis leads to optimal expressive power and models' architecture is little discussed. In this paper, we propose a general form in terms of spectral graph convolution, where the coefficients of polynomial basis are stored in a third-order tensor. Then, we show that the convolution block in existing SGCNs can be derived by performing a certain coefficient decomposition operation on the coefficient tensor. Based on the generalized view, we develop novel spectral graph convolutions CoDeSGC-CP and -Tucker by tensor decomposition CP and Tucker on the coefficient tensor. Extensive experimental results demonstrate that the proposed convolutions achieve favorable performance improvements.","sentences":["Spectral graph convolutional network (SGCN) is a kind of graph neural networks (GNN) based on graph signal filters, and has shown compelling expressivity for modeling graph-structured data.","Most SGCNs adopt polynomial filters and learn the coefficients from the training data.","Many of them focus on which polynomial basis leads to optimal expressive power and models' architecture is little discussed.","In this paper, we propose a general form in terms of spectral graph convolution, where the coefficients of polynomial basis are stored in a third-order tensor.","Then, we show that the convolution block in existing SGCNs can be derived by performing a certain coefficient decomposition operation on the coefficient tensor.","Based on the generalized view, we develop novel spectral graph convolutions CoDeSGC-CP and -Tucker by tensor decomposition CP and Tucker on the coefficient tensor.","Extensive experimental results demonstrate that the proposed convolutions achieve favorable performance improvements."],"url":"http://arxiv.org/abs/2405.03296v1","category":"cs.LG"}
{"created":"2024-05-06 09:16:50","title":"Modeling non stationary noise in pulsar timing array data analysis","abstract":"Pulsar Timing Array (PTA) collaborations recently reported evidence for the presence of a gravitational wave background (GWB) in their datasets. The main candidate that is expected to produce such a GWB is the population of supermassive black hole binaries (SMBHB). Some analyses showed that the recovered signal may exhibit time-dependent properties, i.e. non-stationarity. In this paper, we propose an approximated non-stationary Gaussian process (GP) model obtained from the perturbation of stationary processes. The presented method is applied to the second data release of the European pulsar timing array to search for non-stationary features in the GWB. We analyzed the data in different time slices and showed that the inferred properties of the GWB evolve with time. We find no evidence for such non-stationary behavior and the Bayes factor in favor of the latter is $\\mathcal{B}^{NS}_{S} = 1.5$. We argue that the evolution of the GWB properties most likely comes from the \\mf{improvement of the observation cadence} with time and \\mf{better} characterization of the noise of individual pulsars. Such non-stationary GWB could also be produced by the leakage of non-stationary features in the noise of individual pulsars or by the presence of an eccentric single source.","sentences":["Pulsar Timing Array (PTA) collaborations recently reported evidence for the presence of a gravitational wave background (GWB) in their datasets.","The main candidate that is expected to produce such a GWB is the population of supermassive black hole binaries (SMBHB).","Some analyses showed that the recovered signal may exhibit time-dependent properties, i.e. non-stationarity.","In this paper, we propose an approximated non-stationary Gaussian process (GP) model obtained from the perturbation of stationary processes.","The presented method is applied to the second data release of the European pulsar timing array to search for non-stationary features in the GWB.","We analyzed the data in different time slices and showed that the inferred properties of the GWB evolve with time.","We find no evidence for such non-stationary behavior and the Bayes factor in favor of the latter is $\\mathcal{B}^{NS}_{S} = 1.5$. We argue that the evolution of the GWB properties most likely comes from the \\mf{improvement of the observation cadence} with time and \\mf{better} characterization of the noise of individual pulsars.","Such non-stationary GWB could also be produced by the leakage of non-stationary features in the noise of individual pulsars or by the presence of an eccentric single source."],"url":"http://arxiv.org/abs/2405.03295v1","category":"astro-ph.HE"}
{"created":"2024-05-06 09:15:25","title":"On the generalized Dirichlet beta and Riemann zeta functions and Ramanujan-type formulae for beta and zeta values","abstract":"We define the generalized Dirichlet beta and Riemann zeta functions in terms of the integrals, involving powers of the hyperbolic secant and cosecant functions. The corresponding functional equations are established. Some consequences of the Ramanujan identity for zeta values at odd integers are investigated and new formulae of the Ramanujan type are obtained.","sentences":["We define the generalized Dirichlet beta and Riemann zeta functions in terms of the integrals, involving powers of the hyperbolic secant and cosecant functions.","The corresponding functional equations are established.","Some consequences of the Ramanujan identity for zeta values at odd integers are investigated and new formulae of the Ramanujan type are obtained."],"url":"http://arxiv.org/abs/2405.03294v1","category":"math.CA"}
{"created":"2024-05-06 09:14:58","title":"Deep Learning and genetic algorithms for cosmological Bayesian inference speed-up","abstract":"In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms. Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data. However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations. Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process. Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training. This flexibility enables adaptation to various theoretical models and datasets. We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function. Once sufficient accuracy is achieved, the neural network replaces the original likelihood function. The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets. Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods.","sentences":["In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms.","Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data.","However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations.","Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process.","Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training.","This flexibility enables adaptation to various theoretical models and datasets.","We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function.","Once sufficient accuracy is achieved, the neural network replaces the original likelihood function.","The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets.","Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods."],"url":"http://arxiv.org/abs/2405.03293v1","category":"astro-ph.IM"}
{"created":"2024-05-06 09:09:53","title":"Spontaneous Lorentz symmetry-breaking constraints in Kalb-Ramond gravity","abstract":"In this work, we study timelike and lightlike geodesics in Kalb-Ramond (KR) gravity around a black hole with the goal of constraining the Lorentz symmetry-breaking parameter $l$. The analysis involves studying the precession of the S2 star periastron orbiting Sgr A* and geodesic precession around the Earth. The ratio of precession frequencies for General Relativity (GR) and KR gravity is computed, with Event Horizon Telescope (EHT) results providing a parameter range for the spontaneous symmetry-breaking of $-0.185022 \\leq l \\leq 0.060938$. Utilizing the geodesic precession frequency from the Gravity Probe B (GP-B), the $l$ parameter is further constrained to $-6.30714 \\times 10^{-12} \\leq l \\leq 3.90708 \\times 10^{-12}$, which is consistent with the Schwarzschild limits. Moreover, for timelike geodesics, the innermost circular orbit (ICO) and innermost stable circular orbit (ISCO) are determined and analyzed to illustrate the impact of the symmetry breaking term. Zoom-whirl obstructions are compared with the Schwarzschild solution. Lower and upper limits of the photon sphere for lightlike geodesics are established to demonstrate the influence of KR gravity on the photon sphere. Additionally, the shadow radius is determined for two observers, one situated at a finite distance from the KR black hole, and the other located at an infinite distance, to constrain the symmetry-breaking parameter $l$, with comparisons made to EHT results. The bounds for $l$ derived from constraints on the photon sphere radius for lightlike geodesics yield $-0.0700225 \\leq l \\leq 0.189785$ using EHT data. The findings of this paper align with experimental results in the $l \\rightarrow 0$ limit.","sentences":["In this work, we study timelike and lightlike geodesics in Kalb-Ramond (KR) gravity around a black hole with the goal of constraining the Lorentz symmetry-breaking parameter $l$. The analysis involves studying the precession of the S2 star periastron orbiting Sgr A* and geodesic precession around the Earth.","The ratio of precession frequencies for General Relativity (GR) and KR gravity is computed, with Event Horizon Telescope (EHT) results providing a parameter range for the spontaneous symmetry-breaking of $-0.185022","\\leq l","\\leq 0.060938$. Utilizing the geodesic precession frequency from the Gravity Probe B (GP-B), the $l$ parameter is further constrained to $-6.30714 \\times 10^{-12} \\leq l \\leq 3.90708 \\times 10^{-12}$, which is consistent with the Schwarzschild limits.","Moreover, for timelike geodesics, the innermost circular orbit (ICO) and innermost stable circular orbit (ISCO) are determined and analyzed to illustrate the impact of the symmetry breaking term.","Zoom-whirl obstructions are compared with the Schwarzschild solution.","Lower and upper limits of the photon sphere for lightlike geodesics are established to demonstrate the influence of KR gravity on the photon sphere.","Additionally, the shadow radius is determined for two observers, one situated at a finite distance from the KR black hole, and the other located at an infinite distance, to constrain the symmetry-breaking parameter $l$, with comparisons made to EHT results.","The bounds for $l$ derived from constraints on the photon sphere radius for lightlike geodesics yield $-0.0700225","\\leq l \\leq 0.189785$ using EHT data.","The findings of this paper align with experimental results in the $l \\rightarrow 0$ limit."],"url":"http://arxiv.org/abs/2405.03291v1","category":"gr-qc"}
{"created":"2024-05-06 09:06:33","title":"Exact vacuum spacetimes from Fierz-Pauli","abstract":"Nilpotent deformations of an arbitrary background spacetime are considered . Those are of the form $\\bg_{\\m\\n}+ l_\\m l_\\n$ with $l^2=0$. The relationship between the Ricci tensor of the background metric and the Ricci tensor of the deformed metric is found exactly. It consists of two terms. one is essentially the Fierz-Pauli operator, and the other is new. When the background is flat, the Kerr-Schild family is recovered. Novel examples for more general backgrounds are discussed.","sentences":["Nilpotent deformations of an arbitrary background spacetime are considered .","Those are of the form $\\bg_{\\m\\n}+ l_\\m l_\\n$ with $l^2=0$.","The relationship between the Ricci tensor of the background metric and the Ricci tensor of the deformed metric is found exactly.","It consists of two terms.","one is essentially the Fierz-Pauli operator, and the other is new.","When the background is flat, the Kerr-Schild family is recovered.","Novel examples for more general backgrounds are discussed."],"url":"http://arxiv.org/abs/2405.03289v1","category":"gr-qc"}
{"created":"2024-05-06 09:05:13","title":"Fundamental Bounds on Unequal Error Protection Codes","abstract":"Unequal error protection (UEP) codes can facilitate the transmission of messages with different protection levels. In this paper, we study the achievability bounds on UEP by the generalization of Gilbert-Varshamov (GV) bound. For the first time, we show that under certain conditions, UEP enhances the code rate comparing with time-sharing (TS) strategies asymptotically.","sentences":["Unequal error protection (UEP) codes can facilitate the transmission of messages with different protection levels.","In this paper, we study the achievability bounds on UEP by the generalization of Gilbert-Varshamov (GV) bound.","For the first time, we show that under certain conditions, UEP enhances the code rate comparing with time-sharing (TS) strategies asymptotically."],"url":"http://arxiv.org/abs/2405.03288v1","category":"cs.IT"}
{"created":"2024-05-06 09:02:13","title":"Gravitational lensing of a Schwarzschild-like black hole in Kalb-Ramond gravity","abstract":"In this paper, we investigate the gravitational lensing effect for the Schwarzschild-like black hole spacetime in the background of a Kalb-Ramond (KR) field proposed in [K.~Yang et. al., Phys. Rev. D 108 (2023) 124004]. The solution is characterized by a single extra parameter $l$, which is associated to the Lorentz symmetry breaking induced by the KR field. First, we calculate the exact deflection angle of massive and massless particles for finite distances using elliptic integrals. Then we study this effect in the weak and strong field regimes, discussing the correction of the KR parameter on the coefficients of the expansions in both limits. We also find that increasing $l$ decreases the deflection angle. Furthermore, we use the available data from the Sagittarius $A^{\\star}$ object, which is believed to be a supermassive black hole at the center of our galaxy, to calculate relevant observables, such as, the image position, luminosity, and delay time. The values found could be potentially measured in the weak field regime, though for strong fields one would have to wait for the next generation of interferometers.","sentences":["In this paper, we investigate the gravitational lensing effect for the Schwarzschild-like black hole spacetime in the background of a Kalb-Ramond (KR) field proposed in [K.~Yang et. al., Phys.","Rev. D 108 (2023) 124004].","The solution is characterized by a single extra parameter $l$, which is associated to the Lorentz symmetry breaking induced by the KR field.","First, we calculate the exact deflection angle of massive and massless particles for finite distances using elliptic integrals.","Then we study this effect in the weak and strong field regimes, discussing the correction of the KR parameter on the coefficients of the expansions in both limits.","We also find that increasing $l$ decreases the deflection angle.","Furthermore, we use the available data from the Sagittarius $A^{\\star}$ object, which is believed to be a supermassive black hole at the center of our galaxy, to calculate relevant observables, such as, the image position, luminosity, and delay time.","The values found could be potentially measured in the weak field regime, though for strong fields one would have to wait for the next generation of interferometers."],"url":"http://arxiv.org/abs/2405.03284v1","category":"gr-qc"}
{"created":"2024-05-06 09:01:10","title":"H\u00f6lder Continuity and Harnack estimate for non-homogeneous parabolic equations","abstract":"In this paper we continue the study on intrinsic Harnack inequality for non- homogeneous parabolic equations in non-divergence form initiated by the first author in [1]. We establish a forward-in-time intrinsic Harnack inequality, which in particular implies the H\\\"older continuity of the solutions. We also provide a Harnack type estimate on global scale which quantifies the strong minimum principle. In the time-independent setting, this together with [1] provides an alternative proof of the generalized Harnack inequality proven by the second author in [9].","sentences":["In this paper we continue the study on intrinsic Harnack inequality for non- homogeneous parabolic equations in non-divergence form initiated by the first author in [1].","We establish a forward-in-time intrinsic Harnack inequality, which in particular implies the H\\\"older continuity of the solutions.","We also provide a Harnack type estimate on global scale which quantifies the strong minimum principle.","In the time-independent setting, this together with [1] provides an alternative proof of the generalized Harnack inequality proven by the second author in [9]."],"url":"http://arxiv.org/abs/2405.03283v1","category":"math.AP"}
{"created":"2024-05-06 08:57:55","title":"FDSPC: Fast and Direct Smooth Path Planning via Continuous Curvature Integration","abstract":"In recent decades, global path planning of robot has seen significant advancements. Both heuristic search-based methods and probability sampling-based methods have shown capabilities to find feasible solutions in complex scenarios. However, mainstream global path planning algorithms often produce paths with bends, requiring additional smoothing post-processing. In this work, we propose a fast and direct path planning method based on continuous curvature integration. This method ensures path feasibility while directly generating global smooth paths with constant velocity, thus eliminating the need for post-path-smoothing. Furthermore, we compare the proposed method with existing approaches in terms of solution time, path length, memory usage, and smoothness under multiple scenarios. The proposed method is vastly superior to the average performance of state-of-the-art (SOTA) methods, especially in terms of the self-defined $\\mathcal{S}_2 $ smoothness (mean angle of steering). These results demonstrate the effectiveness and superiority of our approach in several representative environments.","sentences":["In recent decades, global path planning of robot has seen significant advancements.","Both heuristic search-based methods and probability sampling-based methods have shown capabilities to find feasible solutions in complex scenarios.","However, mainstream global path planning algorithms often produce paths with bends, requiring additional smoothing post-processing.","In this work, we propose a fast and direct path planning method based on continuous curvature integration.","This method ensures path feasibility while directly generating global smooth paths with constant velocity, thus eliminating the need for post-path-smoothing.","Furthermore, we compare the proposed method with existing approaches in terms of solution time, path length, memory usage, and smoothness under multiple scenarios.","The proposed method is vastly superior to the average performance of state-of-the-art (SOTA) methods, especially in terms of the self-defined $\\mathcal{S}_2 $ smoothness (mean angle of steering).","These results demonstrate the effectiveness and superiority of our approach in several representative environments."],"url":"http://arxiv.org/abs/2405.03281v1","category":"cs.RO"}
{"created":"2024-05-06 08:56:41","title":"Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity","abstract":"Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance. The difficulty stems from two primary issues: (1) vision-processing mechanisms in the brain are highly intricate and not fully revealed, making it challenging to directly learn a mapping between fMRI and video; (2) the temporal resolution of fMRI is significantly lower than that of natural videos. To overcome these issues, this paper propose a two-stage model named Mind-Animator, which achieves state-of-the-art performance on three public datasets. Specifically, during the fMRI-to-feature stage, we decouple semantic, structural, and motion features from fMRI through fMRI-vision-language tri-modal contrastive learning and sparse causal attention. In the feature-to-video stage, these features are merged to videos by an inflated Stable Diffusion. We substantiate that the reconstructed video dynamics are indeed derived from fMRI, rather than hallucinations of the generative model, through permutation tests. Additionally, the visualization of voxel-wise and ROI-wise importance maps confirms the neurobiological interpretability of our model.","sentences":["Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance.","The difficulty stems from two primary issues: (1) vision-processing mechanisms in the brain are highly intricate and not fully revealed, making it challenging to directly learn a mapping between fMRI and video; (2) the temporal resolution of fMRI is significantly lower than that of natural videos.","To overcome these issues, this paper propose a two-stage model named Mind-Animator, which achieves state-of-the-art performance on three public datasets.","Specifically, during the fMRI-to-feature stage, we decouple semantic, structural, and motion features from fMRI through fMRI-vision-language tri-modal contrastive learning and sparse causal attention.","In the feature-to-video stage, these features are merged to videos by an inflated Stable Diffusion.","We substantiate that the reconstructed video dynamics are indeed derived from fMRI, rather than hallucinations of the generative model, through permutation tests.","Additionally, the visualization of voxel-wise and ROI-wise importance maps confirms the neurobiological interpretability of our model."],"url":"http://arxiv.org/abs/2405.03280v1","category":"cs.CV"}
{"created":"2024-05-06 08:52:11","title":"Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning","abstract":"Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.","sentences":["Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining.","Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs.","Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance.","Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model.","In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning.","RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge.","It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge.","Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality.","In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance.","RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed."],"url":"http://arxiv.org/abs/2405.03279v1","category":"cs.CL"}
{"created":"2024-05-06 08:46:59","title":"Difference ascent sequences and related combinatorial structures","abstract":"Ascent sequences were introduced by Bousquet-M\\'elou, Claesson, Dukes and Kitaev, which are in bijection with unlabeled $(2+2)$-free posets, Fishburn matrices, permutations avoiding a bivincular pattern of length $3$, and Stoimenow matchings. Analogous results for weak ascent sequences have been obtained by B\\'enyi, Claesson and Dukes. Recently, Dukes and Sagan introduced a more general class of sequences which are called $d$-ascent sequences. They showed that some maps from the weak case can be extended to bijections for general $d$ while the extensions of others continue to be injective. The main objective of this paper is to restore these injections to bijections. To be specific, we introduce a class of permutations which we call them $d$-permutations and a class of factorial posets which we call them $d$-posets, both of which are showed to be in bijection with $d$-ascent sequences. Moreover, we also give a direct bijection between a class of matrices with a certain column restriction and Fishburn matrices. Our results give answers to several questions posed by Dukes and Sagan.","sentences":["Ascent sequences were introduced by Bousquet-M\\'elou, Claesson, Dukes and Kitaev, which are in bijection with unlabeled $(2+2)$-free posets, Fishburn matrices, permutations avoiding a bivincular pattern of length $3$, and Stoimenow matchings.","Analogous results for weak ascent sequences have been obtained by B\\'enyi, Claesson and Dukes.","Recently, Dukes and Sagan introduced a more general class of sequences which are called $d$-ascent sequences.","They showed that some maps from the weak case can be extended to bijections for general $d$ while the extensions of others continue to be injective.","The main objective of this paper is to restore these injections to bijections.","To be specific, we introduce a class of permutations which we call them $d$-permutations and a class of factorial posets which we call them $d$-posets, both of which are showed to be in bijection with $d$-ascent sequences.","Moreover, we also give a direct bijection between a class of matrices with a certain column restriction and Fishburn matrices.","Our results give answers to several questions posed by Dukes and Sagan."],"url":"http://arxiv.org/abs/2405.03275v1","category":"math.CO"}
{"created":"2024-05-06 08:41:08","title":"Understanding the effects of spacecraft trajectories through solar coronal mass ejection flux ropes using 3DCOREweb","abstract":"This study investigates the impact of spacecraft positioning and trajectory on in situ signatures of coronal mass ejections (CMEs). Employing the 3DCORE model, a 3D flux rope model that can generate in situ profiles for any given point in space and time, we conduct forward modeling to analyze such signatures for various latitudinal and longitudinal positions, with respect to the flux rope apex, at 0.8au. Using this approach, we explore the appearance of the resulting in situ profiles for different flux rope types, with different handedness and inclination angles, for both high and low twist CMEs. Our findings reveal that even high twist CMEs exhibit distinct differences in signatures between apex hits and flank encounters, with the latter displaying stretched-out profiles with reduced rotation. Notably, constant, non-rotating in situ signatures are only observed for flank encounters of low twist CMEs, suggesting implications for the magnetic field structure within CME legs. Additionally, our study confirms the unambiguous appearance of different flux rope types in in situ signatures, contributing to the broader understanding and interpretation of observational data. Given the model assumptions, this may refute trajectory effects to be the cause for mismatching flux rope types as identified in solar signatures. While acknowledging limitations inherent in our model, such as the assumption of constant twist and non-deformable torus-like shape, we still draw relevant conclusions within the context of global magnetic field structures of CMEs and the potential for distinguishing flux rope types based on in situ observations.","sentences":["This study investigates the impact of spacecraft positioning and trajectory on in situ signatures of coronal mass ejections (CMEs).","Employing the 3DCORE model, a 3D flux rope model that can generate in situ profiles for any given point in space and time, we conduct forward modeling to analyze such signatures for various latitudinal and longitudinal positions, with respect to the flux rope apex, at 0.8au.","Using this approach, we explore the appearance of the resulting in situ profiles for different flux rope types, with different handedness and inclination angles, for both high and low twist CMEs.","Our findings reveal that even high twist CMEs exhibit distinct differences in signatures between apex hits and flank encounters, with the latter displaying stretched-out profiles with reduced rotation.","Notably, constant, non-rotating in situ signatures are only observed for flank encounters of low twist CMEs, suggesting implications for the magnetic field structure within CME legs.","Additionally, our study confirms the unambiguous appearance of different flux rope types in in situ signatures, contributing to the broader understanding and interpretation of observational data.","Given the model assumptions, this may refute trajectory effects to be the cause for mismatching flux rope types as identified in solar signatures.","While acknowledging limitations inherent in our model, such as the assumption of constant twist and non-deformable torus-like shape, we still draw relevant conclusions within the context of global magnetic field structures of CMEs and the potential for distinguishing flux rope types based on in situ observations."],"url":"http://arxiv.org/abs/2405.03271v1","category":"astro-ph.SR"}
{"created":"2024-05-06 08:40:47","title":"Matroid-reachability-based decomposition into arborescences","abstract":"The problem of matroid-reachability-based packing of arborescences was solved by Kir\\'aly. Here we solve the corresponding decomposition problem that turns out to be more complicated. The result is obtained from the solution of the more general problem of matroid-reachability-based $(\\ell,\\ell')$-limited packing of arborescences where we are given a lower bound $\\ell$ and an upper bound $\\ell'$ on the total number of arborescences in the packing. The problem is considered for branchings and in directed hypergraphs as well.","sentences":["The problem of matroid-reachability-based packing of arborescences was solved by Kir\\'aly.","Here we solve the corresponding decomposition problem that turns out to be more complicated.","The result is obtained from the solution of the more general problem of matroid-reachability-based $(\\ell,\\ell')$-limited packing of arborescences where we are given a lower bound $\\ell$ and an upper bound $\\ell'$ on the total number of arborescences in the packing.","The problem is considered for branchings and in directed hypergraphs as well."],"url":"http://arxiv.org/abs/2405.03270v1","category":"math.CO"}
{"created":"2024-05-06 08:39:47","title":"Morse properties in convex projective geometry","abstract":"We study properties of \"hyperbolic directions\" in groups acting cocompactly on properly convex domains in real projective space, from three different perspectives simultaneously: the (coarse) metric geometry of the Hilbert metric, the projective geometry of the boundary of the domain, and the singular value gaps of projective automorphisms. We describe the relationship between different definitions of \"Morse\" and \"regular\" quasi-geodesics arising in these three different contexts. This generalizes several results of Benoist and Guichard to the non-Gromov hyperbolic setting.","sentences":["We study properties of \"hyperbolic directions\" in groups acting cocompactly on properly convex domains in real projective space, from three different perspectives simultaneously: the (coarse) metric geometry of the Hilbert metric, the projective geometry of the boundary of the domain, and the singular value gaps of projective automorphisms.","We describe the relationship between different definitions of \"Morse\" and \"regular\" quasi-geodesics arising in these three different contexts.","This generalizes several results of Benoist and Guichard to the non-Gromov hyperbolic setting."],"url":"http://arxiv.org/abs/2405.03269v1","category":"math.GT"}
{"created":"2024-05-06 08:39:20","title":"On the enumeration of permutations avoiding chains of patterns","abstract":"In 2019, B\\'ona and Smith introduced the notion of strong pattern avoidance, saying that a permutation $\\pi$ strongly avoids a pattern $\\sigma$ if $\\pi$ and $\\pi^2$ both avoid $\\sigma$. Recently, Archer and Geary generalized the idea of strong pattern avoidance to chain avoidance, in which a permutation $\\pi$ avoids a chain of patterns $(\\tau^{(1)}:\\tau^{(2)}:\\cdots:\\tau^{(k)})$ if the $i$-th power of the permutation avoids the pattern $\\tau^{(i)}$ for $1\\leq i\\leq k$. In this paper, we give explicit formulae for the number of sets of permutations avoiding certain chains of patterns. Our results give affirmative answers to two conjectures proposed by Archer and Geary.","sentences":["In 2019, B\\'ona and Smith introduced the notion of strong pattern avoidance, saying that a permutation $\\pi$ strongly avoids a pattern $\\sigma$ if $\\pi$ and $\\pi^2$ both avoid $\\sigma$. Recently, Archer and Geary generalized the idea of strong pattern avoidance to chain avoidance, in which a permutation $\\pi$ avoids a chain of patterns $(\\tau^{(1)}:\\tau^{(2)}:\\cdots:\\tau^{(k)})$ if the $i$-th power of the permutation avoids the pattern $\\tau^{(i)}$ for $1\\leq i\\leq k$.","In this paper, we give explicit formulae for the number of sets of permutations avoiding certain chains of patterns.","Our results give affirmative answers to two conjectures proposed by Archer and Geary."],"url":"http://arxiv.org/abs/2405.03268v1","category":"math.CO"}
{"created":"2024-05-06 08:36:21","title":"Delooping generated groups in homotopy type theory","abstract":"Homotopy type theory is a logical setting based on Martin-L\\\"of type theory in which one can perform geometric constructions and proofs in a synthetic way. Namely, types can be interpreted as spaces (up to continuous deformation) and proofs as homotopy invariant constructions. In this context, the loop spaces of types with a distinguished element (more precisely, pointed connected groupoids), provide a natural representation of groups, what we call here internal groups. The construction which internalizes a given group is called delooping, because it is a formal inverse to the loop space operator. As we recall in the article, this delooping operation has a concrete definition for any group G given by the type of G-torsors. Those are particular sets together with an action of G, which means that they come equipped with an endomorphism for every element of G. We show that, when a generating set is known for the group, we can construct a smaller representation of the type of G-torsors, using the fact that we only need automorphisms for the elements of the generating set. We thus obtain a concise definition of (internal) groups in homotopy type theory, which can be useful to define deloopings without resorting to higher inductive types, or to perform computations on those. We also investigate an abstract construction for the Cayley group of a generated group. Most of the developments performed in the article have been formalized using the cubical version of the Agda proof assistant.","sentences":["Homotopy type theory is a logical setting based on Martin-L\\\"of type theory in which one can perform geometric constructions and proofs in a synthetic way.","Namely, types can be interpreted as spaces (up to continuous deformation) and proofs as homotopy invariant constructions.","In this context, the loop spaces of types with a distinguished element (more precisely, pointed connected groupoids), provide a natural representation of groups, what we call here internal groups.","The construction which internalizes a given group is called delooping, because it is a formal inverse to the loop space operator.","As we recall in the article, this delooping operation has a concrete definition for any group G given by the type of G-torsors.","Those are particular sets together with an action of G, which means that they come equipped with an endomorphism for every element of G. We show that, when a generating set is known for the group, we can construct a smaller representation of the type of G-torsors, using the fact that we only need automorphisms for the elements of the generating set.","We thus obtain a concise definition of (internal) groups in homotopy type theory, which can be useful to define deloopings without resorting to higher inductive types, or to perform computations on those.","We also investigate an abstract construction for the Cayley group of a generated group.","Most of the developments performed in the article have been formalized using the cubical version of the Agda proof assistant."],"url":"http://arxiv.org/abs/2405.03264v1","category":"cs.LO"}
{"created":"2024-05-06 08:35:09","title":"Generalized thermodynamic relations for perfect spin hydrodynamics","abstract":"Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics. They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations. The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections.","sentences":["Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics.","They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations.","The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections."],"url":"http://arxiv.org/abs/2405.03263v1","category":"hep-ph"}
{"created":"2024-05-06 08:34:15","title":"End-to-End Reinforcement Learning of Curative Curtailment with Partial Measurement Availability","abstract":"In the course of the energy transition, the expansion of generation and consumption will change, and many of these technologies, such as PV systems, electric cars and heat pumps, will influence the power flow, especially in the distribution grids. Scalable methods that can make decisions for each grid connection are needed to enable congestion-free grid operation in the distribution grids. This paper presents a novel end-to-end approach to resolving congestion in distribution grids with deep reinforcement learning. Our architecture learns to curtail power and set appropriate reactive power to determine a non-congested and, thus, feasible grid state. State-of-the-art methods such as the optimal power flow (OPF) demand high computational costs and detailed measurements of every bus in a grid. In contrast, the presented method enables decisions under sparse information with just some buses observable in the grid. Distribution grids are generally not yet fully digitized and observable, so this method can be used for decision-making on the majority of low-voltage grids. On a real low-voltage grid the approach resolves 100\\% of violations in the voltage band and 98.8\\% of asset overloads. The results show that decisions can also be made on real grids that guarantee sufficient quality for congestion-free grid operation.","sentences":["In the course of the energy transition, the expansion of generation and consumption will change, and many of these technologies, such as PV systems, electric cars and heat pumps, will influence the power flow, especially in the distribution grids.","Scalable methods that can make decisions for each grid connection are needed to enable congestion-free grid operation in the distribution grids.","This paper presents a novel end-to-end approach to resolving congestion in distribution grids with deep reinforcement learning.","Our architecture learns to curtail power and set appropriate reactive power to determine a non-congested and, thus, feasible grid state.","State-of-the-art methods such as the optimal power flow (OPF) demand high computational costs and detailed measurements of every bus in a grid.","In contrast, the presented method enables decisions under sparse information with just some buses observable in the grid.","Distribution grids are generally not yet fully digitized and observable, so this method can be used for decision-making on the majority of low-voltage grids.","On a real low-voltage grid the approach resolves 100\\% of violations in the voltage band and 98.8\\% of asset overloads.","The results show that decisions can also be made on real grids that guarantee sufficient quality for congestion-free grid operation."],"url":"http://arxiv.org/abs/2405.03262v1","category":"cs.LG"}
{"created":"2024-05-06 08:32:39","title":"The Ising Model Coupled to 2D Gravity: Higher-order Painlev\u00e9 Equations/The $(3,4)$ String Equation","abstract":"In continuation of the work [1], we study a higher-order Painlev\\'{e}-type equation, arising as a string equation of the $3^{rd}$ order reduction of the KP hierarchy. This equation appears at the multi-critical point of the $2$-matrix model with quartic interactions, and describes the Ising phase transition coupled to 2D gravity. We characterize this equation in terms of the isomonodromic deformations of a particular rational connection on $\\mathbb{P}^{1}$. We also identify the (nonautonomous) Hamiltonian structure associated to this equation, and write a suitable $\\tau$-differential for this system. This $\\tau$-differential can be extended to the canonical coordinates of the associated Hamiltonian system, allowing us to verify Conjectures 1. and 2. of [2] in our case. We also present a fairly general formula for the $\\tau$-differential of a special class of resonant connections, which is somewhat simpler than that of [3].   [1] M. Duits, N. Hayford, and S.-Y. Lee. \"The Ising Model Coupled to 2D Gravity: Genus Zero Partition Function\". arXiv preprint, 2023.   [2] A.R. Its and A. Prokhorov. \"On some Hamiltonian properties of the isomonodromic tau functions\". Rev. Math. Phys. 30.7 (2018).   [3] M. Bertola and M.Y. Mo. \"Isomonodromic deformation of resonant rational connections\". Int. Math. Res. Pap. 11 (2005).","sentences":["In continuation of the work [1], we study a higher-order Painlev\\'{e}-type equation, arising as a string equation of the $3^{rd}$ order reduction of the KP hierarchy.","This equation appears at the multi-critical point of the $2$-matrix model with quartic interactions, and describes the Ising phase transition coupled to 2D gravity.","We characterize this equation in terms of the isomonodromic deformations of a particular rational connection on $\\mathbb{P}^{1}$. We also identify the (nonautonomous) Hamiltonian structure associated to this equation, and write a suitable $\\tau$-differential for this system.","This $\\tau$-differential can be extended to the canonical coordinates of the associated Hamiltonian system, allowing us to verify Conjectures 1. and 2. of [2] in our case.","We also present a fairly general formula for the $\\tau$-differential of a special class of resonant connections, which is somewhat simpler than that of [3].   ","[1] M. Duits, N. Hayford, and S.-Y. Lee.","\"The Ising Model Coupled to 2D Gravity: Genus Zero Partition Function\".","arXiv preprint, 2023.   ","[2] A.R.","Its and A. Prokhorov.","\"On some Hamiltonian properties of the isomonodromic tau functions\".","Rev. Math.","Phys. 30.7 (2018).   ","[3] M. Bertola and M.Y. Mo.","\"Isomonodromic deformation of resonant rational connections\".","Int.","Math.","Res. Pap. 11 (2005)."],"url":"http://arxiv.org/abs/2405.03260v1","category":"math-ph"}
{"created":"2024-05-06 08:31:22","title":"The Ising Model Coupled to 2D Gravity: Genus Zero Partition Function","abstract":"We compute the genus $0$ free energy for the $2$-matrix model with quartic interactions, which acts as a generating function for the Ising model's partition function on a random, $4$-regular, planar graph. This rigorously confirms the predictions of V.A. Kazakov and D.V. Boulatov on this model, and provides a new parametric formula for the free energy. We also give a characterization of the phase space of the model. Our analysis is based on a steepest descent Riemann-Hilbert analysis of the associated biorthogonal polynomials and the corresponding isomonodromic $\\tau$-function. A key ingredient in the analysis is a parametrization of the spectral curve.","sentences":["We compute the genus $0$ free energy for the $2$-matrix model with quartic interactions, which acts as a generating function for the Ising model's partition function on a random, $4$-regular, planar graph.","This rigorously confirms the predictions of V.A. Kazakov and D.V. Boulatov on this model, and provides a new parametric formula for the free energy.","We also give a characterization of the phase space of the model.","Our analysis is based on a steepest descent Riemann-Hilbert analysis of the associated biorthogonal polynomials and the corresponding isomonodromic $\\tau$-function.","A key ingredient in the analysis is a parametrization of the spectral curve."],"url":"http://arxiv.org/abs/2405.03259v1","category":"math-ph"}
{"created":"2024-05-06 17:57:58","title":"JWST Observations of Starbursts: Cold Clouds and Plumes Launching in the M82 Outflow","abstract":"In this paper we study the filamentary substructure of 3.3 $\\mu$m PAH emission from JWST/NIRCam observations in the base of the M82 star-burst driven wind. We identify plume-like substructure within the PAH emission with widths of $\\sim$50 pc. Several of the plumes extend to the edge of the field-of-view, and thus are at least 200-300 pc in length. In this region of the outflow, the vast majority ($\\sim$70\\%) of PAH emission is associated with the plumes. We show that those structures contain smaller scale \"clouds\" with widths that are $\\sim$5-15 pc, and they are morphologically similar to the results of \"cloud-crushing\" simulations. We estimate the cloud-crushing time-scales of $\\sim$0.5-3 Myr, depending on assumptions. We show this time scale is consistent with a picture in which these observed PAH clouds survived break-out from the disk rather than being destroyed by the hot wind. The PAH emission in both the midplane and the outflow is shown to tightly correlate with that of Pa$\\alpha$ emission (from HST/NICMOS data), at the scale of both plumes and clouds, though the ratio of PAH-to-Pa$\\alpha$ increases at further distances from the midplane. Finally, we show that the outflow PAH emission is suppressed in regions of the M82 wind that are bright in X-ray emission. Overall, our results are broadly consistent with a picture in which cold gas in galactic outflows is launched via hierarchically structured plumes, and those small scale clouds are more likely to survive the wind environment when collected into the larger plume structure.","sentences":["In this paper we study the filamentary substructure of 3.3 $\\mu$m PAH emission from JWST/NIRCam observations in the base of the M82 star-burst driven wind.","We identify plume-like substructure within the PAH emission with widths of $\\sim$50 pc.","Several of the plumes extend to the edge of the field-of-view, and thus are at least 200-300 pc in length.","In this region of the outflow, the vast majority ($\\sim$70\\%) of PAH emission is associated with the plumes.","We show that those structures contain smaller scale \"clouds\" with widths that are $\\sim$5-15 pc, and they are morphologically similar to the results of \"cloud-crushing\" simulations.","We estimate the cloud-crushing time-scales of $\\sim$0.5-3 Myr, depending on assumptions.","We show this time scale is consistent with a picture in which these observed PAH clouds survived break-out from the disk rather than being destroyed by the hot wind.","The PAH emission in both the midplane and the outflow is shown to tightly correlate with that of Pa$\\alpha$ emission (from HST/NICMOS data), at the scale of both plumes and clouds, though the ratio of PAH-to-Pa$\\alpha$ increases at further distances from the midplane.","Finally, we show that the outflow PAH emission is suppressed in regions of the M82 wind that are bright in X-ray emission.","Overall, our results are broadly consistent with a picture in which cold gas in galactic outflows is launched via hierarchically structured plumes, and those small scale clouds are more likely to survive the wind environment when collected into the larger plume structure."],"url":"http://arxiv.org/abs/2405.03686v1","category":"astro-ph.GA"}
{"created":"2024-05-06 17:57:06","title":"All-in-One Deep Learning Framework for MR Image Reconstruction","abstract":"We introduce a novel, all-in-one deep learning framework for MR image reconstruction, enabling a single model to enhance image quality across multiple aspects of k-space sampling and to be effective across a wide range of clinical and technical scenarios. This DICOM-based algorithm serves as the core of SwiftMR (AIRS Medical, Seoul, Korea), which is FDA-cleared, CE-certified, and commercially available. We first detail the comprehensive development process of the model, including data collection, training pair preparation, model architecture design, and DICOM inference. We then assess the model's capability to enhance image quality in a multi-dimensional manner, specifically across various aspects of k-space sampling. Subsequently, we evaluate several features of the multi-dimensional enhancement: the accuracy of tunable denoising, the effectiveness of super-resolution in each encoding direction, and the reduction of artifacts that become more prominent at lower spatial resolutions. Additionally, we assess its compatibility with various scan parameter sets and its generalizability across scanner vendors not seen during training. Finally, we present specific cases demonstrating the model's utility in reducing scan time across anatomical regions in conjunction with protocol optimization. The proposed model is compatible with a broad spectrum of scenarios, including various vendors, pulse sequences, scan parameters, and anatomical regions. Its DICOM-based operation particularly enhances its applicability for real-world applications. Given its demonstrated effectiveness and versatility, we expect its use to expand in the field of clinical MRI.","sentences":["We introduce a novel, all-in-one deep learning framework for MR image reconstruction, enabling a single model to enhance image quality across multiple aspects of k-space sampling and to be effective across a wide range of clinical and technical scenarios.","This DICOM-based algorithm serves as the core of SwiftMR (AIRS Medical, Seoul, Korea), which is FDA-cleared, CE-certified, and commercially available.","We first detail the comprehensive development process of the model, including data collection, training pair preparation, model architecture design, and DICOM inference.","We then assess the model's capability to enhance image quality in a multi-dimensional manner, specifically across various aspects of k-space sampling.","Subsequently, we evaluate several features of the multi-dimensional enhancement: the accuracy of tunable denoising, the effectiveness of super-resolution in each encoding direction, and the reduction of artifacts that become more prominent at lower spatial resolutions.","Additionally, we assess its compatibility with various scan parameter sets and its generalizability across scanner vendors not seen during training.","Finally, we present specific cases demonstrating the model's utility in reducing scan time across anatomical regions in conjunction with protocol optimization.","The proposed model is compatible with a broad spectrum of scenarios, including various vendors, pulse sequences, scan parameters, and anatomical regions.","Its DICOM-based operation particularly enhances its applicability for real-world applications.","Given its demonstrated effectiveness and versatility, we expect its use to expand in the field of clinical MRI."],"url":"http://arxiv.org/abs/2405.03684v1","category":"eess.IV"}
{"created":"2024-05-06 17:39:53","title":"Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation","abstract":"We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence. Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly. Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem. Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization. To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity. The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired.","sentences":["We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence.","Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly.","Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem.","Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization.","To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity.","The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired."],"url":"http://arxiv.org/abs/2405.03662v1","category":"cs.CV"}
{"created":"2024-05-06 15:59:44","title":"Non-detectable patterns hidden within sequences of bits","abstract":"In this paper we construct families of bit sequences using combinatorial methods. Each sequence is derived by con- verting a collection of numbers encoding certain combinatorial nu- merics from objects exhibiting symmetry in various dimensions. Using the algorithms first described in [1] we show that the NIST testing suite described in publication 800-22 does not detect these symmetries hidden within these sequences.","sentences":["In this paper we construct families of bit sequences using combinatorial methods.","Each sequence is derived by con- verting a collection of numbers encoding certain combinatorial nu- merics from objects exhibiting symmetry in various dimensions.","Using the algorithms first described in [1] we show that the NIST testing suite described in publication 800-22 does not detect these symmetries hidden within these sequences."],"url":"http://arxiv.org/abs/2405.03587v1","category":"math.CO"}
{"created":"2024-05-06 13:51:02","title":"DexSkills: Skill Segmentation Using Haptic Data for Learning Autonomous Long-Horizon Robotic Manipulation Tasks","abstract":"Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations have shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks.","sentences":["Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems.","While learning from human demonstrations have shown encouraging results, they require extensive data collection for training.","Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach.","To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills.","DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly.","Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data.","Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks."],"url":"http://arxiv.org/abs/2405.03476v1","category":"cs.RO"}
{"created":"2024-05-06 09:05:06","title":"Evaluating Eye Movement Biometrics in Virtual Reality: A Comparative Analysis of VR Headset and High-End Eye-Tracker Collected Dataset","abstract":"Previous studies have shown that eye movement data recorded at 1000 Hz can be used to authenticate individuals. This study explores the effectiveness of eye movement-based biometrics (EMB) by utilizing data from an eye-tracking (ET)-enabled virtual reality (VR) headset (GazeBaseVR) and compares it to the performance using data from a high-end eye tracker (GazeBase) that has been downsampled to 250 Hz. The research also aims to assess the biometric potential of both binocular and monocular eye movement data. GazeBaseVR dataset achieves an equal error rate (EER) of 1.67% and a false rejection rate (FRR) at 10^-4 false acceptance rate (FAR) of 22.73% in a binocular configuration. This study underscores the biometric viability of data obtained from eye-tracking-enabled VR headset.","sentences":["Previous studies have shown that eye movement data recorded at 1000 Hz can be used to authenticate individuals.","This study explores the effectiveness of eye movement-based biometrics (EMB) by utilizing data from an eye-tracking (ET)-enabled virtual reality (VR) headset (GazeBaseVR) and compares it to the performance using data from a high-end eye tracker (GazeBase) that has been downsampled to 250 Hz.","The research also aims to assess the biometric potential of both binocular and monocular eye movement data.","GazeBaseVR dataset achieves an equal error rate (EER) of 1.67% and a false rejection rate (FRR) at 10^-4 false acceptance rate (FAR) of 22.73% in a binocular configuration.","This study underscores the biometric viability of data obtained from eye-tracking-enabled VR headset."],"url":"http://arxiv.org/abs/2405.03287v1","category":"cs.HC"}
{"created":"2024-05-06 08:15:29","title":"Exploring the Frontiers of Softmax: Provable Optimization, Applications in Diffusion Model, and Beyond","abstract":"The softmax activation function plays a crucial role in the success of large language models (LLMs), particularly in the self-attention mechanism of the widely adopted Transformer architecture. However, the underlying learning dynamics that contribute to the effectiveness of softmax remain largely unexplored. As a step towards better understanding, this paper provides a theoretical study of the optimization and generalization properties of two-layer softmax neural networks, providing theoretical insights into their superior performance as other activation functions, such as ReLU and exponential. Leveraging the Neural Tangent Kernel (NTK) framework, our analysis reveals that the normalization effect of the softmax function leads to a good perturbation property of the induced NTK matrix, resulting in a good convex region of the loss landscape. Consequently, softmax neural networks can learn the target function in the over-parametrization regime. To demonstrate the broad applicability of our theoretical findings, we apply them to the task of learning score estimation functions in diffusion models, a promising approach for generative modeling. Our analysis shows that gradient-based algorithms can learn the score function with a provable accuracy. Our work provides a deeper understanding of the effectiveness of softmax neural networks and their potential in various domains, paving the way for further advancements in natural language processing and beyond.","sentences":["The softmax activation function plays a crucial role in the success of large language models (LLMs), particularly in the self-attention mechanism of the widely adopted Transformer architecture.","However, the underlying learning dynamics that contribute to the effectiveness of softmax remain largely unexplored.","As a step towards better understanding, this paper provides a theoretical study of the optimization and generalization properties of two-layer softmax neural networks, providing theoretical insights into their superior performance as other activation functions, such as ReLU and exponential.","Leveraging the Neural Tangent Kernel (NTK) framework, our analysis reveals that the normalization effect of the softmax function leads to a good perturbation property of the induced NTK matrix, resulting in a good convex region of the loss landscape.","Consequently, softmax neural networks can learn the target function in the over-parametrization regime.","To demonstrate the broad applicability of our theoretical findings, we apply them to the task of learning score estimation functions in diffusion models, a promising approach for generative modeling.","Our analysis shows that gradient-based algorithms can learn the score function with a provable accuracy.","Our work provides a deeper understanding of the effectiveness of softmax neural networks and their potential in various domains, paving the way for further advancements in natural language processing and beyond."],"url":"http://arxiv.org/abs/2405.03251v1","category":"cs.LG"}
{"created":"2024-05-06 08:00:43","title":"Communication-Efficient Federated Learning with Adaptive Compression under Dynamic Bandwidth","abstract":"Federated learning can train models without directly providing local data to the server. However, the frequent updating of the local model brings the problem of large communication overhead. Recently, scholars have achieved the communication efficiency of federated learning mainly by model compression. But they ignore two problems: 1) network state of each client changes dynamically; 2) network state among clients is not the same. The clients with poor bandwidth update local model slowly, which leads to low efficiency. To address this challenge, we propose a communication-efficient federated learning algorithm with adaptive compression under dynamic bandwidth (called AdapComFL). Concretely, each client performs bandwidth awareness and bandwidth prediction. Then, each client adaptively compresses its local model via the improved sketch mechanism based on his predicted bandwidth. Further, the server aggregates sketched models with different sizes received. To verify the effectiveness of the proposed method, the experiments are based on real bandwidth data which are collected from the network topology we build, and benchmark datasets which are obtained from open repositories. We show the performance of AdapComFL algorithm, and compare it with existing algorithms. The experimental results show that our AdapComFL achieves more efficient communication as well as competitive accuracy compared to existing algorithms.","sentences":["Federated learning can train models without directly providing local data to the server.","However, the frequent updating of the local model brings the problem of large communication overhead.","Recently, scholars have achieved the communication efficiency of federated learning mainly by model compression.","But they ignore two problems: 1) network state of each client changes dynamically; 2) network state among clients is not the same.","The clients with poor bandwidth update local model slowly, which leads to low efficiency.","To address this challenge, we propose a communication-efficient federated learning algorithm with adaptive compression under dynamic bandwidth (called AdapComFL).","Concretely, each client performs bandwidth awareness and bandwidth prediction.","Then, each client adaptively compresses its local model via the improved sketch mechanism based on his predicted bandwidth.","Further, the server aggregates sketched models with different sizes received.","To verify the effectiveness of the proposed method, the experiments are based on real bandwidth data which are collected from the network topology we build, and benchmark datasets which are obtained from open repositories.","We show the performance of AdapComFL algorithm, and compare it with existing algorithms.","The experimental results show that our AdapComFL achieves more efficient communication as well as competitive accuracy compared to existing algorithms."],"url":"http://arxiv.org/abs/2405.03248v1","category":"cs.LG"}
{"created":"2024-05-06 07:51:13","title":"Mind the Gap Between Synthetic and Real: Utilizing Transfer Learning to Probe the Boundaries of Stable Diffusion Generated Data","abstract":"Generative foundation models like Stable Diffusion comprise a diverse spectrum of knowledge in computer vision with the potential for transfer learning, e.g., via generating data to train student models for downstream tasks. This could circumvent the necessity of collecting labeled real-world data, thereby presenting a form of data-free knowledge distillation. However, the resultant student models show a significant drop in accuracy compared to models trained on real data. We investigate possible causes for this drop and focus on the role of the different layers of the student model. By training these layers using either real or synthetic data, we reveal that the drop mainly stems from the model's final layers. Further, we briefly investigate other factors, such as differences in data-normalization between synthetic and real, the impact of data augmentations, texture vs.\\ shape learning, and assuming oracle prompts. While we find that some of those factors can have an impact, they are not sufficient to close the gap towards real data. Building upon our insights that mainly later layers are responsible for the drop, we investigate the data-efficiency of fine-tuning a synthetically trained model with real data applied to only those last layers. Our results suggest an improved trade-off between the amount of real training data used and the model's accuracy. Our findings contribute to the understanding of the gap between synthetic and real data and indicate solutions to mitigate the scarcity of labeled real data.","sentences":["Generative foundation models like Stable Diffusion comprise a diverse spectrum of knowledge in computer vision with the potential for transfer learning, e.g., via generating data to train student models for downstream tasks.","This could circumvent the necessity of collecting labeled real-world data, thereby presenting a form of data-free knowledge distillation.","However, the resultant student models show a significant drop in accuracy compared to models trained on real data.","We investigate possible causes for this drop and focus on the role of the different layers of the student model.","By training these layers using either real or synthetic data, we reveal that the drop mainly stems from the model's final layers.","Further, we briefly investigate other factors, such as differences in data-normalization between synthetic and real, the impact of data augmentations, texture vs.\\ shape learning, and assuming oracle prompts.","While we find that some of those factors can have an impact, they are not sufficient to close the gap towards real data.","Building upon our insights that mainly later layers are responsible for the drop, we investigate the data-efficiency of fine-tuning a synthetically trained model with real data applied to only those last layers.","Our results suggest an improved trade-off between the amount of real training data used and the model's accuracy.","Our findings contribute to the understanding of the gap between synthetic and real data and indicate solutions to mitigate the scarcity of labeled real data."],"url":"http://arxiv.org/abs/2405.03243v1","category":"cs.CV"}
{"created":"2024-05-06 07:48:34","title":"Deep Learning for Detecting and Early Predicting Chronic Obstructive Pulmonary Disease from Spirogram Time Series: A UK Biobank Study","abstract":"Chronic Obstructive Pulmonary Disease (COPD) is a chronic inflammatory lung condition that causes airflow obstruction. The existing methods can only detect patients who already have COPD based on obvious features shown in the spirogram (In this article, the spirogram specifically involves measuring Volume-Flow curve time series). Early prediction of COPD risk is vital for monitoring COPD disease progression, slowing it down, or even preventing its onset. However, these methods fail to early predict an individual's probability of COPD in the future based on subtle features in the spirogram. To address this gap, for the first time, we propose DeepSpiro, a method based on deep learning for early prediction of future COPD risk. DeepSpiro consists of four parts. First, we construct Volume-Flow curves guided by Time-Volume instability smoothing (SpiroSmoother) to enhance the stability of the original Volume-Flow curves precisely. Second, we extract critical features from the evolution of varied-length key patches (SpiroEncoder) to capture the key temporal evolution from original high-dimensional dynamic sequences to a unified low-dimensional temporal representation. Third, we explain the model based on temporal attention and heterogeneous feature fusion (SpiroExplainer), which integrates information from heterogeneous data such as spirogram and demographic information. Fourth, we predict the risk of COPD based on the evolution of key patch concavity (SpiroPredictor), enabling accurate prediction of the risk of disease in high-risk patients who are not yet diagnosed, for up to 1, 2, 3, 4, 5 years, and beyond. We conduct experiments on the UK Biobank dataset. Results show that DeepSpiro achieves an AUC value of 0.8328 in the task of detecting COPD. In early prediction tasks, high-risk and low-risk groups show significant differences in the future, with a p-value of <0.001.","sentences":["Chronic Obstructive Pulmonary Disease (COPD) is a chronic inflammatory lung condition that causes airflow obstruction.","The existing methods can only detect patients who already have COPD based on obvious features shown in the spirogram (In this article, the spirogram specifically involves measuring Volume-Flow curve time series).","Early prediction of COPD risk is vital for monitoring COPD disease progression, slowing it down, or even preventing its onset.","However, these methods fail to early predict an individual's probability of COPD in the future based on subtle features in the spirogram.","To address this gap, for the first time, we propose DeepSpiro, a method based on deep learning for early prediction of future COPD risk.","DeepSpiro consists of four parts.","First, we construct Volume-Flow curves guided by Time-Volume instability smoothing (SpiroSmoother) to enhance the stability of the original Volume-Flow curves precisely.","Second, we extract critical features from the evolution of varied-length key patches (SpiroEncoder) to capture the key temporal evolution from original high-dimensional dynamic sequences to a unified low-dimensional temporal representation.","Third, we explain the model based on temporal attention and heterogeneous feature fusion (SpiroExplainer), which integrates information from heterogeneous data such as spirogram and demographic information.","Fourth, we predict the risk of COPD based on the evolution of key patch concavity (SpiroPredictor), enabling accurate prediction of the risk of disease in high-risk patients who are not yet diagnosed, for up to 1, 2, 3, 4, 5 years, and beyond.","We conduct experiments on the UK Biobank dataset.","Results show that DeepSpiro achieves an AUC value of 0.8328 in the task of detecting COPD.","In early prediction tasks, high-risk and low-risk groups show significant differences in the future, with a p-value of <0.001."],"url":"http://arxiv.org/abs/2405.03239v1","category":"cs.LG"}
{"created":"2024-05-06 07:44:46","title":"Cross-Modal Domain Adaptation in Brain Disease Diagnosis: Maximum Mean Discrepancy-based Convolutional Neural Networks","abstract":"Brain disorders are a major challenge to global health, causing millions of deaths each year. Accurate diagnosis of these diseases relies heavily on advanced medical imaging techniques such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). However, the scarcity of annotated data poses a significant challenge in deploying machine learning models for medical diagnosis. To address this limitation, deep learning techniques have shown considerable promise. Domain adaptation techniques enhance a model's ability to generalize across imaging modalities by transferring knowledge from one domain (e.g., CT images) to another (e.g., MRI images). Such cross-modality adaptation is essential to improve the ability of models to consistently generalize across different imaging modalities. This study collected relevant resources from the Kaggle website and employed the Maximum Mean Difference (MMD) method - a popular domain adaptation method - to reduce the differences between imaging domains. By combining MMD with Convolutional Neural Networks (CNNs), the accuracy and utility of the model is obviously enhanced. The excellent experimental results highlight the great potential of data-driven domain adaptation techniques to improve diagnostic accuracy and efficiency, especially in resource-limited environments. By bridging the gap between different imaging modalities, the study aims to provide clinicians with more reliable diagnostic tools.","sentences":["Brain disorders are a major challenge to global health, causing millions of deaths each year.","Accurate diagnosis of these diseases relies heavily on advanced medical imaging techniques such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT).","However, the scarcity of annotated data poses a significant challenge in deploying machine learning models for medical diagnosis.","To address this limitation, deep learning techniques have shown considerable promise.","Domain adaptation techniques enhance a model's ability to generalize across imaging modalities by transferring knowledge from one domain (e.g., CT images) to another (e.g., MRI images).","Such cross-modality adaptation is essential to improve the ability of models to consistently generalize across different imaging modalities.","This study collected relevant resources from the Kaggle website and employed the Maximum Mean Difference (MMD) method - a popular domain adaptation method - to reduce the differences between imaging domains.","By combining MMD with Convolutional Neural Networks (CNNs), the accuracy and utility of the model is obviously enhanced.","The excellent experimental results highlight the great potential of data-driven domain adaptation techniques to improve diagnostic accuracy and efficiency, especially in resource-limited environments.","By bridging the gap between different imaging modalities, the study aims to provide clinicians with more reliable diagnostic tools."],"url":"http://arxiv.org/abs/2405.03235v1","category":"cs.CV"}
{"created":"2024-05-06 07:33:48","title":"Consistent response prediction for multilayer networks on unknown manifolds","abstract":"Our paper deals with a collection of networks on a common set of nodes, where some of the networks are associated with responses. Assuming that the networks correspond to points on a one-dimensional manifold in a higher dimensional ambient space, we propose an algorithm to consistently predict the response at an unlabeled network. Our model involves a specific multiple random network model, namely the common subspace independent edge model, where the networks share a common invariant subspace, and the heterogeneity amongst the networks is captured by a set of low dimensional matrices. Our algorithm estimates these low dimensional matrices that capture the heterogeneity of the networks, learns the underlying manifold by isomap, and consistently predicts the response at an unlabeled network. We provide theoretical justifications for the use of our algorithm, validated by numerical simulations. Finally, we demonstrate the use of our algorithm on larval Drosophila connectome data.","sentences":["Our paper deals with a collection of networks on a common set of nodes, where some of the networks are associated with responses.","Assuming that the networks correspond to points on a one-dimensional manifold in a higher dimensional ambient space, we propose an algorithm to consistently predict the response at an unlabeled network.","Our model involves a specific multiple random network model, namely the common subspace independent edge model, where the networks share a common invariant subspace, and the heterogeneity amongst the networks is captured by a set of low dimensional matrices.","Our algorithm estimates these low dimensional matrices that capture the heterogeneity of the networks, learns the underlying manifold by isomap, and consistently predicts the response at an unlabeled network.","We provide theoretical justifications for the use of our algorithm, validated by numerical simulations.","Finally, we demonstrate the use of our algorithm on larval Drosophila connectome data."],"url":"http://arxiv.org/abs/2405.03225v1","category":"stat.ME"}
{"created":"2024-05-06 07:33:10","title":"The Kansei Engineering Approach in Web Design:Case of Transportation Website","abstract":"Kansei Engineering (KE) is a user-centered design approach that emphasizes the emotional aspects of user experience. This paper explores the integration of KE in the case of a transportation company that focuses on connecting cargo owners with transportation providers. The methodology involves aligning the design process with the company's strategy, collecting and semantic scaling Kansei words, and evaluating website design through experimental and statistical analyses. Initially, we collaborated with the company to understand their strategic goals, using Use Case and Entity Relationship diagrams to learn about the website functionality. Subsequent steps involved collecting Kansei words that resonate with the company's vision. Website samples from comparable transportation companies were then evaluated by X subject in the survey. Participants were asked to arrange samples based on emotional feedback using a 5-point SD scale. We used Principal Component Analysis (PCA) to identify critical factors affecting users' perceptions of the design. Based on these results, we collaborated with designers to reformulate the website, ensuring the design features aligned with the Kansei principles. The outcome is a user-centric web design to enhance the site's user experience. This study shows that KE can be effective in creating more user-friendly web interfaces in the transportation industry.","sentences":["Kansei Engineering (KE) is a user-centered design approach that emphasizes the emotional aspects of user experience.","This paper explores the integration of KE in the case of a transportation company that focuses on connecting cargo owners with transportation providers.","The methodology involves aligning the design process with the company's strategy, collecting and semantic scaling Kansei words, and evaluating website design through experimental and statistical analyses.","Initially, we collaborated with the company to understand their strategic goals, using Use Case and Entity Relationship diagrams to learn about the website functionality.","Subsequent steps involved collecting Kansei words that resonate with the company's vision.","Website samples from comparable transportation companies were then evaluated by X subject in the survey.","Participants were asked to arrange samples based on emotional feedback using a 5-point SD scale.","We used Principal Component Analysis (PCA) to identify critical factors affecting users' perceptions of the design.","Based on these results, we collaborated with designers to reformulate the website, ensuring the design features aligned with the Kansei principles.","The outcome is a user-centric web design to enhance the site's user experience.","This study shows that KE can be effective in creating more user-friendly web interfaces in the transportation industry."],"url":"http://arxiv.org/abs/2405.03223v1","category":"cs.SE"}
{"created":"2024-05-06 07:12:22","title":"Vietnamese AI Generated Text Detection","abstract":"In recent years, Large Language Models (LLMs) have become integrated into our daily lives, serving as invaluable assistants in completing tasks. Widely embraced by users, the abuse of LLMs is inevitable, particularly in using them to generate text content for various purposes, leading to difficulties in distinguishing between text generated by LLMs and that written by humans. In this study, we present a dataset named ViDetect, comprising 6.800 samples of Vietnamese essay, with 3.400 samples authored by humans and the remainder generated by LLMs, serving the purpose of detecting text generated by AI. We conducted evaluations using state-of-the-art methods, including ViT5, BartPho, PhoBERT, mDeberta V3, and mBERT. These results contribute not only to the growing body of research on detecting text generated by AI but also demonstrate the adaptability and effectiveness of different methods in the Vietnamese language context. This research lays the foundation for future advancements in AI-generated text detection and provides valuable insights for researchers in the field of natural language processing.","sentences":["In recent years, Large Language Models (LLMs) have become integrated into our daily lives, serving as invaluable assistants in completing tasks.","Widely embraced by users, the abuse of LLMs is inevitable, particularly in using them to generate text content for various purposes, leading to difficulties in distinguishing between text generated by LLMs and that written by humans.","In this study, we present a dataset named ViDetect, comprising 6.800 samples of Vietnamese essay, with 3.400 samples authored by humans and the remainder generated by LLMs, serving the purpose of detecting text generated by AI.","We conducted evaluations using state-of-the-art methods, including ViT5, BartPho, PhoBERT, mDeberta V3, and mBERT.","These results contribute not only to the growing body of research on detecting text generated by AI but also demonstrate the adaptability and effectiveness of different methods in the Vietnamese language context.","This research lays the foundation for future advancements in AI-generated text detection and provides valuable insights for researchers in the field of natural language processing."],"url":"http://arxiv.org/abs/2405.03206v1","category":"cs.CL"}
{"created":"2024-05-06 07:10:09","title":"Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions","abstract":"Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the \"logit lens\" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only correct the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.","sentences":["Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs).","However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference.","This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs.","In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias.","We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the \"logit lens\" method to trace and modify the specific value vectors that contribute to the bias.","By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias.","Our interventions not only correct the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets.","This work represents the first comprehensive mechanistic analysis of anchored bias in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs.","Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2."],"url":"http://arxiv.org/abs/2405.03205v1","category":"cs.CL"}
{"created":"2024-05-06 06:31:47","title":"QuadraNet V2: Efficient and Sustainable Training of High-Order Neural Networks with Quadratic Adaptation","abstract":"Machine learning is evolving towards high-order models that necessitate pre-training on extensive datasets, a process associated with significant overheads. Traditional models, despite having pre-trained weights, are becoming obsolete due to architectural differences that obstruct the effective transfer and initialization of these weights. To address these challenges, we introduce a novel framework, QuadraNet V2, which leverages quadratic neural networks to create efficient and sustainable high-order learning models. Our method initializes the primary term of the quadratic neuron using a standard neural network, while the quadratic term is employed to adaptively enhance the learning of data non-linearity or shifts. This integration of pre-trained primary terms with quadratic terms, which possess advanced modeling capabilities, significantly augments the information characterization capacity of the high-order network. By utilizing existing pre-trained weights, QuadraNet V2 reduces the required GPU hours for training by 90\\% to 98.4\\% compared to training from scratch, demonstrating both efficiency and effectiveness.","sentences":["Machine learning is evolving towards high-order models that necessitate pre-training on extensive datasets, a process associated with significant overheads.","Traditional models, despite having pre-trained weights, are becoming obsolete due to architectural differences that obstruct the effective transfer and initialization of these weights.","To address these challenges, we introduce a novel framework, QuadraNet V2, which leverages quadratic neural networks to create efficient and sustainable high-order learning models.","Our method initializes the primary term of the quadratic neuron using a standard neural network, while the quadratic term is employed to adaptively enhance the learning of data non-linearity or shifts.","This integration of pre-trained primary terms with quadratic terms, which possess advanced modeling capabilities, significantly augments the information characterization capacity of the high-order network.","By utilizing existing pre-trained weights, QuadraNet V2 reduces the required GPU hours for training by 90\\% to 98.4\\% compared to training from scratch, demonstrating both efficiency and effectiveness."],"url":"http://arxiv.org/abs/2405.03192v1","category":"cs.LG"}
{"created":"2024-05-06 06:30:17","title":"Adapting Dual-encoder Vision-language Models for Paraphrased Retrieval","abstract":"In the recent years, the dual-encoder vision-language models (\\eg CLIP) have achieved remarkable text-to-image retrieval performance. However, we discover that these models usually results in very different retrievals for a pair of paraphrased queries. Such behavior might render the retrieval system less predictable and lead to user frustration. In this work, we consider the task of paraphrased text-to-image retrieval where a model aims to return similar results given a pair of paraphrased queries. To start with, we collect a dataset of paraphrased image descriptions to facilitate quantitative evaluation for this task. We then hypothesize that the undesired behavior of existing dual-encoder model is due to their text towers which are trained on image-sentence pairs and lack the ability to capture the semantic similarity between paraphrased queries. To improve on this, we investigate multiple strategies for training a dual-encoder model starting from a language model pretrained on a large text corpus. Compared to public dual-encoder models such as CLIP and OpenCLIP, the model trained with our best adaptation strategy achieves a significantly higher ranking similarity for paraphrased queries while maintaining similar zero-shot classification and retrieval accuracy.","sentences":["In the recent years, the dual-encoder vision-language models (\\eg CLIP) have achieved remarkable text-to-image retrieval performance.","However, we discover that these models usually results in very different retrievals for a pair of paraphrased queries.","Such behavior might render the retrieval system less predictable and lead to user frustration.","In this work, we consider the task of paraphrased text-to-image retrieval where a model aims to return similar results given a pair of paraphrased queries.","To start with, we collect a dataset of paraphrased image descriptions to facilitate quantitative evaluation for this task.","We then hypothesize that the undesired behavior of existing dual-encoder model is due to their text towers which are trained on image-sentence pairs and lack the ability to capture the semantic similarity between paraphrased queries.","To improve on this, we investigate multiple strategies for training a dual-encoder model starting from a language model pretrained on a large text corpus.","Compared to public dual-encoder models such as CLIP and OpenCLIP, the model trained with our best adaptation strategy achieves a significantly higher ranking similarity for paraphrased queries while maintaining similar zero-shot classification and retrieval accuracy."],"url":"http://arxiv.org/abs/2405.03190v1","category":"cs.CV"}
{"created":"2024-05-06 06:12:17","title":"Collaborative Satellite Computing through Adaptive DNN Task Splitting and Offloading","abstract":"Satellite computing has emerged as a promising technology for next-generation wireless networks. This innovative technology provides data processing capabilities, which facilitates the widespread implementation of artificial intelligence (AI)-based applications, especially for image processing tasks involving deep neural network (DNN). With the limited computing resources of an individual satellite, independently handling DNN tasks generated by diverse user equipments (UEs) becomes a significant challenge. One viable solution is dividing a DNN task into multiple subtasks and subsequently distributing them across multiple satellites for collaborative computing. However, it is challenging to partition DNN appropriately and allocate subtasks into suitable satellites while ensuring load balancing. To this end, we propose a collaborative satellite computing system designed to improve task processing efficiency in satellite networks. Based on this system, a workload-balanced adaptive task splitting scheme is developed to equitably distribute the workload of DNN slices for collaborative inference, consequently enhancing the utilization of satellite computing resources. Additionally, a self-adaptive task offloading scheme based on a genetic algorithm (GA) is introduced to determine optimal offloading decisions within dynamic network environments. The numerical results illustrate that our proposal can outperform comparable methods in terms of task completion rate, delay, and resource utilization.","sentences":["Satellite computing has emerged as a promising technology for next-generation wireless networks.","This innovative technology provides data processing capabilities, which facilitates the widespread implementation of artificial intelligence (AI)-based applications, especially for image processing tasks involving deep neural network (DNN).","With the limited computing resources of an individual satellite, independently handling DNN tasks generated by diverse user equipments (UEs) becomes a significant challenge.","One viable solution is dividing a DNN task into multiple subtasks and subsequently distributing them across multiple satellites for collaborative computing.","However, it is challenging to partition DNN appropriately and allocate subtasks into suitable satellites while ensuring load balancing.","To this end, we propose a collaborative satellite computing system designed to improve task processing efficiency in satellite networks.","Based on this system, a workload-balanced adaptive task splitting scheme is developed to equitably distribute the workload of DNN slices for collaborative inference, consequently enhancing the utilization of satellite computing resources.","Additionally, a self-adaptive task offloading scheme based on a genetic algorithm (GA) is introduced to determine optimal offloading decisions within dynamic network environments.","The numerical results illustrate that our proposal can outperform comparable methods in terms of task completion rate, delay, and resource utilization."],"url":"http://arxiv.org/abs/2405.03181v1","category":"cs.DC"}
{"created":"2024-05-06 05:04:59","title":"The Role of Predictive Uncertainty and Diversity in Embodied AI and Robot Learning","abstract":"Uncertainty has long been a critical area of study in robotics, particularly when robots are equipped with analytical models. As we move towards the widespread use of deep neural networks in robots, which have demonstrated remarkable performance in research settings, understanding the nuances of uncertainty becomes crucial for their real-world deployment. This guide offers an overview of the importance of uncertainty and provides methods to quantify and evaluate it from an applications perspective.","sentences":["Uncertainty has long been a critical area of study in robotics, particularly when robots are equipped with analytical models.","As we move towards the widespread use of deep neural networks in robots, which have demonstrated remarkable performance in research settings, understanding the nuances of uncertainty becomes crucial for their real-world deployment.","This guide offers an overview of the importance of uncertainty and provides methods to quantify and evaluate it from an applications perspective."],"url":"http://arxiv.org/abs/2405.03164v1","category":"cs.RO"}
{"created":"2024-05-06 04:44:22","title":"Advancing Multimodal Medical Capabilities of Gemini","abstract":"Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as \"equivalent or better\" than the original radiologists' reports. We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained. Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks.","sentences":["Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models.","Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data.","Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as \"equivalent or better\" than the original radiologists' reports.","We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality.","Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks.","In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance.","Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained.","Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks."],"url":"http://arxiv.org/abs/2405.03162v1","category":"cs.CV"}
{"created":"2024-05-06 04:35:01","title":"Decentralized Online Learning in General-Sum Stackelberg Games","abstract":"We study an online learning problem in general-sum Stackelberg games, where players act in a decentralized and strategic manner. We study two settings depending on the type of information for the follower: (1) the limited information setting where the follower only observes its own reward, and (2) the side information setting where the follower has extra side information about the leader's reward. We show that for the follower, myopically best responding to the leader's action is the best strategy for the limited information setting, but not necessarily so for the side information setting -- the follower can manipulate the leader's reward signals with strategic actions, and hence induce the leader's strategy to converge to an equilibrium that is better off for itself. Based on these insights, we study decentralized online learning for both players in the two settings. Our main contribution is to derive last-iterate convergence and sample complexity results in both settings. Notably, we design a new manipulation strategy for the follower in the latter setting, and show that it has an intrinsic advantage against the best response strategy. Our theories are also supported by empirical results.","sentences":["We study an online learning problem in general-sum Stackelberg games, where players act in a decentralized and strategic manner.","We study two settings depending on the type of information for the follower: (1) the limited information setting where the follower only observes its own reward, and (2) the side information setting where the follower has extra side information about the leader's reward.","We show that for the follower, myopically best responding to the leader's action is the best strategy for the limited information setting, but not necessarily so for the side information setting -- the follower can manipulate the leader's reward signals with strategic actions, and hence induce the leader's strategy to converge to an equilibrium that is better off for itself.","Based on these insights, we study decentralized online learning for both players in the two settings.","Our main contribution is to derive last-iterate convergence and sample complexity results in both settings.","Notably, we design a new manipulation strategy for the follower in the latter setting, and show that it has an intrinsic advantage against the best response strategy.","Our theories are also supported by empirical results."],"url":"http://arxiv.org/abs/2405.03158v1","category":"cs.LG"}
{"created":"2024-05-06 04:04:27","title":"Time Series Stock Price Forecasting Based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) Optimization","abstract":"In this paper, a time series algorithm based on Genetic Algorithm (GA) and Long Short-Term Memory Network (LSTM) optimization is used to forecast stock prices effectively, taking into account the trend of the big data era. The data are first analyzed by descriptive statistics, and then the model is built and trained and tested on the dataset. After optimization and adjustment, the mean absolute error (MAE) of the model gradually decreases from 0.11 to 0.01 and tends to be stable, indicating that the model prediction effect is gradually close to the real value. The results on the test set show that the time series algorithm optimized based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) is able to accurately predict the stock prices, and is highly consistent with the actual price trends and values, with strong generalization ability. The MAE on the test set is 2.41, the MSE is 9.84, the RMSE is 3.13, and the R2 is 0.87. This research result not only provides a novel stock price prediction method, but also provides a useful reference for financial market analysis using computer technology and big data.","sentences":["In this paper, a time series algorithm based on Genetic Algorithm (GA) and Long Short-Term Memory Network (LSTM) optimization is used to forecast stock prices effectively, taking into account the trend of the big data era.","The data are first analyzed by descriptive statistics, and then the model is built and trained and tested on the dataset.","After optimization and adjustment, the mean absolute error (MAE) of the model gradually decreases from 0.11 to 0.01 and tends to be stable, indicating that the model prediction effect is gradually close to the real value.","The results on the test set show that the time series algorithm optimized based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) is able to accurately predict the stock prices, and is highly consistent with the actual price trends and values, with strong generalization ability.","The MAE on the test set is 2.41, the MSE is 9.84, the RMSE is 3.13, and the R2 is 0.87.","This research result not only provides a novel stock price prediction method, but also provides a useful reference for financial market analysis using computer technology and big data."],"url":"http://arxiv.org/abs/2405.03151v1","category":"cs.CE"}
{"created":"2024-05-06 03:42:34","title":"Quantifying the Capabilities of LLMs across Scale and Precision","abstract":"Scale is often attributed as one of the factors that cause an increase in the performance of LLMs, resulting in models with billion and trillion parameters. One of the limitations of such large models is the high computational requirements that limit their usage, deployment, and debugging in resource-constrained scenarios. Two commonly used alternatives to bypass these limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of Llama 70B) and lower the memory requirements by using quantization. While these approaches effectively address the limitation of resources, their impact on model performance needs thorough examination. In this study, we perform a comprehensive evaluation to investigate the effect of model scale and quantization on the performance. We experiment with two major families of open-source instruct models ranging from 7 billion to 70 billion parameters. Our extensive zero-shot experiments across various tasks including natural language understanding, reasoning, misinformation detection, and hallucination reveal that larger models generally outperform their smaller counterparts, suggesting that scale remains an important factor in enhancing performance. We found that larger models show exceptional resilience to precision reduction and can maintain high accuracy even at 4-bit quantization for numerous tasks and they serve as a better solution than using smaller models at high precision under similar memory requirements.","sentences":["Scale is often attributed as one of the factors that cause an increase in the performance of LLMs, resulting in models with billion and trillion parameters.","One of the limitations of such large models is the high computational requirements that limit their usage, deployment, and debugging in resource-constrained scenarios.","Two commonly used alternatives to bypass these limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of Llama 70B) and lower the memory requirements by using quantization.","While these approaches effectively address the limitation of resources, their impact on model performance needs thorough examination.","In this study, we perform a comprehensive evaluation to investigate the effect of model scale and quantization on the performance.","We experiment with two major families of open-source instruct models ranging from 7 billion to 70 billion parameters.","Our extensive zero-shot experiments across various tasks including natural language understanding, reasoning, misinformation detection, and hallucination reveal that larger models generally outperform their smaller counterparts, suggesting that scale remains an important factor in enhancing performance.","We found that larger models show exceptional resilience to precision reduction and can maintain high accuracy even at 4-bit quantization for numerous tasks and they serve as a better solution than using smaller models at high precision under similar memory requirements."],"url":"http://arxiv.org/abs/2405.03146v1","category":"cs.LG"}
{"created":"2024-05-06 03:28:47","title":"Automatic Ultrasound Curve Angle Measurement via Affinity Clustering for Adolescent Idiopathic Scoliosis Evaluation","abstract":"The current clinical gold standard for evaluating adolescent idiopathic scoliosis (AIS) is X-ray radiography, using Cobb angle measurement. However, the frequent monitoring of the AIS progression using X-rays poses a challenge due to the cumulative radiation exposure. Although 3D ultrasound has been validated as a reliable and radiation-free alternative for scoliosis assessment, the process of measuring spinal curvature is still carried out manually. Consequently, there is a considerable demand for a fully automatic system that can locate bony landmarks and perform angle measurements. To this end, we introduce an estimation model for automatic ultrasound curve angle (UCA) measurement. The model employs a dual-branch network to detect candidate landmarks and perform vertebra segmentation on ultrasound coronal images. An affinity clustering strategy is utilized within the vertebral segmentation area to illustrate the affinity relationship between candidate landmarks. Subsequently, we can efficiently perform line delineation from a clustered affinity map for UCA measurement. As our method is specifically designed for UCA calculation, this method outperforms other state-of-the-art methods for landmark and line detection tasks. The high correlation between the automatic UCA and Cobb angle (R$^2$=0.858) suggests that our proposed method can potentially replace manual UCA measurement in ultrasound scoliosis assessment.","sentences":["The current clinical gold standard for evaluating adolescent idiopathic scoliosis (AIS) is X-ray radiography, using Cobb angle measurement.","However, the frequent monitoring of the AIS progression using X-rays poses a challenge due to the cumulative radiation exposure.","Although 3D ultrasound has been validated as a reliable and radiation-free alternative for scoliosis assessment, the process of measuring spinal curvature is still carried out manually.","Consequently, there is a considerable demand for a fully automatic system that can locate bony landmarks and perform angle measurements.","To this end, we introduce an estimation model for automatic ultrasound curve angle (UCA) measurement.","The model employs a dual-branch network to detect candidate landmarks and perform vertebra segmentation on ultrasound coronal images.","An affinity clustering strategy is utilized within the vertebral segmentation area to illustrate the affinity relationship between candidate landmarks.","Subsequently, we can efficiently perform line delineation from a clustered affinity map for UCA measurement.","As our method is specifically designed for UCA calculation, this method outperforms other state-of-the-art methods for landmark and line detection tasks.","The high correlation between the automatic UCA and Cobb angle (R$^2$=0.858) suggests that our proposed method can potentially replace manual UCA measurement in ultrasound scoliosis assessment."],"url":"http://arxiv.org/abs/2405.03141v1","category":"eess.IV"}
{"created":"2024-05-06 02:55:50","title":"WDMoE: Wireless Distributed Large Language Models with Mixture of Experts","abstract":"Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but how wireless communications can support LLMs has not been extensively studied. In this paper, we propose a wireless distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE, deploying LLMs collaboratively across edge servers of base station (BS) and mobile devices in the wireless communications system. Specifically, we decompose the MoE layer in LLMs by deploying the gating network and the preceding neural network layer at BS, while distributing the expert networks across the devices. This arrangement leverages the parallel capabilities of expert networks on distributed devices. Moreover, to overcome the instability of wireless communications, we design an expert selection policy by taking into account both the performance of the model and the end-to-end latency, which includes both transmission delay and inference delay. Evaluations conducted across various LLMs and multiple datasets demonstrate that WDMoE not only outperforms existing models, such as Llama 2 with 70 billion parameters, but also significantly reduces end-to-end latency.","sentences":["Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but how wireless communications can support LLMs has not been extensively studied.","In this paper, we propose a wireless distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE, deploying LLMs collaboratively across edge servers of base station (BS) and mobile devices in the wireless communications system.","Specifically, we decompose the MoE layer in LLMs by deploying the gating network and the preceding neural network layer at BS, while distributing the expert networks across the devices.","This arrangement leverages the parallel capabilities of expert networks on distributed devices.","Moreover, to overcome the instability of wireless communications, we design an expert selection policy by taking into account both the performance of the model and the end-to-end latency, which includes both transmission delay and inference delay.","Evaluations conducted across various LLMs and multiple datasets demonstrate that WDMoE not only outperforms existing models, such as Llama 2 with 70 billion parameters, but also significantly reduces end-to-end latency."],"url":"http://arxiv.org/abs/2405.03131v1","category":"cs.IT"}
{"created":"2024-05-06 02:53:51","title":"Active Sensing for Multiuser Beam Tracking with Reconfigurable Intelligent Surface","abstract":"This paper studies a beam tracking problem in which an access point (AP), in collaboration with a reconfigurable intelligent surface (RIS), dynamically adjusts its downlink beamformers and the reflection pattern at the RIS in order to maintain reliable communications with multiple mobile user equipments (UEs). Specifically, the mobile UEs send uplink pilots to the AP periodically during the channel sensing intervals, the AP then adaptively configures the beamformers and the RIS reflection coefficients for subsequent data transmission based on the received pilots. This is an active sensing problem, because channel sensing involves configuring the RIS coefficients during the pilot stage and the optimal sensing strategy should exploit the trajectory of channel state information (CSI) from previously received pilots. Analytical solution to such an active sensing problem is very challenging. In this paper, we propose a deep learning framework utilizing a recurrent neural network (RNN) to automatically summarize the time-varying CSI obtained from the periodically received pilots into state vectors. These state vectors are then mapped to the AP beamformers and RIS reflection coefficients for subsequent downlink data transmissions, as well as the RIS reflection coefficients for the next round of uplink channel sensing. The mappings from the state vectors to the downlink beamformers and the RIS reflection coefficients for both channel sensing and downlink data transmission are performed using graph neural networks (GNNs) to account for the interference among the UEs. Simulations demonstrate significant and interpretable performance improvement of the proposed approach over the existing data-driven methods with nonadaptive channel sensing schemes.","sentences":["This paper studies a beam tracking problem in which an access point (AP), in collaboration with a reconfigurable intelligent surface (RIS), dynamically adjusts its downlink beamformers and the reflection pattern at the RIS in order to maintain reliable communications with multiple mobile user equipments (UEs).","Specifically, the mobile UEs send uplink pilots to the AP periodically during the channel sensing intervals, the AP then adaptively configures the beamformers and the RIS reflection coefficients for subsequent data transmission based on the received pilots.","This is an active sensing problem, because channel sensing involves configuring the RIS coefficients during the pilot stage and the optimal sensing strategy should exploit the trajectory of channel state information (CSI) from previously received pilots.","Analytical solution to such an active sensing problem is very challenging.","In this paper, we propose a deep learning framework utilizing a recurrent neural network (RNN) to automatically summarize the time-varying CSI obtained from the periodically received pilots into state vectors.","These state vectors are then mapped to the AP beamformers and RIS reflection coefficients for subsequent downlink data transmissions, as well as the RIS reflection coefficients for the next round of uplink channel sensing.","The mappings from the state vectors to the downlink beamformers and the RIS reflection coefficients for both channel sensing and downlink data transmission are performed using graph neural networks (GNNs) to account for the interference among the UEs.","Simulations demonstrate significant and interpretable performance improvement of the proposed approach over the existing data-driven methods with nonadaptive channel sensing schemes."],"url":"http://arxiv.org/abs/2405.03129v1","category":"eess.SP"}
{"created":"2024-05-06 02:32:41","title":"AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding","abstract":"The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalker's capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at https://github.com/X-LANCE/AniTalker.","sentences":["The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait.","Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation.","This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements.","AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders.","This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data.","Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations.","This method not only demonstrates AniTalker's capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications.","Synthetic results can be viewed at https://github.com/X-LANCE/AniTalker."],"url":"http://arxiv.org/abs/2405.03121v1","category":"cs.CV"}
{"created":"2024-05-06 02:13:08","title":"Robot Air Hockey: A Manipulation Testbed for Robot Learning with Reinforcement Learning","abstract":"Reinforcement Learning is a promising tool for learning complex policies even in fast-moving and object-interactive domains where human teleoperation or hard-coded policies might fail. To effectively reflect this challenging category of tasks, we introduce a dynamic, interactive RL testbed based on robot air hockey. By augmenting air hockey with a large family of tasks ranging from easy tasks like reaching, to challenging ones like pushing a block by hitting it with a puck, as well as goal-based and human-interactive tasks, our testbed allows a varied assessment of RL capabilities. The robot air hockey testbed also supports sim-to-real transfer with three domains: two simulators of increasing fidelity and a real robot system. Using a dataset of demonstration data gathered through two teleoperation systems: a virtualized control environment, and human shadowing, we assess the testbed with behavior cloning, offline RL, and RL from scratch.","sentences":["Reinforcement Learning is a promising tool for learning complex policies even in fast-moving and object-interactive domains where human teleoperation or hard-coded policies might fail.","To effectively reflect this challenging category of tasks, we introduce a dynamic, interactive RL testbed based on robot air hockey.","By augmenting air hockey with a large family of tasks ranging from easy tasks like reaching, to challenging ones like pushing a block by hitting it with a puck, as well as goal-based and human-interactive tasks, our testbed allows a varied assessment of RL capabilities.","The robot air hockey testbed also supports sim-to-real transfer with three domains: two simulators of increasing fidelity and a real robot system.","Using a dataset of demonstration data gathered through two teleoperation systems: a virtualized control environment, and human shadowing, we assess the testbed with behavior cloning, offline RL, and RL from scratch."],"url":"http://arxiv.org/abs/2405.03113v1","category":"cs.RO"}
{"created":"2024-05-06 01:37:12","title":"Double Self-Sustainable Reconfigurable Intelligent Surfaces Aided Wireless Communications","abstract":"A double self-sustainable reconfigurable intelligent surfaces (RISs) assisted multi-user multiple input multiple output (MIMO) system is investigated. Two RISs are equipped with energy harvesting circuit to achieve self-sustainable transmission. The aim is to minimize the transmission power at the base station (BS), while guaranteeing the quality of service (QoS) requirements of the users and meeting the power consumption requirements of the RISs. A block coordinate descent (BCD) algorithm based on the penalty-based method and successive convex approximation (SCA) is employed to alternatively optimize the active beamforming at the BS and the phase shifts, as well as amplitude coefficients of two RISs. Simulation results show that the required power consumption at the BS for the proposed double self-sustainable RISs system is significantly reduced compared to conventional RIS systems.","sentences":["A double self-sustainable reconfigurable intelligent surfaces (RISs) assisted multi-user multiple input multiple output (MIMO) system is investigated.","Two RISs are equipped with energy harvesting circuit to achieve self-sustainable transmission.","The aim is to minimize the transmission power at the base station (BS), while guaranteeing the quality of service (QoS) requirements of the users and meeting the power consumption requirements of the RISs.","A block coordinate descent (BCD) algorithm based on the penalty-based method and successive convex approximation (SCA) is employed to alternatively optimize the active beamforming at the BS and the phase shifts, as well as amplitude coefficients of two RISs.","Simulation results show that the required power consumption at the BS for the proposed double self-sustainable RISs system is significantly reduced compared to conventional RIS systems."],"url":"http://arxiv.org/abs/2405.03101v1","category":"cs.IT"}
{"created":"2024-05-06 01:21:50","title":"To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models","abstract":"LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches.","sentences":["LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time.","This fact is known to be the cause of privacy and related (e.g., copyright) problems.","Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility.","We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM.","We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively.","A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches."],"url":"http://arxiv.org/abs/2405.03097v1","category":"cs.LG"}
{"created":"2024-05-06 01:05:21","title":"Research on Image Recognition Technology Based on Multimodal Deep Learning","abstract":"This project investigates the human multi-modal behavior identification algorithm utilizing deep neural networks. According to the characteristics of different modal information, different deep neural networks are used to adapt to different modal video information. Through the integration of various deep neural networks, the algorithm successfully identifies behaviors across multiple modalities. In this project, multiple cameras developed by Microsoft Kinect were used to collect corresponding bone point data based on acquiring conventional images. In this way, the motion features in the image can be extracted. Ultimately, the behavioral characteristics discerned through both approaches are synthesized to facilitate the precise identification and categorization of behaviors. The performance of the suggested algorithm was evaluated using the MSR3D data set. The findings from these experiments indicate that the accuracy in recognizing behaviors remains consistently high, suggesting that the algorithm is reliable in various scenarios. Additionally, the tests demonstrate that the algorithm substantially enhances the accuracy of detecting pedestrian behaviors in video footage.","sentences":["This project investigates the human multi-modal behavior identification algorithm utilizing deep neural networks.","According to the characteristics of different modal information, different deep neural networks are used to adapt to different modal video information.","Through the integration of various deep neural networks, the algorithm successfully identifies behaviors across multiple modalities.","In this project, multiple cameras developed by Microsoft Kinect were used to collect corresponding bone point data based on acquiring conventional images.","In this way, the motion features in the image can be extracted.","Ultimately, the behavioral characteristics discerned through both approaches are synthesized to facilitate the precise identification and categorization of behaviors.","The performance of the suggested algorithm was evaluated using the MSR3D data set.","The findings from these experiments indicate that the accuracy in recognizing behaviors remains consistently high, suggesting that the algorithm is reliable in various scenarios.","Additionally, the tests demonstrate that the algorithm substantially enhances the accuracy of detecting pedestrian behaviors in video footage."],"url":"http://arxiv.org/abs/2405.03091v1","category":"cs.CV"}
{"created":"2024-05-05 22:54:51","title":"Traffic Performance GPT (TP-GPT): Real-Time Data Informed Intelligent ChatBot for Transportation Surveillance and Management","abstract":"The digitization of traffic sensing infrastructure has significantly accumulated an extensive traffic data warehouse, which presents unprecedented challenges for transportation analytics. The complexities associated with querying large-scale multi-table databases require specialized programming expertise and labor-intensive development. Additionally, traditional analysis methods have focused mainly on numerical data, often neglecting the semantic aspects that could enhance interpretability and understanding. Furthermore, real-time traffic data access is typically limited due to privacy concerns. To bridge this gap, the integration of Large Language Models (LLMs) into the domain of traffic management presents a transformative approach to addressing the complexities and challenges inherent in modern transportation systems. This paper proposes an intelligent online chatbot, TP-GPT, for efficient customized transportation surveillance and management empowered by a large real-time traffic database. The innovative framework leverages contextual and generative intelligence of language models to generate accurate SQL queries and natural language interpretations by employing transportation-specialized prompts, Chain-of-Thought prompting, few-shot learning, multi-agent collaboration strategy, and chat memory. Experimental study demonstrates that our approach outperforms state-of-the-art baselines such as GPT-4 and PaLM 2 on a challenging traffic-analysis benchmark TransQuery. TP-GPT would aid researchers and practitioners in real-time transportation surveillance and management in a privacy-preserving, equitable, and customizable manner.","sentences":["The digitization of traffic sensing infrastructure has significantly accumulated an extensive traffic data warehouse, which presents unprecedented challenges for transportation analytics.","The complexities associated with querying large-scale multi-table databases require specialized programming expertise and labor-intensive development.","Additionally, traditional analysis methods have focused mainly on numerical data, often neglecting the semantic aspects that could enhance interpretability and understanding.","Furthermore, real-time traffic data access is typically limited due to privacy concerns.","To bridge this gap, the integration of Large Language Models (LLMs) into the domain of traffic management presents a transformative approach to addressing the complexities and challenges inherent in modern transportation systems.","This paper proposes an intelligent online chatbot, TP-GPT, for efficient customized transportation surveillance and management empowered by a large real-time traffic database.","The innovative framework leverages contextual and generative intelligence of language models to generate accurate SQL queries and natural language interpretations by employing transportation-specialized prompts, Chain-of-Thought prompting, few-shot learning, multi-agent collaboration strategy, and chat memory.","Experimental study demonstrates that our approach outperforms state-of-the-art baselines such as GPT-4 and PaLM 2 on a challenging traffic-analysis benchmark TransQuery.","TP-GPT would aid researchers and practitioners in real-time transportation surveillance and management in a privacy-preserving, equitable, and customizable manner."],"url":"http://arxiv.org/abs/2405.03076v1","category":"cs.MA"}
{"created":"2024-05-05 22:40:07","title":"Layered Graph Security Games","abstract":"Security games model strategic interactions in adversarial real-world applications. Such applications often involve extremely large but highly structured strategy sets (e.g., selecting a distribution over all patrol routes in a given graph). In this paper, we represent each player's strategy space using a layered graph whose paths represent an exponentially large strategy space. Our formulation entails not only classic pursuit-evasion games, but also other security games, such as those modeling anti-terrorism and logistical interdiction. We study two-player zero-sum games under two distinct utility models: linear and binary utilities. We show that under linear utilities, Nash equilibrium can be computed in polynomial time, while binary utilities may lead to situations where even computing a best-response is computationally intractable. To this end, we propose a practical algorithm based on incremental strategy generation and mixed integer linear programs. We show through extensive experiments that our algorithm efficiently computes $\\epsilon$-equilibrium for many games of interest. We find that target values and graph structure often have a larger influence on running times as compared to the size of the graph per se.","sentences":["Security games model strategic interactions in adversarial real-world applications.","Such applications often involve extremely large but highly structured strategy sets (e.g., selecting a distribution over all patrol routes in a given graph).","In this paper, we represent each player's strategy space using a layered graph whose paths represent an exponentially large strategy space.","Our formulation entails not only classic pursuit-evasion games, but also other security games, such as those modeling anti-terrorism and logistical interdiction.","We study two-player zero-sum games under two distinct utility models: linear and binary utilities.","We show that under linear utilities, Nash equilibrium can be computed in polynomial time, while binary utilities may lead to situations where even computing a best-response is computationally intractable.","To this end, we propose a practical algorithm based on incremental strategy generation and mixed integer linear programs.","We show through extensive experiments that our algorithm efficiently computes $\\epsilon$-equilibrium for many games of interest.","We find that target values and graph structure often have a larger influence on running times as compared to the size of the graph per se."],"url":"http://arxiv.org/abs/2405.03070v1","category":"cs.GT"}
{"created":"2024-05-05 22:32:01","title":"On Probabilistic and Causal Reasoning with Summation Operators","abstract":"Ibeling et al. (2023). axiomatize increasingly expressive languages of causation and probability, and Mosse et al. (2024) show that reasoning (specifically the satisfiability problem) in each causal language is as difficult, from a computational complexity perspective, as reasoning in its merely probabilistic or \"correlational\" counterpart. Introducing a summation operator to capture common devices that appear in applications -- such as the $do$-calculus of Pearl (2009) for causal inference, which makes ample use of marginalization -- van der Zander et al. (2023) partially extend these earlier complexity results to causal and probabilistic languages with marginalization. We complete this extension, fully characterizing the complexity of probabilistic and causal reasoning with summation, demonstrating that these again remain equally difficult. Surprisingly, allowing free variables for random variable values results in a system that is undecidable, so long as the ranges of these random variables are unrestricted. We finally axiomatize these languages featuring marginalization (or more generally summation), resolving open questions posed by Ibeling et al. (2023).","sentences":["Ibeling et al. (2023).","axiomatize increasingly expressive languages of causation and probability, and Mosse et al.","(2024) show that reasoning (specifically the satisfiability problem) in each causal language is as difficult, from a computational complexity perspective, as reasoning in its merely probabilistic or \"correlational\" counterpart.","Introducing a summation operator to capture common devices that appear in applications -- such as the $do$-calculus of Pearl (2009) for causal inference, which makes ample use of marginalization -- van der Zander et al.","(2023) partially extend these earlier complexity results to causal and probabilistic languages with marginalization.","We complete this extension, fully characterizing the complexity of probabilistic and causal reasoning with summation, demonstrating that these again remain equally difficult.","Surprisingly, allowing free variables for random variable values results in a system that is undecidable, so long as the ranges of these random variables are unrestricted.","We finally axiomatize these languages featuring marginalization (or more generally summation), resolving open questions posed by Ibeling et al. (2023)."],"url":"http://arxiv.org/abs/2405.03069v1","category":"math.LO"}
{"created":"2024-05-05 22:23:49","title":"A census of new globular clusters in the Galactic bulge","abstract":"The number of known globular clusters in the Galactic bulge has been increasing steadily thanks to different new surveys. The aim of this study is to provide a census of the newly revealed globular clusters in the Galactic bulge, and analyze their characteristics. In recent years, many globular clusters have been discovered or identified. The stellar populations to which they belong are indicated in their original studies: they are mostly bulge clusters, with some identified as disk or halo members. We collected 41 new globular clusters revealed in the last decade and compared them to the known bulge clusters. The new clusters are intrinsically faint with $M_V$ of around -6.0 mag. The distance to the Sun of the ensemble of well-known and new bulge clusters is compatible with the Galactocentric distance measurements from the Galactic black hole location. The ensemble sample shows metallicity peaks at [Fe/H] ~ -1.08 $\\pm$ 0.35 and -0.51 $\\pm$ 0.25 dex, confirming previous findings. The age-metallicity relation of the new clusters younger than 10 Gyr is compatible with that of the ex situ samples of the dwarf galaxies Sagittarius, Canis Majoris, and Gaia-Enceladus-Sausage. The clusters with ages between 11.5 and 13.5 Gyr show no age-metallicity relation, because they are all old. This is compatible with their formation in situ in the early Galaxy.","sentences":["The number of known globular clusters in the Galactic bulge has been increasing steadily thanks to different new surveys.","The aim of this study is to provide a census of the newly revealed globular clusters in the Galactic bulge, and analyze their characteristics.","In recent years, many globular clusters have been discovered or identified.","The stellar populations to which they belong are indicated in their original studies: they are mostly bulge clusters, with some identified as disk or halo members.","We collected 41 new globular clusters revealed in the last decade and compared them to the known bulge clusters.","The new clusters are intrinsically faint with $M_V$ of around -6.0 mag.","The distance to the Sun of the ensemble of well-known and new bulge clusters is compatible with the Galactocentric distance measurements from the Galactic black hole location.","The ensemble sample shows metallicity peaks at [Fe/H] ~ -1.08 $\\pm$ 0.35 and -0.51 $\\pm$ 0.25 dex, confirming previous findings.","The age-metallicity relation of the new clusters younger than 10 Gyr is compatible with that of the ex situ samples of the dwarf galaxies Sagittarius, Canis Majoris, and Gaia-Enceladus-Sausage.","The clusters with ages between 11.5 and 13.5 Gyr show no age-metallicity relation, because they are all old.","This is compatible with their formation in situ in the early Galaxy."],"url":"http://arxiv.org/abs/2405.03068v1","category":"astro-ph.GA"}
{"created":"2024-05-05 22:21:15","title":"A scoping review of using Large Language Models (LLMs) to investigate Electronic Health Records (EHRs)","abstract":"Electronic Health Records (EHRs) play an important role in the healthcare system. However, their complexity and vast volume pose significant challenges to data interpretation and analysis. Recent advancements in Artificial Intelligence (AI), particularly the development of Large Language Models (LLMs), open up new opportunities for researchers in this domain. Although prior studies have demonstrated their potential in language understanding and processing in the context of EHRs, a comprehensive scoping review is lacking. This study aims to bridge this research gap by conducting a scoping review based on 329 related papers collected from OpenAlex. We first performed a bibliometric analysis to examine paper trends, model applications, and collaboration networks. Next, we manually reviewed and categorized each paper into one of the seven identified topics: named entity recognition, information extraction, text similarity, text summarization, text classification, dialogue system, and diagnosis and prediction. For each topic, we discussed the unique capabilities of LLMs, such as their ability to understand context, capture semantic relations, and generate human-like text. Finally, we highlighted several implications for researchers from the perspectives of data resources, prompt engineering, fine-tuning, performance measures, and ethical concerns. In conclusion, this study provides valuable insights into the potential of LLMs to transform EHR research and discusses their applications and ethical considerations.","sentences":["Electronic Health Records (EHRs) play an important role in the healthcare system.","However, their complexity and vast volume pose significant challenges to data interpretation and analysis.","Recent advancements in Artificial Intelligence (AI), particularly the development of Large Language Models (LLMs), open up new opportunities for researchers in this domain.","Although prior studies have demonstrated their potential in language understanding and processing in the context of EHRs, a comprehensive scoping review is lacking.","This study aims to bridge this research gap by conducting a scoping review based on 329 related papers collected from OpenAlex.","We first performed a bibliometric analysis to examine paper trends, model applications, and collaboration networks.","Next, we manually reviewed and categorized each paper into one of the seven identified topics: named entity recognition, information extraction, text similarity, text summarization, text classification, dialogue system, and diagnosis and prediction.","For each topic, we discussed the unique capabilities of LLMs, such as their ability to understand context, capture semantic relations, and generate human-like text.","Finally, we highlighted several implications for researchers from the perspectives of data resources, prompt engineering, fine-tuning, performance measures, and ethical concerns.","In conclusion, this study provides valuable insights into the potential of LLMs to transform EHR research and discusses their applications and ethical considerations."],"url":"http://arxiv.org/abs/2405.03066v1","category":"cs.ET"}
{"created":"2024-05-05 22:06:42","title":"RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation","abstract":"Deep reinforcement learning (DRL) is playing an increasingly important role in real-world applications. However, obtaining an optimally performing DRL agent for complex tasks, especially with sparse rewards, remains a significant challenge. The training of a DRL agent can be often trapped in a bottleneck without further progress. In this paper, we propose RICE, an innovative refining scheme for reinforcement learning that incorporates explanation methods to break through the training bottlenecks. The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods, thereby encouraging the agent to explore from the mixed initial states. Through careful design, we can theoretically guarantee that our refining scheme has a tighter sub-optimality bound. We evaluate RICE in various popular RL environments and real-world applications. The results demonstrate that RICE significantly outperforms existing refining schemes in enhancing agent performance.","sentences":["Deep reinforcement learning (DRL) is playing an increasingly important role in real-world applications.","However, obtaining an optimally performing DRL agent for complex tasks, especially with sparse rewards, remains a significant challenge.","The training of a DRL agent can be often trapped in a bottleneck without further progress.","In this paper, we propose RICE, an innovative refining scheme for reinforcement learning that incorporates explanation methods to break through the training bottlenecks.","The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods, thereby encouraging the agent to explore from the mixed initial states.","Through careful design, we can theoretically guarantee that our refining scheme has a tighter sub-optimality bound.","We evaluate RICE in various popular RL environments and real-world applications.","The results demonstrate that RICE significantly outperforms existing refining schemes in enhancing agent performance."],"url":"http://arxiv.org/abs/2405.03064v1","category":"cs.LG"}
{"created":"2024-05-05 22:04:50","title":"Cavity QED systems for steady-state sources of Wigner-negative light","abstract":"We present a theoretical investigation of optical cavity QED systems, as described by the driven, open Jaynes-Cummings model and some of its variants, as potential sources of steady-state Wigner-negative light. We consider temporal modes in the continuous output field from the cavity and demonstrate pronounced negativity in their Wigner distributions for experimentally-relevant parameter regimes. We consider models of both single and collective atomic spin systems, and find a rich structure of Wigner-distribution negativity as the spin size is varied. We also demonstrate an effective realization of all of the models considered using just a single 87Rb atom and based upon combinations of laser- and laser-plus-cavity-driven Raman transitions between magnetic sublevels in a single ground hyperfine state.","sentences":["We present a theoretical investigation of optical cavity QED systems, as described by the driven, open Jaynes-Cummings model and some of its variants, as potential sources of steady-state Wigner-negative light.","We consider temporal modes in the continuous output field from the cavity and demonstrate pronounced negativity in their Wigner distributions for experimentally-relevant parameter regimes.","We consider models of both single and collective atomic spin systems, and find a rich structure of Wigner-distribution negativity as the spin size is varied.","We also demonstrate an effective realization of all of the models considered using just a single 87Rb atom and based upon combinations of laser- and laser-plus-cavity-driven Raman transitions between magnetic sublevels in a single ground hyperfine state."],"url":"http://arxiv.org/abs/2405.03062v1","category":"quant-ph"}
{"created":"2024-05-05 20:25:54","title":"Clip-on lens for scanning tunneling luminescence microscopy","abstract":"We demonstrate and verify the in-situ addition of a collecting lens for electroluminescence experiments to an existing scanning tunneling microscope. We fabricate a simple clip-on lens that we reversibly attach at the sample plate via regular sample transfer tools to collimate the light emitted from a plasmonic tunneling junction to the viewport ordinarily used for optical access. The proximity of the lens to the tunneling junction allows us to exploit the full numerical aperture which helps us achieve good collection efficiencies, demonstrating the quick turnaround of converting an existing setup with optical access into a practical scanning luminescence microscope. We verify the function of the clip-on lens by measuring the bias dependent plasmon of Au, Ag, and spatial luminescence maps.","sentences":["We demonstrate and verify the in-situ addition of a collecting lens for electroluminescence experiments to an existing scanning tunneling microscope.","We fabricate a simple clip-on lens that we reversibly attach at the sample plate via regular sample transfer tools to collimate the light emitted from a plasmonic tunneling junction to the viewport ordinarily used for optical access.","The proximity of the lens to the tunneling junction allows us to exploit the full numerical aperture which helps us achieve good collection efficiencies, demonstrating the quick turnaround of converting an existing setup with optical access into a practical scanning luminescence microscope.","We verify the function of the clip-on lens by measuring the bias dependent plasmon of Au, Ag, and spatial luminescence maps."],"url":"http://arxiv.org/abs/2405.03048v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-05 20:19:14","title":"A Model-Free Kullback-Leibler Divergence Filter for Anomaly Detection in Noisy Data Series","abstract":"We propose a Kullback-Leibler Divergence (KLD) filter to extract anomalies within data series generated by a broad class of proximity sensors, along with the anomaly locations and their relative sizes. The technique applies to devices commonly used in engineering practice, such as those mounted on mobile robots for non-destructive inspection of hazardous or other environments that may not be directly accessible to humans. The raw data generated by this class of sensors can be challenging to analyze due to the prevalence of noise over the signal content. The proposed filter is built to detect the difference of information content between data series collected by the sensor and baseline data series. It is applicable in a model-based or model-free context. The performance of the KLD filter is validated in an industrial-norm setup and benchmarked against a peer industrially-adopted algorithm.","sentences":["We propose a Kullback-Leibler Divergence (KLD) filter to extract anomalies within data series generated by a broad class of proximity sensors, along with the anomaly locations and their relative sizes.","The technique applies to devices commonly used in engineering practice, such as those mounted on mobile robots for non-destructive inspection of hazardous or other environments that may not be directly accessible to humans.","The raw data generated by this class of sensors can be challenging to analyze due to the prevalence of noise over the signal content.","The proposed filter is built to detect the difference of information content between data series collected by the sensor and baseline data series.","It is applicable in a model-based or model-free context.","The performance of the KLD filter is validated in an industrial-norm setup and benchmarked against a peer industrially-adopted algorithm."],"url":"http://arxiv.org/abs/2405.03047v1","category":"eess.SP"}
{"created":"2024-05-05 20:12:37","title":"Swipe2Pair: Secure and Fast In-Band Wireless Device Pairing","abstract":"Wireless device pairing is a critical security mechanism to bootstrap the secure communication between two devices without a pre-shared secret. It has been widely used in many Internet of Things (IoT) applications, such as smart-home and smart-health. Most existing device pairing mechanisms are based on out-of-band channels, e.g., extra sensors or hardware, to validate the proximity of pairing devices. However, out-of-band channels are not universal across all wireless devices, so such a scheme is limited to certain application scenarios or conditions. On the other hand, in-band channel-based device pairing seeks universal applicability by only relying on wireless interfaces. Existing in-band channel-based pairing schemes either require multiple antennas separated by a good distance on one pairing device, which is not feasible in certain scenarios, or require users to repeat multiple sweeps, which is not optimal in terms of usability.   Therefore, an in-band wireless device pairing scheme providing high security while maintaining high usability (simple pairing process and minimal user intervention) is highly desired. In this work, we propose an easy-to-use mutual authentication device pairing scheme, named Swipe2Pair, based on the proximity of pairing devices and randomization of wireless transmission power. We conduct extensive security analysis and collect considerable experimental data under various settings across different environments. Experimental results show that Swipe2Pair achieves high security and usability. It only takes less than one second to complete the pairing process with a simple swipe of one device in front of the other.","sentences":["Wireless device pairing is a critical security mechanism to bootstrap the secure communication between two devices without a pre-shared secret.","It has been widely used in many Internet of Things (IoT) applications, such as smart-home and smart-health.","Most existing device pairing mechanisms are based on out-of-band channels, e.g., extra sensors or hardware, to validate the proximity of pairing devices.","However, out-of-band channels are not universal across all wireless devices, so such a scheme is limited to certain application scenarios or conditions.","On the other hand, in-band channel-based device pairing seeks universal applicability by only relying on wireless interfaces.","Existing in-band channel-based pairing schemes either require multiple antennas separated by a good distance on one pairing device, which is not feasible in certain scenarios, or require users to repeat multiple sweeps, which is not optimal in terms of usability.   ","Therefore, an in-band wireless device pairing scheme providing high security while maintaining high usability (simple pairing process and minimal user intervention) is highly desired.","In this work, we propose an easy-to-use mutual authentication device pairing scheme, named Swipe2Pair, based on the proximity of pairing devices and randomization of wireless transmission power.","We conduct extensive security analysis and collect considerable experimental data under various settings across different environments.","Experimental results show that Swipe2Pair achieves high security and usability.","It only takes less than one second to complete the pairing process with a simple swipe of one device in front of the other."],"url":"http://arxiv.org/abs/2405.03045v1","category":"cs.CR"}
{"created":"2024-05-05 20:07:47","title":"Functional Post-Clustering Selective Inference with Applications to EHR Data Analysis","abstract":"In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases. Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters. Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error. In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time. Our method extends classical selective inference methods for cross-sectional data to longitudinal data. We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors. We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach.","sentences":["In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases.","Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters.","Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error.","In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time.","Our method extends classical selective inference methods for cross-sectional data to longitudinal data.","We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors.","We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach."],"url":"http://arxiv.org/abs/2405.03042v1","category":"stat.ME"}
{"created":"2024-05-05 20:00:22","title":"Performance Evaluation of Real-Time Object Detection for Electric Scooters","abstract":"Electric scooters (e-scooters) have rapidly emerged as a popular mode of transportation in urban areas, yet they pose significant safety challenges. In the United States, the rise of e-scooters has been marked by a concerning increase in related injuries and fatalities. Recently, while deep-learning object detection holds paramount significance in autonomous vehicles to avoid potential collisions, its application in the context of e-scooters remains relatively unexplored. This paper addresses this gap by assessing the effectiveness and efficiency of cutting-edge object detectors designed for e-scooters. To achieve this, the first comprehensive benchmark involving 22 state-of-the-art YOLO object detectors, including five versions (YOLOv3, YOLOv5, YOLOv6, YOLOv7, and YOLOv8), has been established for real-time traffic object detection using a self-collected dataset featuring e-scooters. The detection accuracy, measured in terms of mAP@0.5, ranges from 27.4% (YOLOv7-E6E) to 86.8% (YOLOv5s). All YOLO models, particularly YOLOv3-tiny, have displayed promising potential for real-time object detection in the context of e-scooters. Both the traffic scene dataset (https://zenodo.org/records/10578641) and software program codes (https://github.com/DongChen06/ScooterDet) for model benchmarking in this study are publicly available, which will not only improve e-scooter safety with advanced object detection but also lay the groundwork for tailored solutions, promising a safer and more sustainable urban micromobility landscape.","sentences":["Electric scooters (e-scooters) have rapidly emerged as a popular mode of transportation in urban areas, yet they pose significant safety challenges.","In the United States, the rise of e-scooters has been marked by a concerning increase in related injuries and fatalities.","Recently, while deep-learning object detection holds paramount significance in autonomous vehicles to avoid potential collisions, its application in the context of e-scooters remains relatively unexplored.","This paper addresses this gap by assessing the effectiveness and efficiency of cutting-edge object detectors designed for e-scooters.","To achieve this, the first comprehensive benchmark involving 22 state-of-the-art YOLO object detectors, including five versions (YOLOv3, YOLOv5, YOLOv6, YOLOv7, and YOLOv8), has been established for real-time traffic object detection using a self-collected dataset featuring e-scooters.","The detection accuracy, measured in terms of mAP@0.5, ranges from 27.4% (YOLOv7-E6E) to 86.8% (YOLOv5s).","All YOLO models, particularly YOLOv3-tiny, have displayed promising potential for real-time object detection in the context of e-scooters.","Both the traffic scene dataset (https://zenodo.org/records/10578641) and software program codes (https://github.com/DongChen06/ScooterDet) for model benchmarking in this study are publicly available, which will not only improve e-scooter safety with advanced object detection but also lay the groundwork for tailored solutions, promising a safer and more sustainable urban micromobility landscape."],"url":"http://arxiv.org/abs/2405.03039v1","category":"cs.CV"}
{"created":"2024-05-05 18:13:38","title":"Joint Discrete Precoding and RIS Optimization for RIS-Assisted MU-MIMO Communication Systems","abstract":"This paper considers a multi-user multiple-input multiple-output (MU-MIMO) system where the downlink communication between a base station (BS) and multiple user equipments (UEs) is aided by a reconfigurable intelligent surface (RIS). We study the sum-rate maximization problem with the objective of finding the optimal precoding vectors and RIS configuration. Due to fronthaul limitation, each entry of the precoding vectors must be picked from a finite set of quantization labels. Furthermore, two scenarios for the RIS are investigated, one with continuous infinite-resolution reflection coefficients and another with discrete finite-resolution reflection coefficients. A novel framework is developed which, in contrast to the common literature that only offers sub-optimal solutions for optimization of discrete variables, is able to find the optimal solution to problems involving discrete constraints. Based on the classical weighted minimum mean square error (WMMSE), we transform the original problem into an equivalent weighted sum mean square error (MSE) minimization problem and solve it iteratively. We compute the optimal precoding vectors via an efficient algorithm inspired by sphere decoding (SD). For optimizing the discrete RIS configuration, two solutions based on the SD algorithm are developed: An optimal SD-based algorithm and a low-complexity heuristic method that can efficiently obtain RIS configuration without much loss in optimality. The effectiveness of the presented algorithms is corroborated via numerical simulations where it is shown that the proposed designs are remarkably superior to the commonly used benchmarks.","sentences":["This paper considers a multi-user multiple-input multiple-output (MU-MIMO) system where the downlink communication between a base station (BS) and multiple user equipments (UEs) is aided by a reconfigurable intelligent surface (RIS).","We study the sum-rate maximization problem with the objective of finding the optimal precoding vectors and RIS configuration.","Due to fronthaul limitation, each entry of the precoding vectors must be picked from a finite set of quantization labels.","Furthermore, two scenarios for the RIS are investigated, one with continuous infinite-resolution reflection coefficients and another with discrete finite-resolution reflection coefficients.","A novel framework is developed which, in contrast to the common literature that only offers sub-optimal solutions for optimization of discrete variables, is able to find the optimal solution to problems involving discrete constraints.","Based on the classical weighted minimum mean square error (WMMSE), we transform the original problem into an equivalent weighted sum mean square error (MSE) minimization problem and solve it iteratively.","We compute the optimal precoding vectors via an efficient algorithm inspired by sphere decoding (SD).","For optimizing the discrete RIS configuration, two solutions based on the SD algorithm are developed: An optimal SD-based algorithm and a low-complexity heuristic method that can efficiently obtain RIS configuration without much loss in optimality.","The effectiveness of the presented algorithms is corroborated via numerical simulations where it is shown that the proposed designs are remarkably superior to the commonly used benchmarks."],"url":"http://arxiv.org/abs/2405.03022v1","category":"eess.SP"}
{"created":"2024-05-05 17:37:50","title":"AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation","abstract":"Skin lesion segmentation is a critical task in computer-aided diagnosis systems for dermatological diseases. Accurate segmentation of skin lesions from medical images is essential for early detection, diagnosis, and treatment planning. In this paper, we propose a new model for skin lesion segmentation namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone, and integrates advanced components such as Convolutional Block Attention Module (CBAM), Attention Gate, and Selective Kernel Bottleneck. AC-MambaSeg leverages the Vision Mamba framework for efficient feature extraction, while CBAM and Selective Kernel Bottleneck enhance its ability to focus on informative regions and suppress background noise. We evaluate the performance of AC-MambaSeg on diverse datasets of skin lesion images including ISIC-2018 and PH2; then compare it against existing segmentation methods. Our model shows promising potential for improving computer-aided diagnosis systems and facilitating early detection and treatment of dermatological diseases. Our source code will be made available at: https://github.com/vietthanh2710/AC-MambaSeg.","sentences":["Skin lesion segmentation is a critical task in computer-aided diagnosis systems for dermatological diseases.","Accurate segmentation of skin lesions from medical images is essential for early detection, diagnosis, and treatment planning.","In this paper, we propose a new model for skin lesion segmentation namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone, and integrates advanced components such as Convolutional Block Attention Module (CBAM), Attention Gate, and Selective Kernel Bottleneck.","AC-MambaSeg leverages the Vision Mamba framework for efficient feature extraction, while CBAM and Selective Kernel Bottleneck enhance its ability to focus on informative regions and suppress background noise.","We evaluate the performance of AC-MambaSeg on diverse datasets of skin lesion images including ISIC-2018 and PH2; then compare it against existing segmentation methods.","Our model shows promising potential for improving computer-aided diagnosis systems and facilitating early detection and treatment of dermatological diseases.","Our source code will be made available at: https://github.com/vietthanh2710/AC-MambaSeg."],"url":"http://arxiv.org/abs/2405.03011v1","category":"cs.CV"}
{"created":"2024-05-05 17:36:22","title":"High Order Reasoning for Time Critical Recommendation in Evidence-based Medicine","abstract":"In time-critical decisions, human decision-makers can interact with AI-enabled situation-aware software to evaluate many imminent and possible scenarios, retrieve billions of facts, and estimate different outcomes based on trillions of parameters in a fraction of a second. In high-order reasoning, \"what-if\" questions can be used to challenge the assumptions or pre-conditions of the reasoning, \"why-not\" questions can be used to challenge on the method applied in the reasoning, \"so-what\" questions can be used to challenge the purpose of the decision, and \"how-about\" questions can be used to challenge the applicability of the method. When above high-order reasoning questions are applied to assist human decision-making, it can help humans to make time-critical decisions and avoid false-negative or false-positive types of errors. In this paper, we present a model of high-order reasoning to offer recommendations in evidence-based medicine in a time-critical fashion for the applications in ICU. The Large Language Model (LLM) is used in our system. The experiments demonstrated the LLM exhibited optimal performance in the \"What-if\" scenario, achieving a similarity of 88.52% with the treatment plans of human doctors. In the \"Why-not\" scenario, the best-performing model tended to opt for alternative treatment plans in 70% of cases for patients who died after being discharged from the ICU. In the \"So-what\" scenario, the optimal model provided a detailed analysis of the motivation and significance of treatment plans for ICU patients, with its reasoning achieving a similarity of 55.6% with actual diagnostic information. In the \"How-about\" scenario, the top-performing LLM demonstrated a content similarity of 66.5% in designing treatment plans transferring for similar diseases. Meanwhile, LLMs managed to predict the life status of patients after their discharge from the ICU with an accuracy of 70%.","sentences":["In time-critical decisions, human decision-makers can interact with AI-enabled situation-aware software to evaluate many imminent and possible scenarios, retrieve billions of facts, and estimate different outcomes based on trillions of parameters in a fraction of a second.","In high-order reasoning, \"what-if\" questions can be used to challenge the assumptions or pre-conditions of the reasoning, \"why-not\" questions can be used to challenge on the method applied in the reasoning, \"so-what\" questions can be used to challenge the purpose of the decision, and \"how-about\" questions can be used to challenge the applicability of the method.","When above high-order reasoning questions are applied to assist human decision-making, it can help humans to make time-critical decisions and avoid false-negative or false-positive types of errors.","In this paper, we present a model of high-order reasoning to offer recommendations in evidence-based medicine in a time-critical fashion for the applications in ICU.","The Large Language Model (LLM) is used in our system.","The experiments demonstrated the LLM exhibited optimal performance in the \"What-if\" scenario, achieving a similarity of 88.52% with the treatment plans of human doctors.","In the \"Why-not\" scenario, the best-performing model tended to opt for alternative treatment plans in 70% of cases for patients who died after being discharged from the ICU.","In the \"So-what\" scenario, the optimal model provided a detailed analysis of the motivation and significance of treatment plans for ICU patients, with its reasoning achieving a similarity of 55.6% with actual diagnostic information.","In the \"How-about\" scenario, the top-performing LLM demonstrated a content similarity of 66.5% in designing treatment plans transferring for similar diseases.","Meanwhile, LLMs managed to predict the life status of patients after their discharge from the ICU with an accuracy of 70%."],"url":"http://arxiv.org/abs/2405.03010v1","category":"cs.AI"}
{"created":"2024-05-05 17:36:02","title":"Explainable Malware Detection with Tailored Logic Explained Networks","abstract":"Malware detection is a constant challenge in cybersecurity due to the rapid development of new attack techniques. Traditional signature-based approaches struggle to keep pace with the sheer volume of malware samples. Machine learning offers a promising solution, but faces issues of generalization to unseen samples and a lack of explanation for the instances identified as malware. However, human-understandable explanations are especially important in security-critical fields, where understanding model decisions is crucial for trust and legal compliance. While deep learning models excel at malware detection, their black-box nature hinders explainability. Conversely, interpretable models often fall short in performance. To bridge this gap in this application domain, we propose the use of Logic Explained Networks (LENs), which are a recently proposed class of interpretable neural networks providing explanations in the form of First-Order Logic (FOL) rules. This paper extends the application of LENs to the complex domain of malware detection, specifically using the large-scale EMBER dataset. In the experimental results we show that LENs achieve robustness that exceeds traditional interpretable methods and that are rivaling black-box models. Moreover, we introduce a tailored version of LENs that is shown to generate logic explanations with higher fidelity with respect to the model's predictions.","sentences":["Malware detection is a constant challenge in cybersecurity due to the rapid development of new attack techniques.","Traditional signature-based approaches struggle to keep pace with the sheer volume of malware samples.","Machine learning offers a promising solution, but faces issues of generalization to unseen samples and a lack of explanation for the instances identified as malware.","However, human-understandable explanations are especially important in security-critical fields, where understanding model decisions is crucial for trust and legal compliance.","While deep learning models excel at malware detection, their black-box nature hinders explainability.","Conversely, interpretable models often fall short in performance.","To bridge this gap in this application domain, we propose the use of Logic Explained Networks (LENs), which are a recently proposed class of interpretable neural networks providing explanations in the form of First-Order Logic (FOL) rules.","This paper extends the application of LENs to the complex domain of malware detection, specifically using the large-scale EMBER dataset.","In the experimental results we show that LENs achieve robustness that exceeds traditional interpretable methods and that are rivaling black-box models.","Moreover, we introduce a tailored version of LENs that is shown to generate logic explanations with higher fidelity with respect to the model's predictions."],"url":"http://arxiv.org/abs/2405.03009v1","category":"cs.CR"}
{"created":"2024-05-05 17:28:54","title":"On the performativity of SDG classifications in large bibliometric databases","abstract":"Large bibliometric databases, such as Web of Science, Scopus, and OpenAlex, facilitate bibliometric analyses, but are performative, affecting the visibility of scientific outputs and the impact measurement of participating entities. Recently, these databases have taken up the UN's Sustainable Development Goals (SDGs) in their respective classifications, which have been criticised for their diverging nature. This work proposes using the feature of large language models (LLMs) to learn about the \"data bias\" injected by diverse SDG classifications into bibliometric data by exploring five SDGs. We build a LLM that is fine-tuned in parallel by the diverse SDG classifications inscribed into the databases' SDG classifications. Our results show high sensitivity in model architecture, classified publications, fine-tuning process, and natural language generation. The wide arbitrariness at different levels raises concerns about using LLM in research practice.","sentences":["Large bibliometric databases, such as Web of Science, Scopus, and OpenAlex, facilitate bibliometric analyses, but are performative, affecting the visibility of scientific outputs and the impact measurement of participating entities.","Recently, these databases have taken up the UN's Sustainable Development Goals (SDGs) in their respective classifications, which have been criticised for their diverging nature.","This work proposes using the feature of large language models (LLMs) to learn about the \"data bias\" injected by diverse SDG classifications into bibliometric data by exploring five SDGs.","We build a LLM that is fine-tuned in parallel by the diverse SDG classifications inscribed into the databases' SDG classifications.","Our results show high sensitivity in model architecture, classified publications, fine-tuning process, and natural language generation.","The wide arbitrariness at different levels raises concerns about using LLM in research practice."],"url":"http://arxiv.org/abs/2405.03007v1","category":"cs.DL"}
{"created":"2024-05-05 17:27:22","title":"Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints","abstract":"In safe Reinforcement Learning (RL), safety cost is typically defined as a function dependent on the immediate state and actions. In practice, safety constraints can often be non-Markovian due to the insufficient fidelity of state representation, and safety cost may not be known. We therefore address a general setting where safety labels (e.g., safe or unsafe) are associated with state-action trajectories. Our key contributions are: first, we design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety. This safety model is trained using a labeled safety dataset. Second, using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model. Finally, we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance. We rewrite the constrained optimization problem into its dual problem and derive a gradient-based method to dynamically adjust the tradeoff coefficient during training. Our empirical results demonstrate that this approach is highly scalable and able to satisfy sophisticated non-Markovian safety constraints.","sentences":["In safe Reinforcement Learning (RL), safety cost is typically defined as a function dependent on the immediate state and actions.","In practice, safety constraints can often be non-Markovian due to the insufficient fidelity of state representation, and safety cost may not be known.","We therefore address a general setting where safety labels (e.g., safe or unsafe) are associated with state-action trajectories.","Our key contributions are: first, we design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety.","This safety model is trained using a labeled safety dataset.","Second, using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model.","Finally, we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance.","We rewrite the constrained optimization problem into its dual problem and derive a gradient-based method to dynamically adjust the tradeoff coefficient during training.","Our empirical results demonstrate that this approach is highly scalable and able to satisfy sophisticated non-Markovian safety constraints."],"url":"http://arxiv.org/abs/2405.03005v1","category":"cs.LG"}
{"created":"2024-05-05 17:15:24","title":"Parameter-Efficient Fine-Tuning with Discrete Fourier Transform","abstract":"Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}.","sentences":["Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models.","It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models.","In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform.","Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients.","With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$.","Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification.","For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}."],"url":"http://arxiv.org/abs/2405.03003v1","category":"cs.LG"}
{"created":"2024-05-05 17:06:31","title":"MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning","abstract":"Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.","sentences":["Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy.","In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications.","Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs.","Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48% and 11.31%, respectively, without requiring extensive computational resources or sharing data with third parties.","MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods.","Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain."],"url":"http://arxiv.org/abs/2405.03000v1","category":"cs.CL"}
{"created":"2024-05-05 16:45:46","title":"RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification","abstract":"Recent advancements in AI have democratized its deployment as a healthcare assistant. While pretrained models from large-scale visual and audio datasets have demonstrably generalized to this task, surprisingly, no studies have explored pretrained speech models, which, as human-originated sounds, intuitively would share closer resemblance to lung sounds. This paper explores the efficacy of pretrained speech models for respiratory sound classification. We find that there is a characterization gap between speech and lung sound samples, and to bridge this gap, data augmentation is essential. However, the most widely used augmentation technique for audio and speech, SpecAugment, requires 2-dimensional spectrogram format and cannot be applied to models pretrained on speech waveforms. To address this, we propose RepAugment, an input-agnostic representation-level augmentation technique that outperforms SpecAugment, but is also suitable for respiratory sound classification with waveform pretrained models. Experimental results show that our approach outperforms the SpecAugment, demonstrating a substantial improvement in the accuracy of minority disease classes, reaching up to 7.14%.","sentences":["Recent advancements in AI have democratized its deployment as a healthcare assistant.","While pretrained models from large-scale visual and audio datasets have demonstrably generalized to this task, surprisingly, no studies have explored pretrained speech models, which, as human-originated sounds, intuitively would share closer resemblance to lung sounds.","This paper explores the efficacy of pretrained speech models for respiratory sound classification.","We find that there is a characterization gap between speech and lung sound samples, and to bridge this gap, data augmentation is essential.","However, the most widely used augmentation technique for audio and speech, SpecAugment, requires 2-dimensional spectrogram format and cannot be applied to models pretrained on speech waveforms.","To address this, we propose RepAugment, an input-agnostic representation-level augmentation technique that outperforms SpecAugment, but is also suitable for respiratory sound classification with waveform pretrained models.","Experimental results show that our approach outperforms the SpecAugment, demonstrating a substantial improvement in the accuracy of minority disease classes, reaching up to 7.14%."],"url":"http://arxiv.org/abs/2405.02996v1","category":"cs.SD"}
{"created":"2024-05-05 16:11:06","title":"Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education","abstract":"This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.","sentences":["This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform.","We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75).","This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters.","The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery."],"url":"http://arxiv.org/abs/2405.02985v1","category":"cs.CL"}
{"created":"2024-05-05 16:07:23","title":"E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods","abstract":"This study introduces the continuous Educational Turkish Sign Language (E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th, and 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and includes performances from 11 signers. Turkish, an agglutinative language, poses unique challenges for sign language translation, particularly with a vocabulary where 64% are singleton words and 85% are rare words, appearing less than five times. We developed two baseline models to address these challenges: the Pose to Text Transformer (P2T-T) and the Graph Neural Network based Transformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and 3.28% BLEU-4 score, presenting a significant challenge compared to existing benchmarks. The P2T-T model, while demonstrating slightly lower performance in BLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we benchmarked our model using the well-known PHOENIX-Weather 2014T dataset to validate our approach.","sentences":["This study introduces the continuous Educational Turkish Sign Language (E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th, and 8th grades.","The dataset comprises 1,410 videos totaling nearly 24 hours and includes performances from 11 signers.","Turkish, an agglutinative language, poses unique challenges for sign language translation, particularly with a vocabulary where 64% are singleton words and 85% are rare words, appearing less than five times.","We developed two baseline models to address these challenges: the Pose to Text Transformer (P2T-T) and the Graph Neural Network based Transformer (GNN-T) models.","The GNN-T model achieved 19.13% BLEU-1 score and 3.28% BLEU-4 score, presenting a significant challenge compared to existing benchmarks.","The P2T-T model, while demonstrating slightly lower performance in BLEU scores, achieved a higher ROUGE-L score of 22.09%.","Additionally, we benchmarked our model using the well-known PHOENIX-Weather 2014T dataset to validate our approach."],"url":"http://arxiv.org/abs/2405.02984v1","category":"cs.CL"}
{"created":"2024-05-05 15:59:22","title":"Self-Organized Construction by Minimal Surprise","abstract":"For the robots to achieve a desired behavior, we can program them directly, train them, or give them an innate driver that makes the robots themselves desire the targeted behavior. With the minimal surprise approach, we implant in our robots the desire to make their world predictable. Here, we apply minimal surprise to collective construction. Simulated robots push blocks in a 2D torus grid world. In two variants of our experiment we either allow for emergent behaviors or predefine the expected environment of the robots. In either way, we evolve robot behaviors that move blocks to structure their environment and make it more predictable. The resulting controllers can be applied in collective construction by robots.","sentences":["For the robots to achieve a desired behavior, we can program them directly, train them, or give them an innate driver that makes the robots themselves desire the targeted behavior.","With the minimal surprise approach, we implant in our robots the desire to make their world predictable.","Here, we apply minimal surprise to collective construction.","Simulated robots push blocks in a 2D torus grid world.","In two variants of our experiment we either allow for emergent behaviors or predefine the expected environment of the robots.","In either way, we evolve robot behaviors that move blocks to structure their environment and make it more predictable.","The resulting controllers can be applied in collective construction by robots."],"url":"http://arxiv.org/abs/2405.02980v1","category":"cs.RO"}
{"created":"2024-05-05 15:31:47","title":"Multi-Agent RL-Based Industrial AIGC Service Offloading over Wireless Edge Networks","abstract":"Currently, the generative model has garnered considerable attention due to its application in addressing the challenge of scarcity of abnormal samples in the industrial Internet of Things (IoT). However, challenges persist regarding the edge deployment of generative models and the optimization of joint edge AI-generated content (AIGC) tasks. In this paper, we focus on the edge optimization of AIGC task execution and propose GMEL, a generative model-driven industrial AIGC collaborative edge learning framework. This framework aims to facilitate efficient few-shot learning by leveraging realistic sample synthesis and edge-based optimization capabilities. First, a multi-task AIGC computational offloading model is presented to ensure the efficient execution of heterogeneous AIGC tasks on edge servers. Then, we propose an attention-enhanced multi-agent reinforcement learning (AMARL) algorithm aimed at refining offloading policies within the IoT system, thereby supporting generative model-driven edge learning. Finally, our experimental results demonstrate the effectiveness of the proposed algorithm in optimizing the total system latency of the edge-based AIGC task completion.","sentences":["Currently, the generative model has garnered considerable attention due to its application in addressing the challenge of scarcity of abnormal samples in the industrial Internet of Things (IoT).","However, challenges persist regarding the edge deployment of generative models and the optimization of joint edge AI-generated content (AIGC) tasks.","In this paper, we focus on the edge optimization of AIGC task execution and propose GMEL, a generative model-driven industrial AIGC collaborative edge learning framework.","This framework aims to facilitate efficient few-shot learning by leveraging realistic sample synthesis and edge-based optimization capabilities.","First, a multi-task AIGC computational offloading model is presented to ensure the efficient execution of heterogeneous AIGC tasks on edge servers.","Then, we propose an attention-enhanced multi-agent reinforcement learning (AMARL) algorithm aimed at refining offloading policies within the IoT system, thereby supporting generative model-driven edge learning.","Finally, our experimental results demonstrate the effectiveness of the proposed algorithm in optimizing the total system latency of the edge-based AIGC task completion."],"url":"http://arxiv.org/abs/2405.02972v1","category":"cs.NI"}
{"created":"2024-05-05 15:27:56","title":"Towards a Flexible and High-Fidelity Approach to Distributed DNN Training Emulation","abstract":"We propose NeuronaBox, a flexible, user-friendly, and high-fidelity approach to emulate DNN training workloads. We argue that to accurately observe performance, it is possible to execute the training workload on a subset of real nodes and emulate the networked execution environment along with the collective communication operations. Initial results from a proof-of-concept implementation show that NeuronaBox replicates the behavior of actual systems with high accuracy, with an error margin of less than 1% between the emulated measurements and the real system.","sentences":["We propose NeuronaBox, a flexible, user-friendly, and high-fidelity approach to emulate DNN training workloads.","We argue that to accurately observe performance, it is possible to execute the training workload on a subset of real nodes and emulate the networked execution environment along with the collective communication operations.","Initial results from a proof-of-concept implementation show that NeuronaBox replicates the behavior of actual systems with high accuracy, with an error margin of less than 1% between the emulated measurements and the real system."],"url":"http://arxiv.org/abs/2405.02969v1","category":"cs.LG"}
{"created":"2024-05-05 15:27:05","title":"CoverLib: Classifiers-equipped Experience Library by Iterative Problem Distribution Coverage Maximization for Domain-tuned Motion Planning","abstract":"Library-based methods are known to be very effective for fast motion planning by adapting an experience retrieved from a precomputed library. This article presents CoverLib, a principled approach for constructing and utilizing such a library. CoverLib iteratively adds an experience-classifier-pair to the library, where each classifier corresponds to an adaptable region of the experience within the problem space. This iterative process is an active procedure, as it selects the next experience based on its ability to effectively cover the uncovered region. During the query phase, these classifiers are utilized to select an experience that is expected to be adaptable for a given problem. Experimental results demonstrate that CoverLib effectively mitigates the trade-off between plannability and speed observed in global (e.g. sampling-based) and local (e.g. optimization-based) methods. As a result, it achieves both fast planning and high success rates over the problem domain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib seamlessly integrates with various adaptation methods, including nonlinear programming-based and sampling-based algorithms.","sentences":["Library-based methods are known to be very effective for fast motion planning by adapting an experience retrieved from a precomputed library.","This article presents CoverLib, a principled approach for constructing and utilizing such a library.","CoverLib iteratively adds an experience-classifier-pair to the library, where each classifier corresponds to an adaptable region of the experience within the problem space.","This iterative process is an active procedure, as it selects the next experience based on its ability to effectively cover the uncovered region.","During the query phase, these classifiers are utilized to select an experience that is expected to be adaptable for a given problem.","Experimental results demonstrate that CoverLib effectively mitigates the trade-off between plannability and speed observed in global (e.g. sampling-based) and local (e.g. optimization-based) methods.","As a result, it achieves both fast planning and high success rates over the problem domain.","Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib seamlessly integrates with various adaptation methods, including nonlinear programming-based and sampling-based algorithms."],"url":"http://arxiv.org/abs/2405.02968v1","category":"cs.RO"}
{"created":"2024-05-05 15:20:36","title":"Robust Collaborative Perception without External Localization and Clock Devices","abstract":"A consistent spatial-temporal coordination across multiple agents is fundamental for collaborative perception, which seeks to improve perception abilities through information exchange among agents. To achieve this spatial-temporal alignment, traditional methods depend on external devices to provide localization and clock signals. However, hardware-generated signals could be vulnerable to noise and potentially malicious attack, jeopardizing the precision of spatial-temporal alignment. Rather than relying on external hardwares, this work proposes a novel approach: aligning by recognizing the inherent geometric patterns within the perceptual data of various agents. Following this spirit, we propose a robust collaborative perception system that operates independently of external localization and clock devices. The key module of our system,~\\emph{FreeAlign}, constructs a salient object graph for each agent based on its detected boxes and uses a graph neural network to identify common subgraphs between agents, leading to accurate relative pose and time. We validate \\emph{FreeAlign} on both real-world and simulated datasets. The results show that, the ~\\emph{FreeAlign} empowered robust collaborative perception system perform comparably to systems relying on precise localization and clock devices.","sentences":["A consistent spatial-temporal coordination across multiple agents is fundamental for collaborative perception, which seeks to improve perception abilities through information exchange among agents.","To achieve this spatial-temporal alignment, traditional methods depend on external devices to provide localization and clock signals.","However, hardware-generated signals could be vulnerable to noise and potentially malicious attack, jeopardizing the precision of spatial-temporal alignment.","Rather than relying on external hardwares, this work proposes a novel approach: aligning by recognizing the inherent geometric patterns within the perceptual data of various agents.","Following this spirit, we propose a robust collaborative perception system that operates independently of external localization and clock devices.","The key module of our system,~\\emph{FreeAlign}, constructs a salient object graph for each agent based on its detected boxes and uses a graph neural network to identify common subgraphs between agents, leading to accurate relative pose and time.","We validate \\emph{FreeAlign} on both real-world and simulated datasets.","The results show that, the ~\\emph{FreeAlign} empowered robust collaborative perception system perform comparably to systems relying on precise localization and clock devices."],"url":"http://arxiv.org/abs/2405.02965v1","category":"cs.AI"}
{"created":"2024-05-05 14:53:51","title":"Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents","abstract":"In this paper, we introduce a simulacrum of hospital called Agent Hospital that simulates the entire process of treating illness. All patients, nurses, and doctors are autonomous agents powered by large language models (LLMs). Our central goal is to enable a doctor agent to learn how to treat illness within the simulacrum. To do so, we propose a method called MedAgent-Zero. As the simulacrum can simulate disease onset and progression based on knowledge bases and LLMs, doctor agents can keep accumulating experience from both successful and unsuccessful cases. Simulation experiments show that the treatment performance of doctor agents consistently improves on various tasks. More interestingly, the knowledge the doctor agents have acquired in Agent Hospital is applicable to real-world medicare benchmarks. After treating around ten thousand patients (real-world doctors may take over two years), the evolved doctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the MedQA dataset that covers major respiratory diseases. This work paves the way for advancing the applications of LLM-powered agent techniques in medical scenarios.","sentences":["In this paper, we introduce a simulacrum of hospital called Agent Hospital that simulates the entire process of treating illness.","All patients, nurses, and doctors are autonomous agents powered by large language models (LLMs).","Our central goal is to enable a doctor agent to learn how to treat illness within the simulacrum.","To do so, we propose a method called MedAgent-Zero.","As the simulacrum can simulate disease onset and progression based on knowledge bases and LLMs, doctor agents can keep accumulating experience from both successful and unsuccessful cases.","Simulation experiments show that the treatment performance of doctor agents consistently improves on various tasks.","More interestingly, the knowledge the doctor agents have acquired in Agent Hospital is applicable to real-world medicare benchmarks.","After treating around ten thousand patients (real-world doctors may take over two years), the evolved doctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the MedQA dataset that covers major respiratory diseases.","This work paves the way for advancing the applications of LLM-powered agent techniques in medical scenarios."],"url":"http://arxiv.org/abs/2405.02957v1","category":"cs.AI"}
{"created":"2024-05-05 14:07:23","title":"Design, analysis, and manufacturing of a glass-plastic hybrid minimalist aspheric panoramic annular lens","abstract":"We propose a high-performance glass-plastic hybrid minimalist aspheric panoramic annular lens (ASPAL) to solve several major limitations of the traditional panoramic annular lens (PAL), such as large size, high weight, and complex system. The field of view (FoV) of the ASPAL is 360{\\deg}x(35{\\deg}~110{\\deg}) and the imaging quality is close to the diffraction limit. This large FoV ASPAL is composed of only 4 lenses. Moreover, we establish a physical structure model of PAL using the ray tracing method and study the influence of its physical parameters on compactness ratio. In addition, for the evaluation of local tolerances of annular surfaces, we propose a tolerance analysis method suitable for ASPAL. This analytical method can effectively analyze surface irregularities on annular surfaces and provide clear guidance on manufacturing tolerances for ASPAL. Benefiting from high-precision glass molding and injection molding aspheric lens manufacturing techniques, we finally manufactured 20 ASPALs in small batches. The weight of an ASPAL prototype is only 8.5 g. Our framework provides promising insights for the application of panoramic systems in space and weight-constrained environmental sensing scenarios such as intelligent security, micro-UAVs, and micro-robots.","sentences":["We propose a high-performance glass-plastic hybrid minimalist aspheric panoramic annular lens (ASPAL) to solve several major limitations of the traditional panoramic annular lens (PAL), such as large size, high weight, and complex system.","The field of view (FoV) of the ASPAL is 360{\\deg}x(35{\\deg}~110{\\deg}) and the imaging quality is close to the diffraction limit.","This large FoV ASPAL is composed of only 4 lenses.","Moreover, we establish a physical structure model of PAL using the ray tracing method and study the influence of its physical parameters on compactness ratio.","In addition, for the evaluation of local tolerances of annular surfaces, we propose a tolerance analysis method suitable for ASPAL.","This analytical method can effectively analyze surface irregularities on annular surfaces and provide clear guidance on manufacturing tolerances for ASPAL.","Benefiting from high-precision glass molding and injection molding aspheric lens manufacturing techniques, we finally manufactured 20 ASPALs in small batches.","The weight of an ASPAL prototype is only 8.5 g.","Our framework provides promising insights for the application of panoramic systems in space and weight-constrained environmental sensing scenarios such as intelligent security, micro-UAVs, and micro-robots."],"url":"http://arxiv.org/abs/2405.02942v1","category":"physics.optics"}
{"created":"2024-05-05 13:56:12","title":"On the tractability of SHAP explanations under Markovian distributions","abstract":"Thanks to its solid theoretical foundation, the SHAP framework is arguably one the most widely utilized frameworks for local explainability of ML models. Despite its popularity, its exact computation is known to be very challenging, proven to be NP-Hard in various configurations. Recent works have unveiled positive complexity results regarding the computation of the SHAP score for specific model families, encompassing decision trees, random forests, and some classes of boolean circuits. Yet, all these positive results hinge on the assumption of feature independence, often simplistic in real-world scenarios. In this article, we investigate the computational complexity of the SHAP score by relaxing this assumption and introducing a Markovian perspective. We show that, under the Markovian assumption, computing the SHAP score for the class of Weighted automata, Disjoint DNFs and Decision Trees can be performed in polynomial time, offering a first positive complexity result for the problem of SHAP score computation that transcends the limitations of the feature independence assumption.","sentences":["Thanks to its solid theoretical foundation, the SHAP framework is arguably one the most widely utilized frameworks for local explainability of ML models.","Despite its popularity, its exact computation is known to be very challenging, proven to be NP-Hard in various configurations.","Recent works have unveiled positive complexity results regarding the computation of the SHAP score for specific model families, encompassing decision trees, random forests, and some classes of boolean circuits.","Yet, all these positive results hinge on the assumption of feature independence, often simplistic in real-world scenarios.","In this article, we investigate the computational complexity of the SHAP score by relaxing this assumption and introducing a Markovian perspective.","We show that, under the Markovian assumption, computing the SHAP score for the class of Weighted automata, Disjoint DNFs and Decision Trees can be performed in polynomial time, offering a first positive complexity result for the problem of SHAP score computation that transcends the limitations of the feature independence assumption."],"url":"http://arxiv.org/abs/2405.02936v1","category":"cs.LG"}
{"created":"2024-05-05 13:15:11","title":"Unified Dynamic Scanpath Predictors Outperform Individually Trained Models","abstract":"Previous research on scanpath prediction has mainly focused on group models, disregarding the fact that the scanpaths and attentional behaviors of individuals are diverse. The disregard of these differences is especially detrimental to social human-robot interaction, whereby robots commonly emulate human gaze based on heuristics or predefined patterns. However, human gaze patterns are heterogeneous and varying behaviors can significantly affect the outcomes of such human-robot interactions. To fill this gap, we developed a deep learning-based social cue integration model for saliency prediction to instead predict scanpaths in videos. Our model learned scanpaths by recursively integrating fixation history and social cues through a gating mechanism and sequential attention. We evaluated our approach on gaze datasets of dynamic social scenes, observed under the free-viewing condition. The introduction of fixation history into our models makes it possible to train a single unified model rather than the resource-intensive approach of training individual models for each set of scanpaths. We observed that the late neural integration approach surpasses early fusion when training models on a large dataset, in comparison to a smaller dataset with a similar distribution. Results also indicate that a single unified model, trained on all the observers' scanpaths, performs on par or better than individually trained models. We hypothesize that this outcome is a result of the group saliency representations instilling universal attention in the model, while the supervisory signal guides it to learn personalized attentional behaviors, providing the unified model a benefit over individual models due to its implicit representation of universal attention.","sentences":["Previous research on scanpath prediction has mainly focused on group models, disregarding the fact that the scanpaths and attentional behaviors of individuals are diverse.","The disregard of these differences is especially detrimental to social human-robot interaction, whereby robots commonly emulate human gaze based on heuristics or predefined patterns.","However, human gaze patterns are heterogeneous and varying behaviors can significantly affect the outcomes of such human-robot interactions.","To fill this gap, we developed a deep learning-based social cue integration model for saliency prediction to instead predict scanpaths in videos.","Our model learned scanpaths by recursively integrating fixation history and social cues through a gating mechanism and sequential attention.","We evaluated our approach on gaze datasets of dynamic social scenes, observed under the free-viewing condition.","The introduction of fixation history into our models makes it possible to train a single unified model rather than the resource-intensive approach of training individual models for each set of scanpaths.","We observed that the late neural integration approach surpasses early fusion when training models on a large dataset, in comparison to a smaller dataset with a similar distribution.","Results also indicate that a single unified model, trained on all the observers' scanpaths, performs on par or better than individually trained models.","We hypothesize that this outcome is a result of the group saliency representations instilling universal attention in the model, while the supervisory signal guides it to learn personalized attentional behaviors, providing the unified model a benefit over individual models due to its implicit representation of universal attention."],"url":"http://arxiv.org/abs/2405.02929v1","category":"cs.CV"}
{"created":"2024-05-05 13:10:16","title":"Project Hephaistos - II. Dyson sphere candidates from Gaia DR3, 2MASS, and WISE","abstract":"The search for extraterrestrial intelligence is currently being pursued using multiple techniques and in different wavelength bands. Dyson spheres, megastructures that could be constructed by advanced civilizations to harness the radiation energy of their host stars, represent a potential technosignature, that in principle may be hiding in public data already collected as part of large astronomical surveys. In this study, we present a comprehensive search for partial Dyson spheres by analyzing optical and infrared observations from Gaia, 2MASS, and WISE. We develop a pipeline that employs multiple filters to identify potential candidates and reject interlopers in a sample of five million objects, which incorporates a convolutional neural network to help identify confusion in WISE data. Finally, the pipeline identifies 7 candidates deserving of further analysis. All of these objects are M-dwarfs, for which astrophysical phenomena cannot easily account for the observed infrared excess emission.","sentences":["The search for extraterrestrial intelligence is currently being pursued using multiple techniques and in different wavelength bands.","Dyson spheres, megastructures that could be constructed by advanced civilizations to harness the radiation energy of their host stars, represent a potential technosignature, that in principle may be hiding in public data already collected as part of large astronomical surveys.","In this study, we present a comprehensive search for partial Dyson spheres by analyzing optical and infrared observations from Gaia, 2MASS, and WISE.","We develop a pipeline that employs multiple filters to identify potential candidates and reject interlopers in a sample of five million objects, which incorporates a convolutional neural network to help identify confusion in WISE data.","Finally, the pipeline identifies 7 candidates deserving of further analysis.","All of these objects are M-dwarfs, for which astrophysical phenomena cannot easily account for the observed infrared excess emission."],"url":"http://arxiv.org/abs/2405.02927v1","category":"astro-ph.SR"}
{"created":"2024-05-05 12:42:09","title":"Simulation of Optical Tactile Sensors Supporting Slip and Rotation using Path Tracing and IMPM","abstract":"Optical tactile sensors are extensively utilized in intelligent robot manipulation due to their ability to acquire high-resolution tactile information at a lower cost. However, achieving adequate reality and versatility in simulating optical tactile sensors is challenging. In this paper, we propose a simulation method and validate its effectiveness through experiments. We utilize path tracing for image rendering, achieving higher similarity to real data than the baseline method in simulating pressing scenarios. Additionally, we apply the improved Material Point Method(IMPM) algorithm to simulate the relative rest between the object and the elastomer surface when the object is in motion, enabling more accurate simulation of complex manipulations such as slip and rotation.","sentences":["Optical tactile sensors are extensively utilized in intelligent robot manipulation due to their ability to acquire high-resolution tactile information at a lower cost.","However, achieving adequate reality and versatility in simulating optical tactile sensors is challenging.","In this paper, we propose a simulation method and validate its effectiveness through experiments.","We utilize path tracing for image rendering, achieving higher similarity to real data than the baseline method in simulating pressing scenarios.","Additionally, we apply the improved Material Point Method(IMPM) algorithm to simulate the relative rest between the object and the elastomer surface when the object is in motion, enabling more accurate simulation of complex manipulations such as slip and rotation."],"url":"http://arxiv.org/abs/2405.02914v1","category":"cs.RO"}
{"created":"2024-05-05 12:38:10","title":"Multimodal Sense-Informed Prediction of 3D Human Motions","abstract":"Predicting future human pose is a fundamental application for machine intelligence, which drives robots to plan their behavior and paths ahead of time to seamlessly accomplish human-robot collaboration in real-world 3D scenarios. Despite encouraging results, existing approaches rarely consider the effects of the external scene on the motion sequence, leading to pronounced artifacts and physical implausibilities in the predictions. To address this limitation, this work introduces a novel multi-modal sense-informed motion prediction approach, which conditions high-fidelity generation on two modal information: external 3D scene, and internal human gaze, and is able to recognize their salience for future human activity. Furthermore, the gaze information is regarded as the human intention, and combined with both motion and scene features, we construct a ternary intention-aware attention to supervise the generation to match where the human wants to reach. Meanwhile, we introduce semantic coherence-aware attention to explicitly distinguish the salient point clouds and the underlying ones, to ensure a reasonable interaction of the generated sequence with the 3D scene. On two real-world benchmarks, the proposed method achieves state-of-the-art performance both in 3D human pose and trajectory prediction.","sentences":["Predicting future human pose is a fundamental application for machine intelligence, which drives robots to plan their behavior and paths ahead of time to seamlessly accomplish human-robot collaboration in real-world 3D scenarios.","Despite encouraging results, existing approaches rarely consider the effects of the external scene on the motion sequence, leading to pronounced artifacts and physical implausibilities in the predictions.","To address this limitation, this work introduces a novel multi-modal sense-informed motion prediction approach, which conditions high-fidelity generation on two modal information: external 3D scene, and internal human gaze, and is able to recognize their salience for future human activity.","Furthermore, the gaze information is regarded as the human intention, and combined with both motion and scene features, we construct a ternary intention-aware attention to supervise the generation to match where the human wants to reach.","Meanwhile, we introduce semantic coherence-aware attention to explicitly distinguish the salient point clouds and the underlying ones, to ensure a reasonable interaction of the generated sequence with the 3D scene.","On two real-world benchmarks, the proposed method achieves state-of-the-art performance both in 3D human pose and trajectory prediction."],"url":"http://arxiv.org/abs/2405.02911v1","category":"cs.CV"}
{"created":"2024-05-05 10:52:09","title":"Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English","abstract":"People communicate in more than 7,000 languages around the world, with around 780 languages spoken in India alone. Despite this linguistic diversity, research on Sentiment Analysis has predominantly focused on English text data, resulting in a disproportionate availability of sentiment resources for English. This paper examines the performance of transformer models in Sentiment Analysis tasks across multilingual datasets and text that has undergone machine translation. By comparing the effectiveness of these models in different linguistic contexts, we gain insights into their performance variations and potential implications for sentiment analysis across diverse languages. We also discuss the shortcomings and potential for future work towards the end.","sentences":["People communicate in more than 7,000 languages around the world, with around 780 languages spoken in India alone.","Despite this linguistic diversity, research on Sentiment Analysis has predominantly focused on English text data, resulting in a disproportionate availability of sentiment resources for English.","This paper examines the performance of transformer models in Sentiment Analysis tasks across multilingual datasets and text that has undergone machine translation.","By comparing the effectiveness of these models in different linguistic contexts, we gain insights into their performance variations and potential implications for sentiment analysis across diverse languages.","We also discuss the shortcomings and potential for future work towards the end."],"url":"http://arxiv.org/abs/2405.02887v1","category":"cs.CL"}
{"created":"2024-05-05 10:28:06","title":"FedConPE: Efficient Federated Conversational Bandits with Heterogeneous Clients","abstract":"Conversational recommender systems have emerged as a potent solution for efficiently eliciting user preferences. These systems interactively present queries associated with \"key terms\" to users and leverage user feedback to estimate user preferences more efficiently. Nonetheless, most existing algorithms adopt a centralized approach. In this paper, we introduce FedConPE, a phase elimination-based federated conversational bandit algorithm, where $M$ agents collaboratively solve a global contextual linear bandit problem with the help of a central server while ensuring secure data management. To effectively coordinate all the clients and aggregate their collected data, FedConPE uses an adaptive approach to construct key terms that minimize uncertainty across all dimensions in the feature space. Furthermore, compared with existing federated linear bandit algorithms, FedConPE offers improved computational and communication efficiency as well as enhanced privacy protections. Our theoretical analysis shows that FedConPE is minimax near-optimal in terms of cumulative regret. We also establish upper bounds for communication costs and conversation frequency. Comprehensive evaluations demonstrate that FedConPE outperforms existing conversational bandit algorithms while using fewer conversations.","sentences":["Conversational recommender systems have emerged as a potent solution for efficiently eliciting user preferences.","These systems interactively present queries associated with \"key terms\" to users and leverage user feedback to estimate user preferences more efficiently.","Nonetheless, most existing algorithms adopt a centralized approach.","In this paper, we introduce FedConPE, a phase elimination-based federated conversational bandit algorithm, where $M$ agents collaboratively solve a global contextual linear bandit problem with the help of a central server while ensuring secure data management.","To effectively coordinate all the clients and aggregate their collected data, FedConPE uses an adaptive approach to construct key terms that minimize uncertainty across all dimensions in the feature space.","Furthermore, compared with existing federated linear bandit algorithms, FedConPE offers improved computational and communication efficiency as well as enhanced privacy protections.","Our theoretical analysis shows that FedConPE is minimax near-optimal in terms of cumulative regret.","We also establish upper bounds for communication costs and conversation frequency.","Comprehensive evaluations demonstrate that FedConPE outperforms existing conversational bandit algorithms while using fewer conversations."],"url":"http://arxiv.org/abs/2405.02881v1","category":"cs.LG"}
{"created":"2024-05-05 10:27:03","title":"Blending Distributed NeRFs with Tri-stage Robust Pose Optimization","abstract":"Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF.","sentences":["Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity.","However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision.","These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage.","In this paper, we present a distributed NeRF system with tri-stage pose optimization.","In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy.","In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization.","On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems.","In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization.","After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios.","Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF."],"url":"http://arxiv.org/abs/2405.02880v1","category":"cs.CV"}
{"created":"2024-05-05 09:35:44","title":"Non cooperative Liquidity Games and their application to bond market trading","abstract":"We present a new type of game, the Liquidity Game. We draw inspiration from the UK government bond market and apply game theoretic approaches to its analysis. In Liquidity Games, market participants (agents) use non-cooperative games where the players' utility is directly defined by the liquidity of the game itself, offering a paradigm shift in our understanding of market dynamics. Each player's utility is intricately linked to the liquidity generated within the game, making the utility endogenous and dynamic. Players are not just passive recipients of utility based on external factors but active participants whose strategies and actions collectively shape and are shaped by the liquidity of the market. This reflexivity introduces a level of complexity and realism previously unattainable in conventional models.   We apply Liquidity Game theoretic approaches to a simple UK bond market interaction and present results for market design and strategic behavior of participants. We tackle one of the largest issues within this mechanism, namely what strategy should market makers utilize when uncertain about the type of market maker they are interacting with, and what structure might regulators wish to see.","sentences":["We present a new type of game, the Liquidity Game.","We draw inspiration from the UK government bond market and apply game theoretic approaches to its analysis.","In Liquidity Games, market participants (agents) use non-cooperative games where the players' utility is directly defined by the liquidity of the game itself, offering a paradigm shift in our understanding of market dynamics.","Each player's utility is intricately linked to the liquidity generated within the game, making the utility endogenous and dynamic.","Players are not just passive recipients of utility based on external factors but active participants whose strategies and actions collectively shape and are shaped by the liquidity of the market.","This reflexivity introduces a level of complexity and realism previously unattainable in conventional models.   ","We apply Liquidity Game theoretic approaches to a simple UK bond market interaction and present results for market design and strategic behavior of participants.","We tackle one of the largest issues within this mechanism, namely what strategy should market makers utilize when uncertain about the type of market maker they are interacting with, and what structure might regulators wish to see."],"url":"http://arxiv.org/abs/2405.02865v1","category":"cs.GT"}
{"created":"2024-05-05 09:20:38","title":"Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models","abstract":"We introduce LexBench, a comprehensive evaluation suite enabled to test language models (LMs) on ten semantic phrase processing tasks. Unlike prior studies, it is the first work to propose a framework from the comparative perspective to model the general semantic phrase (i.e., lexical collocation) and three fine-grained semantic phrases, including idiomatic expression, noun compound, and verbal construction. Thanks to \\ourbenchmark, we assess the performance of 15 LMs across model architectures and parameter scales in classification, extraction, and interpretation tasks. Through the experiments, we first validate the scaling law and find that, as expected, large models excel better than the smaller ones in most tasks. Second, we investigate further through the scaling semantic relation categorization and find that few-shot LMs still lag behind vanilla fine-tuned models in the task. Third, through human evaluation, we find that the performance of strong models is comparable to the human level regarding semantic phrase processing. Our benchmarking findings can serve future research aiming to improve the generic capability of LMs on semantic phrase comprehension. Our source code and data are available at https://github.com/jacklanda/LexBench","sentences":["We introduce LexBench, a comprehensive evaluation suite enabled to test language models (LMs) on ten semantic phrase processing tasks.","Unlike prior studies, it is the first work to propose a framework from the comparative perspective to model the general semantic phrase (i.e., lexical collocation) and three fine-grained semantic phrases, including idiomatic expression, noun compound, and verbal construction.","Thanks to \\ourbenchmark, we assess the performance of 15 LMs across model architectures and parameter scales in classification, extraction, and interpretation tasks.","Through the experiments, we first validate the scaling law and find that, as expected, large models excel better than the smaller ones in most tasks.","Second, we investigate further through the scaling semantic relation categorization and find that few-shot LMs still lag behind vanilla fine-tuned models in the task.","Third, through human evaluation, we find that the performance of strong models is comparable to the human level regarding semantic phrase processing.","Our benchmarking findings can serve future research aiming to improve the generic capability of LMs on semantic phrase comprehension.","Our source code and data are available at https://github.com/jacklanda/LexBench"],"url":"http://arxiv.org/abs/2405.02861v1","category":"cs.CL"}
{"created":"2024-05-05 08:43:07","title":"Halfway Escape Optimization: A Quantum-Inspired Solution for Complex Optimization Problems","abstract":"This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a novel quantum-inspired metaheuristic designed to address complex optimization problems characterized by rugged landscapes and high-dimensionality with an efficient convergence rate. The study presents a comprehensive comparative evaluation of HEO's performance against established optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating complex optimization landscapes and providing valuable insights into its performance. The simple test of HEO in Traveling Salesman Problem (TSP) also infers its feasibility in real-time applications.","sentences":["This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a novel quantum-inspired metaheuristic designed to address complex optimization problems characterized by rugged landscapes and high-dimensionality with an efficient convergence rate.","The study presents a comprehensive comparative evaluation of HEO's performance against established optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO).","The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating complex optimization landscapes and providing valuable insights into its performance.","The simple test of HEO in Traveling Salesman Problem (TSP) also infers its feasibility in real-time applications."],"url":"http://arxiv.org/abs/2405.02850v1","category":"cs.NE"}
{"created":"2024-05-05 08:42:20","title":"Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study","abstract":"Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance. Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability. We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods. This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as \"over-the-counter\" (OTC) trading, and commonly occurring between \"market makers\". The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance. The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model. We explore the implications of market rigidity on market structure and consider the element of stability, in market design. This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications.","sentences":["Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance.","Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability.","We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods.","This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as \"over-the-counter\" (OTC) trading, and commonly occurring between \"market makers\".","The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance.","The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model.","We explore the implications of market rigidity on market structure and consider the element of stability, in market design.","This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications."],"url":"http://arxiv.org/abs/2405.02849v1","category":"q-fin.CP"}
{"created":"2024-05-05 08:40:22","title":"Responsible AI: Portraits with Intelligent Bibliometrics","abstract":"Shifting the focus from principles to practical implementation, responsible artificial intelligence (AI) has garnered considerable attention across academia, industry, and society at large. Despite being in its nascent stages, this emerging field grapples with nebulous concepts and intricate knowledge frameworks. By analyzing three prevailing concepts - explainable AI, trustworthy AI, and ethical AI, this study defined responsible AI and identified its core principles. Methodologically, this study successfully demonstrated the implementation of leveraging AI's capabilities into bibliometrics for enhanced knowledge discovery and the cross-validation of experimentally examined models with domain insights. Empirically, this study investigated 17,799 research articles contributed by the AI community since 2015. This involves recognizing key technological players and their relationships, unveiling the topical landscape and hierarchy of responsible AI, charting its evolution, and elucidating the interplay between the responsibility principles and primary AI techniques. An analysis of a core cohort comprising 380 articles from multiple disciplines captures the most recent advancements in responsible AI. As one of the pioneering bibliometric studies dedicated to exploring responsible AI, this study will provide comprehensive macro-level insights, enhancing the understanding of responsible AI while furnishing valuable knowledge support for AI regulation and governance initiatives.","sentences":["Shifting the focus from principles to practical implementation, responsible artificial intelligence (AI) has garnered considerable attention across academia, industry, and society at large.","Despite being in its nascent stages, this emerging field grapples with nebulous concepts and intricate knowledge frameworks.","By analyzing three prevailing concepts - explainable AI, trustworthy AI, and ethical AI, this study defined responsible AI and identified its core principles.","Methodologically, this study successfully demonstrated the implementation of leveraging AI's capabilities into bibliometrics for enhanced knowledge discovery and the cross-validation of experimentally examined models with domain insights.","Empirically, this study investigated 17,799 research articles contributed by the AI community since 2015.","This involves recognizing key technological players and their relationships, unveiling the topical landscape and hierarchy of responsible AI, charting its evolution, and elucidating the interplay between the responsibility principles and primary AI techniques.","An analysis of a core cohort comprising 380 articles from multiple disciplines captures the most recent advancements in responsible AI.","As one of the pioneering bibliometric studies dedicated to exploring responsible AI, this study will provide comprehensive macro-level insights, enhancing the understanding of responsible AI while furnishing valuable knowledge support for AI regulation and governance initiatives."],"url":"http://arxiv.org/abs/2405.02846v1","category":"cs.AI"}
{"created":"2024-05-05 06:25:52","title":"Nip in the Bud: Forecasting and Interpreting Post-exploitation Attacks in Real-time through Cyber Threat Intelligence Reports","abstract":"Advanced Persistent Threat (APT) attacks have caused significant damage worldwide. Various Endpoint Detection and Response (EDR) systems are deployed by enterprises to fight against potential threats. However, EDR suffers from high false positives. In order not to affect normal operations, analysts need to investigate and filter detection results before taking countermeasures, in which heavy manual labor and alarm fatigue cause analysts miss optimal response time, thereby leading to information leakage and destruction. Therefore, we propose Endpoint Forecasting and Interpreting (EFI), a real-time attack forecast and interpretation system, which can automatically predict next move during post-exploitation and explain it in technique-level, then dispatch strategies to EDR for advance reinforcement. First, we use Cyber Threat Intelligence (CTI) reports to extract the attack scene graph (ASG) that can be mapped to low-level system logs to strengthen attack samples. Second, we build a serialized graph forecast model, which is combined with the attack provenance graph (APG) provided by EDR to generate an attack forecast graph (AFG) to predict the next move. Finally, we utilize the attack template graph (ATG) and graph alignment plus algorithm for technique-level interpretation to automatically dispatch strategies for EDR to reinforce system in advance. EFI can avoid the impact of existing EDR false positives, and can reduce the attack surface of system without affecting the normal operations. We collect a total of 3,484 CTI reports, generate 1,429 ASGs, label 8,000 sentences, tag 10,451 entities, and construct 256 ATGs. Experimental results on both DARPA Engagement and large scale CTI dataset show that the alignment score between the AFG predicted by EFI and the real attack graph is able to exceed 0.8, the forecast and interpretation precision of EFI can reach 91.8%.","sentences":["Advanced Persistent Threat (APT) attacks have caused significant damage worldwide.","Various Endpoint Detection and Response (EDR) systems are deployed by enterprises to fight against potential threats.","However, EDR suffers from high false positives.","In order not to affect normal operations, analysts need to investigate and filter detection results before taking countermeasures, in which heavy manual labor and alarm fatigue cause analysts miss optimal response time, thereby leading to information leakage and destruction.","Therefore, we propose Endpoint Forecasting and Interpreting (EFI), a real-time attack forecast and interpretation system, which can automatically predict next move during post-exploitation and explain it in technique-level, then dispatch strategies to EDR for advance reinforcement.","First, we use Cyber Threat Intelligence (CTI) reports to extract the attack scene graph (ASG) that can be mapped to low-level system logs to strengthen attack samples.","Second, we build a serialized graph forecast model, which is combined with the attack provenance graph (APG) provided by EDR to generate an attack forecast graph (AFG) to predict the next move.","Finally, we utilize the attack template graph (ATG) and graph alignment plus algorithm for technique-level interpretation to automatically dispatch strategies for EDR to reinforce system in advance.","EFI can avoid the impact of existing EDR false positives, and can reduce the attack surface of system without affecting the normal operations.","We collect a total of 3,484 CTI reports, generate 1,429 ASGs, label 8,000 sentences, tag 10,451 entities, and construct 256 ATGs.","Experimental results on both DARPA Engagement and large scale CTI dataset show that the alignment score between the AFG predicted by EFI and the real attack graph is able to exceed 0.8, the forecast and interpretation precision of EFI can reach 91.8%."],"url":"http://arxiv.org/abs/2405.02826v1","category":"cs.CR"}
{"created":"2024-05-05 06:01:31","title":"Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction","abstract":"Sim2real transfer has received increasing attention lately due to the success of learning robotic tasks in simulation end-to-end. While there has been a lot of progress in transferring vision-based navigation policies, the existing sim2real strategy for audio-visual navigation performs data augmentation empirically without measuring the acoustic gap. The sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real. We propose the first treatment of sim2real for audio-visual navigation by disentangling it into acoustic field prediction (AFP) and waypoint navigation. We first validate our design choice in the SoundSpaces simulator and show improvement on the Continuous AudioGoal navigation benchmark. We then collect real-world data to measure the spectral difference between the simulation and the real world by training AFP models that only take a specific frequency subband as input. We further propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured spectral difference and the energy distribution of the received audio, which improves the performance on the real data. Lastly, we build a real robot platform and show that the transferred policy can successfully navigate to sounding objects. This work demonstrates the potential of building intelligent agents that can see, hear, and act entirely from simulation, and transferring them to the real world.","sentences":["Sim2real transfer has received increasing attention lately due to the success of learning robotic tasks in simulation end-to-end.","While there has been a lot of progress in transferring vision-based navigation policies, the existing sim2real strategy for audio-visual navigation performs data augmentation empirically without measuring the acoustic gap.","The sound differs from light in that it spans across much wider frequencies and thus requires a different solution for sim2real.","We propose the first treatment of sim2real for audio-visual navigation by disentangling it into acoustic field prediction (AFP) and waypoint navigation.","We first validate our design choice in the SoundSpaces simulator and show improvement on the Continuous AudioGoal navigation benchmark.","We then collect real-world data to measure the spectral difference between the simulation and the real world by training AFP models that only take a specific frequency subband as input.","We further propose a frequency-adaptive strategy that intelligently selects the best frequency band for prediction based on both the measured spectral difference and the energy distribution of the received audio, which improves the performance on the real data.","Lastly, we build a real robot platform and show that the transferred policy can successfully navigate to sounding objects.","This work demonstrates the potential of building intelligent agents that can see, hear, and act entirely from simulation, and transferring them to the real world."],"url":"http://arxiv.org/abs/2405.02821v1","category":"cs.SD"}
{"created":"2024-05-05 05:43:49","title":"Site-Specific Deployment Optimization of Intelligent Reflecting Surface for Coverage Enhancement","abstract":"Intelligent Reflecting Surface (IRS) is a promising technology for next generation wireless networks. Despite substantial research in IRS-aided communications, the assumed antenna and channel models are typically simplified without considering site-specific characteristics, which in turn critically affect the IRS deployment and performance in a given environment. In this paper, we first investigate the link-level performance of active or passive IRS taking into account the IRS element radiation pattern (ERP) as well as the antenna radiation pattern of the access point (AP). Then the network-level coverage performance is evaluated/optimized in site-specific multi-building scenarios, by properly deploying multiple IRSs on candidate building facets to serve a given set of users or Points of Interests (PoIs). The problem is reduced to an integer linear programming (ILP) based on given link-level metrics, which is then solved efficiently under moderate network sizes. Numerical results confirm the impact of AP antenna/IRS element pattern on the link-level performance. In addition, it is found that active IRSs, though associated with higher hardware complexity and cost, significantly improve the site-specific network coverage performance in terms of average ergodic rate and fairness among the PoIs as well as the range of serving area, compared with passive IRSs that have a much larger number of elements.","sentences":["Intelligent Reflecting Surface (IRS) is a promising technology for next generation wireless networks.","Despite substantial research in IRS-aided communications, the assumed antenna and channel models are typically simplified without considering site-specific characteristics, which in turn critically affect the IRS deployment and performance in a given environment.","In this paper, we first investigate the link-level performance of active or passive IRS taking into account the IRS element radiation pattern (ERP) as well as the antenna radiation pattern of the access point (AP).","Then the network-level coverage performance is evaluated/optimized in site-specific multi-building scenarios, by properly deploying multiple IRSs on candidate building facets to serve a given set of users or Points of Interests (PoIs).","The problem is reduced to an integer linear programming (ILP) based on given link-level metrics, which is then solved efficiently under moderate network sizes.","Numerical results confirm the impact of AP antenna/IRS element pattern on the link-level performance.","In addition, it is found that active IRSs, though associated with higher hardware complexity and cost, significantly improve the site-specific network coverage performance in terms of average ergodic rate and fairness among the PoIs as well as the range of serving area, compared with passive IRSs that have a much larger number of elements."],"url":"http://arxiv.org/abs/2405.02818v1","category":"cs.IT"}
{"created":"2024-05-05 05:08:38","title":"Region-specific Risk Quantification for Interpretable Prognosis of COVID-19","abstract":"The COVID-19 pandemic has strained global public health, necessitating accurate diagnosis and intervention to control disease spread and reduce mortality rates. This paper introduces an interpretable deep survival prediction model designed specifically for improved understanding and trust in COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale pretrained image encoder, Risk-specific Grad-CAM, and anatomical region detection techniques, our approach produces regional interpretable outcomes that effectively capture essential disease features while focusing on rare but critical abnormal regions. Our model's predictive results provide enhanced clarity and transparency through risk area localization, enabling clinicians to make informed decisions regarding COVID-19 diagnosis with better understanding of prognostic insights. We evaluate the proposed method on a multi-center survival dataset and demonstrate its effectiveness via quantitative and qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and time-dependent AUCs (0.799 and 0.691). These results suggest that our explainable deep survival prediction model surpasses traditional survival analysis methods in risk prediction, improving interpretability for clinical decision making and enhancing AI system trustworthiness.","sentences":["The COVID-19 pandemic has strained global public health, necessitating accurate diagnosis and intervention to control disease spread and reduce mortality rates.","This paper introduces an interpretable deep survival prediction model designed specifically for improved understanding and trust in COVID-19 prognosis using chest X-ray (CXR) images.","By integrating a large-scale pretrained image encoder, Risk-specific Grad-CAM, and anatomical region detection techniques, our approach produces regional interpretable outcomes that effectively capture essential disease features while focusing on rare but critical abnormal regions.","Our model's predictive results provide enhanced clarity and transparency through risk area localization, enabling clinicians to make informed decisions regarding COVID-19 diagnosis with better understanding of prognostic insights.","We evaluate the proposed method on a multi-center survival dataset and demonstrate its effectiveness via quantitative and qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and time-dependent AUCs (0.799 and 0.691).","These results suggest that our explainable deep survival prediction model surpasses traditional survival analysis methods in risk prediction, improving interpretability for clinical decision making and enhancing AI system trustworthiness."],"url":"http://arxiv.org/abs/2405.02815v1","category":"cs.CV"}
{"created":"2024-05-05 05:06:07","title":"NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli","abstract":"Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive research into LLMs across various disciplines, including the social sciences. Notably, studies have revealed that LLMs possess emotional intelligence, which can be further developed through positive emotional stimuli. This discovery raises an intriguing question: can negative emotions similarly influence LLMs, potentially enhancing their performance? In response to this question, we introduce NegativePrompt, a novel approach underpinned by psychological principles, involving ten specifically designed negative emotional stimuli. We embark on rigorous experimental evaluations of five LLMs including Flan-T5-Large, Vicuna, Llama 2, ChatGPT, and GPT-4, across a set of 45 tasks. The results are revealing: NegativePrompt markedly enhances the performance of LLMs, evidenced by relative improvements of 12.89% in Instruction Induction tasks and 46.25% in BIG-Bench tasks. Moreover, we conduct attention visualization experiments to decipher the underlying mechanisms of NegativePrompt's influence. Our research contributes significantly to the understanding of LLMs and emotion interaction, demonstrating the practical efficacy of NegativePrompt as an emotion-driven method and offering novel insights for the enhancement of LLMs in real-world applications. The code is available at https://github.com/wangxu0820/NegativePrompt.","sentences":["Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications.","This widespread adoption has spurred extensive research into LLMs across various disciplines, including the social sciences.","Notably, studies have revealed that LLMs possess emotional intelligence, which can be further developed through positive emotional stimuli.","This discovery raises an intriguing question: can negative emotions similarly influence LLMs, potentially enhancing their performance?","In response to this question, we introduce NegativePrompt, a novel approach underpinned by psychological principles, involving ten specifically designed negative emotional stimuli.","We embark on rigorous experimental evaluations of five LLMs including Flan-T5-Large, Vicuna, Llama 2, ChatGPT, and GPT-4, across a set of 45 tasks.","The results are revealing:","NegativePrompt markedly enhances the performance of LLMs, evidenced by relative improvements of 12.89% in Instruction Induction tasks and 46.25% in BIG-Bench tasks.","Moreover, we conduct attention visualization experiments to decipher the underlying mechanisms of NegativePrompt's influence.","Our research contributes significantly to the understanding of LLMs and emotion interaction, demonstrating the practical efficacy of NegativePrompt as an emotion-driven method and offering novel insights for the enhancement of LLMs in real-world applications.","The code is available at https://github.com/wangxu0820/NegativePrompt."],"url":"http://arxiv.org/abs/2405.02814v1","category":"cs.CL"}
{"created":"2024-05-05 04:00:03","title":"Kinematic analysis of structural mechanics based on convolutional neural network","abstract":"Attempt to use convolutional neural network to achieve kinematic analysis of plane bar structure. Through 3dsMax animation software and OpenCV module, self-build image dataset of geometrically stable system and geometrically unstable system. we construct and train convolutional neural network model based on the TensorFlow and Keras deep learning platform framework. The model achieves 100% accuracy on the training set, validation set, and test set. The accuracy on the additional test set is 93.7%, indicating that convolutional neural network can learn and master the relevant knowledge of kinematic analysis of structural mechanics. In the future, the generalization ability of the model can be improved through the diversity of dataset, which has the potential to surpass human experts for complex structures. Convolutional neural network has certain practical value in the field of kinematic analysis of structural mechanics. Using visualization technology, we reveal how convolutional neural network learns and recognizes structural features. Using pre-trained VGG16 model for feature extraction and fine-tuning, we found that the generalization ability is inferior to the self-built model.","sentences":["Attempt to use convolutional neural network to achieve kinematic analysis of plane bar structure.","Through 3dsMax animation software and OpenCV module, self-build image dataset of geometrically stable system and geometrically unstable system.","we construct and train convolutional neural network model based on the TensorFlow and Keras deep learning platform framework.","The model achieves 100% accuracy on the training set, validation set, and test set.","The accuracy on the additional test set is 93.7%, indicating that convolutional neural network can learn and master the relevant knowledge of kinematic analysis of structural mechanics.","In the future, the generalization ability of the model can be improved through the diversity of dataset, which has the potential to surpass human experts for complex structures.","Convolutional neural network has certain practical value in the field of kinematic analysis of structural mechanics.","Using visualization technology, we reveal how convolutional neural network learns and recognizes structural features.","Using pre-trained VGG16 model for feature extraction and fine-tuning, we found that the generalization ability is inferior to the self-built model."],"url":"http://arxiv.org/abs/2405.02807v1","category":"cs.LG"}
{"created":"2024-05-05 03:23:38","title":"Permutation time irreversibility in sleep electroencephalograms: Dependence on sleep stage and the effect of equal values","abstract":"Time irreversibility (TIR) refers to the manifestation of nonequilibrium brain activity influenced by various physiological conditions; however, the influence of sleep on electroencephalogram (EEG) TIR has not been sufficiently investigated. In this paper, a comprehensive study on permutation TIR (pTIR) of EEG data under different sleep stages is conducted. Two basic ordinal patterns (i.e., the original and amplitude permutations) are distinguished to simplify sleep EEGs, and then the influences of equal values and forbidden permutation on pTIR are elucidated. To detect pTIR of brain electric signals, 5 groups of EEGs in the awake, stages I, II, III, and rapid eye movement (REM) stages are collected from the public Polysomnographic Database in PhysioNet. Test results suggested that the pTIR of sleep EEGs significantly decreases as the sleep stage increases (p<0.001), with the awake and REM EEGs, demonstrating greater differences than others. Comparative analysis and numerical simulations support the importance of equal values. Distribution of equal states, a simple quantification of amplitude fluctuations, significantly increases with the sleep stage (p<0.001). If these equalities are ignored, incorrect probabilistic differences may arise in the forward-backward and symmetric permutations of TIR, leading to contradictory results; moreover, the ascending and descending orders for symmetric permutations also lead different outcomes in sleep EEGs. Overall, pTIR in sleep EEGs contributes to our understanding of quantitative TIR and classification of sleep EEGs.","sentences":["Time irreversibility (TIR) refers to the manifestation of nonequilibrium brain activity influenced by various physiological conditions; however, the influence of sleep on electroencephalogram (EEG) TIR has not been sufficiently investigated.","In this paper, a comprehensive study on permutation TIR (pTIR) of EEG data under different sleep stages is conducted.","Two basic ordinal patterns (i.e., the original and amplitude permutations) are distinguished to simplify sleep EEGs, and then the influences of equal values and forbidden permutation on pTIR are elucidated.","To detect pTIR of brain electric signals, 5 groups of EEGs in the awake, stages I, II, III, and rapid eye movement (REM) stages are collected from the public Polysomnographic Database in PhysioNet.","Test results suggested that the pTIR of sleep EEGs significantly decreases as the sleep stage increases (p<0.001), with the awake and REM EEGs, demonstrating greater differences than others.","Comparative analysis and numerical simulations support the importance of equal values.","Distribution of equal states, a simple quantification of amplitude fluctuations, significantly increases with the sleep stage (p<0.001).","If these equalities are ignored, incorrect probabilistic differences may arise in the forward-backward and symmetric permutations of TIR, leading to contradictory results; moreover, the ascending and descending orders for symmetric permutations also lead different outcomes in sleep EEGs.","Overall, pTIR in sleep EEGs contributes to our understanding of quantitative TIR and classification of sleep EEGs."],"url":"http://arxiv.org/abs/2405.02802v1","category":"stat.CO"}
{"created":"2024-05-05 03:15:52","title":"Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models","abstract":"In recent years, AI-Generated Content (AIGC) has witnessed rapid advancements, facilitating the generation of music, images, and other forms of artistic expression across various industries. However, researches on general multi-modal music generation model remain scarce. To fill this gap, we propose a multi-modal music generation framework Mozart's Touch. It could generate aligned music with the cross-modality inputs, such as images, videos and text. Mozart's Touch is composed of three main components: Multi-modal Captioning Module, Large Language Model (LLM) Understanding & Bridging Module, and Music Generation Module. Unlike traditional approaches, Mozart's Touch requires no training or fine-tuning pre-trained models, offering efficiency and transparency through clear, interpretable prompts. We also introduce \"LLM-Bridge\" method to resolve the heterogeneous representation problems between descriptive texts of different modalities. We conduct a series of objective and subjective evaluations on the proposed model, and results indicate that our model surpasses the performance of current state-of-the-art models. Our codes and examples is availble at: https://github.com/WangTooNaive/MozartsTouch","sentences":["In recent years, AI-Generated Content (AIGC) has witnessed rapid advancements, facilitating the generation of music, images, and other forms of artistic expression across various industries.","However, researches on general multi-modal music generation model remain scarce.","To fill this gap, we propose a multi-modal music generation framework Mozart's Touch.","It could generate aligned music with the cross-modality inputs, such as images, videos and text.","Mozart's Touch is composed of three main components: Multi-modal Captioning Module, Large Language Model (LLM) Understanding & Bridging Module, and Music Generation Module.","Unlike traditional approaches, Mozart's Touch requires no training or fine-tuning pre-trained models, offering efficiency and transparency through clear, interpretable prompts.","We also introduce \"LLM-Bridge\" method to resolve the heterogeneous representation problems between descriptive texts of different modalities.","We conduct a series of objective and subjective evaluations on the proposed model, and results indicate that our model surpasses the performance of current state-of-the-art models.","Our codes and examples is availble at: https://github.com/WangTooNaive/MozartsTouch"],"url":"http://arxiv.org/abs/2405.02801v1","category":"cs.SD"}
{"created":"2024-05-05 03:10:45","title":"Designing Distinguishable Mid-Air Ultrasound Tactons with Temporal Parameters","abstract":"Mid-air ultrasound technology offers new design opportunities for contactless tactile patterns (i.e., Tactons) in user applications. Yet, few guidelines exist for making ultrasound Tactons easy to distinguish for users. In this paper, we investigated the distinguishability of temporal parameters of ultrasound Tactons in five studies (n=72 participants). Study 1 established the discrimination thresholds for amplitude-modulated (AM) frequencies. In Studies 2-5, we investigated distinguishable ultrasound Tactons by creating four Tacton sets based on mechanical vibrations in the literature and collected similarity ratings for the ultrasound Tactons. We identified a subset of temporal parameters, such as rhythm and low envelope frequency, that could create distinguishable ultrasound Tactons. Also, a strong correlation (mean Spearman's $\\rho$=0.75) existed between similarity ratings for ultrasound Tactons and similarities of mechanical Tactons from the literature, suggesting vibrotactile designers can transfer their knowledge to ultrasound design. We present design guidelines and future directions for creating distinguishable mid-air ultrasound Tactons.","sentences":["Mid-air ultrasound technology offers new design opportunities for contactless tactile patterns (i.e., Tactons) in user applications.","Yet, few guidelines exist for making ultrasound Tactons easy to distinguish for users.","In this paper, we investigated the distinguishability of temporal parameters of ultrasound Tactons in five studies (n=72 participants).","Study 1 established the discrimination thresholds for amplitude-modulated (AM) frequencies.","In Studies 2-5, we investigated distinguishable ultrasound Tactons by creating four Tacton sets based on mechanical vibrations in the literature and collected similarity ratings for the ultrasound Tactons.","We identified a subset of temporal parameters, such as rhythm and low envelope frequency, that could create distinguishable ultrasound Tactons.","Also, a strong correlation (mean Spearman's $\\rho$=0.75) existed between similarity ratings for ultrasound Tactons and similarities of mechanical Tactons from the literature, suggesting vibrotactile designers can transfer their knowledge to ultrasound design.","We present design guidelines and future directions for creating distinguishable mid-air ultrasound Tactons."],"url":"http://arxiv.org/abs/2405.02800v1","category":"cs.HC"}
{"created":"2024-05-05 02:11:57","title":"Efficient Text-driven Motion Generation via Latent Consistency Training","abstract":"Motion diffusion models have recently proven successful for text-driven human motion generation. Despite their excellent generation performance, they are challenging to infer in real time due to the multi-step sampling mechanism that involves tens or hundreds of repeat function evaluation iterations. To this end, we investigate a motion latent consistency Training (MLCT) for motion generation to alleviate the computation and time consumption during iteration inference. It applies diffusion pipelines to low-dimensional motion latent spaces to mitigate the computational burden of each function evaluation. Explaining the diffusion process with probabilistic flow ordinary differential equation (PF-ODE) theory, the MLCT allows extremely few steps infer between the prior distribution to the motion latent representation distribution via maintaining consistency of the outputs over the trajectory of PF-ODE. Especially, we introduce a quantization constraint to optimize motion latent representations that are bounded, regular, and well-reconstructed compared to traditional variational constraints. Furthermore, we propose a conditional PF-ODE trajectory simulation method, which improves the conditional generation performance with minimal additional training costs. Extensive experiments on two human motion generation benchmarks show that the proposed model achieves state-of-the-art performance with less than 10\\% time cost.","sentences":["Motion diffusion models have recently proven successful for text-driven human motion generation.","Despite their excellent generation performance, they are challenging to infer in real time due to the multi-step sampling mechanism that involves tens or hundreds of repeat function evaluation iterations.","To this end, we investigate a motion latent consistency Training (MLCT) for motion generation to alleviate the computation and time consumption during iteration inference.","It applies diffusion pipelines to low-dimensional motion latent spaces to mitigate the computational burden of each function evaluation.","Explaining the diffusion process with probabilistic flow ordinary differential equation (PF-ODE) theory, the MLCT allows extremely few steps infer between the prior distribution to the motion latent representation distribution via maintaining consistency of the outputs over the trajectory of PF-ODE.","Especially, we introduce a quantization constraint to optimize motion latent representations that are bounded, regular, and well-reconstructed compared to traditional variational constraints.","Furthermore, we propose a conditional PF-ODE trajectory simulation method, which improves the conditional generation performance with minimal additional training costs.","Extensive experiments on two human motion generation benchmarks show that the proposed model achieves state-of-the-art performance with less than 10\\% time cost."],"url":"http://arxiv.org/abs/2405.02791v1","category":"cs.CV"}
{"created":"2024-05-05 00:08:00","title":"Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs","abstract":"This work focuses on leveraging and selecting from vast, unlabeled, open data to pre-fine-tune a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales. However, they often prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution. Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. We demonstrate the efficacy of our methodology across a diverse array of tasks (NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently surpasses other selection methods. Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour. Our code is open-sourced (Code repository: https://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible.","sentences":["This work focuses on leveraging and selecting from vast, unlabeled, open data to pre-fine-tune a pre-trained language model.","The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels.","While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales.","However, they often prioritize data that aligns with the target distribution.","While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution.","Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution.","We show the optimality of this approach for fine-tuning tasks under certain conditions.","We demonstrate the efficacy of our methodology across a diverse array of tasks (NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently surpasses other selection methods.","Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour.","Our code is open-sourced (Code repository: https://anonymous.4open.science/r/DV4LLM-D761/ ).","While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible."],"url":"http://arxiv.org/abs/2405.02774v1","category":"cs.LG"}
{"created":"2024-05-04 23:16:48","title":"MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning","abstract":"The volume of unlabelled Earth observation (EO) data is huge, but many important applications lack labelled training data. However, EO data offers the unique opportunity to pair data from different modalities and sensors automatically based on geographic location and time, at virtually no human labor cost. We seize this opportunity to create a diverse multi-modal pretraining dataset at global scale. Using this new corpus of 1.2 million locations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to learn general-purpose representations for optical satellite images. Our approach builds on the ConvNeXt V2 architecture, a fully convolutional masked autoencoder (MAE). Drawing upon a suite of multi-modal pretext tasks, we demonstrate that our MP-MAE approach outperforms both MAEs pretrained on ImageNet and MAEs pretrained on domain-specific satellite images. This is shown on several downstream tasks including image classification and semantic segmentation. We find that multi-modal pretraining notably improves the linear probing performance, e.g. 4pp on BigEarthNet and 16pp on So2Sat, compared to pretraining on optical satellite images only. We show that this also leads to better label and parameter efficiency which are crucial aspects in global scale applications.","sentences":["The volume of unlabelled Earth observation (EO) data is huge, but many important applications lack labelled training data.","However, EO data offers the unique opportunity to pair data from different modalities and sensors automatically based on geographic location and time, at virtually no human labor cost.","We seize this opportunity to create a diverse multi-modal pretraining dataset at global scale.","Using this new corpus of 1.2 million locations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to learn general-purpose representations for optical satellite images.","Our approach builds on the ConvNeXt V2 architecture, a fully convolutional masked autoencoder (MAE).","Drawing upon a suite of multi-modal pretext tasks, we demonstrate that our MP-MAE approach outperforms both MAEs pretrained on ImageNet and MAEs pretrained on domain-specific satellite images.","This is shown on several downstream tasks including image classification and semantic segmentation.","We find that multi-modal pretraining notably improves the linear probing performance, e.g. 4pp on BigEarthNet and 16pp on So2Sat, compared to pretraining on optical satellite images only.","We show that this also leads to better label and parameter efficiency which are crucial aspects in global scale applications."],"url":"http://arxiv.org/abs/2405.02771v1","category":"cs.CV"}
{"created":"2024-05-04 22:50:39","title":"PhilHumans: Benchmarking Machine Learning for Personal Health","abstract":"The use of machine learning in Healthcare has the potential to improve patient outcomes as well as broaden the reach and affordability of Healthcare. The history of other application areas indicates that strong benchmarks are essential for the development of intelligent systems. We present Personal Health Interfaces Leveraging HUman-MAchine Natural interactions (PhilHumans), a holistic suite of benchmarks for machine learning across different Healthcare settings - talk therapy, diet coaching, emergency care, intensive care, obstetric sonography - as well as different learning settings, such as action anticipation, timeseries modeling, insight mining, language modeling, computer vision, reinforcement learning and program synthesis","sentences":["The use of machine learning in Healthcare has the potential to improve patient outcomes as well as broaden the reach and affordability of Healthcare.","The history of other application areas indicates that strong benchmarks are essential for the development of intelligent systems.","We present Personal Health Interfaces Leveraging HUman-MAchine Natural interactions (PhilHumans), a holistic suite of benchmarks for machine learning across different Healthcare settings - talk therapy, diet coaching, emergency care, intensive care, obstetric sonography - as well as different learning settings, such as action anticipation, timeseries modeling, insight mining, language modeling, computer vision, reinforcement learning and program synthesis"],"url":"http://arxiv.org/abs/2405.02770v1","category":"cs.LG"}
{"created":"2024-05-04 22:02:58","title":"Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning","abstract":"While humans excel at continual learning (CL), deep neural networks (DNNs) exhibit catastrophic forgetting. A salient feature of the brain that allows effective CL is that it utilizes multiple modalities for learning and inference, which is underexplored in DNNs. Therefore, we study the role and interactions of multiple modalities in mitigating forgetting and introduce a benchmark for multimodal continual learning. Our findings demonstrate that leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations. This makes the model less vulnerable to modality-specific regularities and considerably mitigates forgetting. Furthermore, we observe that individual modalities exhibit varying degrees of robustness to distribution shift. Finally, we propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality. Our method sets a strong baseline that enables both single- and multimodal inference. Our study provides a promising case for further exploring the role of multiple modalities in enabling CL and provides a standard benchmark for future research.","sentences":["While humans excel at continual learning (CL), deep neural networks (DNNs) exhibit catastrophic forgetting.","A salient feature of the brain that allows effective CL is that it utilizes multiple modalities for learning and inference, which is underexplored in DNNs.","Therefore, we study the role and interactions of multiple modalities in mitigating forgetting and introduce a benchmark for multimodal continual learning.","Our findings demonstrate that leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations.","This makes the model less vulnerable to modality-specific regularities and considerably mitigates forgetting.","Furthermore, we observe that individual modalities exhibit varying degrees of robustness to distribution shift.","Finally, we propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality.","Our method sets a strong baseline that enables both single- and multimodal inference.","Our study provides a promising case for further exploring the role of multiple modalities in enabling CL and provides a standard benchmark for future research."],"url":"http://arxiv.org/abs/2405.02766v1","category":"cs.LG"}
{"created":"2024-05-04 22:02:24","title":"Detecting Edited Knowledge in Language Models","abstract":"Knowledge editing techniques (KEs) can update language models' obsolete or inaccurate knowledge learned from pre-training. However, KE also faces potential malicious applications, e.g. inserting misinformation and toxic content. Moreover, in the context of responsible AI, it is instructive for end-users to know whether a generated output is driven by edited knowledge or first-hand knowledge from pre-training. To this end, we study detecting edited knowledge in language models by introducing a novel task: given an edited model and a specific piece of knowledge the model generates, our objective is to classify the knowledge as either \"non-edited\" (based on the pre-training), or ``edited'' (based on subsequent editing). We initiate the task with two state-of-the-art KEs, two language models, and two datasets. We further propose a simple classifier, RepReg, a logistic regression model that takes hidden state representations as input features. Our results reveal that RepReg establishes a strong baseline, achieving a peak accuracy of 99.81%, and 97.79% in out-of-domain settings. Second, RepReg achieves near-optimal performance with a limited training set (200 training samples), and it maintains its performance even in out-of-domain settings. Last, we find it more challenging to separate edited and non-edited knowledge when they contain the same subject or object.","sentences":["Knowledge editing techniques (KEs) can update language models' obsolete or inaccurate knowledge learned from pre-training.","However, KE also faces potential malicious applications, e.g. inserting misinformation and toxic content.","Moreover, in the context of responsible AI, it is instructive for end-users to know whether a generated output is driven by edited knowledge or first-hand knowledge from pre-training.","To this end, we study detecting edited knowledge in language models by introducing a novel task: given an edited model and a specific piece of knowledge the model generates, our objective is to classify the knowledge as either \"non-edited\" (based on the pre-training), or ``edited'' (based on subsequent editing).","We initiate the task with two state-of-the-art KEs, two language models, and two datasets.","We further propose a simple classifier, RepReg, a logistic regression model that takes hidden state representations as input features.","Our results reveal that RepReg establishes a strong baseline, achieving a peak accuracy of 99.81%, and 97.79% in out-of-domain settings.","Second, RepReg achieves near-optimal performance with a limited training set (200 training samples), and it maintains its performance even in out-of-domain settings.","Last, we find it more challenging to separate edited and non-edited knowledge when they contain the same subject or object."],"url":"http://arxiv.org/abs/2405.02765v1","category":"cs.CL"}
{"created":"2024-05-04 20:59:06","title":"Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning","abstract":"Deep reinforcement learning (DRL) has demonstrated remarkable performance in many continuous control tasks. However, a significant obstacle to the real-world application of DRL is the lack of safety guarantees. Although DRL agents can satisfy system safety in expectation through reward shaping, designing agents to consistently meet hard constraints (e.g., safety specifications) at every time step remains a formidable challenge. In contrast, existing work in the field of safe control provides guarantees on persistent satisfaction of hard safety constraints. However, these methods require explicit analytical system dynamics models to synthesize safe control, which are typically inaccessible in DRL settings. In this paper, we present a model-free safe control algorithm, the implicit safe set algorithm, for synthesizing safeguards for DRL agents that ensure provable safety throughout training. The proposed algorithm synthesizes a safety index (barrier certificate) and a subsequent safe control law solely by querying a black-box dynamic function (e.g., a digital twin simulator). Moreover, we theoretically prove that the implicit safe set algorithm guarantees finite time convergence to the safe set and forward invariance for both continuous-time and discrete-time systems. We validate the proposed algorithm on the state-of-the-art Safety Gym benchmark, where it achieves zero safety violations while gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art safe DRL methods. Furthermore, the resulting algorithm scales well to high-dimensional systems with parallel computing.","sentences":["Deep reinforcement learning (DRL) has demonstrated remarkable performance in many continuous control tasks.","However, a significant obstacle to the real-world application of DRL is the lack of safety guarantees.","Although DRL agents can satisfy system safety in expectation through reward shaping, designing agents to consistently meet hard constraints (e.g., safety specifications) at every time step remains a formidable challenge.","In contrast, existing work in the field of safe control provides guarantees on persistent satisfaction of hard safety constraints.","However, these methods require explicit analytical system dynamics models to synthesize safe control, which are typically inaccessible in DRL settings.","In this paper, we present a model-free safe control algorithm, the implicit safe set algorithm, for synthesizing safeguards for DRL agents that ensure provable safety throughout training.","The proposed algorithm synthesizes a safety index (barrier certificate) and a subsequent safe control law solely by querying a black-box dynamic function (e.g., a digital twin simulator).","Moreover, we theoretically prove that the implicit safe set algorithm guarantees finite time convergence to the safe set and forward invariance for both continuous-time and discrete-time systems.","We validate the proposed algorithm on the state-of-the-art Safety Gym benchmark, where it achieves zero safety violations while gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art safe DRL methods.","Furthermore, the resulting algorithm scales well to high-dimensional systems with parallel computing."],"url":"http://arxiv.org/abs/2405.02754v1","category":"cs.RO"}
{"created":"2024-05-04 20:38:41","title":"Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding","abstract":"Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or contextually unfaithful content. LLMs utilize two primary knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric) knowledge from input prompts. The study addresses the open question of how LLMs effectively balance these knowledge sources during the generation process, specifically in the context of open-domain question answering. To address this issue, we introduce a novel approach integrating contrastive decoding with adversarial irrelevant passages as negative samples to enhance robust context grounding during generation. Notably, our method operates at inference time without requiring further training. We conduct comprehensive experiments to demonstrate its applicability and effectiveness, providing empirical evidence showcasing its superiority over existing methodologies. Our code is publicly available at: https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding.","sentences":["Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or contextually unfaithful content.","LLMs utilize two primary knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric) knowledge from input prompts.","The study addresses the open question of how LLMs effectively balance these knowledge sources during the generation process, specifically in the context of open-domain question answering.","To address this issue, we introduce a novel approach integrating contrastive decoding with adversarial irrelevant passages as negative samples to enhance robust context grounding during generation.","Notably, our method operates at inference time without requiring further training.","We conduct comprehensive experiments to demonstrate its applicability and effectiveness, providing empirical evidence showcasing its superiority over existing methodologies.","Our code is publicly available at: https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding."],"url":"http://arxiv.org/abs/2405.02750v1","category":"cs.CL"}
{"created":"2024-05-04 20:25:04","title":"Wobbling motion in triaxial nuclei","abstract":"The experimental evidence for the collective wobbling motion of triaxial nuclei is reviewed. The classification into transverse and longitudinal in the presence of quasiparticle excitations is discussed. The description by means of the quasiparticle+triaxial rotor model is discussed in detail.   The structure of the states is analyzed using the spin-coherent-state and spin-squeezed-state representations of the reduced density matrices of the total and particle angular momenta, which distill the corresponding classical precessional motions. Various approximate solutions of the quasiparticle+triaxial rotor model are evaluated.   The microscopic studies of wobbling in the small-amplitude random phase approximation are discussed. Selected studies of wobbling by means of the triaxial projected shell model are presented, which focus on how this microscopic approach removes certain deficiencies of the semi-microscopic quasiparticle+triaxial rotor model.","sentences":["The experimental evidence for the collective wobbling motion of triaxial nuclei is reviewed.","The classification into transverse and longitudinal in the presence of quasiparticle excitations is discussed.","The description by means of the quasiparticle+triaxial rotor model is discussed in detail.   ","The structure of the states is analyzed using the spin-coherent-state and spin-squeezed-state representations of the reduced density matrices of the total and particle angular momenta, which distill the corresponding classical precessional motions.","Various approximate solutions of the quasiparticle+triaxial rotor model are evaluated.   ","The microscopic studies of wobbling in the small-amplitude random phase approximation are discussed.","Selected studies of wobbling by means of the triaxial projected shell model are presented, which focus on how this microscopic approach removes certain deficiencies of the semi-microscopic quasiparticle+triaxial rotor model."],"url":"http://arxiv.org/abs/2405.02747v1","category":"nucl-th"}
{"created":"2024-05-04 19:04:51","title":"Relations Prediction for Knowledge Graph Completion using Large Language Models","abstract":"Knowledge Graphs have been widely used to represent facts in a structured format. Due to their large scale applications, knowledge graphs suffer from being incomplete. The relation prediction task obtains knowledge graph completion by assigning one or more possible relations to each pair of nodes. In this work, we make use of the knowledge graph node names to fine-tune a large language model for the relation prediction task. By utilizing the node names only we enable our model to operate sufficiently in the inductive settings. Our experiments show that we accomplish new scores on a widely used knowledge graph benchmark.","sentences":["Knowledge Graphs have been widely used to represent facts in a structured format.","Due to their large scale applications, knowledge graphs suffer from being incomplete.","The relation prediction task obtains knowledge graph completion by assigning one or more possible relations to each pair of nodes.","In this work, we make use of the knowledge graph node names to fine-tune a large language model for the relation prediction task.","By utilizing the node names only we enable our model to operate sufficiently in the inductive settings.","Our experiments show that we accomplish new scores on a widely used knowledge graph benchmark."],"url":"http://arxiv.org/abs/2405.02738v1","category":"cs.CL"}
{"created":"2024-05-04 19:00:43","title":"Girth and groomed radius of jets recoiling against isolated photons in lead-lead and proton-proton collisions at $\\sqrt{s_\\mathrm{NN}}$ = 5.02 TeV","abstract":"This Letter presents the first measurements of the groomed jet radius $R_\\mathrm{g}$ and the jet girth $g$ in events with an isolated photon recoiling against a jet in lead-lead (PbPb) and proton-proton (pp) collisions at the LHC at a nucleon-nucleon center-of-mass energy of 5.02 TeV. The observables $R_\\mathrm{g}$ and $g$ provide a quantitative measure of how narrow or broad a jet is. The analysis uses PbPb and pp data samples with integrated luminosities of 1.7 nb$^{-1}$ and 301 pb$^{-1}$, respectively, collected with the CMS experiment in 2018 and 2017. Events are required to have a photon with transverse momentum $p_\\mathrm{T}^\\gamma$ 100 GeV and at least one jet back-to-back in azimuth with respect to the photon and with transverse momentum $p_\\mathrm{T}^\\text{jet}$ such that $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.4. The measured $R_\\mathrm{g}$ and $g$ distributions are unfolded to the particle level, which facilitates the comparison between the PbPb and pp results and with theoretical predictions. It is found that jets with $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.8, i.e., those that closely balance the photon $p_\\mathrm{T}^\\gamma$, are narrower in PbPb than in pp collisions. Relaxing the selection to include jets with $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.4 reduces the narrowing of the angular structure of jets in PbPb relative to the pp reference. This shows that selection bias effects associated with jet energy loss play an important role in the interpretation of jet substructure measurements.","sentences":["This Letter presents the first measurements of the groomed jet radius $R_\\mathrm{g}$ and the jet girth $g$ in events with an isolated photon recoiling against a jet in lead-lead (PbPb) and proton-proton (pp) collisions at the LHC at a nucleon-nucleon center-of-mass energy of 5.02 TeV.","The observables $R_\\mathrm{g}$ and $g$ provide a quantitative measure of how narrow or broad a jet is.","The analysis uses PbPb and pp data samples with integrated luminosities of 1.7 nb$^{-1}$ and 301 pb$^{-1}$, respectively, collected with the CMS experiment in 2018 and 2017.","Events are required to have a photon with transverse momentum $p_\\mathrm{T}^\\gamma$ 100 GeV and at least one jet back-to-back in azimuth with respect to the photon and with transverse momentum $p_\\mathrm{T}^\\text{jet}$ such that $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.4.","The measured $R_\\mathrm{g}$ and $g$ distributions are unfolded to the particle level, which facilitates the comparison between the PbPb and pp results and with theoretical predictions.","It is found that jets with $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.8, i.e., those that closely balance the photon $p_\\mathrm{T}^\\gamma$, are narrower in PbPb than in pp collisions.","Relaxing the selection to include jets with $p_\\mathrm{T}^\\text{jet}/p_\\mathrm{T}^\\gamma$ $>$ 0.4 reduces the narrowing of the angular structure of jets in PbPb relative to the pp reference.","This shows that selection bias effects associated with jet energy loss play an important role in the interpretation of jet substructure measurements."],"url":"http://arxiv.org/abs/2405.02737v1","category":"nucl-ex"}
{"created":"2024-05-04 18:31:38","title":"Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles","abstract":"This systematic review focuses on anomaly detection for connected and autonomous vehicles. The initial database search identified 2160 articles, of which 203 were included in this review after rigorous screening and assessment. This study revealed that the most commonly used Artificial Intelligence (AI) algorithms employed in anomaly detection are neural networks like LSTM, CNN, and autoencoders, alongside one-class SVM. Most anomaly-based models were trained using real-world operational vehicle data, although anomalies, such as attacks and faults, were often injected artificially into the datasets. These models were evaluated mostly using five key evaluation metrics: recall, accuracy, precision, F1-score, and false positive rate. The most frequently used selection of evaluation metrics used for anomaly detection models were accuracy, precision, recall, and F1-score. This systematic review presents several recommendations. First, there is a need to incorporate multiple evaluation metrics to provide a comprehensive assessment of the anomaly detection models. Second, only a small proportion of the studies have made their models open source, indicating a need to share models publicly to facilitate collaboration within the research community, and to validate and compare findings effectively. Third, there is a need for benchmarking datasets with predefined anomalies or cyberattacks to test and improve the effectiveness of the proposed anomaly-based detection models. Furthermore, there is a need for future research to investigate the deployment of anomaly detection to a vehicle to assess its performance on the road. There is a notable lack of research done on intrusion detection systems using different protocols to CAN, such as Ethernet and FlexRay.","sentences":["This systematic review focuses on anomaly detection for connected and autonomous vehicles.","The initial database search identified 2160 articles, of which 203 were included in this review after rigorous screening and assessment.","This study revealed that the most commonly used Artificial Intelligence (AI) algorithms employed in anomaly detection are neural networks like LSTM, CNN, and autoencoders, alongside one-class SVM.","Most anomaly-based models were trained using real-world operational vehicle data, although anomalies, such as attacks and faults, were often injected artificially into the datasets.","These models were evaluated mostly using five key evaluation metrics: recall, accuracy, precision, F1-score, and false positive rate.","The most frequently used selection of evaluation metrics used for anomaly detection models were accuracy, precision, recall, and F1-score.","This systematic review presents several recommendations.","First, there is a need to incorporate multiple evaluation metrics to provide a comprehensive assessment of the anomaly detection models.","Second, only a small proportion of the studies have made their models open source, indicating a need to share models publicly to facilitate collaboration within the research community, and to validate and compare findings effectively.","Third, there is a need for benchmarking datasets with predefined anomalies or cyberattacks to test and improve the effectiveness of the proposed anomaly-based detection models.","Furthermore, there is a need for future research to investigate the deployment of anomaly detection to a vehicle to assess its performance on the road.","There is a notable lack of research done on intrusion detection systems using different protocols to CAN, such as Ethernet and FlexRay."],"url":"http://arxiv.org/abs/2405.02731v1","category":"cs.LG"}
{"created":"2024-05-04 16:53:19","title":"The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses","abstract":"Generative Artificial Intelligence (AI) is integrated into everyday technology, including news, education, and social media. AI has further pervaded private conversations as conversational partners, auto-completion, and response suggestions. As social media becomes young people's main method of peer support exchange, we need to understand when and how AI can facilitate and assist in such exchanges in a beneficial, safe, and socially appropriate way. We asked 622 young people to complete an online survey and evaluate blinded human- and AI-generated responses to help-seeking messages. We found that participants preferred the AI-generated response to situations about relationships, self-expression, and physical health. However, when addressing a sensitive topic, like suicidal thoughts, young people preferred the human response. We also discuss the role of training in online peer support exchange and its implications for supporting young people's well-being. Disclaimer: This paper includes sensitive topics, including suicide ideation. Reader discretion is advised.","sentences":["Generative Artificial Intelligence (AI) is integrated into everyday technology, including news, education, and social media.","AI has further pervaded private conversations as conversational partners, auto-completion, and response suggestions.","As social media becomes young people's main method of peer support exchange, we need to understand when and how AI can facilitate and assist in such exchanges in a beneficial, safe, and socially appropriate way.","We asked 622 young people to complete an online survey and evaluate blinded human-","and AI-generated responses to help-seeking messages.","We found that participants preferred the AI-generated response to situations about relationships, self-expression, and physical health.","However, when addressing a sensitive topic, like suicidal thoughts, young people preferred the human response.","We also discuss the role of training in online peer support exchange and its implications for supporting young people's well-being.","Disclaimer:","This paper includes sensitive topics, including suicide ideation.","Reader discretion is advised."],"url":"http://arxiv.org/abs/2405.02711v1","category":"cs.HC"}
{"created":"2024-05-04 15:37:22","title":"Stable Diffusion Dataset Generation for Downstream Classification Tasks","abstract":"Recent advances in generative artificial intelligence have enabled the creation of high-quality synthetic data that closely mimics real-world data. This paper explores the adaptation of the Stable Diffusion 2.0 model for generating synthetic datasets, using Transfer Learning, Fine-Tuning and generation parameter optimisation techniques to improve the utility of the dataset for downstream classification tasks. We present a class-conditional version of the model that exploits a Class-Encoder and optimisation of key generation parameters. Our methodology led to synthetic datasets that, in a third of cases, produced models that outperformed those trained on real datasets.","sentences":["Recent advances in generative artificial intelligence have enabled the creation of high-quality synthetic data that closely mimics real-world data.","This paper explores the adaptation of the Stable Diffusion 2.0 model for generating synthetic datasets, using Transfer Learning, Fine-Tuning and generation parameter optimisation techniques to improve the utility of the dataset for downstream classification tasks.","We present a class-conditional version of the model that exploits a Class-Encoder and optimisation of key generation parameters.","Our methodology led to synthetic datasets that, in a third of cases, produced models that outperformed those trained on real datasets."],"url":"http://arxiv.org/abs/2405.02698v1","category":"cs.LG"}
{"created":"2024-05-04 14:57:28","title":"Boosting 3D Neuron Segmentation with 2D Vision Transformer Pre-trained on Natural Images","abstract":"Neuron reconstruction, one of the fundamental tasks in neuroscience, rebuilds neuronal morphology from 3D light microscope imaging data. It plays a critical role in analyzing the structure-function relationship of neurons in the nervous system. However, due to the scarcity of neuron datasets and high-quality SWC annotations, it is still challenging to develop robust segmentation methods for single neuron reconstruction. To address this limitation, we aim to distill the consensus knowledge from massive natural image data to aid the segmentation model in learning the complex neuron structures. Specifically, in this work, we propose a novel training paradigm that leverages a 2D Vision Transformer model pre-trained on large-scale natural images to initialize our Transformer-based 3D neuron segmentation model with a tailored 2D-to-3D weight transferring strategy. Our method builds a knowledge sharing connection between the abundant natural and the scarce neuron image domains to improve the 3D neuron segmentation ability in a data-efficiency manner. Evaluated on a popular benchmark, BigNeuron, our method enhances neuron segmentation performance by 8.71% over the model trained from scratch with the same amount of training samples.","sentences":["Neuron reconstruction, one of the fundamental tasks in neuroscience, rebuilds neuronal morphology from 3D light microscope imaging data.","It plays a critical role in analyzing the structure-function relationship of neurons in the nervous system.","However, due to the scarcity of neuron datasets and high-quality SWC annotations, it is still challenging to develop robust segmentation methods for single neuron reconstruction.","To address this limitation, we aim to distill the consensus knowledge from massive natural image data to aid the segmentation model in learning the complex neuron structures.","Specifically, in this work, we propose a novel training paradigm that leverages a 2D Vision Transformer model pre-trained on large-scale natural images to initialize our Transformer-based 3D neuron segmentation model with a tailored 2D-to-3D weight transferring strategy.","Our method builds a knowledge sharing connection between the abundant natural and the scarce neuron image domains to improve the 3D neuron segmentation ability in a data-efficiency manner.","Evaluated on a popular benchmark, BigNeuron, our method enhances neuron segmentation performance by 8.71% over the model trained from scratch with the same amount of training samples."],"url":"http://arxiv.org/abs/2405.02686v1","category":"cs.CV"}
{"created":"2024-05-04 14:57:09","title":"FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer","abstract":"Federated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL). However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients. To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer. Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients. Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge.","sentences":["Federated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL).","However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients.","To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer.","Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients.","Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge."],"url":"http://arxiv.org/abs/2405.02685v1","category":"cs.LG"}
{"created":"2024-05-04 14:48:19","title":"Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge","abstract":"Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.","sentences":["Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments.","At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data.","Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch.","Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature.","In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge.","The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers.","Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate."],"url":"http://arxiv.org/abs/2405.02682v1","category":"cs.DC"}
{"created":"2024-05-04 14:46:55","title":"Spider RIS: Mobilizing Intelligent Surfaces for Enhanced Wireless Communications","abstract":"In this study, we introduce Spider RIS technology, which offers an innovative solution to the challenges encountered in movable antennas (MAs) and unmanned aerial vehicle (UAV)-enabled communication systems. By combining the dynamic adaptation capability of MAs and the flexible location advantages of UAVs, this technology offers a dynamic and movable RIS, which can flexibly optimize physical locations within the two-dimensional movement platform. Spider RIS aims to enhance the communication efficiency and reliability of wireless networks, particularly in obstructive environments, by elevating the signal quality and achievable rate. The motivation of Spider RIS is based on the ability to fully exploit the spatial variability of wireless channels and maximize channel capacity even with a limited number of reflecting elements by overcoming the limitations of traditional fixed RIS and energy-intensive UAV systems. Considering the geometry-based millimeter wave channel model, we present the design of a three-stage angular-based hybrid beamforming system empowered by Spider RIS: First, analog beamformers are designed using angular information, followed by the generation of digital precoder/combiner based on the effective channel observed from baseband stage. Subsequently, the joint dynamic positioning with phase shift design of the Spider RIS is optimized using particle swarm optimization, maximizing the achievable rate of the systems.","sentences":["In this study, we introduce Spider RIS technology, which offers an innovative solution to the challenges encountered in movable antennas (MAs) and unmanned aerial vehicle (UAV)-enabled communication systems.","By combining the dynamic adaptation capability of MAs and the flexible location advantages of UAVs, this technology offers a dynamic and movable RIS, which can flexibly optimize physical locations within the two-dimensional movement platform.","Spider RIS aims to enhance the communication efficiency and reliability of wireless networks, particularly in obstructive environments, by elevating the signal quality and achievable rate.","The motivation of Spider RIS is based on the ability to fully exploit the spatial variability of wireless channels and maximize channel capacity even with a limited number of reflecting elements by overcoming the limitations of traditional fixed RIS and energy-intensive UAV systems.","Considering the geometry-based millimeter wave channel model, we present the design of a three-stage angular-based hybrid beamforming system empowered by Spider RIS:","First, analog beamformers are designed using angular information, followed by the generation of digital precoder/combiner based on the effective channel observed from baseband stage.","Subsequently, the joint dynamic positioning with phase shift design of the Spider RIS is optimized using particle swarm optimization, maximizing the achievable rate of the systems."],"url":"http://arxiv.org/abs/2405.02681v1","category":"cs.IT"}
{"created":"2024-05-04 14:45:31","title":"Pr\u00e9visions m\u00e9t\u00e9orologiques bas\u00e9es sur l'intelligence artificielle : une r\u00e9volution peut en cacher une autre","abstract":"Artificial intelligence (AI), based on deep-learning algorithm using high-quality reanalysis datasets, is showing enormous potential for weather forecasting. In this context, the European Centre for Medium-Range Weather Forecasts (ECMWF) is developing a new forecasting system based on AI. Verification results of deterministic forecast for now are promising. However, the realism of weather forecasts based on AI is often questioned. Here, different types of realism are identified and we discuss, in particular, the relationship between structural realism and predictability of weather events. Furthermore, a statistical analysis of deterministic forecasts based on AI points to a realism/performance dilemma that a probabilistic approach should help to solve. -- L'intelligence artificielle (IA) bouleverse aujourd'hui le monde de la pr\\'evision m\\'et\\'eorologique avec l'utilisation d'algorithmes d'apprentissage profond nourris par des champs de r\\'eanalyses. Dans ce contexte, le Centre Europ\\'een pour les Pr\\'evisions M\\'et\\'eorologiques \\`a Moyen Terme (CEPMMT) a d\\'ecid\\'e de d\\'evelopper un nouveau syst\\`eme de pr\\'evisions resposant sur l'IA. Ces pr\\'evisions, pour le moment de type d\\'eterministe, montrent des r\\'esultats prometteurs. Toutefois, le r\\'ealisme de ce type de pr\\'evisions reposant sur l'IA est souvent questionn\\'e. Ici, nous identifions diff\\'erents types de r\\'ealisme et interrogeons notamment le rapport entre r\\'ealisme structurel et pr\\'evisibilit\\'e des \\'ev\\^enements m\\'et\\'eorologiques. Une analyse statistique de pr\\'evisions d\\'eterministes reposant sur l'IA laisse apparaitre un dilemme r\\'ealisme/performance qu'une approche probabiliste devrait aider \\`a r\\'esoudre.","sentences":["Artificial intelligence (AI), based on deep-learning algorithm using high-quality reanalysis datasets, is showing enormous potential for weather forecasting.","In this context, the European Centre for Medium-Range Weather Forecasts (ECMWF) is developing a new forecasting system based on AI.","Verification results of deterministic forecast for now are promising.","However, the realism of weather forecasts based on AI is often questioned.","Here, different types of realism are identified and we discuss, in particular, the relationship between structural realism and predictability of weather events.","Furthermore, a statistical analysis of deterministic forecasts based on AI points to a realism/performance dilemma that a probabilistic approach should help to solve.","-- L'intelligence artificielle (IA) bouleverse aujourd'hui le monde de la pr\\'evision m\\'et\\'eorologique avec l'utilisation d'algorithmes d'apprentissage profond nourris par des champs de r\\'eanalyses.","Dans ce contexte, le Centre Europ\\'een pour les Pr\\'evisions M\\'et\\'eorologiques \\`a Moyen Terme (CEPMMT) a d\\'ecid\\'e de d\\'evelopper un nouveau syst\\`eme de pr\\'evisions resposant sur l'IA.","Ces pr\\'evisions, pour le moment de type d\\'eterministe, montrent des r\\'esultats prometteurs.","Toutefois, le r\\'ealisme de ce type de pr\\'evisions reposant sur l'IA est souvent questionn\\'e.","Ici, nous identifions diff\\'erents types de r\\'ealisme et interrogeons notamment le rapport entre r\\'ealisme structurel et pr\\'evisibilit\\'e des \\'ev\\^enements","m\\'et\\'eorologiques.","Une analyse statistique de pr\\'evisions d\\'eterministes reposant sur l'IA laisse apparaitre un dilemme r\\'ealisme/performance qu'une approche probabiliste devrait aider \\`a r\\'esoudre."],"url":"http://arxiv.org/abs/2405.02679v1","category":"physics.ao-ph"}
{"created":"2024-05-04 14:43:31","title":"Position Paper: Quo Vadis, Unsupervised Time Series Anomaly Detection?","abstract":"The current state of machine learning scholarship in Timeseries Anomaly Detection (TAD) is plagued by the persistent use of flawed evaluation metrics, inconsistent benchmarking practices, and a lack of proper justification for the choices made in novel deep learning-based model designs. Our paper presents a critical analysis of the status quo in TAD, revealing the misleading track of current research and highlighting problematic methods, and evaluation practices. Our position advocates for a shift in focus from pursuing only the novelty in model design to improving benchmarking practices, creating non-trivial datasets, and placing renewed emphasis on studying the utility of model architectures for specific tasks. Our findings demonstrate the need for rigorous evaluation protocols, the creation of simple baselines, and the revelation that state-of-the-art deep anomaly detection models effectively learn linear mappings. These findings suggest the need for more exploration and development of simple and interpretable TAD methods. The increment of model complexity in the state-of-the-art deep-learning based models unfortunately offers very little improvement. We offer insights and suggestions for the field to move forward.","sentences":["The current state of machine learning scholarship in Timeseries Anomaly Detection (TAD) is plagued by the persistent use of flawed evaluation metrics, inconsistent benchmarking practices, and a lack of proper justification for the choices made in novel deep learning-based model designs.","Our paper presents a critical analysis of the status quo in TAD, revealing the misleading track of current research and highlighting problematic methods, and evaluation practices.","Our position advocates for a shift in focus from pursuing only the novelty in model design to improving benchmarking practices, creating non-trivial datasets, and placing renewed emphasis on studying the utility of model architectures for specific tasks.","Our findings demonstrate the need for rigorous evaluation protocols, the creation of simple baselines, and the revelation that state-of-the-art deep anomaly detection models effectively learn linear mappings.","These findings suggest the need for more exploration and development of simple and interpretable TAD methods.","The increment of model complexity in the state-of-the-art deep-learning based models unfortunately offers very little improvement.","We offer insights and suggestions for the field to move forward."],"url":"http://arxiv.org/abs/2405.02678v1","category":"cs.LG"}
{"created":"2024-05-04 14:29:05","title":"Quranic Audio Dataset: Crowdsourced and Labeled Recitation from Non-Arabic Speakers","abstract":"This paper addresses the challenge of learning to recite the Quran for non-Arabic speakers. We explore the possibility of crowdsourcing a carefully annotated Quranic dataset, on top of which AI models can be built to simplify the learning process. In particular, we use the volunteer-based crowdsourcing genre and implement a crowdsourcing API to gather audio assets. We integrated the API into an existing mobile application called NamazApp to collect audio recitations. We developed a crowdsourcing platform called Quran Voice for annotating the gathered audio assets. As a result, we have collected around 7000 Quranic recitations from a pool of 1287 participants across more than 11 non-Arabic countries, and we have annotated 1166 recitations from the dataset in six categories. We have achieved a crowd accuracy of 0.77, an inter-rater agreement of 0.63 between the annotators, and 0.89 between the labels assigned by the algorithm and the expert judgments.","sentences":["This paper addresses the challenge of learning to recite the Quran for non-Arabic speakers.","We explore the possibility of crowdsourcing a carefully annotated Quranic dataset, on top of which AI models can be built to simplify the learning process.","In particular, we use the volunteer-based crowdsourcing genre and implement a crowdsourcing API to gather audio assets.","We integrated the API into an existing mobile application called NamazApp to collect audio recitations.","We developed a crowdsourcing platform called Quran Voice for annotating the gathered audio assets.","As a result, we have collected around 7000 Quranic recitations from a pool of 1287 participants across more than 11 non-Arabic countries, and we have annotated 1166 recitations from the dataset in six categories.","We have achieved a crowd accuracy of 0.77, an inter-rater agreement of 0.63 between the annotators, and 0.89 between the labels assigned by the algorithm and the expert judgments."],"url":"http://arxiv.org/abs/2405.02675v1","category":"cs.SD"}
{"created":"2024-05-04 14:23:59","title":"Ambush strategy enhances organisms' performance in rock-paper-scissors games","abstract":"We study a five-species cyclic system wherein individuals of one species strategically adapt their movements to enhance their performance in the spatial rock-paper-scissors game. Environmental cues enable the awareness of the presence of organisms targeted for elimination in the cyclic game. If the local density of target organisms is sufficiently high, individuals move towards concentrated areas for direct attack; otherwise, they employ an ambush tactic, maximising the chances of success by targeting regions likely to be dominated by opponents. Running stochastic simulations, we discover that the ambush strategy enhances the likelihood of individual success compared to direct attacks alone, leading to uneven spatial patterns characterised by spiral waves. We compute the autocorrelation function and measure how the ambush tactic unbalances the organisms' spatial organisation by calculating the characteristic length scale of typical spatial domains of each species. We demonstrate that the threshold for local species density influences the ambush strategy's effectiveness, while the neighbourhood perception range significantly impacts decision-making accuracy. The outcomes show that long-range perception improves performance by over 60\\%, although there is potential interference in decision-making under high attack triggers. Understanding how organisms' adaptation to their environment enhances their performance may be helpful not only for ecologists but also for data scientists aiming to improve artificial intelligence systems.","sentences":["We study a five-species cyclic system wherein individuals of one species strategically adapt their movements to enhance their performance in the spatial rock-paper-scissors game.","Environmental cues enable the awareness of the presence of organisms targeted for elimination in the cyclic game.","If the local density of target organisms is sufficiently high, individuals move towards concentrated areas for direct attack; otherwise, they employ an ambush tactic, maximising the chances of success by targeting regions likely to be dominated by opponents.","Running stochastic simulations, we discover that the ambush strategy enhances the likelihood of individual success compared to direct attacks alone, leading to uneven spatial patterns characterised by spiral waves.","We compute the autocorrelation function and measure how the ambush tactic unbalances the organisms' spatial organisation by calculating the characteristic length scale of typical spatial domains of each species.","We demonstrate that the threshold for local species density influences the ambush strategy's effectiveness, while the neighbourhood perception range significantly impacts decision-making accuracy.","The outcomes show that long-range perception improves performance by over 60\\%, although there is potential interference in decision-making under high attack triggers.","Understanding how organisms' adaptation to their environment enhances their performance may be helpful not only for ecologists but also for data scientists aiming to improve artificial intelligence systems."],"url":"http://arxiv.org/abs/2405.02674v1","category":"q-bio.PE"}
{"created":"2024-05-04 13:25:06","title":"MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering","abstract":"A major roadblock in the seamless digitization of medical records remains the lack of interoperability of existing records. Extracting relevant medical information required for further treatment planning or even research is a time consuming labour intensive task involving the much valuable time of doctors. In this demo paper we present, MedPromptExtract an automated tool using a combination of semi supervised learning, large language models, natural lanuguage processing and prompt engineering to convert unstructured medical records to structured data which is amenable to further analysis.","sentences":["A major roadblock in the seamless digitization of medical records remains the lack of interoperability of existing records.","Extracting relevant medical information required for further treatment planning or even research is a time consuming labour intensive task involving the much valuable time of doctors.","In this demo paper we present, MedPromptExtract an automated tool using a combination of semi supervised learning, large language models, natural lanuguage processing and prompt engineering to convert unstructured medical records to structured data which is amenable to further analysis."],"url":"http://arxiv.org/abs/2405.02664v1","category":"cs.AI"}
{"created":"2024-05-04 12:42:55","title":"Enhancing Cooperation through Selective Interaction and Long-term Experiences in Multi-Agent Reinforcement Learning","abstract":"The significance of network structures in promoting group cooperation within social dilemmas has been widely recognized. Prior studies attribute this facilitation to the assortment of strategies driven by spatial interactions. Although reinforcement learning has been employed to investigate the impact of dynamic interaction on the evolution of cooperation, there remains a lack of understanding about how agents develop neighbour selection behaviours and the formation of strategic assortment within an explicit interaction structure. To address this, our study introduces a computational framework based on multi-agent reinforcement learning in the spatial Prisoner's Dilemma game. This framework allows agents to select dilemma strategies and interacting neighbours based on their long-term experiences, differing from existing research that relies on preset social norms or external incentives. By modelling each agent using two distinct Q-networks, we disentangle the coevolutionary dynamics between cooperation and interaction. The results indicate that long-term experience enables agents to develop the ability to identify non-cooperative neighbours and exhibit a preference for interaction with cooperative ones. This emergent self-organizing behaviour leads to the clustering of agents with similar strategies, thereby increasing network reciprocity and enhancing group cooperation.","sentences":["The significance of network structures in promoting group cooperation within social dilemmas has been widely recognized.","Prior studies attribute this facilitation to the assortment of strategies driven by spatial interactions.","Although reinforcement learning has been employed to investigate the impact of dynamic interaction on the evolution of cooperation, there remains a lack of understanding about how agents develop neighbour selection behaviours and the formation of strategic assortment within an explicit interaction structure.","To address this, our study introduces a computational framework based on multi-agent reinforcement learning in the spatial Prisoner's Dilemma game.","This framework allows agents to select dilemma strategies and interacting neighbours based on their long-term experiences, differing from existing research that relies on preset social norms or external incentives.","By modelling each agent using two distinct Q-networks, we disentangle the coevolutionary dynamics between cooperation and interaction.","The results indicate that long-term experience enables agents to develop the ability to identify non-cooperative neighbours and exhibit a preference for interaction with cooperative ones.","This emergent self-organizing behaviour leads to the clustering of agents with similar strategies, thereby increasing network reciprocity and enhancing group cooperation."],"url":"http://arxiv.org/abs/2405.02654v1","category":"cs.MA"}
{"created":"2024-05-04 12:39:15","title":"Isopignistic Canonical Decomposition via Belief Evolution Network","abstract":"Developing a general information processing model in uncertain environments is fundamental for the advancement of explainable artificial intelligence. Dempster-Shafer theory of evidence is a well-known and effective reasoning method for representing epistemic uncertainty, which is closely related to subjective probability theory and possibility theory. Although they can be transformed to each other under some particular belief structures, there remains a lack of a clear and interpretable transformation process, as well as a unified approach for information processing. In this paper, we aim to address these issues from the perspectives of isopignistic belief functions and the hyper-cautious transferable belief model. Firstly, we propose an isopignistic transformation based on the belief evolution network. This transformation allows for the adjustment of the information granule while retaining the potential decision outcome. The isopignistic transformation is integrated with a hyper-cautious transferable belief model to establish a new canonical decomposition. This decomposition offers a reverse path between the possibility distribution and its isopignistic mass functions. The result of the canonical decomposition, called isopignistic function, is an identical information content distribution to reflect the propensity and relative commitment degree of the BPA. Furthermore, this paper introduces a method to reconstruct the basic belief assignment by adjusting the isopignistic function. It explores the advantages of this approach in modeling and handling uncertainty within the hyper-cautious transferable belief model. More general, this paper establishes a theoretical basis for building general models of artificial intelligence based on probability theory, Dempster-Shafer theory, and possibility theory.","sentences":["Developing a general information processing model in uncertain environments is fundamental for the advancement of explainable artificial intelligence.","Dempster-Shafer theory of evidence is a well-known and effective reasoning method for representing epistemic uncertainty, which is closely related to subjective probability theory and possibility theory.","Although they can be transformed to each other under some particular belief structures, there remains a lack of a clear and interpretable transformation process, as well as a unified approach for information processing.","In this paper, we aim to address these issues from the perspectives of isopignistic belief functions and the hyper-cautious transferable belief model.","Firstly, we propose an isopignistic transformation based on the belief evolution network.","This transformation allows for the adjustment of the information granule while retaining the potential decision outcome.","The isopignistic transformation is integrated with a hyper-cautious transferable belief model to establish a new canonical decomposition.","This decomposition offers a reverse path between the possibility distribution and its isopignistic mass functions.","The result of the canonical decomposition, called isopignistic function, is an identical information content distribution to reflect the propensity and relative commitment degree of the BPA.","Furthermore, this paper introduces a method to reconstruct the basic belief assignment by adjusting the isopignistic function.","It explores the advantages of this approach in modeling and handling uncertainty within the hyper-cautious transferable belief model.","More general, this paper establishes a theoretical basis for building general models of artificial intelligence based on probability theory, Dempster-Shafer theory, and possibility theory."],"url":"http://arxiv.org/abs/2405.02653v1","category":"cs.AI"}
{"created":"2024-05-04 12:37:07","title":"Deep Pulse-Signal Magnification for remote Heart Rate Estimation in Compressed Videos","abstract":"Recent advancements in remote heart rate measurement (rPPG), motivated by data-driven approaches, have significantly improved accuracy. However, certain challenges, such as video compression, still remain: recovering the rPPG signal from highly compressed videos is particularly complex. Although several studies have highlighted the difficulties and impact of video compression for this, effective solutions remain limited. In this paper, we present a novel approach to address the impact of video compression on rPPG estimation, which leverages a pulse-signal magnification transformation to adapt compressed videos to an uncompressed data domain in which the rPPG signal is magnified. We validate the effectiveness of our model by exhaustive evaluations on two publicly available datasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database performance at several compression rates. Additionally, we assess the robustness of our approach on two additional highly compressed and widely-used datasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate estimation results.","sentences":["Recent advancements in remote heart rate measurement (rPPG), motivated by data-driven approaches, have significantly improved accuracy.","However, certain challenges, such as video compression, still remain: recovering the rPPG signal from highly compressed videos is particularly complex.","Although several studies have highlighted the difficulties and impact of video compression for this, effective solutions remain limited.","In this paper, we present a novel approach to address the impact of video compression on rPPG estimation, which leverages a pulse-signal magnification transformation to adapt compressed videos to an uncompressed data domain in which the rPPG signal is magnified.","We validate the effectiveness of our model by exhaustive evaluations on two publicly available datasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database performance at several compression rates.","Additionally, we assess the robustness of our approach on two additional highly compressed and widely-used datasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate estimation results."],"url":"http://arxiv.org/abs/2405.02652v1","category":"cs.CV"}
{"created":"2024-05-04 12:29:00","title":"Identifying Narrative Patterns and Outliers in Holocaust Testimonies Using Topic Modeling","abstract":"The vast collection of Holocaust survivor testimonies presents invaluable historical insights but poses challenges for manual analysis. This paper leverages advanced Natural Language Processing (NLP) techniques to explore the USC Shoah Foundation Holocaust testimony corpus. By treating testimonies as structured question-and-answer sections, we apply topic modeling to identify key themes. We experiment with BERTopic, which leverages recent advances in language modeling technology. We align testimony sections into fixed parts, revealing the evolution of topics across the corpus of testimonies. This highlights both a common narrative schema and divergences between subgroups based on age and gender. We introduce a novel method to identify testimonies within groups that exhibit atypical topic distributions resembling those of other groups. This study offers unique insights into the complex narratives of Holocaust survivors, demonstrating the power of NLP to illuminate historical discourse and identify potential deviations in survivor experiences.","sentences":["The vast collection of Holocaust survivor testimonies presents invaluable historical insights but poses challenges for manual analysis.","This paper leverages advanced Natural Language Processing (NLP) techniques to explore the USC Shoah Foundation Holocaust testimony corpus.","By treating testimonies as structured question-and-answer sections, we apply topic modeling to identify key themes.","We experiment with BERTopic, which leverages recent advances in language modeling technology.","We align testimony sections into fixed parts, revealing the evolution of topics across the corpus of testimonies.","This highlights both a common narrative schema and divergences between subgroups based on age and gender.","We introduce a novel method to identify testimonies within groups that exhibit atypical topic distributions resembling those of other groups.","This study offers unique insights into the complex narratives of Holocaust survivors, demonstrating the power of NLP to illuminate historical discourse and identify potential deviations in survivor experiences."],"url":"http://arxiv.org/abs/2405.02650v1","category":"cs.CL"}
{"created":"2024-05-04 12:24:29","title":"Generic Multi-modal Representation Learning for Network Traffic Analysis","abstract":"Network traffic analysis is fundamental for network management, troubleshooting, and security. Tasks such as traffic classification, anomaly detection, and novelty discovery are fundamental for extracting operational information from network data and measurements. We witness the shift from deep packet inspection and basic machine learning to Deep Learning (DL) approaches where researchers define and test a custom DL architecture designed for each specific problem. We here advocate the need for a general DL architecture flexible enough to solve different traffic analysis tasks. We test this idea by proposing a DL architecture based on generic data adaptation modules, followed by an integration module that summarises the extracted information into a compact and rich intermediate representation (i.e. embeddings). The result is a flexible Multi-modal Autoencoder (MAE) pipeline that can solve different use cases. We demonstrate the architecture with traffic classification (TC) tasks since they allow us to quantitatively compare results with state-of-the-art solutions. However, we argue that the MAE architecture is generic and can be used to learn representations useful in multiple scenarios. On TC, the MAE performs on par or better than alternatives while avoiding cumbersome feature engineering, thus streamlining the adoption of DL solutions for traffic analysis.","sentences":["Network traffic analysis is fundamental for network management, troubleshooting, and security.","Tasks such as traffic classification, anomaly detection, and novelty discovery are fundamental for extracting operational information from network data and measurements.","We witness the shift from deep packet inspection and basic machine learning to Deep Learning (DL) approaches where researchers define and test a custom DL architecture designed for each specific problem.","We here advocate the need for a general DL architecture flexible enough to solve different traffic analysis tasks.","We test this idea by proposing a DL architecture based on generic data adaptation modules, followed by an integration module that summarises the extracted information into a compact and rich intermediate representation (i.e. embeddings).","The result is a flexible Multi-modal Autoencoder (MAE) pipeline that can solve different use cases.","We demonstrate the architecture with traffic classification (TC) tasks since they allow us to quantitatively compare results with state-of-the-art solutions.","However, we argue that the MAE architecture is generic and can be used to learn representations useful in multiple scenarios.","On TC, the MAE performs on par or better than alternatives while avoiding cumbersome feature engineering, thus streamlining the adoption of DL solutions for traffic analysis."],"url":"http://arxiv.org/abs/2405.02649v1","category":"cs.LG"}
{"created":"2024-05-04 12:22:02","title":"A Conformal Prediction Score that is Robust to Label Noise","abstract":"Conformal Prediction (CP) quantifies network uncertainty by building a small prediction set with a pre-defined probability that the correct class is within this set. In this study we tackle the problem of CP calibration based on a validation set with noisy labels. We introduce a conformal score that is robust to label noise. The noise-free conformal score is estimated using the noisy labeled data and the noise level. In the test phase the noise-free score is used to form the prediction set. We applied the proposed algorithm to several standard medical imaging classification datasets. We show that our method outperforms current methods by a large margin, in terms of the average size of the prediction set, while maintaining the required coverage.","sentences":["Conformal Prediction (CP) quantifies network uncertainty by building a small prediction set with a pre-defined probability that the correct class is within this set.","In this study we tackle the problem of CP calibration based on a validation set with noisy labels.","We introduce a conformal score that is robust to label noise.","The noise-free conformal score is estimated using the noisy labeled data and the noise level.","In the test phase the noise-free score is used to form the prediction set.","We applied the proposed algorithm to several standard medical imaging classification datasets.","We show that our method outperforms current methods by a large margin, in terms of the average size of the prediction set, while maintaining the required coverage."],"url":"http://arxiv.org/abs/2405.02648v1","category":"cs.LG"}
{"created":"2024-05-04 11:47:32","title":"EM-based Algorithm for Unsupervised Clustering of Measurements from a Radar Sensor Network","abstract":"This paper deals with the problem of clustering data returned by a radar sensor network that monitors a region where multiple moving targets are present. The network is formed by nodes with limited functionalities that transmit the estimates of target positions (after a detection) to a fusion center without any association between measurements and targets. To solve the problem at hand, we resort to model-based learning algorithms and instead of applying the plain maximum likelihood approach, due to the related computational requirements, we exploit the latent variable model coupled with the expectation-maximization algorithm. The devised estimation procedure returns posterior probabilities that are used to cluster the huge amount of data collected by the fusion center. Remarkably, we also consider challenging scenarios with an unknown number of targets and estimate it by means of the model order selection rules. The clustering performance of the proposed strategy is compared to that of conventional data-driven methods over synthetic data. The numerical examples point out that the herein proposed solutions can provide reliable clustering performance overcoming the considered competitors.","sentences":["This paper deals with the problem of clustering data returned by a radar sensor network that monitors a region where multiple moving targets are present.","The network is formed by nodes with limited functionalities that transmit the estimates of target positions (after a detection) to a fusion center without any association between measurements and targets.","To solve the problem at hand, we resort to model-based learning algorithms and instead of applying the plain maximum likelihood approach, due to the related computational requirements, we exploit the latent variable model coupled with the expectation-maximization algorithm.","The devised estimation procedure returns posterior probabilities that are used to cluster the huge amount of data collected by the fusion center.","Remarkably, we also consider challenging scenarios with an unknown number of targets and estimate it by means of the model order selection rules.","The clustering performance of the proposed strategy is compared to that of conventional data-driven methods over synthetic data.","The numerical examples point out that the herein proposed solutions can provide reliable clustering performance overcoming the considered competitors."],"url":"http://arxiv.org/abs/2405.02643v1","category":"eess.SP"}
{"created":"2024-05-04 11:22:53","title":"PrivSGP-VR: Differentially Private Variance-Reduced Stochastic Gradient Push with Tight Utility Bounds","abstract":"In this paper, we propose a differentially private decentralized learning method (termed PrivSGP-VR) which employs stochastic gradient push with variance reduction and guarantees $(\\epsilon, \\delta)$-differential privacy (DP) for each node. Our theoretical analysis shows that, under DP Gaussian noise with constant variance, PrivSGP-VR achieves a sub-linear convergence rate of $\\mathcal{O}(1/\\sqrt{nK})$, where $n$ and $K$ are the number of nodes and iterations, respectively, which is independent of stochastic gradient variance, and achieves a linear speedup with respect to $n$. Leveraging the moments accountant method, we further derive an optimal $K$ to maximize the model utility under certain privacy budget in decentralized settings. With this optimized $K$, PrivSGP-VR achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}{\\delta} \\right)}/(\\sqrt{n}J\\epsilon) \\right)$, where $J$ and $d$ are the number of local samples and the dimension of decision variable, respectively, which matches that of the server-client distributed counterparts, and exhibits an extra factor of $1/\\sqrt{n}$ improvement compared to that of the existing decentralized counterparts, such as A(DP)$^2$SGD. Extensive experiments corroborate our theoretical findings, especially in terms of the maximized utility with optimized $K$, in fully decentralized settings.","sentences":["In this paper, we propose a differentially private decentralized learning method (termed PrivSGP-VR) which employs stochastic gradient push with variance reduction and guarantees $(\\epsilon, \\delta)$-differential privacy (DP) for each node.","Our theoretical analysis shows that, under DP Gaussian noise with constant variance, PrivSGP-VR achieves a sub-linear convergence rate of $\\mathcal{O}(1/\\sqrt{nK})$, where $n$ and $K$ are the number of nodes and iterations, respectively, which is independent of stochastic gradient variance, and achieves a linear speedup with respect to $n$. Leveraging the moments accountant method, we further derive an optimal $K$ to maximize the model utility under certain privacy budget in decentralized settings.","With this optimized $K$, PrivSGP-VR achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}{\\delta} \\right)}/(\\sqrt{n}J\\epsilon) \\right)$, where $J$ and $d$ are the number of local samples and the dimension of decision variable, respectively, which matches that of the server-client distributed counterparts, and exhibits an extra factor of $1/\\sqrt{n}$ improvement compared to that of the existing decentralized counterparts, such as A(DP)$^2$SGD.","Extensive experiments corroborate our theoretical findings, especially in terms of the maximized utility with optimized $K$, in fully decentralized settings."],"url":"http://arxiv.org/abs/2405.02638v1","category":"cs.LG"}
{"created":"2024-05-04 11:22:16","title":"TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants","abstract":"Conversational information seeking has evolved rapidly in the last few years with the development of Large Language Models (LLMs), providing the basis for interpreting and responding in a naturalistic manner to user requests. The extended TREC Interactive Knowledge Assistance Track (iKAT) collection aims to enable researchers to test and evaluate their Conversational Search Agents (CSA). The collection contains a set of 36 personalized dialogues over 20 different topics each coupled with a Personal Text Knowledge Base (PTKB) that defines the bespoke user personas. A total of 344 turns with approximately 26,000 passages are provided as assessments on relevance, as well as additional assessments on generated responses over four key dimensions: relevance, completeness, groundedness, and naturalness. The collection challenges CSA to efficiently navigate diverse personal contexts, elicit pertinent persona information, and employ context for relevant conversations. The integration of a PTKB and the emphasis on decisional search tasks contribute to the uniqueness of this test collection, making it an essential benchmark for advancing research in conversational and interactive knowledge assistants.","sentences":["Conversational information seeking has evolved rapidly in the last few years with the development of Large Language Models (LLMs), providing the basis for interpreting and responding in a naturalistic manner to user requests.","The extended TREC Interactive Knowledge Assistance Track (iKAT) collection aims to enable researchers to test and evaluate their Conversational Search Agents (CSA).","The collection contains a set of 36 personalized dialogues over 20 different topics each coupled with a Personal Text Knowledge Base (PTKB) that defines the bespoke user personas.","A total of 344 turns with approximately 26,000 passages are provided as assessments on relevance, as well as additional assessments on generated responses over four key dimensions: relevance, completeness, groundedness, and naturalness.","The collection challenges CSA to efficiently navigate diverse personal contexts, elicit pertinent persona information, and employ context for relevant conversations.","The integration of a PTKB and the emphasis on decisional search tasks contribute to the uniqueness of this test collection, making it an essential benchmark for advancing research in conversational and interactive knowledge assistants."],"url":"http://arxiv.org/abs/2405.02637v1","category":"cs.IR"}
{"created":"2024-05-04 11:05:52","title":"Onboard Out-of-Calibration Detection of Deep Learning Models using Conformal Prediction","abstract":"The black box nature of deep learning models complicate their usage in critical applications such as remote sensing. Conformal prediction is a method to ensure trust in such scenarios. Subject to data exchangeability, conformal prediction provides finite sample coverage guarantees in the form of a prediction set that is guaranteed to contain the true class within a user defined error rate. In this letter we show that conformal prediction algorithms are related to the uncertainty of the deep learning model and that this relation can be used to detect if the deep learning model is out-of-calibration. Popular classification models like Resnet50, Densenet161, InceptionV3, and MobileNetV2 are applied on remote sensing datasets such as the EuroSAT to demonstrate how under noisy scenarios the model outputs become untrustworthy. Furthermore an out-of-calibration detection procedure relating the model uncertainty and the average size of the conformal prediction set is presented.","sentences":["The black box nature of deep learning models complicate their usage in critical applications such as remote sensing.","Conformal prediction is a method to ensure trust in such scenarios.","Subject to data exchangeability, conformal prediction provides finite sample coverage guarantees in the form of a prediction set that is guaranteed to contain the true class within a user defined error rate.","In this letter we show that conformal prediction algorithms are related to the uncertainty of the deep learning model and that this relation can be used to detect if the deep learning model is out-of-calibration.","Popular classification models like Resnet50, Densenet161, InceptionV3, and MobileNetV2 are applied on remote sensing datasets such as the EuroSAT to demonstrate how under noisy scenarios the model outputs become untrustworthy.","Furthermore an out-of-calibration detection procedure relating the model uncertainty and the average size of the conformal prediction set is presented."],"url":"http://arxiv.org/abs/2405.02634v1","category":"cs.LG"}
{"created":"2024-05-04 10:09:27","title":"Contrastive Dual-Interaction Graph Neural Network for Molecular Property Prediction","abstract":"Molecular property prediction is a key component of AI-driven drug discovery and molecular characterization learning. Despite recent advances, existing methods still face challenges such as limited ability to generalize, and inadequate representation of learning from unlabeled data, especially for tasks specific to molecular structures. To address these limitations, we introduce DIG-Mol, a novel self-supervised graph neural network framework for molecular property prediction. This architecture leverages the power of contrast learning with dual interaction mechanisms and unique molecular graph enhancement strategies. DIG-Mol integrates a momentum distillation network with two interconnected networks to efficiently improve molecular characterization. The framework's ability to extract key information about molecular structure and higher-order semantics is supported by minimizing loss of contrast. We have established DIG-Mol's state-of-the-art performance through extensive experimental evaluation in a variety of molecular property prediction tasks. In addition to demonstrating superior transferability in a small number of learning scenarios, our visualizations highlight DIG-Mol's enhanced interpretability and representation capabilities. These findings confirm the effectiveness of our approach in overcoming challenges faced by traditional methods and mark a significant advance in molecular property prediction.","sentences":["Molecular property prediction is a key component of AI-driven drug discovery and molecular characterization learning.","Despite recent advances, existing methods still face challenges such as limited ability to generalize, and inadequate representation of learning from unlabeled data, especially for tasks specific to molecular structures.","To address these limitations, we introduce DIG-Mol, a novel self-supervised graph neural network framework for molecular property prediction.","This architecture leverages the power of contrast learning with dual interaction mechanisms and unique molecular graph enhancement strategies.","DIG-Mol integrates a momentum distillation network with two interconnected networks to efficiently improve molecular characterization.","The framework's ability to extract key information about molecular structure and higher-order semantics is supported by minimizing loss of contrast.","We have established DIG-Mol's state-of-the-art performance through extensive experimental evaluation in a variety of molecular property prediction tasks.","In addition to demonstrating superior transferability in a small number of learning scenarios, our visualizations highlight DIG-Mol's enhanced interpretability and representation capabilities.","These findings confirm the effectiveness of our approach in overcoming challenges faced by traditional methods and mark a significant advance in molecular property prediction."],"url":"http://arxiv.org/abs/2405.02628v1","category":"cs.LG"}
{"created":"2024-05-04 08:43:45","title":"Learning Linear Utility Functions From Pairwise Comparison Queries","abstract":"We study learnability of linear utility functions from pairwise comparison queries. In particular, we consider two learning objectives. The first objective is to predict out-of-sample responses to pairwise comparisons, whereas the second is to approximately recover the true parameters of the utility function. We show that in the passive learning setting, linear utilities are efficiently learnable with respect to the first objective, both when query responses are uncorrupted by noise, and under Tsybakov noise when the distributions are sufficiently \"nice\". In contrast, we show that utility parameters are not learnable for a large set of data distributions without strong modeling assumptions, even when query responses are noise-free. Next, we proceed to analyze the learning problem in an active learning setting. In this case, we show that even the second objective is efficiently learnable, and present algorithms for both the noise-free and noisy query response settings. Our results thus exhibit a qualitative learnability gap between passive and active learning from pairwise preference queries, demonstrating the value of the ability to select pairwise queries for utility learning.","sentences":["We study learnability of linear utility functions from pairwise comparison queries.","In particular, we consider two learning objectives.","The first objective is to predict out-of-sample responses to pairwise comparisons, whereas the second is to approximately recover the true parameters of the utility function.","We show that in the passive learning setting, linear utilities are efficiently learnable with respect to the first objective, both when query responses are uncorrupted by noise, and under Tsybakov noise when the distributions are sufficiently \"nice\".","In contrast, we show that utility parameters are not learnable for a large set of data distributions without strong modeling assumptions, even when query responses are noise-free.","Next, we proceed to analyze the learning problem in an active learning setting.","In this case, we show that even the second objective is efficiently learnable, and present algorithms for both the noise-free and noisy query response settings.","Our results thus exhibit a qualitative learnability gap between passive and active learning from pairwise preference queries, demonstrating the value of the ability to select pairwise queries for utility learning."],"url":"http://arxiv.org/abs/2405.02612v1","category":"cs.LG"}
{"created":"2024-05-04 08:27:12","title":"UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model","abstract":"Traditional unsupervised optical flow methods are vulnerable to occlusions and motion boundaries due to lack of object-level information. Therefore, we propose UnSAMFlow, an unsupervised flow network that also leverages object information from the latest foundation model Segment Anything Model (SAM). We first include a self-supervised semantic augmentation module tailored to SAM masks. We also analyze the poor gradient landscapes of traditional smoothness losses and propose a new smoothness definition based on homography instead. A simple yet effective mask feature module has also been added to further aggregate features on the object level. With all these adaptations, our method produces clear optical flow estimation with sharp boundaries around objects, which outperforms state-of-the-art methods on both KITTI and Sintel datasets. Our method also generalizes well across domains and runs very efficiently.","sentences":["Traditional unsupervised optical flow methods are vulnerable to occlusions and motion boundaries due to lack of object-level information.","Therefore, we propose UnSAMFlow, an unsupervised flow network that also leverages object information from the latest foundation model Segment Anything Model (SAM).","We first include a self-supervised semantic augmentation module tailored to SAM masks.","We also analyze the poor gradient landscapes of traditional smoothness losses and propose a new smoothness definition based on homography instead.","A simple yet effective mask feature module has also been added to further aggregate features on the object level.","With all these adaptations, our method produces clear optical flow estimation with sharp boundaries around objects, which outperforms state-of-the-art methods on both KITTI and Sintel datasets.","Our method also generalizes well across domains and runs very efficiently."],"url":"http://arxiv.org/abs/2405.02608v1","category":"cs.CV"}
{"created":"2024-05-06 17:56:59","title":"Low complexity among principal fully irreducible elements of Out($F_3$)","abstract":"We find the shortest realized stretch factor for a fully irreducible $\\varphi\\in\\mathrm{Out}(F_3)$ and show that it is realized by a \"principal\" fully irreducible element. We also show that it is the only principal fully irreducible produced by a single fold in any rank.","sentences":["We find the shortest realized stretch factor for a fully irreducible $\\varphi\\in\\mathrm{Out}(F_3)$ and show that it is realized by a \"principal\" fully irreducible element.","We also show that it is the only principal fully irreducible produced by a single fold in any rank."],"url":"http://arxiv.org/abs/2405.03681v1","category":"math.GR"}
{"created":"2024-05-06 17:46:17","title":"Harnessing metastability for grain size control in multiprincipal element alloys during additive manufacturing","abstract":"Controlling microstructure in fusion-based metal additive manufacturing (AM) remains a challenge due to numerous parameters directly impacting solidification conditions. Multiprincipal element alloys (MPEAs) offer a vast compositional design space for microstructural engineering due to their chemical complexity and exceptional properties. Here, we establish a novel alloy design paradigm in MPEAs for AM using the FeMnCoCr system. By exploiting the decreasing phase stability with increasing Mn content, we achieve notable grain refinement and breakdown of columnar grain growth. We combine thermodynamic modeling, operando synchrotron X-ray diffraction, multiscale microstructural characterization, and mechanical testing to gain insight into the solidification physics and its ramifications on the resulting microstructure. This work paves way for tailoring grain sizes through targeted manipulation of phase stability, thereby advancing microstructure control in AM.","sentences":["Controlling microstructure in fusion-based metal additive manufacturing (AM) remains a challenge due to numerous parameters directly impacting solidification conditions.","Multiprincipal element alloys (MPEAs) offer a vast compositional design space for microstructural engineering due to their chemical complexity and exceptional properties.","Here, we establish a novel alloy design paradigm in MPEAs for AM using the FeMnCoCr system.","By exploiting the decreasing phase stability with increasing Mn content, we achieve notable grain refinement and breakdown of columnar grain growth.","We combine thermodynamic modeling, operando synchrotron X-ray diffraction, multiscale microstructural characterization, and mechanical testing to gain insight into the solidification physics and its ramifications on the resulting microstructure.","This work paves way for tailoring grain sizes through targeted manipulation of phase stability, thereby advancing microstructure control in AM."],"url":"http://arxiv.org/abs/2405.03670v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 17:44:40","title":"Data-Driven Model Identification Near a Supercritical Hopf Bifurcation Using Phase-Based Approaches","abstract":"A data-driven model identification strategy is developed for dynamical systems near a supercritical Hopf bifurcation with nonautonomous inputs. This strategy draws on phase-amplitude reduction techniques, leveraging an analytical representation for the phase and amplitude response curves of the Hopf normal form to infer system parameters. Fitting can be performed by recording the system output during the relaxation to the stable limit cycle after applying as few as two carefully timed pulse inputs. This strategy is illustrated in two examples with relevance to circadian oscillations. In each example, the proposed model identification strategy allows for the formulation, solution, and implementation of a closed loop nonlinear optimal control problem.","sentences":["A data-driven model identification strategy is developed for dynamical systems near a supercritical Hopf bifurcation with nonautonomous inputs.","This strategy draws on phase-amplitude reduction techniques, leveraging an analytical representation for the phase and amplitude response curves of the Hopf normal form to infer system parameters.","Fitting can be performed by recording the system output during the relaxation to the stable limit cycle after applying as few as two carefully timed pulse inputs.","This strategy is illustrated in two examples with relevance to circadian oscillations.","In each example, the proposed model identification strategy allows for the formulation, solution, and implementation of a closed loop nonlinear optimal control problem."],"url":"http://arxiv.org/abs/2405.03668v1","category":"math.DS"}
{"created":"2024-05-06 17:43:39","title":"Fault Detection and Monitoring using an Information-Driven Strategy: Method, Theory, and Application","abstract":"The ability to detect when a system undergoes an incipient fault is of paramount importance in preventing a critical failure. In this work, we propose an information-driven fault detection method based on a novel concept drift detector. The method is tailored to identifying drifts in input-output relationships of additive noise models (i.e., model drifts) and is based on a distribution-free mutual information (MI) estimator. Our scheme does not require prior faulty examples and can be applied distribution-free over a large class of system models. Our core contributions are twofold. First, we demonstrate the connection between fault detection, model drift detection, and testing independence between two random variables. Second, we prove several theoretical properties of the proposed MI-based fault detection scheme: (i) strong consistency, (ii) exponentially fast detection of the non-faulty case, and (iii) control of both significance levels and power of the test. To conclude, we validate our theory with synthetic data and the benchmark dataset N-CMAPSS of aircraft turbofan engines. These empirical results support the usefulness of our methodology in many practical and realistic settings, and the theoretical results show performance guarantees that other methods cannot offer.","sentences":["The ability to detect when a system undergoes an incipient fault is of paramount importance in preventing a critical failure.","In this work, we propose an information-driven fault detection method based on a novel concept drift detector.","The method is tailored to identifying drifts in input-output relationships of additive noise models (i.e., model drifts) and is based on a distribution-free mutual information (MI) estimator.","Our scheme does not require prior faulty examples and can be applied distribution-free over a large class of system models.","Our core contributions are twofold.","First, we demonstrate the connection between fault detection, model drift detection, and testing independence between two random variables.","Second, we prove several theoretical properties of the proposed MI-based fault detection scheme: (i) strong consistency, (ii) exponentially fast detection of the non-faulty case, and (iii) control of both significance levels and power of the test.","To conclude, we validate our theory with synthetic data and the benchmark dataset N-CMAPSS of aircraft turbofan engines.","These empirical results support the usefulness of our methodology in many practical and realistic settings, and the theoretical results show performance guarantees that other methods cannot offer."],"url":"http://arxiv.org/abs/2405.03667v1","category":"eess.SP"}
{"created":"2024-05-06 17:33:42","title":"Computational complexity and quantum interpretations","abstract":"In computational complexity theory, it remains to be understood whether $\\textbf{BQP}$ is the same as $\\textbf{BPP}$. Prima facie, one would expect that this mathematical question is quite unrelated to the foundational question of whether the quantum state is an element of reality or of the observer's knowledge. By contrast, here we argue that the complexity of computation in a physical theory may constrain its physical interpretation. Specifically in the quantum case, we argue that a subjective interpretation of the quantum mechanics favors the proposition $\\textbf{BQP} = \\textbf{BPP}$. Therefore, if $\\textbf{BPP} \\subset \\textbf{BQP}$, then a realist interpretation of quantum mechanics would be favored.","sentences":["In computational complexity theory, it remains to be understood whether $\\textbf{BQP}$ is the same as $\\textbf{BPP}$. Prima facie, one would expect that this mathematical question is quite unrelated to the foundational question of whether the quantum state is an element of reality or of the observer's knowledge.","By contrast, here we argue that the complexity of computation in a physical theory may constrain its physical interpretation.","Specifically in the quantum case, we argue that a subjective interpretation of the quantum mechanics favors the proposition $\\textbf{BQP} = \\textbf{BPP}$.","Therefore, if $\\textbf{BPP} \\subset \\textbf{BQP}$, then a realist interpretation of quantum mechanics would be favored."],"url":"http://arxiv.org/abs/2405.03657v1","category":"quant-ph"}
{"created":"2024-05-06 17:09:41","title":"Entanglement in selected Binary Tree States: Dicke/Total spin states, particle number projected BCS states","abstract":"Binary Tree States (BTS) are states whose decomposition on a quantum register basis formed by a set of qubits can be made sequentially. Such states sometimes appear naturally in many-body systems treated in Fock space when a global symmetry is imposed, like the total spin or particle number symmetries. Examples are the Dicke states, the eigenstates of the total spin for a set of particles having individual spin $1/2$, or states obtained by projecting a BCS states onto particle number, also called projected BCS in small superfluid systems. Starting from a BTS state described on the set of $n$ qubits or orbitals, the entanglement entropy of any subset of $ k$ qubits is analyzed. Specifically, a practical method is developed to access the $k$ qubits/particles von Neumann entanglement entropy of the subsystem of interest. Properties of these entropies are discussed, including scaling properties, upper bounds, or how these entropies correlate with fluctuations. Illustrations are given for the Dicke state and the projected BCS states.","sentences":["Binary Tree States (BTS) are states whose decomposition on a quantum register basis formed by a set of qubits can be made sequentially.","Such states sometimes appear naturally in many-body systems treated in Fock space when a global symmetry is imposed, like the total spin or particle number symmetries.","Examples are the Dicke states, the eigenstates of the total spin for a set of particles having individual spin $1/2$, or states obtained by projecting a BCS states onto particle number, also called projected BCS in small superfluid systems.","Starting from a BTS state described on the set of $n$ qubits or orbitals, the entanglement entropy of any subset of $ k$ qubits is analyzed.","Specifically, a practical method is developed to access the $k$ qubits/particles von Neumann entanglement entropy of the subsystem of interest.","Properties of these entropies are discussed, including scaling properties, upper bounds, or how these entropies correlate with fluctuations.","Illustrations are given for the Dicke state and the projected BCS states."],"url":"http://arxiv.org/abs/2405.03647v1","category":"nucl-th"}
{"created":"2024-05-06 17:09:10","title":"Content-Oblivious Leader Election on Rings","abstract":"In content-oblivious computation, n nodes wish to compute a given task over an asynchronous network that suffers from an extremely harsh type of noise, which corrupts the content of all messages across all channels. In a recent work, Censor-Hillel, Cohen, Gelles, and Sela (Distributed Computing, 2023) showed how to perform arbitrary computations in a content-oblivious way in 2-edge connected networks but only if the network has a distinguished node (called root) to initiate the computation.   Our goal is to remove this assumption, which was conjectured to be necessary. Achieving this goal essentially reduces to performing a content-oblivious leader election since an elected leader can then serve as the root required to perform arbitrary content-oblivious computations. We focus on ring networks, which are the simplest 2-edge connected graphs. On oriented rings, we obtain a leader election algorithm with message complexity O(n*ID_max), where ID_max is the maximal assigned ID. As it turns out, this dependency on $ID_max$ is inherent: we show a lower bound of Omega(n*log(ID_max/n)) messages for content-oblivious leader election algorithms. We also extend our results to non-oriented rings, where nodes cannot tell which channel leads to which neighbor. In this case, however, the algorithm does not terminate but only reaches quiescence.","sentences":["In content-oblivious computation, n nodes wish to compute a given task over an asynchronous network that suffers from an extremely harsh type of noise, which corrupts the content of all messages across all channels.","In a recent work, Censor-Hillel, Cohen, Gelles, and Sela (Distributed Computing, 2023) showed how to perform arbitrary computations in a content-oblivious way in 2-edge connected networks but only if the network has a distinguished node (called root) to initiate the computation.   ","Our goal is to remove this assumption, which was conjectured to be necessary.","Achieving this goal essentially reduces to performing a content-oblivious leader election since an elected leader can then serve as the root required to perform arbitrary content-oblivious computations.","We focus on ring networks, which are the simplest 2-edge connected graphs.","On oriented rings, we obtain a leader election algorithm with message complexity O(n*ID_max), where ID_max is the maximal assigned ID.","As it turns out, this dependency on $ID_max$ is inherent: we show a lower bound of Omega(n*log(ID_max/n)) messages for content-oblivious leader election algorithms.","We also extend our results to non-oriented rings, where nodes cannot tell which channel leads to which neighbor.","In this case, however, the algorithm does not terminate but only reaches quiescence."],"url":"http://arxiv.org/abs/2405.03646v1","category":"cs.DS"}
{"created":"2024-05-06 17:03:04","title":"Unsolved problems on joinings, multiple mixing, spectrum, and rank","abstract":"The note is devoted to multiple mixing, spectrum, rank and self-joinings of measure-preserving transformations. We recall famous open problems, discuss related questions and some known results. A hypothetical example of an automorphism of the class $MSJ(2)\\setminus MSJ(3)$ has Lebesgue spectrum, infinite rank and does not have multiple mixing. If its spectrum is simple, then we get a solution to the problems of Banach, Rokhlin and del Junco-Rudolph. The existence of such an example, of course, seems unlikely, but any facts confirming the impossibility of this amazing situation have not yet been discovered. They not found even under the condition that its local rank is positive, which ensures the finite spectral multiplicity.","sentences":["The note is devoted to multiple mixing, spectrum, rank and self-joinings of measure-preserving transformations.","We recall famous open problems, discuss related questions and some known results.","A hypothetical example of an automorphism of the class $MSJ(2)\\setminus MSJ(3)$ has Lebesgue spectrum, infinite rank and does not have multiple mixing.","If its spectrum is simple, then we get a solution to the problems of Banach, Rokhlin and del Junco-Rudolph.","The existence of such an example, of course, seems unlikely, but any facts confirming the impossibility of this amazing situation have not yet been discovered.","They not found even under the condition that its local rank is positive, which ensures the finite spectral multiplicity."],"url":"http://arxiv.org/abs/2405.03641v1","category":"math.DS"}
{"created":"2024-05-06 16:53:05","title":"Nonequilibrium relaxation and odd-even effect in finite-temperature electron gases","abstract":"Pauli blocking in Fermi liquids imposes strong phase-space constraints on quasiparticle lifetimes, leading to a well-known quadratic-in-temperature decay rate of quasiparticle modes at low temperatures. In two-dimensional systems, however, even longer-lived modes are predicted (dubbed \"odd-parity\" modes). Here, we present an efficient method to evaluate the full spectrum of relaxational eigenmodes of a Fermi liquid within kinetic theory. We employ this method to study the experimentally relevant case of a Fermi liquid with screened Coulomb interactions and map out the decay rates of quasiparticle modes beyond the asymptotic low temperature limit up to the Fermi temperature, thus covering the temperature range of typical experiments. We confirm the existence of anomalously long-lived odd-parity modes and provide a comprehensive classification and detailed analysis of the relaxation spectrum. In particular, we find that (i) the odd-parity effect in the decay rates extends to temperatures as large as $T=0.1T_F$; (ii) there is only a small number of long-lived odd-parity modes, with an infinite number of remaining modes that show standard Fermi-liquid scaling; (iii) the ratio between the odd- and even-parity lifetimes is tunable with the Coulomb interaction strength, not just temperature, which reflects a difference in the microscopic relaxation mechanism of the modes. Our findings provide a comprehensive description of the nonequilibrium relaxation behavior of two-dimensional electron gases and bridge a significant gap in our understanding of these systems.","sentences":["Pauli blocking in Fermi liquids imposes strong phase-space constraints on quasiparticle lifetimes, leading to a well-known quadratic-in-temperature decay rate of quasiparticle modes at low temperatures.","In two-dimensional systems, however, even longer-lived modes are predicted (dubbed \"odd-parity\" modes).","Here, we present an efficient method to evaluate the full spectrum of relaxational eigenmodes of a Fermi liquid within kinetic theory.","We employ this method to study the experimentally relevant case of a Fermi liquid with screened Coulomb interactions and map out the decay rates of quasiparticle modes beyond the asymptotic low temperature limit up to the Fermi temperature, thus covering the temperature range of typical experiments.","We confirm the existence of anomalously long-lived odd-parity modes and provide a comprehensive classification and detailed analysis of the relaxation spectrum.","In particular, we find that (i) the odd-parity effect in the decay rates extends to temperatures as large as $T=0.1T_F$; (ii) there is only a small number of long-lived odd-parity modes, with an infinite number of remaining modes that show standard Fermi-liquid scaling; (iii) the ratio between the odd- and even-parity lifetimes is tunable with the Coulomb interaction strength, not just temperature, which reflects a difference in the microscopic relaxation mechanism of the modes.","Our findings provide a comprehensive description of the nonequilibrium relaxation behavior of two-dimensional electron gases and bridge a significant gap in our understanding of these systems."],"url":"http://arxiv.org/abs/2405.03635v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 16:50:42","title":"Neural Graph Mapping for Dense SLAM with Efficient Loop Closure","abstract":"Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation. This prevents efficient incorporation of loop closure constraints and limits scalability. To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system. Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration. Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime. Our code is available at https://kth-rpl.github.io/neural_graph_mapping/.","sentences":["Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation.","This prevents efficient incorporation of loop closure constraints and limits scalability.","To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system.","Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration.","Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime.","Our code is available at https://kth-rpl.github.io/neural_graph_mapping/."],"url":"http://arxiv.org/abs/2405.03633v1","category":"cs.CV"}
{"created":"2024-05-06 16:48:44","title":"Bifurcations and phase-space structures in KCN molecular system","abstract":"In this work, we analyze the evolution of the phase-space structures of KCN molecular system as a function of the vibrational energy using Lagrangian descriptors. For low energies, the motion is mostly regular around the absolute minimum of the potential energy surface. As the energy increases, the phase space combines regions with regular and chaotic motion, a difference that is well captured by the Lagrangian descriptors. We show that the dynamics is mostly governed by the invariant manifolds of the stretch periodic orbits located at the top of one of the energetic barriers of the system. Furthermore, we show a perfect agreement between the bifurcation theory and the differences observed in the phase-space structures as the vibrational energy is modified. The accuracy of our calculations is also assessed by explicit comparison with the invariant manifolds computed using linear dynamics.","sentences":["In this work, we analyze the evolution of the phase-space structures of KCN molecular system as a function of the vibrational energy using Lagrangian descriptors.","For low energies, the motion is mostly regular around the absolute minimum of the potential energy surface.","As the energy increases, the phase space combines regions with regular and chaotic motion, a difference that is well captured by the Lagrangian descriptors.","We show that the dynamics is mostly governed by the invariant manifolds of the stretch periodic orbits located at the top of one of the energetic barriers of the system.","Furthermore, we show a perfect agreement between the bifurcation theory and the differences observed in the phase-space structures as the vibrational energy is modified.","The accuracy of our calculations is also assessed by explicit comparison with the invariant manifolds computed using linear dynamics."],"url":"http://arxiv.org/abs/2405.03631v1","category":"nlin.CD"}
{"created":"2024-05-06 16:47:47","title":"Krylov complexity of deformed conformal field theories","abstract":"We consider a perturbative expansion of the Lanczos coefficients and the Krylov complexity for two-dimensional conformal field theories under integrable deformations. Specifically, we explore the consequences of $T\\bar{T}$, $J\\bar{{T}}$, and $J\\bar{{J}}$ deformations, focusing on first-order corrections in the deformation parameter. Under $T\\bar{{T}}$ deformation, we demonstrate that the Lanczos coefficients $b_n$ exhibit unexpected behavior, deviating from linear growth within the valid perturbative regime. Notably, the Krylov exponent characterizing the rate of exponential growth of complexity surpasses that of the undeformed theory, suggesting a potential violation of the conjectured operator growth bound within the realm of perturbative analysis. In contrast to this, both $J\\bar{{J}}$ and $J\\bar{{T}}$ deformations induce no first order correction to either the linear growth of Lanczos coefficients at large-$n$ or the Krylov exponent and hence the results for these two deformations align with those of the undeformed theory.","sentences":["We consider a perturbative expansion of the Lanczos coefficients and the Krylov complexity for two-dimensional conformal field theories under integrable deformations.","Specifically, we explore the consequences of $T\\bar{T}$, $J\\bar{{T}}$, and $J\\bar{{J}}$ deformations, focusing on first-order corrections in the deformation parameter.","Under $T\\bar{{T}}$ deformation, we demonstrate that the Lanczos coefficients $b_n$ exhibit unexpected behavior, deviating from linear growth within the valid perturbative regime.","Notably, the Krylov exponent characterizing the rate of exponential growth of complexity surpasses that of the undeformed theory, suggesting a potential violation of the conjectured operator growth bound within the realm of perturbative analysis.","In contrast to this, both $J\\bar{{J}}$ and $J\\bar{{T}}$ deformations induce no first order correction to either the linear growth of Lanczos coefficients at large-$n$ or the Krylov exponent and hence the results for these two deformations align with those of the undeformed theory."],"url":"http://arxiv.org/abs/2405.03630v1","category":"hep-th"}
{"created":"2024-05-06 16:47:01","title":"Configuration-Constrained Tube MPC for Tracking","abstract":"This paper proposes a novel tube-based Model Predictive Control (MPC) framework for tracking varying setpoint references with linear systems subject to additive and multiplicative uncertainties. The MPC controllers designed using this framework exhibit recursively feasible for changing references, and robust asymptotic stability for piecewise constant references. The framework leverages configuration-constrained polytopes to parameterize the tubes, offering flexibility to optimize their shape. The efficacy of the approach is demonstrated through two numerical examples. The first example illustrates the theoretical results, and the second uses the framework to design a lane-change controller for an autonomous vehicle.","sentences":["This paper proposes a novel tube-based Model Predictive Control (MPC) framework for tracking varying setpoint references with linear systems subject to additive and multiplicative uncertainties.","The MPC controllers designed using this framework exhibit recursively feasible for changing references, and robust asymptotic stability for piecewise constant references.","The framework leverages configuration-constrained polytopes to parameterize the tubes, offering flexibility to optimize their shape.","The efficacy of the approach is demonstrated through two numerical examples.","The first example illustrates the theoretical results, and the second uses the framework to design a lane-change controller for an autonomous vehicle."],"url":"http://arxiv.org/abs/2405.03629v1","category":"eess.SY"}
{"created":"2024-05-06 16:45:48","title":"State-Aware Timeliness in Energy Harvesting IoT Systems Monitoring a Markovian Source","abstract":"In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source. The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state. We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process. We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process. Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem. Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states.","sentences":["In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source.","The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state.","We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process.","We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process.","Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem.","Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states."],"url":"http://arxiv.org/abs/2405.03628v1","category":"cs.IT"}
{"created":"2024-05-06 16:34:44","title":"The trade-offs between Monolithic vs. Distributed Architectures","abstract":"Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages. Such outcomes can detrimentally affect a company's business operations and resource allocation. This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics. It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications. Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications. A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance.","sentences":["Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages.","Such outcomes can detrimentally affect a company's business operations and resource allocation.","This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics.","It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications.","Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications.","A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance."],"url":"http://arxiv.org/abs/2405.03619v1","category":"cs.SE"}
{"created":"2024-05-06 16:31:11","title":"Fields of definition of dynamical systems on $\\mathbb{P}^{1}$. Improvements on a result of Silverman","abstract":"J. Silverman proved that a dynamical system on $\\mathbb{P}^{1}$ descends to the field of moduli if it is polynomial or it has even degree, but for non-polynomial ones of odd degree the picture is less clear. We give a complete characterization of which dynamical systems over $\\mathbb{P}^{1}$ descend to the field of moduli.","sentences":["J. Silverman proved that a dynamical system on $\\mathbb{P}^{1}$ descends to the field of moduli if it is polynomial or it has even degree, but for non-polynomial ones of odd degree the picture is less clear.","We give a complete characterization of which dynamical systems over $\\mathbb{P}^{1}$ descend to the field of moduli."],"url":"http://arxiv.org/abs/2405.03612v1","category":"math.NT"}
{"created":"2024-05-06 16:30:27","title":"Network analysis for the steady-state thermodynamic uncertainty relation","abstract":"We perform network analysis of a system described by the master equation to estimate the lower bound of the steady-state current noise, starting from the level 2.5 large deviation function and using the graph theory approach. When the transition rates are uniform, and the system is driven to a non-equilibrium steady state by unidirectional transitions, we derive a noise lower bound, which accounts for fluctuations of sojourn times at all states and is expressed using mesh currents. This bound is applied to the uncertainty in the signal-to-noise ratio of the fluctuating computation time of a schematic Brownian computation plus reset process described by a graph containing one cycle. Unlike the mixed and pseudo-entropy bounds that increase logarithmically with the length of the intended computation path, this bound depends on the number of extraneous predecessors and thus captures the logical irreversibility.","sentences":["We perform network analysis of a system described by the master equation to estimate the lower bound of the steady-state current noise, starting from the level 2.5 large deviation function and using the graph theory approach.","When the transition rates are uniform, and the system is driven to a non-equilibrium steady state by unidirectional transitions, we derive a noise lower bound, which accounts for fluctuations of sojourn times at all states and is expressed using mesh currents.","This bound is applied to the uncertainty in the signal-to-noise ratio of the fluctuating computation time of a schematic Brownian computation plus reset process described by a graph containing one cycle.","Unlike the mixed and pseudo-entropy bounds that increase logarithmically with the length of the intended computation path, this bound depends on the number of extraneous predecessors and thus captures the logical irreversibility."],"url":"http://arxiv.org/abs/2405.03611v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-06 16:17:48","title":"Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations","abstract":"We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.","sentences":["We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology.","Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges.","First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned.","To overcome that, we propose an estimator based on the Strang splitting scheme.","Second, since the velocity is rarely observed we adjust the estimator for partial observations.","We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood.","These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality.","Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator.","With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice.","However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators.","We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras."],"url":"http://arxiv.org/abs/2405.03606v1","category":"stat.ME"}
{"created":"2024-05-06 16:10:17","title":"Reverse Intersystem Crossing Dynamics in Vibronically Modulated Inverted Singlet-Triplet Gap System: A Wigner Phase Space Study","abstract":"We inspect the origin of the inverted singlet-triplet gap (INVEST) and slow change in the reverse intersystem crossing (rISC) rate with temperature, as recently observed. A Wigner phase space study reveals, that though INVEST is found at equilibrium geometry, variation in the exchange interaction and the doubles-excitation for other geometries in the harmonic region leads to non- INVEST behavior. This highlights the importance of nuclear degrees of freedom for the INVEST phenomenon and in this case, geometric puckering of the studied molecule determines INVEST and the associated rISC dynamics.","sentences":["We inspect the origin of the inverted singlet-triplet gap (INVEST) and slow change in the reverse intersystem crossing (rISC) rate with temperature, as recently observed.","A Wigner phase space study reveals, that though INVEST is found at equilibrium geometry, variation in the exchange interaction and the doubles-excitation for other geometries in the harmonic region leads to non- INVEST behavior.","This highlights the importance of nuclear degrees of freedom for the INVEST phenomenon and in this case, geometric puckering of the studied molecule determines INVEST and the associated rISC dynamics."],"url":"http://arxiv.org/abs/2405.03598v1","category":"physics.chem-ph"}
{"created":"2024-05-06 16:01:28","title":"Deep Clustering with Self-Supervision using Pairwise Similarities","abstract":"Deep clustering incorporates embedding into clustering to find a lower-dimensional space appropriate for clustering. In this paper, we propose a novel deep clustering framework with self-supervision using pairwise similarities (DCSS). The proposed method consists of two successive phases. In the first phase, we propose to form hypersphere-like groups of similar data points, i.e. one hypersphere per cluster, employing an autoencoder that is trained using cluster-specific losses. The hyper-spheres are formed in the autoencoder's latent space. In the second phase, we propose to employ pairwise similarities to create a $K$-dimensional space that is capable of accommodating more complex cluster distributions, hence providing more accurate clustering performance. $K$ is the number of clusters. The autoencoder's latent space obtained in the first phase is used as the input of the second phase. The effectiveness of both phases is demonstrated on seven benchmark datasets by conducting a rigorous set of experiments.","sentences":["Deep clustering incorporates embedding into clustering to find a lower-dimensional space appropriate for clustering.","In this paper, we propose a novel deep clustering framework with self-supervision using pairwise similarities (DCSS).","The proposed method consists of two successive phases.","In the first phase, we propose to form hypersphere-like groups of similar data points, i.e. one hypersphere per cluster, employing an autoencoder that is trained using cluster-specific losses.","The hyper-spheres are formed in the autoencoder's latent space.","In the second phase, we propose to employ pairwise similarities to create a $K$-dimensional space that is capable of accommodating more complex cluster distributions, hence providing more accurate clustering performance.","$K$ is the number of clusters.","The autoencoder's latent space obtained in the first phase is used as the input of the second phase.","The effectiveness of both phases is demonstrated on seven benchmark datasets by conducting a rigorous set of experiments."],"url":"http://arxiv.org/abs/2405.03590v1","category":"cs.LG"}
{"created":"2024-05-06 16:01:13","title":"Communities for the Lagrangian Dynamics of the Turbulent Velocity Gradient Tensor: A Network Participation Approach","abstract":"Complex network analysis methods have been widely applied to nonlinear systems, but applications within fluid mechanics are relatively few. In this paper, we use a network for the Lagrangian dynamics of the velocity gradient tensor (VGT), where each node is a flow state, and the probability of transitioning between states follows from a direct numerical simulation of statistically steady and isotropic turbulence. The network representation of the VGT dynamics is much more compact than the continuous, joint distribution of a set of invariants for the tensor. We focus on choosing optimal variables to discretize and classify the VGT states. To this end, we test several classifications based on topology and various properties of the background flow coherent structures. We do this using the notion of \"community\" or \"module\", namely clusters of nodes that are optimally distinct while also containing diverse nodal functions. The best classification based upon VGT invariants often adopted in the literature combines the sign of the principal invariants, $Q$ and $R$, and the sign of the discriminant function, $\\Delta$, separating regions where the VGT eigenvalues are real and complex. We further improve this classification by including the relative magnitude of the non-normal contribution to the dynamics of the enstrophy and straining stemming from a Schur decomposition of the VGT. The traditional focus on the second VGT principal invariant, $Q$, implies consideration of the difference between the enstrophy and strain-rate magnitude without the non-normal parts. The fact that including the non-normality leads to a better VGT classification highlights the importance of unclosed and complex terms contributing to the VGT dynamics, namely the pressure Hessian and viscous terms, to which the VGT non-normality is intrinsically related.","sentences":["Complex network analysis methods have been widely applied to nonlinear systems, but applications within fluid mechanics are relatively few.","In this paper, we use a network for the Lagrangian dynamics of the velocity gradient tensor (VGT), where each node is a flow state, and the probability of transitioning between states follows from a direct numerical simulation of statistically steady and isotropic turbulence.","The network representation of the VGT dynamics is much more compact than the continuous, joint distribution of a set of invariants for the tensor.","We focus on choosing optimal variables to discretize and classify the VGT states.","To this end, we test several classifications based on topology and various properties of the background flow coherent structures.","We do this using the notion of \"community\" or \"module\", namely clusters of nodes that are optimally distinct while also containing diverse nodal functions.","The best classification based upon VGT invariants often adopted in the literature combines the sign of the principal invariants, $Q$ and $R$, and the sign of the discriminant function, $\\Delta$, separating regions where the VGT eigenvalues are real and complex.","We further improve this classification by including the relative magnitude of the non-normal contribution to the dynamics of the enstrophy and straining stemming from a Schur decomposition of the VGT.","The traditional focus on the second VGT principal invariant, $Q$, implies consideration of the difference between the enstrophy and strain-rate magnitude without the non-normal parts.","The fact that including the non-normality leads to a better VGT classification highlights the importance of unclosed and complex terms contributing to the VGT dynamics, namely the pressure Hessian and viscous terms, to which the VGT non-normality is intrinsically related."],"url":"http://arxiv.org/abs/2405.03589v1","category":"physics.flu-dyn"}
{"created":"2024-05-06 15:53:55","title":"Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting","abstract":"Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.","sentences":["Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy.","They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series.","In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state.","These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver.","As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD).","Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model.","The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values.","Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead.","Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model."],"url":"http://arxiv.org/abs/2405.03582v1","category":"cs.LG"}
{"created":"2024-05-06 15:51:23","title":"A Valuation Framework for Customers Impacted by Extreme Temperature-Related Outages","abstract":"Extreme temperature outages can lead to not just economic losses but also various non-energy impacts (NEI) due to significant degradation of indoor operating conditions caused by service disruptions. However, existing resilience assessment approaches lack specificity for extreme temperature conditions. They often overlook temperature-related mortality and neglect the customer characteristics and grid response in the calculation, despite the significant influence of these factors on NEI-related economic losses. This paper aims to address these gaps by introducing a comprehensive framework to estimate the impact of resilience enhancement not only on the direct economic losses incurred by customers but also on potential NEI, including mortality and the value of statistical life during extreme temperature-related outages. The proposed resilience valuation integrates customer characteristics and grid response variables based on a scalable grid simulation environment. This study adopts a holistic approach to quantify customer-oriented economic impacts, utilizing probabilistic loss scenarios that incorporate health-related factors and damage/loss models as a function of exposure for valuation. The proposed methodology is demonstrated through comparative resilient outage planning, using grid response models emulating a Texas weather zone during the 2021 winter storm Uri. The case study results show that enhanced outage planning with hardened infrastructure can improve the system resilience and thereby reduce the relative risk of mortality by 16% and save the total costs related to non-energy impacts by 74%. These findings underscore the efficacy of the framework by assessing the financial implications of each case, providing valuable insights for decision-makers and stakeholders involved in extreme-weather related resilience planning for risk management and mitigation strategies.","sentences":["Extreme temperature outages can lead to not just economic losses but also various non-energy impacts (NEI) due to significant degradation of indoor operating conditions caused by service disruptions.","However, existing resilience assessment approaches lack specificity for extreme temperature conditions.","They often overlook temperature-related mortality and neglect the customer characteristics and grid response in the calculation, despite the significant influence of these factors on NEI-related economic losses.","This paper aims to address these gaps by introducing a comprehensive framework to estimate the impact of resilience enhancement not only on the direct economic losses incurred by customers but also on potential NEI, including mortality and the value of statistical life during extreme temperature-related outages.","The proposed resilience valuation integrates customer characteristics and grid response variables based on a scalable grid simulation environment.","This study adopts a holistic approach to quantify customer-oriented economic impacts, utilizing probabilistic loss scenarios that incorporate health-related factors and damage/loss models as a function of exposure for valuation.","The proposed methodology is demonstrated through comparative resilient outage planning, using grid response models emulating a Texas weather zone during the 2021 winter storm Uri.","The case study results show that enhanced outage planning with hardened infrastructure can improve the system resilience and thereby reduce the relative risk of mortality by 16% and save the total costs related to non-energy impacts by 74%.","These findings underscore the efficacy of the framework by assessing the financial implications of each case, providing valuable insights for decision-makers and stakeholders involved in extreme-weather related resilience planning for risk management and mitigation strategies."],"url":"http://arxiv.org/abs/2405.03575v1","category":"eess.SY"}
{"created":"2024-05-06 15:48:14","title":"RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous Driving Research","abstract":"This paper introduces RoboCar, an open-source research platform for autonomous driving developed at the University of Luxembourg. RoboCar provides a modular, cost-effective framework for the development of experimental Autonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform integrates a robust hardware and software architecture that aligns with the vehicle's existing systems, minimizing the need for extensive modifications. It supports various autonomous driving functions and has undergone real-world testing on public roads in Luxembourg City. This paper outlines the platform's architecture, integration challenges, and initial test results, offering insights into its application in advancing autonomous driving research. RoboCar is available to anyone at https://github.com/sntubix/robocar and is released under an open-source MIT license.","sentences":["This paper introduces RoboCar, an open-source research platform for autonomous driving developed at the University of Luxembourg.","RoboCar provides a modular, cost-effective framework for the development of experimental Autonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV.","The platform integrates a robust hardware and software architecture that aligns with the vehicle's existing systems, minimizing the need for extensive modifications.","It supports various autonomous driving functions and has undergone real-world testing on public roads in Luxembourg City.","This paper outlines the platform's architecture, integration challenges, and initial test results, offering insights into its application in advancing autonomous driving research.","RoboCar is available to anyone at https://github.com/sntubix/robocar and is released under an open-source MIT license."],"url":"http://arxiv.org/abs/2405.03572v1","category":"cs.RO"}
{"created":"2024-05-06 15:47:00","title":"Josephson junction of minimally twisted bilayer graphene","abstract":"We theoretically investigate the transport properties of Josephson junctions composed of superconductor/minimally twisted bilayer graphene/superconductor structures. In the presence of an out-of-plane electric field, the low energy physics is best described by a network of chiral domain-wall states. Depending on system parameters, they lead to the emergence of zig-zag or pseudo-Landau level modes with distinct transport characteristics. Specifically, we find zig-zag modes feature linear dispersion of Andreev bound states, resulting in a $4\\pi$-periodic Josephson current. In contrast, pseudo-Landau level modes exhibit flat Andreev bound states and, consequently, a vanishing bulk Josephson current. Interestingly, edge states can give rise to $4\\pi$-periodic Josephson response in the pseudo-Landau level regime. We also discuss experimental signatures of such responses.","sentences":["We theoretically investigate the transport properties of Josephson junctions composed of superconductor/minimally twisted bilayer graphene/superconductor structures.","In the presence of an out-of-plane electric field, the low energy physics is best described by a network of chiral domain-wall states.","Depending on system parameters, they lead to the emergence of zig-zag or pseudo-Landau level modes with distinct transport characteristics.","Specifically, we find zig-zag modes feature linear dispersion of Andreev bound states, resulting in a $4\\pi$-periodic Josephson current.","In contrast, pseudo-Landau level modes exhibit flat Andreev bound states and, consequently, a vanishing bulk Josephson current.","Interestingly, edge states can give rise to $4\\pi$-periodic Josephson response in the pseudo-Landau level regime.","We also discuss experimental signatures of such responses."],"url":"http://arxiv.org/abs/2405.03571v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 15:42:30","title":"Majority consensus thresholds in competitive Lotka--Volterra populations","abstract":"One of the key challenges in synthetic biology is devising robust signaling primitives for engineered microbial consortia. In such systems, a fundamental signal amplification problem is the majority consensus problem: given a system with two input species with initial difference of $\\Delta$ in population sizes, what is the probability that the system reaches a state in which only the initial majority species is present?   In this work, we consider a discrete and stochastic version of competitive Lotka--Volterra dynamics, a standard model of microbial community dynamics. We identify new threshold properties for majority consensus under different types of interference competition:   - We show that under so-called self-destructive interference competition between the two input species, majority consensus can be reached with high probability if the initial difference satisfies $\\Delta \\in \\Omega(\\log^2 n)$, where $n$ is the initial population size. This gives an exponential improvement compared to the previously known bound of $\\Omega(\\sqrt{n \\log n})$ by Cho et al. [Distributed Computing, 2021] given for a special case of the competitive Lotka--Volterra model. In contrast, we show that an initial gap of $\\Delta \\in \\Omega(\\sqrt{\\log n})$ is necessary.   - On the other hand, we prove that under non-self-destructive interference competition, an initial gap of $\\Omega(\\sqrt{n})$ is necessary to succeed with high probability and that a $\\Omega(\\sqrt{n \\log n})$ gap is sufficient.   This shows a strong qualitative gap between the performance of self-destructive and non-self-destructive interference competition. Moreover, we show that if in addition the populations exhibit interference competition between the individuals of the same species, then majority consensus cannot always be solved with high probability, no matter what the difference in the initial population counts.","sentences":["One of the key challenges in synthetic biology is devising robust signaling primitives for engineered microbial consortia.","In such systems, a fundamental signal amplification problem is the majority consensus problem: given a system with two input species with initial difference of $\\Delta$ in population sizes, what is the probability that the system reaches a state in which only the initial majority species is present?   ","In this work, we consider a discrete and stochastic version of competitive Lotka--Volterra dynamics, a standard model of microbial community dynamics.","We identify new threshold properties for majority consensus under different types of interference competition:   - We show that under so-called self-destructive interference competition between the two input species, majority consensus can be reached with high probability if the initial difference satisfies $\\Delta \\in \\Omega(\\log^2 n)$, where $n$ is the initial population size.","This gives an exponential improvement compared to the previously known bound of $\\Omega(\\sqrt{n \\log n})$ by Cho et al.","[Distributed Computing, 2021] given for a special case of the competitive Lotka--Volterra model.","In contrast, we show that an initial gap of $\\Delta \\in \\Omega(\\sqrt{\\log n})$ is necessary.   -","On the other hand, we prove that under non-self-destructive interference competition, an initial gap of $\\Omega(\\sqrt{n})$ is necessary to succeed with high probability and that a $\\Omega(\\sqrt{n \\log n})$ gap is sufficient.   ","This shows a strong qualitative gap between the performance of self-destructive and non-self-destructive interference competition.","Moreover, we show that if in addition the populations exhibit interference competition between the individuals of the same species, then majority consensus cannot always be solved with high probability, no matter what the difference in the initial population counts."],"url":"http://arxiv.org/abs/2405.03568v1","category":"cs.DC"}
{"created":"2024-05-06 15:32:09","title":"Model- and Data-Based Control of Self-Balancing Robots: Practical Educational Approach with LabVIEW and Arduino","abstract":"A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system. This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach. Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller. On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model. In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated. All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors. The control law and the user interface are constructed using the LabVIEW-LINX toolkit. A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform.","sentences":["A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system.","This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach.","Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller.","On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model.","In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated.","All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors.","The control law and the user interface are constructed using the LabVIEW-LINX toolkit.","A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform."],"url":"http://arxiv.org/abs/2405.03561v1","category":"cs.RO"}
{"created":"2024-05-06 15:29:55","title":"Converse Lyapunov Results for Stability of Switched Systems with Average Dwell-Time","abstract":"This article provides a characterization of stability for switched nonlinear systems under average dwell-time constraints, in terms of necessary and sufficient conditions involving multiple Lyapunov functions. Earlier converse results focus on switched systems with dwell-time constraints only, and the resulting inequalities depend on the flow of individual subsystems. With the help of a counterexample, we show that a lower bound that guarantees stability for dwell-time switching signals may not necessarily imply stability for switching signals with same lower bound on the average dwell-time. Based on these two observations, we provide a converse result for the average dwell-time constrained systems in terms of inequalities which do not depend on the flow of individual subsystems and are easier to check. The particular case of linear switched systems is studied as a corollary to our main result.","sentences":["This article provides a characterization of stability for switched nonlinear systems under average dwell-time constraints, in terms of necessary and sufficient conditions involving multiple Lyapunov functions.","Earlier converse results focus on switched systems with dwell-time constraints only, and the resulting inequalities depend on the flow of individual subsystems.","With the help of a counterexample, we show that a lower bound that guarantees stability for dwell-time switching signals may not necessarily imply stability for switching signals with same lower bound on the average dwell-time.","Based on these two observations, we provide a converse result for the average dwell-time constrained systems in terms of inequalities which do not depend on the flow of individual subsystems and are easier to check.","The particular case of linear switched systems is studied as a corollary to our main result."],"url":"http://arxiv.org/abs/2405.03560v1","category":"math.OC"}
{"created":"2024-05-06 15:27:43","title":"Progress in Computational Understanding of Ferroelectric Mechanisms in HfO$_2$","abstract":"Since the first report of ferroelectricity in nanoscale HfO$_2$-based thin films in 2011, this silicon-compatible binary oxide has quickly garnered intense interest in academia and industry, and continues to do so. Despite its deceivingly simple chemical composition, the ferroelectric physics supported by HfO$_2$ is remarkably complex, arguably rivaling that of perovskite ferroelectrics. Computational investigations, especially those utilizing first-principles density functional theory (DFT), have significantly advanced our understanding of the nature of ferroelectricity in these thin films. In this review, we provide an in-depth discussion of the computational efforts to understand ferroelectric hafnia, comparing various metastable polar phases and examining the critical factors necessary for their stabilization. The intricate nature of HfO$_2$ is intimately related to the complex interplay among diverse structural polymorphs, dopants and their charge-compensating oxygen vacancies, and unconventional switching mechanisms of domains and domain walls, which can sometimes yield conflicting theoretical predictions and theoretical-experimental discrepancies. We also discuss opportunities enabled by machine-learning-assisted molecular dynamics and phase-field simulations to go beyond DFT modeling, probing the dynamical properties of ferroelectric HfO$_2$ and tackling pressing issues such as high coercive fields.","sentences":["Since the first report of ferroelectricity in nanoscale HfO$_2$-based thin films in 2011, this silicon-compatible binary oxide has quickly garnered intense interest in academia and industry, and continues to do so.","Despite its deceivingly simple chemical composition, the ferroelectric physics supported by HfO$_2$ is remarkably complex, arguably rivaling that of perovskite ferroelectrics.","Computational investigations, especially those utilizing first-principles density functional theory (DFT), have significantly advanced our understanding of the nature of ferroelectricity in these thin films.","In this review, we provide an in-depth discussion of the computational efforts to understand ferroelectric hafnia, comparing various metastable polar phases and examining the critical factors necessary for their stabilization.","The intricate nature of HfO$_2$ is intimately related to the complex interplay among diverse structural polymorphs, dopants and their charge-compensating oxygen vacancies, and unconventional switching mechanisms of domains and domain walls, which can sometimes yield conflicting theoretical predictions and theoretical-experimental discrepancies.","We also discuss opportunities enabled by machine-learning-assisted molecular dynamics and phase-field simulations to go beyond DFT modeling, probing the dynamical properties of ferroelectric HfO$_2$ and tackling pressing issues such as high coercive fields."],"url":"http://arxiv.org/abs/2405.03558v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 15:14:15","title":"A cavity-microscope for micrometer-scale control of atom-photon interactions","abstract":"Cavity quantum electrodynamics offers the possibility to observe and control the motion of few or individual atoms, enabling the realization of various quantum technological tasks such as quantum-enhanced metrology or quantum simulation of strongly-correlated matter. A core limitation of these experiments lies in the mode structure of the cavity field, which is hard-coded in the shape and geometry of the mirrors. As a result, most applications of cavity QED trade spatial resolution for enhanced sensitivity. Here, we propose and demonstrate a cavity-microscope device capable of controlling in space and time the coupling between atoms and light in a single-mode high-finesse cavity, reaching a spatial resolution an order-of-magnitude lower than the cavity mode waist. This is achieved through local Floquet engineering of the atomic level structure, imprinting a corresponding atom-field coupling. We illustrate this capability by engineering micrometer-scale coupling, using cavity-assisted atomic measurements and optimization. Our system forms an optical device with a single optical axis and has the same footprint and complexity as a standard Fabry-Perot cavity or confocal lens pair, and can be used for any atomic species. This technique opens a wide range of perspectives from ultra-fast, cavity-enhanced mid-circuit readout to the quantum simulation of fully connected models of quantum matter such as the Sachdev-Ye-Kitaev model.","sentences":["Cavity quantum electrodynamics offers the possibility to observe and control the motion of few or individual atoms, enabling the realization of various quantum technological tasks such as quantum-enhanced metrology or quantum simulation of strongly-correlated matter.","A core limitation of these experiments lies in the mode structure of the cavity field, which is hard-coded in the shape and geometry of the mirrors.","As a result, most applications of cavity QED trade spatial resolution for enhanced sensitivity.","Here, we propose and demonstrate a cavity-microscope device capable of controlling in space and time the coupling between atoms and light in a single-mode high-finesse cavity, reaching a spatial resolution an order-of-magnitude lower than the cavity mode waist.","This is achieved through local Floquet engineering of the atomic level structure, imprinting a corresponding atom-field coupling.","We illustrate this capability by engineering micrometer-scale coupling, using cavity-assisted atomic measurements and optimization.","Our system forms an optical device with a single optical axis and has the same footprint and complexity as a standard Fabry-Perot cavity or confocal lens pair, and can be used for any atomic species.","This technique opens a wide range of perspectives from ultra-fast, cavity-enhanced mid-circuit readout to the quantum simulation of fully connected models of quantum matter such as the Sachdev-Ye-Kitaev model."],"url":"http://arxiv.org/abs/2405.03550v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 14:44:06","title":"ReinWiFi: A Reinforcement-Learning-Based Framework for the Application-Layer QoS Optimization of WiFi Networks","abstract":"In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference. Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized. Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown. Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action. It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism.","sentences":["In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference.","Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized.","Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown.","Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action.","It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism."],"url":"http://arxiv.org/abs/2405.03526v1","category":"cs.NI"}
{"created":"2024-05-06 14:40:44","title":"Basilisk: Achieving Competitive Performance with Open EDA Tools on an Open-Source Linux-Capable RISC-V SoC","abstract":"We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC). We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization. The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design. Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications.","sentences":["We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC).","We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization.","The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design.","Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications."],"url":"http://arxiv.org/abs/2405.03523v1","category":"cs.AR"}
{"created":"2024-05-06 14:39:13","title":"Almost periodicity and boundary values of Dirichlet series","abstract":"We employ almost periodicity to establish analogues of the Hardy--Stein identity and the Littlewood--Paley formula for Hardy spaces of Dirichlet series. A construction of Saksman and Seip shows that the limits in this Littlewood--Paley formula cannot be interchanged. We apply this construction to show that the limits in the definition of the mean counting function for Dirichlet series cannot be interchanged. These are essentially statements about the two different kinds of boundary values that we associate with Dirichlet series that converge to a bounded analytic function in a half-plane. The treatment of the mean counting function also involves an investigation of the zero sets and Blaschke products of such Dirichlet series.","sentences":["We employ almost periodicity to establish analogues of the Hardy--Stein identity and the Littlewood--Paley formula for Hardy spaces of Dirichlet series.","A construction of Saksman and Seip shows that the limits in this Littlewood--Paley formula cannot be interchanged.","We apply this construction to show that the limits in the definition of the mean counting function for Dirichlet series cannot be interchanged.","These are essentially statements about the two different kinds of boundary values that we associate with Dirichlet series that converge to a bounded analytic function in a half-plane.","The treatment of the mean counting function also involves an investigation of the zero sets and Blaschke products of such Dirichlet series."],"url":"http://arxiv.org/abs/2405.03522v1","category":"math.CA"}
{"created":"2024-05-06 14:26:08","title":"Development of Ultra-Portable 3D Mapping Systems for Emergency Services","abstract":"Miniaturization of cameras and LiDAR sensors has enabled the development of wearable 3D mapping systems for emergency responders. These systems have the potential to revolutionize response capabilities by providing real-time, high-fidelity maps of dynamic and hazardous environments. We present our recent efforts towards the development of such ultra-portable 3D mapping systems. We review four different sensor configurations, either helmet-mounted or body-worn, with two different mapping algorithms that were implemented and evaluated during field trials. The paper discusses the experimental results with the aim to stimulate further discussion within the portable 3D mapping research community.","sentences":["Miniaturization of cameras and LiDAR sensors has enabled the development of wearable 3D mapping systems for emergency responders.","These systems have the potential to revolutionize response capabilities by providing real-time, high-fidelity maps of dynamic and hazardous environments.","We present our recent efforts towards the development of such ultra-portable 3D mapping systems.","We review four different sensor configurations, either helmet-mounted or body-worn, with two different mapping algorithms that were implemented and evaluated during field trials.","The paper discusses the experimental results with the aim to stimulate further discussion within the portable 3D mapping research community."],"url":"http://arxiv.org/abs/2405.03514v1","category":"cs.RO"}
{"created":"2024-05-06 14:20:49","title":"Emergence of condensation patterns in kinetic equations for opinion dynamics","abstract":"In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity. To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density. In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting. From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution. Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size. In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects.","sentences":["In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity.","To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density.","In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting.","From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution.","Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size.","In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects."],"url":"http://arxiv.org/abs/2405.03507v1","category":"nlin.AO"}
{"created":"2024-05-06 14:20:42","title":"Spin-Wave Voices: Sonification of Nanoscale Spin Waves as an Engagement and Research Tool","abstract":"Magnonics is an emerging research field that addresses the use of spin waves (magnons), purely magnetic waves, for information transport and processing. Spin waves are a potential replacement for electric current in modern computational devices that would make them more compact and energy efficient. The field is yet little known, even among physicists. Additionally, with the development of new measuring techniques and computational physics, the obtained magnetic data becomes more complex, in some cases including 3D vector fields and time-resolution. This work presents an approach to the audio-visual representation of the spin waves and discusses its use as a tool for science communication exhibits and possible data analysis tool. The work also details an instance of such an exhibit presented at the annual international digital art exhibition Ars Electronica Festival in 2022.","sentences":["Magnonics is an emerging research field that addresses the use of spin waves (magnons), purely magnetic waves, for information transport and processing.","Spin waves are a potential replacement for electric current in modern computational devices that would make them more compact and energy efficient.","The field is yet little known, even among physicists.","Additionally, with the development of new measuring techniques and computational physics, the obtained magnetic data becomes more complex, in some cases including 3D vector fields and time-resolution.","This work presents an approach to the audio-visual representation of the spin waves and discusses its use as a tool for science communication exhibits and possible data analysis tool.","The work also details an instance of such an exhibit presented at the annual international digital art exhibition Ars Electronica Festival in 2022."],"url":"http://arxiv.org/abs/2405.03506v1","category":"cs.HC"}
{"created":"2024-05-06 14:14:30","title":"Human-Variability-Respecting Optimal Control for Physical Human-Machine Interaction","abstract":"Physical Human-Machine Interaction plays a pivotal role in facilitating collaboration across various domains. When designing appropriate model-based controllers to assist a human in the interaction, the accuracy of the human model is crucial for the resulting overall behavior of the coupled system. When looking at state-of-the-art control approaches, most methods rely on a deterministic model or no model at all of the human behavior. This poses a gap to the current neuroscientific standard regarding human movement modeling, which uses stochastic optimal control models that include signal-dependent noise processes and therefore describe the human behavior much more accurate than the deterministic counterparts. To close this gap by including these stochastic human models in the control design, we introduce a novel design methodology resulting in a Human-Variability-Respecting Optimal Control that explicitly incorporates the human noise processes and their influence on the mean and variability behavior of a physically coupled human-machine system. Our approach results in an improved overall system performance, i.e. higher accuracy and lower variability in target point reaching, while allowing to shape the joint variability, for example to preserve human natural variability patterns.","sentences":["Physical Human-Machine Interaction plays a pivotal role in facilitating collaboration across various domains.","When designing appropriate model-based controllers to assist a human in the interaction, the accuracy of the human model is crucial for the resulting overall behavior of the coupled system.","When looking at state-of-the-art control approaches, most methods rely on a deterministic model or no model at all of the human behavior.","This poses a gap to the current neuroscientific standard regarding human movement modeling, which uses stochastic optimal control models that include signal-dependent noise processes and therefore describe the human behavior much more accurate than the deterministic counterparts.","To close this gap by including these stochastic human models in the control design, we introduce a novel design methodology resulting in a Human-Variability-Respecting Optimal Control that explicitly incorporates the human noise processes and their influence on the mean and variability behavior of a physically coupled human-machine system.","Our approach results in an improved overall system performance, i.e. higher accuracy and lower variability in target point reaching, while allowing to shape the joint variability, for example to preserve human natural variability patterns."],"url":"http://arxiv.org/abs/2405.03502v1","category":"eess.SY"}
{"created":"2024-05-06 14:07:05","title":"Long ranged stress correlations in the hard sphere liquid","abstract":"The smooth emergence of shear elasticity is an hallmark of the liquid to glass transition. In a liquid, viscous stresses arise from local structural rearrangements. In the solid, Eshelby has shown that stresses around an inclusion decay as a power law $r^{-D}$, where $D$ is the dimension of the system. We study glass-forming hard sphere fluids by simulation and observe the emergence of the unscreened power-law Eshelby pattern in the stress correlations of the isotropic liquid state. By a detailed tensorial analysis, we show that the fluctuating force field, viz.~the divergence of the stress field, relaxes to zero with time in all states, while the shear stress correlations develop spatial power-law structures inside regions that grow with longitudinal and transverse sound speeds; we observe the predicted exponents $r^{-D}$ and $r^{-D-2}$. In Brownian systems, shear stresses relax diffusively within these regions, with the diffusion coefficient determined by the shear modulus and the friction coefficient.","sentences":["The smooth emergence of shear elasticity is an hallmark of the liquid to glass transition.","In a liquid, viscous stresses arise from local structural rearrangements.","In the solid, Eshelby has shown that stresses around an inclusion decay as a power law $r^{-D}$, where $D$ is the dimension of the system.","We study glass-forming hard sphere fluids by simulation and observe the emergence of the unscreened power-law Eshelby pattern in the stress correlations of the isotropic liquid state.","By a detailed tensorial analysis, we show that the fluctuating force field, viz.~the divergence of the stress field, relaxes to zero with time in all states, while the shear stress correlations develop spatial power-law structures inside regions that grow with longitudinal and transverse sound speeds; we observe the predicted exponents $r^{-D}$ and $r^{-D-2}$. In Brownian systems, shear stresses relax diffusively within these regions, with the diffusion coefficient determined by the shear modulus and the friction coefficient."],"url":"http://arxiv.org/abs/2405.03497v1","category":"cond-mat.soft"}
{"created":"2024-05-06 14:07:02","title":"Price-Aware Automated Market Makers: Models Beyond Brownian Prices and Static Liquidity","abstract":"In this paper, we introduce a suite of models for price-aware automated market making platforms willing to optimize their quotes. These models incorporate advanced price dynamics, including stochastic volatility, jumps, and microstructural price models based on Hawkes processes. Additionally, we address the variability in demand from liquidity takers through models that employ either Hawkes or Markov-modulated Poisson processes. Each model is analyzed with particular emphasis placed on the complexity of the numerical methods required to compute optimal quotes.","sentences":["In this paper, we introduce a suite of models for price-aware automated market making platforms willing to optimize their quotes.","These models incorporate advanced price dynamics, including stochastic volatility, jumps, and microstructural price models based on Hawkes processes.","Additionally, we address the variability in demand from liquidity takers through models that employ either Hawkes or Markov-modulated Poisson processes.","Each model is analyzed with particular emphasis placed on the complexity of the numerical methods required to compute optimal quotes."],"url":"http://arxiv.org/abs/2405.03496v1","category":"q-fin.TR"}
{"created":"2024-05-06 14:04:45","title":"Quantum Ising Spin-Glass Otto Engine","abstract":"We investigate a quantum Otto engine with a quantum Ising spin glass as the working medium to explore the scaling behavior of work output and thermodynamic performance concerning system size, particularly near the critical point. Specifically, we explore the two operating modes of the Otto engine, namely the heat engine and refrigerator modes. We observe a double-peaked structure in the heat engine regime, leading to superlinear scaling in both work output and thermodynamic performance near the critical point. Additionally, in the refrigerator regime, superlinear scaling in refrigerator efficiency can be achieved at high and low temperatures, significantly outperforming models with uniform Ising interactions. These findings suggest that disorder and frustration in quantum Ising spin-glass systems could significantly impact thermodynamic performance in quantum heat engines and refrigerators, potentially opening up new avenues for improvement.","sentences":["We investigate a quantum Otto engine with a quantum Ising spin glass as the working medium to explore the scaling behavior of work output and thermodynamic performance concerning system size, particularly near the critical point.","Specifically, we explore the two operating modes of the Otto engine, namely the heat engine and refrigerator modes.","We observe a double-peaked structure in the heat engine regime, leading to superlinear scaling in both work output and thermodynamic performance near the critical point.","Additionally, in the refrigerator regime, superlinear scaling in refrigerator efficiency can be achieved at high and low temperatures, significantly outperforming models with uniform Ising interactions.","These findings suggest that disorder and frustration in quantum Ising spin-glass systems could significantly impact thermodynamic performance in quantum heat engines and refrigerators, potentially opening up new avenues for improvement."],"url":"http://arxiv.org/abs/2405.03495v1","category":"quant-ph"}
{"created":"2024-05-06 13:54:56","title":"Managing Renewable Energy Resources Using Equity-Market Risk Tools - the Efficient Frontiers","abstract":"The energy market, and specifically the renewable sector carries volatility and risks, similar to the financial market. Here, we leverage on a well-established, return-risk approach, commonly used by equity portfolio-managers and apply it to energy resources. We visualize the relationship between the resources' costs and their risks in terms of efficient frontiers. We apply this analysis to publically available data for various US regions: Central, Eastern and Western coasts. Since risk management is contingent on costs, this approach sheds useful light in assessing dynamic pricing in modern electrical grids. By integrating geographical and temporal dimensions into our research, we aim at providing more nuanced and context-specific recommendations for energy resource allocation. This approach may help decision-makers in the renewable energy sector to make informed choices that account for regional variations, climatic conditions, and long-term performance trends.","sentences":["The energy market, and specifically the renewable sector carries volatility and risks, similar to the financial market.","Here, we leverage on a well-established, return-risk approach, commonly used by equity portfolio-managers and apply it to energy resources.","We visualize the relationship between the resources' costs and their risks in terms of efficient frontiers.","We apply this analysis to publically available data for various US regions: Central, Eastern and Western coasts.","Since risk management is contingent on costs, this approach sheds useful light in assessing dynamic pricing in modern electrical grids.","By integrating geographical and temporal dimensions into our research, we aim at providing more nuanced and context-specific recommendations for energy resource allocation.","This approach may help decision-makers in the renewable energy sector to make informed choices that account for regional variations, climatic conditions, and long-term performance trends."],"url":"http://arxiv.org/abs/2405.03482v1","category":"eess.SY"}
{"created":"2024-05-06 13:53:09","title":"AnchorGT: Efficient and Flexible Attention Architecture for Scalable Graph Transformers","abstract":"Graph Transformers (GTs) have significantly advanced the field of graph representation learning by overcoming the limitations of message-passing graph neural networks (GNNs) and demonstrating promising performance and expressive power. However, the quadratic complexity of self-attention mechanism in GTs has limited their scalability, and previous approaches to address this issue often suffer from expressiveness degradation or lack of versatility. To address this issue, we propose AnchorGT, a novel attention architecture for GTs with global receptive field and almost linear complexity, which serves as a flexible building block to improve the scalability of a wide range of GT models. Inspired by anchor-based GNNs, we employ structurally important $k$-dominating node set as anchors and design an attention mechanism that focuses on the relationship between individual nodes and anchors, while retaining the global receptive field for all nodes. With its intuitive design, AnchorGT can easily replace the attention module in various GT models with different network architectures and structural encodings, resulting in reduced computational overhead without sacrificing performance. In addition, we theoretically prove that AnchorGT attention can be strictly more expressive than Weisfeiler-Lehman test, showing its superiority in representing graph structures. Our experiments on three state-of-the-art GT models demonstrate that their AnchorGT variants can achieve better results while being faster and significantly more memory efficient.","sentences":["Graph Transformers (GTs) have significantly advanced the field of graph representation learning by overcoming the limitations of message-passing graph neural networks (GNNs) and demonstrating promising performance and expressive power.","However, the quadratic complexity of self-attention mechanism in GTs has limited their scalability, and previous approaches to address this issue often suffer from expressiveness degradation or lack of versatility.","To address this issue, we propose AnchorGT, a novel attention architecture for GTs with global receptive field and almost linear complexity, which serves as a flexible building block to improve the scalability of a wide range of GT models.","Inspired by anchor-based GNNs, we employ structurally important $k$-dominating node set as anchors and design an attention mechanism that focuses on the relationship between individual nodes and anchors, while retaining the global receptive field for all nodes.","With its intuitive design, AnchorGT can easily replace the attention module in various GT models with different network architectures and structural encodings, resulting in reduced computational overhead without sacrificing performance.","In addition, we theoretically prove that AnchorGT attention can be strictly more expressive than Weisfeiler-Lehman test, showing its superiority in representing graph structures.","Our experiments on three state-of-the-art GT models demonstrate that their AnchorGT variants can achieve better results while being faster and significantly more memory efficient."],"url":"http://arxiv.org/abs/2405.03481v1","category":"cs.LG"}
{"created":"2024-05-06 13:53:02","title":"A central limit theorem associated with a sequence of positive line bundles","abstract":"We prove a central limit theorem for random currents of integration along the zero divisors of standard Gaussian holomorphic sections in a sequence of holomorphic line bundles with Hermitian metrics of class $\\mathscr{C}^{3}$ over a compact K\\\"{a}hler manifold. In the course of our analysis, we derive first-order asymptotics and upper decay estimates for near and off-diagonal Bergman kernels, respectively. These results are essential for determining the statistical properties of the zeros of random holomorphic sections.","sentences":["We prove a central limit theorem for random currents of integration along the zero divisors of standard Gaussian holomorphic sections in a sequence of holomorphic line bundles with Hermitian metrics of class $\\mathscr{C}^{3}$ over a compact K\\\"{a}hler manifold.","In the course of our analysis, we derive first-order asymptotics and upper decay estimates for near and off-diagonal Bergman kernels, respectively.","These results are essential for determining the statistical properties of the zeros of random holomorphic sections."],"url":"http://arxiv.org/abs/2405.03479v1","category":"math.CV"}
{"created":"2024-05-06 13:45:44","title":"Motion Planning under Uncertainty: Integrating Learning-Based Multi-Modal Predictors into Branch Model Predictive Control","abstract":"In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior. To address this, recent advancements in learningbased motion predictors output multi-modal predictions. We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions. The framework includes an online scenario-selection process guided by topology and collision risk criteria. This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable. Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved. Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method.","sentences":["In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior.","To address this, recent advancements in learningbased motion predictors output multi-modal predictions.","We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions.","The framework includes an online scenario-selection process guided by topology and collision risk criteria.","This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable.","Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved.","Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method."],"url":"http://arxiv.org/abs/2405.03470v1","category":"cs.RO"}
{"created":"2024-05-06 13:40:17","title":"A family of air-stable chalcogenide solid electrolytes in Li$_2$BMQ$_4$ (B = Ca, Sr and Ba; M = Si, Ge and Sn; Q = O, S and Se) systems","abstract":"Combining high-throughput first-principles calculations and experimental measurements, we have identified a novel family of fast lithium-ion chalcogenide conductors in Li$_2$BMQ$_4$ (2114, B = Ca, Sr and Ba; M = Si, Ge and Sn; Q = O, S and Se) systems. Our calculations demonstrate that most of the thermodynamically and kinetically stable sulfides and selenides in this new system exhibit ultralow Li$^+$ ion migration activation energy (0.16 eV ~ 0.56 eV) and considerable bandgaps varying between ~ 2 eV and 3.5 eV. We have successfully synthesized Li$_2$BaSnS$_4$ and Li$_2$SrSiS$_4$, and they exhibit excellent moisture stability through H$_2$S gas measurements. Electrochemical impedance measurements indicate 2114 systems show the typical features of solid ionic conductors, with a room-temperature Li$^+$ conductivity close to 5$\\times$10$^{-4}$ mS/cm aligning with our molecular dynamics simulations. Furthermore, we have theoretically investigated the substitution of Cl$^-$ at S$^{2-}$ site. The doped compounds display significantly higher conductivity, with an increase of about three orders of magnitude (up to a maximum of 0.72 mS/cm) compared to the undoped compounds. These findings offer valuable insights for the further exploration of potential chalcogenide solid electrolyte materials with robust air stability and enhanced ionic conductivity for practical applications in lithium-ion batteries.","sentences":["Combining high-throughput first-principles calculations and experimental measurements, we have identified a novel family of fast lithium-ion chalcogenide conductors in Li$_2$BMQ$_4$ (2114, B = Ca, Sr and Ba; M = Si, Ge and Sn; Q = O, S and Se) systems.","Our calculations demonstrate that most of the thermodynamically and kinetically stable sulfides and selenides in this new system exhibit ultralow Li$^+$ ion migration activation energy (0.16 eV ~ 0.56 eV) and considerable bandgaps varying between ~ 2 eV and 3.5 eV. We have successfully synthesized Li$_2$BaSnS$_4$ and Li$_2$SrSiS$_4$, and they exhibit excellent moisture stability through H$_2$S gas measurements.","Electrochemical impedance measurements indicate 2114 systems show the typical features of solid ionic conductors, with a room-temperature Li$^+$ conductivity close to 5$\\times$10$^{-4}$ mS/cm aligning with our molecular dynamics simulations.","Furthermore, we have theoretically investigated the substitution of Cl$^-$ at S$^{2-}$ site.","The doped compounds display significantly higher conductivity, with an increase of about three orders of magnitude (up to a maximum of 0.72 mS/cm) compared to the undoped compounds.","These findings offer valuable insights for the further exploration of potential chalcogenide solid electrolyte materials with robust air stability and enhanced ionic conductivity for practical applications in lithium-ion batteries."],"url":"http://arxiv.org/abs/2405.03466v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 13:22:56","title":"The spectral genus of an isolated hypersurface singularity and a conjecture relating to the Milnor number","abstract":"In this paper, we introduce the notion of spectral genus $\\widetilde{p}_{g}$ of a deformation of a germ of an isolated hypersurface singularity $(\\mathbb{C}^{n+1}, 0) \\to (\\mathbb{C}, 0)$, defined as a sum of small exponents of monodromy eigenvalues. The number of these is equal to the geometric genus $p_{g}$, and hence $\\widetilde{p}_g$ can be considered as a secondary invariant to it. We then explore a secondary version of the Durfee conjecture on $p_{g}$, regarding the relationship between $\\widetilde{p}_{g}$ the Milnor number $\\mu$, to the effect that $$\\frac{\\mu}{(n+2)!} > \\widetilde{p}_g.$$ We provide evidence by confirming the conjecture in several cases, including homogeneous singularities and singularities with large Newton polyhedra, and quasi-homogeneous or irreducible curve singularities.   Our conjecture is shown to relate closely to the asymptotic behavior of the holomorphic analytic torsion of the sheaf of holomorphic functions on a degeneration of projective varieties, potentially indicating deeper geometric and analytic connections.","sentences":["In this paper, we introduce the notion of spectral genus $\\widetilde{p}_{g}$ of a deformation of a germ of an isolated hypersurface singularity $(\\mathbb{C}^{n+1}, 0) \\to (\\mathbb{C}, 0)$, defined as a sum of small exponents of monodromy eigenvalues.","The number of these is equal to the geometric genus $p_{g}$, and hence $\\widetilde{p}_g$ can be considered as a secondary invariant to it.","We then explore a secondary version of the Durfee conjecture on $p_{g}$, regarding the relationship between $\\widetilde{p}_{g}$ the Milnor number $\\mu$, to the effect that $$\\frac{\\mu}{(n+2)!}","> \\widetilde{p}_g.$$ We provide evidence by confirming the conjecture in several cases, including homogeneous singularities and singularities with large Newton polyhedra, and quasi-homogeneous or irreducible curve singularities.   ","Our conjecture is shown to relate closely to the asymptotic behavior of the holomorphic analytic torsion of the sheaf of holomorphic functions on a degeneration of projective varieties, potentially indicating deeper geometric and analytic connections."],"url":"http://arxiv.org/abs/2405.03450v1","category":"math.AG"}
{"created":"2024-05-06 13:08:18","title":"Anomalous Nernst effect in the noncollinear antiferromagnet Mn$_5$Si$_3$","abstract":"Investigating the off-diagonal components of the conductivity and thermoelectric tensor of materials hosting complex antiferromagnetic structures has become a viable method to reveal the effects of topology and chirality on the electronic transport in these systems. In this respect, Mn$_5$Si$_3$ is an interesting metallic compound that exhibits several antiferromagnetic phases below 100 K with different collinear and noncollinear arrangements of Mn magnetic moments. Previous investigations have shown that the transitions between the various phases give rise to large changes of the anomalous Hall effect. Here, we report measurements of the anomalous Nernst effect of Mn$_5$Si$_3$ single crystals. Below 25 K we observe a sign change of the zero-field Nernst signal with a concomitant decrease of the Hall signal and a gradual reduction of the remanent magnetization which we attribute to a subtle rearrangement of the magnetic moment configuration at low temperatures.","sentences":["Investigating the off-diagonal components of the conductivity and thermoelectric tensor of materials hosting complex antiferromagnetic structures has become a viable method to reveal the effects of topology and chirality on the electronic transport in these systems.","In this respect, Mn$_5$Si$_3$ is an interesting metallic compound that exhibits several antiferromagnetic phases below 100 K with different collinear and noncollinear arrangements of Mn magnetic moments.","Previous investigations have shown that the transitions between the various phases give rise to large changes of the anomalous Hall effect.","Here, we report measurements of the anomalous Nernst effect of Mn$_5$Si$_3$ single crystals.","Below 25 K we observe a sign change of the zero-field Nernst signal with a concomitant decrease of the Hall signal and a gradual reduction of the remanent magnetization which we attribute to a subtle rearrangement of the magnetic moment configuration at low temperatures."],"url":"http://arxiv.org/abs/2405.03438v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 12:54:22","title":"Improved Forward-Forward Contrastive Learning","abstract":"The backpropagation algorithm, or backprop, is a widely utilized optimization technique in deep learning. While there's growing evidence suggesting that models trained with backprop can accurately explain neuronal data, no backprop-like method has yet been discovered in the biological brain for learning. Moreover, employing a naive implementation of backprop in the brain has several drawbacks. In 2022, Geoffrey Hinton proposed a biologically plausible learning method known as the Forward-Forward (FF) algorithm. Shortly after this paper, a modified version called FFCL was introduced. However, FFCL had limitations, notably being a three-stage learning system where the final stage still relied on regular backpropagation. In our approach, we address these drawbacks by eliminating the last two stages of FFCL and completely removing regular backpropagation. Instead, we rely solely on local updates, offering a more biologically plausible alternative.","sentences":["The backpropagation algorithm, or backprop, is a widely utilized optimization technique in deep learning.","While there's growing evidence suggesting that models trained with backprop can accurately explain neuronal data, no backprop-like method has yet been discovered in the biological brain for learning.","Moreover, employing a naive implementation of backprop in the brain has several drawbacks.","In 2022, Geoffrey Hinton proposed a biologically plausible learning method known as the Forward-Forward (FF) algorithm.","Shortly after this paper, a modified version called FFCL was introduced.","However, FFCL had limitations, notably being a three-stage learning system where the final stage still relied on regular backpropagation.","In our approach, we address these drawbacks by eliminating the last two stages of FFCL and completely removing regular backpropagation.","Instead, we rely solely on local updates, offering a more biologically plausible alternative."],"url":"http://arxiv.org/abs/2405.03432v1","category":"cs.LG"}
{"created":"2024-05-06 12:47:16","title":"Geometry-aware framework for deep energy method: an application to structural mechanics with hyperelastic materials","abstract":"Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models. Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs. Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation. In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries. As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries. Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost. Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work. We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries. An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM. We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity. The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model.","sentences":["Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models.","Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs.","Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation.","In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries.","As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries.","Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost.","Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work.","We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries.","An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM.","We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity.","The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model."],"url":"http://arxiv.org/abs/2405.03427v1","category":"cs.LG"}
{"created":"2024-05-06 12:40:15","title":"Implantable Adaptive Cells: differentiable architecture search to improve the performance of any trained U-shaped network","abstract":"This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS). We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model. Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch. The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method. The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset. The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains.","sentences":["This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS).","We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model.","Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch.","The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method.","The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset.","The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains."],"url":"http://arxiv.org/abs/2405.03420v1","category":"cs.CV"}
{"created":"2024-05-06 12:24:49","title":"SL-SLAM: A robust visual-inertial SLAM based deep feature extraction and matching","abstract":"This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments. By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter. Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations. We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches. Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches. The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness. For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam.","sentences":["This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments.","By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter.","Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations.","We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches.","Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches.","The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness.","For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam."],"url":"http://arxiv.org/abs/2405.03413v1","category":"cs.RO"}
{"created":"2024-05-06 12:23:59","title":"Compact minimal submanifolds of the Riemannian symmetric spaces $SU(n)/SO(n)$, $Sp(n)/U(n)$, $SO(2n)/U(n)$, $SU(2n)/Sp(n)$ via complex-valued eigenfunctions","abstract":"In this work we construct new multi-dimensional families of compact minimal submanifolds, of the classical Riemannian symmetric spaces $SU(n)/SO(n)$, $Sp(n)/U(n)$, $SO(2n)/U(n)$ and $SU(2n)/Sp(n)$, of codimension two.","sentences":["In this work we construct new multi-dimensional families of compact minimal submanifolds, of the classical Riemannian symmetric spaces $SU(n)/SO(n)$, $Sp(n)/U(n)$, $SO(2n)/U(n)$ and $SU(2n)/Sp(n)$, of codimension two."],"url":"http://arxiv.org/abs/2405.03412v1","category":"math.DG"}
{"created":"2024-05-06 12:22:11","title":"Greedy Heuristics for Sampling-based Motion Planning in High-Dimensional State Spaces","abstract":"Sampling-based motion planning algorithms are very effective at finding solutions in high-dimensional continuous state spaces as they do not require prior approximations of the problem domain compared to traditional discrete graph-based searches. The anytime version of the Rapidly-exploring Random Trees (RRT) algorithm, denoted as RRT*, often finds high-quality solutions by incrementally approximating and searching the problem domain through random sampling. However, due to its low sampling efficiency and slow convergence rate, research has proposed many variants of RRT*, incorporating different heuristics and sampling strategies to overcome the constraints in complex planning problems. Yet, these approaches address specific convergence aspects of RRT* limitations, leaving a need for a sampling-based algorithm that can quickly find better solutions in complex high-dimensional state spaces with a faster convergence rate for practical motion planning applications. This article unifies and leverages the greedy search and heuristic techniques used in various RRT* variants to develop a greedy version of the anytime Rapidly-exploring Random Trees algorithm, denoted as Greedy RRT* (G-RRT*). It improves the initial solution-finding time of RRT* by maintaining two trees rooted at both the start and goal ends, advancing toward each other using greedy connection heuristics. It also accelerates the convergence rate of RRT* by introducing a greedy version of direct informed sampling procedure, which guides the sampling towards the promising region of the problem domain based on heuristics. We validate our approach on simulated planning problems, manipulation problems on Barrett WAM Arms, and on a self-reconfigurable robot, Panthera. Results show that G-RRT* produces asymptotically optimal solution paths and outperforms state-of-the-art RRT* variants, especially in high-dimensional planning problems.","sentences":["Sampling-based motion planning algorithms are very effective at finding solutions in high-dimensional continuous state spaces as they do not require prior approximations of the problem domain compared to traditional discrete graph-based searches.","The anytime version of the Rapidly-exploring Random Trees (RRT) algorithm, denoted as RRT*, often finds high-quality solutions by incrementally approximating and searching the problem domain through random sampling.","However, due to its low sampling efficiency and slow convergence rate, research has proposed many variants of RRT*, incorporating different heuristics and sampling strategies to overcome the constraints in complex planning problems.","Yet, these approaches address specific convergence aspects of RRT* limitations, leaving a need for a sampling-based algorithm that can quickly find better solutions in complex high-dimensional state spaces with a faster convergence rate for practical motion planning applications.","This article unifies and leverages the greedy search and heuristic techniques used in various RRT* variants to develop a greedy version of the anytime Rapidly-exploring Random Trees algorithm, denoted as Greedy RRT* (G-RRT*).","It improves the initial solution-finding time of RRT* by maintaining two trees rooted at both the start and goal ends, advancing toward each other using greedy connection heuristics.","It also accelerates the convergence rate of RRT* by introducing a greedy version of direct informed sampling procedure, which guides the sampling towards the promising region of the problem domain based on heuristics.","We validate our approach on simulated planning problems, manipulation problems on Barrett WAM Arms, and on a self-reconfigurable robot, Panthera.","Results show that G-RRT* produces asymptotically optimal solution paths and outperforms state-of-the-art RRT* variants, especially in high-dimensional planning problems."],"url":"http://arxiv.org/abs/2405.03411v1","category":"cs.RO"}
{"created":"2024-05-06 12:20:16","title":"An Image Quality Evaluation and Masking Algorithm Based On Pre-trained Deep Neural Networks","abstract":"With the growing amount of astronomical data, there is an increasing need for automated data processing pipelines, which can extract scientific information from observation data without human interventions. A critical aspect of these pipelines is the image quality evaluation and masking algorithm, which evaluates image qualities based on various factors such as cloud coverage, sky brightness, scattering light from the optical system, point spread function size and shape, and read-out noise. Occasionally, the algorithm requires masking of areas severely affected by noise. However, the algorithm often necessitates significant human interventions, reducing data processing efficiency. In this study, we present a deep learning based image quality evaluation algorithm that uses an autoencoder to learn features of high quality astronomical images. The trained autoencoder enables automatic evaluation of image quality and masking of noise affected areas. We have evaluated the performance of our algorithm using two test cases: images with point spread functions of varying full width half magnitude, and images with complex backgrounds. In the first scenario, our algorithm could effectively identify variations of the point spread functions, which can provide valuable reference information for photometry. In the second scenario, our method could successfully mask regions affected by complex regions, which could significantly increase the photometry accuracy. Our algorithm can be employed to automatically evaluate image quality obtained by different sky surveying projects, further increasing the speed and robustness of data processing pipelines.","sentences":["With the growing amount of astronomical data, there is an increasing need for automated data processing pipelines, which can extract scientific information from observation data without human interventions.","A critical aspect of these pipelines is the image quality evaluation and masking algorithm, which evaluates image qualities based on various factors such as cloud coverage, sky brightness, scattering light from the optical system, point spread function size and shape, and read-out noise.","Occasionally, the algorithm requires masking of areas severely affected by noise.","However, the algorithm often necessitates significant human interventions, reducing data processing efficiency.","In this study, we present a deep learning based image quality evaluation algorithm that uses an autoencoder to learn features of high quality astronomical images.","The trained autoencoder enables automatic evaluation of image quality and masking of noise affected areas.","We have evaluated the performance of our algorithm using two test cases: images with point spread functions of varying full width half magnitude, and images with complex backgrounds.","In the first scenario, our algorithm could effectively identify variations of the point spread functions, which can provide valuable reference information for photometry.","In the second scenario, our method could successfully mask regions affected by complex regions, which could significantly increase the photometry accuracy.","Our algorithm can be employed to automatically evaluate image quality obtained by different sky surveying projects, further increasing the speed and robustness of data processing pipelines."],"url":"http://arxiv.org/abs/2405.03408v1","category":"astro-ph.IM"}
{"created":"2024-05-06 12:15:08","title":"Non-singular flows with twisted saddle orbit on orientable 3-manifolds","abstract":"In this paper we consider non-singular Morse-Smale flows on closed orientable 3-manifolds, under the assumption that among the periodic orbits of the flow there is only one saddle orbit and it is twisted. It is found that any manifold admitting such flows is either a lens space, or a connected sum of a lens space with a projective space, or Seifert manifolds with base sphere and three special layers. A complete topological classification of the described flows is obtained and the number of their equivalence classes on each admissible manifold is calculated.","sentences":["In this paper we consider non-singular Morse-Smale flows on closed orientable 3-manifolds, under the assumption that among the periodic orbits of the flow there is only one saddle orbit and it is twisted.","It is found that any manifold admitting such flows is either a lens space, or a connected sum of a lens space with a projective space, or Seifert manifolds with base sphere and three special layers.","A complete topological classification of the described flows is obtained and the number of their equivalence classes on each admissible manifold is calculated."],"url":"http://arxiv.org/abs/2405.03404v1","category":"math.DS"}
{"created":"2024-05-06 12:01:55","title":"Domains of existence of slice regular functions in one quaternionic variable","abstract":"Recently, we introduced domains of slice regularity in the space $\\mathbb{H}$ of quaternions and also proved that domains of slice regularity satisfy a symmetry with respect to paths, called $2$-path-symmetry. In this paper, we give a full characterization by showing that all $2$-path-symmetric slice-open sets are domains of slice regularity. In fact, we will prove a counterpart of the Cartan-Thullen theorem for slice regular functions, namely that a slice-open set is a domain of existence for some slice regular function if and only if it is a domain of slice regularity, if and only if it is slice-regularly convex, if and only if it is $2$-path-symmetric. As a tool, we also prove an interpolation theorem of independent interest.","sentences":["Recently, we introduced domains of slice regularity in the space $\\mathbb{H}$ of quaternions and also proved that domains of slice regularity satisfy a symmetry with respect to paths, called $2$-path-symmetry.","In this paper, we give a full characterization by showing that all $2$-path-symmetric slice-open sets are domains of slice regularity.","In fact, we will prove a counterpart of the Cartan-Thullen theorem for slice regular functions, namely that a slice-open set is a domain of existence for some slice regular function if and only if it is a domain of slice regularity, if and only if it is slice-regularly convex, if and only if it is $2$-path-symmetric.","As a tool, we also prove an interpolation theorem of independent interest."],"url":"http://arxiv.org/abs/2405.03394v1","category":"math.CV"}
{"created":"2024-05-06 11:56:14","title":"On-site scale factor linearity calibration of MEMS triaxial gyroscopes","abstract":"The calibration of MEMS triaxial gyroscopes is crucial for achieving precise attitude estimation for various wearable health monitoring applications. However, gyroscope calibration poses greater challenges compared to accelerometers and magnetometers. This paper introduces an efficient method for calibrating MEMS triaxial gyroscopes via only a servo motor, making it well-suited for field environments. The core strategy of the method involves utilizing the fact that the dot product of the measured gravity and the rotational speed in a fixed frame remains constant. To eliminate the influence of rotating centrifugal force on the accelerometer, the accelerometer data is measured while stationary. The proposed calibration experiment scheme, which allows gyroscopic measurements when operating each axis at a specific rotation speed, making it easier to evaluate the linearity across a related speed range constituted by a series of rotation speeds. Moreover, solely the classical least squares algorithm proves adequate for estimating the scale factor, notably streamlining the analysis of the calibration process. Extensive numerical simulations were conducted to analyze the proposed method's performance in calibrating a triaxial gyroscope model. Experimental validation was also carried out using a commercially available MEMS inertial measurement unit (LSM9DS1 from Arduino nano 33 BLE SENSE) and a servo motor capable of controlling precise speed. The experimental results effectively demonstrate the efficacy of the proposed calibration approach.","sentences":["The calibration of MEMS triaxial gyroscopes is crucial for achieving precise attitude estimation for various wearable health monitoring applications.","However, gyroscope calibration poses greater challenges compared to accelerometers and magnetometers.","This paper introduces an efficient method for calibrating MEMS triaxial gyroscopes via only a servo motor, making it well-suited for field environments.","The core strategy of the method involves utilizing the fact that the dot product of the measured gravity and the rotational speed in a fixed frame remains constant.","To eliminate the influence of rotating centrifugal force on the accelerometer, the accelerometer data is measured while stationary.","The proposed calibration experiment scheme, which allows gyroscopic measurements when operating each axis at a specific rotation speed, making it easier to evaluate the linearity across a related speed range constituted by a series of rotation speeds.","Moreover, solely the classical least squares algorithm proves adequate for estimating the scale factor, notably streamlining the analysis of the calibration process.","Extensive numerical simulations were conducted to analyze the proposed method's performance in calibrating a triaxial gyroscope model.","Experimental validation was also carried out using a commercially available MEMS inertial measurement unit (LSM9DS1 from Arduino nano 33 BLE SENSE) and a servo motor capable of controlling precise speed.","The experimental results effectively demonstrate the efficacy of the proposed calibration approach."],"url":"http://arxiv.org/abs/2405.03393v1","category":"cs.RO"}
{"created":"2024-05-06 11:54:58","title":"A note on adjoint reality in simple complex Lie algebras","abstract":"Let $G$ be a Lie group with Lie algebra $\\mathfrak g$. In the paper \"Reality of unipotent elements in simple Lie groups, Bull. Sci. Math., 185, 2023, 103261\" by K. Gongopadhyay and C. Maity, an infinitesimal version of the notion of classical reality, namely adjoint reality, has been introduced. An element $X \\in \\mathfrak g$ is adjoint real if $-X$ belongs to the adjoint orbit of $X$ in $\\mathfrak g$. In this paper, we investigate the adjoint real and the strongly adjoint real semisimple elements in complex simple classical Lie algebras. We also prove that every element in a complex symplectic Lie algebra is adjoint real.","sentences":["Let $G$ be a Lie group with Lie algebra $\\mathfrak g$.","In the paper \"Reality of unipotent elements in simple Lie groups, Bull.","Sci.","Math., 185, 2023, 103261\" by K. Gongopadhyay and C. Maity, an infinitesimal version of the notion of classical reality, namely adjoint reality, has been introduced.","An element $X \\in \\mathfrak g$ is adjoint real if $-X$ belongs to the adjoint orbit of $X$ in $\\mathfrak g$.","In this paper, we investigate the adjoint real and the strongly adjoint real semisimple elements in complex simple classical Lie algebras.","We also prove that every element in a complex symplectic Lie algebra is adjoint real."],"url":"http://arxiv.org/abs/2405.03392v1","category":"math.GR"}
{"created":"2024-05-06 11:53:58","title":"Prediction of chaotic dynamics and extreme events: A recurrence-free quantum reservoir computing approach","abstract":"In chaotic dynamical systems, extreme events manifest in time series as unpredictable large-amplitude peaks. Although deterministic, extreme events appear seemingly randomly, which makes their forecasting difficult. By learning the dynamics from observables (data), reservoir computers can time-accurately predict extreme events and chaotic dynamics, but they may require many degrees of freedom (large reservoirs). In this paper, by exploiting quantum-computer ans\\\"atze and entanglement, we design reservoir computers with compact reservoirs and accurate prediction capabilities. First, we propose the recurrence-free quantum reservoir computer (RF-QRC) architecture. By developing ad-hoc quantum feature maps and removing recurrent connections, the RF-QRC has quantum circuits with small depths. This allows the RF-QRC to scale well with higher-dimensional chaotic systems, which makes it suitable for hardware implementation. Second, we forecast the temporal chaotic dynamics and their long-term statistics of low- and higher-dimensional dynamical systems. We find that RF-QRC requires smaller reservoirs than classical reservoir computers. Third, we apply the RF-QRC to the time prediction of extreme events in a model of a turbulent shear flow with turbulent bursts. We find that the RF-QRC has a longer predictability than the classical reservoir computer. The results and analyses indicate that quantum-computer ans\\\"atze offer nonlinear expressivity and computational scalability, which are useful for forecasting chaotic dynamics and extreme events. This work opens new opportunities for using quantum machine learning on near-term quantum computers.","sentences":["In chaotic dynamical systems, extreme events manifest in time series as unpredictable large-amplitude peaks.","Although deterministic, extreme events appear seemingly randomly, which makes their forecasting difficult.","By learning the dynamics from observables (data), reservoir computers can time-accurately predict extreme events and chaotic dynamics, but they may require many degrees of freedom (large reservoirs).","In this paper, by exploiting quantum-computer ans\\\"atze and entanglement, we design reservoir computers with compact reservoirs and accurate prediction capabilities.","First, we propose the recurrence-free quantum reservoir computer (RF-QRC) architecture.","By developing ad-hoc quantum feature maps and removing recurrent connections, the RF-QRC has quantum circuits with small depths.","This allows the RF-QRC to scale well with higher-dimensional chaotic systems, which makes it suitable for hardware implementation.","Second, we forecast the temporal chaotic dynamics and their long-term statistics of low- and higher-dimensional dynamical systems.","We find that RF-QRC requires smaller reservoirs than classical reservoir computers.","Third, we apply the RF-QRC to the time prediction of extreme events in a model of a turbulent shear flow with turbulent bursts.","We find that the RF-QRC has a longer predictability than the classical reservoir computer.","The results and analyses indicate that quantum-computer ans\\\"atze offer nonlinear expressivity and computational scalability, which are useful for forecasting chaotic dynamics and extreme events.","This work opens new opportunities for using quantum machine learning on near-term quantum computers."],"url":"http://arxiv.org/abs/2405.03390v1","category":"quant-ph"}
{"created":"2024-05-06 11:36:55","title":"Anisotropic transport properties in prismatic topological insulator nanowires","abstract":"The surface of a three dimensional topological insulator (TI) hosts surface states whose properties are determined by a Dirac-like equation. The electronic system on the surface of TI nanowires with polygonal cross-sectional shape adopts the corresponding polygonal shape. In a constant transverse magnetic field, such an electronic system exhibits rich properties as different facets of the polygon experience different values of the magnetic field due to the changing magnetic field projection between facets. We investigate the energy spectrum and transport properties of nanowires where we consider three different polygonal shapes, all showing distinct properties visible in the energy spectrum and transport properties. Here we propose that the wire conductance can be used to differentiate between cross-sectional shapes of the nanowire by rotating the magnetic field around the wire. Distinguishing between the different shapes also works in the presence of impurities as long as conductance steps are discernible, thus revealing the sub-band structure.","sentences":["The surface of a three dimensional topological insulator (TI) hosts surface states whose properties are determined by a Dirac-like equation.","The electronic system on the surface of TI nanowires with polygonal cross-sectional shape adopts the corresponding polygonal shape.","In a constant transverse magnetic field, such an electronic system exhibits rich properties as different facets of the polygon experience different values of the magnetic field due to the changing magnetic field projection between facets.","We investigate the energy spectrum and transport properties of nanowires where we consider three different polygonal shapes, all showing distinct properties visible in the energy spectrum and transport properties.","Here we propose that the wire conductance can be used to differentiate between cross-sectional shapes of the nanowire by rotating the magnetic field around the wire.","Distinguishing between the different shapes also works in the presence of impurities as long as conductance steps are discernible, thus revealing the sub-band structure."],"url":"http://arxiv.org/abs/2405.03380v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 11:32:39","title":"Upper Bound for the Free Energy of Dilute Bose Gases at Low Temperature","abstract":"We consider a Bose gas at density $\\rho > 0$, interacting through a repulsive potential $V \\in L^2 (\\mathbb{R}^3)$ with scattering length $\\mathfrak{a} > 0$. We prove an upper bound for the free energy of the system, valid at low temperature $T \\lesssim \\rho \\mathfrak{a}$. Combined with the recent lower bound obtained in \\cite{HabHaiNamSeiTri-23}, our estimate resolves the free energy per unit volume up to and including the Lee--Huang--Yang order $\\mathfrak{a} \\rho^2 (\\rho \\mathfrak{a}^3)^{1/2}$.","sentences":["We consider a Bose gas at density $\\rho > 0$, interacting through a repulsive potential $V \\in L^2 (\\mathbb{R}^3)$ with scattering length $\\mathfrak{a} > 0$.","We prove an upper bound for the free energy of the system, valid at low temperature $T \\lesssim \\rho \\mathfrak{a}$. Combined with the recent lower bound obtained in \\cite{HabHaiNamSeiTri-23}, our estimate resolves the free energy per unit volume up to and including the Lee--Huang--Yang order $\\mathfrak{a} \\rho^2 (\\rho \\mathfrak{a}^3)^{1/2}$."],"url":"http://arxiv.org/abs/2405.03378v1","category":"math-ph"}
{"created":"2024-05-06 11:31:26","title":"High-dimensional quantum key distribution using orbital angular momentum of single photons from a colloidal quantum dot at room temperature","abstract":"High-dimensional quantum key distribution (HDQKD) is a promising avenue to address the inherent limitations of basic QKD protocols. However, experimental realizations of HDQKD to date have relied on indeterministic photon sources that limit the achievable key rate. In this paper, we demonstrate a full emulation of a HDQKD system using a single colloidal giant quantum dot (gQD) as a deterministic, compact and room-temperature single-photon source (SPS). We demonstrate a practical protocol by encoding information in a high-dimensional space ($d = 3$) of the orbital angular momentum of the photons. Our experimental configuration incorporates two spatial light modulators for encoding and decoding the spatial information carried by individual photons. Our experimental demonstration establishes the feasibility of utilizing high radiative quantum yield gQDs as practical SPSs for HDQKD. We also demonstrate experimentally secure qudit transmission exceeding one secure bit per photon, thus already beating the traditional d=2 QKD capacity.","sentences":["High-dimensional quantum key distribution (HDQKD) is a promising avenue to address the inherent limitations of basic QKD protocols.","However, experimental realizations of HDQKD to date have relied on indeterministic photon sources that limit the achievable key rate.","In this paper, we demonstrate a full emulation of a HDQKD system using a single colloidal giant quantum dot (gQD) as a deterministic, compact and room-temperature single-photon source (SPS).","We demonstrate a practical protocol by encoding information in a high-dimensional space ($d = 3$) of the orbital angular momentum of the photons.","Our experimental configuration incorporates two spatial light modulators for encoding and decoding the spatial information carried by individual photons.","Our experimental demonstration establishes the feasibility of utilizing high radiative quantum yield gQDs as practical SPSs for HDQKD.","We also demonstrate experimentally secure qudit transmission exceeding one secure bit per photon, thus already beating the traditional d=2 QKD capacity."],"url":"http://arxiv.org/abs/2405.03377v1","category":"quant-ph"}
{"created":"2024-05-06 11:30:55","title":"CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer","abstract":"The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.","sentences":["The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities.","However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research.","To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers.","Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer.","This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution.","This method improves the estimation of distributions for cross-entropy coding.","Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data.","By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB).","This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis.","Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset.","Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5."],"url":"http://arxiv.org/abs/2405.03376v1","category":"cs.LG"}
{"created":"2024-05-06 11:30:36","title":"Three-temperature radiation hydrodynamics with PLUTO: Thermal and kinematic signatures of accreting protoplanets","abstract":"In circumstellar disks around young stars, the gravitational influence of nascent planets produces telltale patterns in density, temperature, and kinematics. To better understand these signatures, we first performed 3D hydrodynamical simulations of a 0.012 $M_{\\odot}$ disk, with a Saturn-mass planet orbiting circularly in-plane at 40 au. We tested four different disk thermodynamic prescriptions (in increasing order of complexity, local isothermality, $\\beta$-cooling, two-temperature radiation hydrodynamics, and three-temperature radiation hydrodynamics), finding that $\\beta$-cooling offers a reasonable approximation for the three-temperature approach when the planet is not massive or luminous enough to substantially alter the background temperature and density structure. Thereafter, using the three-temperature scheme, we relaxed this assumption, simulating a range of different planet masses (Neptune-mass, Saturn-mass, Jupiter-mass) and accretion luminosities (0, $10^{-3} L_{\\odot}$) in the same disk. Our investigation revealed that signatures of disk-planet interaction strengthen with increasing planet mass, with circumplanetary flows becoming prominent in the high-planet-mass regime. Accretion luminosity, which adds pressure support around the planet, was found to weaken the midplane Doppler-flip, potentially visible in optically thin tracers like C$^{18}$O, while strengthening the spiral signature, particularly in upper disk layers sensitive to thicker lines, like those of $^{12}$CO.","sentences":["In circumstellar disks around young stars, the gravitational influence of nascent planets produces telltale patterns in density, temperature, and kinematics.","To better understand these signatures, we first performed 3D hydrodynamical simulations of a 0.012 $M_{\\odot}$ disk, with a Saturn-mass planet orbiting circularly in-plane at 40 au.","We tested four different disk thermodynamic prescriptions (in increasing order of complexity, local isothermality, $\\beta$-cooling, two-temperature radiation hydrodynamics, and three-temperature radiation hydrodynamics), finding that $\\beta$-cooling offers a reasonable approximation for the three-temperature approach when the planet is not massive or luminous enough to substantially alter the background temperature and density structure.","Thereafter, using the three-temperature scheme, we relaxed this assumption, simulating a range of different planet masses (Neptune-mass, Saturn-mass, Jupiter-mass) and accretion luminosities (0, $10^{-3} L_{\\odot}$) in the same disk.","Our investigation revealed that signatures of disk-planet interaction strengthen with increasing planet mass, with circumplanetary flows becoming prominent in the high-planet-mass regime.","Accretion luminosity, which adds pressure support around the planet, was found to weaken the midplane Doppler-flip, potentially visible in optically thin tracers like C$^{18}$O, while strengthening the spiral signature, particularly in upper disk layers sensitive to thicker lines, like those of $^{12}$CO."],"url":"http://arxiv.org/abs/2405.03375v1","category":"astro-ph.EP"}
{"created":"2024-05-06 11:27:27","title":"Knowledge-aware Text-Image Retrieval for Remote Sensing Images","abstract":"Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images. To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images. By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications. Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.","sentences":["Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide.","By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only.","For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images.","To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images.","By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching.","Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications.","Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods."],"url":"http://arxiv.org/abs/2405.03373v1","category":"cs.CV"}
{"created":"2024-05-06 11:13:46","title":"Exploring anisotropic pressure and spatial correlations in strongly confined hard-disk fluids. Exact results","abstract":"This study examines the transverse and longitudinal properties of hard disks confined in narrow channels. We employ an exact mapping of the system onto a one-dimensional polydisperse, nonadditive mixture of hard rods with equal chemical potentials. We compute various thermodynamic properties, including the transverse and longitudinal equations of state, along with their behaviors at both low and high densities. Structural properties are analyzed using the two-body correlation function and the radial distribution function, tailored for the highly anisotropic geometry of this system. The results are corroborated by computer simulations.","sentences":["This study examines the transverse and longitudinal properties of hard disks confined in narrow channels.","We employ an exact mapping of the system onto a one-dimensional polydisperse, nonadditive mixture of hard rods with equal chemical potentials.","We compute various thermodynamic properties, including the transverse and longitudinal equations of state, along with their behaviors at both low and high densities.","Structural properties are analyzed using the two-body correlation function and the radial distribution function, tailored for the highly anisotropic geometry of this system.","The results are corroborated by computer simulations."],"url":"http://arxiv.org/abs/2405.03362v1","category":"cond-mat.soft"}
{"created":"2024-05-06 11:02:50","title":"Markov Chain-based Optimization Time Analysis of Bivalent Ant Colony Optimization for Sorting and LeadingOnes","abstract":"So far, only few bounds on the runtime behavior of Ant Colony Optimization (ACO) have been reported. To alleviate this situation, we investigate the ACO variant we call Bivalent ACO (BACO) that uses exactly two pheromone values. We provide and successfully apply a new Markov chain-based approach to calculate the expected optimization time, i. e., the expected number of iterations until the algorithm terminates. This approach allows to derive exact formulae for the expected optimization time for the problems Sorting and LeadingOnes. It turns out that the ratio of the two pheromone values significantly governs the runtime behavior of BACO. To the best of our knowledge, for the first time, we can present tight bounds for Sorting ($\\Theta(n^3)$) with a specifically chosen objective function and prove the missing lower bound $\\Omega(n^2)$ for LeadingOnes which, thus, is tightly bounded by $\\Theta(n^2)$. We show that despite we have a drastically simplified ant algorithm with respect to the influence of the pheromones on the solving process, known bounds on the expected optimization time for the problems OneMax ($O(n\\log n)$) and LeadingOnes ($O(n^2)$) can be re-produced as a by-product of our approach. Experiments validate our theoretical findings.","sentences":["So far, only few bounds on the runtime behavior of Ant Colony Optimization (ACO) have been reported.","To alleviate this situation, we investigate the ACO variant we call Bivalent ACO (BACO) that uses exactly two pheromone values.","We provide and successfully apply a new Markov chain-based approach to calculate the expected optimization time, i. e., the expected number of iterations until the algorithm terminates.","This approach allows to derive exact formulae for the expected optimization time for the problems Sorting and LeadingOnes.","It turns out that the ratio of the two pheromone values significantly governs the runtime behavior of BACO.","To the best of our knowledge, for the first time, we can present tight bounds for Sorting ($\\Theta(n^3)$) with a specifically chosen objective function and prove the missing lower bound $\\Omega(n^2)$ for LeadingOnes which, thus, is tightly bounded by $\\Theta(n^2)$. We show that despite we have a drastically simplified ant algorithm with respect to the influence of the pheromones on the solving process, known bounds on the expected optimization time for the problems OneMax ($O(n\\log n)$) and LeadingOnes ($O(n^2)$) can be re-produced as a by-product of our approach.","Experiments validate our theoretical findings."],"url":"http://arxiv.org/abs/2405.03353v1","category":"cs.NE"}
{"created":"2024-05-06 10:51:09","title":"FAIR 2.0: Extending the FAIR Guiding Principles to Address Semantic Interoperability","abstract":"FAIR data presupposes their successful communication between machines and humans while preserving their meaning and reference, requiring all parties involved to share the same background knowledge. Inspired by English as a natural language, we investigate the linguistic structure that ensures reliable communication of information and draw parallels with data structures, understanding both as models of systems of interest. We conceptualize semantic interoperability as comprising terminological and propositional interoperability. The former includes ontological (i.e., same meaning) and referential (i.e., same referent/extension) interoperability and the latter schema (i.e., same data schema) and logical (i.e., same logical framework) interoperability. Since no best ontology and no best data schema exists, establishing semantic interoperability and FAIRness of data and metadata requires the provision of a comprehensive set of relevant ontological and referential entity mappings and schema crosswalks. We therefore propose appropriate additions to the FAIR Guiding Principles, leading to FAIR 2.0. Furthermore, achieving FAIRness of data requires the provision of FAIR services in addition to organizing data into FAIR Digital Objects. FAIR services include a terminology, a schema, and an operations service.","sentences":["FAIR data presupposes their successful communication between machines and humans while preserving their meaning and reference, requiring all parties involved to share the same background knowledge.","Inspired by English as a natural language, we investigate the linguistic structure that ensures reliable communication of information and draw parallels with data structures, understanding both as models of systems of interest.","We conceptualize semantic interoperability as comprising terminological and propositional interoperability.","The former includes ontological (i.e., same meaning) and referential (i.e., same referent/extension) interoperability and the latter schema (i.e., same data schema) and logical (i.e., same logical framework) interoperability.","Since no best ontology and no best data schema exists, establishing semantic interoperability and FAIRness of data and metadata requires the provision of a comprehensive set of relevant ontological and referential entity mappings and schema crosswalks.","We therefore propose appropriate additions to the FAIR Guiding Principles, leading to FAIR 2.0.","Furthermore, achieving FAIRness of data requires the provision of FAIR services in addition to organizing data into FAIR Digital Objects.","FAIR services include a terminology, a schema, and an operations service."],"url":"http://arxiv.org/abs/2405.03345v1","category":"cs.DB"}
{"created":"2024-05-06 10:39:03","title":"Quantum Algorithms for Inverse Participation Ratio Estimation in multi-qubit and multi-qudit systems","abstract":"Inverse Participation Ratios (IPRs) and the related Participation Entropies quantify the spread of a quantum state over a selected basis of the Hilbert space, offering insights into the equilibrium and non-equilibrium properties of the system. In this work, we propose three quantum algorithms to estimate IPRs on multi-qubit and multi-qudit quantum devices. The first algorithm allows for the estimation of IPRs in the computational basis by single-qubit measurements, while the second one enables measurement of IPR in the eigenbasis of a selected Hamiltonian, without the knowledge about the eigenstates of the system. Next, we provide an algorithm for IPR in the computational basis for a multi-qudit system. We discuss resources required by the algorithms and benchmark them by investigating the one-axis twisting protocol, the thermalization in a deformed PXP model, and the ground state of a spin-$1$ AKLT chain in a transverse field.","sentences":["Inverse Participation Ratios (IPRs) and the related Participation Entropies quantify the spread of a quantum state over a selected basis of the Hilbert space, offering insights into the equilibrium and non-equilibrium properties of the system.","In this work, we propose three quantum algorithms to estimate IPRs on multi-qubit and multi-qudit quantum devices.","The first algorithm allows for the estimation of IPRs in the computational basis by single-qubit measurements, while the second one enables measurement of IPR in the eigenbasis of a selected Hamiltonian, without the knowledge about the eigenstates of the system.","Next, we provide an algorithm for IPR in the computational basis for a multi-qudit system.","We discuss resources required by the algorithms and benchmark them by investigating the one-axis twisting protocol, the thermalization in a deformed PXP model, and the ground state of a spin-$1$ AKLT chain in a transverse field."],"url":"http://arxiv.org/abs/2405.03338v1","category":"quant-ph"}
{"created":"2024-05-06 09:25:40","title":"Verification of Perrin's theory of the motion of dilute spheroidal colloids","abstract":"Brownian motion is of central importance for understanding diffusive transport in biology, chemistry, and physics. For spherical particles, the theory was developed by Einstein, whereas a theoretical description of the motion of spheroids was given by F. Perrin. Here, we report the systematic verification of Perrin's theory 90 years after its publication. To this end, we synthesized oblate and prolate core-shell spheroids with different aspect ratios and tracked their three-dimensional diffusive motion in high dilution using confocal fluorescence microscopy. The experimental data for the dependence of translational and rotational diffusion on aspect ratio are in excellent agreement with the theoretical predictions. The crossover dynamics from anisotropic to isotropic diffusion as a hallmark for translation rotation coupling are also found as predicted. This verifies Perrin's theory as a cornerstone for understanding diffusive transport and underlines the excellent suitability of the particle system for testing more detailed theory.","sentences":["Brownian motion is of central importance for understanding diffusive transport in biology, chemistry, and physics.","For spherical particles, the theory was developed by Einstein, whereas a theoretical description of the motion of spheroids was given by F. Perrin.","Here, we report the systematic verification of Perrin's theory 90 years after its publication.","To this end, we synthesized oblate and prolate core-shell spheroids with different aspect ratios and tracked their three-dimensional diffusive motion in high dilution using confocal fluorescence microscopy.","The experimental data for the dependence of translational and rotational diffusion on aspect ratio are in excellent agreement with the theoretical predictions.","The crossover dynamics from anisotropic to isotropic diffusion as a hallmark for translation rotation coupling are also found as predicted.","This verifies Perrin's theory as a cornerstone for understanding diffusive transport and underlines the excellent suitability of the particle system for testing more detailed theory."],"url":"http://arxiv.org/abs/2405.03304v1","category":"cond-mat.soft"}
{"created":"2024-05-06 09:20:17","title":"Online Clustering of Known and Emerging Malware Families","abstract":"Malware attacks have become significantly more frequent and sophisticated in recent years. Therefore, malware detection and classification are critical components of information security. Due to the large amount of malware samples available, it is essential to categorize malware samples according to their malicious characteristics. Clustering algorithms are thus becoming more widely used in computer security to analyze the behavior of malware variants and discover new malware families. Online clustering algorithms help us to understand malware behavior and produce a quicker response to new threats. This paper introduces a novel machine learning-based model for the online clustering of malicious samples into malware families. Streaming data is divided according to the clustering decision rule into samples from known and new emerging malware families. The streaming data is classified using the weighted k-nearest neighbor classifier into known families, and the online k-means algorithm clusters the remaining streaming data and achieves a purity of clusters from 90.20% for four clusters to 93.34% for ten clusters. This work is based on static analysis of portable executable files for the Windows operating system. Experimental results indicate that the proposed online clustering model can create high-purity clusters corresponding to malware families. This allows malware analysts to receive similar malware samples, speeding up their analysis.","sentences":["Malware attacks have become significantly more frequent and sophisticated in recent years.","Therefore, malware detection and classification are critical components of information security.","Due to the large amount of malware samples available, it is essential to categorize malware samples according to their malicious characteristics.","Clustering algorithms are thus becoming more widely used in computer security to analyze the behavior of malware variants and discover new malware families.","Online clustering algorithms help us to understand malware behavior and produce a quicker response to new threats.","This paper introduces a novel machine learning-based model for the online clustering of malicious samples into malware families.","Streaming data is divided according to the clustering decision rule into samples from known and new emerging malware families.","The streaming data is classified using the weighted k-nearest neighbor classifier into known families, and the online k-means algorithm clusters the remaining streaming data and achieves a purity of clusters from 90.20% for four clusters to 93.34% for ten clusters.","This work is based on static analysis of portable executable files for the Windows operating system.","Experimental results indicate that the proposed online clustering model can create high-purity clusters corresponding to malware families.","This allows malware analysts to receive similar malware samples, speeding up their analysis."],"url":"http://arxiv.org/abs/2405.03298v1","category":"cs.CR"}
{"created":"2024-05-06 09:10:42","title":"Source Region and Launch Characteristics of Magnetic-arch-blowout Solar Coronal Mass Ejections Driven by Homologous Compact-flare Blowout Jets","abstract":"We study the formation of four coronal mass ejections (CMEs) originating from homologous blowout jets. All of the blowout jets originated from NOAA active region (AR) 11515 on 2012 July 2, within a time interval of $\\approx$14 hr. All of the CMEs were wide (angular widths $\\approx$95$-$150$^\\circ$), and propagated with speeds ranging between $\\approx$300$-$500 km s$^{-1}$ in LASCO coronagraph images. Observations at various EUV wavelengths in Solar Dynamics Observatory/Atmospheric Imaging Assembly images reveal that in all the cases, the source region of the jets lies at the boundary of the leading part of AR 11515 that hosts a small filament before each event. Coronal magnetic field modeling based on nonlinear force free extrapolations indicate that in each case the filament is contained inside of a magnetic flux rope that remains constrained by overlying compact loops. The southern footpoint of each filament is rooted in the negative polarity region where the eruption onsets occur. This negative-polarity region undergoes continuous flux changes, including emergence and cancellation with opposite polarity in the vicinity of the flux rope, and the EUV images reveal brightening episodes near the filament's southeastern footpoint before each eruption. Therefore, these flux changes are likely the cause of the subsequent eruptions. These four homologous eruptions originate near adjacent feet of two large-scale loop systems connecting from that positive-polarity part of the AR to two remote negative-polarity regions, and result in large-scale consequences in the solar corona.","sentences":["We study the formation of four coronal mass ejections (CMEs) originating from homologous blowout jets.","All of the blowout jets originated from NOAA active region (AR) 11515 on 2012 July 2, within a time interval of $\\approx$14 hr.","All of the CMEs were wide (angular widths $\\approx$95$-$150$^\\circ$), and propagated with speeds ranging between $\\approx$300$-$500 km s$^{-1}$ in LASCO coronagraph images.","Observations at various EUV wavelengths in Solar Dynamics Observatory/Atmospheric Imaging Assembly images reveal that in all the cases, the source region of the jets lies at the boundary of the leading part of AR 11515 that hosts a small filament before each event.","Coronal magnetic field modeling based on nonlinear force free extrapolations indicate that in each case the filament is contained inside of a magnetic flux rope that remains constrained by overlying compact loops.","The southern footpoint of each filament is rooted in the negative polarity region where the eruption onsets occur.","This negative-polarity region undergoes continuous flux changes, including emergence and cancellation with opposite polarity in the vicinity of the flux rope, and the EUV images reveal brightening episodes near the filament's southeastern footpoint before each eruption.","Therefore, these flux changes are likely the cause of the subsequent eruptions.","These four homologous eruptions originate near adjacent feet of two large-scale loop systems connecting from that positive-polarity part of the AR to two remote negative-polarity regions, and result in large-scale consequences in the solar corona."],"url":"http://arxiv.org/abs/2405.03292v1","category":"astro-ph.SR"}
{"created":"2024-05-06 09:06:41","title":"Coordinating Cooperative Perception in Urban Air Mobility for Enhanced Environmental Awareness","abstract":"The trend for Urban Air Mobility (UAM) is growing with prospective air taxis, parcel deliverers, and medical and industrial services. Safe and efficient UAM operation relies on timely communication and reliable data exchange. In this paper, we explore Cooperative Perception (CP) for Unmanned Aircraft Systems (UAS), considering the unique communication needs involving high dynamics and a large number of UAS. We propose a hybrid approach combining local broadcast with a central CP service, inspired by centrally managed U-space and broadcast mechanisms from automotive and aviation domains. In a simulation study, we show that our approach significantly enhances the environmental awareness for UAS compared to fully distributed approaches, with an increased communication channel load, which we also evaluate. These findings prompt a discussion on communication strategies for CP in UAM and the potential of a centralized CP service in future research.","sentences":["The trend for Urban Air Mobility (UAM) is growing with prospective air taxis, parcel deliverers, and medical and industrial services.","Safe and efficient UAM operation relies on timely communication and reliable data exchange.","In this paper, we explore Cooperative Perception (CP) for Unmanned Aircraft Systems (UAS), considering the unique communication needs involving high dynamics and a large number of UAS.","We propose a hybrid approach combining local broadcast with a central CP service, inspired by centrally managed U-space and broadcast mechanisms from automotive and aviation domains.","In a simulation study, we show that our approach significantly enhances the environmental awareness for UAS compared to fully distributed approaches, with an increased communication channel load, which we also evaluate.","These findings prompt a discussion on communication strategies for CP in UAM and the potential of a centralized CP service in future research."],"url":"http://arxiv.org/abs/2405.03290v1","category":"cs.NI"}
{"created":"2024-05-06 09:04:40","title":"Study of quantum decoherence at Protvino to ORCA experiment","abstract":"Protvino to ORCA (Oscillation Research with Cosmics in the Abyss) (P2O) is an upcoming neutrino oscillation experiment with a very long baseline of 2595 km. Due to the substantial baseline, this experiment provides a unique opportunity to study the earth matter effects over very large distances. This makes it a suitable experiment to investigate the environmental decoherence in neutrino oscillations, where the neutrino system could interact with a stochastic environment and lead to a loss in the coherence of neutrino states. In this work, we consider an open quantum system framework to simulate the neutrino oscillations in P2O experiment and obtain bounds on the decoherence parameters in different phenomenological models. We assume that the decoherence parameter $\\Gamma$ depends on neutrino energy $E_\\nu$ as $\\Gamma_{ij}(E_\\nu) = \\Gamma_{0} (\\frac{E_\\nu}{E_0})^n$. Further, we use these bounds to study the effect on the neutrino mass ordering sensitivity and CP violation sensitivity of P2O experiment.","sentences":["Protvino to ORCA (Oscillation Research with Cosmics in the Abyss) (P2O) is an upcoming neutrino oscillation experiment with a very long baseline of 2595 km.","Due to the substantial baseline, this experiment provides a unique opportunity to study the earth matter effects over very large distances.","This makes it a suitable experiment to investigate the environmental decoherence in neutrino oscillations, where the neutrino system could interact with a stochastic environment and lead to a loss in the coherence of neutrino states.","In this work, we consider an open quantum system framework to simulate the neutrino oscillations in P2O experiment and obtain bounds on the decoherence parameters in different phenomenological models.","We assume that the decoherence parameter $\\Gamma$ depends on neutrino energy $E_\\nu$ as $\\Gamma_{ij}(E_\\nu) = \\Gamma_{0} (\\frac{E_\\nu}{E_0})^n$. Further, we use these bounds to study the effect on the neutrino mass ordering sensitivity and CP violation sensitivity of P2O experiment."],"url":"http://arxiv.org/abs/2405.03286v1","category":"hep-ph"}
{"created":"2024-05-06 08:49:22","title":"Interactions and cold collisions of AlF in the ground and excited electronic states with He","abstract":"Aluminium monofluoride (AlF) is a promising candidate for laser cooling and the production of dense ultracold molecular gases, thanks to its relatively high chemical stability and diagonal Frank-Condon factors. In this study, we examine the interactions and collisions of AlF in its $X^1\\Sigma^+$, $a^3\\Pi$, and $A^{1}\\Pi$ electronic states with ground-state He using state-of-the-art \\textit{ab initio} quantum chemistry techniques. We construct accurate potential energy surfaces (PESs) employing either the explicitly correlated coupled-cluster CCSD(T)-F12 method augmented by the CCSDT correction or the multireference configuration-interaction method for higher-excited electronic states. Subsequently, we employ these PESs in coupled-channel calculations to determine the scattering cross-sections for AlF+He collisions and bound states of the complex. We estimate the uncertainty of the calculated PESs and apply it to assess the uncertainty of the scattering results. We find a relatively low sensitivity of the cross-sections to the variation of the PESs, but the positions of shape resonances remain uncertain. The present results are relevant for further improvements and optimizations of buffer-gas cooling of AlF molecules.","sentences":["Aluminium monofluoride (AlF) is a promising candidate for laser cooling and the production of dense ultracold molecular gases, thanks to its relatively high chemical stability and diagonal Frank-Condon factors.","In this study, we examine the interactions and collisions of AlF in its $X^1\\Sigma^+$, $a^3\\Pi$, and $A^{1}\\Pi$ electronic states with ground-state He using state-of-the-art \\textit{ab initio} quantum chemistry techniques.","We construct accurate potential energy surfaces (PESs) employing either the explicitly correlated coupled-cluster CCSD(T)-F12 method augmented by the CCSDT correction or the multireference configuration-interaction method for higher-excited electronic states.","Subsequently, we employ these PESs in coupled-channel calculations to determine the scattering cross-sections for AlF+He collisions and bound states of the complex.","We estimate the uncertainty of the calculated PESs and apply it to assess the uncertainty of the scattering results.","We find a relatively low sensitivity of the cross-sections to the variation of the PESs, but the positions of shape resonances remain uncertain.","The present results are relevant for further improvements and optimizations of buffer-gas cooling of AlF molecules."],"url":"http://arxiv.org/abs/2405.03276v1","category":"physics.chem-ph"}
{"created":"2024-05-06 08:44:38","title":"MACE: A Machine learning Approach to Chemistry Emulation","abstract":"The chemistry of an astrophysical environment is closely coupled to its dynamics, the latter often found to be complex. Hence, to properly model these environments a 3D context is necessary. However, solving chemical kinetics within a 3D hydro simulation is computationally infeasible for a even a modest parameter study. In order to develop a feasible 3D hydro-chemical simulation, the classical chemical approach needs to be replaced by a faster alternative. We present mace, a Machine learning Approach to Chemistry Emulation, as a proof-of-concept work on emulating chemistry in a dynamical environment. Using the context of AGB outflows, we have developed an architecture that combines the use of an autoencoder (to reduce the dimensionality of the chemical network) and a set of latent ordinary differential equations (that are solved to perform the temporal evolution of the reduced features). Training this architecture with an integrated scheme makes it possible to successfully reproduce a full chemical pathway in a dynamical environment. mace outperforms its classical analogue on average by a factor 26. Furthermore, its efficient implementation in PyTorch results in a sub-linear scaling with respect to the number of hydrodynamical simulation particles.","sentences":["The chemistry of an astrophysical environment is closely coupled to its dynamics, the latter often found to be complex.","Hence, to properly model these environments a 3D context is necessary.","However, solving chemical kinetics within a 3D hydro simulation is computationally infeasible for a even a modest parameter study.","In order to develop a feasible 3D hydro-chemical simulation, the classical chemical approach needs to be replaced by a faster alternative.","We present mace, a Machine learning Approach to Chemistry Emulation, as a proof-of-concept work on emulating chemistry in a dynamical environment.","Using the context of AGB outflows, we have developed an architecture that combines the use of an autoencoder (to reduce the dimensionality of the chemical network) and a set of latent ordinary differential equations (that are solved to perform the temporal evolution of the reduced features).","Training this architecture with an integrated scheme makes it possible to successfully reproduce a full chemical pathway in a dynamical environment.","mace outperforms its classical analogue on average by a factor 26.","Furthermore, its efficient implementation in PyTorch results in a sub-linear scaling with respect to the number of hydrodynamical simulation particles."],"url":"http://arxiv.org/abs/2405.03274v1","category":"physics.comp-ph"}
{"created":"2024-05-06 08:42:34","title":"WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning","abstract":"Multimodal information, together with our knowledge, help us to understand the complex and dynamic world. Large language models (LLM) and large multimodal models (LMM), however, still struggle to emulate this capability. In this paper, we present WorldQA, a video understanding dataset designed to push the boundaries of multimodal world models with three appealing properties: (1) Multimodal Inputs: The dataset comprises 1007 question-answer pairs and 303 videos, necessitating the analysis of both auditory and visual data for successful interpretation. (2) World Knowledge: We identify five essential types of world knowledge for question formulation. This approach challenges models to extend their capabilities beyond mere perception. (3) Long-Chain Reasoning: Our dataset introduces an average reasoning step of 4.45, notably surpassing other videoQA datasets. Furthermore, we introduce WorldRetriever, an agent designed to synthesize expert knowledge into a coherent reasoning chain, thereby facilitating accurate responses to WorldQA queries. Extensive evaluations of 13 prominent LLMs and LMMs reveal that WorldRetriever, although being the most effective model, achieved only 70% of humanlevel performance in multiple-choice questions. This finding highlights the necessity for further advancement in the reasoning and comprehension abilities of models. Our experiments also yield several key insights. For instance, while humans tend to perform better with increased frames, current LMMs, including WorldRetriever, show diminished performance under similar conditions. We hope that WorldQA,our methodology, and these insights could contribute to the future development of multimodal world models.","sentences":["Multimodal information, together with our knowledge, help us to understand the complex and dynamic world.","Large language models (LLM) and large multimodal models (LMM), however, still struggle to emulate this capability.","In this paper, we present WorldQA, a video understanding dataset designed to push the boundaries of multimodal world models with three appealing properties: (1) Multimodal Inputs: The dataset comprises 1007 question-answer pairs and 303 videos, necessitating the analysis of both auditory and visual data for successful interpretation.","(2) World Knowledge:","We identify five essential types of world knowledge for question formulation.","This approach challenges models to extend their capabilities beyond mere perception.","(3) Long-Chain Reasoning: Our dataset introduces an average reasoning step of 4.45, notably surpassing other videoQA datasets.","Furthermore, we introduce WorldRetriever, an agent designed to synthesize expert knowledge into a coherent reasoning chain, thereby facilitating accurate responses to WorldQA queries.","Extensive evaluations of 13 prominent LLMs and LMMs reveal that WorldRetriever, although being the most effective model, achieved only 70% of humanlevel performance in multiple-choice questions.","This finding highlights the necessity for further advancement in the reasoning and comprehension abilities of models.","Our experiments also yield several key insights.","For instance, while humans tend to perform better with increased frames, current LMMs, including WorldRetriever, show diminished performance under similar conditions.","We hope that WorldQA,our methodology, and these insights could contribute to the future development of multimodal world models."],"url":"http://arxiv.org/abs/2405.03272v1","category":"cs.CV"}
{"created":"2024-05-06 08:38:14","title":"Characterizing the Dilemma of Performance and Index Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory","abstract":"Vector searches on large-scale datasets are critical to modern online services like web search and RAG, which necessity storing the datasets and their index on the secondary storage like SSD. In this paper, we are the first to characterize the trade-off of performance and index size in existing SSD-based graph and cluster indexes: to improve throughput by {5.7\\,$\\times$} and {1.7\\,$\\times$}, these indexes have to pay a {5.8\\,$\\times$} storage amplification and {7.7\\,$\\times$} with respect to the dataset size, respectively. The root cause is that the coarse-grained access of SSD mismatches the fine-grained random read required by vector indexes with small amplification.   This paper argues that second-tier memory, such as remote DRAM/NVM connected via RDMA or CXL, is a powerful storage for addressing the problem from a system's perspective, thanks to its fine-grained access granularity. However, putting existing indexes -- primarily designed for SSD -- directly on second-tier memory cannot fully utilize its power. Meanwhile, second-tier memory still behaves more like storage, so using it as DRAM is also inefficient. To this end, we build a graph and cluster index that centers around the performance features of second-tier memory. With careful execution engine and index layout designs, we show that vector indexes can achieve optimal performance with orders of magnitude smaller index amplification, on a variety of second-tier memory devices.   Based on our improved graph and vector indexes on second-tier memory, we further conduct a systematic study between them to facilitate developers choosing the right index for their workloads. Interestingly, the findings on the second-tier memory contradict the ones on SSDs.","sentences":["Vector searches on large-scale datasets are critical to modern online services like web search and RAG, which necessity storing the datasets and their index on the secondary storage like SSD.","In this paper, we are the first to characterize the trade-off of performance and index size in existing SSD-based graph and cluster indexes: to improve throughput by {5.7\\,$\\times$} and {1.7\\,$\\times$}, these indexes have to pay a {5.8\\,$\\times$} storage amplification and {7.7\\,$\\times$} with respect to the dataset size, respectively.","The root cause is that the coarse-grained access of SSD mismatches the fine-grained random read required by vector indexes with small amplification.   ","This paper argues that second-tier memory, such as remote DRAM/NVM connected via RDMA or CXL, is a powerful storage for addressing the problem from a system's perspective, thanks to its fine-grained access granularity.","However, putting existing indexes -- primarily designed for SSD -- directly on second-tier memory cannot fully utilize its power.","Meanwhile, second-tier memory still behaves more like storage, so using it as DRAM is also inefficient.","To this end, we build a graph and cluster index that centers around the performance features of second-tier memory.","With careful execution engine and index layout designs, we show that vector indexes can achieve optimal performance with orders of magnitude smaller index amplification, on a variety of second-tier memory devices.   ","Based on our improved graph and vector indexes on second-tier memory, we further conduct a systematic study between them to facilitate developers choosing the right index for their workloads.","Interestingly, the findings on the second-tier memory contradict the ones on SSDs."],"url":"http://arxiv.org/abs/2405.03267v1","category":"cs.DC"}
{"created":"2024-05-06 08:36:57","title":"Ladder top-quark condensation imprints in supercooled electroweak phase transition","abstract":"The electroweak (EW) phase transition in the early Universe might be supercooled due to the presence of the classical scale invariance involving Beyond the Standard Model (BSM) sectors and the supercooling could persist down till a later epoch around which the QCD chiral phase transition is supposed to take place. Since this supercooling period keeps masslessness for all the six SM quarks, it has simply been argued that the QCD phase transition is the first order, and so is the EW one. However, not only the QCD coupling but also the top Yukawa and the Higgs quartic couplings get strong at around the QCD scale due to the renormalization group running, hence this scenario is potentially subject to a rigorous nonperturbative analysis. In this work, we employ the ladder Schwinger-Dyson (LSD) analysis based on the Cornwall-Jackiw-Tomboulis formalism at the two-loop level in such a gauge-Higgs-Yukawa system. We show that the chiral broken QCD vacuum emerges with the nonperturbative top condensate and the lightness of all six quarks is guaranteed due to the accidental U(1) axial symmetry presented in the top-Higgs sector. We employ a quark-meson model-like description in the mean field approximation to address the impact on the EW phase transition arising due to the top quark condensation at the QCD phase transition epoch. In the model, the LSD results are encoded to constrain the model parameter space. We then observe the cosmological phase transition of the first-order type and discuss the induced gravitational wave (GW) productions. We find that in addition to the conventional GW signals sourced from an expected BSM at around or over the TeV scale, the dynamical topponium-Higgs system can yield another power spectrum sensitive to the BBO, LISA, and DECIGO, etc.","sentences":["The electroweak (EW) phase transition in the early Universe might be supercooled due to the presence of the classical scale invariance involving Beyond the Standard Model (BSM) sectors and the supercooling could persist down till a later epoch around which the QCD chiral phase transition is supposed to take place.","Since this supercooling period keeps masslessness for all the six SM quarks, it has simply been argued that the QCD phase transition is the first order, and so is the EW one.","However, not only the QCD coupling but also the top Yukawa and the Higgs quartic couplings get strong at around the QCD scale due to the renormalization group running, hence this scenario is potentially subject to a rigorous nonperturbative analysis.","In this work, we employ the ladder Schwinger-Dyson (LSD) analysis based on the Cornwall-Jackiw-Tomboulis formalism at the two-loop level in such a gauge-Higgs-Yukawa system.","We show that the chiral broken QCD vacuum emerges with the nonperturbative top condensate and the lightness of all six quarks is guaranteed due to the accidental U(1) axial symmetry presented in the top-Higgs sector.","We employ a quark-meson model-like description in the mean field approximation to address the impact on the EW phase transition arising due to the top quark condensation at the QCD phase transition epoch.","In the model, the LSD results are encoded to constrain the model parameter space.","We then observe the cosmological phase transition of the first-order type and discuss the induced gravitational wave (GW) productions.","We find that in addition to the conventional GW signals sourced from an expected BSM at around or over the TeV scale, the dynamical topponium-Higgs system can yield another power spectrum sensitive to the BBO, LISA, and DECIGO, etc."],"url":"http://arxiv.org/abs/2405.03265v1","category":"hep-ph"}
{"created":"2024-05-06 08:33:46","title":"A nonlinear criterion for characterizing high-dimensional multipartite entanglement","abstract":"Understanding entanglement of potentially high-dimensional multipartite quantum systems is crucial across different disciplines in quantum sciences. We take inspiration from covariance matrix based techniques to derive a nonlinear criterion that can be used to lower bound the dimensionality vector of mixed quantum states, revealing both the level of multipartiteness and the dimensionality of the entanglement in the quantum states. The technique is based on a system of inequalities that has to be satisfied by all quantum states with a given entanglement dimensionality vector, which can be checked via linear programming. We test our condition on paradigmatic classes of high-dimensional multipartite entangled states like imperfect Greenberger-Horne-Zeilinger (GHZ) states and find that, in comparison with other available criteria our method provides a significant advantage, which is enhanced especially in the case that the dimensions of the individual particles are different from each other.","sentences":["Understanding entanglement of potentially high-dimensional multipartite quantum systems is crucial across different disciplines in quantum sciences.","We take inspiration from covariance matrix based techniques to derive a nonlinear criterion that can be used to lower bound the dimensionality vector of mixed quantum states, revealing both the level of multipartiteness and the dimensionality of the entanglement in the quantum states.","The technique is based on a system of inequalities that has to be satisfied by all quantum states with a given entanglement dimensionality vector, which can be checked via linear programming.","We test our condition on paradigmatic classes of high-dimensional multipartite entangled states like imperfect Greenberger-Horne-Zeilinger (GHZ) states and find that, in comparison with other available criteria our method provides a significant advantage, which is enhanced especially in the case that the dimensions of the individual particles are different from each other."],"url":"http://arxiv.org/abs/2405.03261v1","category":"quant-ph"}
{"created":"2024-05-06 08:24:06","title":"Multi-Modality Spatio-Temporal Forecasting via Self-Supervised Learning","abstract":"Multi-modality spatio-temporal (MoST) data extends spatio-temporal (ST) data by incorporating multiple modalities, which is prevalent in monitoring systems, encompassing diverse traffic demands and air quality assessments. Despite significant strides in ST modeling in recent years, there remains a need to emphasize harnessing the potential of information from different modalities. Robust MoST forecasting is more challenging because it possesses (i) high-dimensional and complex internal structures and (ii) dynamic heterogeneity caused by temporal, spatial, and modality variations. In this study, we propose a novel MoST learning framework via Self-Supervised Learning, namely MoSSL, which aims to uncover latent patterns from temporal, spatial, and modality perspectives while quantifying dynamic heterogeneity. Experiment results on two real-world MoST datasets verify the superiority of our approach compared with the state-of-the-art baselines. Model implementation is available at https://github.com/beginner-sketch/MoSSL.","sentences":["Multi-modality spatio-temporal (MoST) data extends spatio-temporal (ST) data by incorporating multiple modalities, which is prevalent in monitoring systems, encompassing diverse traffic demands and air quality assessments.","Despite significant strides in ST modeling in recent years, there remains a need to emphasize harnessing the potential of information from different modalities.","Robust MoST forecasting is more challenging because it possesses (i) high-dimensional and complex internal structures and (ii) dynamic heterogeneity caused by temporal, spatial, and modality variations.","In this study, we propose a novel MoST learning framework via Self-Supervised Learning, namely MoSSL, which aims to uncover latent patterns from temporal, spatial, and modality perspectives while quantifying dynamic heterogeneity.","Experiment results on two real-world MoST datasets verify the superiority of our approach compared with the state-of-the-art baselines.","Model implementation is available at https://github.com/beginner-sketch/MoSSL."],"url":"http://arxiv.org/abs/2405.03255v1","category":"cs.LG"}
{"created":"2024-05-06 08:17:21","title":"A Universal List Decoding Algorithm with Application to Decoding of Polar Codes","abstract":"This paper is concerned with a guessing codeword decoding (GCD) of linear block codes. Compared with the guessing noise decoding (GND), which is only efficient for high-rate codes, the GCD is efficient for not only high-rate codes but also low-rate codes. We prove that the GCD typically requires a fewer number of queries than the GND. Compared with the ordered statistics decoding (OSD), the GCD does not require the online Gaussian elimination (GE). In addition to limiting the maximum number of searches, we suggest limiting the radius of searches in terms of soft weights or tolerated performance loss to further reduce the decoding complexity, resulting in the so-called truncated GCD. The performance gap between the truncated GCD and the optimal decoding can be upper bounded approximately by the saddlepoint approach or other numerical approaches. The derived upper bound captures the relationship between the performance and the decoding parameters, enabling us to balance the performance and the complexity by optimizing the decoding parameters of the truncated GCD. We also introduce a parallel implementation of the (truncated) GCD algorithm to reduce decoding latency without compromising performance. Another contribution of this paper is the application of the GCD to the polar codes. We propose a multiple-bit-wise decoding algorithm over a pruned tree for the polar codes, referred to as the successive-cancellation list (SCL) decoding algorithm by GCD. First, we present a strategy for pruning the conventional polar decoding tree based on the complexity analysis rather than the specific bit patterns. Then we apply the GCD algorithm in parallel aided by the early stopping criteria to the leaves of the pruned tree. Simulation results show that, without any performance loss as justified by analysis, the proposed decoding algorithm can significantly reduce the decoding latency of the polar codes.","sentences":["This paper is concerned with a guessing codeword decoding (GCD) of linear block codes.","Compared with the guessing noise decoding (GND), which is only efficient for high-rate codes, the GCD is efficient for not only high-rate codes but also low-rate codes.","We prove that the GCD typically requires a fewer number of queries than the GND.","Compared with the ordered statistics decoding (OSD), the GCD does not require the online Gaussian elimination (GE).","In addition to limiting the maximum number of searches, we suggest limiting the radius of searches in terms of soft weights or tolerated performance loss to further reduce the decoding complexity, resulting in the so-called truncated GCD.","The performance gap between the truncated GCD and the optimal decoding can be upper bounded approximately by the saddlepoint approach or other numerical approaches.","The derived upper bound captures the relationship between the performance and the decoding parameters, enabling us to balance the performance and the complexity by optimizing the decoding parameters of the truncated GCD.","We also introduce a parallel implementation of the (truncated) GCD algorithm to reduce decoding latency without compromising performance.","Another contribution of this paper is the application of the GCD to the polar codes.","We propose a multiple-bit-wise decoding algorithm over a pruned tree for the polar codes, referred to as the successive-cancellation list (SCL) decoding algorithm by GCD.","First, we present a strategy for pruning the conventional polar decoding tree based on the complexity analysis rather than the specific bit patterns.","Then we apply the GCD algorithm in parallel aided by the early stopping criteria to the leaves of the pruned tree.","Simulation results show that, without any performance loss as justified by analysis, the proposed decoding algorithm can significantly reduce the decoding latency of the polar codes."],"url":"http://arxiv.org/abs/2405.03252v1","category":"cs.IT"}
{"created":"2024-05-06 08:12:13","title":"A survey to measure cognitive biases influencing mobility choices","abstract":"In this paper, we describe a survey about the perceptions of 4 mobility modes (car, bus, bicycle, walking) and the preferences of users for 6 modal choice factors. This survey has gathered 650 answers in 2023, that are published as open data. In this study, we analyse these results to highlight the influence of 3 cognitive biases on mobility decisions: halo bias, choice-supportive bias, and reactance. These cognitive biases are proposed as plausible explanations of the observed behaviour, where the population tends to stick to individual cars despite urban policies aiming at favouring soft mobility. This model can serve as the basis for a simulator of mobility decisions in a virtual town, and the gathered data can be used to initialise this population with realistic attributes. Work is ongoing to design a simulation-based serious game where the player takes the role of an urban manager faced with planning choices to make their city more sustainable.","sentences":["In this paper, we describe a survey about the perceptions of 4 mobility modes (car, bus, bicycle, walking) and the preferences of users for 6 modal choice factors.","This survey has gathered 650 answers in 2023, that are published as open data.","In this study, we analyse these results to highlight the influence of 3 cognitive biases on mobility decisions: halo bias, choice-supportive bias, and reactance.","These cognitive biases are proposed as plausible explanations of the observed behaviour, where the population tends to stick to individual cars despite urban policies aiming at favouring soft mobility.","This model can serve as the basis for a simulator of mobility decisions in a virtual town, and the gathered data can be used to initialise this population with realistic attributes.","Work is ongoing to design a simulation-based serious game where the player takes the role of an urban manager faced with planning choices to make their city more sustainable."],"url":"http://arxiv.org/abs/2405.03250v1","category":"cs.CY"}
{"created":"2024-05-06 07:59:56","title":"The colliding-wind binary HD 168112","abstract":"Radio surveys of early-type stars have revealed a number of non-thermal emitters. Most of these have been shown to be binaries, where the collision between the two stellar winds is responsible for the non-thermal emission. HD 168112 is a non-thermal radio emitter, whose binary nature has only recently been confirmed spectroscopically. We obtained independent spectroscopic observations to determine its orbit, in addition to radio observations to see if the thermal or non-thermal nature of the emission changes during the periastron passage. We monitored HD 168112 spectroscopically for a 13 year time span. From these data, we determined the orbital parameters, which we compared to the previous results in the literature. From the spectral index of the radio observations, we found how the nature of the emission changes as the system goes through periastron. Combining our results with other literature data allowed us to further constrain the orbital and stellar parameters. We find HD 168112 to have an orbital period of P = 512.17+0.41-0.11 d, an eccentricity of e = 0.7533+0.0053-0.0124, and a mass ratio close to one. From our spectroscopic modelling, we derived the stellar parameters, but we had difficulty arriving at a spectroscopic mass ratio of one. The radio observations around periastron show only thermal emission, suggesting that most of the synchrotron photons are absorbed in the two stellar winds at that phase. Combining our data with the optical interferometry detection, we could constrain the inclination angle to i ~ 63 deg, and the mass of each component to ~ 26 Msun. We have provided an independent spectroscopic confirmation of the binary nature of HD 168112. Although detected as a non-thermal radio emitter, near periastron the radio emission of this highly eccentric system is thermal and is mainly formed in the colliding-wind region. [abridged]","sentences":["Radio surveys of early-type stars have revealed a number of non-thermal emitters.","Most of these have been shown to be binaries, where the collision between the two stellar winds is responsible for the non-thermal emission.","HD 168112 is a non-thermal radio emitter, whose binary nature has only recently been confirmed spectroscopically.","We obtained independent spectroscopic observations to determine its orbit, in addition to radio observations to see if the thermal or non-thermal nature of the emission changes during the periastron passage.","We monitored HD 168112 spectroscopically for a 13 year time span.","From these data, we determined the orbital parameters, which we compared to the previous results in the literature.","From the spectral index of the radio observations, we found how the nature of the emission changes as the system goes through periastron.","Combining our results with other literature data allowed us to further constrain the orbital and stellar parameters.","We find HD 168112 to have an orbital period of P = 512.17+0.41-0.11 d, an eccentricity of e = 0.7533+0.0053-0.0124, and a mass ratio close to one.","From our spectroscopic modelling, we derived the stellar parameters, but we had difficulty arriving at a spectroscopic mass ratio of one.","The radio observations around periastron show only thermal emission, suggesting that most of the synchrotron photons are absorbed in the two stellar winds at that phase.","Combining our data with the optical interferometry detection, we could constrain the inclination angle to i ~ 63 deg, and the mass of each component to ~ 26 Msun.","We have provided an independent spectroscopic confirmation of the binary nature of HD 168112.","Although detected as a non-thermal radio emitter, near periastron the radio emission of this highly eccentric system is thermal and is mainly formed in the colliding-wind region.","[abridged]"],"url":"http://arxiv.org/abs/2405.03247v1","category":"astro-ph.SR"}
{"created":"2024-05-06 07:53:40","title":"How improving performance may imply losing consistency in event-triggered consensus","abstract":"Event-triggered control is often argued to lower the average triggering rate compared to time-triggered control while still achieving a desired control goal, e.g., the same performance level. However, this property, often called consistency, cannot be taken for granted and can be hard to analyze in many settings. In particular, although numerous decentralized event-triggered control schemes have been proposed in the past years, their performance properties with respect to time-triggered control remain mostly unexplored. In this paper, we therefore examine the performance properties of event-triggered control (relative to time-triggered control) for a single-integrator consensus problem with a level-triggering rule. We consider the long-term average quadratic deviation from consensus as a performance measure. For this setting, we show that enriching the information the local controllers use improves the performance of the consensus algorithm but renders a previously consistent event-triggered control scheme inconsistent. In addition, we do so while deploying optimal control inputs which we derive for both information cases and all triggering schemes. With this insight, we can furthermore explain the relationship between two contrasting consistency results from the literature on decentralized event-triggered control. We support our theoretical findings with simulation results.","sentences":["Event-triggered control is often argued to lower the average triggering rate compared to time-triggered control while still achieving a desired control goal, e.g., the same performance level.","However, this property, often called consistency, cannot be taken for granted and can be hard to analyze in many settings.","In particular, although numerous decentralized event-triggered control schemes have been proposed in the past years, their performance properties with respect to time-triggered control remain mostly unexplored.","In this paper, we therefore examine the performance properties of event-triggered control (relative to time-triggered control) for a single-integrator consensus problem with a level-triggering rule.","We consider the long-term average quadratic deviation from consensus as a performance measure.","For this setting, we show that enriching the information the local controllers use improves the performance of the consensus algorithm but renders a previously consistent event-triggered control scheme inconsistent.","In addition, we do so while deploying optimal control inputs which we derive for both information cases and all triggering schemes.","With this insight, we can furthermore explain the relationship between two contrasting consistency results from the literature on decentralized event-triggered control.","We support our theoretical findings with simulation results."],"url":"http://arxiv.org/abs/2405.03245v1","category":"eess.SY"}
{"created":"2024-05-06 07:50:57","title":"Global existence and scattering of small data smooth solutions to a class of quasilinear wave systems on $\\mathbb{R}^2\\times\\mathbb{T}$","abstract":"In this paper, we are concerned with the global existence and scattering of small data smooth solutions to a class of quasilinear wave systems on the product space $\\mathbb{R}^2\\times\\mathbb{T}$. These quasilinear wave systems include 3D irrotational potential flow equation of Chaplygin gases, 3D relativistic membrane equation, some 3D quasilinear wave equations which come from the corresponding Lagrangian functionals as perturbations of the Lagrangian densities of linear waves, and nonlinear wave maps system. Through looking for some suitable transformations of unknown functions, the nonlinear wave system can be reduced into a more tractable form. Subsequently, by applying the vector-field method together with the ghost weight technique as well as deriving some kinds of weighted $L^\\infty-L^\\infty$ and $L^\\infty-L^2$ estimates of solution $w$ to the 2D linear wave equation $\\Box w=f(t,x)$, the global existence and scattering of small data solutions are established.","sentences":["In this paper, we are concerned with the global existence and scattering of small data smooth solutions to a class of quasilinear wave systems on the product space $\\mathbb{R}^2\\times\\mathbb{T}$. These quasilinear wave systems include 3D irrotational potential flow equation of Chaplygin gases, 3D relativistic membrane equation, some 3D quasilinear wave equations which come from the corresponding Lagrangian functionals as perturbations of the Lagrangian densities of linear waves, and nonlinear wave maps system.","Through looking for some suitable transformations of unknown functions, the nonlinear wave system can be reduced into a more tractable form.","Subsequently, by applying the vector-field method together with the ghost weight technique as well as deriving some kinds of weighted $L^\\infty-L^\\infty$ and $L^\\infty-L^2$ estimates of solution $w$ to the 2D linear wave equation $\\Box w=f(t,x)$, the global existence and scattering of small data solutions are established."],"url":"http://arxiv.org/abs/2405.03242v1","category":"math.AP"}
{"created":"2024-05-06 07:47:55","title":"Topological Photonic Structures with Broken Reflection Symmetry","abstract":"In this work, we present a mathematical theory for Dirac points and interface modes in honeycomb topological photonic structures consisting of impenetrable obstacles. Starting from a honeycomb lattice of obstacles attaining $120^\\circ$-rotation symmetry and horizontal reflection symmetry, we apply the boundary integral equation method to show the existence of Dirac points for the first two bands at the vertices of the Brillouin zone. We then study interface modes in a joint honeycomb photonic structure, which consists of two periodic lattices obtained by perturbing the honeycomb one with Dirac points differently. The perturbations break the reflection symmetry of the system, as a result, they annihilate the Dirac points and generate two structures with different topological phases, which mimics the quantum valley Hall effect in topological insulators. We investigate the interface modes that decay exponentially away from the interface of the joint structure in several configurations with different interface geometries, including the zigzag interface, the armchair interface, and the rational interfaces. Using the layer potential technique and asymptotic analysis, we first characterize the band-gap opening for the two perturbed periodic structures and derive the asymptotic expansions of the Bloch modes near the band gap surfaces. By formulating the eigenvalue problem for each joint honeycomb structure using boundary integral equations over the interface and analyzing the characteristic values of the associated boundary integral operators, we prove the existence of interface modes when the perturbation is small.","sentences":["In this work, we present a mathematical theory for Dirac points and interface modes in honeycomb topological photonic structures consisting of impenetrable obstacles.","Starting from a honeycomb lattice of obstacles attaining $120^\\circ$-rotation symmetry and horizontal reflection symmetry, we apply the boundary integral equation method to show the existence of Dirac points for the first two bands at the vertices of the Brillouin zone.","We then study interface modes in a joint honeycomb photonic structure, which consists of two periodic lattices obtained by perturbing the honeycomb one with Dirac points differently.","The perturbations break the reflection symmetry of the system, as a result, they annihilate the Dirac points and generate two structures with different topological phases, which mimics the quantum valley Hall effect in topological insulators.","We investigate the interface modes that decay exponentially away from the interface of the joint structure in several configurations with different interface geometries, including the zigzag interface, the armchair interface, and the rational interfaces.","Using the layer potential technique and asymptotic analysis, we first characterize the band-gap opening for the two perturbed periodic structures and derive the asymptotic expansions of the Bloch modes near the band gap surfaces.","By formulating the eigenvalue problem for each joint honeycomb structure using boundary integral equations over the interface and analyzing the characteristic values of the associated boundary integral operators, we prove the existence of interface modes when the perturbation is small."],"url":"http://arxiv.org/abs/2405.03238v1","category":"math-ph"}
{"created":"2024-05-06 07:44:07","title":"A Reliable Framework for Human-in-the-Loop Anomaly Detection in Time Series","abstract":"Time series anomaly detection is a critical machine learning task for numerous applications, such as finance, healthcare, and industrial systems. However, even high-performed models may exhibit potential issues such as biases, leading to unreliable outcomes and misplaced confidence. While model explanation techniques, particularly visual explanations, offer valuable insights to detect such issues by elucidating model attributions of their decision, many limitations still exist -- They are primarily instance-based and not scalable across dataset, and they provide one-directional information from the model to the human side, lacking a mechanism for users to address detected issues. To fulfill these gaps, we introduce HILAD, a novel framework designed to foster a dynamic and bidirectional collaboration between humans and AI for enhancing anomaly detection models in time series. Through our visual interface, HILAD empowers domain experts to detect, interpret, and correct unexpected model behaviors at scale. Our evaluation with two time series datasets and user studies demonstrates the effectiveness of HILAD in fostering a deeper human understanding, immediate corrective actions, and the reliability enhancement of models.","sentences":["Time series anomaly detection is a critical machine learning task for numerous applications, such as finance, healthcare, and industrial systems.","However, even high-performed models may exhibit potential issues such as biases, leading to unreliable outcomes and misplaced confidence.","While model explanation techniques, particularly visual explanations, offer valuable insights to detect such issues by elucidating model attributions of their decision, many limitations still exist --","They are primarily instance-based and not scalable across dataset, and they provide one-directional information from the model to the human side, lacking a mechanism for users to address detected issues.","To fulfill these gaps, we introduce HILAD, a novel framework designed to foster a dynamic and bidirectional collaboration between humans and AI for enhancing anomaly detection models in time series.","Through our visual interface, HILAD empowers domain experts to detect, interpret, and correct unexpected model behaviors at scale.","Our evaluation with two time series datasets and user studies demonstrates the effectiveness of HILAD in fostering a deeper human understanding, immediate corrective actions, and the reliability enhancement of models."],"url":"http://arxiv.org/abs/2405.03234v1","category":"cs.HC"}
{"created":"2024-05-06 07:42:33","title":"Inertial Relaxed Proximal Linearized ADMM for Nonconvex Optimization under Minimal Continuity Assumption","abstract":"This paper proposes an Inertial Relaxed Proximal Linearized Alternating Direction Method of Multipliers (IRPL-ADMM) for solving general multi-block nonconvex composite optimization problems. Distinguishing itself from existing ADMM-style algorithms, our approach imposes a less stringent condition, specifically requiring continuity in only one block of the objective function. It incorporates an inertial strategy for primal variable updates, and a relaxed strategy for dual variable updates. The fundamental concept underlying our algorithm is based on novel \\textit{regular penalty update rules}, ensuring that the penalty increases but not excessively fast. We devise a novel potential function to facilitate our convergence analysis and extend our methods from deterministic optimization problems to finite-sum stochastic settings. We establish the iteration complexity for both scenarios for achieving an approximate stationary solution. Under the Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point convergence results for the IRPL-ADMM algorithm. Finally, some experiments have been conducted on two machine learning tasks to show the effectiveness of our approaches.","sentences":["This paper proposes an Inertial Relaxed Proximal Linearized Alternating Direction Method of Multipliers (IRPL-ADMM) for solving general multi-block nonconvex composite optimization problems.","Distinguishing itself from existing ADMM-style algorithms, our approach imposes a less stringent condition, specifically requiring continuity in only one block of the objective function.","It incorporates an inertial strategy for primal variable updates, and a relaxed strategy for dual variable updates.","The fundamental concept underlying our algorithm is based on novel \\textit{regular penalty update rules}, ensuring that the penalty increases but not excessively fast.","We devise a novel potential function to facilitate our convergence analysis and extend our methods from deterministic optimization problems to finite-sum stochastic settings.","We establish the iteration complexity for both scenarios for achieving an approximate stationary solution.","Under the Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point convergence results for the IRPL-ADMM algorithm.","Finally, some experiments have been conducted on two machine learning tasks to show the effectiveness of our approaches."],"url":"http://arxiv.org/abs/2405.03233v1","category":"math.OC"}
{"created":"2024-05-06 07:42:16","title":"Successive Interference Cancellation for Optical Fiber Using Discrete Constellations","abstract":"Successive interference cancellation is used to detect discrete modulation symbols transmitted over a 1000 km fiber-optic link. A transmitter and receiver are presented that have linear complexity in the number of transmitted symbols and achieve the information rates of previous studies that use continuous modulations.","sentences":["Successive interference cancellation is used to detect discrete modulation symbols transmitted over a 1000 km fiber-optic link.","A transmitter and receiver are presented that have linear complexity in the number of transmitted symbols and achieve the information rates of previous studies that use continuous modulations."],"url":"http://arxiv.org/abs/2405.03232v1","category":"cs.IT"}
{"created":"2024-05-06 07:41:31","title":"Slicing for Dense Smart Factory Network: Current State, Scenarios, Challenges and Expectations","abstract":"In the era of Industry 4.0, smart factories have emerged as a paradigm shift, redefining manufacturing with the integration of advanced digital technologies. Central to this transformation is the deployment of 5G networks, offering unprecedented levels of connectivity, speed, reliability, and ultra-low latency. Among the revolutionary features of 5G is network slicing, a technology that offers enhanced capabilities through the customization of network resources by allowing multiple logical networks (or slices) to run on top of a shared physical infrastructure. This capability is particularly crucial in the densely packed and highly dynamic environment of smart factories, where diverse applications - from robotic automation to real-time analytics - demand varying network requirements. In this paper, we present a comprehensive overview of the integration of slicing in smart factory networks, emphasizing its critical role in enhancing operational efficiency and supporting the diverse requirements of future manufacturing processes. We elaborate on the recent advances, and technical scenarios, including indoor factory propagation conditions, traffic characteristics, system requirements, slice-aware radio resource management, network elements, enabling technologies and current standardisation efforts. Additionally, we identify open research challenges as well as key technical issues stifling deployments. Finally, we speculate on the future trajectory of slicing-enabled smart factories, emphasizing the need for continuous adaptation to emerging technologies.","sentences":["In the era of Industry 4.0, smart factories have emerged as a paradigm shift, redefining manufacturing with the integration of advanced digital technologies.","Central to this transformation is the deployment of 5G networks, offering unprecedented levels of connectivity, speed, reliability, and ultra-low latency.","Among the revolutionary features of 5G is network slicing, a technology that offers enhanced capabilities through the customization of network resources by allowing multiple logical networks (or slices) to run on top of a shared physical infrastructure.","This capability is particularly crucial in the densely packed and highly dynamic environment of smart factories, where diverse applications - from robotic automation to real-time analytics - demand varying network requirements.","In this paper, we present a comprehensive overview of the integration of slicing in smart factory networks, emphasizing its critical role in enhancing operational efficiency and supporting the diverse requirements of future manufacturing processes.","We elaborate on the recent advances, and technical scenarios, including indoor factory propagation conditions, traffic characteristics, system requirements, slice-aware radio resource management, network elements, enabling technologies and current standardisation efforts.","Additionally, we identify open research challenges as well as key technical issues stifling deployments.","Finally, we speculate on the future trajectory of slicing-enabled smart factories, emphasizing the need for continuous adaptation to emerging technologies."],"url":"http://arxiv.org/abs/2405.03230v1","category":"eess.SP"}
{"created":"2024-05-06 07:39:44","title":"Symmetry and Dynamical Analysis of a Discrete Time Model: The Higher Order Berverton-Holt Equation","abstract":"In this paper, we study the higher-order Beverton-Holt equation. We derive nontrivial symmetries, and thereafter, solutions are obtained. For constant rate and carrying capacity, we study the periodic nature of the solution and analyze the stability of the equilibrium points have been analyzed.","sentences":["In this paper, we study the higher-order Beverton-Holt equation.","We derive nontrivial symmetries, and thereafter, solutions are obtained.","For constant rate and carrying capacity, we study the periodic nature of the solution and analyze the stability of the equilibrium points have been analyzed."],"url":"http://arxiv.org/abs/2405.03227v1","category":"math.DS"}
{"created":"2024-05-06 07:34:40","title":"Dynamics of spatial phase coherence in a dissipative Bose-Hubbard atomic system","abstract":"We investigate the loss of spatial coherence of one-dimensional bosonic gases in optical lattices illuminated by a near-resonant excitation laser. Because the atoms recoil in a random direction after each spontaneous emission, the atomic momentum distribution progressively broadens. Equivalently, the spatial correlation function (the Fourier-conjugate quantity of the momentum distribution) progressively narrows down as more photons are scattered. Here we measure the correlation function of the matter field for fixed distances corresponding to nearest-neighbor (n-n) and next-nearest-neighbor (n-n-n) sites of the optical lattice as a function of time, hereafter called n-n and n-n-n correlators. For strongly interacting lattice gases, we find that the n-n correlator $C_1$ decays as a power-law at long times, $C_1\\propto 1/t^{\\alpha}$, in stark contrast with the exponential decay expected for independent particles. The power-law decay reflects a non-trivial dissipative many-body dynamics, where interactions change drastically the interplay between fluorescence destroying spatial coherence, and coherent tunnelling between neighboring sites restoring spatial coherence at short distances. The observed decay exponent $\\alpha \\approx 0.54(6) $ is in good agreement with the prediction $\\alpha=1/2$ from a dissipative Bose-Hubbard model accounting for the fluorescence-induced decoherence. Furthermore, we find that the n-n correlator $C_1$ controls the n-n-n correlator $C_2$ through the relation $C_2 \\approx C_1^2$, also in accordance with the dissipative Bose-Hubbard model.","sentences":["We investigate the loss of spatial coherence of one-dimensional bosonic gases in optical lattices illuminated by a near-resonant excitation laser.","Because the atoms recoil in a random direction after each spontaneous emission, the atomic momentum distribution progressively broadens.","Equivalently, the spatial correlation function (the Fourier-conjugate quantity of the momentum distribution) progressively narrows down as more photons are scattered.","Here we measure the correlation function of the matter field for fixed distances corresponding to nearest-neighbor (n-n) and next-nearest-neighbor (n-n-n) sites of the optical lattice as a function of time, hereafter called n-n and n-n-n correlators.","For strongly interacting lattice gases, we find that the n-n correlator $C_1$ decays as a power-law at long times, $C_1\\propto 1/t^{\\alpha}$, in stark contrast with the exponential decay expected for independent particles.","The power-law decay reflects a non-trivial dissipative many-body dynamics, where interactions change drastically the interplay between fluorescence destroying spatial coherence, and coherent tunnelling between neighboring sites restoring spatial coherence at short distances.","The observed decay exponent $\\alpha \\approx 0.54(6) $ is in good agreement with the prediction $\\alpha=1/2$ from a dissipative Bose-Hubbard model accounting for the fluorescence-induced decoherence.","Furthermore, we find that the n-n correlator $C_1$ controls the n-n-n correlator $C_2$ through the relation $C_2 \\approx C_1^2$, also in accordance with the dissipative Bose-Hubbard model."],"url":"http://arxiv.org/abs/2405.03226v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 07:31:41","title":"Computational Efficient Width-Wise Early Exits in Modulation Classification","abstract":"Deep learning (DL) techniques are increasingly pervasive across various domains, including wireless communication, where they extract insights from raw radio signals. However, the computational demands of DL pose significant challenges, particularly in distributed wireless networks like Cell-free networks, where deploying DL models on edge devices becomes hard due to heightened computational loads. These computational loads escalate with larger input sizes, often correlating with improved model performance. To mitigate this challenge, Early Exiting (EE) techniques have been introduced in DL, primarily targeting the depth of the model. This approach enables models to exit during inference based on specified criteria, leveraging entropy measures at intermediate exits. Doing so makes less complex samples exit early, reducing computational load and inference time. In our contribution, we propose a novel width-wise exiting strategy for Convolutional Neural Network (CNN)-based architectures. By selectively adjusting the input size, we aim to regulate computational demands effectively. Our approach aims to decrease the average computational load during inference while maintaining performance levels comparable to conventional models. We specifically investigate Modulation Classification, a well-established application of DL in wireless communication. Our experimental results show substantial reductions in computational load, with an average decrease of 28%, and particularly notable reductions of 65% in high-SNR scenarios. Through this work, we present a practical solution for reducing computational demands in deep learning applications, particularly within the domain of wireless communication.","sentences":["Deep learning (DL) techniques are increasingly pervasive across various domains, including wireless communication, where they extract insights from raw radio signals.","However, the computational demands of DL pose significant challenges, particularly in distributed wireless networks like Cell-free networks, where deploying DL models on edge devices becomes hard due to heightened computational loads.","These computational loads escalate with larger input sizes, often correlating with improved model performance.","To mitigate this challenge, Early Exiting (EE) techniques have been introduced in DL, primarily targeting the depth of the model.","This approach enables models to exit during inference based on specified criteria, leveraging entropy measures at intermediate exits.","Doing so makes less complex samples exit early, reducing computational load and inference time.","In our contribution, we propose a novel width-wise exiting strategy for Convolutional Neural Network (CNN)-based architectures.","By selectively adjusting the input size, we aim to regulate computational demands effectively.","Our approach aims to decrease the average computational load during inference while maintaining performance levels comparable to conventional models.","We specifically investigate Modulation Classification, a well-established application of DL in wireless communication.","Our experimental results show substantial reductions in computational load, with an average decrease of 28%, and particularly notable reductions of 65% in high-SNR scenarios.","Through this work, we present a practical solution for reducing computational demands in deep learning applications, particularly within the domain of wireless communication."],"url":"http://arxiv.org/abs/2405.03222v1","category":"eess.SP"}
{"created":"2024-05-06 07:28:55","title":"General Procedure to Provide High-Probability Guarantees for Stochastic Saddle Point Problems","abstract":"This paper considers smooth strongly convex and strongly concave (SC-SC) stochastic saddle point (SSP) problems. Suppose there is an arbitrary oracle that in expectation returns an $\\epsilon$-solution in the sense of certain gaps, which can be the duality gap or its weaker variants. We propose a general PB-SSP framework to guarantee an $\\epsilon$ small duality gap solution with high probability via only $\\mathcal{O}\\big(\\log \\frac{1}{p}\\cdot\\text{poly}(\\log \\kappa)\\big)$ calls of this oracle, where $p\\in(0,1)$ is the confidence level and $\\kappa$ is the condition number. When applied to the sample average approximation (SAA) oracle, in addition to equipping the solution with high probability, our approach even improves the sample complexity by a factor of $\\text{poly}(\\kappa)$, since the high-probability argument enables us to circumvent some key difficulties of the uniform stability analysis of SAA.","sentences":["This paper considers smooth strongly convex and strongly concave (SC-SC) stochastic saddle point (SSP) problems.","Suppose there is an arbitrary oracle that in expectation returns an $\\epsilon$-solution in the sense of certain gaps, which can be the duality gap or its weaker variants.","We propose a general PB-SSP framework to guarantee an $\\epsilon$ small duality gap solution with high probability via only $\\mathcal{O}\\big(\\log \\frac{1}{p}\\cdot\\text{poly}(\\log \\kappa)\\big)$ calls of this oracle, where $p\\in(0,1)$ is the confidence level and $\\kappa$ is the condition number.","When applied to the sample average approximation (SAA) oracle, in addition to equipping the solution with high probability, our approach even improves the sample complexity by a factor of $\\text{poly}(\\kappa)$, since the high-probability argument enables us to circumvent some key difficulties of the uniform stability analysis of SAA."],"url":"http://arxiv.org/abs/2405.03219v1","category":"math.OC"}
{"created":"2024-05-06 07:27:30","title":"Elevator, Escalator or Neither? Classifying Pedestrian Conveyor State Using Inertial Navigation System","abstract":"Classifying a pedestrian in one of the three conveyor states of \"elevator,\" \"escalator\" and \"neither\" is fundamental to many applications such as indoor localization and people flow analysis. We estimate, for the first time, the pedestrian conveyor state given the inertial navigation system (INS) readings of accelerometer, gyroscope and magnetometer sampled from the phone. Our problem is challenging because the INS signals of the conveyor state are coupled and perturbed by unpredictable arbitrary human actions, confusing the decision process. We propose ELESON, a novel, effective and lightweight INS-based deep learning approach to classify whether a pedestrian is in an elevator, escalator or neither. ELESON utilizes a motion feature extractor to decouple the conveyor state from human action in the feature space, and a magnetic feature extractor to account for the speed difference between elevator and escalator. Given the results of the extractors, it employs an evidential state classifier to estimate the confidence of the pedestrian states. Based on extensive experiments conducted on twenty hours of real pedestrian data, we demonstrate that ELESON outperforms significantly the state-of-the-art approaches (where combined INS signals of both the conveyor state and human actions are processed together), with 15% classification improvement in F1 score, stronger confidence discriminability with 10% increase in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements on smartphones.","sentences":["Classifying a pedestrian in one of the three conveyor states of \"elevator,\" \"escalator\" and \"neither\" is fundamental to many applications such as indoor localization and people flow analysis.","We estimate, for the first time, the pedestrian conveyor state given the inertial navigation system (INS) readings of accelerometer, gyroscope and magnetometer sampled from the phone.","Our problem is challenging because the INS signals of the conveyor state are coupled and perturbed by unpredictable arbitrary human actions, confusing the decision process.","We propose ELESON, a novel, effective and lightweight INS-based deep learning approach to classify whether a pedestrian is in an elevator, escalator or neither.","ELESON utilizes a motion feature extractor to decouple the conveyor state from human action in the feature space, and a magnetic feature extractor to account for the speed difference between elevator and escalator.","Given the results of the extractors, it employs an evidential state classifier to estimate the confidence of the pedestrian states.","Based on extensive experiments conducted on twenty hours of real pedestrian data, we demonstrate that ELESON outperforms significantly the state-of-the-art approaches (where combined INS signals of both the conveyor state and human actions are processed together), with 15% classification improvement in F1 score, stronger confidence discriminability with 10% increase in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements on smartphones."],"url":"http://arxiv.org/abs/2405.03218v1","category":"cs.CV"}
{"created":"2024-05-06 07:19:26","title":"On the coincidence of the Hausdorff and box dimensions for some affine-invariant sets","abstract":"Let $ K $ be a compact subset of the $d$-torus invariant under an expanding diagonal endomorphism with $ s $ distinct eigenvalues. Suppose the symbolic coding of $K$ satisfies weak specification. When $ s \\leq 2 $, we prove that the following three statements are equivalent: (A) the Hausdorff and box dimensions of $ K $ coincide; (B) with respect to some gauge function, the Hausdorff measure of $ K $ is positive and finite; (C) the Hausdorff dimension of the measure of maximal entropy on $ K $ attains the Hausdorff dimension of $ K $. When $ s \\geq 3 $, we find some examples in which (A) does not hold but (C) holds, which is a new phenomenon not appearing in the planar cases. Through a different probabilistic approach, we establish the equivalence of (A) and (B) for Bedford-McMullen sponges.","sentences":["Let $ K $ be a compact subset of the $d$-torus invariant under an expanding diagonal endomorphism with $ s $ distinct eigenvalues.","Suppose the symbolic coding of $K$ satisfies weak specification.","When $ s \\leq 2 $, we prove that the following three statements are equivalent: (A) the Hausdorff and box dimensions of $ K $ coincide; (B) with respect to some gauge function, the Hausdorff measure of $ K $ is positive and finite; (C) the Hausdorff dimension of the measure of maximal entropy on $ K $ attains the Hausdorff dimension of $ K $.","When $ s \\geq 3 $, we find some examples in which (A) does not hold but (C) holds, which is a new phenomenon not appearing in the planar cases.","Through a different probabilistic approach, we establish the equivalence of (A) and (B) for Bedford-McMullen sponges."],"url":"http://arxiv.org/abs/2405.03213v1","category":"math.DS"}
{"created":"2024-05-06 07:18:09","title":"On bursty star formation during cosmological reionization - how does it influence the baryon mass content of dark matter halos?","abstract":"The baryon mass content of dark matter halos in the early Universe depends on global factors - e.g. ionising ultraviolet (UV) radiation background - and local factors - e.g. star formation efficiency and assembly history. We use a lightweight semi-analytical model to investigate how local and global factors impact halo baryon mass content at redshifts of $z\\geq 5$. Our model incorporates a time delay between when stars form and when they produce feedback, which drive bursts of star formation, and a mass and redshift dependent UV background, which captures the influence of cosmological reionization on gas accretion onto halos. We use statistically representative halo assembly histories and assume that the cosmological gas accretion rate is proportional to the halo mass accretion rate. Delayed feedback leads to oscillations in gas mass with cosmic time, behaviour that cannot be captured with instantaneous feedback. Highly efficient star formation drives stronger oscillations, while strong feedback impacts when oscillations occur; in contrast, inefficient star formation and weak feedback produce similar long-term behaviour to that observed in instantaneous feedback models. If the delayed feedback timescale is too long, a halo retains its gas reservoir but the feedback suppresses star formation. Our model predicts that lower mass systems ($\\leq 10^7 \\text{M}_\\odot$) at $z \\leq 10$ should be strongly gas deficient, whereas higher mass systems retain their gas reservoirs because they are sufficiently massive to continue accreting gas through cosmological reionization. Interestingly, in higher mass halos, the median $m_\\star/(m_\\star+m_\\text{g}) \\simeq 0.01-0.05$, but is a factor of 3-5 smaller when feedback is delayed. Our model does not include seed supermassive black hole feedback, which is necessary to explain massive quenched galaxies in the early Universe.","sentences":["The baryon mass content of dark matter halos in the early Universe depends on global factors - e.g. ionising ultraviolet (UV) radiation background - and local factors - e.g. star formation efficiency and assembly history.","We use a lightweight semi-analytical model to investigate how local and global factors impact halo baryon mass content at redshifts of $z\\geq 5$. Our model incorporates a time delay between when stars form and when they produce feedback, which drive bursts of star formation, and a mass and redshift dependent UV background, which captures the influence of cosmological reionization on gas accretion onto halos.","We use statistically representative halo assembly histories and assume that the cosmological gas accretion rate is proportional to the halo mass accretion rate.","Delayed feedback leads to oscillations in gas mass with cosmic time, behaviour that cannot be captured with instantaneous feedback.","Highly efficient star formation drives stronger oscillations, while strong feedback impacts when oscillations occur; in contrast, inefficient star formation and weak feedback produce similar long-term behaviour to that observed in instantaneous feedback models.","If the delayed feedback timescale is too long, a halo retains its gas reservoir but the feedback suppresses star formation.","Our model predicts that lower mass systems ($\\leq 10^7 \\text{M}_\\odot$) at $z \\leq 10$ should be strongly gas deficient, whereas higher mass systems retain their gas reservoirs because they are sufficiently massive to continue accreting gas through cosmological reionization.","Interestingly, in higher mass halos, the median $m_\\star/(m_\\star+m_\\text{g})","\\simeq 0.01-0.05$, but is a factor of 3-5 smaller when feedback is delayed.","Our model does not include seed supermassive black hole feedback, which is necessary to explain massive quenched galaxies in the early Universe."],"url":"http://arxiv.org/abs/2405.03211v1","category":"astro-ph.GA"}
{"created":"2024-05-06 07:15:17","title":"Evidence for a cyclotron absorption line and spectral transition in EXO 2030+375 during 2021 giant outburst","abstract":"Based on HXMT observations of EXO 2030+375 during its 2021 giant outburst, we report the analysis of pulse variations and the broadband X-ray spectrum, and find the presence of a potential cyclotron resonant scattering feature with the fundamental line at 47 keV from both average spectra and phase-resolved spectroscopy. During the outburst, the source reached an X-ray luminosity of $\\sim 1\\times 10^{38}$ erg /cm/s from 2-105 keV at a distance of 7.1 kpc. The X-ray pulsar at the spin period of 41.27 seconds exhibits complex timing and spectral variations with both energy and luminosity during the outburst. The shapes of the pulses profiles show the single main peak above 20 keV, while appear to exhibit multi-peak patterns in low energy bands, and the transition of pulse profiles from multi-peak to single-peak is observed at $0.8\\times 10^{38}$ erg /cm/s, which suggests the evolution from the subcritical luminosity (pencil-beam dominated) to supercritical luminosity (fan-beam dominated) regimes. A dip structure before the energy of the cyclotron resonant scattering features is found in the pulse fraction-energy relation near the peak luminosity. A detailed analysis of spectral parameters showed that the power-law photon index exhibits three distinct trends as luminosity increases, and these changes also signify a spectral transition from sub-critical to super-critical regimes. The critical luminosity infers the magnetic field of $(4.8-6.0)\\times 10^{12}$ G, which supports the presence of the cyclotron line at 47 keV. A Comptonization model applied for the broad X-ray spectra during the outburst also suggests the surface magnetic field ranging from $(5-9)\\times 10^{12}$ G.","sentences":["Based on HXMT observations of EXO 2030+375 during its 2021 giant outburst, we report the analysis of pulse variations and the broadband X-ray spectrum, and find the presence of a potential cyclotron resonant scattering feature with the fundamental line at 47 keV from both average spectra and phase-resolved spectroscopy.","During the outburst, the source reached an X-ray luminosity of $\\sim 1\\times 10^{38}$ erg /cm/s from 2-105 keV at a distance of 7.1 kpc.","The X-ray pulsar at the spin period of 41.27 seconds exhibits complex timing and spectral variations with both energy and luminosity during the outburst.","The shapes of the pulses profiles show the single main peak above 20 keV, while appear to exhibit multi-peak patterns in low energy bands, and the transition of pulse profiles from multi-peak to single-peak is observed at $0.8\\times 10^{38}$ erg /cm/s, which suggests the evolution from the subcritical luminosity (pencil-beam dominated) to supercritical luminosity (fan-beam dominated) regimes.","A dip structure before the energy of the cyclotron resonant scattering features is found in the pulse fraction-energy relation near the peak luminosity.","A detailed analysis of spectral parameters showed that the power-law photon index exhibits three distinct trends as luminosity increases, and these changes also signify a spectral transition from sub-critical to super-critical regimes.","The critical luminosity infers the magnetic field of $(4.8-6.0)\\times 10^{12}$ G, which supports the presence of the cyclotron line at 47 keV.","A Comptonization model applied for the broad X-ray spectra during the outburst also suggests the surface magnetic field ranging from $(5-9)\\times 10^{12}$ G."],"url":"http://arxiv.org/abs/2405.03209v1","category":"astro-ph.HE"}
{"created":"2024-05-06 07:13:30","title":"A First-Engineering Principles Model for Dynamical Simulation of a Calciner in Cement Production","abstract":"We present an index-1 differential-algebraic equation (DAE) model for dynamic simulation of a calciner in the pyro-section of a cement plant. The model is based on first engineering principles and integrates reactor geometry, thermo-physical properties, transport phenomena, stoichiometry and kinetics, mass and energy balances, and algebraic volume and internal energy equations in a systematic manner. The model can be used for dynamic simulation of the calciner. We also provide simulation results that are qualitatively correct. The calciner model is part of an overall model for dynamical simulation of the pyro-section in a cement plant. This model can be used in design of control and optimization systems to improve the energy efficiency and \\ce{CO2} emission from cement plants.","sentences":["We present an index-1 differential-algebraic equation (DAE) model for dynamic simulation of a calciner in the pyro-section of a cement plant.","The model is based on first engineering principles and integrates reactor geometry, thermo-physical properties, transport phenomena, stoichiometry and kinetics, mass and energy balances, and algebraic volume and internal energy equations in a systematic manner.","The model can be used for dynamic simulation of the calciner.","We also provide simulation results that are qualitatively correct.","The calciner model is part of an overall model for dynamical simulation of the pyro-section in a cement plant.","This model can be used in design of control and optimization systems to improve the energy efficiency and \\ce{CO2} emission from cement plants."],"url":"http://arxiv.org/abs/2405.03208v1","category":"math.DS"}
{"created":"2024-05-06 07:12:45","title":"A Philosophical Introduction to Language Models - Part II: The Way Forward","abstract":"In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues related to interpretability, examining evidence from causal intervention methods about the nature of LLMs' internal representations and computations. We also discuss the implications of multimodal and modular extensions of LLMs, recent debates about whether such systems may meet minimal criteria for consciousness, and concerns about secrecy and reproducibility in LLM research. Finally, we discuss whether LLM-like systems may be relevant to modeling aspects of human cognition, if their architectural characteristics and learning scenario are adequately constrained.","sentences":["In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part.","We focus particularly on issues related to interpretability, examining evidence from causal intervention methods about the nature of LLMs' internal representations and computations.","We also discuss the implications of multimodal and modular extensions of LLMs, recent debates about whether such systems may meet minimal criteria for consciousness, and concerns about secrecy and reproducibility in LLM research.","Finally, we discuss whether LLM-like systems may be relevant to modeling aspects of human cognition, if their architectural characteristics and learning scenario are adequately constrained."],"url":"http://arxiv.org/abs/2405.03207v1","category":"cs.CL"}
{"created":"2024-05-06 07:05:20","title":"Extension groups for the $C^*$-algebras associated with $\u03bb$-graph systems","abstract":"A $\\lambda$-graph system is a labeled Bratteli diagram with certain additional structure, which presents a subshift. The class of the $C^*$-algebras $\\mathcal{O}_{\\frak L}$ associated with the $\\lambda$-graph systems is a generalized class of the class of Cuntz--Krieger algebras. In this paper, we will compute the strong extension groups $\\operatorname{Ext}_{\\operatorname{s}}(\\mathcal{O}_{\\frak L})$ for the $C^*$-algebras associated with $\\lambda$-graph systems ${\\frak L}$ and study their relation with the weak extension group $\\operatorname{Ext}_{\\operatorname{w}}(\\mathcal{O}_{\\frak L})$.","sentences":["A $\\lambda$-graph system is a labeled Bratteli diagram with certain additional structure, which presents a subshift.","The class of the $C^*$-algebras $\\mathcal{O}_{\\frak L}$ associated with the $\\lambda$-graph systems is a generalized class of the class of Cuntz--Krieger algebras.","In this paper, we will compute the strong extension groups $\\operatorname{Ext}_{\\operatorname{s}}(\\mathcal{O}_{\\frak L})$ for the $C^*$-algebras associated with $\\lambda$-graph systems ${\\frak L}$ and study their relation with the weak extension group $\\operatorname{Ext}_{\\operatorname{w}}(\\mathcal{O}_{\\frak L})$."],"url":"http://arxiv.org/abs/2405.03204v1","category":"math.OA"}
{"created":"2024-05-06 06:56:39","title":"Experimental Investigation of Repurposed Kaplan Turbines as Variable-Speed Propellers for Maximizing Frequency Containment Reserve","abstract":"This study explores the practical viability of repurposing aging Kaplan turbines into variable-speed propellers by employing full-size frequency converters. The motivation behind this approach is to improve the provision of \\emph{Frequency Containment Reserve} (FCR) while reducing fatigue in the Kaplan blades servomechanism. We evaluate the performance of these modified Kaplan turbines against the one of another hydro asset composed of the same Kaplan turbine hybridized with a \\emph{Battery Energy Storage System} (BESS). Experiments are conducted on a one-of-its-kind reduced-scale model testing platform. Our findings reveal that Kaplan turbines repurposed as variable-speed propellers exhibit similar dynamic response characteristics compared to the standalone Kaplan operation, with the added benefit of effectively eliminating blade movements. Furthermore, the ability to control the speed increases the hydraulic efficiency for certain operating points. In summary, investment in variable speed technology emerges as a viable alternative to BESS-based hydropower hybridization.","sentences":["This study explores the practical viability of repurposing aging Kaplan turbines into variable-speed propellers by employing full-size frequency converters.","The motivation behind this approach is to improve the provision of \\emph{Frequency Containment Reserve} (FCR) while reducing fatigue in the Kaplan blades servomechanism.","We evaluate the performance of these modified Kaplan turbines against the one of another hydro asset composed of the same Kaplan turbine hybridized with a \\emph{Battery Energy Storage System} (BESS).","Experiments are conducted on a one-of-its-kind reduced-scale model testing platform.","Our findings reveal that Kaplan turbines repurposed as variable-speed propellers exhibit similar dynamic response characteristics compared to the standalone Kaplan operation, with the added benefit of effectively eliminating blade movements.","Furthermore, the ability to control the speed increases the hydraulic efficiency for certain operating points.","In summary, investment in variable speed technology emerges as a viable alternative to BESS-based hydropower hybridization."],"url":"http://arxiv.org/abs/2405.03201v1","category":"eess.SY"}
{"created":"2024-05-06 06:53:52","title":"A Dynamical Simulation Model of a Cement Clinker Rotary Kiln","abstract":"This study provides a systematic description and results of a dynamical simulation model of a rotary kiln for clinker, based on first engineering principles. The model is built upon thermophysical, chemical, and transportation models for both the formation of clinker phases and fuel combustion in the kiln. The model is presented as a 1D model with counter-flow between gas and clinker phases and is demonstrated by a simulation using industrially relevant input. An advantage of the proposed model is that it provides the evolution of the individual compounds for both the fuel and clinker. As such, the model comprises a stepping stone for evaluating the development of process control systems for existing cement plants.","sentences":["This study provides a systematic description and results of a dynamical simulation model of a rotary kiln for clinker, based on first engineering principles.","The model is built upon thermophysical, chemical, and transportation models for both the formation of clinker phases and fuel combustion in the kiln.","The model is presented as a 1D model with counter-flow between gas and clinker phases and is demonstrated by a simulation using industrially relevant input.","An advantage of the proposed model is that it provides the evolution of the individual compounds for both the fuel and clinker.","As such, the model comprises a stepping stone for evaluating the development of process control systems for existing cement plants."],"url":"http://arxiv.org/abs/2405.03200v1","category":"math.DS"}
{"created":"2024-05-06 06:47:44","title":"Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting","abstract":"Deep learning methods have been exerting their strengths in long-term time series forecasting. However, they often struggle to strike a balance between expressive power and computational efficiency. Here, we propose the Coarsened Perceptron Network (CP-Net), a novel architecture that efficiently enhances the predictive capability of MLPs while maintains a linear computational complexity. It utilizes a coarsening strategy as the backbone that leverages two-stage convolution-based sampling blocks. Based purely on convolution, they provide the functionality of extracting short-term semantic and contextual patterns, which is relatively deficient in the global point-wise projection of the MLP layer. With the architectural simplicity and low runtime, our experiments on seven time series forecasting benchmarks demonstrate that CP-Net achieves an improvement of 4.1% compared to the SOTA method. The model further shows effective utilization of the exposed information with a consistent improvement as the look-back window expands.","sentences":["Deep learning methods have been exerting their strengths in long-term time series forecasting.","However, they often struggle to strike a balance between expressive power and computational efficiency.","Here, we propose the Coarsened Perceptron Network (CP-Net), a novel architecture that efficiently enhances the predictive capability of MLPs while maintains a linear computational complexity.","It utilizes a coarsening strategy as the backbone that leverages two-stage convolution-based sampling blocks.","Based purely on convolution, they provide the functionality of extracting short-term semantic and contextual patterns, which is relatively deficient in the global point-wise projection of the MLP layer.","With the architectural simplicity and low runtime, our experiments on seven time series forecasting benchmarks demonstrate that CP-Net achieves an improvement of 4.1% compared to the SOTA method.","The model further shows effective utilization of the exposed information with a consistent improvement as the look-back window expands."],"url":"http://arxiv.org/abs/2405.03199v1","category":"cs.LG"}
{"created":"2024-05-06 06:45:22","title":"Design and Analysis of Massive Uncoupled Unsourced Random Access with Bayesian Joint Decoding","abstract":"In this paper, we investigate unsourced random access for massive machine-type communications (mMTC) in the sixth-generation (6G) wireless networks. Firstly, we establish a high-efficiency uncoupled framework for massive unsourced random access without extra parity check bits. Then, we design a low-complexity Bayesian joint decoding algorithm, including codeword detection and stitching. In particular, we present a Bayesian codeword detection approach by exploiting Bayes-optimal divergence-free orthogonal approximate message passing in the case of unknown priors. The output long-term channel statistic information is well leveraged to stitch codewords for recovering the original message. Thus, the spectral efficiency is improved by avoiding the use of parity bits. Moreover, we analyze the performance of the proposed Bayesian joint decoding-based massive uncoupled unsourced random access scheme in terms of computational complexity and error probability of decoding. Furthermore, by asymptotic analysis, we obtain some useful insights for the design of massive unsourced random access. Finally, extensive simulation results confirm the effectiveness of the proposed scheme in 6G wireless networks.","sentences":["In this paper, we investigate unsourced random access for massive machine-type communications (mMTC) in the sixth-generation (6G) wireless networks.","Firstly, we establish a high-efficiency uncoupled framework for massive unsourced random access without extra parity check bits.","Then, we design a low-complexity Bayesian joint decoding algorithm, including codeword detection and stitching.","In particular, we present a Bayesian codeword detection approach by exploiting Bayes-optimal divergence-free orthogonal approximate message passing in the case of unknown priors.","The output long-term channel statistic information is well leveraged to stitch codewords for recovering the original message.","Thus, the spectral efficiency is improved by avoiding the use of parity bits.","Moreover, we analyze the performance of the proposed Bayesian joint decoding-based massive uncoupled unsourced random access scheme in terms of computational complexity and error probability of decoding.","Furthermore, by asymptotic analysis, we obtain some useful insights for the design of massive unsourced random access.","Finally, extensive simulation results confirm the effectiveness of the proposed scheme in 6G wireless networks."],"url":"http://arxiv.org/abs/2405.03196v1","category":"cs.IT"}
{"created":"2024-05-06 06:31:13","title":"Exploiting Matrix Information Geometry for Integrated Decoding of Massive Uncoupled Unsourced Random Access","abstract":"In this paper, we explore an efficient uncoupled unsourced random access (UURA) scheme for 6G massive communication. UURA is a typical framework of unsourced random access that addresses the problems of codeword detection and message stitching, without the use of check bits. Firstly, we establish a framework for UURA, allowing for immediate decoding of sub-messages upon arrival. Thus, the processing delay is effectively reduced due to the decreasing waiting time. Next, we propose an integrated decoding algorithm for sub-messages by leveraging matrix information geometry (MIG) theory. Specifically, MIG is applied to measure the feature similarities of codewords belonging to the same user equipment, and thus sub-message can be stitched once it is received. This enables the timely recovery of a portion of the original message by simultaneously detecting and stitching codewords within the current sub-slot. Furthermore, we analyze the performance of the proposed integrated decoding-based UURA scheme in terms of computational complexity and convergence rate. Finally, we present extensive simulation results to validate the effectiveness of the proposed scheme in 6G wireless networks.","sentences":["In this paper, we explore an efficient uncoupled unsourced random access (UURA) scheme for 6G massive communication.","UURA is a typical framework of unsourced random access that addresses the problems of codeword detection and message stitching, without the use of check bits.","Firstly, we establish a framework for UURA, allowing for immediate decoding of sub-messages upon arrival.","Thus, the processing delay is effectively reduced due to the decreasing waiting time.","Next, we propose an integrated decoding algorithm for sub-messages by leveraging matrix information geometry (MIG) theory.","Specifically, MIG is applied to measure the feature similarities of codewords belonging to the same user equipment, and thus sub-message can be stitched once it is received.","This enables the timely recovery of a portion of the original message by simultaneously detecting and stitching codewords within the current sub-slot.","Furthermore, we analyze the performance of the proposed integrated decoding-based UURA scheme in terms of computational complexity and convergence rate.","Finally, we present extensive simulation results to validate the effectiveness of the proposed scheme in 6G wireless networks."],"url":"http://arxiv.org/abs/2405.03191v1","category":"cs.IT"}
{"created":"2024-05-06 06:29:32","title":"Coarsening in Bent-core Liquid Crystals: Intermediate Splay Bend State en route to the Twist Bend Phase","abstract":"We use molecular dynamics simulations to study coarsening dynamics in achiral banana-shaped bent-core liquid crystals following a quench from the high concentration polar smectic (SmX) phase to lower concentrations that favor the exotic twist-bend (TB) phase. Our novel result is the identification of an intermediate splay-bend state emerging prior to the eventual TB phase. The latter coarsens via the annihilation of {\\it beta lines} which are analogous to string defects in nematic liquid crystals. Our findings are relevant for a large class of chiral systems assembled from achiral entities.","sentences":["We use molecular dynamics simulations to study coarsening dynamics in achiral banana-shaped bent-core liquid crystals following a quench from the high concentration polar smectic (SmX) phase to lower concentrations that favor the exotic twist-bend (TB) phase.","Our novel result is the identification of an intermediate splay-bend state emerging prior to the eventual TB phase.","The latter coarsens via the annihilation of {\\it beta lines} which are analogous to string defects in nematic liquid crystals.","Our findings are relevant for a large class of chiral systems assembled from achiral entities."],"url":"http://arxiv.org/abs/2405.03189v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-06 06:28:44","title":"Hyperbolic Geometric Latent Diffusion Model for Graph Generation","abstract":"Diffusion models have made significant contributions to computer vision, sparking a growing interest in the community recently regarding the application of them to graph generation. Existing discrete graph diffusion models exhibit heightened computational complexity and diminished training efficiency. A preferable and natural way is to directly diffuse the graph within the latent space. However, due to the non-Euclidean structure of graphs is not isotropic in the latent space, the existing latent diffusion models effectively make it difficult to capture and preserve the topological information of graphs. To address the above challenges, we propose a novel geometrically latent diffusion framework HypDiff. Specifically, we first establish a geometrically latent space with interpretability measures based on hyperbolic geometry, to define anisotropic latent diffusion processes for graphs. Then, we propose a geometrically latent diffusion process that is constrained by both radial and angular geometric properties, thereby ensuring the preservation of the original topological properties in the generative graphs. Extensive experimental results demonstrate the superior effectiveness of HypDiff for graph generation with various topologies.","sentences":["Diffusion models have made significant contributions to computer vision, sparking a growing interest in the community recently regarding the application of them to graph generation.","Existing discrete graph diffusion models exhibit heightened computational complexity and diminished training efficiency.","A preferable and natural way is to directly diffuse the graph within the latent space.","However, due to the non-Euclidean structure of graphs is not isotropic in the latent space, the existing latent diffusion models effectively make it difficult to capture and preserve the topological information of graphs.","To address the above challenges, we propose a novel geometrically latent diffusion framework HypDiff.","Specifically, we first establish a geometrically latent space with interpretability measures based on hyperbolic geometry, to define anisotropic latent diffusion processes for graphs.","Then, we propose a geometrically latent diffusion process that is constrained by both radial and angular geometric properties, thereby ensuring the preservation of the original topological properties in the generative graphs.","Extensive experimental results demonstrate the superior effectiveness of HypDiff for graph generation with various topologies."],"url":"http://arxiv.org/abs/2405.03188v1","category":"cs.LG"}
{"created":"2024-05-06 06:23:06","title":"Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner","abstract":"Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system. Existing methods aim to reconstruct STTD using low-dimensional models. However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations. Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation. To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables. To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes. We further enable modeling in irregular spaces such as sensor graphs using spectral embedding. Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics. It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns. We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales. Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach extends to different data domains, output resolutions, and network topologies. Comprehensive model analyses provide further insight into the inductive bias of STTD. We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks.","sentences":["Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system.","Existing methods aim to reconstruct STTD using low-dimensional models.","However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations.","Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation.","To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables.","To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes.","We further enable modeling in irregular spaces such as sensor graphs using spectral embedding.","Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics.","It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns.","We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales.","Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach extends to different data domains, output resolutions, and network topologies.","Comprehensive model analyses provide further insight into the inductive bias of STTD.","We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks."],"url":"http://arxiv.org/abs/2405.03185v1","category":"cs.LG"}
{"created":"2024-05-06 06:20:54","title":"Kolmogorovian Censorship, Predictive Incompleteness, and the locality loophole in Bell experiments","abstract":"In the foundations of quantum mechanics, the Kolmogorovian Censorship (KC) stipulates that quantum probabilities can be identified with classical, Kolmogorovian probabilities when considering a specified measurement context. Then in any given measurement context it is possible to build a Kolmogorovian probability distribution, or equivalently a hidden variable theory; however this distribution must be matched to the chosen context. In a loophole-free Bell test, the remote random choices of measurements (polarizers orientations) have the purpose to prevent that this matching can be obtained from any relativistically causal transmission between the source and the detectors. Then the matching (required to violate Bell's inequalities) may be obtained either by an instantaneous influence at a distance between the source and the detectors (explicit nonlocality), or by assuming that it is pre-established before the actual experiment takes place (super-determinism). If both influence at a distance and super-determinism are not accepted on physical grounds, a third way is still available, called \"predictive incompleteness\": it tells that the usual quantum state $\\psi$ is incomplete, as long as the measurement context has not been specified. In agreement with the general quantum framework called CSM (Contexts, Systems and Modalities) we argue that predictive incompleteness is the correct quantum way to understand the violation of Bell's inequalities.","sentences":["In the foundations of quantum mechanics, the Kolmogorovian Censorship (KC) stipulates that quantum probabilities can be identified with classical, Kolmogorovian probabilities when considering a specified measurement context.","Then in any given measurement context it is possible to build a Kolmogorovian probability distribution, or equivalently a hidden variable theory; however this distribution must be matched to the chosen context.","In a loophole-free Bell test, the remote random choices of measurements (polarizers orientations) have the purpose to prevent that this matching can be obtained from any relativistically causal transmission between the source and the detectors.","Then the matching (required to violate Bell's inequalities) may be obtained either by an instantaneous influence at a distance between the source and the detectors (explicit nonlocality), or by assuming that it is pre-established before the actual experiment takes place (super-determinism).","If both influence at a distance and super-determinism are not accepted on physical grounds, a third way is still available, called \"predictive incompleteness\": it tells that the usual quantum state $\\psi$ is incomplete, as long as the measurement context has not been specified.","In agreement with the general quantum framework called CSM (Contexts, Systems and Modalities) we argue that predictive incompleteness is the correct quantum way to understand the violation of Bell's inequalities."],"url":"http://arxiv.org/abs/2405.03184v1","category":"quant-ph"}
{"created":"2024-05-06 06:15:35","title":"Reference-free dual-comb spectroscopy with inbuilt coherence","abstract":"We demonstrate a simple system for dual-comb spectroscopy based on two inherently coherent optical frequency combs generated via seeded parametric down-conversion. The inbuilt coherence is established by making the two combs share a common comb line. We show that the inbuilt coherence makes it possible to use a simple numerical post-processing procedure to compensate for small drifts of the dual-comb interferogram arrival-time and phase. This enables long-time coherent averaging of the interferograms.","sentences":["We demonstrate a simple system for dual-comb spectroscopy based on two inherently coherent optical frequency combs generated via seeded parametric down-conversion.","The inbuilt coherence is established by making the two combs share a common comb line.","We show that the inbuilt coherence makes it possible to use a simple numerical post-processing procedure to compensate for small drifts of the dual-comb interferogram arrival-time and phase.","This enables long-time coherent averaging of the interferograms."],"url":"http://arxiv.org/abs/2405.03182v1","category":"physics.optics"}
{"created":"2024-05-06 06:00:37","title":"Solutions of the equation $a_n + (a_{n-1} + \\cdots (a_2 + (a_1 + x^{r_1})^{r_2}\\cdots )^{r_{n}} = b\\, x$","abstract":"We establish a novel upper bound for the real solutions of the equation specified in the title, employing a generalized derivation-division algorithm. As a consequence, we also derive a new set of Chebyshev functions adapted specifically for this problem.","sentences":["We establish a novel upper bound for the real solutions of the equation specified in the title, employing a generalized derivation-division algorithm.","As a consequence, we also derive a new set of Chebyshev functions adapted specifically for this problem."],"url":"http://arxiv.org/abs/2405.03179v1","category":"math.DS"}
{"created":"2024-05-06 05:57:46","title":"FIMP-HGA: A Novel Approach to Addressing the Partitioning Min-Max Weighted Matching Problem","abstract":"The Partitioning Min-Max Weighted Matching (PMMWM) problem, being a practical NP-hard problem, integrates the task of partitioning the vertices of a bipartite graph into disjoint sets of limited size with the classical Maximum-Weight Perfect Matching (MPWM) problem. Initially introduced in 2015, the state-of-the-art method for addressing PMMWM is the MP$_{\\text{LS}}$. In this paper, we present a novel approach, the Fast Iterative Match-Partition Hybrid Genetic Algorithm (FIMP-HGA), for addressing PMMWM. Similar to MP$_{\\text{LS}}$, FIMP-HGA divides the solving into match and partition stages, iteratively refining the solution. In the match stage, we propose the KM-M algorithm, which reduces matching complexity through incremental adjustments, significantly enhancing runtime efficiency. For the partition stage, we introduce a Hybrid Genetic Algorithm (HGA) incorporating an elite strategy and design a Greedy Partition Crossover (GPX) operator alongside a Multilevel Local Search (MLS) to optimize individuals in the population. Population initialization employs various methods, including the multi-way Karmarkar-Karp (KK) algorithm, ensuring both quality and diversity. At each iteration, the bipartite graph is adjusted based on the current solution, aiming for continuous improvement. To conduct comprehensive experiments, we develop a new instance generation method compatible with existing approaches, resulting in four benchmark groups. Extensive experiments evaluate various algorithm modules, accurately assessing each module's impact on improvement. Evaluation results on our benchmarks demonstrate that the proposed FIMP-HGA significantly enhances solution quality compared to MP$_{\\text{LS}}$, meanwhile reducing runtime by 3 to 20 times.","sentences":["The Partitioning Min-Max Weighted Matching (PMMWM) problem, being a practical NP-hard problem, integrates the task of partitioning the vertices of a bipartite graph into disjoint sets of limited size with the classical Maximum-Weight Perfect Matching (MPWM) problem.","Initially introduced in 2015, the state-of-the-art method for addressing PMMWM is the MP$_{\\text{LS}}$. In this paper, we present a novel approach, the Fast Iterative Match-Partition Hybrid Genetic Algorithm (FIMP-HGA), for addressing PMMWM.","Similar to MP$_{\\text{LS}}$, FIMP-HGA divides the solving into match and partition stages, iteratively refining the solution.","In the match stage, we propose the KM-M algorithm, which reduces matching complexity through incremental adjustments, significantly enhancing runtime efficiency.","For the partition stage, we introduce a Hybrid Genetic Algorithm (HGA) incorporating an elite strategy and design a Greedy Partition Crossover (GPX) operator alongside a Multilevel Local Search (MLS) to optimize individuals in the population.","Population initialization employs various methods, including the multi-way Karmarkar-Karp (KK) algorithm, ensuring both quality and diversity.","At each iteration, the bipartite graph is adjusted based on the current solution, aiming for continuous improvement.","To conduct comprehensive experiments, we develop a new instance generation method compatible with existing approaches, resulting in four benchmark groups.","Extensive experiments evaluate various algorithm modules, accurately assessing each module's impact on improvement.","Evaluation results on our benchmarks demonstrate that the proposed FIMP-HGA significantly enhances solution quality compared to MP$_{\\text{LS}}$, meanwhile reducing runtime by 3 to 20 times."],"url":"http://arxiv.org/abs/2405.03176v1","category":"cs.NE"}
{"created":"2024-05-06 05:39:24","title":"Dimensional reduction gauge and low-dimensionalization in four dimensional QCD","abstract":"Motivated by one-dimensional color-electric flux-tube formation in four-dimensional (4D) QCD, we investigate a possibility of low-dimensionalization in 4D QCD. We propose a new gauge fixing of ``dimensional reduction (DR) gauge\" defined so as to minimize $R_{\\mathrm{DR}}~\\equiv~\\int d^{4}s ~ \\mathrm{Tr} \\left[ A_{x}^{2}(s) + A_{y}^{2}(s) \\right]$, which has a residual gauge symmetry for the gauge function $\\Omega (t,z)$ like 2D QCD on the $t$-$z$ plane. We investigate low-dimensionalization in the DR gauge in SU(3) lattice QCD at $\\beta = 6.0$. The amplitude of $A_{x}(s)$ and $A_{y}(s)$ are found to be strongly suppressed in the DR gauge. We consider ``$tz$-projection'' of $A_{x,y}(s) \\to 0$ for the gauge configuration generated in the DR gauge, in a similar sense to Abelian projection in the maximally Abelian gauge. By the $tz$-projection in the DR gauge, the interquark potential is not changed, and $A_{t}(s)$ and $A_{z}(s)$ play a dominant role in quark confinement. In the DR gauge, we calculate a spatial correlation $\\langle \\mathrm{Tr} A_{\\perp}(s) A_{\\perp}(s+ra_{\\perp}) \\rangle ~ (\\perp = x,y)$ and estimate the spatial mass of $A_{\\perp}(s) ~ (\\perp = x,y)$ as $M \\simeq 1.7 ~ \\mathrm{GeV}$. It is conjectured that this large mass makes $A_{\\perp}(s)$ inactive and realizes the dominance of $A_{t}(s)$ and $A_{z}(s)$ in infrared region in the DR gauge. We also calculate the spatial correlation of two temporal link-variables and find that the correlation decreases as $\\exp (-mr)$ with $m \\simeq 0.6 ~ \\mathrm{GeV}$. Using a crude approximation, 4D QCD is reduced into an ensemble of 2D QCD systems with the coupling of $g_{\\rm 2D} = g m$.","sentences":["Motivated by one-dimensional color-electric flux-tube formation in four-dimensional (4D) QCD, we investigate a possibility of low-dimensionalization in 4D QCD.","We propose a new gauge fixing of ``dimensional reduction (DR) gauge\" defined so as to minimize $R_{\\mathrm{DR}}~\\equiv~\\int d^{4}s ~ \\mathrm{Tr} \\left[ A_{x}^{2}(s) + A_{y}^{2}(s) \\right]$, which has a residual gauge symmetry for the gauge function $\\Omega (t,z)$ like 2D QCD on the $t$-$z$ plane.","We investigate low-dimensionalization in the DR gauge in SU(3) lattice QCD at $\\beta = 6.0$. The amplitude of $A_{x}(s)$ and $A_{y}(s)$ are found to be strongly suppressed in the DR gauge.","We consider ``$tz$-projection'' of $A_{x,y}(s) \\to 0$ for the gauge configuration generated in the DR gauge, in a similar sense to Abelian projection in the maximally Abelian gauge.","By the $tz$-projection in the DR gauge, the interquark potential is not changed, and $A_{t}(s)$ and $A_{z}(s)$ play a dominant role in quark confinement.","In the DR gauge, we calculate a spatial correlation $\\langle \\mathrm{Tr} A_{\\perp}(s) A_{\\perp}(s+ra_{\\perp})","\\rangle ~","(\\perp = x,y)$ and estimate the spatial mass of $A_{\\perp}(s) ~","(\\perp = x,y)$ as $M \\simeq 1.7 ~ \\mathrm{GeV}$. It is conjectured that this large mass makes $A_{\\perp}(s)$ inactive and realizes the dominance of $A_{t}(s)$ and $A_{z}(s)$ in infrared region in the DR gauge.","We also calculate the spatial correlation of two temporal link-variables and find that the correlation decreases as $\\exp (-mr)$ with $m \\simeq 0.6 ~","\\mathrm{GeV}$. Using a crude approximation, 4D QCD is reduced into an ensemble of 2D QCD systems with the coupling of $g_{\\rm 2D} = g m$."],"url":"http://arxiv.org/abs/2405.03172v1","category":"hep-lat"}
{"created":"2024-05-06 05:22:58","title":"Fate of Two-Particle Bound States in the Continuum in non-Hermitian Systems","abstract":"We unveil the existence of two-particle bound state in the continuum (BIC) in a one-dimensional interacting nonreciprocal lattice with a generalized boundary condition. By applying the Bethe-ansatz method, we can exactly solve the wavefunction and eigenvalue of the bound state in the continuum band, which enable us to precisely determine the phase diagrams of BIC. Our results demonstrate that the non-reciprocal hopping can delocalize the bound state and thus shrink the region of BIC. By analyzing the wavefunction, we identify the existence of two types of BICs with different spatial distributions and analytically derive the corresponding threshold values for the breakdown of BICs. The BIC with similar properties is also found to exist in another system with an impurity potential.","sentences":["We unveil the existence of two-particle bound state in the continuum (BIC) in a one-dimensional interacting nonreciprocal lattice with a generalized boundary condition.","By applying the Bethe-ansatz method, we can exactly solve the wavefunction and eigenvalue of the bound state in the continuum band, which enable us to precisely determine the phase diagrams of BIC.","Our results demonstrate that the non-reciprocal hopping can delocalize the bound state and thus shrink the region of BIC.","By analyzing the wavefunction, we identify the existence of two types of BICs with different spatial distributions and analytically derive the corresponding threshold values for the breakdown of BICs.","The BIC with similar properties is also found to exist in another system with an impurity potential."],"url":"http://arxiv.org/abs/2405.03168v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 05:22:40","title":"TF4CTR: Twin Focus Framework for CTR Prediction via Adaptive Sample Differentiation","abstract":"Effective feature interaction modeling is critical for enhancing the accuracy of click-through rate (CTR) prediction in industrial recommender systems. Most of the current deep CTR models resort to building complex network architectures to better capture intricate feature interactions or user behaviors. However, we identify two limitations in these models: (1) the samples given to the model are undifferentiated, which may lead the model to learn a larger number of easy samples in a single-minded manner while ignoring a smaller number of hard samples, thus reducing the model's generalization ability; (2) differentiated feature interaction encoders are designed to capture different interactions information but receive consistent supervision signals, thereby limiting the effectiveness of the encoder. To bridge the identified gaps, this paper introduces a novel CTR prediction framework by integrating the plug-and-play Twin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic Fusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR). Specifically, the framework employs the SSEM at the bottom of the model to differentiate between samples, thereby assigning a more suitable encoder for each sample. Meanwhile, the TF Loss provides tailored supervision signals to both simple and complex encoders. Moreover, the DFM dynamically fuses the feature interaction information captured by the encoders, resulting in more accurate predictions. Experiments on five real-world datasets confirm the effectiveness and compatibility of the framework, demonstrating its capacity to enhance various representative baselines in a model-agnostic manner. To facilitate reproducible research, our open-sourced code and detailed running logs will be made available at: https://github.com/salmon1802/TF4CTR.","sentences":["Effective feature interaction modeling is critical for enhancing the accuracy of click-through rate (CTR) prediction in industrial recommender systems.","Most of the current deep CTR models resort to building complex network architectures to better capture intricate feature interactions or user behaviors.","However, we identify two limitations in these models: (1) the samples given to the model are undifferentiated, which may lead the model to learn a larger number of easy samples in a single-minded manner while ignoring a smaller number of hard samples, thus reducing the model's generalization ability; (2) differentiated feature interaction encoders are designed to capture different interactions information but receive consistent supervision signals, thereby limiting the effectiveness of the encoder.","To bridge the identified gaps, this paper introduces a novel CTR prediction framework by integrating the plug-and-play Twin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic Fusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR).","Specifically, the framework employs the SSEM at the bottom of the model to differentiate between samples, thereby assigning a more suitable encoder for each sample.","Meanwhile, the TF Loss provides tailored supervision signals to both simple and complex encoders.","Moreover, the DFM dynamically fuses the feature interaction information captured by the encoders, resulting in more accurate predictions.","Experiments on five real-world datasets confirm the effectiveness and compatibility of the framework, demonstrating its capacity to enhance various representative baselines in a model-agnostic manner.","To facilitate reproducible research, our open-sourced code and detailed running logs will be made available at: https://github.com/salmon1802/TF4CTR."],"url":"http://arxiv.org/abs/2405.03167v1","category":"cs.IR"}
{"created":"2024-05-06 05:16:43","title":"An Efficient All-to-All GCD Algorithm for Low Entropy RSA Key Factorization","abstract":"RSA is an incredibly successful and useful asymmetric encryption algorithm. One of the types of implementation flaws in RSA is low entropy of the key generation, specifically the prime number creation stage. This can occur due to flawed usage of random prime number generator libraries, or on computers where there is a lack of a source of external entropy. These implementation flaws result in some RSA keys sharing prime factors, which means that the full factorization of the public modulus can be recovered incredibly efficiently by performing a computation GCD between the two public key moduli that share the prime factor. However, since one does not know which of the composite moduli share a prime factor a-priori, to determine if any such shared prime factors exist, an all-to-all GCD attack (also known as a batch GCD attack, or a bulk GCD attack) can be performed on the available public keys so as to recover any shared prime factors. This study describes a novel all-to-all batch GCD algorithm, which will be referred to as the binary tree batch GCD algorithm, that is more efficient than the current best batch GCD algorithm (the remainder tree batch GCD algorithm). A comparison against the best existing batch GCD method (which is a product tree followed by a remainder tree computation) is given using a dataset of random RSA moduli that are constructed such that some of the moduli share prime factors. This proposed binary tree batch GCD algorithm has better runtime than the existing remainder tree batch GCD algorithm, although asymptotically it has nearly identical scaling and its complexity is dependent on how many shared prime factors exist in the set of RSA keys. In practice, the implementation of the proposed binary tree batch GCD algorithm has a roughly 6x speedup compared to the standard remainder tree batch GCD approach.","sentences":["RSA is an incredibly successful and useful asymmetric encryption algorithm.","One of the types of implementation flaws in RSA is low entropy of the key generation, specifically the prime number creation stage.","This can occur due to flawed usage of random prime number generator libraries, or on computers where there is a lack of a source of external entropy.","These implementation flaws result in some RSA keys sharing prime factors, which means that the full factorization of the public modulus can be recovered incredibly efficiently by performing a computation GCD between the two public key moduli that share the prime factor.","However, since one does not know which of the composite moduli share a prime factor a-priori, to determine if any such shared prime factors exist, an all-to-all GCD attack (also known as a batch GCD attack, or a bulk GCD attack) can be performed on the available public keys so as to recover any shared prime factors.","This study describes a novel all-to-all batch GCD algorithm, which will be referred to as the binary tree batch GCD algorithm, that is more efficient than the current best batch GCD algorithm (the remainder tree batch GCD algorithm).","A comparison against the best existing batch GCD method (which is a product tree followed by a remainder tree computation) is given using a dataset of random RSA moduli that are constructed such that some of the moduli share prime factors.","This proposed binary tree batch GCD algorithm has better runtime than the existing remainder tree batch GCD algorithm, although asymptotically it has nearly identical scaling and its complexity is dependent on how many shared prime factors exist in the set of RSA keys.","In practice, the implementation of the proposed binary tree batch GCD algorithm has a roughly 6x speedup compared to the standard remainder tree batch GCD approach."],"url":"http://arxiv.org/abs/2405.03166v1","category":"cs.CR"}
{"created":"2024-05-06 04:55:55","title":"Magnetic Ordering of Ammonium Cations in NH$_4$I, NH$_4$Br and NH$_4$Cl","abstract":"The different types of magnetism arise mainly from how electrons move and interact with each other. In this work, we show how protons (H$^+$) also exhibit magnetic behavior. We measured the magnetic susceptibility of the ammonium halides and identified pronounced increases at 232 K, 233 K and 243 K for NH$_4$I, NH$_4$Br and NH$_4$Cl, respectively, which all coincide to the geometric ordering of its ammonium cations. With extensive literature establishing the fact that the ammonium cations exhibit rotational motion even towards the lowest temperatures, we take into account that the orbital motion of the protons carries a magnetic moment and find it to be larger than that of the paired electrons. Consequently, the structural phase transitions are magnetically-driven as the system attempts to lift 8-fold energy degeneracies of the proton orbitals via Jahn-Teller distortions. Our findings identify that NH$_4$$^+$ cations are capable of comprising magnetism which appears to be ubiquitous in ammonia-based molecular solids.","sentences":["The different types of magnetism arise mainly from how electrons move and interact with each other.","In this work, we show how protons (H$^+$) also exhibit magnetic behavior.","We measured the magnetic susceptibility of the ammonium halides and identified pronounced increases at 232 K, 233 K and 243 K for NH$_4$I, NH$_4$Br and NH$_4$Cl, respectively, which all coincide to the geometric ordering of its ammonium cations.","With extensive literature establishing the fact that the ammonium cations exhibit rotational motion even towards the lowest temperatures, we take into account that the orbital motion of the protons carries a magnetic moment and find it to be larger than that of the paired electrons.","Consequently, the structural phase transitions are magnetically-driven as the system attempts to lift 8-fold energy degeneracies of the proton orbitals via Jahn-Teller distortions.","Our findings identify that NH$_4$$^+$ cations are capable of comprising magnetism which appears to be ubiquitous in ammonia-based molecular solids."],"url":"http://arxiv.org/abs/2405.03163v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 04:39:23","title":"Solutions to ${\\rm SU}(n+1)$ Toda system with cone singularities via toric curves on compact Riemann surfaces","abstract":"On a compact Riemann surface (X) with finite punctures (P_1, \\ldots, P_k), we define toric curves as multi-valued, totally unramified holomorphic maps to (\\mathbb{P}^n) with monodromy in a maximal torus of ({\\rm PSU}(n+1)). \\textit{Toric solutions} for the ({\\rm SU}(n+1)) system on $X\\setminus\\{P_1,\\ldots, P_k\\}$ are recognized by their associated {\\it toric} curves in (\\mathbb{P}^n). We introduce a character n-ensemble as an (n)-tuple of meromorphic one-forms with simple poles and purely imaginary periods, generating toric curves on (X) minus finitely many points. We establish on $X$ a correspondence between character $n$-ensembles and toric solutions to the ({\\rm SU}(n+1)) system with finitely many cone singularities. Our approach not only broadens seminal solutions for up to two cone singularities on the Riemann sphere, as classified by Jost-Wang (Int. Math. Res. Not., (6):277-290, 2002) and Lin-Wei-Ye (Invent. Math., 190(1):169-207, 2012), but also advances beyond the limits of Lin-Yang-Zhong's existence theorems (J. Differential Geom., 114(2):337-391, 2020) by introducing a new solution class.","sentences":["On a compact Riemann surface (X) with finite punctures (P_1, \\ldots, P_k), we define toric curves as multi-valued, totally unramified holomorphic maps to (\\mathbb{P}^n) with monodromy in a maximal torus of ({\\rm PSU}(n+1)).","\\textit{Toric solutions} for the ({\\rm SU}(n+1)) system on $X\\setminus\\{P_1,\\ldots, P_k\\}$ are recognized by their associated {\\it toric} curves in (\\mathbb{P}^n).","We introduce a character n-ensemble as an (n)-tuple of meromorphic one-forms with simple poles and purely imaginary periods, generating toric curves on (X) minus finitely many points.","We establish on $X$ a correspondence between character $n$-ensembles and toric solutions to the ({\\rm SU}(n+1))","system with finitely many cone singularities.","Our approach not only broadens seminal solutions for up to two cone singularities on the Riemann sphere, as classified by Jost-Wang (Int.","Math. Res.","Not., (6):277-290, 2002) and Lin-Wei-Ye (Invent.","Math., 190(1):169-207, 2012), but also advances beyond the limits of Lin-Yang-Zhong's existence theorems (J. Differential Geom., 114(2):337-391, 2020) by introducing a new solution class."],"url":"http://arxiv.org/abs/2405.03161v1","category":"math.DG"}
{"created":"2024-05-06 04:22:28","title":"Strain Induced Kramers-Weyl Phase in III-V Zinc Blende Systems","abstract":"We present theoretical observations on the topological nature of strained III-V semiconductors. By $k \\cdot p$ perturbation, it can be shown that the strain-engineered conduction band hosts a Kramers-Weyl node at the $\\Gamma$ point. It is theoretically shown a curated strain can create and then tune the sign of the topological charge. Furthermore, we outline experimental methods for both the realization and detection of strain-induced topological phase transitions.","sentences":["We present theoretical observations on the topological nature of strained III-V semiconductors.","By $k \\cdot p$ perturbation, it can be shown that the strain-engineered conduction band hosts a Kramers-Weyl node at the $\\Gamma$ point.","It is theoretically shown a curated strain can create and then tune the sign of the topological charge.","Furthermore, we outline experimental methods for both the realization and detection of strain-induced topological phase transitions."],"url":"http://arxiv.org/abs/2405.03156v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 04:12:01","title":"Learning Nonlinear Dynamics Using Kalman Smoothing","abstract":"Identifying Ordinary Differential Equations (ODEs) from measurement data requires both fitting the dynamics and assimilating, either implicitly or explicitly, the measurement data. The Sparse Identification of Nonlinear Dynamics (SINDy) method involves a derivative estimation step (and optionally, smoothing) and a sparse regression step on a library of candidate ODE terms. Kalman smoothing is a classical framework for assimilating the measurement data with known noise statistics. Previously, derivatives in SINDy and its python package, pysindy, had been estimated by finite difference, L1 total variation minimization, or local filters like Savitzky-Golay. In contrast, Kalman allows discovering ODEs that best recreate the essential dynamics in simulation, even in cases when it does not perform as well at recovering coefficients, as measured by their F1 score and mean absolute error. We have incorporated Kalman smoothing, along with hyperparameter optimization, into the existing pysindy architecture, allowing for rapid adoption of the method. Numerical experiments on a number of dynamical systems show Kalman smoothing to be the most amenable to parameter selection and best at preserving problem structure in the presence of noise.","sentences":["Identifying Ordinary Differential Equations (ODEs) from measurement data requires both fitting the dynamics and assimilating, either implicitly or explicitly, the measurement data.","The Sparse Identification of Nonlinear Dynamics (SINDy) method involves a derivative estimation step (and optionally, smoothing) and a sparse regression step on a library of candidate ODE terms.","Kalman smoothing is a classical framework for assimilating the measurement data with known noise statistics.","Previously, derivatives in SINDy and its python package, pysindy, had been estimated by finite difference, L1 total variation minimization, or local filters like Savitzky-Golay.","In contrast, Kalman allows discovering ODEs that best recreate the essential dynamics in simulation, even in cases when it does not perform as well at recovering coefficients, as measured by their F1 score and mean absolute error.","We have incorporated Kalman smoothing, along with hyperparameter optimization, into the existing pysindy architecture, allowing for rapid adoption of the method.","Numerical experiments on a number of dynamical systems show Kalman smoothing to be the most amenable to parameter selection and best at preserving problem structure in the presence of noise."],"url":"http://arxiv.org/abs/2405.03154v1","category":"math.DS"}
{"created":"2024-05-06 04:06:45","title":"Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines","abstract":"In the digital age, the prevalence of misleading news headlines poses a significant challenge to information integrity, necessitating robust detection mechanisms. This study explores the efficacy of Large Language Models (LLMs) in identifying misleading versus non-misleading news headlines. Utilizing a dataset of 60 articles, sourced from both reputable and questionable outlets across health, science & tech, and business domains, we employ three LLMs- ChatGPT-3.5, ChatGPT-4, and Gemini-for classification. Our analysis reveals significant variance in model performance, with ChatGPT-4 demonstrating superior accuracy, especially in cases with unanimous annotator agreement on misleading headlines. The study emphasizes the importance of human-centered evaluation in developing LLMs that can navigate the complexities of misinformation detection, aligning technical proficiency with nuanced human judgment. Our findings contribute to the discourse on AI ethics, emphasizing the need for models that are not only technically advanced but also ethically aligned and sensitive to the subtleties of human interpretation.","sentences":["In the digital age, the prevalence of misleading news headlines poses a significant challenge to information integrity, necessitating robust detection mechanisms.","This study explores the efficacy of Large Language Models (LLMs) in identifying misleading versus non-misleading news headlines.","Utilizing a dataset of 60 articles, sourced from both reputable and questionable outlets across health, science & tech, and business domains, we employ three LLMs- ChatGPT-3.5, ChatGPT-4, and Gemini-for classification.","Our analysis reveals significant variance in model performance, with ChatGPT-4 demonstrating superior accuracy, especially in cases with unanimous annotator agreement on misleading headlines.","The study emphasizes the importance of human-centered evaluation in developing LLMs that can navigate the complexities of misinformation detection, aligning technical proficiency with nuanced human judgment.","Our findings contribute to the discourse on AI ethics, emphasizing the need for models that are not only technically advanced but also ethically aligned and sensitive to the subtleties of human interpretation."],"url":"http://arxiv.org/abs/2405.03153v1","category":"cs.CL"}
{"created":"2024-05-06 03:58:33","title":"Observing S-Matrix Pole Flow in Resonance Interplay: Cold Collisions of Ultracold Atoms in a Miniature Laser-based Accelerator","abstract":"We provide an overview of experiments exploring resonances in the collision of ultracold clouds of atoms. Using a laser-based accelerator that capitalizes on the energy resolution provided by the ultracold atomic setting, we unveil resonance phenomena such as Feshbach and shape resonances in their quintessential form by literally photographing the halo of outgoing scattered atoms. We exploit the tunability of magnetic Feshbach resonances to instigate an interplay between scattering resonances. By experimentally recording the scattering in a parameter space spanned by collision energy and magnetic field, we capture the imprint of the $S$-matrix pole flow in the complex energy plane. After revisiting experiments that place a Feshbach resonance in the proximity of a shape resonance and an anti-bound state, respectively, we discuss the possibility of using $S$-matrix pole interplay between two Feshbach resonances to create a bound-state-in-the-continuum.","sentences":["We provide an overview of experiments exploring resonances in the collision of ultracold clouds of atoms.","Using a laser-based accelerator that capitalizes on the energy resolution provided by the ultracold atomic setting, we unveil resonance phenomena such as Feshbach and shape resonances in their quintessential form by literally photographing the halo of outgoing scattered atoms.","We exploit the tunability of magnetic Feshbach resonances to instigate an interplay between scattering resonances.","By experimentally recording the scattering in a parameter space spanned by collision energy and magnetic field, we capture the imprint of the $S$-matrix pole flow in the complex energy plane.","After revisiting experiments that place a Feshbach resonance in the proximity of a shape resonance and an anti-bound state, respectively, we discuss the possibility of using $S$-matrix pole interplay between two Feshbach resonances to create a bound-state-in-the-continuum."],"url":"http://arxiv.org/abs/2405.03149v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 03:55:54","title":"Counting Subnetworks Under Gene Duplication in Genetic Regulatory Networks","abstract":"Gene duplication is a fundamental evolutionary mechanism that contributes to biological complexity and diversity (Fortna et al., 2004). Traditionally, research has focused on the duplication of gene sequences (Zhang, 1914). However, evidence suggests that the duplication of regulatory elements may also play a significant role in the evolution of genomic functions (Teichmann and Babu, 2004; Hallin and Landry, 2019). In this work, the evolution of regulatory relationships belonging to gene-specific-substructures in a GRN are modeled. In the model, a network grows from an initial configuration by repeatedly choosing a random gene to duplicate. The likelihood that the regulatory relationships associated with the selected gene are retained through duplication is determined by a vector of probabilities. Occurrences of gene-family-specific substructures are counted under the gene duplication model. In this thesis, gene-family-specific substructures are referred to as subnetwork motifs. These subnetwork motifs are motivated by network motifs which are patterns of interconnections that recur more often in a specialized network than in a random network (Milo et al., 2002). Subnetwork motifs differ from network motifs in the way that subnetwork motifs are instances of gene-family-specific substructures while network motifs are isomorphic substructures. These subnetwork motifs are counted under Full and Partial Duplication, which differ in the way in which regulation relationships are inherited. Full duplication occurs when all regulatory links are inherited at each duplication step, and Partial Duplication occurs when regulation inheritance varies at each duplication step. Moments for the number of occurrences of subnetwork motifs are determined in each model. The results presented offer a method for discovering subnetwork motifs that are significant in a GRN under gene duplication.","sentences":["Gene duplication is a fundamental evolutionary mechanism that contributes to biological complexity and diversity (Fortna et al., 2004).","Traditionally, research has focused on the duplication of gene sequences (Zhang, 1914).","However, evidence suggests that the duplication of regulatory elements may also play a significant role in the evolution of genomic functions (Teichmann and Babu, 2004; Hallin and Landry, 2019).","In this work, the evolution of regulatory relationships belonging to gene-specific-substructures in a GRN are modeled.","In the model, a network grows from an initial configuration by repeatedly choosing a random gene to duplicate.","The likelihood that the regulatory relationships associated with the selected gene are retained through duplication is determined by a vector of probabilities.","Occurrences of gene-family-specific substructures are counted under the gene duplication model.","In this thesis, gene-family-specific substructures are referred to as subnetwork motifs.","These subnetwork motifs are motivated by network motifs which are patterns of interconnections that recur more often in a specialized network than in a random network (Milo et al., 2002).","Subnetwork motifs differ from network motifs in the way that subnetwork motifs are instances of gene-family-specific substructures while network motifs are isomorphic substructures.","These subnetwork motifs are counted under Full and Partial Duplication, which differ in the way in which regulation relationships are inherited.","Full duplication occurs when all regulatory links are inherited at each duplication step, and Partial Duplication occurs when regulation inheritance varies at each duplication step.","Moments for the number of occurrences of subnetwork motifs are determined in each model.","The results presented offer a method for discovering subnetwork motifs that are significant in a GRN under gene duplication."],"url":"http://arxiv.org/abs/2405.03148v1","category":"q-bio.MN"}
{"created":"2024-05-06 03:52:38","title":"Data Format Standardization and DICOM Integration for Hyperpolarized 13C MRI","abstract":"Hyperpolarized (HP) 13C MRI has shown promise as a valuable modality for in vivo measurements of metabolism and is currently in human trials at 15 research sites worldwide. With this growth it is important to adopt standardized data storage practices as it will allow sites to meaningfully compare data.   In this paper we (1) describe data that we believe should be stored and (2) demonstrate pipelines and methods that utilize the Digital Imaging and Communications in Medicine (DICOM) standard. This includes proposing a set of minimum set of information that is specific to HP 13C MRI studies. We then show where the majority of these can be fit into existing DICOM Attributes, primarily via the \"Contrast/Bolus\" module.   We also demonstrate pipelines for utilizing DICOM for HP 13C MRI. DICOM is the most common standard for clinical medical image storage and provides the flexibility to accommodate the unique aspects of HP 13C MRI, including the HP agent information but also spectroscopic and metabolite dimensions. The pipelines shown include creating DICOM objects for studies on human and animal imaging systems with various pulse sequences. We also show a python-based method to efficiently modify DICOM objects to incorporate the unique HP 13C MRI information that is not captured by existing pipelines. Moreover, we propose best practices for HP 13C MRI data storage that will support future multi-site trials, research studies and technical developments of this imaging technique.","sentences":["Hyperpolarized (HP) 13C MRI has shown promise as a valuable modality for in vivo measurements of metabolism and is currently in human trials at 15 research sites worldwide.","With this growth it is important to adopt standardized data storage practices as it will allow sites to meaningfully compare data.   ","In this paper we (1) describe data that we believe should be stored and (2) demonstrate pipelines and methods that utilize the Digital Imaging and Communications in Medicine (DICOM) standard.","This includes proposing a set of minimum set of information that is specific to HP 13C MRI studies.","We then show where the majority of these can be fit into existing DICOM Attributes, primarily via the \"Contrast/Bolus\" module.   ","We also demonstrate pipelines for utilizing DICOM for HP 13C MRI.","DICOM is the most common standard for clinical medical image storage and provides the flexibility to accommodate the unique aspects of HP 13C MRI, including the HP agent information but also spectroscopic and metabolite dimensions.","The pipelines shown include creating DICOM objects for studies on human and animal imaging systems with various pulse sequences.","We also show a python-based method to efficiently modify DICOM objects to incorporate the unique HP 13C MRI information that is not captured by existing pipelines.","Moreover, we propose best practices for HP 13C MRI data storage that will support future multi-site trials, research studies and technical developments of this imaging technique."],"url":"http://arxiv.org/abs/2405.03147v1","category":"physics.med-ph"}
{"created":"2024-05-06 03:33:29","title":"A novel fourth-order scheme for two-dimensional Riesz space fractional nonlinear reaction-diffusion equations and its optimal preconditioned solver","abstract":"A novel fourth-order finite difference formula coupling the Crank-Nicolson explicit linearized method is proposed to solve Riesz space fractional nonlinear reaction-diffusion equations in two dimensions. Theoretically, under the Lipschitz assumption on the nonlinear term, the proposed high-order scheme is proved to be unconditionally stable and convergent in the discrete $L_2$-norm. Moreover, a $\\tau$-matrix based preconditioner is developed to speed up the convergence of the conjugate gradient method with an optimal convergence rate (a convergence rate independent of mesh sizes) for solving the symmetric discrete linear system. Theoretical analysis shows that the spectra of the preconditioned matrices are uniformly bounded in the open interval $(3/8,2)$. To the best of our knowledge, this is the first attempt to develop a preconditioned iterative solver with a mesh-independent convergence rate for the linearized high-order scheme. Numerical examples are given to validate the accuracy of the scheme and the effectiveness of the proposed preconditioned solver.","sentences":["A novel fourth-order finite difference formula coupling the Crank-Nicolson explicit linearized method is proposed to solve Riesz space fractional nonlinear reaction-diffusion equations in two dimensions.","Theoretically, under the Lipschitz assumption on the nonlinear term, the proposed high-order scheme is proved to be unconditionally stable and convergent in the discrete $L_2$-norm.","Moreover, a $\\tau$-matrix based preconditioner is developed to speed up the convergence of the conjugate gradient method with an optimal convergence rate (a convergence rate independent of mesh sizes) for solving the symmetric discrete linear system.","Theoretical analysis shows that the spectra of the preconditioned matrices are uniformly bounded in the open interval $(3/8,2)$. To the best of our knowledge, this is the first attempt to develop a preconditioned iterative solver with a mesh-independent convergence rate for the linearized high-order scheme.","Numerical examples are given to validate the accuracy of the scheme and the effectiveness of the proposed preconditioned solver."],"url":"http://arxiv.org/abs/2405.03143v1","category":"math.NA"}
{"created":"2024-05-06 03:20:59","title":"Calculated Unconventional Superconductivity via Charge Fluctuations in Kagome Metal CsV3Sb5","abstract":"Electrons on Kagome lattice exhibit a wealth of features including Dirac points, van Hove singularities and flatbands. When the Fermi level is placed at the van Hove saddle point, the Fermi surface is perfectly nested and a rich variety of electronic instabilities is known to occur. The material realization of such scenario is a recently discovered Kagome system CsV3Sb5 whose superconductivity near charge-density wave instability at low temperatures points to an unconventional, non-electron-phonon, pairing mechanism. Here we use a recently developed combination of density functional theory with momentum and frequency-resolved self-energies deduced from the so-called fluctuational-exchange-type random phase approximation to study charge fluctuation mediated pairing tendencies in CsV3Sb5. Based on our numerical diagonalization of the BCS gap equation, two competing solutions emerge from these calculations with A_{1g} (anisotropic s-wave-like) and B_{2g} (d_{x2-y2},d_{xy}-like) symmetries of the superconducting order parameter. Our evaluated Eliashberg spectral functions {\\alpha}2F({\\omega}) are purely due to electronic correlations; they were found to be strongly peaked in the vicinity of frequency 7 meV that sets the scale of charge fluctuations. The superconducting coupling constants for the leading pairing channels are estimated as a function of the nearest neighbor Coulomb interaction V, a well-known prime parameter of the extended Hubbard model. They were found in the range of 0.2-0.4 depending on V. We evaluate the superconducting T_{c} close to the values that are observed experimentally that point to the charge fluctuations to provide a substantial contribution to the pairing mechanism in CsV3Sb5.","sentences":["Electrons on Kagome lattice exhibit a wealth of features including Dirac points, van Hove singularities and flatbands.","When the Fermi level is placed at the van Hove saddle point, the Fermi surface is perfectly nested and a rich variety of electronic instabilities is known to occur.","The material realization of such scenario is a recently discovered Kagome system CsV3Sb5 whose superconductivity near charge-density wave instability at low temperatures points to an unconventional, non-electron-phonon, pairing mechanism.","Here we use a recently developed combination of density functional theory with momentum and frequency-resolved self-energies deduced from the so-called fluctuational-exchange-type random phase approximation to study charge fluctuation mediated pairing tendencies in CsV3Sb5.","Based on our numerical diagonalization of the BCS gap equation, two competing solutions emerge from these calculations with A_{1g} (anisotropic s-wave-like) and B_{2g} (d_{x2-y2},d_{xy}-like) symmetries of the superconducting order parameter.","Our evaluated Eliashberg spectral functions {\\alpha}2F({\\omega}) are purely due to electronic correlations; they were found to be strongly peaked in the vicinity of frequency 7 meV that sets the scale of charge fluctuations.","The superconducting coupling constants for the leading pairing channels are estimated as a function of the nearest neighbor Coulomb interaction V, a well-known prime parameter of the extended Hubbard model.","They were found in the range of 0.2-0.4 depending on V. We evaluate the superconducting T_{c} close to the values that are observed experimentally that point to the charge fluctuations to provide a substantial contribution to the pairing mechanism in CsV3Sb5."],"url":"http://arxiv.org/abs/2405.03137v1","category":"cond-mat.supr-con"}
{"created":"2024-05-06 03:12:17","title":"CURLING - I. The Influence of Point-like Image Approximation on the Outcomes of Cluster Strong Lens Modeling","abstract":"Cluster-scale strong lensing is a powerful tool for exploring the properties of dark matter and constraining cosmological models. However, due to the complex parameter space, pixelized strong lens modeling in galaxy clusters is computationally expensive, leading to the point-source approximation of strongly lensed extended images, potentially introducing systematic biases. Herein, as the first paper of the ClUsteR strong Lens modelIng for the Next-Generation observations (CURLING) program, we use lensing ray-tracing simulations to quantify the biases and uncertainties arising from the point-like image approximation for JWST-like observations. Our results indicate that the approximation works well for reconstructing the total cluster mass distribution, but can bias the magnification measurements near critical curves and the constraints on the cosmological parameters, the total matter density of the Universe $\\Omega_{\\rm m}$, and dark energy equation of state parameter $w$. To mitigate the biases, we propose incorporating the extended surface brightness distribution of lensed sources into the modeling. This approach reduces the bias in magnification from 46.2 per cent to 0.09 per cent for $\\mu \\sim 1000$. Furthermore, the median values of cosmological parameters align more closely with the fiducial model. In addition to the improved accuracy, we also demonstrate that the constraining power can be substantially enhanced. In conclusion, it is necessary to model cluster-scale strong lenses with pixelized multiple images, especially for estimating the intrinsic luminosity of highly magnified sources and accurate cosmography in the era of high-precision observations.","sentences":["Cluster-scale strong lensing is a powerful tool for exploring the properties of dark matter and constraining cosmological models.","However, due to the complex parameter space, pixelized strong lens modeling in galaxy clusters is computationally expensive, leading to the point-source approximation of strongly lensed extended images, potentially introducing systematic biases.","Herein, as the first paper of the ClUsteR strong Lens modelIng for the Next-Generation observations (CURLING) program, we use lensing ray-tracing simulations to quantify the biases and uncertainties arising from the point-like image approximation for JWST-like observations.","Our results indicate that the approximation works well for reconstructing the total cluster mass distribution, but can bias the magnification measurements near critical curves and the constraints on the cosmological parameters, the total matter density of the Universe $\\Omega_{\\rm m}$, and dark energy equation of state parameter $w$. To mitigate the biases, we propose incorporating the extended surface brightness distribution of lensed sources into the modeling.","This approach reduces the bias in magnification from 46.2 per cent to 0.09 per cent for $\\mu \\sim 1000$.","Furthermore, the median values of cosmological parameters align more closely with the fiducial model.","In addition to the improved accuracy, we also demonstrate that the constraining power can be substantially enhanced.","In conclusion, it is necessary to model cluster-scale strong lenses with pixelized multiple images, especially for estimating the intrinsic luminosity of highly magnified sources and accurate cosmography in the era of high-precision observations."],"url":"http://arxiv.org/abs/2405.03135v1","category":"astro-ph.CO"}
{"created":"2024-05-06 17:52:04","title":"Why is SAM Robust to Label Noise?","abstract":"Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape. In particular, the peak performance under label noise occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples. Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets.","sentences":["Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks.","However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise.","Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape.","In particular, the peak performance under label noise occurs with early stopping, far before the loss converges.","We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian.","The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples.","Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance.","We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian.","We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks.","Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets."],"url":"http://arxiv.org/abs/2405.03676v1","category":"cs.LG"}
{"created":"2024-05-06 17:50:35","title":"Topological Quantum Batteries","abstract":"We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features. Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs). First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs. We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase. Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states. Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries. Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state. Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced. Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering.","sentences":["We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features.","Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs).","First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs.","We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase.","Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states.","Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries.","Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state.","Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced.","Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering."],"url":"http://arxiv.org/abs/2405.03675v1","category":"quant-ph"}
{"created":"2024-05-06 17:49:32","title":"Anti-Heroes: An Ethics-focused Method for Responsible Designer Intentions","abstract":"HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process. In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles. Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue. The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes. Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students. We propose implications of Anti-Heros for technology and design education and practice.","sentences":["HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process.","In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles.","Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue.","The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes.","Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students.","We propose implications of Anti-Heros for technology and design education and practice."],"url":"http://arxiv.org/abs/2405.03674v1","category":"cs.HC"}
{"created":"2024-05-06 17:41:13","title":"A New Robust Partial $p$-Wasserstein-Based Metric for Comparing Distributions","abstract":"The $2$-Wasserstein distance is sensitive to minor geometric differences between distributions, making it a very powerful dissimilarity metric. However, due to this sensitivity, a small outlier mass can also cause a significant increase in the $2$-Wasserstein distance between two similar distributions. Similarly, sampling discrepancy can cause the empirical $2$-Wasserstein distance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a rate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$ for $1$-Wasserstein distance.   We introduce a new family of distances parameterized by $k \\ge 0$, called $k$-RPW, that is based on computing the partial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric properties, (2) $k$-RPW is robust to small outlier mass while retaining the sensitivity of $2$-Wasserstein distance to minor geometric differences, and (3) when $k$ is a constant, $k$-RPW distance between empirical distributions on $n$ samples in $\\mathbb{R}^2$ converges to the true distance at a rate of $n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the $2$-Wasserstein distance.   Using the partial $p$-Wasserstein distance, we extend our distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$ appropriately, we can reduce our distance to the total variation, $p$-Wasserstein, and the L\\'evy-Prokhorov distances. Experiments show that our distance function achieves higher accuracy in comparison to the $1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on noisy real-world data sets.","sentences":["The $2$-Wasserstein distance is sensitive to minor geometric differences between distributions, making it a very powerful dissimilarity metric.","However, due to this sensitivity, a small outlier mass can also cause a significant increase in the $2$-Wasserstein distance between two similar distributions.","Similarly, sampling discrepancy can cause the empirical $2$-Wasserstein distance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a rate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$ for $1$-Wasserstein distance.   ","We introduce a new family of distances parameterized by $k \\ge 0$, called $k$-RPW, that is based on computing the partial $2$-Wasserstein distance.","We show that (1) $k$-RPW satisfies the metric properties, (2) $k$-RPW is robust to small outlier mass while retaining the sensitivity of $2$-Wasserstein distance to minor geometric differences, and (3) when $k$ is a constant, $k$-RPW distance between empirical distributions on $n$ samples in $\\mathbb{R}^2$ converges to the true distance at a rate of $n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the $2$-Wasserstein distance.   ","Using the partial $p$-Wasserstein distance, we extend our distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$ appropriately, we can reduce our distance to the total variation, $p$-Wasserstein, and the L\\'evy-Prokhorov distances.","Experiments show that our distance function achieves higher accuracy in comparison to the $1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on noisy real-world data sets."],"url":"http://arxiv.org/abs/2405.03664v1","category":"cs.LG"}
{"created":"2024-05-06 17:14:34","title":"Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders","abstract":"Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.","sentences":["Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance.","Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization.","DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE.","While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale.","In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.","We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries.","Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation.","At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items.","Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches.","Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines."],"url":"http://arxiv.org/abs/2405.03651v1","category":"cs.IR"}
{"created":"2024-05-06 16:59:08","title":"Planetary Nebulae of the Large Magellanic Cloud I: A multiwavelength analysis","abstract":"Planetary nebulae (PNe) have three main components: a central star (CS), ionised gas and dust in the nebula. Each of them contains critical chemical fingerprints of their evolution, serving as tracers of the evolution, nucleosynthesis and dust production that occurred during the preceding asymptotic giant branch (AGB) phase. We aim to build a bridge to link the PN phase to the evolution of their progenitors, trying to better understand the dust production and mass-loss mechanism during the final AGB phase. Here, we present a comprehensive study of nine Large Magellanic Cloud (LMC) spherical or elliptical PNe whose observations from the ultraviolet (UV) through the infrared (IR) are available in the literature. We characterize nebulae and CSs, finding information as the amount of gas that makes up the nebula and the dust that surrounds the CS, necessary to reconstruct the evolutionary history of mass-loss and dust production. We compare the observed energy distribution of the selected PNe to that obtained from photoionization modeling, taking into account the presence of dust. The physical and chemical parameters of the central stars are then compared with the predictions from the evolutionary tracks. We characterized the source, assigning to each CS a progenitor, early-AGB mass. We estimated the mass of the nebula and the dust-to-gas ratio. For 5 objects, we find evidence for the presence of a near-IR bump, which would be connected to the presence of hot dust.","sentences":["Planetary nebulae (PNe) have three main components: a central star (CS), ionised gas and dust in the nebula.","Each of them contains critical chemical fingerprints of their evolution, serving as tracers of the evolution, nucleosynthesis and dust production that occurred during the preceding asymptotic giant branch (AGB) phase.","We aim to build a bridge to link the PN phase to the evolution of their progenitors, trying to better understand the dust production and mass-loss mechanism during the final AGB phase.","Here, we present a comprehensive study of nine Large Magellanic Cloud (LMC) spherical or elliptical PNe whose observations from the ultraviolet (UV) through the infrared (IR) are available in the literature.","We characterize nebulae and CSs, finding information as the amount of gas that makes up the nebula and the dust that surrounds the CS, necessary to reconstruct the evolutionary history of mass-loss and dust production.","We compare the observed energy distribution of the selected PNe to that obtained from photoionization modeling, taking into account the presence of dust.","The physical and chemical parameters of the central stars are then compared with the predictions from the evolutionary tracks.","We characterized the source, assigning to each CS a progenitor, early-AGB mass.","We estimated the mass of the nebula and the dust-to-gas ratio.","For 5 objects, we find evidence for the presence of a near-IR bump, which would be connected to the presence of hot dust."],"url":"http://arxiv.org/abs/2405.03640v1","category":"astro-ph.GA"}
{"created":"2024-05-06 16:52:26","title":"On a Completion of Cohomological Functors Generalising Tate Cohomology II","abstract":"Viewing group cohomology as a so-called cohomological functor, G. Mislin has generalised Tate cohomology from finite groups to all discrete groups by defining a completion for cohomological functors in [24]. For any cohomological functor $T^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ we have constructed its Mislin completion $\\widehat{T}^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ in [15] under mild assumptions on the abelian categories $\\mathcal{C}$ and $\\mathcal{D}$ which generalises Tate cohomology to all $T1$ topological groups. In this paper we investigate the properties of Mislin completions. As their main feature, Mislin completions of Ext-functors detect finite projective dimension of objects in the domain category. We establish a version of dimension shifting, an Eckmann-Shapiro result as well as cohomology products such as external products, cup products and Yoneda products.","sentences":["Viewing group cohomology as a so-called cohomological functor, G. Mislin has generalised Tate cohomology from finite groups to all discrete groups by defining a completion for cohomological functors in [24].","For any cohomological functor $T^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ we have constructed its Mislin completion $\\widehat{T}^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ in [15] under mild assumptions on the abelian categories $\\mathcal{C}$ and $\\mathcal{D}$ which generalises Tate cohomology to all $T1$ topological groups.","In this paper we investigate the properties of Mislin completions.","As their main feature, Mislin completions of Ext-functors detect finite projective dimension of objects in the domain category.","We establish a version of dimension shifting, an Eckmann-Shapiro result as well as cohomology products such as external products, cup products and Yoneda products."],"url":"http://arxiv.org/abs/2405.03634v1","category":"math.GR"}
{"created":"2024-05-06 16:49:11","title":"LaserEscape: Detecting and Mitigating Optical Probing Attacks","abstract":"The security of integrated circuits (ICs) can be broken by sophisticated physical attacks relying on failure analysis methods. Optical probing is one of the most prominent examples of such attacks, which can be accomplished in a matter of days, even with limited knowledge of the IC under attack. Unfortunately, few countermeasures are proposed in the literature, and none has been fabricated and tested in practice. These countermeasures usually require changing the standard cell libraries and, thus, are incompatible with digital and programmable platforms, such as field programmable gate arrays (FPGAs). In this work, we shift our attention from preventing the attack to detecting and responding to it. We introduce LaserEscape, the first fully digital and FPGA-compatible countermeasure to detect and mitigate optical probing attacks. LaserEscape incorporates digital delay-based sensors to reliably detect the physical alteration on the fabric caused by laser beam irradiations in real time. Furthermore, as a response to the attack, LaserEscape deploys real-time hiding approaches using randomized hardware reconfigurability. It realizes 1) moving target defense (MTD) to physically move the sensitive circuity under attack out of the probing field of focus to protect secret keys and 2) polymorphism to logically obfuscate the functionality of the targeted circuit to counter function extraction and reverse engineering attempts. We demonstrate the effectiveness and resiliency of our approach by performing optical probing attacks on protected and unprotected designs on a 28-nm FPGA. Our results show that optical probing attacks can be reliably detected and mitigated without interrupting the chip's operation.","sentences":["The security of integrated circuits (ICs) can be broken by sophisticated physical attacks relying on failure analysis methods.","Optical probing is one of the most prominent examples of such attacks, which can be accomplished in a matter of days, even with limited knowledge of the IC under attack.","Unfortunately, few countermeasures are proposed in the literature, and none has been fabricated and tested in practice.","These countermeasures usually require changing the standard cell libraries and, thus, are incompatible with digital and programmable platforms, such as field programmable gate arrays (FPGAs).","In this work, we shift our attention from preventing the attack to detecting and responding to it.","We introduce LaserEscape, the first fully digital and FPGA-compatible countermeasure to detect and mitigate optical probing attacks.","LaserEscape incorporates digital delay-based sensors to reliably detect the physical alteration on the fabric caused by laser beam irradiations in real time.","Furthermore, as a response to the attack, LaserEscape deploys real-time hiding approaches using randomized hardware reconfigurability.","It realizes 1) moving target defense (MTD) to physically move the sensitive circuity under attack out of the probing field of focus to protect secret keys and 2) polymorphism to logically obfuscate the functionality of the targeted circuit to counter function extraction and reverse engineering attempts.","We demonstrate the effectiveness and resiliency of our approach by performing optical probing attacks on protected and unprotected designs on a 28-nm FPGA.","Our results show that optical probing attacks can be reliably detected and mitigated without interrupting the chip's operation."],"url":"http://arxiv.org/abs/2405.03632v1","category":"cs.CR"}
{"created":"2024-05-06 16:36:40","title":"Searching for the two poles of the $\u039e(1820)$ in the $\u03c8(3686) \\to \\bar\u039e^+ \\bar{K}^0 \u03a3^{*-}(\u03c0^- \u039b)$ decay","abstract":"We propose the reaction $\\psi(3686) \\to \\bar{\\Xi}^+ \\bar{K}^0 \\Sigma^{*-}$, with the $\\Sigma^{*-}$ decaying to $\\pi^- \\Lambda$ in order to show evidence for the existence of two $\\Xi(1820)$ states, one around $1824$ MeV and narrow, and another one around $1875$ MeV and wide. The phase space for $\\bar{K}^0 \\Sigma^{*-}$ production reduces the effect of the lower mass resonance, magnifying the effect of the higher mass resonance that shows clearly over the phase space. The estimated rate of the production is bigger than the one of the $\\psi(3686) \\to \\bar{\\Xi}^+ K^- \\Lambda$ reaction, where a clear peak for $\\Xi(1820)$ was observed by the BESIII collaboration, what makes the Beijing facility ideal to carry out the reaction proposed.","sentences":["We propose the reaction $\\psi(3686) \\to \\bar{\\Xi}^+ \\bar{K}^0 \\Sigma^{*-}$, with the $\\Sigma^{*-}$ decaying to $\\pi^- \\Lambda$ in order to show evidence for the existence of two $\\Xi(1820)$ states, one around $1824$ MeV and narrow, and another one around $1875$ MeV and wide.","The phase space for $\\bar{K}^0 \\Sigma^{*-}$ production reduces the effect of the lower mass resonance, magnifying the effect of the higher mass resonance that shows clearly over the phase space.","The estimated rate of the production is bigger than the one of the $\\psi(3686) \\to \\bar{\\Xi}^+ K^- \\Lambda$ reaction, where a clear peak for $\\Xi(1820)$ was observed by the BESIII collaboration, what makes the Beijing facility ideal to carry out the reaction proposed."],"url":"http://arxiv.org/abs/2405.03622v1","category":"hep-ph"}
{"created":"2024-05-06 16:24:53","title":"On a Completion of Cohomological Functors Generalising Tate Cohomology I","abstract":"Viewing group cohomology as a so-called cohomological functor, G. Mislin has generalised Tate cohomology from finite groups to all discrete groups by defining a completion for cohomological functors in [27]. If $T^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ is any cohomological functor, then we construct its Mislin completion $\\widehat{T}^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ under the assumption that the abelian category $\\mathcal{C}$ has enough projectives and that in the abelian category $\\mathcal{D}$ all countable direct limits exist and are exact. This takes Tate cohomology to settings where it has never been introduced such as in condensed mathematics. Through the latter, one can define Tate cohomology for any $T1$ topological group. More specifically, we generalise four constructions of Mislin completions from the literature, prove that they yield isomorphic cohomological functors and provide explicit formulae for their connecting homomorphisms. For any morphism $f: M \\rightarrow N$ in $\\mathcal{C}$ we develop formulae for $\\widehat{T}^n(f): \\widehat{T}^n(M) \\rightarrow \\widehat{T}^n(N)$ in terms of each construction.","sentences":["Viewing group cohomology as a so-called cohomological functor, G. Mislin has generalised Tate cohomology from finite groups to all discrete groups by defining a completion for cohomological functors in [27].","If $T^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ is any cohomological functor, then we construct its Mislin completion $\\widehat{T}^{\\bullet}: \\mathcal{C} \\rightarrow \\mathcal{D}$ under the assumption that the abelian category $\\mathcal{C}$ has enough projectives and that in the abelian category $\\mathcal{D}$ all countable direct limits exist and are exact.","This takes Tate cohomology to settings where it has never been introduced such as in condensed mathematics.","Through the latter, one can define Tate cohomology for any $T1$ topological group.","More specifically, we generalise four constructions of Mislin completions from the literature, prove that they yield isomorphic cohomological functors and provide explicit formulae for their connecting homomorphisms.","For any morphism $f: M \\rightarrow N$ in $\\mathcal{C}$ we develop formulae for $\\widehat{T}^n(f): \\widehat{T}^n(M) \\rightarrow \\widehat{T}^n(N)$ in terms of each construction."],"url":"http://arxiv.org/abs/2405.03610v1","category":"math.GR"}
{"created":"2024-05-06 16:07:27","title":"Improving the Ranging Performance of Random ISAC Signals Through Pulse Shaping Design","abstract":"In this paper, we propose a novel pulse shaping design for single-carrier integrated sensing and communication (ISAC) transmission. Due to the communication information embedded in the ISAC signal, the resulting auto-correlation function (ACF) is determined by both the information-conveying random symbol sequence and the signaling pulse, where the former leads to random fluctuations in the sidelobes of the ACF, impairing the range estimation performance. To overcome this challenge, we first analyze the statistical characteristics of the random ACF under the symbol-wise pulse shaping (SWPS) regime. As a step further, we formulate an optimization problem to design ISAC pulse shaping filters, which minimizes the average integrated sidelobe level ratio (ISLR) while meeting the Nyquist criterion, subject to power and bandwidth constraints. We then show that the problem can be recast as a convex quadratic program by expressing it in the frequency domain, which can be readily solved through standard tools. Numerical results demonstrate that the proposed pulse shaping design achieves substantial ranging sidelobe reduction compared to the celebrated root-raised cosine (RRC) pulse shaping, given that the communication throughput is unchanged.","sentences":["In this paper, we propose a novel pulse shaping design for single-carrier integrated sensing and communication (ISAC) transmission.","Due to the communication information embedded in the ISAC signal, the resulting auto-correlation function (ACF) is determined by both the information-conveying random symbol sequence and the signaling pulse, where the former leads to random fluctuations in the sidelobes of the ACF, impairing the range estimation performance.","To overcome this challenge, we first analyze the statistical characteristics of the random ACF under the symbol-wise pulse shaping (SWPS) regime.","As a step further, we formulate an optimization problem to design ISAC pulse shaping filters, which minimizes the average integrated sidelobe level ratio (ISLR) while meeting the Nyquist criterion, subject to power and bandwidth constraints.","We then show that the problem can be recast as a convex quadratic program by expressing it in the frequency domain, which can be readily solved through standard tools.","Numerical results demonstrate that the proposed pulse shaping design achieves substantial ranging sidelobe reduction compared to the celebrated root-raised cosine (RRC) pulse shaping, given that the communication throughput is unchanged."],"url":"http://arxiv.org/abs/2405.03597v1","category":"eess.SP"}
{"created":"2024-05-06 15:54:00","title":"Finite size effect on gluon dissociation of J/psi in relativistic heavy ion collisions","abstract":"Thermal quantities, including the the entropy density and gluon spectrum, of quark matter within a box that is finite in the longitudinal direction are calculated using a bag model. Under the assumption of entropy conservation, the corresponding gluon dissociation rate of J/psi is studied. It reaches a maximum at a certain longitudinal size L_m, below which the suppression is weak even if the temperature becomes higher than that without the finite size effect, and above which the dissociation rate approaches to the thermodynamic limit gradually with increasing longitudinal size of the fireball.","sentences":["Thermal quantities, including the the entropy density and gluon spectrum, of quark matter within a box that is finite in the longitudinal direction are calculated using a bag model.","Under the assumption of entropy conservation, the corresponding gluon dissociation rate of J/psi is studied.","It reaches a maximum at a certain longitudinal size L_m, below which the suppression is weak even if the temperature becomes higher than that without the finite size effect, and above which the dissociation rate approaches to the thermodynamic limit gradually with increasing longitudinal size of the fireball."],"url":"http://arxiv.org/abs/2405.03583v1","category":"nucl-th"}
{"created":"2024-05-06 15:53:18","title":"Some Statistical and Data Challenges When Building Early-Stage Digital Experimentation and Measurement Capabilities","abstract":"Digital experimentation and measurement (DEM) capabilities -- the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact -- are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions. Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others. Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era.   This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities. We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities. We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities. This is done using a ranking under lower uncertainty model, enabling one to construct a business case. We also examine what ingredients are necessary to run digital experiments. In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods. Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs' data efficiency.","sentences":["Digital experimentation and measurement (DEM) capabilities -- the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact -- are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions.","Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others.","Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era.   ","This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities.","We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities.","We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities.","This is done using a ranking under lower uncertainty model, enabling one to construct a business case.","We also examine what ingredients are necessary to run digital experiments.","In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods.","Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs' data efficiency."],"url":"http://arxiv.org/abs/2405.03579v1","category":"stat.AP"}
{"created":"2024-05-06 15:17:44","title":"Polynomials whose divisors are enumerated by $SL_2(N_0)$","abstract":"We consider a certain left action by the monoid $SL_2(\\mathbf{N}_0)$ on the set of divisor pairs $\\mathcal{D}_f := \\{ (m, n) \\in \\mathbf{N}_0 \\times \\mathbf{N}_0 : m \\lvert f(n) \\}$ where $f \\in \\mathbf{Z}[x]$ is a polynomial with integer coefficients. We classify all polynomials in $\\mathbf{Z}[x]$ for which this action extends to an invertible map $\\hat{F}_f: SL_2(\\mathbf{N}_0) \\rightarrow \\mathcal{D}_f$. We call such polynomials $\\textit{enumerable}$. One of these polynomials happens to be $f(n) = n^2 + 1$. It is a well-known conjecture that there exist infinitely many primes of the form $p = n^2 + 1$. We construct a sequence $\\mathcal{S}$ on the naturals defined by the recursions   $$ \\begin{cases} \\mathcal{S}(4k) = 2\\mathcal{S}(2k) - \\mathcal{S}(k) \\\\ \\mathcal{S}(4k+1) = 2\\mathcal{S}(2k) + \\mathcal{S}(2k+1) \\\\ \\mathcal{S}(4k+2) = 2\\mathcal{S}(2k+1) + \\mathcal{S}(2k) \\\\ \\mathcal{S}(4k+3) = 2\\mathcal{S}(2k+1) - \\mathcal{S}(k) \\\\ \\end{cases} $$ with initial conditions $\\mathcal{S}(1) = 0$, $\\mathcal{S}(2) = 1$, $\\mathcal{S}(3) = 1$.   $$\\{ \\mathcal{S}(k) \\}_{k \\in \\mathbf{N}} = \\{0,1,1,2,3,3,2,3,7,8,5,5,8,7,3, \\cdots \\}$$   $\\mathcal{S}$ is shown to have the properties   $1.$ For all $n \\in \\mathbf{N}_0$, we have $\\mathcal{S}(2^n) = \\mathcal{S}(2^{n+1} - 1) = n$.   $2.$ For all $n \\in \\mathbf{N}_0$, the size of the fiber of $n$ under $\\mathcal{S}$ satisfies $|\\mathcal{S}^{-1}(\\{n\\})| = \\tau(n^2 + 1)$ where $\\tau$ is the divisor counting function.   $3.$ For all $n \\in \\mathbf{N}_0$, the integer $n^2 + 1$ is prime if and only if $\\mathcal{S}^{-1}(\\{n\\}) = \\{2^n, 2^{n+1} - 1\\}$.   $4.$ $\\mathcal{S}(k)$ is a $2$-regular sequence.","sentences":["We consider a certain left action by the monoid $SL_2(\\mathbf{N}_0)$ on the set of divisor pairs $\\mathcal{D}_f := \\{ (m, n) \\in \\mathbf{N}_0","\\times \\mathbf{N}_0 : m \\lvert f(n) \\}$ where $f \\in \\mathbf{Z}[x]$ is a polynomial with integer coefficients.","We classify all polynomials in $\\mathbf{Z}[x]$ for which this action extends to an invertible map $\\hat{F}_f: SL_2(\\mathbf{N}_0)","\\rightarrow \\mathcal{D}_f$.","We call such polynomials $\\textit{enumerable}$. One of these polynomials happens to be $f(n)","= n^2 + 1$.","It is a well-known conjecture that there exist infinitely many primes of the form $p = n^2 + 1$.","We construct a sequence $\\mathcal{S}$ on the naturals defined by the recursions   $$ \\begin{cases} \\mathcal{S}(4k) = 2\\mathcal{S}(2k) - \\mathcal{S}(k) \\\\ \\mathcal{S}(4k+1) = 2\\mathcal{S}(2k)","+ \\mathcal{S}(2k+1) \\\\ \\mathcal{S}(4k+2) = 2\\mathcal{S}(2k+1) + \\mathcal{S}(2k) \\\\ \\mathcal{S}(4k+3) = 2\\mathcal{S}(2k+1) - \\mathcal{S}(k) \\\\ \\end{cases} $$ with initial conditions $\\mathcal{S}(1) = 0$, $\\mathcal{S}(2) = 1$, $\\mathcal{S}(3) = 1$.   $$\\{ \\mathcal{S}(k) \\}_{k \\in \\mathbf{N}} = \\{0,1,1,2,3,3,2,3,7,8,5,5,8,7,3, \\cdots \\}$$   $\\mathcal{S}$ is shown to have the properties   $1.$ For all $n \\in \\mathbf{N}_0$, we have $\\mathcal{S}(2^n)","= \\mathcal{S}(2^{n+1} - 1) = n$.   $2.$ For all $n \\in \\mathbf{N}_0$, the size of the fiber of $n$ under $\\mathcal{S}$ satisfies $|\\mathcal{S}^{-1}(\\{n\\})| = \\tau(n^2 + 1)$ where $\\tau$ is the divisor counting function.   ","$3.$ For all $n \\in \\mathbf{N}_0$, the integer $n^2 + 1$ is prime if and only if $\\mathcal{S}^{-1}(\\{n\\}) = \\{2^n, 2^{n+1} - 1\\}$.   $4.$ $\\mathcal{S}(k)$ is a $2$-regular sequence."],"url":"http://arxiv.org/abs/2405.03552v1","category":"math.NT"}
{"created":"2024-05-06 14:47:08","title":"New physics as a possible explanation for the Amaterasu particle","abstract":"The Telescope Array experiment has recently reported the most energetic event detected in the hybrid technique era, with a reconstructed energy of 240 EeV, which has been named \"Amaterasu\" after the Shinto deity. Its origin is intriguing since no powerful enough candidate sources are located within the region consistent with its propagation horizon and arrival direction. In this work, we investigate the possibility of describing its origin in a scenario of new physics, specifically under a Lorentz Invariance Violation (LIV) assumption. The kinematics of UHECR propagation under a phenomenological LIV approach is investigated. The total mean free path for a particle with Amaterasu's energy increases from a few Mpc to hundreds of Mpc for $-\\delta_{\\rm{had},0} > 10^{-22}$, expanding significantly the region from which it could have originated. A combined fit of the spectrum and composition data of Telescope Array under different LIV assumptions was performed. The data is best fitted with some level of LIV both with and without Amaterasu. The improvement of the LIV fit is larger when Amaterasu is considered. New physics in the form of LIV could, thus, provide a plausible explanation for the Amaterasu particle.","sentences":["The Telescope Array experiment has recently reported the most energetic event detected in the hybrid technique era, with a reconstructed energy of 240 EeV, which has been named \"Amaterasu\" after the Shinto deity.","Its origin is intriguing since no powerful enough candidate sources are located within the region consistent with its propagation horizon and arrival direction.","In this work, we investigate the possibility of describing its origin in a scenario of new physics, specifically under a Lorentz Invariance Violation (LIV) assumption.","The kinematics of UHECR propagation under a phenomenological LIV approach is investigated.","The total mean free path for a particle with Amaterasu's energy increases from a few Mpc to hundreds of Mpc for $-\\delta_{\\rm{had},0} > 10^{-22}$, expanding significantly the region from which it could have originated.","A combined fit of the spectrum and composition data of Telescope Array under different LIV assumptions was performed.","The data is best fitted with some level of LIV both with and without Amaterasu.","The improvement of the LIV fit is larger when Amaterasu is considered.","New physics in the form of LIV could, thus, provide a plausible explanation for the Amaterasu particle."],"url":"http://arxiv.org/abs/2405.03528v1","category":"astro-ph.HE"}
{"created":"2024-05-06 14:29:24","title":"GI-SMN: Gradient Inversion Attack against Federated Learning without Prior Knowledge","abstract":"Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data. Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks. However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc. Consequently, these attacks are not possible in real-world scenarios. To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks. The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching. GI-SMN enables the reconstruction of user data with high similarity in batches. Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics. Additionally, it also can overcome gradient pruning and differential privacy defenses.","sentences":["Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data.","Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks.","However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc.","Consequently, these attacks are not possible in real-world scenarios.","To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks.","The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching.","GI-SMN enables the reconstruction of user data with high similarity in batches.","Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics.","Additionally, it also can overcome gradient pruning and differential privacy defenses."],"url":"http://arxiv.org/abs/2405.03516v1","category":"cs.LG"}
{"created":"2024-05-06 14:25:58","title":"QBER: Quantifying Cyber Risks for Strategic Decisions","abstract":"Quantifying cyber risks is essential for organizations to grasp their vulnerability to threats and make informed decisions. However, current approaches still need to work on blending economic viewpoints to provide insightful analysis. To bridge this gap, we introduce QBER approach to offer decision-makers measurable risk metrics. The QBER evaluates losses from cyberattacks, performs detailed risk analyses based on existing cybersecurity measures, and provides thorough cost assessments. Our contributions involve outlining cyberattack probabilities and risks, identifying Technical, Economic, and Legal (TEL) impacts, creating a model to gauge impacts, suggesting risk mitigation strategies, and examining trends and challenges in implementing widespread Cyber Risk Quantification (CRQ). The QBER approach serves as a guided approach for organizations to assess risks and strategically invest in cybersecurity.","sentences":["Quantifying cyber risks is essential for organizations to grasp their vulnerability to threats and make informed decisions.","However, current approaches still need to work on blending economic viewpoints to provide insightful analysis.","To bridge this gap, we introduce QBER approach to offer decision-makers measurable risk metrics.","The QBER evaluates losses from cyberattacks, performs detailed risk analyses based on existing cybersecurity measures, and provides thorough cost assessments.","Our contributions involve outlining cyberattack probabilities and risks, identifying Technical, Economic, and Legal (TEL) impacts, creating a model to gauge impacts, suggesting risk mitigation strategies, and examining trends and challenges in implementing widespread Cyber Risk Quantification (CRQ).","The QBER approach serves as a guided approach for organizations to assess risks and strategically invest in cybersecurity."],"url":"http://arxiv.org/abs/2405.03513v1","category":"cs.CR"}
{"created":"2024-05-06 14:10:45","title":"Physical properties and electronic structure of the two-gap superconductor V$_{2}$Ga$_{5}$","abstract":"We present a thorough investigation of the physical properties and superconductivity of the binary intermetallic V2Ga5. Electrical resistivity and specific heat measurements show that V2Ga5 enters its superconducting state below Tsc = 3.5 K, with a critical field of Hc2,perp c(Hc2,para c) = 6.5(4.1) kOe. With H perp c, the peak effect was observed in resistivity measurements, indicating the ultrahigh quality of the single crystal studied. The resistivity measurements under high pressure reveal that the Tsc is suppressed linearly with pressure and reaches absolute zero around 20 GPa. Specific heat and muon spin relaxation measurements both indicate that the two-gap s-wave model best describes the superconductivity of V2Ga5. The spectra obtained from angle-resolved photoemission spectroscopy measurements suggest that two superconducting gaps open at the Fermi surface around the Z and {\\Gamma} points. These results are verified by first-principles band structure calculations. We therefore conclude that V2Ga5 is a phonon-mediated two-gap s-wave superconductor","sentences":["We present a thorough investigation of the physical properties and superconductivity of the binary intermetallic V2Ga5.","Electrical resistivity and specific heat measurements show that V2Ga5 enters its superconducting state below Tsc = 3.5 K, with a critical field of Hc2,perp c(Hc2,para c) = 6.5(4.1) kOe.","With H perp c, the peak effect was observed in resistivity measurements, indicating the ultrahigh quality of the single crystal studied.","The resistivity measurements under high pressure reveal that the Tsc is suppressed linearly with pressure and reaches absolute zero around 20 GPa.","Specific heat and muon spin relaxation measurements both indicate that the two-gap s-wave model best describes the superconductivity of V2Ga5.","The spectra obtained from angle-resolved photoemission spectroscopy measurements suggest that two superconducting gaps open at the Fermi surface around the Z and {\\Gamma} points.","These results are verified by first-principles band structure calculations.","We therefore conclude that V2Ga5 is a phonon-mediated two-gap s-wave superconductor"],"url":"http://arxiv.org/abs/2405.03499v1","category":"cond-mat.supr-con"}
{"created":"2024-05-06 13:57:17","title":"Precision-based designs for sequential randomized experiments","abstract":"In this paper, we consider an experimental setting where units enter the experiment sequentially. Our goal is to form stopping rules which lead to estimators of treatment effects with a given precision. We propose a fixed-width confidence interval design (FWCID) where the experiment terminates once a pre-specified confidence interval width is achieved. We show that under this design, the difference-in-means estimator is a consistent estimator of the average treatment effect and standard confidence intervals have asymptotic guarantees of coverage and efficiency for several versions of the design. In addition, we propose a version of the design that we call fixed power design (FPD) where a given power is asymptotically guaranteed for a given treatment effect, without the need to specify the variances of the outcomes under treatment or control. In addition, this design also gives a consistent difference-in-means estimator with correct coverage of the corresponding standard confidence interval. We complement our theoretical findings with Monte Carlo simulations where we compare our proposed designs with standard designs in the sequential experiments literature, showing that our designs outperform these designs in several important aspects. We believe our results to be relevant for many experimental settings where units enter sequentially, such as in clinical trials, as well as in online A/B tests used by the tech and e-commerce industry.","sentences":["In this paper, we consider an experimental setting where units enter the experiment sequentially.","Our goal is to form stopping rules which lead to estimators of treatment effects with a given precision.","We propose a fixed-width confidence interval design (FWCID) where the experiment terminates once a pre-specified confidence interval width is achieved.","We show that under this design, the difference-in-means estimator is a consistent estimator of the average treatment effect and standard confidence intervals have asymptotic guarantees of coverage and efficiency for several versions of the design.","In addition, we propose a version of the design that we call fixed power design (FPD) where a given power is asymptotically guaranteed for a given treatment effect, without the need to specify the variances of the outcomes under treatment or control.","In addition, this design also gives a consistent difference-in-means estimator with correct coverage of the corresponding standard confidence interval.","We complement our theoretical findings with Monte Carlo simulations where we compare our proposed designs with standard designs in the sequential experiments literature, showing that our designs outperform these designs in several important aspects.","We believe our results to be relevant for many experimental settings where units enter sequentially, such as in clinical trials, as well as in online A/B tests used by the tech and e-commerce industry."],"url":"http://arxiv.org/abs/2405.03487v1","category":"math.ST"}
{"created":"2024-05-06 13:50:11","title":"On isolated hypersurface singularities: algebra-geometric and symplectic aspects","abstract":"These notes are based on a seminar which took place in the autumn of 2022 at the Mathematical Institute of the University of Leiden.   Its goal was to understand the recent work of J. Evans and Y. Lekili on the symplectic cohomology of the Milnor fiber for specific classes of isolated singularities. This work uses inputs from several fields, notably from algebraic geometry, in particular singularity theory, and from symplectic geometry. The main aim of the notes is to make the work of J. Evans and Y. Lekili more accessible by explaining the main ideas from these fields and indicate how these play a role in this work.","sentences":["These notes are based on a seminar which took place in the autumn of 2022 at the Mathematical Institute of the University of Leiden.   ","Its goal was to understand the recent work of J. Evans and Y. Lekili on the symplectic cohomology of the Milnor fiber for specific classes of isolated singularities.","This work uses inputs from several fields, notably from algebraic geometry, in particular singularity theory, and from symplectic geometry.","The main aim of the notes is to make the work of J. Evans and Y. Lekili more accessible by explaining the main ideas from these fields and indicate how these play a role in this work."],"url":"http://arxiv.org/abs/2405.03475v1","category":"math.AG"}
{"created":"2024-05-06 13:29:34","title":"SSyncOA: Self-synchronizing Object-aligned Watermarking to Resist Cropping-paste Attacks","abstract":"Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images. The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation. However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack. With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA. Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively. To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end. Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process. Extensive experiments demonstrate the superiority of our method over other SOTAs.","sentences":["Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images.","The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation.","However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack.","With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA.","Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively.","To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end.","Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process.","Extensive experiments demonstrate the superiority of our method over other SOTAs."],"url":"http://arxiv.org/abs/2405.03458v1","category":"cs.CV"}
{"created":"2024-05-06 13:29:23","title":"Calibration of Uncertainties of the Gaia DR3 Catalog Based on Data on Wide Binary Stars of the Galaxy Field","abstract":"The catalog of wide binary stars by El-Badry et al. (2021) [arXiv:2101.05282], created on the basis of Gaia EDR3 data and including more than a million pairs, was used to analyze Gaia DR3 data obtained independently for their components. By comparison with the model distribution, it is shown that the catalog contains approximately 2.5 times fewer binary stars than would be expected in the absence of spatial incompleteness. It is confirmed that the radius of spatial completeness of the catalog is on average close to 200 pc and depends on the absolute magnitude of the main component. The spatial density of binary stars in the catalog depends weakly on the difference in the magnitudes of the components, and significantly depends on the physical distance between the components. A high correlation between the degree of agreement between the characteristics and the reliability of the pair was found for radial velocities. Qualitative agreement is observed for metallicity [Fe/H] estimates and, to a lesser extent, for absorption A_G estimates. No agreement was found for the ages of the stars, which indicates their great uncertainty in the ensemble, consisting mainly of main sequence stars. Age estimates for pairs with evolved components show significantly better agreement than for the dataset as a whole. Using the parameters of the components of the pairs from Gaia DR3, an independent estimate of the uncertainties in the radial velocities and metallicities depending on the apparent magnitude of the sources was performed. Estimates of the probable median values of errors in the radial velocities and metallicities of Gaia DR3 sources are proposed. Depending on the apparent magnitude, they exceed the median error values given in the catalog: for radial velocities by 1.5-3 times, for metallicities [Fe/H] by 7-25 times.","sentences":["The catalog of wide binary stars by El-Badry et al. (2021)","[arXiv:2101.05282], created on the basis of Gaia EDR3 data and including more than a million pairs, was used to analyze Gaia DR3 data obtained independently for their components.","By comparison with the model distribution, it is shown that the catalog contains approximately 2.5 times fewer binary stars than would be expected in the absence of spatial incompleteness.","It is confirmed that the radius of spatial completeness of the catalog is on average close to 200 pc and depends on the absolute magnitude of the main component.","The spatial density of binary stars in the catalog depends weakly on the difference in the magnitudes of the components, and significantly depends on the physical distance between the components.","A high correlation between the degree of agreement between the characteristics and the reliability of the pair was found for radial velocities.","Qualitative agreement is observed for metallicity [Fe/H] estimates and, to a lesser extent, for absorption A_G estimates.","No agreement was found for the ages of the stars, which indicates their great uncertainty in the ensemble, consisting mainly of main sequence stars.","Age estimates for pairs with evolved components show significantly better agreement than for the dataset as a whole.","Using the parameters of the components of the pairs from Gaia DR3, an independent estimate of the uncertainties in the radial velocities and metallicities depending on the apparent magnitude of the sources was performed.","Estimates of the probable median values of errors in the radial velocities and metallicities of Gaia DR3 sources are proposed.","Depending on the apparent magnitude, they exceed the median error values given in the catalog: for radial velocities by 1.5-3 times, for metallicities [Fe/H] by 7-25 times."],"url":"http://arxiv.org/abs/2405.03457v1","category":"astro-ph.SR"}
{"created":"2024-05-06 13:15:53","title":"Donaldson divisors and spectral invariants","abstract":"We establish a comparison between spectral invariants for a symplectic manifold and a Donaldson divisor therein, and answer a question of Borman from 2012 on the reduction of Entov--Polterovich quasimorphisms, under a reasonable assumption. The method involves a quantitative interpretation of Biran--Khanevsky's quantum Gysin sequence.","sentences":["We establish a comparison between spectral invariants for a symplectic manifold and a Donaldson divisor therein, and answer a question of Borman from 2012 on the reduction of Entov--Polterovich quasimorphisms, under a reasonable assumption.","The method involves a quantitative interpretation of Biran--Khanevsky's quantum Gysin sequence."],"url":"http://arxiv.org/abs/2405.03444v1","category":"math.SG"}
{"created":"2024-05-06 12:44:04","title":"Symplectic torus actions on complete intersections","abstract":"In [20] the author classified complete intersections of dimension $8k$ admitting a Hamiltonian circle action, under a certain assumption on the fixed point set. In this paper, we improve this result in two directions. Firstly, in dimension $8$ we remove the assumption on the fixed point set. Secondly, in any dimension we prove the result under an analogous assumption on the fixed point set. We also give some applications of our methods to the unimodality of Betti numbers of symplectic manifolds having a Hamiltonian $S^1$-action, and discuss the relation to symplectic rationality problems.","sentences":["In [20] the author classified complete intersections of dimension $8k$ admitting a Hamiltonian circle action, under a certain assumption on the fixed point set.","In this paper, we improve this result in two directions.","Firstly, in dimension $8$ we remove the assumption on the fixed point set.","Secondly, in any dimension we prove the result under an analogous assumption on the fixed point set.","We also give some applications of our methods to the unimodality of Betti numbers of symplectic manifolds having a Hamiltonian $S^1$-action, and discuss the relation to symplectic rationality problems."],"url":"http://arxiv.org/abs/2405.03424v1","category":"math.SG"}
{"created":"2024-05-06 12:42:35","title":"Dirichlet problem for degenerate Hessian quotient type curvature equations","abstract":"In the paper, we prove the existence and uniqueness results of the $C^{1,1}$ regular graphic hypersurface for Dirichlet problem of a class of degenerate Hessian quotient type curvature equations under the condition $\\psi^{\\frac{1}{k-l}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$. Specially, we also consider the second order derivative estimates for the corresponding degenerate Hessian type curvature equations under the optimal condition $\\psi^{\\frac{1}{k-1}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$.","sentences":["In the paper, we prove the existence and uniqueness results of the $C^{1,1}$ regular graphic hypersurface for Dirichlet problem of a class of degenerate Hessian quotient type curvature equations under the condition $\\psi^{\\frac{1}{k-l}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$. Specially, we also consider the second order derivative estimates for the corresponding degenerate Hessian type curvature equations under the optimal condition $\\psi^{\\frac{1}{k-1}}\\in C^{1,1}(\\overline{\\Omega}\\times\\mathbb{R}\\times\\mathbb{S}^n)$."],"url":"http://arxiv.org/abs/2405.03422v1","category":"math.AP"}
{"created":"2024-05-06 12:31:34","title":"Mental health of computing professionals and students: A systematic literature review","abstract":"The intersections of mental health and computing education is under-examined. In this systematic literature review, we evaluate the state-of-the-art of research in mental health and well-being interventions, assessments, and concerns like anxiety and depression in computer science and computing education. The studies evaluated occurred across the computing education pipeline from introductory to PhD courses and found some commonalities contributing to high reporting of anxiety and depression in those studied. In addition, interventions that were designed to address mental health topics often revolved around self-guidance. Based on our review of the literature, we recommend increasing sample sizes and focusing on the design and development of tools and interventions specifically designed for computing professionals and students.","sentences":["The intersections of mental health and computing education is under-examined.","In this systematic literature review, we evaluate the state-of-the-art of research in mental health and well-being interventions, assessments, and concerns like anxiety and depression in computer science and computing education.","The studies evaluated occurred across the computing education pipeline from introductory to PhD courses and found some commonalities contributing to high reporting of anxiety and depression in those studied.","In addition, interventions that were designed to address mental health topics often revolved around self-guidance.","Based on our review of the literature, we recommend increasing sample sizes and focusing on the design and development of tools and interventions specifically designed for computing professionals and students."],"url":"http://arxiv.org/abs/2405.03416v1","category":"cs.CY"}
{"created":"2024-05-06 12:21:41","title":"One-side Liouville theorems under an exponential growth condition for Kolmogorov operators","abstract":"It is known that for a possibly degenerate hypoelliptic Ornstein-Uhlenbeck operator $$ L= \\frac{1}{2}\\text{ tr} (QD^2 ) + \\langle Ax, D \\rangle = \\frac{1}{2}\\text{ div} (Q D ) + \\langle Ax, D \\rangle,\\;\\; x \\in R^N, $$ all (globally) bounded solutions of $Lu=0$ on $R^N$ are constant if and only if all the eigenvalues of $A$ have non-positive real parts (i.e., $s(A) \\le 0)$. We show that if $Q$ is positive definite and $s(A) \\le 0$, then any non-negative solution $v$ of $Lv=0$ on $R^N$ which has at most an exponential growth is indeed constant. Thus under a non-degeneracy condition we relax the boundedness assumption on the harmonic functions and maintain the sharp condition on the eigenvalues of $A$. We also prove a related one-side Liouville theorem in the case of hypoelliptic Ornstein-Uhlenbeck operators.","sentences":["It is known that for a possibly degenerate hypoelliptic Ornstein-Uhlenbeck operator $$ L= \\frac{1}{2}\\text{ tr} (QD^2 ) +","\\langle Ax, D \\rangle = \\frac{1}{2}\\text{ div} (Q D )","+","\\langle Ax, D \\rangle,\\;\\; x \\in R^N, $$ all (globally) bounded solutions of $Lu=0$ on $R^N$ are constant if and only if all the eigenvalues of $A$ have non-positive real parts (i.e., $s(A)","\\le 0)$.","We show that if $Q$ is positive definite and $s(A) \\le 0$, then any non-negative solution $v$ of $Lv=0$ on $R^N$ which has at most an exponential growth is indeed constant.","Thus under a non-degeneracy condition we relax the boundedness assumption on the harmonic functions and maintain the sharp condition on the eigenvalues of $A$.","We also prove a related one-side Liouville theorem in the case of hypoelliptic Ornstein-Uhlenbeck operators."],"url":"http://arxiv.org/abs/2405.03410v1","category":"math.AP"}
{"created":"2024-05-06 12:18:41","title":"$k$-convex hypersurfaces with prescribed Weingarten curvature in warped product manifolds","abstract":"In this paper, we consider Weingarten curvature equations for $k$-convex hypersurfaces with $n<2k$ in a warped product manifold $\\overline{M}=I\\times_{\\lambda}M$. Based on the conjecture proposed by Ren-Wang in \\cite{Ren2}, which is valid for $k\\geq n-2$, we derive curvature estimates for equation $\\sigma_k(\\kappa)= \\psi (V, \\nu (V))$ through a straightforward proof. Furthermore, we also obtain an existence result for the star-shaped compact hypersurface $\\Sigma$ satisfying the above equation by the degree theory under some sufficient conditions.","sentences":["In this paper, we consider Weingarten curvature equations for $k$-convex hypersurfaces with $n<2k$ in a warped product manifold $\\overline{M}=I\\times_{\\lambda}M$. Based on the conjecture proposed by Ren-Wang in \\cite{Ren2}, which is valid for $k\\geq n-2$, we derive curvature estimates for equation $\\sigma_k(\\kappa)= \\psi (V, \\nu (V))$ through a straightforward proof.","Furthermore, we also obtain an existence result for the star-shaped compact hypersurface $\\Sigma$ satisfying the above equation by the degree theory under some sufficient conditions."],"url":"http://arxiv.org/abs/2405.03407v1","category":"math.AP"}
{"created":"2024-05-06 12:04:12","title":"Consistent Electroweak Phenomenology of a Nearly Degenerate $Z'$ Boson","abstract":"Extracting constraints on kinetic mixing between a new $U(1)'$ gauge boson hiding under the Standard Model $Z$ boson resonance requires the formalism of non-Hermitian two-point correlation functions at 1-loop order. We derive self-consistent collider constraints on $Z'$ bosons with kinetic mixing in a narrow mass window around the $Z$ boson, considering both model-independent and model-dependent bounds. Our treatment elucidates the importance of both avoided level crossing and width suppression due to the quantum Zeno effect in interpreting the existing constraints. We also discuss the implications for future measurements on the $Z$-pole.","sentences":["Extracting constraints on kinetic mixing between a new $U(1)'$ gauge boson hiding under the Standard Model $Z$ boson resonance requires the formalism of non-Hermitian two-point correlation functions at 1-loop order.","We derive self-consistent collider constraints on $Z'$ bosons with kinetic mixing in a narrow mass window around the $Z$ boson, considering both model-independent and model-dependent bounds.","Our treatment elucidates the importance of both avoided level crossing and width suppression due to the quantum Zeno effect in interpreting the existing constraints.","We also discuss the implications for future measurements on the $Z$-pole."],"url":"http://arxiv.org/abs/2405.03396v1","category":"hep-ph"}
{"created":"2024-05-06 11:29:17","title":"Olfactory search","abstract":"The task of olfactory search is ubiquitous in nature and in technology, from animals in the quest of food or of a mating partner, to robots searching for the source of hazardous fumes in a chemical plant. Here, we focus on the algorithmic approach to this task: we systematically review the different olfactory search strategies. Special emphasis is given to the formal description as a Partially Observable Markov Decision Processes, which allows the computation of optimal actions and helps clarifying the relationships between several effective heuristic search strategies.","sentences":["The task of olfactory search is ubiquitous in nature and in technology, from animals in the quest of food or of a mating partner, to robots searching for the source of hazardous fumes in a chemical plant.","Here, we focus on the algorithmic approach to this task: we systematically review the different olfactory search strategies.","Special emphasis is given to the formal description as a Partially Observable Markov Decision Processes, which allows the computation of optimal actions and helps clarifying the relationships between several effective heuristic search strategies."],"url":"http://arxiv.org/abs/2405.03374v1","category":"physics.bio-ph"}
{"created":"2024-05-06 11:19:23","title":"Equivariant Tannaka-Krein reconstruction and quantum automorphism groups of discrete structures","abstract":"We define quantum automorphism groups of a wide range of discrete structures. The central tool for their construction is a generalisation of the Tannaka-Krein reconstruction theorem. For any direct sum of matrix algebras $M$, and any concrete unitary 2-category of finite type Hilbert-$M$-bimodules $\\mathcal{C}$, under reasonable conditions, we construct an algebraic quantum group $\\mathbb{G}$ which acts on $M$ by $\\alpha$, such that the category of $\\alpha$-equivariant corepresentations of $\\mathbb{G}$ on finite type Hilbert-$M$-bimodules is equivalent to $\\mathcal{C}$. Moreover, we explicitly describe how to get such categories from connected locally finite discrete structures. As an example, we define the quantum automorphism group of a quantum Cayley graph.","sentences":["We define quantum automorphism groups of a wide range of discrete structures.","The central tool for their construction is a generalisation of the Tannaka-Krein reconstruction theorem.","For any direct sum of matrix algebras $M$, and any concrete unitary 2-category of finite type Hilbert-$M$-bimodules $\\mathcal{C}$, under reasonable conditions, we construct an algebraic quantum group $\\mathbb{G}$ which acts on $M$ by $\\alpha$, such that the category of $\\alpha$-equivariant corepresentations of $\\mathbb{G}$ on finite type Hilbert-$M$-bimodules is equivalent to $\\mathcal{C}$. Moreover, we explicitly describe how to get such categories from connected locally finite discrete structures.","As an example, we define the quantum automorphism group of a quantum Cayley graph."],"url":"http://arxiv.org/abs/2405.03364v1","category":"math.OA"}
{"created":"2024-05-06 11:13:50","title":"Telextiles: End-to-end Remote Transmission of Fabric Tactile Sensation","abstract":"The tactile sensation of textiles is critical in determining the comfort of clothing. For remote use, such as online shopping, users cannot physically touch the textile of clothes, making it difficult to evaluate its tactile sensation. Tactile sensing and actuation devices are required to transmit the tactile sensation of textiles. The sensing device needs to recognize different garments, even with hand-held sensors. In addition, the existing actuation device can only present a limited number of known patterns and cannot transmit unknown tactile sensations of textiles. To address these issues, we propose Telextiles, an interface that can remotely transmit tactile sensations of textiles by creating a latent space that reflects the proximity of textiles through contrastive self-supervised learning. We confirm that textiles with similar tactile features are located close to each other in the latent space through a two-dimensional plot. We then compress the latent features for known textile samples into the 1D distance and apply the 16 textile samples to the rollers in the order of the distance. The roller is rotated to select the textile with the closest feature if an unknown textile is detected.","sentences":["The tactile sensation of textiles is critical in determining the comfort of clothing.","For remote use, such as online shopping, users cannot physically touch the textile of clothes, making it difficult to evaluate its tactile sensation.","Tactile sensing and actuation devices are required to transmit the tactile sensation of textiles.","The sensing device needs to recognize different garments, even with hand-held sensors.","In addition, the existing actuation device can only present a limited number of known patterns and cannot transmit unknown tactile sensations of textiles.","To address these issues, we propose Telextiles, an interface that can remotely transmit tactile sensations of textiles by creating a latent space that reflects the proximity of textiles through contrastive self-supervised learning.","We confirm that textiles with similar tactile features are located close to each other in the latent space through a two-dimensional plot.","We then compress the latent features for known textile samples into the 1D distance and apply the 16 textile samples to the rollers in the order of the distance.","The roller is rotated to select the textile with the closest feature if an unknown textile is detected."],"url":"http://arxiv.org/abs/2405.03363v1","category":"cs.HC"}
{"created":"2024-05-06 10:59:15","title":"Retinexmamba: Retinex-based Mamba for Low-light Image Enhancement","abstract":"In the field of low-light image enhancement, both traditional Retinex methods and advanced deep learning techniques such as Retinexformer have shown distinct advantages and limitations. Traditional Retinex methods, designed to mimic the human eye's perception of brightness and color, decompose images into illumination and reflection components but struggle with noise management and detail preservation under low light conditions. Retinexformer enhances illumination estimation through traditional self-attention mechanisms, but faces challenges with insufficient interpretability and suboptimal enhancement effects. To overcome these limitations, this paper introduces the RetinexMamba architecture. RetinexMamba not only captures the physical intuitiveness of traditional Retinex methods but also integrates the deep learning framework of Retinexformer, leveraging the computational efficiency of State Space Models (SSMs) to enhance processing speed. This architecture features innovative illumination estimators and damage restorer mechanisms that maintain image quality during enhancement. Moreover, RetinexMamba replaces the IG-MSA (Illumination-Guided Multi-Head Attention) in Retinexformer with a Fused-Attention mechanism, improving the model's interpretability. Experimental evaluations on the LOL dataset show that RetinexMamba outperforms existing deep learning approaches based on Retinex theory in both quantitative and qualitative metrics, confirming its effectiveness and superiority in enhancing low-light images.","sentences":["In the field of low-light image enhancement, both traditional Retinex methods and advanced deep learning techniques such as Retinexformer have shown distinct advantages and limitations.","Traditional Retinex methods, designed to mimic the human eye's perception of brightness and color, decompose images into illumination and reflection components but struggle with noise management and detail preservation under low light conditions.","Retinexformer enhances illumination estimation through traditional self-attention mechanisms, but faces challenges with insufficient interpretability and suboptimal enhancement effects.","To overcome these limitations, this paper introduces the RetinexMamba architecture.","RetinexMamba not only captures the physical intuitiveness of traditional Retinex methods but also integrates the deep learning framework of Retinexformer, leveraging the computational efficiency of State Space Models (SSMs) to enhance processing speed.","This architecture features innovative illumination estimators and damage restorer mechanisms that maintain image quality during enhancement.","Moreover, RetinexMamba replaces the IG-MSA (Illumination-Guided Multi-Head Attention) in Retinexformer with a Fused-Attention mechanism, improving the model's interpretability.","Experimental evaluations on the LOL dataset show that RetinexMamba outperforms existing deep learning approaches based on Retinex theory in both quantitative and qualitative metrics, confirming its effectiveness and superiority in enhancing low-light images."],"url":"http://arxiv.org/abs/2405.03349v1","category":"cs.CV"}
{"created":"2024-05-06 10:09:35","title":"Policy Learning for Balancing Short-Term and Long-Term Rewards","abstract":"Empirical researchers and decision-makers spanning various domains frequently seek profound insights into the long-term impacts of interventions. While the significance of long-term outcomes is undeniable, an overemphasis on them may inadvertently overshadow short-term gains. Motivated by this, this paper formalizes a new framework for learning the optimal policy that effectively balances both long-term and short-term rewards, where some long-term outcomes are allowed to be missing. In particular, we first present the identifiability of both rewards under mild assumptions. Next, we deduce the semiparametric efficiency bounds, along with the consistency and asymptotic normality of their estimators. We also reveal that short-term outcomes, if associated, contribute to improving the estimator of the long-term reward. Based on the proposed estimators, we develop a principled policy learning approach and further derive the convergence rates of regret and estimation errors associated with the learned policy. Extensive experiments are conducted to validate the effectiveness of the proposed method, demonstrating its practical applicability.","sentences":["Empirical researchers and decision-makers spanning various domains frequently seek profound insights into the long-term impacts of interventions.","While the significance of long-term outcomes is undeniable, an overemphasis on them may inadvertently overshadow short-term gains.","Motivated by this, this paper formalizes a new framework for learning the optimal policy that effectively balances both long-term and short-term rewards, where some long-term outcomes are allowed to be missing.","In particular, we first present the identifiability of both rewards under mild assumptions.","Next, we deduce the semiparametric efficiency bounds, along with the consistency and asymptotic normality of their estimators.","We also reveal that short-term outcomes, if associated, contribute to improving the estimator of the long-term reward.","Based on the proposed estimators, we develop a principled policy learning approach and further derive the convergence rates of regret and estimation errors associated with the learned policy.","Extensive experiments are conducted to validate the effectiveness of the proposed method, demonstrating its practical applicability."],"url":"http://arxiv.org/abs/2405.03329v1","category":"cs.LG"}
{"created":"2024-05-06 09:38:11","title":"Locally semicomplete weakly distance-regular digraphs","abstract":"A digraph is semicomplete if any two vertices are connected by at least one arc and is locally semicomplete if the out-neighbourhood (resp. in-neighbourhood) of any vertex induces a semicomplete digraph. In this paper, we characterize all locally semicomplete weakly distance-regular digraphs under the assumption of commutativity.","sentences":["A digraph is semicomplete if any two vertices are connected by at least one arc and is locally semicomplete if the out-neighbourhood (resp.","in-neighbourhood) of any vertex induces a semicomplete digraph.","In this paper, we characterize all locally semicomplete weakly distance-regular digraphs under the assumption of commutativity."],"url":"http://arxiv.org/abs/2405.03310v1","category":"math.CO"}
{"created":"2024-05-06 08:49:37","title":"Distributed Adaptive Spatial Filtering with Inexact Local Solvers","abstract":"The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network. The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available. In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method. We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate. We also provide numerical simulations to validate these theoretical results.","sentences":["The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network.","The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available.","In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method.","We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate.","We also provide numerical simulations to validate these theoretical results."],"url":"http://arxiv.org/abs/2405.03277v1","category":"eess.SP"}
{"created":"2024-05-06 08:44:17","title":"Evaluation of Drivers' Interaction Ability at Social Scenarios: A Process-Based Framework","abstract":"Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles. In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions. To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring. We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance. By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically. We validated our framework at unsignalized intersections as a typical scenario. Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings.","sentences":["Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles.","In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions.","To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring.","We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance.","By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically.","We validated our framework at unsignalized intersections as a typical scenario.","Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings."],"url":"http://arxiv.org/abs/2405.03273v1","category":"cs.RO"}
{"created":"2024-05-06 07:40:13","title":"TED: Accelerate Model Training by Internal Generalization","abstract":"Large language models have demonstrated strong performance in recent years, but the high cost of training drives the need for efficient methods to compress dataset sizes. We propose TED pruning, a method that addresses the challenge of overfitting under high pruning ratios by quantifying the model's ability to improve performance on pruned data while fitting retained data, known as Internal Generalization (IG). TED uses an optimization objective based on Internal Generalization Distance (IGD), measuring changes in IG before and after pruning to align with true generalization performance and achieve implicit regularization. The IGD optimization objective was verified to allow the model to achieve the smallest upper bound on generalization error. The impact of small mask fluctuations on IG is studied through masks and Taylor approximation, and fast estimation of IGD is enabled. In analyzing continuous training dynamics, the prior effect of IGD is validated, and a progressive pruning strategy is proposed. Experiments on image classification, natural language understanding, and large language model fine-tuning show TED achieves lossless performance with 60-70\\% of the data. Upon acceptance, our code will be made publicly available.","sentences":["Large language models have demonstrated strong performance in recent years, but the high cost of training drives the need for efficient methods to compress dataset sizes.","We propose TED pruning, a method that addresses the challenge of overfitting under high pruning ratios by quantifying the model's ability to improve performance on pruned data while fitting retained data, known as Internal Generalization (IG).","TED uses an optimization objective based on Internal Generalization Distance (IGD), measuring changes in IG before and after pruning to align with true generalization performance and achieve implicit regularization.","The IGD optimization objective was verified to allow the model to achieve the smallest upper bound on generalization error.","The impact of small mask fluctuations on IG is studied through masks and Taylor approximation, and fast estimation of IGD is enabled.","In analyzing continuous training dynamics, the prior effect of IGD is validated, and a progressive pruning strategy is proposed.","Experiments on image classification, natural language understanding, and large language model fine-tuning show TED achieves lossless performance with 60-70\\% of the data.","Upon acceptance, our code will be made publicly available."],"url":"http://arxiv.org/abs/2405.03228v1","category":"cs.LG"}
{"created":"2024-05-06 07:30:31","title":"Spatial and Surface Correspondence Field for Interaction Transfer","abstract":"In this paper, we introduce a new method for the task of interaction transfer. Given an example interaction between a source object and an agent, our method can automatically infer both surface and spatial relationships for the agent and target objects within the same category, yielding more accurate and valid transfers. Specifically, our method characterizes the example interaction using a combined spatial and surface representation. We correspond the agent points and object points related to the representation to the target object space using a learned spatial and surface correspondence field, which represents objects as deformed and rotated signed distance fields. With the corresponded points, an optimization is performed under the constraints of our spatial and surface interaction representation and additional regularization. Experiments conducted on human-chair and hand-mug interaction transfer tasks show that our approach can handle larger geometry and topology variations between source and target shapes, significantly outperforming state-of-the-art methods.","sentences":["In this paper, we introduce a new method for the task of interaction transfer.","Given an example interaction between a source object and an agent, our method can automatically infer both surface and spatial relationships for the agent and target objects within the same category, yielding more accurate and valid transfers.","Specifically, our method characterizes the example interaction using a combined spatial and surface representation.","We correspond the agent points and object points related to the representation to the target object space using a learned spatial and surface correspondence field, which represents objects as deformed and rotated signed distance fields.","With the corresponded points, an optimization is performed under the constraints of our spatial and surface interaction representation and additional regularization.","Experiments conducted on human-chair and hand-mug interaction transfer tasks show that our approach can handle larger geometry and topology variations between source and target shapes, significantly outperforming state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.03221v1","category":"cs.CV"}
{"created":"2024-05-06 07:20:20","title":"Asymptotic behavior toward viscous shock for impermeable wall and inflow problem of barotropic Navier-Stokes equations","abstract":"We consider the compressible barotropic Navier-Stokes equations in a half-line and study the time-asymptotic behavior toward the outgoing viscous shock wave. Precisely, we consider the two boundary problems: impermeable wall and inflow problems, where the velocity at the boundary is given as a constant state. For both problems, when the asymptotic profile determined by the prescribed constant states at the boundary and far-fields is a viscous shock, we show that the solution asymptotically converges to the shifted viscous shock profiles uniformly in space, under the condition that initial perturbation is small enough in H1 norm. We do not impose the zero mass condition on initial data, which improves the previous results by Matsumura and Mei [20] for impermeable case, and by Huang, Matsumura and Shi [8] for inflow case. Moreover, for the inflow case, we remove the assumption in [8]. Our results are based on the method of a-contraction with shifts, as the first extension of the method to the boundary value problems.","sentences":["We consider the compressible barotropic Navier-Stokes equations in a half-line and study the time-asymptotic behavior toward the outgoing viscous shock wave.","Precisely, we consider the two boundary problems: impermeable wall and inflow problems, where the velocity at the boundary is given as a constant state.","For both problems, when the asymptotic profile determined by the prescribed constant states at the boundary and far-fields is a viscous shock, we show that the solution asymptotically converges to the shifted viscous shock profiles uniformly in space, under the condition that initial perturbation is small enough in H1 norm.","We do not impose the zero mass condition on initial data, which improves the previous results by Matsumura and Mei","[20] for impermeable case, and by Huang, Matsumura and Shi","[8] for inflow case.","Moreover, for the inflow case, we remove the assumption in [8].","Our results are based on the method of a-contraction with shifts, as the first extension of the method to the boundary value problems."],"url":"http://arxiv.org/abs/2405.03214v1","category":"math.AP"}
{"created":"2024-05-06 06:45:23","title":"StyleSeg V2: Towards Robust One-shot Segmentation of Brain Tissue via Optimization-free Registration Error Perception","abstract":"One-shot segmentation of brain tissue requires training registration-segmentation (reg-seg) dual-model iteratively, where reg-model aims to provide pseudo masks of unlabeled images for seg-model by warping a carefully-labeled atlas. However, the imperfect reg-model induces image-mask misalignment, poisoning the seg-model subsequently. Recent StyleSeg bypasses this bottleneck by replacing the unlabeled images with their warped copies of atlas, but needs to borrow the diverse image patterns via style transformation. Here, we present StyleSeg V2, inherited from StyleSeg but granted the ability of perceiving the registration errors. The motivation is that good registration behaves in a mirrored fashion for mirrored images. Therefore, almost at no cost, StyleSeg V2 can have reg-model itself \"speak out\" incorrectly-aligned regions by simply mirroring (symmetrically flipping the brain) its input, and the registration errors are symmetric inconsistencies between the outputs of original and mirrored inputs. Consequently, StyleSeg V2 allows the seg-model to make use of correctly-aligned regions of unlabeled images and also enhances the fidelity of style-transformed warped atlas image by weighting the local transformation strength according to registration errors. The experimental results on three public datasets demonstrate that our proposed StyleSeg V2 outperforms other state-of-the-arts by considerable margins, and exceeds StyleSeg by increasing the average Dice by at least 2.4%.","sentences":["One-shot segmentation of brain tissue requires training registration-segmentation (reg-seg) dual-model iteratively, where reg-model aims to provide pseudo masks of unlabeled images for seg-model by warping a carefully-labeled atlas.","However, the imperfect reg-model induces image-mask misalignment, poisoning the seg-model subsequently.","Recent StyleSeg bypasses this bottleneck by replacing the unlabeled images with their warped copies of atlas, but needs to borrow the diverse image patterns via style transformation.","Here, we present StyleSeg V2, inherited from StyleSeg but granted the ability of perceiving the registration errors.","The motivation is that good registration behaves in a mirrored fashion for mirrored images.","Therefore, almost at no cost, StyleSeg V2 can have reg-model itself \"speak out\" incorrectly-aligned regions by simply mirroring (symmetrically flipping the brain) its input, and the registration errors are symmetric inconsistencies between the outputs of original and mirrored inputs.","Consequently, StyleSeg V2 allows the seg-model to make use of correctly-aligned regions of unlabeled images and also enhances the fidelity of style-transformed warped atlas image by weighting the local transformation strength according to registration errors.","The experimental results on three public datasets demonstrate that our proposed StyleSeg V2 outperforms other state-of-the-arts by considerable margins, and exceeds StyleSeg by increasing the average Dice by at least 2.4%."],"url":"http://arxiv.org/abs/2405.03197v1","category":"cs.CV"}
{"created":"2024-05-06 06:26:24","title":"On the invariants of L-functions of degree 2, I: twisted degree and internal shift","abstract":"This is the first part of a series of papers where the behaviour of the invariants under twist by Dirichlet characters is studied for $L$-functions of degree 2. Here we show, under suitable conditions, that degree and internal shift remain unchanged under twist. The ultimate goal of the series is to prove a general version of Weil converse theorem with minimal assumptions on the shape of the functional equation of the twists.","sentences":["This is the first part of a series of papers where the behaviour of the invariants under twist by Dirichlet characters is studied for $L$-functions of degree 2.","Here we show, under suitable conditions, that degree and internal shift remain unchanged under twist.","The ultimate goal of the series is to prove a general version of Weil converse theorem with minimal assumptions on the shape of the functional equation of the twists."],"url":"http://arxiv.org/abs/2405.03186v1","category":"math.NT"}
{"created":"2024-05-06 05:47:27","title":"Performance Upper Bound of the Grover-Mixer Quantum Alternating Operator Ansatz","abstract":"The Quantum Alternating Operator Ansatz (QAOA) represents a branch of quantum algorithms designed for solving combinatorial optimization problems. A specific variant, the Grover-Mixer Quantum Alternating Operator Ansatz (GM-QAOA), ensures uniform amplitude across states that share equivalent objective values. This property makes the algorithm independent of the problem structure, focusing instead on the distribution of objective values within the problem. In this work, we prove the probability upper bound for measuring a computational basis state from a GM-QAOA circuit with a given depth, which is a critical factor in QAOA cost. From this, we derive the upper bounds for the probability of sampling an optimal solution and for the approximation ratio of maximum optimization problems, based on the objective value distribution. Using numerical analysis, we link the distribution to the problem size and build the regression models that relate the problem size, QAOA depth, and performance upper bound. Our results suggest that the GM-QAOA provides a quadratic enhancement in sampling probability and requires circuit depth that scales exponentially with problem size to maintain consistent performance.","sentences":["The Quantum Alternating Operator Ansatz (QAOA) represents a branch of quantum algorithms designed for solving combinatorial optimization problems.","A specific variant, the Grover-Mixer Quantum","Alternating Operator Ansatz (GM-QAOA), ensures uniform amplitude across states that share equivalent objective values.","This property makes the algorithm independent of the problem structure, focusing instead on the distribution of objective values within the problem.","In this work, we prove the probability upper bound for measuring a computational basis state from a GM-QAOA circuit with a given depth, which is a critical factor in QAOA cost.","From this, we derive the upper bounds for the probability of sampling an optimal solution and for the approximation ratio of maximum optimization problems, based on the objective value distribution.","Using numerical analysis, we link the distribution to the problem size and build the regression models that relate the problem size, QAOA depth, and performance upper bound.","Our results suggest that the GM-QAOA provides a quadratic enhancement in sampling probability and requires circuit depth that scales exponentially with problem size to maintain consistent performance."],"url":"http://arxiv.org/abs/2405.03173v1","category":"quant-ph"}
{"created":"2024-05-06 05:36:29","title":"Oracle-Checker Scheme for Evaluating a Generative Large Language Model","abstract":"This work presents a novel approach called oracle-checker scheme for evaluating the answer given by a generative large language model (LLM). Two types of checkers are presented. The first type of checker follows the idea of property testing. The second type of checker follows the idea of program checking. Their applications are demonstrated in two separate contexts, entity extraction and paraphrase decision, respectively.","sentences":["This work presents a novel approach called oracle-checker scheme for evaluating the answer given by a generative large language model (LLM).","Two types of checkers are presented.","The first type of checker follows the idea of property testing.","The second type of checker follows the idea of program checking.","Their applications are demonstrated in two separate contexts, entity extraction and paraphrase decision, respectively."],"url":"http://arxiv.org/abs/2405.03170v1","category":"cs.CL"}
{"created":"2024-05-06 04:37:36","title":"Moore Determinant of Dual Quaternion Hermitian Matrices","abstract":"In this paper, we extend the Chen and Moore determinants of quaternion Hermitian matrices to dual quaternion Hermitian matrices. We show the Chen determinant of dual quaternion Hermitian matrix is invariant under the addition, switching, multiplication, and unitary operations at the both hand sides. We then show the Chen and Moore determinants are equal to each other, and they are also equal to the products of eigenvalues. The characteristic polynomial of a dual quaternion Hermitian matrix is also studied.","sentences":["In this paper, we extend the Chen and Moore determinants of quaternion Hermitian matrices to dual quaternion Hermitian matrices.","We show the Chen determinant of dual quaternion Hermitian matrix is invariant under the addition, switching, multiplication, and unitary operations at the both hand sides.","We then show the Chen and Moore determinants are equal to each other, and they are also equal to the products of eigenvalues.","The characteristic polynomial of a dual quaternion Hermitian matrix is also studied."],"url":"http://arxiv.org/abs/2405.03160v1","category":"math.RA"}
{"created":"2024-05-06 04:05:19","title":"MMGER: Multi-modal and Multi-granularity Generative Error Correction with LLM for Joint Accent and Speech Recognition","abstract":"Despite notable advancements in automatic speech recognition (ASR), performance tends to degrade when faced with adverse conditions. Generative error correction (GER) leverages the exceptional text comprehension capabilities of large language models (LLM), delivering impressive performance in ASR error correction, where N-best hypotheses provide valuable information for transcription prediction. However, GER encounters challenges such as fixed N-best hypotheses, insufficient utilization of acoustic information, and limited specificity to multi-accent scenarios. In this paper, we explore the application of GER in multi-accent scenarios. Accents represent deviations from standard pronunciation norms, and the multi-task learning framework for simultaneous ASR and accent recognition (AR) has effectively addressed the multi-accent scenarios, making it a prominent solution. In this work, we propose a unified ASR-AR GER model, named MMGER, leveraging multi-modal correction, and multi-granularity correction. Multi-task ASR-AR learning is employed to provide dynamic 1-best hypotheses and accent embeddings. Multi-modal correction accomplishes fine-grained frame-level correction by force-aligning the acoustic features of speech with the corresponding character-level 1-best hypothesis sequence. Multi-granularity correction supplements the global linguistic information by incorporating regular 1-best hypotheses atop fine-grained multi-modal correction to achieve coarse-grained utterance-level correction. MMGER effectively mitigates the limitations of GER and tailors LLM-based ASR error correction for the multi-accent scenarios. Experiments conducted on the multi-accent Mandarin KeSpeech dataset demonstrate the efficacy of MMGER, achieving a 26.72% relative improvement in AR accuracy and a 27.55% relative reduction in ASR character error rate, compared to a well-established standard baseline.","sentences":["Despite notable advancements in automatic speech recognition (ASR), performance tends to degrade when faced with adverse conditions.","Generative error correction (GER) leverages the exceptional text comprehension capabilities of large language models (LLM), delivering impressive performance in ASR error correction, where N-best hypotheses provide valuable information for transcription prediction.","However, GER encounters challenges such as fixed N-best hypotheses, insufficient utilization of acoustic information, and limited specificity to multi-accent scenarios.","In this paper, we explore the application of GER in multi-accent scenarios.","Accents represent deviations from standard pronunciation norms, and the multi-task learning framework for simultaneous ASR and accent recognition (AR) has effectively addressed the multi-accent scenarios, making it a prominent solution.","In this work, we propose a unified ASR-AR GER model, named MMGER, leveraging multi-modal correction, and multi-granularity correction.","Multi-task ASR-AR learning is employed to provide dynamic 1-best hypotheses and accent embeddings.","Multi-modal correction accomplishes fine-grained frame-level correction by force-aligning the acoustic features of speech with the corresponding character-level 1-best hypothesis sequence.","Multi-granularity correction supplements the global linguistic information by incorporating regular 1-best hypotheses atop fine-grained multi-modal correction to achieve coarse-grained utterance-level correction.","MMGER effectively mitigates the limitations of GER and tailors LLM-based ASR error correction for the multi-accent scenarios.","Experiments conducted on the multi-accent Mandarin KeSpeech dataset demonstrate the efficacy of MMGER, achieving a 26.72% relative improvement in AR accuracy and a 27.55% relative reduction in ASR character error rate, compared to a well-established standard baseline."],"url":"http://arxiv.org/abs/2405.03152v1","category":"eess.AS"}
{"created":"2024-05-06 03:32:22","title":"Weak diamond and pcf theory","abstract":"We obtain bounds on the cardinality of $pcf(\\mathfrak{a})$ from instances of weak diamond. Consequently, under mild assumptions there are many singular cardinals of the from $\\aleph_\\delta$ for which $2^{\\aleph_\\delta}<\\aleph_{(|\\delta|^{+3})}$. For example, if every limit cardinal is a strong limit cardinal then this bound holds at a class of singular cardinals.","sentences":["We obtain bounds on the cardinality of $pcf(\\mathfrak{a})$ from instances of weak diamond.","Consequently, under mild assumptions there are many singular cardinals of the from $\\aleph_\\delta$ for which $2^{\\aleph_\\delta}<\\aleph_{(|\\delta|^{+3})}$. For example, if every limit cardinal is a strong limit cardinal then this bound holds at a class of singular cardinals."],"url":"http://arxiv.org/abs/2405.03142v1","category":"math.LO"}
{"created":"2024-05-06 03:12:36","title":"FOBNN: Fast Oblivious Binarized Neural Network Inference","abstract":"The superior performance of deep learning has propelled the rise of Deep Learning as a Service, enabling users to transmit their private data to service providers for model execution and inference retrieval. Nevertheless, the primary concern remains safeguarding the confidentiality of sensitive user data while optimizing the efficiency of secure protocols. To address this, we develop a fast oblivious binarized neural network inference framework, FOBNN. Specifically, we customize binarized convolutional neural networks to enhance oblivious inference, design two fast algorithms for binarized convolutions, and optimize network structures experimentally under constrained costs. Initially, we meticulously analyze the range of intermediate values in binarized convolutions to minimize bit representation, resulting in the Bit Length Bounding (BLB) algorithm. Subsequently, leveraging the efficiency of bitwise operations in BLB, we further enhance performance by employing pure bitwise operations for each binary digit position, yielding the Layer-wise Bit Accumulation (LBA) algorithm. Theoretical analysis validates FOBNN's security and indicates up to $2 \\times$ improvement in computational and communication costs compared to the state-of-the-art method. We demonstrates our framework's effectiveness in RNA function prediction within bioinformatics. Rigorous experimental assessments confirm that our oblivious inference solutions not only maintain but often exceed the original accuracy, surpassing prior efforts.","sentences":["The superior performance of deep learning has propelled the rise of Deep Learning as a Service, enabling users to transmit their private data to service providers for model execution and inference retrieval.","Nevertheless, the primary concern remains safeguarding the confidentiality of sensitive user data while optimizing the efficiency of secure protocols.","To address this, we develop a fast oblivious binarized neural network inference framework, FOBNN.","Specifically, we customize binarized convolutional neural networks to enhance oblivious inference, design two fast algorithms for binarized convolutions, and optimize network structures experimentally under constrained costs.","Initially, we meticulously analyze the range of intermediate values in binarized convolutions to minimize bit representation, resulting in the Bit Length Bounding (BLB) algorithm.","Subsequently, leveraging the efficiency of bitwise operations in BLB, we further enhance performance by employing pure bitwise operations for each binary digit position, yielding the Layer-wise Bit Accumulation (LBA) algorithm.","Theoretical analysis validates FOBNN's security and indicates up to $2 \\times$ improvement in computational and communication costs compared to the state-of-the-art method.","We demonstrates our framework's effectiveness in RNA function prediction within bioinformatics.","Rigorous experimental assessments confirm that our oblivious inference solutions not only maintain but often exceed the original accuracy, surpassing prior efforts."],"url":"http://arxiv.org/abs/2405.03136v1","category":"cs.CR"}
{"created":"2024-05-06 03:06:04","title":"A Multi-Agent Rollout Approach for Highway Bottleneck Decongenston in Mixed Autonomy","abstract":"The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility. This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment. The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time. We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm. By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies. Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate.","sentences":["The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility.","This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment.","The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time.","We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm.","By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies.","Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate."],"url":"http://arxiv.org/abs/2405.03132v1","category":"cs.MA"}
{"created":"2024-05-06 02:35:29","title":"Revealing Decision Conservativeness Through Inverse Distributionally Robust Optimization","abstract":"This paper introduces Inverse Distributionally Robust Optimization (I-DRO) as a method to infer the conservativeness level of a decision-maker, represented by the size of a Wasserstein metric-based ambiguity set, from the optimal decisions made using Forward Distributionally Robust Optimization (F-DRO). By leveraging the Karush-Kuhn-Tucker (KKT) conditions of the convex F-DRO model, we formulate I-DRO as a bi-linear program, which can be solved using off-the-shelf optimization solvers. Additionally, this formulation exhibits several advantageous properties. We demonstrate that I-DRO not only guarantees the existence and uniqueness of an optimal solution but also establishes the necessary and sufficient conditions for this optimal solution to accurately match the actual conservativeness level in F-DRO. Furthermore, we identify three extreme scenarios that may impact I-DRO effectiveness. Our case study applies F-DRO for power system scheduling under uncertainty and employs I-DRO to recover the conservativeness level of system operators. Numerical experiments based on an IEEE 5-bus system and a realistic NYISO 11-zone system demonstrate I-DRO performance in both normal and extreme scenarios.","sentences":["This paper introduces Inverse Distributionally Robust Optimization (I-DRO) as a method to infer the conservativeness level of a decision-maker, represented by the size of a Wasserstein metric-based ambiguity set, from the optimal decisions made using Forward Distributionally Robust Optimization (F-DRO).","By leveraging the Karush-Kuhn-Tucker (KKT) conditions of the convex F-DRO model, we formulate I-DRO as a bi-linear program, which can be solved using off-the-shelf optimization solvers.","Additionally, this formulation exhibits several advantageous properties.","We demonstrate that I-DRO not only guarantees the existence and uniqueness of an optimal solution but also establishes the necessary and sufficient conditions for this optimal solution to accurately match the actual conservativeness level in F-DRO.","Furthermore, we identify three extreme scenarios that may impact I-DRO effectiveness.","Our case study applies F-DRO for power system scheduling under uncertainty and employs I-DRO to recover the conservativeness level of system operators.","Numerical experiments based on an IEEE 5-bus system and a realistic NYISO 11-zone system demonstrate I-DRO performance in both normal and extreme scenarios."],"url":"http://arxiv.org/abs/2405.03123v1","category":"math.OC"}
{"created":"2024-05-06 02:23:34","title":"Determined Multichannel Blind Source Separation with Clustered Source Model","abstract":"The independent low-rank matrix analysis (ILRMA) method stands out as a prominent technique for multichannel blind audio source separation. It leverages nonnegative matrix factorization (NMF) and nonnegative canonical polyadic decomposition (NCPD) to model source parameters. While it effectively captures the low-rank structure of sources, the NMF model overlooks inter-channel dependencies. On the other hand, NCPD preserves intrinsic structure but lacks interpretable latent factors, making it challenging to incorporate prior information as constraints. To address these limitations, we introduce a clustered source model based on nonnegative block-term decomposition (NBTD). This model defines blocks as outer products of vectors (clusters) and matrices (for spectral structure modeling), offering interpretable latent vectors. Moreover, it enables straightforward integration of orthogonality constraints to ensure independence among source images. Experimental results demonstrate that our proposed method outperforms ILRMA and its extensions in anechoic conditions and surpasses the original ILRMA in simulated reverberant environments.","sentences":["The independent low-rank matrix analysis (ILRMA) method stands out as a prominent technique for multichannel blind audio source separation.","It leverages nonnegative matrix factorization (NMF) and nonnegative canonical polyadic decomposition (NCPD) to model source parameters.","While it effectively captures the low-rank structure of sources, the NMF model overlooks inter-channel dependencies.","On the other hand, NCPD preserves intrinsic structure but lacks interpretable latent factors, making it challenging to incorporate prior information as constraints.","To address these limitations, we introduce a clustered source model based on nonnegative block-term decomposition (NBTD).","This model defines blocks as outer products of vectors (clusters) and matrices (for spectral structure modeling), offering interpretable latent vectors.","Moreover, it enables straightforward integration of orthogonality constraints to ensure independence among source images.","Experimental results demonstrate that our proposed method outperforms ILRMA and its extensions in anechoic conditions and surpasses the original ILRMA in simulated reverberant environments."],"url":"http://arxiv.org/abs/2405.03118v1","category":"cs.SD"}
{"created":"2024-05-06 02:02:57","title":"Intra-task Mutual Attention based Vision Transformer for Few-Shot Learning","abstract":"Humans possess remarkable ability to accurately classify new, unseen images after being exposed to only a few examples. Such ability stems from their capacity to identify common features shared between new and previously seen images while disregarding distractions such as background variations. However, for artificial neural network models, determining the most relevant features for distinguishing between two images with limited samples presents a challenge. In this paper, we propose an intra-task mutual attention method for few-shot learning, that involves splitting the support and query samples into patches and encoding them using the pre-trained Vision Transformer (ViT) architecture. Specifically, we swap the class (CLS) token and patch tokens between the support and query sets to have the mutual attention, which enables each set to focus on the most useful information. This facilitates the strengthening of intra-class representations and promotes closer proximity between instances of the same class. For implementation, we adopt the ViT-based network architecture and utilize pre-trained model parameters obtained through self-supervision. By leveraging Masked Image Modeling as a self-supervised training task for pre-training, the pre-trained model yields semantically meaningful representations while successfully avoiding supervision collapse. We then employ a meta-learning method to fine-tune the last several layers and CLS token modules. Our strategy significantly reduces the num- ber of parameters that require fine-tuning while effectively uti- lizing the capability of pre-trained model. Extensive experiments show that our framework is simple, effective and computationally efficient, achieving superior performance as compared to the state-of-the-art baselines on five popular few-shot classification benchmarks under the 5-shot and 1-shot scenarios","sentences":["Humans possess remarkable ability to accurately classify new, unseen images after being exposed to only a few examples.","Such ability stems from their capacity to identify common features shared between new and previously seen images while disregarding distractions such as background variations.","However, for artificial neural network models, determining the most relevant features for distinguishing between two images with limited samples presents a challenge.","In this paper, we propose an intra-task mutual attention method for few-shot learning, that involves splitting the support and query samples into patches and encoding them using the pre-trained Vision Transformer (ViT) architecture.","Specifically, we swap the class (CLS) token and patch tokens between the support and query sets to have the mutual attention, which enables each set to focus on the most useful information.","This facilitates the strengthening of intra-class representations and promotes closer proximity between instances of the same class.","For implementation, we adopt the ViT-based network architecture and utilize pre-trained model parameters obtained through self-supervision.","By leveraging Masked Image Modeling as a self-supervised training task for pre-training, the pre-trained model yields semantically meaningful representations while successfully avoiding supervision collapse.","We then employ a meta-learning method to fine-tune the last several layers and CLS token modules.","Our strategy significantly reduces the num- ber of parameters that require fine-tuning while effectively uti- lizing the capability of pre-trained model.","Extensive experiments show that our framework is simple, effective and computationally efficient, achieving superior performance as compared to the state-of-the-art baselines on five popular few-shot classification benchmarks under the 5-shot and 1-shot scenarios"],"url":"http://arxiv.org/abs/2405.03109v1","category":"cs.CV"}
{"created":"2024-05-06 01:18:36","title":"Loss Jump During Loss Switch in Solving PDEs with Neural Networks","abstract":"Using neural networks to solve partial differential equations (PDEs) is gaining popularity as an alternative approach in the scientific computing community. Neural networks can integrate different types of information into the loss function. These include observation data, governing equations, and variational forms, etc. These loss functions can be broadly categorized into two types: observation data loss directly constrains and measures the model output, while other loss functions indirectly model the performance of the network, which can be classified as model loss. However, this alternative approach lacks a thorough understanding of its underlying mechanisms, including theoretical foundations and rigorous characterization of various phenomena. This work focuses on investigating how different loss functions impact the training of neural networks for solving PDEs. We discover a stable loss-jump phenomenon: when switching the loss function from the data loss to the model loss, which includes different orders of derivative information, the neural network solution significantly deviates from the exact solution immediately. Further experiments reveal that this phenomenon arises from the different frequency preferences of neural networks under different loss functions. We theoretically analyze the frequency preference of neural networks under model loss. This loss-jump phenomenon provides a valuable perspective for examining the underlying mechanisms of neural networks in solving PDEs.","sentences":["Using neural networks to solve partial differential equations (PDEs) is gaining popularity as an alternative approach in the scientific computing community.","Neural networks can integrate different types of information into the loss function.","These include observation data, governing equations, and variational forms, etc.","These loss functions can be broadly categorized into two types: observation data loss directly constrains and measures the model output, while other loss functions indirectly model the performance of the network, which can be classified as model loss.","However, this alternative approach lacks a thorough understanding of its underlying mechanisms, including theoretical foundations and rigorous characterization of various phenomena.","This work focuses on investigating how different loss functions impact the training of neural networks for solving PDEs.","We discover a stable loss-jump phenomenon: when switching the loss function from the data loss to the model loss, which includes different orders of derivative information, the neural network solution significantly deviates from the exact solution immediately.","Further experiments reveal that this phenomenon arises from the different frequency preferences of neural networks under different loss functions.","We theoretically analyze the frequency preference of neural networks under model loss.","This loss-jump phenomenon provides a valuable perspective for examining the underlying mechanisms of neural networks in solving PDEs."],"url":"http://arxiv.org/abs/2405.03095v1","category":"cs.LG"}
{"created":"2024-05-06 01:11:25","title":"Local-projective-measurement-enhanced quantum battery capacity","abstract":"Quantum batteries have significant potential applications for future industry and daily life. The capacity is an important indicator for a battery. Methods to improve the capacity of quantum batteries are important. We consider quantum batteries given by bipartite quantum systems and study the enhancement of the battery capacity under local projective measurements on a subsystem of the quantum state. By using two-qubit Bell-diagonal states and X-type states as examples, we show that quantum battery capacity with respect to the whole system or a subsystem can be improved by local projective measurements. Our theoretical analysis will provide ideas for the experimental development of quantum batteries.","sentences":["Quantum batteries have significant potential applications for future industry and daily life.","The capacity is an important indicator for a battery.","Methods to improve the capacity of quantum batteries are important.","We consider quantum batteries given by bipartite quantum systems and study the enhancement of the battery capacity under local projective measurements on a subsystem of the quantum state.","By using two-qubit Bell-diagonal states and X-type states as examples, we show that quantum battery capacity with respect to the whole system or a subsystem can be improved by local projective measurements.","Our theoretical analysis will provide ideas for the experimental development of quantum batteries."],"url":"http://arxiv.org/abs/2405.03093v1","category":"quant-ph"}
{"created":"2024-05-06 01:04:56","title":"A continuum and computational framework for viscoelastodynamics: III. A nonlinear theory","abstract":"We continue our investigation of viscoelasticity by extending the Holzapfel-Simo approach discussed in Part I to the fully nonlinear regime. By scrutinizing the relaxation property for the non-equilibrium stresses, it is revealed that a kinematic assumption akin to the Green-Naghdi type is necessary in the design of the potential. This insight underscores a link between the so-called additive plasticity and the viscoelasticity model under consideration, further inspiring our development of a nonlinear viscoelasticity theory. Our strategy is based on Hill's hyperelasticity framework and leverages the concept of generalized strains. Notably, the adopted kinematic assumption makes the proposed theory fundamentally different from the existing models rooted in the notion of the intermediate configuration. The computation aspects, including the consistent linearization, constitutive integration, and modular implementation, are addressed in detail. A suite of numerical examples is provided to demonstrate the capability of the proposed model in characterizing viscoelastic material behaviors at large strains.","sentences":["We continue our investigation of viscoelasticity by extending the Holzapfel-Simo approach discussed in Part I to the fully nonlinear regime.","By scrutinizing the relaxation property for the non-equilibrium stresses, it is revealed that a kinematic assumption akin to the Green-Naghdi type is necessary in the design of the potential.","This insight underscores a link between the so-called additive plasticity and the viscoelasticity model under consideration, further inspiring our development of a nonlinear viscoelasticity theory.","Our strategy is based on Hill's hyperelasticity framework and leverages the concept of generalized strains.","Notably, the adopted kinematic assumption makes the proposed theory fundamentally different from the existing models rooted in the notion of the intermediate configuration.","The computation aspects, including the consistent linearization, constitutive integration, and modular implementation, are addressed in detail.","A suite of numerical examples is provided to demonstrate the capability of the proposed model in characterizing viscoelastic material behaviors at large strains."],"url":"http://arxiv.org/abs/2405.03090v1","category":"math.NA"}
{"created":"2024-05-06 00:58:23","title":"Structure-Preserving Network Compression Via Low-Rank Induced Training Through Linear Layers Composition","abstract":"Deep Neural Networks (DNNs) have achieved remarkable success in addressing many previously unsolvable tasks. However, the storage and computational requirements associated with DNNs pose a challenge for deploying these trained models on resource-limited devices. Therefore, a plethora of compression and pruning techniques have been proposed in recent years. Low-rank decomposition techniques are among the approaches most utilized to address this problem. Compared to post-training compression, compression-promoted training is still under-explored. In this paper, we present a theoretically-justified novel approach, termed Low-Rank Induced Training (LoRITa), that promotes low-rankness through the composition of linear layers and compresses by using singular value truncation. This is achieved without the need to change the structure at inference time or require constrained and/or additional optimization, other than the standard weight decay regularization. Moreover, LoRITa eliminates the need to (i) initialize with pre-trained models and (ii) specify rank selection prior to training. Our experimental results (i) demonstrate the effectiveness of our approach using MNIST on Fully Connected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 on Convolutional Neural Networks, and (ii) illustrate that we achieve either competitive or SOTA results when compared to leading structured pruning methods in terms of FLOPs and parameters drop.","sentences":["Deep Neural Networks (DNNs) have achieved remarkable success in addressing many previously unsolvable tasks.","However, the storage and computational requirements associated with DNNs pose a challenge for deploying these trained models on resource-limited devices.","Therefore, a plethora of compression and pruning techniques have been proposed in recent years.","Low-rank decomposition techniques are among the approaches most utilized to address this problem.","Compared to post-training compression, compression-promoted training is still under-explored.","In this paper, we present a theoretically-justified novel approach, termed Low-Rank Induced Training (LoRITa), that promotes low-rankness through the composition of linear layers and compresses by using singular value truncation.","This is achieved without the need to change the structure at inference time or require constrained and/or additional optimization, other than the standard weight decay regularization.","Moreover, LoRITa eliminates the need to (i) initialize with pre-trained models and (ii) specify rank selection prior to training.","Our experimental results (i) demonstrate the effectiveness of our approach using MNIST on Fully Connected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 on Convolutional Neural Networks, and (ii) illustrate that we achieve either competitive or SOTA results when compared to leading structured pruning methods in terms of FLOPs and parameters drop."],"url":"http://arxiv.org/abs/2405.03089v1","category":"cs.LG"}
{"created":"2024-05-05 23:52:57","title":"Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning","abstract":"Reinforcement learning with multiple, potentially conflicting objectives is pervasive in real-world applications, while this problem remains theoretically under-explored. This paper tackles the multi-objective reinforcement learning (MORL) problem and introduces an innovative actor-critic algorithm named MOAC which finds a policy by iteratively making trade-offs among conflicting reward signals. Notably, we provide the first analysis of finite-time Pareto-stationary convergence and corresponding sample complexity in both discounted and average reward settings. Our approach has two salient features: (a) MOAC mitigates the cumulative estimation bias resulting from finding an optimal common gradient descent direction out of stochastic samples. This enables provable convergence rate and sample complexity guarantees independent of the number of objectives; (b) With proper momentum coefficient, MOAC initializes the weights of individual policy gradients using samples from the environment, instead of manual initialization. This enhances the practicality and robustness of our algorithm. Finally, experiments conducted on a real-world dataset validate the effectiveness of our proposed method.","sentences":["Reinforcement learning with multiple, potentially conflicting objectives is pervasive in real-world applications, while this problem remains theoretically under-explored.","This paper tackles the multi-objective reinforcement learning (MORL) problem and introduces an innovative actor-critic algorithm named MOAC which finds a policy by iteratively making trade-offs among conflicting reward signals.","Notably, we provide the first analysis of finite-time Pareto-stationary convergence and corresponding sample complexity in both discounted and average reward settings.","Our approach has two salient features: (a) MOAC mitigates the cumulative estimation bias resulting from finding an optimal common gradient descent direction out of stochastic samples.","This enables provable convergence rate and sample complexity guarantees independent of the number of objectives; (b) With proper momentum coefficient, MOAC initializes the weights of individual policy gradients using samples from the environment, instead of manual initialization.","This enhances the practicality and robustness of our algorithm.","Finally, experiments conducted on a real-world dataset validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2405.03082v1","category":"cs.LG"}
{"created":"2024-05-05 23:17:44","title":"Explicit Expressions for the First 20 Moments of the Area Under Dyck and Motzkin Paths","abstract":"Starting from AJ Bu's recent article that computed explicit expressions for the GENERATING functions of sums of powers of areas under Dyck and Motzkin paths, we deduce from them explicit expressions for the actual sequences. This enables taking the limits of the scaled moments and confirming, in an entirely elementary way, that they tend to those of the area under Brownian Excursion (up to any specified moment).","sentences":["Starting from AJ Bu's recent article that computed explicit expressions for the GENERATING functions of sums of powers of areas under Dyck and Motzkin paths, we deduce from them explicit expressions for the actual sequences.","This enables taking the limits of the scaled moments and confirming, in an entirely elementary way, that they tend to those of the area under Brownian Excursion (up to any specified moment)."],"url":"http://arxiv.org/abs/2405.03079v1","category":"math.CO"}
{"created":"2024-05-05 23:11:46","title":"Multiphysics Enabled Numerical Modeling of a Plasma Based Electrically Small VHF-UHF Antenna","abstract":"A three-dimensional model of a novel plasma based electrically small antenna is developed for investigating the gas properties and antenna parameters under a low pressure, low plasma temperature environment. The antenna exhibits dipole antenna-like behavior with wide-band impedance matching from $213-700$ MHz. Plasma is sustained by $0.9$ W of RF input power at $100$ MHz and the gas pressure is strategically controlled at $500$ mili-Torr. The simulated $S_{11}$ is verified against the available experimental data and further antenna parameters are extracted. The proposed ESA shows dipole-like radiation pattern with a radiation efficiency of $16\\%$ at $700$ MHz. The performance metric for ESAs, the Chu-limit, is exceeded by this antenna with the $Bandwidth\\times Efficiency$ reaching $0.168$ with a $ka$ of $0.5571$.The findings from this letter demonstrate the practicability of using COMSOL Multiphysics as a tool for predicting plasma behavior and antenna performance while the boundary conditions for all the coupled physics are respected.","sentences":["A three-dimensional model of a novel plasma based electrically small antenna is developed for investigating the gas properties and antenna parameters under a low pressure, low plasma temperature environment.","The antenna exhibits dipole antenna-like behavior with wide-band impedance matching from $213-700$ MHz.","Plasma is sustained by $0.9$ W of RF input power at $100$ MHz and the gas pressure is strategically controlled at $500$ mili-Torr.","The simulated $S_{11}$ is verified against the available experimental data and further antenna parameters are extracted.","The proposed ESA shows dipole-like radiation pattern with a radiation efficiency of $16\\%$ at $700$ MHz.","The performance metric for ESAs, the Chu-limit, is exceeded by this antenna with the $Bandwidth\\times Efficiency$ reaching $0.168$ with a $ka$ of $0.5571$.The findings from this letter demonstrate the practicability of using COMSOL Multiphysics as a tool for predicting plasma behavior and antenna performance while the boundary conditions for all the coupled physics are respected."],"url":"http://arxiv.org/abs/2405.03077v1","category":"physics.plasm-ph"}
{"created":"2024-05-05 22:53:14","title":"Convergence and Complexity Guarantee for Inexact First-order Riemannian Optimization Algorithms","abstract":"We analyze inexact Riemannian gradient descent (RGD) where Riemannian gradients and retractions are inexactly (and cheaply) computed. Our focus is on understanding when inexact RGD converges and what is the complexity in the general nonconvex and constrained setting. We answer these questions in a general framework of tangential Block Majorization-Minimization (tBMM). We establish that tBMM converges to an $\\epsilon$-stationary point within $O(\\epsilon^{-2})$ iterations. Under a mild assumption, the results still hold when the subproblem is solved inexactly in each iteration provided the total optimality gap is bounded. Our general analysis applies to a wide range of classical algorithms with Riemannian constraints including inexact RGD and proximal gradient method on Stiefel manifolds. We numerically validate that tBMM shows improved performance over existing methods when applied to various problems, including nonnegative tensor decomposition with Riemannian constraints, regularized nonnegative matrix factorization, and low-rank matrix recovery problems.","sentences":["We analyze inexact Riemannian gradient descent (RGD) where Riemannian gradients and retractions are inexactly (and cheaply) computed.","Our focus is on understanding when inexact RGD converges and what is the complexity in the general nonconvex and constrained setting.","We answer these questions in a general framework of tangential Block Majorization-Minimization (tBMM).","We establish that tBMM converges to an $\\epsilon$-stationary point within $O(\\epsilon^{-2})$ iterations.","Under a mild assumption, the results still hold when the subproblem is solved inexactly in each iteration provided the total optimality gap is bounded.","Our general analysis applies to a wide range of classical algorithms with Riemannian constraints including inexact RGD and proximal gradient method on Stiefel manifolds.","We numerically validate that tBMM shows improved performance over existing methods when applied to various problems, including nonnegative tensor decomposition with Riemannian constraints, regularized nonnegative matrix factorization, and low-rank matrix recovery problems."],"url":"http://arxiv.org/abs/2405.03073v1","category":"math.OC"}
{"created":"2024-05-05 22:18:22","title":"Powering the Future of IoT: Federated Learning for Optimized Power Consumption and Enhanced Privacy","abstract":"The widespread use of the Internet of Things has led to the development of large amounts of perception data, making it necessary to develop effective and scalable data analysis tools. Federated Learning emerges as a promising paradigm to address the inherent challenges of power consumption and data privacy in IoT environments. This paper explores the transformative potential of FL in enhancing the longevity of IoT devices by mitigating power consumption and enhancing privacy and security measures. We delve into the intricacies of FL, elucidating its components and applications within IoT ecosystems. Additionally, we discuss the critical characteristics and challenges of IoT, highlighting the need for such machine learning solutions in processing perception data. While FL introduces many benefits for IoT sustainability, it also has limitations. Through a comprehensive discussion and analysis, this paper elucidates the opportunities and constraints of FL in shaping the future of sustainable and secure IoT systems. Our findings highlight the importance of developing new approaches and conducting additional research to maximise the benefits of FL in creating a secure and privacy-focused IoT environment.","sentences":["The widespread use of the Internet of Things has led to the development of large amounts of perception data, making it necessary to develop effective and scalable data analysis tools.","Federated Learning emerges as a promising paradigm to address the inherent challenges of power consumption and data privacy in IoT environments.","This paper explores the transformative potential of FL in enhancing the longevity of IoT devices by mitigating power consumption and enhancing privacy and security measures.","We delve into the intricacies of FL, elucidating its components and applications within IoT ecosystems.","Additionally, we discuss the critical characteristics and challenges of IoT, highlighting the need for such machine learning solutions in processing perception data.","While FL introduces many benefits for IoT sustainability, it also has limitations.","Through a comprehensive discussion and analysis, this paper elucidates the opportunities and constraints of FL in shaping the future of sustainable and secure IoT systems.","Our findings highlight the importance of developing new approaches and conducting additional research to maximise the benefits of FL in creating a secure and privacy-focused IoT environment."],"url":"http://arxiv.org/abs/2405.03065v1","category":"cs.CR"}
{"created":"2024-05-05 22:05:02","title":"Stability of a Generalized Debiased Lasso with Applications to Resampling-Based Variable Selection","abstract":"Suppose that we first apply the Lasso to a design matrix, and then update one of its columns. In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly. In this work, we propose an approximate formula for updating a debiased Lasso coefficient. We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix's columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\\ sub-Gaussian row vectors and i.i.d.\\ Gaussian noise. Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number. Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes. In contrast, rigorously establishing distributional limit properties (e.g.\\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory. As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter.","sentences":["Suppose that we first apply the Lasso to a design matrix, and then update one of its columns.","In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly.","In this work, we propose an approximate formula for updating a debiased Lasso coefficient.","We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix's columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\\ sub-Gaussian row vectors and i.i.d.\\ Gaussian noise.","Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number.","Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes.","In contrast, rigorously establishing distributional limit properties (e.g.\\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory.","As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter."],"url":"http://arxiv.org/abs/2405.03063v1","category":"math.ST"}
{"created":"2024-05-05 21:44:03","title":"Active Preference Learning for Ordering Items In- and Out-of-sample","abstract":"Learning an ordering of items based on noisy pairwise comparisons is useful when item-specific labels are difficult to assign, for example, when annotators have to make subjective assessments. Algorithms have been proposed for actively sampling comparisons of items to minimize the number of annotations necessary for learning an accurate ordering. However, many ignore shared structure between items, treating them as unrelated, limiting sample efficiency and precluding generalization to new items. In this work, we study active learning with pairwise preference feedback for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error incurred by active learning strategies under a logistic preference model, in terms of the aleatoric and epistemic uncertainty in comparisons, and propose two algorithms designed to greedily minimize this bound. We evaluate these algorithms in two realistic image ordering tasks, including one with comparisons made by human annotators, and demonstrate superior sample efficiency compared to non-contextual ranking approaches and active preference learning baselines.","sentences":["Learning an ordering of items based on noisy pairwise comparisons is useful when item-specific labels are difficult to assign, for example, when annotators have to make subjective assessments.","Algorithms have been proposed for actively sampling comparisons of items to minimize the number of annotations necessary for learning an accurate ordering.","However, many ignore shared structure between items, treating them as unrelated, limiting sample efficiency and precluding generalization to new items.","In this work, we study active learning with pairwise preference feedback for ordering items with contextual attributes, both in- and out-of-sample.","We give an upper bound on the expected ordering error incurred by active learning strategies under a logistic preference model, in terms of the aleatoric and epistemic uncertainty in comparisons, and propose two algorithms designed to greedily minimize this bound.","We evaluate these algorithms in two realistic image ordering tasks, including one with comparisons made by human annotators, and demonstrate superior sample efficiency compared to non-contextual ranking approaches and active preference learning baselines."],"url":"http://arxiv.org/abs/2405.03059v1","category":"cs.LG"}
{"created":"2024-05-05 21:41:43","title":"Enhancing High-Level Synthesis with Automated Pragma Insertion and Code Transformation Framework","abstract":"High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.","sentences":["High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs.","These tools offer benefits such as reduced development time and enhanced performance.","However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps.","Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary.","Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   ","To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers.","Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels.","Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels.","Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results."],"url":"http://arxiv.org/abs/2405.03058v1","category":"cs.SE"}
{"created":"2024-05-05 21:07:53","title":"Optical phased array using phase-controlled optical frequency comb","abstract":"We developed an optical phased array using an optical frequency comb and demonstrated its proof-of-principle. Optical phased arrays have been actively developed in recent years as a technology that can control the wavefront of light without any mechanical devices like phased array radar. Conventional optical phased arrays have been implemented using optical integrated circuits, but it has been difficult to achieve broadband operation with simple control. This is because control and calibration of a large number of phase modulators are required for each wavelength, and the dispersion of the waveguide makes whole bandwidth phase control of ultrashort pulses difficult. In contrast, we have developed a novel optical phased array that realizes wavefront control of ultrashort pulses generated by mode-locked laser by phase control of the comb, using high controllability of the comb and an optical array antenna with free-space optics. This is achieved by simply controlling the ratio of the two radio frequencies of the comb to realize a broadband optical phased array while suppressing environmental fluctuations. Experiments demonstrated broadband optical dot scanning at an optical frequency by forming an optical dot pattern and suppressing the environmental fluctuation by controlling the comb frequency. This innovative optical technology enables direct control of wavefronts by optical frequencies, i.e. controlling transverse modes by longitudinal modes.","sentences":["We developed an optical phased array using an optical frequency comb and demonstrated its proof-of-principle.","Optical phased arrays have been actively developed in recent years as a technology that can control the wavefront of light without any mechanical devices like phased array radar.","Conventional optical phased arrays have been implemented using optical integrated circuits, but it has been difficult to achieve broadband operation with simple control.","This is because control and calibration of a large number of phase modulators are required for each wavelength, and the dispersion of the waveguide makes whole bandwidth phase control of ultrashort pulses difficult.","In contrast, we have developed a novel optical phased array that realizes wavefront control of ultrashort pulses generated by mode-locked laser by phase control of the comb, using high controllability of the comb and an optical array antenna with free-space optics.","This is achieved by simply controlling the ratio of the two radio frequencies of the comb to realize a broadband optical phased array while suppressing environmental fluctuations.","Experiments demonstrated broadband optical dot scanning at an optical frequency by forming an optical dot pattern and suppressing the environmental fluctuation by controlling the comb frequency.","This innovative optical technology enables direct control of wavefronts by optical frequencies, i.e. controlling transverse modes by longitudinal modes."],"url":"http://arxiv.org/abs/2405.03053v1","category":"physics.optics"}
{"created":"2024-05-05 21:06:07","title":"A View on Out-of-Distribution Identification from a Statistical Testing Theory Perspective","abstract":"We study the problem of efficiently detecting Out-of-Distribution (OOD) samples at test time in supervised and unsupervised learning contexts. While ML models are typically trained under the assumption that training and test data stem from the same distribution, this is often not the case in realistic settings, thus reliably detecting distribution shifts is crucial at deployment. We re-formulate the OOD problem under the lenses of statistical testing and then discuss conditions that render the OOD problem identifiable in statistical terms. Building on this framework, we study convergence guarantees of an OOD test based on the Wasserstein distance, and provide a simple empirical evaluation.","sentences":["We study the problem of efficiently detecting Out-of-Distribution (OOD) samples at test time in supervised and unsupervised learning contexts.","While ML models are typically trained under the assumption that training and test data stem from the same distribution, this is often not the case in realistic settings, thus reliably detecting distribution shifts is crucial at deployment.","We re-formulate the OOD problem under the lenses of statistical testing and then discuss conditions that render the OOD problem identifiable in statistical terms.","Building on this framework, we study convergence guarantees of an OOD test based on the Wasserstein distance, and provide a simple empirical evaluation."],"url":"http://arxiv.org/abs/2405.03052v1","category":"cs.LG"}
{"created":"2024-05-05 20:49:52","title":"Real-time solution of quadratic optimization problems with banded matrices and indicator variables","abstract":"We consider mixed-integer quadratic optimization problems with banded matrices and indicator variables. These problems arise pervasively in statistical inference problems with time-series data, where the banded matrix captures the temporal relationship of the underlying process. In particular, the problem studied arises in monitoring problems, where the decision-maker wants to detect changes or anomalies. We propose to solve these problems using decision diagrams. In particular we show how to exploit the temporal dependencies to construct diagrams with size polynomial in the number of decision variables. We also describe how to construct the convex hull of the set under study from the decision diagrams, and how to deploy the method online to solve the problems in milliseconds via a shortest path algorithm.","sentences":["We consider mixed-integer quadratic optimization problems with banded matrices and indicator variables.","These problems arise pervasively in statistical inference problems with time-series data, where the banded matrix captures the temporal relationship of the underlying process.","In particular, the problem studied arises in monitoring problems, where the decision-maker wants to detect changes or anomalies.","We propose to solve these problems using decision diagrams.","In particular we show how to exploit the temporal dependencies to construct diagrams with size polynomial in the number of decision variables.","We also describe how to construct the convex hull of the set under study from the decision diagrams, and how to deploy the method online to solve the problems in milliseconds via a shortest path algorithm."],"url":"http://arxiv.org/abs/2405.03051v1","category":"math.OC"}
{"created":"2024-05-05 20:11:10","title":"Ultrastrong coupling limit to quantum mean force Gibbs state for anharmonic environment","abstract":"The equilibrium state of a quantum system can deviate from the Gibbs state if the system-environment coupling is not weak. An analytical expression for this mean force Gibbs state (MFGS) is known in the ultrastrong coupling (USC) regime for the Caldeira-Leggett (CL) model that assumes a harmonic environment. Here, we derive analytical expressions for the MFGS in the USC regime for more general system-environment models. For all the generalized models considered here, we find the USC state to be diagonal in the basis set by the system-environment interaction, just like in the CL case. While for the generic model considered, the corresponding USC-MFGS state is found to alter from the CL-result, we do identify a class of models more general than the CL-model for which the CL-USC result remains unchanged. We also provide numerical verification for our results. These results provide key tools for the study of strong coupling thermodynamics under more realistic system-environment models, going beyond the CL-model.","sentences":["The equilibrium state of a quantum system can deviate from the Gibbs state if the system-environment coupling is not weak.","An analytical expression for this mean force Gibbs state (MFGS) is known in the ultrastrong coupling (USC) regime for the Caldeira-Leggett (CL) model that assumes a harmonic environment.","Here, we derive analytical expressions for the MFGS in the USC regime for more general system-environment models.","For all the generalized models considered here, we find the USC state to be diagonal in the basis set by the system-environment interaction, just like in the CL case.","While for the generic model considered, the corresponding USC-MFGS state is found to alter from the CL-result, we do identify a class of models more general than the CL-model for which the CL-USC result remains unchanged.","We also provide numerical verification for our results.","These results provide key tools for the study of strong coupling thermodynamics under more realistic system-environment models, going beyond the CL-model."],"url":"http://arxiv.org/abs/2405.03044v1","category":"quant-ph"}
{"created":"2024-05-05 19:06:56","title":"FlexKalmanNet: A Modular AI-Enhanced Kalman Filter Framework Applied to Spacecraft Motion Estimation","abstract":"The estimation of relative motion between spacecraft increasingly relies on feature-matching computer vision, which feeds data into a recursive filtering algorithm. Kalman filters, although efficient in noise compensation, demand extensive tuning of system and noise models. This paper introduces FlexKalmanNet, a novel modular framework that bridges this gap by integrating a deep fully connected neural network with Kalman filter-based motion estimation algorithms. FlexKalmanNet's core innovation is its ability to learn any Kalman filter parameter directly from measurement data, coupled with the flexibility to utilize various Kalman filter variants. This is achieved through a notable design decision to outsource the sequential computation from the neural network to the Kalman filter variant, enabling a purely feedforward neural network architecture. This architecture, proficient at handling complex, nonlinear features without the dependency on recurrent network modules, captures global data patterns more effectively. Empirical evaluation using data from NASA's Astrobee simulation environment focuses on learning unknown parameters of an Extended Kalman filter for spacecraft pose and twist estimation. The results demonstrate FlexKalmanNet's rapid training convergence, high accuracy, and superior performance against manually tuned Extended Kalman filters.","sentences":["The estimation of relative motion between spacecraft increasingly relies on feature-matching computer vision, which feeds data into a recursive filtering algorithm.","Kalman filters, although efficient in noise compensation, demand extensive tuning of system and noise models.","This paper introduces FlexKalmanNet, a novel modular framework that bridges this gap by integrating a deep fully connected neural network with Kalman filter-based motion estimation algorithms.","FlexKalmanNet's core innovation is its ability to learn any Kalman filter parameter directly from measurement data, coupled with the flexibility to utilize various Kalman filter variants.","This is achieved through a notable design decision to outsource the sequential computation from the neural network to the Kalman filter variant, enabling a purely feedforward neural network architecture.","This architecture, proficient at handling complex, nonlinear features without the dependency on recurrent network modules, captures global data patterns more effectively.","Empirical evaluation using data from NASA's Astrobee simulation environment focuses on learning unknown parameters of an Extended Kalman filter for spacecraft pose and twist estimation.","The results demonstrate FlexKalmanNet's rapid training convergence, high accuracy, and superior performance against manually tuned Extended Kalman filters."],"url":"http://arxiv.org/abs/2405.03034v1","category":"cs.RO"}
{"created":"2024-05-05 18:50:35","title":"Distributed Learning for Dynamic Congestion Games","abstract":"Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Google Maps and Waze). Yet such platforms myopically recommend the currently shortest path to users, and selfish users are unwilling to travel to longer paths of varying traffic conditions to explore. Prior studies focus on one-shot congestion games without information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a distributed manner. Our analysis shows that, as compared to the social optimum in minimizing the long-term social cost via optimal exploration-exploitation tradeoff, the myopic routing policy leads to severe under-exploration of stochastic paths with the price of anarchy (PoA) greater than \\(2\\). Besides, it fails to ensure the correct learning convergence about users' traffic hazard beliefs. To mitigate the efficiency loss, we first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even \\(\\text{PoA}=\\infty\\). Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group. Our CHAR successfully ensures PoA less than \\(\\frac{5}{4}\\), which cannot be further reduced by any other informational mechanism. Additionally, we experiment with real-world data to verify our CHAR's good average performance.","sentences":["Today mobile users learn and share their traffic observations via crowdsourcing platforms (e.g., Google Maps and Waze).","Yet such platforms myopically recommend the currently shortest path to users, and selfish users are unwilling to travel to longer paths of varying traffic conditions to explore.","Prior studies focus on one-shot congestion games without information learning, while our work studies how users learn and alter traffic conditions on stochastic paths in a distributed manner.","Our analysis shows that, as compared to the social optimum in minimizing the long-term social cost via optimal exploration-exploitation tradeoff, the myopic routing policy leads to severe under-exploration of stochastic paths with the price of anarchy (PoA) greater than \\(2\\).","Besides, it fails to ensure the correct learning convergence about users' traffic hazard beliefs.","To mitigate the efficiency loss, we first show that existing information-hiding mechanisms and deterministic path-recommendation mechanisms in Bayesian persuasion literature do not work with even \\(\\text{PoA}=\\infty\\).","Accordingly, we propose a new combined hiding and probabilistic recommendation (CHAR) mechanism to hide all information from a selected user group and provide state-dependent probabilistic recommendations to the other user group.","Our CHAR successfully ensures PoA less than \\(\\frac{5}{4}\\), which cannot be further reduced by any other informational mechanism.","Additionally, we experiment with real-world data to verify our CHAR's good average performance."],"url":"http://arxiv.org/abs/2405.03031v1","category":"cs.GT"}
{"created":"2024-05-05 18:47:55","title":"Quantum Corrections to the Decay Law in Flight","abstract":"The deviation of the decay law from the exponential is a well known effect of quantum mechanics. Here we analyze the relativistic survival probabilities, $S(t,p)$, where $p$ is the momentum of the decaying particle and provide analytical expressions for $S(t,p)$ in the exponential (E) as well as the nonexponential (NE) regions at small and large times. Under minimal assumptions on the spectral density function, analytical expressions for the critical times of transition from the NE to the E at small times and the E to NE at large times are derived. The dependence of the decay law on the relativistic Lorentz factor, $\\gamma = 1/\\sqrt{1 - v^2/c^2}$, reveals several interesting features. In the short time regime of the decay law, the critical time, $\\tau_{st}$, shows a steady increase with $\\gamma$, thus implying a larger NE region for particles decaying in flight. Comparing $S(t,p)$ with the well known time dilation formula, $e^{-\\Gamma t/\\gamma}$, in the exponential region, an expression for the critical $\\gamma$ where $S(t,p)$ deviates most from $e^{-\\Gamma t/\\gamma}$ is presented. This is a purely quantum correction. Under particular conditions on the resonance parameters, there also exists a critical $\\gamma$ at large times which decides if the NE region shifts backward or forward in time as compared to that for a particle at rest. All the above analytical results are supported by calculations involving realistic decays of hadrons and leptons.","sentences":["The deviation of the decay law from the exponential is a well known effect of quantum mechanics.","Here we analyze the relativistic survival probabilities, $S(t,p)$, where $p$ is the momentum of the decaying particle and provide analytical expressions for $S(t,p)$ in the exponential (E) as well as the nonexponential (NE) regions at small and large times.","Under minimal assumptions on the spectral density function, analytical expressions for the critical times of transition from the NE to the E at small times and the E to NE at large times are derived.","The dependence of the decay law on the relativistic Lorentz factor, $\\gamma = 1/\\sqrt{1 - v^2/c^2}$, reveals several interesting features.","In the short time regime of the decay law, the critical time, $\\tau_{st}$, shows a steady increase with $\\gamma$, thus implying a larger NE region for particles decaying in flight.","Comparing $S(t,p)$ with the well known time dilation formula, $e^{-\\Gamma t/\\gamma}$, in the exponential region, an expression for the critical $\\gamma$ where $S(t,p)$ deviates most from $e^{-\\Gamma t/\\gamma}$ is presented.","This is a purely quantum correction.","Under particular conditions on the resonance parameters, there also exists a critical $\\gamma$ at large times which decides if the NE region shifts backward or forward in time as compared to that for a particle at rest.","All the above analytical results are supported by calculations involving realistic decays of hadrons and leptons."],"url":"http://arxiv.org/abs/2405.03030v1","category":"hep-ph"}
{"created":"2024-05-05 17:49:33","title":"Background risk model in presence of heavy tails under dependence","abstract":"We study the background risk model under a various forms of dependence and some distribution classes of heavy tails. First, we study the asymptotic behavior of tail expectation of a portfolios with unequal heavytailedness, of non-random sums and randomly stopped sums, under some dependence structure which contains the independence as a special case. Further we investigate the asymptotic behavior of a pair of randomly weighted sums, generalizing the dependence structure among random vector components, our results contains the finite ruin probability in bi-dimensional discrete time risk model with unequal heavytailedness. Next, we carry out asymptotic analysis of the tail distortion risk measures in background risk model under various forms of dependence, with multivariate regularly varying risk distributions in each portfolio.","sentences":["We study the background risk model under a various forms of dependence and some distribution classes of heavy tails.","First, we study the asymptotic behavior of tail expectation of a portfolios with unequal heavytailedness, of non-random sums and randomly stopped sums, under some dependence structure which contains the independence as a special case.","Further we investigate the asymptotic behavior of a pair of randomly weighted sums, generalizing the dependence structure among random vector components, our results contains the finite ruin probability in bi-dimensional discrete time risk model with unequal heavytailedness.","Next, we carry out asymptotic analysis of the tail distortion risk measures in background risk model under various forms of dependence, with multivariate regularly varying risk distributions in each portfolio."],"url":"http://arxiv.org/abs/2405.03014v1","category":"math.PR"}
{"created":"2024-05-05 16:49:19","title":"Graded quasi-Baer $\\ast$-ring characterization of Steinberg algebras","abstract":"Given a graded ample, Hausdorff groupoid $G$, and an involutive field $K$, we consider the Steinberg algebra $A_K(G)$. We obtain necessary and sufficient conditions on $G$ under which the annihilator of any graded ideal of $A_K(G)$ is generated by a homogeneous projection. This property is called graded quasi-Baer $\\ast$. We use the Steinberg algebra model to characterize graded quasi-Baer $\\ast$ Leavitt path algebras.","sentences":["Given a graded ample, Hausdorff groupoid $G$, and an involutive field $K$, we consider the Steinberg algebra $A_K(G)$. We obtain necessary and sufficient conditions on $G$ under which the annihilator of any graded ideal of $A_K(G)$ is generated by a homogeneous projection.","This property is called graded quasi-Baer $\\ast$. We use the Steinberg algebra model to characterize graded quasi-Baer $\\ast$ Leavitt path algebras."],"url":"http://arxiv.org/abs/2405.02997v1","category":"math.RA"}
{"created":"2024-05-05 16:12:45","title":"Harvesting Energy from Soil-Air Temperature Differences for Batteryless IoT Devices: A Case Study","abstract":"The temperature difference between soil and air holds the potential to generate energy to power many low-power IoT devices. However, there is a lack of studies in the literature that explore the nuances of soil-air thermal energy harvesting. This paper offers a comprehensive discussion on soil-air thermal energy harvesting. We engineer a custom Soil-air Thermoelectric Generator (SoTEG) that incorporates an off-the-shelf TEG and an efficient heat transfer network. A detailed discussion of the design and analysis of SoTEG is presented along with a versatile simulation model which can be used to simulate the performance of the harvester under different ambient conditions. Investigations using the model and results gathered from experiments demonstrate that the SoTEG has a heat transfer efficiency of 34.5% with room for improvement and can power a load from temperature differences as low as 3 {\\deg}C between soil and air, or 1 {\\deg}C across the TEG. Power generated by SoTEG at 3 {\\deg}C difference amounts to 110 {\\mu}Wor a power density of 11.58mW/m2. When connected to a Power Management Unit (PMU), the combined system generates around 30 {\\mu}Wat 3 {\\deg}C. During a 14-day outdoor deployment in a winter month, the maximum power generated by the combined system is 337 {\\mu}W when the temperature difference across the TEG is 2.75 {\\deg}C. Additionally, the model analysis reveals that the weather conditions have an impact on the harvester. While Solar radiation enhances power generation, wind can either improve or diminish the harvested energy depending on whether it is day or night.","sentences":["The temperature difference between soil and air holds the potential to generate energy to power many low-power IoT devices.","However, there is a lack of studies in the literature that explore the nuances of soil-air thermal energy harvesting.","This paper offers a comprehensive discussion on soil-air thermal energy harvesting.","We engineer a custom Soil-air Thermoelectric Generator (SoTEG) that incorporates an off-the-shelf TEG and an efficient heat transfer network.","A detailed discussion of the design and analysis of SoTEG is presented along with a versatile simulation model which can be used to simulate the performance of the harvester under different ambient conditions.","Investigations using the model and results gathered from experiments demonstrate that the SoTEG has a heat transfer efficiency of 34.5% with room for improvement and can power a load from temperature differences as low as 3 {\\deg}C between soil and air, or 1 {\\deg}C across the TEG.","Power generated by SoTEG at 3 {\\deg}C difference amounts to 110 {\\mu}Wor a power density of 11.58mW/m2.","When connected to a Power Management Unit (PMU), the combined system generates around 30 {\\mu}Wat 3 {\\deg}C. During a 14-day outdoor deployment in a winter month, the maximum power generated by the combined system is 337 {\\mu}W when the temperature difference across the TEG is 2.75 {\\deg}C.","Additionally, the model analysis reveals that the weather conditions have an impact on the harvester.","While Solar radiation enhances power generation, wind can either improve or diminish the harvested energy depending on whether it is day or night."],"url":"http://arxiv.org/abs/2405.02986v1","category":"eess.SY"}
{"created":"2024-05-05 15:52:03","title":"Four-hundred Very Metal-poor Stars Studied with LAMOST and Subaru. III. Dynamically Tagged Groups and Chemodynamical Properties","abstract":"Very metal-poor (VMP) stars record the signatures of early accreted galaxies, making them essential tools for unraveling the early stages of Galaxy formation. Understanding the origin of VMP stars requires comprehensive studies of their chemical compositions and kinematics, which are currently lacking. Hence, we conduct a chemodynamical analysis of 352 VMP stars selected from one of the largest uniform high-resolution VMP star samples, jointly obtained from LAMOST and Subaru. We apply a friends-of-friends clustering algorithm to the master catalog of this high-resolution sample, which consists of 5778 VMP stars. It results in 131 dynamically tagged groups with 89 associated with known substructures in the Milky Way, including Gaia-Sausage-Enceladus (GSE), Thamnos, Helmi streams, Sequoia, Wukong, Pontus, and the very metal-poor disk (VMPD). Our findings are: (i) the VMPD shows lower Zn abundances than the rest, which indicates that it could be a relic of small stellar systems; (ii) Sequoia shows moderately high r-process abundances; (iii) Helmi streams show deficiencies in carbon and light neutron-capture elements; (iv) the fraction of carbon-enhanced metal-poor stars with no enhancement in heavy elements (CEMP-no stars) seems low in the VMPD and the Helmi streams; and (v) a subgroup in GSE exhibits a very high fraction of r-process enhanced stars, with four out of five showing [Eu/Fe]> +1.0. The abundance patterns of other elements in VMP substructures largely match the whole VMP sample. We also study large-scale correlations between abundance ratios and kinematics without classifying stars into substructures, but it does not yield significant correlations once the overall chemical evolution is considered for most elements.","sentences":["Very metal-poor (VMP) stars record the signatures of early accreted galaxies, making them essential tools for unraveling the early stages of Galaxy formation.","Understanding the origin of VMP stars requires comprehensive studies of their chemical compositions and kinematics, which are currently lacking.","Hence, we conduct a chemodynamical analysis of 352 VMP stars selected from one of the largest uniform high-resolution VMP star samples, jointly obtained from LAMOST and Subaru.","We apply a friends-of-friends clustering algorithm to the master catalog of this high-resolution sample, which consists of 5778 VMP stars.","It results in 131 dynamically tagged groups with 89 associated with known substructures in the Milky Way, including Gaia-Sausage-Enceladus (GSE), Thamnos, Helmi streams, Sequoia, Wukong, Pontus, and the very metal-poor disk (VMPD).","Our findings are: (i) the VMPD shows lower Zn abundances than the rest, which indicates that it could be a relic of small stellar systems; (ii) Sequoia shows moderately high r-process abundances; (iii) Helmi streams show deficiencies in carbon and light neutron-capture elements; (iv) the fraction of carbon-enhanced metal-poor stars with no enhancement in heavy elements (CEMP-no stars) seems low in the VMPD and the Helmi streams; and (v) a subgroup in GSE exhibits a very high fraction of r-process enhanced stars, with four out of five showing [Eu/Fe]> +1.0.","The abundance patterns of other elements in VMP substructures largely match the whole VMP sample.","We also study large-scale correlations between abundance ratios and kinematics without classifying stars into substructures, but it does not yield significant correlations once the overall chemical evolution is considered for most elements."],"url":"http://arxiv.org/abs/2405.02978v1","category":"astro-ph.GA"}
{"created":"2024-05-05 15:37:27","title":"Importance of Gas Heating in Capacitively Coupled Radiofrequency Plasma-assisted Synthesis of Carbon Nanomaterials","abstract":"In pursuit of diamond nanoparticles, a capacitively-coupled radio frequency (CCRF) flow-through plasma reactor was operated with methane argon gas mixtures. Signatures of the final product obtained microscopically and spectroscopically indicated that the product was an amorphous form of graphite. This result was consistent irrespective of combinations of the macroscopic reactor settings. To explain the observed synthesis output, measurements of C2 and gas properties were carried out by laser-induced fluorescence and optical emission spectroscopy. Strikingly, the results indicated a strong gas temperature gradient of 100 K per mm from the center of the reactor to the wall. Based on additional plasma imaging, a model of hot constricted region (filamentation region) was then formulated. It illustrated that, while the hot constricted region was present, the bulk of the gas was not hot enough to facilitate diamond sp3 formation: characterized by much lower reaction rates, when compared to sp2, sp3 formation kinetics are expected to become exponentially slow. This result was further confirmed by experiments under identical conditions but with a H2/CH4 mixture, where no output material was detected: if graphitic sp2 formation was expected as the main output material from the methane feedstock, atomic hydrogen would then be expected to etch it away in situ, such that the net production of that sp2-hybridized solid material is nearly a zero.","sentences":["In pursuit of diamond nanoparticles, a capacitively-coupled radio frequency (CCRF) flow-through plasma reactor was operated with methane argon gas mixtures.","Signatures of the final product obtained microscopically and spectroscopically indicated that the product was an amorphous form of graphite.","This result was consistent irrespective of combinations of the macroscopic reactor settings.","To explain the observed synthesis output, measurements of C2 and gas properties were carried out by laser-induced fluorescence and optical emission spectroscopy.","Strikingly, the results indicated a strong gas temperature gradient of 100 K per mm from the center of the reactor to the wall.","Based on additional plasma imaging, a model of hot constricted region (filamentation region) was then formulated.","It illustrated that, while the hot constricted region was present, the bulk of the gas was not hot enough to facilitate diamond sp3 formation: characterized by much lower reaction rates, when compared to sp2, sp3 formation kinetics are expected to become exponentially slow.","This result was further confirmed by experiments under identical conditions but with a H2/CH4 mixture, where no output material was detected: if graphitic sp2 formation was expected as the main output material from the methane feedstock, atomic hydrogen would then be expected to etch it away in situ, such that the net production of that sp2-hybridized solid material is nearly a zero."],"url":"http://arxiv.org/abs/2405.02974v1","category":"physics.plasm-ph"}
{"created":"2024-05-05 15:07:56","title":"Preventive Audits for Data Applications Before Data Sharing in the Power IoT","abstract":"With the increase in data volume, more types of data are being used and shared, especially in the power Internet of Things (IoT). However, the processes of data sharing may lead to unexpected information leakage because of the ubiquitous relevance among the different data, thus it is necessary for data owners to conduct preventive audits for data applications before data sharing to avoid the risk of key information leakage. Considering that the same data may play completely different roles in different application scenarios, data owners should know the expected data applications of the data buyers in advance and provide modified data that are less relevant to the private information of the data owners and more relevant to the nonprivate information that the data buyers need. In this paper, data sharing in the power IoT is regarded as the background, and the mutual information of the data and their implicit information is selected as the data feature parameter to indicate the relevance between the data and their implicit information or the ability to infer the implicit information from the data. Therefore, preventive audits should be conducted based on changes in the data feature parameters before and after data sharing. The probability exchange adjustment method is proposed as the theoretical basis of preventive audits under simplified consumption, and the corresponding optimization models are constructed and extended to more practical scenarios with multivariate characteristics. Finally, case studies are used to validate the effectiveness of the proposed preventive audits.","sentences":["With the increase in data volume, more types of data are being used and shared, especially in the power Internet of Things (IoT).","However, the processes of data sharing may lead to unexpected information leakage because of the ubiquitous relevance among the different data, thus it is necessary for data owners to conduct preventive audits for data applications before data sharing to avoid the risk of key information leakage.","Considering that the same data may play completely different roles in different application scenarios, data owners should know the expected data applications of the data buyers in advance and provide modified data that are less relevant to the private information of the data owners and more relevant to the nonprivate information that the data buyers need.","In this paper, data sharing in the power IoT is regarded as the background, and the mutual information of the data and their implicit information is selected as the data feature parameter to indicate the relevance between the data and their implicit information or the ability to infer the implicit information from the data.","Therefore, preventive audits should be conducted based on changes in the data feature parameters before and after data sharing.","The probability exchange adjustment method is proposed as the theoretical basis of preventive audits under simplified consumption, and the corresponding optimization models are constructed and extended to more practical scenarios with multivariate characteristics.","Finally, case studies are used to validate the effectiveness of the proposed preventive audits."],"url":"http://arxiv.org/abs/2405.02963v1","category":"cs.CR"}
{"created":"2024-05-05 15:01:00","title":"JOSENet: A Joint Stream Embedding Network for Violence Detection in Surveillance Videos","abstract":"Due to the ever-increasing availability of video surveillance cameras and the growing need for crime prevention, the violence detection task is attracting greater attention from the research community. With respect to other action recognition tasks, violence detection in surveillance videos shows additional issues, such as the presence of a significant variety of real fight scenes. Unfortunately, available datasets seem to be very small compared with other action recognition datasets. Moreover, in surveillance applications, people in the scenes always differ for each video and the background of the footage differs for each camera. Also, violent actions in real-life surveillance videos must be detected quickly to prevent unwanted consequences, thus models would definitely benefit from a reduction in memory usage and computational costs. Such problems make classical action recognition methods difficult to be adopted. To tackle all these issues, we introduce JOSENet, a novel self-supervised framework that provides outstanding performance for violence detection in surveillance videos. The proposed model receives two spatiotemporal video streams, i.e., RGB frames and optical flows, and involves a new regularized self-supervised learning approach for videos. JOSENet provides improved performance compared to self-supervised state-of-the-art methods, while requiring one-fourth of the number of frames per video segment and a reduced frame rate. The source code and the instructions to reproduce our experiments are available at https://github.com/ispamm/JOSENet.","sentences":["Due to the ever-increasing availability of video surveillance cameras and the growing need for crime prevention, the violence detection task is attracting greater attention from the research community.","With respect to other action recognition tasks, violence detection in surveillance videos shows additional issues, such as the presence of a significant variety of real fight scenes.","Unfortunately, available datasets seem to be very small compared with other action recognition datasets.","Moreover, in surveillance applications, people in the scenes always differ for each video and the background of the footage differs for each camera.","Also, violent actions in real-life surveillance videos must be detected quickly to prevent unwanted consequences, thus models would definitely benefit from a reduction in memory usage and computational costs.","Such problems make classical action recognition methods difficult to be adopted.","To tackle all these issues, we introduce JOSENet, a novel self-supervised framework that provides outstanding performance for violence detection in surveillance videos.","The proposed model receives two spatiotemporal video streams, i.e., RGB frames and optical flows, and involves a new regularized self-supervised learning approach for videos.","JOSENet provides improved performance compared to self-supervised state-of-the-art methods, while requiring one-fourth of the number of frames per video segment and a reduced frame rate.","The source code and the instructions to reproduce our experiments are available at https://github.com/ispamm/JOSENet."],"url":"http://arxiv.org/abs/2405.02961v1","category":"cs.CV"}
{"created":"2024-05-05 14:48:13","title":"Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training","abstract":"Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to a related but unlabeled target domain. While the source model is a key avenue for acquiring target pseudolabels, the generated pseudolabels may exhibit source bias. In the conventional SFDA pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to initialize the source model at the start of source training, and subsequently discarded. Despite having diverse features important for generalization, the pre-trained feature extractor can overfit to the source data distribution during source training and forget relevant target domain knowledge. Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process. The proposed framework is flexible and allows us to plug modern pre-trained networks into the adaptation process to leverage their stronger representation learning capabilities. For adaptation, we propose the Co-learn algorithm to improve target pseudolabel quality collaboratively through the source model and a pre-trained feature extractor. Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension Co-learn++ to further incorporate CLIP's zero-shot classification decisions. We evaluate on 3 benchmark datasets and include more challenging scenarios such as open-set, partial-set and open-partial SFDA. Experimental results demonstrate that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods.","sentences":["Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to a related but unlabeled target domain.","While the source model is a key avenue for acquiring target pseudolabels, the generated pseudolabels may exhibit source bias.","In the conventional SFDA pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to initialize the source model at the start of source training, and subsequently discarded.","Despite having diverse features important for generalization, the pre-trained feature extractor can overfit to the source data distribution during source training and forget relevant target domain knowledge.","Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process.","The proposed framework is flexible and allows us to plug modern pre-trained networks into the adaptation process to leverage their stronger representation learning capabilities.","For adaptation, we propose the Co-learn algorithm to improve target pseudolabel quality collaboratively through the source model and a pre-trained feature extractor.","Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension Co-learn++ to further incorporate CLIP's zero-shot classification decisions.","We evaluate on 3 benchmark datasets and include more challenging scenarios such as open-set, partial-set and open-partial SFDA.","Experimental results demonstrate that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods."],"url":"http://arxiv.org/abs/2405.02954v1","category":"cs.CV"}
{"created":"2024-05-05 14:28:05","title":"Longitudinal Momentum Spectra of pair created in a pulsed field at finite times: Are Oscillations \"Real\"","abstract":"We investigate the production of electron-positron pairs from the vacuum in a time-varying, spatially uniform pulsed electric field given by $E(t) = E_0 sech^2(t/\\tau)$, with height of $E_0$ and width of $\\tau$. Sah recently discussed the problem of pair production after a finite evolution time [9]. This raises questions about the instantaneous appearance of particles in pair production and their behavior at intermediate times when using a formalism that involves solving an evolution equation for a dynamical quantity. Is it possible to make general statements about this behavior? To address these questions, we analytically compute the probability of $(e^+ e^-)$ pair production in momentum space using the exact solution of the one-particle time-dependent Dirac equation, and we compare the result with quantum kinetic theory (QKT). Both approaches allow us to study the particle momentum spectrum at any instant in time and can potentially unveil valuable information regarding quantum non-equilibrium physics. We analyze both approaches' Longitudinal Momentum Spectrum (LMS) of the created particles at finite times. We observe oscillatory structure in the LMS. This oscillation behavior at finite time clearly illustrates the quantum interference effects associated with particle production. It is worth noting that both approaches exhibit quantum interference patterns at finite times, manifested as oscillations observed in the LMS. This reveals that these oscillations are not due to transient excitations and basis-dependent signatures. Again, we emphasize that the oscillations seen in the LMS from both approaches are not artifacts but possess significant physical relevance.","sentences":["We investigate the production of electron-positron pairs from the vacuum in a time-varying, spatially uniform pulsed electric field given by $E(t) = E_0 sech^2(t/\\tau)$, with height of $E_0$ and width of $\\tau$. Sah recently discussed the problem of pair production after a finite evolution time [9].","This raises questions about the instantaneous appearance of particles in pair production and their behavior at intermediate times when using a formalism that involves solving an evolution equation for a dynamical quantity.","Is it possible to make general statements about this behavior?","To address these questions, we analytically compute the probability of $(e^+ e^-)$ pair production in momentum space using the exact solution of the one-particle time-dependent Dirac equation, and we compare the result with quantum kinetic theory (QKT).","Both approaches allow us to study the particle momentum spectrum at any instant in time and can potentially unveil valuable information regarding quantum non-equilibrium physics.","We analyze both approaches' Longitudinal Momentum Spectrum (LMS) of the created particles at finite times.","We observe oscillatory structure in the LMS.","This oscillation behavior at finite time clearly illustrates the quantum interference effects associated with particle production.","It is worth noting that both approaches exhibit quantum interference patterns at finite times, manifested as oscillations observed in the LMS.","This reveals that these oscillations are not due to transient excitations and basis-dependent signatures.","Again, we emphasize that the oscillations seen in the LMS from both approaches are not artifacts but possess significant physical relevance."],"url":"http://arxiv.org/abs/2405.02947v1","category":"hep-ph"}
{"created":"2024-05-05 14:12:48","title":"Imaging Signal Recovery Using Neural Network Priors Under Uncertain Forward Model Parameters","abstract":"Inverse imaging problems (IIPs) arise in various applications, with the main objective of reconstructing an image from its compressed measurements. This problem is often ill-posed for being under-determined with multiple interchangeably consistent solutions. The best solution inherently depends on prior knowledge or assumptions, such as the sparsity of the image. Furthermore, the reconstruction process for most IIPs relies significantly on the imaging (i.e. forward model) parameters, which might not be fully known, or the measurement device may undergo calibration drifts. These uncertainties in the forward model create substantial challenges, where inaccurate reconstructions usually happen when the postulated parameters of the forward model do not fully match the actual ones. In this work, we devoted to tackling accurate reconstruction under the context of a set of possible forward model parameters that exist. Here, we propose a novel Moment-Aggregation (MA) framework that is compatible with the popular IIP solution by using a neural network prior. Specifically, our method can reconstruct the signal by considering all candidate parameters of the forward model simultaneously during the update of the neural network. We theoretically demonstrate the convergence of the MA framework, which has a similar complexity with reconstruction under the known forward model parameters. Proof-of-concept experiments demonstrate that the proposed MA achieves performance comparable to the forward model with the known precise parameter in reconstruction across both compressive sensing and phase retrieval applications, with a PSNR gap of 0.17 to 1.94 over various datasets, including MNIST, X-ray, Glas, and MoNuseg. This highlights our method's significant potential in reconstruction under an uncertain forward model.","sentences":["Inverse imaging problems (IIPs) arise in various applications, with the main objective of reconstructing an image from its compressed measurements.","This problem is often ill-posed for being under-determined with multiple interchangeably consistent solutions.","The best solution inherently depends on prior knowledge or assumptions, such as the sparsity of the image.","Furthermore, the reconstruction process for most IIPs relies significantly on the imaging (i.e. forward model) parameters, which might not be fully known, or the measurement device may undergo calibration drifts.","These uncertainties in the forward model create substantial challenges, where inaccurate reconstructions usually happen when the postulated parameters of the forward model do not fully match the actual ones.","In this work, we devoted to tackling accurate reconstruction under the context of a set of possible forward model parameters that exist.","Here, we propose a novel Moment-Aggregation (MA) framework that is compatible with the popular IIP solution by using a neural network prior.","Specifically, our method can reconstruct the signal by considering all candidate parameters of the forward model simultaneously during the update of the neural network.","We theoretically demonstrate the convergence of the MA framework, which has a similar complexity with reconstruction under the known forward model parameters.","Proof-of-concept experiments demonstrate that the proposed MA achieves performance comparable to the forward model with the known precise parameter in reconstruction across both compressive sensing and phase retrieval applications, with a PSNR gap of 0.17 to 1.94 over various datasets, including MNIST, X-ray, Glas, and MoNuseg.","This highlights our method's significant potential in reconstruction under an uncertain forward model."],"url":"http://arxiv.org/abs/2405.02944v1","category":"cs.CV"}
{"created":"2024-05-05 14:07:39","title":"On the incompleteness of $G_2$-moduli spaces along degenerating families of $G_2$-manifolds","abstract":"We derive a formula for the energy of a path in the moduli space of a compact $G_2$-manifold with vanishing first Betti number for the volume-normalised $L^2$-metric. This allows us to give simple sufficient conditions for a path of torsion-free $G_2$-structures to have finite energy and length. We deduce that the compact $G_2$-manifolds produced by the generalised Kummer construction have incomplete moduli spaces. Under some assumptions, we also state a necessary condition for the limit of a path of torsion-free $G_2$-structures to be at infinite distance in the moduli space.","sentences":["We derive a formula for the energy of a path in the moduli space of a compact $G_2$-manifold with vanishing first Betti number for the volume-normalised $L^2$-metric.","This allows us to give simple sufficient conditions for a path of torsion-free $G_2$-structures to have finite energy and length.","We deduce that the compact $G_2$-manifolds produced by the generalised Kummer construction have incomplete moduli spaces.","Under some assumptions, we also state a necessary condition for the limit of a path of torsion-free $G_2$-structures to be at infinite distance in the moduli space."],"url":"http://arxiv.org/abs/2405.02943v1","category":"math.DG"}
{"created":"2024-05-05 14:00:27","title":"Pogorelov type estimates for semi-convex solutions of Hessian equations and related rigidity theorems","abstract":"In this paper, we establish Pogorelov type $C^2$ estimates for semi-convex admissible solutions to the Dirichlet problem of $k$-Hessian equation with general right hand side. Under some sufficient conditions, we apply such estimates to obtain rigidity theorems for semi-convex admissible solutions of $k$-Hessian equation, which can be seen as a improvement of Li-Ren-Wang and Chu-Dinew's rigidity theorem for $k$-Hessian equation. When $2k>n$, we also obtain Pogorelov type $C^2$ estimates for admissible solutions to the Dirichlet problem of $k$-Hessian equation based on a concavity inequality, which is inspired by the Ren-Wang's work on the global curvature estimates for the $n-1$ and $n-2$ curvature equation.","sentences":["In this paper, we establish Pogorelov type $C^2$ estimates for semi-convex admissible solutions to the Dirichlet problem of $k$-Hessian equation with general right hand side.","Under some sufficient conditions, we apply such estimates to obtain rigidity theorems for semi-convex admissible solutions of $k$-Hessian equation, which can be seen as a improvement of Li-Ren-Wang and Chu-Dinew's rigidity theorem for $k$-Hessian equation.","When $2k>n$, we also obtain Pogorelov type $C^2$ estimates for admissible solutions to the Dirichlet problem of $k$-Hessian equation based on a concavity inequality, which is inspired by the Ren-Wang's work on the global curvature estimates for the $n-1$ and $n-2$ curvature equation."],"url":"http://arxiv.org/abs/2405.02939v1","category":"math.AP"}
{"created":"2024-05-05 13:54:02","title":"Enabling Patient-side Disease Prediction via the Integration of Patient Narratives","abstract":"Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures. However, most recent disease prediction approaches heavily rely on laboratory test outcomes (e.g., blood tests and medical imaging from X-rays). Gaining access to such data for precise disease prediction is often a complex task from the standpoint of a patient and is always only available post-patient consultation. To make disease prediction available from patient-side, we propose Personalized Medical Disease Prediction (PoMP), which predicts diseases using patient health narratives including textual descriptions and demographic information. By applying PoMP, patients can gain a clearer comprehension of their conditions, empowering them to directly seek appropriate medical specialists and thereby reducing the time spent navigating healthcare communication to locate suitable doctors. We conducted extensive experiments using real-world data from Haodf to showcase the effectiveness of PoMP.","sentences":["Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures.","However, most recent disease prediction approaches heavily rely on laboratory test outcomes (e.g., blood tests and medical imaging from X-rays).","Gaining access to such data for precise disease prediction is often a complex task from the standpoint of a patient and is always only available post-patient consultation.","To make disease prediction available from patient-side, we propose Personalized Medical Disease Prediction (PoMP), which predicts diseases using patient health narratives including textual descriptions and demographic information.","By applying PoMP, patients can gain a clearer comprehension of their conditions, empowering them to directly seek appropriate medical specialists and thereby reducing the time spent navigating healthcare communication to locate suitable doctors.","We conducted extensive experiments using real-world data from Haodf to showcase the effectiveness of PoMP."],"url":"http://arxiv.org/abs/2405.02935v1","category":"cs.CL"}
{"created":"2024-05-05 13:45:18","title":"Projected gradient descent algorithm for $\\textit{ab initio}$ crystal structure relaxation under a fixed unit cell volume","abstract":"This paper is concerned with $\\textit{ab initio}$ crystal structure relaxation under a fixed unit cell volume, which is a step in calculating the static equations of state and forms the basis of thermodynamic property calculations for materials. The task can be formulated as an energy minimization with a determinant constraint. Widely used line minimization-based methods (e.g., conjugate gradient method) lack both efficiency and convergence guarantees due to the nonconvex nature of the feasible region as well as the significant differences in the curvatures of the potential energy surface with respect to atomic and lattice components. To this end, we propose a projected gradient descent algorithm named PANBB. It is equipped with (i) search direction projections onto the tangent spaces of the nonconvex feasible region for lattice vectors, (ii) distinct curvature-aware initial trial step sizes for atomic and lattice updates, and (iii) a nonrestrictive line minimization criterion as the stopping rule for the inner loop. It can be proved that PANBB favors theoretical convergence to equilibrium states. Across a benchmark set containing 223 structures from various categories, PANBB achieves average speedup factors of approximately 1.41 and 1.45 over the conjugate gradient method and direct inversion in the iterative subspace implemented in off-the-shelf simulation software, respectively. Moreover, it normally converges on all the systems, manifesting its unparalleled robustness. As an application, we calculate the static equations of state for the high-entropy alloy AlCoCrFeNi, which remains elusive owing to 160 atoms representing both chemical and magnetic disorder and the strong local lattice distortion. The results are consistent with the previous calculations and are further validated by experimental thermodynamic data.","sentences":["This paper is concerned with $\\textit{ab initio}$ crystal structure relaxation under a fixed unit cell volume, which is a step in calculating the static equations of state and forms the basis of thermodynamic property calculations for materials.","The task can be formulated as an energy minimization with a determinant constraint.","Widely used line minimization-based methods (e.g., conjugate gradient method) lack both efficiency and convergence guarantees due to the nonconvex nature of the feasible region as well as the significant differences in the curvatures of the potential energy surface with respect to atomic and lattice components.","To this end, we propose a projected gradient descent algorithm named PANBB.","It is equipped with (i) search direction projections onto the tangent spaces of the nonconvex feasible region for lattice vectors, (ii) distinct curvature-aware initial trial step sizes for atomic and lattice updates, and (iii) a nonrestrictive line minimization criterion as the stopping rule for the inner loop.","It can be proved that PANBB favors theoretical convergence to equilibrium states.","Across a benchmark set containing 223 structures from various categories, PANBB achieves average speedup factors of approximately 1.41 and 1.45 over the conjugate gradient method and direct inversion in the iterative subspace implemented in off-the-shelf simulation software, respectively.","Moreover, it normally converges on all the systems, manifesting its unparalleled robustness.","As an application, we calculate the static equations of state for the high-entropy alloy AlCoCrFeNi, which remains elusive owing to 160 atoms representing both chemical and magnetic disorder and the strong local lattice distortion.","The results are consistent with the previous calculations and are further validated by experimental thermodynamic data."],"url":"http://arxiv.org/abs/2405.02934v1","category":"physics.comp-ph"}
{"created":"2024-05-05 13:26:31","title":"Optimal Signals and Detectors Based on Correlation and Energy","abstract":"In continuation of an earlier study, we explore a Neymann-Pearson hypothesis testing scenario where, under the null hypothesis ($\\cal{H}_0$), the received signal is a white noise process $N_t$, which is not Gaussian in general, and under the alternative hypothesis ($\\cal{H}_1$), the received signal comprises a deterministic transmitted signal $s_t$ corrupted by additive white noise, the sum of $N_t$ and another noise process originating from the transmitter, denoted as $Z_t$, which is not necessarily Gaussian either. Our approach focuses on detectors that are based on the correlation and energy of the received signal, which are motivated by implementation simplicity. We optimize the detector parameters to achieve the best trade-off between missed-detection and false-alarm error exponents. First, we optimize the detectors for a given signal, resulting in a non-linear relation between the signal and correlator weights to be optimized. Subsequently, we optimize the transmitted signal and the detector parameters jointly, revealing that the optimal signal is a balanced ternary signal and the correlator has at most three different coefficients, thus facilitating a computationally feasible solution.","sentences":["In continuation of an earlier study, we explore a Neymann-Pearson hypothesis testing scenario where, under the null hypothesis ($\\cal{H}_0$), the received signal is a white noise process $N_t$, which is not Gaussian in general, and under the alternative hypothesis ($\\cal{H}_1$), the received signal comprises a deterministic transmitted signal $s_t$ corrupted by additive white noise, the sum of $N_t$ and another noise process originating from the transmitter, denoted as $Z_t$, which is not necessarily Gaussian either.","Our approach focuses on detectors that are based on the correlation and energy of the received signal, which are motivated by implementation simplicity.","We optimize the detector parameters to achieve the best trade-off between missed-detection and false-alarm error exponents.","First, we optimize the detectors for a given signal, resulting in a non-linear relation between the signal and correlator weights to be optimized.","Subsequently, we optimize the transmitted signal and the detector parameters jointly, revealing that the optimal signal is a balanced ternary signal and the correlator has at most three different coefficients, thus facilitating a computationally feasible solution."],"url":"http://arxiv.org/abs/2405.02931v1","category":"cs.IT"}
{"created":"2024-05-05 13:10:28","title":"Probabilistic cellular automata with local transition matrices: synchronization, ergodicity, and inference","abstract":"We introduce a new class of probabilistic cellular automata that are capable of exhibiting rich dynamics such as synchronization and ergodicity and can be easily inferred from data. The system is a finite-state locally interacting Markov chain on a circular graph. Each site's subsequent state is random, with a distribution determined by its neighborhood's empirical distribution multiplied by a local transition matrix. We establish sufficient and necessary conditions on the local transition matrix for synchronization and ergodicity. Also, we introduce novel least squares estimators for inferring the local transition matrix from various types of data, which may consist of either multiple trajectories, a long trajectory, or ensemble sequences without trajectory information. Under suitable identifiability conditions, we show the asymptotic normality of these estimators and provide non-asymptotic bounds for their accuracy.","sentences":["We introduce a new class of probabilistic cellular automata that are capable of exhibiting rich dynamics such as synchronization and ergodicity and can be easily inferred from data.","The system is a finite-state locally interacting Markov chain on a circular graph.","Each site's subsequent state is random, with a distribution determined by its neighborhood's empirical distribution multiplied by a local transition matrix.","We establish sufficient and necessary conditions on the local transition matrix for synchronization and ergodicity.","Also, we introduce novel least squares estimators for inferring the local transition matrix from various types of data, which may consist of either multiple trajectories, a long trajectory, or ensemble sequences without trajectory information.","Under suitable identifiability conditions, we show the asymptotic normality of these estimators and provide non-asymptotic bounds for their accuracy."],"url":"http://arxiv.org/abs/2405.02928v1","category":"math.PR"}
{"created":"2024-05-05 13:06:30","title":"Optimal Sampling for Uncertainty-of-Information Minimization in a Remote Monitoring System","abstract":"In this paper, we study a remote monitoring system where a receiver observes a remote binary Markov source and decides whether to sample and fetch the source's state over a randomly delayed channel. Due to transmission delay, the observation of the source is imperfect, resulting in the uncertainty of the source's state at the receiver. We thus use uncertainty of information as the metric to characterize the performance of the system. Measured by Shannon's entropy, uncertainty of information reflects how much we do not know about the latest source's state in the absence of new information. The current research for uncertainty of information idealizes the transmission delay as one time slot, but not under random delay. Moreover, uncertainty of information varies with the latest observation of the source's state, making it different from other age of information related functions. Motivated by the above reasons, we formulate a uncertainty of information minimization problem under random delay. Typically, such a problem which takes actions based on the imperfect observations can be modeled as a partially observed Markov decision process. By introducing belief state, we transform this process into a semi-Markov decision process. To solve this problem, we first provide an optimal sampling policy employing a two layered bisection relative value iteration algorithm. Furthermore, we propose a sub-optimal index policy with low complexity based on the special properties of belief state. Numerical simulations illustrate that both of the proposed sampling policies outperforms two other benchmarks. Moreover, the performance of the sub-optimal policy approaches to that of the optimal policy, particularly under large delay.","sentences":["In this paper, we study a remote monitoring system where a receiver observes a remote binary Markov source and decides whether to sample and fetch the source's state over a randomly delayed channel.","Due to transmission delay, the observation of the source is imperfect, resulting in the uncertainty of the source's state at the receiver.","We thus use uncertainty of information as the metric to characterize the performance of the system.","Measured by Shannon's entropy, uncertainty of information reflects how much we do not know about the latest source's state in the absence of new information.","The current research for uncertainty of information idealizes the transmission delay as one time slot, but not under random delay.","Moreover, uncertainty of information varies with the latest observation of the source's state, making it different from other age of information related functions.","Motivated by the above reasons, we formulate a uncertainty of information minimization problem under random delay.","Typically, such a problem which takes actions based on the imperfect observations can be modeled as a partially observed Markov decision process.","By introducing belief state, we transform this process into a semi-Markov decision process.","To solve this problem, we first provide an optimal sampling policy employing a two layered bisection relative value iteration algorithm.","Furthermore, we propose a sub-optimal index policy with low complexity based on the special properties of belief state.","Numerical simulations illustrate that both of the proposed sampling policies outperforms two other benchmarks.","Moreover, the performance of the sub-optimal policy approaches to that of the optimal policy, particularly under large delay."],"url":"http://arxiv.org/abs/2405.02924v1","category":"cs.IT"}
{"created":"2024-05-06 17:38:20","title":"Competitive strategies to use \"warm start\" algorithms with predictions","abstract":"We consider the problem of learning and using predictions for warm start algorithms with predictions. In this setting, an algorithm is given an instance of a problem, and a prediction of the solution. The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance. Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function. In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost. We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move. We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories. This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$. Thus the guarantee holds for all $k$ simultaneously.","sentences":["We consider the problem of learning and using predictions for warm start algorithms with predictions.","In this setting, an algorithm is given an instance of a problem, and a prediction of the solution.","The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance.","Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   ","In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function.","In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost.","We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   ","Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move.","We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories.","This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$.","Thus the guarantee holds for all $k$ simultaneously."],"url":"http://arxiv.org/abs/2405.03661v1","category":"cs.DS"}
{"created":"2024-05-06 15:59:29","title":"Dissipative gradient nonlinearities prevent $\u03b4$-formations in local and nonlocal attraction-repulsion chemotaxis models","abstract":"We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent. Additionally, a source also involving some expression of the gradient of the species is incorporated.","sentences":["We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent.","Additionally, a source also involving some expression of the gradient of the species is incorporated."],"url":"http://arxiv.org/abs/2405.03586v1","category":"math.AP"}
{"created":"2024-05-06 14:53:32","title":"Asymptotic-preserving hybridizable discontinuous Galerkin method for the Westervelt quasilinear wave equation","abstract":"We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves. More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit. Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv = \\nabla \\psi$. The established theoretical results are illustrated with some numerical experiments.","sentences":["We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves.","More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit.","Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv","= \\nabla \\psi$.","The established theoretical results are illustrated with some numerical experiments."],"url":"http://arxiv.org/abs/2405.03535v1","category":"math.NA"}
{"created":"2024-05-06 14:41:46","title":"On anomalous dissipation induced by transport noise","abstract":"In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions. The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs. The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time. Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity. Further, we discuss physical interpretations.","sentences":["In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions.","The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs.","The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time.","Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity.","Further, we discuss physical interpretations."],"url":"http://arxiv.org/abs/2405.03525v1","category":"math.AP"}
{"created":"2024-05-06 12:58:21","title":"Annealed adaptive importance sampling method in PINNs for solving high dimensional partial differential equations","abstract":"Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs). However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima. To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density. Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs. Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets. Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs. The adaptive sampling framework introduced here can be integrated into various PINN frameworks.","sentences":["Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs).","However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima.","To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density.","Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs.","Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets.","Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs.","The adaptive sampling framework introduced here can be integrated into various PINN frameworks."],"url":"http://arxiv.org/abs/2405.03433v1","category":"math.NA"}
{"created":"2024-05-06 12:28:42","title":"Adaptive Accelerated Composite Minimization","abstract":"The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms. Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant. In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent. We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo). Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem).","sentences":["The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms.","Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant.","In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent.","We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo).","Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem)."],"url":"http://arxiv.org/abs/2405.03414v1","category":"math.OC"}
{"created":"2024-05-06 11:02:02","title":"Modality Prompts for Arbitrary Modality Salient Object Detection","abstract":"This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images. A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy. Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality. In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts. Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features. Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion. For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively. Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation.","sentences":["This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images.","A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy.","Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality.","In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts.","Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features.","Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion.","For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively.","Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation."],"url":"http://arxiv.org/abs/2405.03351v1","category":"cs.CV"}
{"created":"2024-05-06 10:50:17","title":"An efficient hierarchical Bayesian method for the Kuopio tomography challenge 2023","abstract":"The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary. Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem. The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky. Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner. Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection. The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections.","sentences":["The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary.","Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem.","The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky.","Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner.","Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection.","The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections."],"url":"http://arxiv.org/abs/2405.03343v1","category":"math.NA"}
{"created":"2024-05-06 09:02:54","title":"A continuous approach for computing the pseudospectra of linear operators","abstract":"We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy. Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions. The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned. The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method.","sentences":["We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy.","Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions.","The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned.","The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method."],"url":"http://arxiv.org/abs/2405.03285v1","category":"math.NA"}
{"created":"2024-05-06 04:36:02","title":"DeepMpMRI: Tensor-decomposition Regularized Learning for Fast and High-Fidelity Multi-Parametric Microstructural MR Imaging","abstract":"Deep learning has emerged as a promising approach for learning the nonlinear mapping between diffusion-weighted MR images and tissue parameters, which enables automatic and deep understanding of the brain microstructures. However, the efficiency and accuracy in the multi-parametric estimations are still limited since previous studies tend to estimate multi-parametric maps with dense sampling and isolated signal modeling. This paper proposes DeepMpMRI, a unified framework for fast and high-fidelity multi-parametric estimation from various diffusion models using sparsely sampled q-space data. DeepMpMRI is equipped with a newly designed tensor-decomposition-based regularizer to effectively capture fine details by exploiting the correlation across parameters. In addition, we introduce a Nesterov-based adaptive learning algorithm that optimizes the regularization parameter dynamically to enhance the performance. DeepMpMRI is an extendable framework capable of incorporating flexible network architecture. Experimental results demonstrate the superiority of our approach over 5 state-of-the-art methods in simultaneously estimating multi-parametric maps for various diffusion models with fine-grained details both quantitatively and qualitatively, achieving 4.5 - 22.5$\\times$ acceleration compared to the dense sampling of a total of 270 diffusion gradients.","sentences":["Deep learning has emerged as a promising approach for learning the nonlinear mapping between diffusion-weighted MR images and tissue parameters, which enables automatic and deep understanding of the brain microstructures.","However, the efficiency and accuracy in the multi-parametric estimations are still limited since previous studies tend to estimate multi-parametric maps with dense sampling and isolated signal modeling.","This paper proposes DeepMpMRI, a unified framework for fast and high-fidelity multi-parametric estimation from various diffusion models using sparsely sampled q-space data.","DeepMpMRI is equipped with a newly designed tensor-decomposition-based regularizer to effectively capture fine details by exploiting the correlation across parameters.","In addition, we introduce a Nesterov-based adaptive learning algorithm that optimizes the regularization parameter dynamically to enhance the performance.","DeepMpMRI is an extendable framework capable of incorporating flexible network architecture.","Experimental results demonstrate the superiority of our approach over 5 state-of-the-art methods in simultaneously estimating multi-parametric maps for various diffusion models with fine-grained details both quantitatively and qualitatively, achieving 4.5 - 22.5$\\times$ acceleration compared to the dense sampling of a total of 270 diffusion gradients."],"url":"http://arxiv.org/abs/2405.03159v1","category":"cs.CV"}
{"created":"2024-05-06 03:39:50","title":"PTQ4SAM: Post-Training Quantization for Segment Anything","abstract":"Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks. However, as a large-scale model, the immense memory and computation costs hinder its practical deployment. In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM. First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations. We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline. Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions. Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly. Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM. For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration. The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}.","sentences":["Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks.","However, as a large-scale model, the immense memory and computation costs hinder its practical deployment.","In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM.","First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations.","We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline.","Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions.","Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly.","Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM.","For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration.","The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}."],"url":"http://arxiv.org/abs/2405.03144v1","category":"cs.CV"}
{"created":"2024-05-06 02:42:17","title":"MambaJSCC: Deep Joint Source-Channel Coding with Visual State Space Model","abstract":"Lightweight and efficient deep joint source-channel coding (JSCC) is a key technology for semantic communications. In this paper, we design a novel JSCC scheme named MambaJSCC, which utilizes a visual state space model with channel adaptation (VSSM-CA) block as its backbone for transmitting images over wireless channels. The VSSM-CA block utilizes VSSM to integrate two-dimensional images with the state space, enabling feature extraction and encoding processes to operate with linear complexity. It also incorporates channel state information (CSI) via a newly proposed CSI embedding method. This method deploys a shared CSI encoding module within both the encoder and decoder to encode and inject the CSI into each VSSM-CA block, improving the adaptability of a single model to varying channel conditions. Experimental results show that MambaJSCC not only outperforms Swin Transformer based JSCC (SwinJSCC) but also significantly reduces parameter size, computational overhead, and inference delay (ID). For example, with employing an equal number of the VSSM-CA blocks and the Swin Transformer blocks, MambaJSCC achieves a 0.48 dB gain in peak-signal-to-noise ratio (PSNR) over SwinJSCC while requiring only 53.3% multiply-accumulate operations, 53.8% of the parameters, and 44.9% of ID.","sentences":["Lightweight and efficient deep joint source-channel coding (JSCC) is a key technology for semantic communications.","In this paper, we design a novel JSCC scheme named MambaJSCC, which utilizes a visual state space model with channel adaptation (VSSM-CA) block as its backbone for transmitting images over wireless channels.","The VSSM-CA block utilizes VSSM to integrate two-dimensional images with the state space, enabling feature extraction and encoding processes to operate with linear complexity.","It also incorporates channel state information (CSI) via a newly proposed CSI embedding method.","This method deploys a shared CSI encoding module within both the encoder and decoder to encode and inject the CSI into each VSSM-CA block, improving the adaptability of a single model to varying channel conditions.","Experimental results show that MambaJSCC not only outperforms Swin Transformer based JSCC (SwinJSCC) but also significantly reduces parameter size, computational overhead, and inference delay (ID).","For example, with employing an equal number of the VSSM-CA blocks and the Swin Transformer blocks, MambaJSCC achieves a 0.48 dB gain in peak-signal-to-noise ratio (PSNR) over SwinJSCC while requiring only 53.3% multiply-accumulate operations, 53.8% of the parameters, and 44.9% of ID."],"url":"http://arxiv.org/abs/2405.03125v1","category":"cs.IT"}
{"created":"2024-05-06 02:42:01","title":"Dimension of homogeneous iterated function systems with algebraic translations","abstract":"Let $ \\mu $ be the self-similar measure associated with a homogeneous iterated function system $ \\Phi = \\{ \\lambda x + t_j \\}_{j=1}^m $ on ${\\Bbb R}$ and a probability vector $ (p_{j})_{j=1}^m$, where $0\\neq \\lambda\\in (-1,1)$ and $t_j\\in {\\Bbb R}$. Recently by modifying the arguments of Varj\\'u (2019), Rapaport and Varj\\'u (2024) showed that if $t_1,\\ldots, t_m$ are rational numbers and $0<\\lambda<1$, then $$ \\dim \\mu =\\min\\Big \\{ 1, \\; \\frac{\\sum_{j=1}^m p_{j}\\log p_{j}}{ \\log |\\lambda| }\\Big\\}$$ unless $ \\Phi $ has exact overlaps. In this paper, we further show that the above equality holds in the case when $t_1,\\ldots, t_m$ are algebraic numbers and $0<|\\lambda|<1$. This is done by adapting and extending the ideas employed in the recent papers of Breuillard, Rapaport and Varj\\'u.","sentences":["Let $ \\mu $ be the self-similar measure associated with a homogeneous iterated function system $ \\Phi = \\{ \\lambda x + t_j \\}_{j=1}^m $ on ${\\Bbb R}$ and a probability vector $ (p_{j})_{j=1}^m$, where $0\\neq \\lambda\\in (-1,1)$ and $t_j\\in {\\Bbb R}$. Recently by modifying the arguments of Varj\\'u (2019), Rapaport and Varj\\'u (2024) showed that if $t_1,\\ldots, t_m$ are rational numbers and $0<\\lambda<1$, then $$ \\dim \\mu =\\min\\Big \\{ 1, \\; \\frac{\\sum_{j=1}^m p_{j}\\log p_{j}}{ \\log |\\lambda| }\\Big\\}$$ unless $ \\Phi $ has exact overlaps.","In this paper, we further show that the above equality holds in the case when $t_1,\\ldots, t_m$ are algebraic numbers and $0<|\\lambda|<1$. This is done by adapting and extending the ideas employed in the recent papers of Breuillard, Rapaport and Varj\\'u."],"url":"http://arxiv.org/abs/2405.03124v1","category":"math.DS"}
{"created":"2024-05-06 01:54:21","title":"Impact of Postshock Turbulence on the Radio Spectrum of Radio Relic Shocks in Merging Clusters","abstract":"This study investigates the impact of magnetic turbulence on cosmic ray (CR) electrons through Fermi-II acceleration behind merger-driven shocks in the intracluster medium and examines how the ensuing synchrotron radio emission is influenced by the decay of magnetic energy through dissipation in the postshock region. We adopt simplified models for the momentum diffusion coefficient, specifically considering transit-time-damping resonance with fast-mode waves and gyroresonance with Alfv\\'en waves. Utilizing analytic solutions derived from diffusive shock acceleration theory, at the shock location, we introduce a CR spectrum that is either shock-injected or shock-reaccelerated. We then track its temporal evolution along the Lagrangian fluid element in the time domain. The resulting CR spectra are mapped onto a spherical shell configuration to estimate the surface brightness profile of the model radio relics. Turbulent acceleration proves to be a significant factor in delaying the aging of postshock CR electrons, while decaying magnetic fields have marginal impacts due to the dominance of inverse Compton cooling over synchrotron cooling. However, the decay of magnetic fields substantially reduces synchrotron radiation. Consequently, the spatial distribution of the postshock magnetic fields affects the volume-integrated radio spectrum and its spectral index. We demonstrate that the Mach numbers estimated from the integrated spectral index tend to be higher than the actual shock Mach numbers, highlighting the necessity for accurate modeling of postshock magnetic turbulence in interpreting observations of radio relics.","sentences":["This study investigates the impact of magnetic turbulence on cosmic ray (CR) electrons through Fermi-II acceleration behind merger-driven shocks in the intracluster medium and examines how the ensuing synchrotron radio emission is influenced by the decay of magnetic energy through dissipation in the postshock region.","We adopt simplified models for the momentum diffusion coefficient, specifically considering transit-time-damping resonance with fast-mode waves and gyroresonance with Alfv\\'en waves.","Utilizing analytic solutions derived from diffusive shock acceleration theory, at the shock location, we introduce a CR spectrum that is either shock-injected or shock-reaccelerated.","We then track its temporal evolution along the Lagrangian fluid element in the time domain.","The resulting CR spectra are mapped onto a spherical shell configuration to estimate the surface brightness profile of the model radio relics.","Turbulent acceleration proves to be a significant factor in delaying the aging of postshock CR electrons, while decaying magnetic fields have marginal impacts due to the dominance of inverse Compton cooling over synchrotron cooling.","However, the decay of magnetic fields substantially reduces synchrotron radiation.","Consequently, the spatial distribution of the postshock magnetic fields affects the volume-integrated radio spectrum and its spectral index.","We demonstrate that the Mach numbers estimated from the integrated spectral index tend to be higher than the actual shock Mach numbers, highlighting the necessity for accurate modeling of postshock magnetic turbulence in interpreting observations of radio relics."],"url":"http://arxiv.org/abs/2405.03108v1","category":"astro-ph.HE"}
{"created":"2024-05-06 01:38:11","title":"Linear-Quadratic Mean Field Stackelberg Stochastic Differential Game with Partial Information and Common Noise","abstract":"This paper is concerned with a linear-quadratic mean field Stackelberg stochastic differential game with partial information and common noise, which contains a leader and a large number of followers. To be specific, the followers face a large population Nash game after the leader first announces his strategy, while the leader will then optimize his own cost functional on consideration of the followers' reactions. The state equation of the leader and followers are both general stochastic differential equations, where the diffusion terms contain both the control and state variables. However, the followers' average state terms enter into the drift term of the leader's state equation, reflecting that the leader's state is influenced by the followers' states. By virtue of stochastic maximum principle with partial information and optimal filter technique, we deduce the open-loop adapted decentralized strategies and feedback decentralized strategies of this leader-followers system, and demonstrate that the decentralized strategies are the corresponding $\\varepsilon$-Stackelberg-Nash equilibrium.","sentences":["This paper is concerned with a linear-quadratic mean field Stackelberg stochastic differential game with partial information and common noise, which contains a leader and a large number of followers.","To be specific, the followers face a large population Nash game after the leader first announces his strategy, while the leader will then optimize his own cost functional on consideration of the followers' reactions.","The state equation of the leader and followers are both general stochastic differential equations, where the diffusion terms contain both the control and state variables.","However, the followers' average state terms enter into the drift term of the leader's state equation, reflecting that the leader's state is influenced by the followers' states.","By virtue of stochastic maximum principle with partial information and optimal filter technique, we deduce the open-loop adapted decentralized strategies and feedback decentralized strategies of this leader-followers system, and demonstrate that the decentralized strategies are the corresponding $\\varepsilon$-Stackelberg-Nash equilibrium."],"url":"http://arxiv.org/abs/2405.03102v1","category":"math.OC"}
{"created":"2024-05-05 22:54:43","title":"AnoGAN for Tabular Data: A Novel Approach to Anomaly Detection","abstract":"Anomaly detection, a critical facet in data analysis, involves identifying patterns that deviate from expected behavior. This research addresses the complexities inherent in anomaly detection, exploring challenges and adapting to sophisticated malicious activities. With applications spanning cybersecurity, healthcare, finance, and surveillance, anomalies often signify critical information or potential threats. Inspired by the success of Anomaly Generative Adversarial Network (AnoGAN) in image domains, our research extends its principles to tabular data. Our contributions include adapting AnoGAN's principles to a new domain and promising advancements in detecting previously undetectable anomalies. This paper delves into the multifaceted nature of anomaly detection, considering the dynamic evolution of normal behavior, context-dependent anomaly definitions, and data-related challenges like noise and imbalances.","sentences":["Anomaly detection, a critical facet in data analysis, involves identifying patterns that deviate from expected behavior.","This research addresses the complexities inherent in anomaly detection, exploring challenges and adapting to sophisticated malicious activities.","With applications spanning cybersecurity, healthcare, finance, and surveillance, anomalies often signify critical information or potential threats.","Inspired by the success of Anomaly Generative Adversarial Network (AnoGAN) in image domains, our research extends its principles to tabular data.","Our contributions include adapting AnoGAN's principles to a new domain and promising advancements in detecting previously undetectable anomalies.","This paper delves into the multifaceted nature of anomaly detection, considering the dynamic evolution of normal behavior, context-dependent anomaly definitions, and data-related challenges like noise and imbalances."],"url":"http://arxiv.org/abs/2405.03075v1","category":"cs.LG"}
{"created":"2024-05-05 21:20:46","title":"A Greedy Quantum Route-Generation Algorithm","abstract":"Routing and scheduling problems with time windows have long been important optimization problems for logistics and planning. Many classical heuristics and exact methods exist for such problems. However, there are no satisfactory methods for generating routes using quantum computing (QC), for mainly two reasons: inequality constraints, and the trade-off of feasibility and solution quality. Inequality constraints are typically handled using slack variables; and feasible solutions are found by filtering samples. These challenges are amplified in the presence of noise inherent in QC. Here, we propose a greedy algorithm that generates routes by using information from all samples obtained from the quantum computer. By noticing the relationship between qubits in our formulation as a directed acyclic graph (DAG), we designed an algorithm that adaptively constructs a feasible solution.   We prove its convergence to a feasible solution, and illustrate its efficacy by solving the Fleet Sizing Vehicle Routing Problem with Time Windows (FSVRPTW). Our computational results show that this method obtains a lower objective value than the current state-of-the-art annealing approaches, both classical and hybrid, for the same amount of time using D-Wave Hybrid Solvers. We also show its robustness to noise on D-Wave Advantage 4.1 through computational results as compared to the filtering approach on DWaveSampler, even when the filtering approach is given a longer annealing time, and a larger sample size.","sentences":["Routing and scheduling problems with time windows have long been important optimization problems for logistics and planning.","Many classical heuristics and exact methods exist for such problems.","However, there are no satisfactory methods for generating routes using quantum computing (QC), for mainly two reasons: inequality constraints, and the trade-off of feasibility and solution quality.","Inequality constraints are typically handled using slack variables; and feasible solutions are found by filtering samples.","These challenges are amplified in the presence of noise inherent in QC.","Here, we propose a greedy algorithm that generates routes by using information from all samples obtained from the quantum computer.","By noticing the relationship between qubits in our formulation as a directed acyclic graph (DAG), we designed an algorithm that adaptively constructs a feasible solution.   ","We prove its convergence to a feasible solution, and illustrate its efficacy by solving the Fleet Sizing Vehicle Routing Problem with Time Windows (FSVRPTW).","Our computational results show that this method obtains a lower objective value than the current state-of-the-art annealing approaches, both classical and hybrid, for the same amount of time using D-Wave Hybrid Solvers.","We also show its robustness to noise on D-Wave Advantage 4.1 through computational results as compared to the filtering approach on DWaveSampler, even when the filtering approach is given a longer annealing time, and a larger sample size."],"url":"http://arxiv.org/abs/2405.03054v1","category":"quant-ph"}
{"created":"2024-05-05 15:00:10","title":"Silk Damping in Scalar-Induced Gravitational Waves: A Novel Probe for New Physics","abstract":"Silk damping is well known in the study of cosmic microwave background (CMB) and accounts for suppression of the angular power spectrum of CMB on large angular multipoles. In this Letter, we study the effect of Silk damping on the scalar-induced gravitational waves (SIGWs). Resulting from the dissipation of cosmic fluid, the Silk damping notably suppresses the energy-density spectrum of SIGWs on scales comparable to a diffusion scale at the decoupling time of feebly-interacting particles. The effect offers a novel observable for probing the underlying particle interaction, especially for those mediated by heavy gauge bosons beyond the standard model of particles. We anticipate that pulsar timing arrays are sensitive to gauge bosons with mass $\\sim10^{3}-10^{4}\\,\\mathrm{GeV}$, while space- and ground-based interferometers to those with mass $\\sim10^7-10^{12}\\,\\mathrm{GeV}$, leading to essential complements to on-going and future experiments of high-energy physics.","sentences":["Silk damping is well known in the study of cosmic microwave background (CMB) and accounts for suppression of the angular power spectrum of CMB on large angular multipoles.","In this Letter, we study the effect of Silk damping on the scalar-induced gravitational waves (SIGWs).","Resulting from the dissipation of cosmic fluid, the Silk damping notably suppresses the energy-density spectrum of SIGWs on scales comparable to a diffusion scale at the decoupling time of feebly-interacting particles.","The effect offers a novel observable for probing the underlying particle interaction, especially for those mediated by heavy gauge bosons beyond the standard model of particles.","We anticipate that pulsar timing arrays are sensitive to gauge bosons with mass $\\sim10^{3}-10^{4}\\,\\mathrm{GeV}$, while space- and ground-based interferometers to those with mass $\\sim10^7-10^{12}\\,\\mathrm{GeV}$, leading to essential complements to on-going and future experiments of high-energy physics."],"url":"http://arxiv.org/abs/2405.02960v1","category":"astro-ph.CO"}
{"created":"2024-05-05 10:13:55","title":"Exploring the Improvement of Evolutionary Computation via Large Language Models","abstract":"Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains. However, as the complexity of problems increases, the limitations of EC have become more apparent. The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields. By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements. This presents a promising direction for future research at the intersection of LLMs and EC.","sentences":["Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains.","However, as the complexity of problems increases, the limitations of EC have become more apparent.","The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields.","By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements.","This presents a promising direction for future research at the intersection of LLMs and EC."],"url":"http://arxiv.org/abs/2405.02876v1","category":"cs.NE"}
{"created":"2024-05-05 07:21:17","title":"Scene-Adaptive Person Search via Bilateral Modulations","abstract":"Person search aims to localize specific a target person from a gallery set of images with various scenes. As the scene of moving pedestrian changes, the captured person image inevitably bring in lots of background noise and foreground noise on the person feature, which are completely unrelated to the person identity, leading to severe performance degeneration. To address this issue, we present a Scene-Adaptive Person Search (SEAS) model by introducing bilateral modulations to simultaneously eliminate scene noise and maintain a consistent person representation to adapt to various scenes. In SEAS, a Background Modulation Network (BMN) is designed to encode the feature extracted from the detected bounding box into a multi-granularity embedding, which reduces the input of background noise from multiple levels with norm-aware. Additionally, to mitigate the effect of foreground noise on the person feature, SEAS introduces a Foreground Modulation Network (FMN) to compute the clutter reduction offset for the person embedding based on the feature map of the scene image. By bilateral modulations on both background and foreground within an end-to-end manner, SEAS obtains consistent feature representations without scene noise. SEAS can achieve state-of-the-art (SOTA) performance on two benchmark datasets, CUHK-SYSU with 97.1\\% mAP and PRW with 60.5\\% mAP. The code is available at https://github.com/whbdmu/SEAS.","sentences":["Person search aims to localize specific a target person from a gallery set of images with various scenes.","As the scene of moving pedestrian changes, the captured person image inevitably bring in lots of background noise and foreground noise on the person feature, which are completely unrelated to the person identity, leading to severe performance degeneration.","To address this issue, we present a Scene-Adaptive Person Search (SEAS) model by introducing bilateral modulations to simultaneously eliminate scene noise and maintain a consistent person representation to adapt to various scenes.","In SEAS, a Background Modulation Network (BMN) is designed to encode the feature extracted from the detected bounding box into a multi-granularity embedding, which reduces the input of background noise from multiple levels with norm-aware.","Additionally, to mitigate the effect of foreground noise on the person feature, SEAS introduces a Foreground Modulation Network (FMN) to compute the clutter reduction offset for the person embedding based on the feature map of the scene image.","By bilateral modulations on both background and foreground within an end-to-end manner, SEAS obtains consistent feature representations without scene noise.","SEAS can achieve state-of-the-art (SOTA) performance on two benchmark datasets, CUHK-SYSU with 97.1\\% mAP and PRW with 60.5\\% mAP.","The code is available at https://github.com/whbdmu/SEAS."],"url":"http://arxiv.org/abs/2405.02834v1","category":"cs.CV"}
{"created":"2024-05-05 07:15:47","title":"Fast One-Stage Unsupervised Domain Adaptive Person Search","abstract":"Unsupervised person search aims to localize a particular target person from a gallery set of scene images without annotations, which is extremely challenging due to the unexpected variations of the unlabeled domains. However, most existing methods dedicate to developing multi-stage models to adapt domain variations while using clustering for iterative model training, which inevitably increases model complexity. To address this issue, we propose a Fast One-stage Unsupervised person Search (FOUS) which complementary integrates domain adaptaion with label adaptaion within an end-to-end manner without iterative clustering. To minimize the domain discrepancy, FOUS introduced an Attention-based Domain Alignment Module (ADAM) which can not only align various domains for both detection and ReID tasks but also construct an attention mechanism to reduce the adverse impacts of low-quality candidates resulting from unsupervised detection. Moreover, to avoid the redundant iterative clustering mode, FOUS adopts a prototype-guided labeling method which minimizes redundant correlation computations for partial samples and assigns noisy coarse label groups efficiently. The coarse label groups will be continuously refined via label-flexible training network with an adaptive selection strategy. With the adapted domains and labels, FOUS can achieve the state-of-the-art (SOTA) performance on two benchmark datasets, CUHK-SYSU and PRW. The code is available at https://github.com/whbdmu/FOUS.","sentences":["Unsupervised person search aims to localize a particular target person from a gallery set of scene images without annotations, which is extremely challenging due to the unexpected variations of the unlabeled domains.","However, most existing methods dedicate to developing multi-stage models to adapt domain variations while using clustering for iterative model training, which inevitably increases model complexity.","To address this issue, we propose a Fast One-stage Unsupervised person Search (FOUS) which complementary integrates domain adaptaion with label adaptaion within an end-to-end manner without iterative clustering.","To minimize the domain discrepancy, FOUS introduced an Attention-based Domain Alignment Module (ADAM) which can not only align various domains for both detection and ReID tasks but also construct an attention mechanism to reduce the adverse impacts of low-quality candidates resulting from unsupervised detection.","Moreover, to avoid the redundant iterative clustering mode, FOUS adopts a prototype-guided labeling method which minimizes redundant correlation computations for partial samples and assigns noisy coarse label groups efficiently.","The coarse label groups will be continuously refined via label-flexible training network with an adaptive selection strategy.","With the adapted domains and labels, FOUS can achieve the state-of-the-art (SOTA) performance on two benchmark datasets, CUHK-SYSU and PRW.","The code is available at https://github.com/whbdmu/FOUS."],"url":"http://arxiv.org/abs/2405.02832v1","category":"cs.CV"}
{"created":"2024-05-05 06:21:58","title":"Adaptive Guidance Learning for Camouflaged Object Detection","abstract":"Camouflaged object detection (COD) aims to segment objects visually embedded in their surroundings, which is a very challenging task due to the high similarity between the objects and the background. To address it, most methods often incorporate additional information (e.g., boundary, texture, and frequency clues) to guide feature learning for better detecting camouflaged objects from the background. Although progress has been made, these methods are basically individually tailored to specific auxiliary cues, thus lacking adaptability and not consistently achieving high segmentation performance. To this end, this paper proposes an adaptive guidance learning network, dubbed \\textit{AGLNet}, which is a unified end-to-end learnable model for exploring and adapting different additional cues in CNN models to guide accurate camouflaged feature learning. Specifically, we first design a straightforward additional information generation (AIG) module to learn additional camouflaged object cues, which can be adapted for the exploration of effective camouflaged features. Then we present a hierarchical feature combination (HFC) module to deeply integrate additional cues and image features to guide camouflaged feature learning in a multi-level fusion manner.Followed by a recalibration decoder (RD), different features are further aggregated and refined for accurate object prediction. Extensive experiments on three widely used COD benchmark datasets demonstrate that the proposed method achieves significant performance improvements under different additional cues, and outperforms the recent 20 state-of-the-art methods by a large margin. Our code will be made publicly available at: \\textcolor{blue}{{https://github.com/ZNan-Chen/AGLNet}}.","sentences":["Camouflaged object detection (COD) aims to segment objects visually embedded in their surroundings, which is a very challenging task due to the high similarity between the objects and the background.","To address it, most methods often incorporate additional information (e.g., boundary, texture, and frequency clues) to guide feature learning for better detecting camouflaged objects from the background.","Although progress has been made, these methods are basically individually tailored to specific auxiliary cues, thus lacking adaptability and not consistently achieving high segmentation performance.","To this end, this paper proposes an adaptive guidance learning network, dubbed \\textit{AGLNet}, which is a unified end-to-end learnable model for exploring and adapting different additional cues in CNN models to guide accurate camouflaged feature learning.","Specifically, we first design a straightforward additional information generation (AIG) module to learn additional camouflaged object cues, which can be adapted for the exploration of effective camouflaged features.","Then we present a hierarchical feature combination (HFC) module to deeply integrate additional cues and image features to guide camouflaged feature learning in a multi-level fusion manner.","Followed by a recalibration decoder (RD), different features are further aggregated and refined for accurate object prediction.","Extensive experiments on three widely used COD benchmark datasets demonstrate that the proposed method achieves significant performance improvements under different additional cues, and outperforms the recent 20 state-of-the-art methods by a large margin.","Our code will be made publicly available at: \\textcolor{blue}{{https://github.com/ZNan-Chen/AGLNet}}."],"url":"http://arxiv.org/abs/2405.02824v1","category":"cs.CV"}
{"created":"2024-05-05 05:43:20","title":"HuixiangDou-CR: Coreference Resolution in Group Chats","abstract":"How to eliminate pronominal reference in group chats? In this work, we have preprocessed 58k authentic chat data and manually annotated 2.3k questions. The reliability of this annotation was confirmed by the scaling law. After this, we conducted fine-tuning on Qwen models, ranging from 0.5B to 32B parameters. The optimal version improved 29.07 in F1 score. This confirms the viability of fine-tuning Large Language Model (LLM) for downstream Natural Language Processing (NLP) tasks. Our contributions are: 1) Created Supervised Fine-Tuning (SFT) training data in alpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2) Developed a method for acquiring high-quality data leveraging scaling law principle. The script, raw data with alpaca format and experiments track are open-sourced on Github https://github.com/InternLM/HuixiangDou/tree/main/web/tools, HuggingFace https://huggingface.co/tpoisonooo and WandB https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo . The privacy of the data involved has been authorized by users.","sentences":["How to eliminate pronominal reference in group chats?","In this work, we have preprocessed 58k authentic chat data and manually annotated 2.3k questions.","The reliability of this annotation was confirmed by the scaling law.","After this, we conducted fine-tuning on Qwen models, ranging from 0.5B to 32B parameters.","The optimal version improved 29.07 in F1 score.","This confirms the viability of fine-tuning Large Language Model (LLM) for downstream Natural Language Processing (NLP) tasks.","Our contributions are: 1) Created Supervised Fine-Tuning (SFT) training data in alpaca format, along with a set of Low-Rank Adaptation (LoRA) weights, and 2) Developed a method for acquiring high-quality data leveraging scaling law principle.","The script, raw data with alpaca format and experiments track are open-sourced on Github https://github.com/InternLM/HuixiangDou/tree/main/web/tools, HuggingFace https://huggingface.co/tpoisonooo and WandB https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo .","The privacy of the data involved has been authorized by users."],"url":"http://arxiv.org/abs/2405.02817v1","category":"cs.CL"}
{"created":"2024-05-05 04:29:22","title":"Adaptive deep density approximation for stochastic dynamical systems","abstract":"In this paper we consider adaptive deep neural network approximation for stochastic dynamical systems. Based on the Liouville equation associated with the stochastic dynamical systems, a new temporal KRnet (tKRnet) is proposed to approximate the probability density functions (PDFs) of the state variables. The tKRnet gives an explicit density model for the solution of the Liouville equation, which alleviates the curse of dimensionality issue that limits the application of traditional grid based numerical methods. To efficiently train the tKRnet, an adaptive procedure is developed to generate collocation points for the corresponding residual loss function, where samples are generated iteratively using the approximate density function at each iteration. A temporal decomposition technique is also employed to improve the long-time integration. Theoretical analysis of our proposed method is provided, and numerical examples are presented to demonstrate its performance.","sentences":["In this paper we consider adaptive deep neural network approximation for stochastic dynamical systems.","Based on the Liouville equation associated with the stochastic dynamical systems, a new temporal KRnet (tKRnet) is proposed to approximate the probability density functions (PDFs) of the state variables.","The tKRnet gives an explicit density model for the solution of the Liouville equation, which alleviates the curse of dimensionality issue that limits the application of traditional grid based numerical methods.","To efficiently train the tKRnet, an adaptive procedure is developed to generate collocation points for the corresponding residual loss function, where samples are generated iteratively using the approximate density function at each iteration.","A temporal decomposition technique is also employed to improve the long-time integration.","Theoretical analysis of our proposed method is provided, and numerical examples are presented to demonstrate its performance."],"url":"http://arxiv.org/abs/2405.02810v1","category":"math.NA"}
{"created":"2024-05-05 02:44:04","title":"Adapting to Distribution Shift by Visual Domain Prompt Generation","abstract":"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.","sentences":["In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts.","To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains.","Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization.","Additionally, domain-centric designs are not flavored in their works.","Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages.","In this work, we propose an approach on top of the pre-computed features of the foundation model.","Specifically, we build a knowledge bank to learn the transferable knowledge from source domains.","Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt.","The domain prompt then directs the visual features towards a particular domain via a guidance module.","Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction.","Extensive experiments are conducted to validate the domain knowledge extraction.","The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet."],"url":"http://arxiv.org/abs/2405.02797v1","category":"cs.CV"}
{"created":"2024-05-05 02:10:00","title":"Confidential and Protected Disease Classifier using Fully Homomorphic Encryption","abstract":"With the rapid surge in the prevalence of Large Language Models (LLMs), individuals are increasingly turning to conversational AI for initial insights across various domains, including health-related inquiries such as disease diagnosis. Many users seek potential causes on platforms like ChatGPT or Bard before consulting a medical professional for their ailment. These platforms offer valuable benefits by streamlining the diagnosis process, alleviating the significant workload of healthcare practitioners, and saving users both time and money by avoiding unnecessary doctor visits. However, Despite the convenience of such platforms, sharing personal medical data online poses risks, including the presence of malicious platforms or potential eavesdropping by attackers. To address privacy concerns, we propose a novel framework combining FHE and Deep Learning for a secure and private diagnosis system. Operating on a question-and-answer-based model akin to an interaction with a medical practitioner, this end-to-end secure system employs Fully Homomorphic Encryption (FHE) to handle encrypted input data. Given FHE's computational constraints, we adapt deep neural networks and activation functions to the encryted domain. Further, we also propose a faster algorithm to compute summation of ciphertext elements. Through rigorous experiments, we demonstrate the efficacy of our approach. The proposed framework achieves strict security and privacy with minimal loss in performance.","sentences":["With the rapid surge in the prevalence of Large Language Models (LLMs), individuals are increasingly turning to conversational AI for initial insights across various domains, including health-related inquiries such as disease diagnosis.","Many users seek potential causes on platforms like ChatGPT or Bard before consulting a medical professional for their ailment.","These platforms offer valuable benefits by streamlining the diagnosis process, alleviating the significant workload of healthcare practitioners, and saving users both time and money by avoiding unnecessary doctor visits.","However, Despite the convenience of such platforms, sharing personal medical data online poses risks, including the presence of malicious platforms or potential eavesdropping by attackers.","To address privacy concerns, we propose a novel framework combining FHE and Deep Learning for a secure and private diagnosis system.","Operating on a question-and-answer-based model akin to an interaction with a medical practitioner, this end-to-end secure system employs Fully Homomorphic Encryption (FHE) to handle encrypted input data.","Given FHE's computational constraints, we adapt deep neural networks and activation functions to the encryted domain.","Further, we also propose a faster algorithm to compute summation of ciphertext elements.","Through rigorous experiments, we demonstrate the efficacy of our approach.","The proposed framework achieves strict security and privacy with minimal loss in performance."],"url":"http://arxiv.org/abs/2405.02790v1","category":"cs.CR"}
{"created":"2024-05-05 02:08:50","title":"Antenna Failure Resilience: Deep Learning-Enabled Robust DOA Estimation with Single Snapshot Sparse Arrays","abstract":"Recent advancements in Deep Learning (DL) for Direction of Arrival (DOA) estimation have highlighted its superiority over traditional methods, offering faster inference, enhanced super-resolution, and robust performance in low Signal-to-Noise Ratio (SNR) environments. Despite these advancements, existing research predominantly focuses on multi-snapshot scenarios, a limitation in the context of automotive radar systems which demand high angular resolution and often rely on limited snapshots, sometimes as scarce as a single snapshot. Furthermore, the increasing interest in sparse arrays for automotive radar, owing to their cost-effectiveness and reduced antenna element coupling, presents additional challenges including susceptibility to random sensor failures. This paper introduces a pioneering DL framework featuring a sparse signal augmentation layer, meticulously crafted to bolster single snapshot DOA estimation across diverse sparse array setups and amidst antenna failures. To our best knowledge, this is the first work to tackle this issue. Our approach improves the adaptability of deep learning techniques to overcome the unique difficulties posed by sparse arrays with single snapshot. We conduct thorough evaluations of our network's performance using simulated and real-world data, showcasing the efficacy and real-world viability of our proposed solution. The code and real-world dataset employed in this study are available at https://github.com/ruxinzh/Deep_RSA_DOA.","sentences":["Recent advancements in Deep Learning (DL) for Direction of Arrival (DOA) estimation have highlighted its superiority over traditional methods, offering faster inference, enhanced super-resolution, and robust performance in low Signal-to-Noise Ratio (SNR) environments.","Despite these advancements, existing research predominantly focuses on multi-snapshot scenarios, a limitation in the context of automotive radar systems which demand high angular resolution and often rely on limited snapshots, sometimes as scarce as a single snapshot.","Furthermore, the increasing interest in sparse arrays for automotive radar, owing to their cost-effectiveness and reduced antenna element coupling, presents additional challenges including susceptibility to random sensor failures.","This paper introduces a pioneering DL framework featuring a sparse signal augmentation layer, meticulously crafted to bolster single snapshot DOA estimation across diverse sparse array setups and amidst antenna failures.","To our best knowledge, this is the first work to tackle this issue.","Our approach improves the adaptability of deep learning techniques to overcome the unique difficulties posed by sparse arrays with single snapshot.","We conduct thorough evaluations of our network's performance using simulated and real-world data, showcasing the efficacy and real-world viability of our proposed solution.","The code and real-world dataset employed in this study are available at https://github.com/ruxinzh/Deep_RSA_DOA."],"url":"http://arxiv.org/abs/2405.02788v1","category":"eess.SP"}
{"created":"2024-05-05 02:03:42","title":"Fused attention mechanism-based ore sorting network","abstract":"Deep learning has had a significant impact on the identification and classification of mineral resources, especially playing a key role in efficiently and accurately identifying different minerals, which is important for improving the efficiency and accuracy of mining. However, traditional ore sorting meth- ods often suffer from inefficiency and lack of accuracy, especially in complex mineral environments. To address these challenges, this study proposes a method called OreYOLO, which incorporates an attentional mechanism and a multi-scale feature fusion strategy, based on ore data from gold and sul- fide ores. By introducing the progressive feature pyramid structure into YOLOv5 and embedding the attention mechanism in the feature extraction module, the detection performance and accuracy of the model are greatly improved. In order to adapt to the diverse ore sorting scenarios and the deployment requirements of edge devices, the network structure is designed to be lightweight, which achieves a low number of parameters (3.458M) and computational complexity (6.3GFLOPs) while maintaining high accuracy (99.3% and 99.2%, respectively). In the experimental part, a target detection dataset containing 6000 images of gold and sulfuric iron ore is constructed for gold and sulfuric iron ore classification training, and several sets of comparison experiments are set up, including the YOLO series, EfficientDet, Faster-RCNN, and CenterNet, etc., and the experiments prove that OreYOLO outperforms the commonly used high-performance object detection of these architectures","sentences":["Deep learning has had a significant impact on the identification and classification of mineral resources, especially playing a key role in efficiently and accurately identifying different minerals, which is important for improving the efficiency and accuracy of mining.","However, traditional ore sorting meth- ods often suffer from inefficiency and lack of accuracy, especially in complex mineral environments.","To address these challenges, this study proposes a method called OreYOLO, which incorporates an attentional mechanism and a multi-scale feature fusion strategy, based on ore data from gold and sul- fide ores.","By introducing the progressive feature pyramid structure into YOLOv5 and embedding the attention mechanism in the feature extraction module, the detection performance and accuracy of the model are greatly improved.","In order to adapt to the diverse ore sorting scenarios and the deployment requirements of edge devices, the network structure is designed to be lightweight, which achieves a low number of parameters (3.458M) and computational complexity (6.3GFLOPs) while maintaining high accuracy (99.3% and 99.2%, respectively).","In the experimental part, a target detection dataset containing 6000 images of gold and sulfuric iron ore is constructed for gold and sulfuric iron ore classification training, and several sets of comparison experiments are set up, including the YOLO series, EfficientDet, Faster-RCNN, and CenterNet, etc., and the experiments prove that OreYOLO outperforms the commonly used high-performance object detection of these architectures"],"url":"http://arxiv.org/abs/2405.02785v1","category":"cs.CV"}
{"created":"2024-05-04 23:42:02","title":"SkinGrip: An Adaptive Soft Robotic Manipulator with Capacitive Sensing for Whole-Limb Bathing Assistance","abstract":"Robotics presents a promising opportunity for enhancing bathing assistance, potentially to alleviate labor shortages and reduce care costs, while offering consistent and gentle care for individuals with physical disabilities. However, ensuring flexible and efficient cleaning of the human body poses challenges as it involves direct physical contact between the human and the robot, and necessitates simple, safe, and effective control. In this paper, we introduce a soft, expandable robotic manipulator with embedded capacitive proximity sensing arrays, designed for safe and efficient bathing assistance. We conduct a thorough evaluation of our soft manipulator, comparing it with a baseline rigid end effector in a human study involving 12 participants across $96$ bathing trails. Our soft manipulator achieves an an average cleaning effectiveness of 88.8% on arms and 81.4% on legs, far exceeding the performance of the baseline. Participant feedback further validates the manipulator's ability to maintain safety, comfort, and thorough cleaning.","sentences":["Robotics presents a promising opportunity for enhancing bathing assistance, potentially to alleviate labor shortages and reduce care costs, while offering consistent and gentle care for individuals with physical disabilities.","However, ensuring flexible and efficient cleaning of the human body poses challenges as it involves direct physical contact between the human and the robot, and necessitates simple, safe, and effective control.","In this paper, we introduce a soft, expandable robotic manipulator with embedded capacitive proximity sensing arrays, designed for safe and efficient bathing assistance.","We conduct a thorough evaluation of our soft manipulator, comparing it with a baseline rigid end effector in a human study involving 12 participants across $96$ bathing trails.","Our soft manipulator achieves an an average cleaning effectiveness of 88.8% on arms and 81.4% on legs, far exceeding the performance of the baseline.","Participant feedback further validates the manipulator's ability to maintain safety, comfort, and thorough cleaning."],"url":"http://arxiv.org/abs/2405.02772v1","category":"cs.RO"}
{"created":"2024-05-04 22:05:58","title":"A Decade in a Systematic Review: The Evolution and Impact of Cell Painting","abstract":"High-content image-based assays have fueled significant discoveries in the life sciences in the past decade (2013-2023), including novel insights into disease etiology, mechanism of action, new therapeutics, and toxicology predictions. Here, we systematically review the substantial methodological advancements and applications of Cell Painting. Advancements include improvements in the Cell Painting protocol, assay adaptations for different types of perturbations and applications, and improved methodologies for feature extraction, quality control, and batch effect correction. Moreover, machine learning methods recently surpassed classical approaches in their ability to extract biologically useful information from Cell Painting images. Cell Painting data have been used alone or in combination with other -omics data to decipher the mechanism of action of a compound, its toxicity profile, and many other biological effects. Overall, key methodological advances have expanded the ability of Cell Painting to capture cellular responses to various perturbations. Future advances will likely lie in advancing computational and experimental techniques, developing new publicly available datasets, and integrating them with other high-content data types.","sentences":["High-content image-based assays have fueled significant discoveries in the life sciences in the past decade (2013-2023), including novel insights into disease etiology, mechanism of action, new therapeutics, and toxicology predictions.","Here, we systematically review the substantial methodological advancements and applications of Cell Painting.","Advancements include improvements in the Cell Painting protocol, assay adaptations for different types of perturbations and applications, and improved methodologies for feature extraction, quality control, and batch effect correction.","Moreover, machine learning methods recently surpassed classical approaches in their ability to extract biologically useful information from Cell Painting images.","Cell Painting data have been used alone or in combination with other -omics data to decipher the mechanism of action of a compound, its toxicity profile, and many other biological effects.","Overall, key methodological advances have expanded the ability of Cell Painting to capture cellular responses to various perturbations.","Future advances will likely lie in advancing computational and experimental techniques, developing new publicly available datasets, and integrating them with other high-content data types."],"url":"http://arxiv.org/abs/2405.02767v1","category":"q-bio.SC"}
{"created":"2024-05-04 21:11:11","title":"A Kinetic Model of Solar Wind Generation and Heating by Kinetic Alfv\u00e9n Wave Turbulence","abstract":"We present results from a kinetic model of collisionless gyrotropic coronal hole protons heated by cyclotron and Landau resonant dissipation of critically balanced kinetic Alfv\\'en waves. The model incorporates the kinetic effects of gravity, ambipolar electric field, ponderomotive force of the large-scale Alfv\\'en waves and the mirror force in a super-radially expanding flux tube. The flow speed is self- consistently obtained as the bulk flow of the proton distribution. Two cases, taking the intensities of the turbulent spectra to be balanced in the parallel propagation direction or imbalanced at a ratio of 9:1 show almost no difference. The distributions develop parallel extensions outward due to the Landau resonance, but exhibit very little perpendicular heating under the parameter choices used here. The speeds and temperatures fall well short of the requirements for a fast solar wind. Rather than continuing to explore the parameter space of this system, we propose that a modified model using turbulent spectra based on the recent helicity barrier scenario would be an appropriate next step.","sentences":["We present results from a kinetic model of collisionless gyrotropic coronal hole protons heated by cyclotron and Landau resonant dissipation of critically balanced kinetic Alfv\\'en waves.","The model incorporates the kinetic effects of gravity, ambipolar electric field, ponderomotive force of the large-scale Alfv\\'en waves and the mirror force in a super-radially expanding flux tube.","The flow speed is self- consistently obtained as the bulk flow of the proton distribution.","Two cases, taking the intensities of the turbulent spectra to be balanced in the parallel propagation direction or imbalanced at a ratio of 9:1 show almost no difference.","The distributions develop parallel extensions outward due to the Landau resonance, but exhibit very little perpendicular heating under the parameter choices used here.","The speeds and temperatures fall well short of the requirements for a fast solar wind.","Rather than continuing to explore the parameter space of this system, we propose that a modified model using turbulent spectra based on the recent helicity barrier scenario would be an appropriate next step."],"url":"http://arxiv.org/abs/2405.02757v1","category":"physics.space-ph"}
{"created":"2024-05-04 19:53:03","title":"Beyond Performance: Quantifying and Mitigating Label Bias in LLMs","abstract":"Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit label bias -- an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model's predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.","sentences":["Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples.","However, recent work revealed they also exhibit label bias -- an undesirable preference toward predicting certain answers over others.","Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored.","In this study, we evaluate different approaches to quantifying label bias in a model's predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs.","Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard.","We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias.","Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability."],"url":"http://arxiv.org/abs/2405.02743v1","category":"cs.CL"}
{"created":"2024-05-04 17:47:45","title":"Taming Equilibrium Bias in Risk-Sensitive Multi-Agent Reinforcement Learning","abstract":"We study risk-sensitive multi-agent reinforcement learning under general-sum Markov games, where agents optimize the entropic risk measure of rewards with possibly diverse risk preferences. We show that using the regret naively adapted from existing literature as a performance metric could induce policies with equilibrium bias that favor the most risk-sensitive agents and overlook the other agents. To address such deficiency of the naive regret, we propose a novel notion of regret, which we call risk-balanced regret, and show through a lower bound that it overcomes the issue of equilibrium bias. Furthermore, we develop a self-play algorithm for learning Nash, correlated, and coarse correlated equilibria in risk-sensitive Markov games. We prove that the proposed algorithm attains near-optimal regret guarantees with respect to the risk-balanced regret.","sentences":["We study risk-sensitive multi-agent reinforcement learning under general-sum Markov games, where agents optimize the entropic risk measure of rewards with possibly diverse risk preferences.","We show that using the regret naively adapted from existing literature as a performance metric could induce policies with equilibrium bias that favor the most risk-sensitive agents and overlook the other agents.","To address such deficiency of the naive regret, we propose a novel notion of regret, which we call risk-balanced regret, and show through a lower bound that it overcomes the issue of equilibrium bias.","Furthermore, we develop a self-play algorithm for learning Nash, correlated, and coarse correlated equilibria in risk-sensitive Markov games.","We prove that the proposed algorithm attains near-optimal regret guarantees with respect to the risk-balanced regret."],"url":"http://arxiv.org/abs/2405.02724v1","category":"cs.LG"}
{"created":"2024-05-04 17:29:18","title":"Active Signal Emitter Placement In Complex Environments","abstract":"Placement of electromagnetic signal emitting devices, such as light sources, has important usage in for signal coverage tasks. Automatic placement of these devices is challenging because of the complex interaction of the signal and environment due to reflection, refraction and scattering. In this work, we iteratively improve the placement of these devices by interleaving device placement and sensing actions, correcting errors in the model of the signal propagation. To this end, we propose a novel factor-graph based belief model which combines the measurements taken by the robot and an analytical light propagation model. This model allows accurately modelling the uncertainty of the light propagation with respect to the obstacles, which greatly improves the informative path planning routine. Additionally, we propose a method for determining when to re-plan the emitter placements to balance a trade-off between information about a specific configuration and frequent updating of the configuration. This method incorporates the uncertainty from belief model to adaptively determine when re-configuration is needed. We find that our system has a 9.8% median error reduction compared to a baseline system in simulations in the most difficult environment. We also run on-robot tests and determine that our system performs favorably compared to the baseline.","sentences":["Placement of electromagnetic signal emitting devices, such as light sources, has important usage in for signal coverage tasks.","Automatic placement of these devices is challenging because of the complex interaction of the signal and environment due to reflection, refraction and scattering.","In this work, we iteratively improve the placement of these devices by interleaving device placement and sensing actions, correcting errors in the model of the signal propagation.","To this end, we propose a novel factor-graph based belief model which combines the measurements taken by the robot and an analytical light propagation model.","This model allows accurately modelling the uncertainty of the light propagation with respect to the obstacles, which greatly improves the informative path planning routine.","Additionally, we propose a method for determining when to re-plan the emitter placements to balance a trade-off between information about a specific configuration and frequent updating of the configuration.","This method incorporates the uncertainty from belief model to adaptively determine when re-configuration is needed.","We find that our system has a 9.8% median error reduction compared to a baseline system in simulations in the most difficult environment.","We also run on-robot tests and determine that our system performs favorably compared to the baseline."],"url":"http://arxiv.org/abs/2405.02719v1","category":"cs.RO"}
{"created":"2024-05-04 17:24:09","title":"AFter: Attention-based Fusion Router for RGBT Tracking","abstract":"Multi-modal feature fusion as a core investigative component of RGBT tracking emerges numerous fusion studies in recent years. However, existing RGBT tracking methods widely adopt fixed fusion structures to integrate multi-modal feature, which are hard to handle various challenges in dynamic scenarios. To address this problem, this work presents a novel \\emph{A}ttention-based \\emph{F}usion rou\\emph{ter} called AFter, which optimizes the fusion structure to adapt to the dynamic challenging scenarios, for robust RGBT tracking. In particular, we design a fusion structure space based on the hierarchical attention network, each attention-based fusion unit corresponding to a fusion operation and a combination of these attention units corresponding to a fusion structure. Through optimizing the combination of attention-based fusion units, we can dynamically select the fusion structure to adapt to various challenging scenarios. Unlike complex search of different structures in neural architecture search algorithms, we develop a dynamic routing algorithm, which equips each attention-based fusion unit with a router, to predict the combination weights for efficient optimization of the fusion structure. Extensive experiments on five mainstream RGBT tracking datasets demonstrate the superior performance of the proposed AFter against state-of-the-art RGBT trackers. We release the code in https://github.com/Alexadlu/AFter.","sentences":["Multi-modal feature fusion as a core investigative component of RGBT tracking emerges numerous fusion studies in recent years.","However, existing RGBT tracking methods widely adopt fixed fusion structures to integrate multi-modal feature, which are hard to handle various challenges in dynamic scenarios.","To address this problem, this work presents a novel \\emph{A}ttention-based \\emph{F}usion rou\\emph{ter} called AFter, which optimizes the fusion structure to adapt to the dynamic challenging scenarios, for robust RGBT tracking.","In particular, we design a fusion structure space based on the hierarchical attention network, each attention-based fusion unit corresponding to a fusion operation and a combination of these attention units corresponding to a fusion structure.","Through optimizing the combination of attention-based fusion units, we can dynamically select the fusion structure to adapt to various challenging scenarios.","Unlike complex search of different structures in neural architecture search algorithms, we develop a dynamic routing algorithm, which equips each attention-based fusion unit with a router, to predict the combination weights for efficient optimization of the fusion structure.","Extensive experiments on five mainstream RGBT tracking datasets demonstrate the superior performance of the proposed AFter against state-of-the-art RGBT trackers.","We release the code in https://github.com/Alexadlu/AFter."],"url":"http://arxiv.org/abs/2405.02717v1","category":"cs.CV"}
{"created":"2024-05-04 16:21:05","title":"Machine Learning Data Practices through a Data Curation Lens: An Evaluation Framework","abstract":"Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.","sentences":["Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes.","Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning.","In response, this paper examines data practices in machine learning dataset development through the lens of data curation.","We evaluate data practices in machine learning as data curation practices.","To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric.","Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed.","We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles.","Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for.","We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices."],"url":"http://arxiv.org/abs/2405.02703v1","category":"cs.CY"}
{"created":"2024-05-04 15:21:03","title":"Improved All-Pairs Approximate Shortest Paths in Congested Clique","abstract":"In this paper, we present new algorithms for approximating All-Pairs Shortest Paths (APSP) in the Congested Clique model. We present randomized algorithms for weighted undirected graphs.   Our first contribution is an $O(1)$-approximate APSP algorithm taking just $O(\\log \\log \\log n)$ rounds. Prior to our work, the fastest algorithms that give an $O(1)$-approximation for APSP take $\\operatorname{poly}(\\log{n})$ rounds in weighted undirected graphs, and $\\operatorname{poly}(\\log \\log n)$ rounds in unweighted undirected graphs.   If we terminate the execution of the algorithm early, we obtain an $O(t)$-round algorithm that yields an $O \\big( (\\log n)^{1/2^t} \\big) $ distance approximation for a parameter $t$. The trade-off between $t$ and the approximation quality provides flexibility for different scenarios, allowing the algorithm to adapt to specific requirements. In particular, we can get an $O \\big( (\\log n)^{1/2^t} \\big) $-approximation for any constant $t$ in $O(1)$-rounds. Such result was previously known only for the special case that $t=0$.   A key ingredient in our algorithm is a lemma that allows to improve an $O(a)$-approximation for APSP to an $O(\\sqrt{a})$-approximation for APSP in $O(1)$ rounds. To prove the lemma, we develop several new tools, including $O(1)$-round algorithms for computing the $k$ closest nodes, a certain type of hopset, and skeleton graphs.","sentences":["In this paper, we present new algorithms for approximating All-Pairs Shortest Paths (APSP) in the Congested Clique model.","We present randomized algorithms for weighted undirected graphs.   ","Our first contribution is an $O(1)$-approximate APSP algorithm taking just $O(\\log \\log \\log n)$ rounds.","Prior to our work, the fastest algorithms that give an $O(1)$-approximation for APSP take $\\operatorname{poly}(\\log{n})$ rounds in weighted undirected graphs, and $\\operatorname{poly}(\\log \\log n)$ rounds in unweighted undirected graphs.   ","If we terminate the execution of the algorithm early, we obtain an $O(t)$-round algorithm that yields an $O \\big( (\\log n)^{1/2^t} \\big) $ distance approximation for a parameter $t$. The trade-off between $t$ and the approximation quality provides flexibility for different scenarios, allowing the algorithm to adapt to specific requirements.","In particular, we can get an $O \\big( (\\log n)^{1/2^t} \\big) $-approximation for any constant $t$ in $O(1)$-rounds.","Such result was previously known only for the special case that $t=0$.   A key ingredient in our algorithm is a lemma that allows to improve an $O(a)$-approximation for APSP to an $O(\\sqrt{a})$-approximation for APSP in $O(1)$ rounds.","To prove the lemma, we develop several new tools, including $O(1)$-round algorithms for computing the $k$ closest nodes, a certain type of hopset, and skeleton graphs."],"url":"http://arxiv.org/abs/2405.02695v1","category":"cs.DS"}
{"created":"2024-05-04 13:22:51","title":"Products of involutions in symplectic groups over general fields (II)","abstract":"Let $s$ be an $n$-dimensional symplectic form over a field $\\mathbb{F}$ of characteristic other than $2$, with $n>2$.   In a previous article, we have proved that if $\\mathbb{F}$ is infinite then every element of the symplectic group $\\mathrm{Sp}(s)$ is the product of four involutions if $n$ is a multiple of $4$ and of five involutions otherwise.   Here, we adapt this result to all finite fields with characteristic not $2$, with the sole exception of the very special situation where $n=4$ and $|\\mathbb{F}|=3$, a special case which we study extensively.","sentences":["Let $s$ be an $n$-dimensional symplectic form over a field $\\mathbb{F}$ of characteristic other than $2$, with $n>2$.   In a previous article, we have proved that if $\\mathbb{F}$ is infinite then every element of the symplectic group $\\mathrm{Sp}(s)$ is the product of four involutions if $n$ is a multiple of $4$ and of five involutions otherwise.   ","Here, we adapt this result to all finite fields with characteristic not $2$, with the sole exception of the very special situation where $n=4$ and $|\\mathbb{F}|=3$, a special case which we study extensively."],"url":"http://arxiv.org/abs/2405.02663v1","category":"math.GR"}
{"created":"2024-05-04 09:36:45","title":"Accelerating Autonomy: Insights from Pro Racers in the Era of Autonomous Racing - An Expert Interview Study","abstract":"This research aims to investigate professional racing drivers' expertise to develop an understanding of their cognitive and adaptive skills to create new autonomy algorithms. An expert interview study was conducted with 11 professional race drivers, data analysts, and racing instructors from across prominent racing leagues. The interviews were conducted using an exploratory, non-standardized expert interview format guided by a set of prepared questions. The study investigates drivers' exploration strategies to reach their vehicle limits and contrasts them with the capabilities of state-of-the-art autonomous racing software stacks. Participants were questioned about the techniques and skills they have developed to quickly approach and maneuver at the vehicle limit, ultimately minimizing lap times. The analysis of the interviews was grounded in Mayring's qualitative content analysis framework, which facilitated the organization of the data into multiple categories and subcategories. Our findings create insights into human behavior regarding reaching a vehicle's limit and minimizing lap times. We conclude from the findings the development of new autonomy software modules that allow for more adaptive vehicle behavior. By emphasizing the distinct nuances between manual and autonomous driving techniques, the paper encourages further investigation into human drivers' strategies to maximize their vehicles' capabilities.","sentences":["This research aims to investigate professional racing drivers' expertise to develop an understanding of their cognitive and adaptive skills to create new autonomy algorithms.","An expert interview study was conducted with 11 professional race drivers, data analysts, and racing instructors from across prominent racing leagues.","The interviews were conducted using an exploratory, non-standardized expert interview format guided by a set of prepared questions.","The study investigates drivers' exploration strategies to reach their vehicle limits and contrasts them with the capabilities of state-of-the-art autonomous racing software stacks.","Participants were questioned about the techniques and skills they have developed to quickly approach and maneuver at the vehicle limit, ultimately minimizing lap times.","The analysis of the interviews was grounded in Mayring's qualitative content analysis framework, which facilitated the organization of the data into multiple categories and subcategories.","Our findings create insights into human behavior regarding reaching a vehicle's limit and minimizing lap times.","We conclude from the findings the development of new autonomy software modules that allow for more adaptive vehicle behavior.","By emphasizing the distinct nuances between manual and autonomous driving techniques, the paper encourages further investigation into human drivers' strategies to maximize their vehicles' capabilities."],"url":"http://arxiv.org/abs/2405.02620v1","category":"cs.RO"}
{"created":"2024-05-06 17:36:44","title":"A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose","abstract":"Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.","sentences":["Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate.","Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation.","In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses.","Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world.","During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images.","We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection.","We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization.","These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods.","We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information.","Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset."],"url":"http://arxiv.org/abs/2405.03659v1","category":"cs.CV"}
{"created":"2024-05-06 17:06:11","title":"Classification of Breast Cancer Histopathology Images using a Modified Supervised Contrastive Learning Method","abstract":"Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases. However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available. This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives. Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data. We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method. This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space.","sentences":["Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases.","However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available.","This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives.","Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data.","We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method.","This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space."],"url":"http://arxiv.org/abs/2405.03642v1","category":"cs.CV"}
{"created":"2024-05-06 16:55:30","title":"Collage: Light-Weight Low-Precision Strategy for LLM Training","abstract":"Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice.","sentences":["Large models training is plagued by the intense compute cost and limited hardware memory.","A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful.","We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process.","We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted.","To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies.","Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit.","Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice."],"url":"http://arxiv.org/abs/2405.03637v1","category":"cs.LG"}
{"created":"2024-05-06 16:05:12","title":"Towards Utilizing Scanning Gate Microscopy as a High-Resolution Probe of Valley Splitting in Si/SiGe Heterostructures","abstract":"A detailed understanding of the material properties that affect the splitting between the two low-lying valley states in Si/SiGe heterostructures will be increasingly important as the number of spin qubits is increased. Scanning gate microscopy has been proposed as a method to measure the spatial variation of the valley splitting as a tip-induced dot is moved around in the plane of the Si quantum well. We develop a simulation using an electrostatic model of the scanning gate microscope tip and the overlapping gate structure combined with an approximate solution to the three-dimensional Schr\\\"odinger-Poisson equation in the device stack. Using this simulation, we show that a tip-induced quantum dot formed near source and drain electrodes can be adiabatically moved to a region far from the gate electrodes. We argue that by spatially translating the tip-induced dot across a defect in the Si/SiGe interface, changes in valley splitting can be detected.","sentences":["A detailed understanding of the material properties that affect the splitting between the two low-lying valley states in Si/SiGe heterostructures will be increasingly important as the number of spin qubits is increased.","Scanning gate microscopy has been proposed as a method to measure the spatial variation of the valley splitting as a tip-induced dot is moved around in the plane of the Si quantum well.","We develop a simulation using an electrostatic model of the scanning gate microscope tip and the overlapping gate structure combined with an approximate solution to the three-dimensional Schr\\\"odinger-Poisson equation in the device stack.","Using this simulation, we show that a tip-induced quantum dot formed near source and drain electrodes can be adiabatically moved to a region far from the gate electrodes.","We argue that by spatially translating the tip-induced dot across a defect in the Si/SiGe interface, changes in valley splitting can be detected."],"url":"http://arxiv.org/abs/2405.03596v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 16:01:39","title":"Constrained inhomogeneous spherical equations: average-case hardness","abstract":"In this paper we analyze computational properties of the Diophantine problem (and its search variant) for spherical equations $\\prod_{i=1}^m z_i^{-1} c_i z_i = 1$ (and its variants) over the class of finite metabelian groups $G_{p,n}=\\mathbb{Z}_p^n \\rtimes \\mathbb{Z}_p^\\ast$, where $n\\in\\mathbb{N}$ and $p$ is prime. We prove that the problem of finding solutions for certain constrained spherical equations is computationally hard on average (assuming that some lattice approximation problem is hard in the worst case).","sentences":["In this paper we analyze computational properties of the Diophantine problem (and its search variant) for spherical equations $\\prod_{i=1}^m z_i^{-1} c_i z_i","= 1$ (and its variants) over the class of finite metabelian groups $G_{p,n}=\\mathbb{Z}_p^n \\rtimes \\mathbb{Z}_p^\\ast$, where $n\\in\\mathbb{N}$ and $p$ is prime.","We prove that the problem of finding solutions for certain constrained spherical equations is computationally hard on average (assuming that some lattice approximation problem is hard in the worst case)."],"url":"http://arxiv.org/abs/2405.03591v1","category":"math.GR"}
{"created":"2024-05-06 15:53:41","title":"$D^0$ meson production in $pp$ collisions at large $Q_s^2$","abstract":"The impact of the non-linear effects in the QCD dynamics on the observables is directly related to the magnitude of the saturation scale $Q_s$, which is predicted to increase with the energy, rapidity and multiplicity. In this paper, we investigate the $D^0$ meson production in $pp$ collisions at forward rapidities and/or high multiplicities considering the Color Glass Condensate (CGC) formalism and the solutions of the running coupling Balitsky - Kovchegov (BK) equation. The contributions of gluon - and charm - initiated processes are taken into account, and a comparison with the current LHCb data is performed. The impact of an intrinsic charm component in the proton's wave function is also estimated. Predictions for the self-normalized yields of $D^0$ mesons as a function of the multiplicity of coproduced charged hadrons are presented, considering $pp$ collisions at $\\sqrt{s} = 13$ TeV and different values of the meson rapidity. A comparison with the predictions for the kaon and isolated photon production is performed. Our results indicate that a future experimental analysis of the $D^0$ meson production at forward rapidities and high multiplicities can be useful to probe the CGC formalism and to disentangle the contribution of initial - and final - state effects.","sentences":["The impact of the non-linear effects in the QCD dynamics on the observables is directly related to the magnitude of the saturation scale $Q_s$, which is predicted to increase with the energy, rapidity and multiplicity.","In this paper, we investigate the $D^0$ meson production in $pp$ collisions at forward rapidities and/or high multiplicities considering the Color Glass Condensate (CGC) formalism and the solutions of the running coupling Balitsky - Kovchegov (BK) equation.","The contributions of gluon - and charm - initiated processes are taken into account, and a comparison with the current LHCb data is performed.","The impact of an intrinsic charm component in the proton's wave function is also estimated.","Predictions for the self-normalized yields of $D^0$ mesons as a function of the multiplicity of coproduced charged hadrons are presented, considering $pp$ collisions at $\\sqrt{s} = 13$ TeV and different values of the meson rapidity.","A comparison with the predictions for the kaon and isolated photon production is performed.","Our results indicate that a future experimental analysis of the $D^0$ meson production at forward rapidities and high multiplicities can be useful to probe the CGC formalism and to disentangle the contribution of initial - and final - state effects."],"url":"http://arxiv.org/abs/2405.03581v1","category":"hep-ph"}
{"created":"2024-05-06 14:47:19","title":"Quasi-Monte Carlo for Bayesian design of experiment problems governed by parametric PDEs","abstract":"This paper contributes to the study of optimal experimental design for Bayesian inverse problems governed by partial differential equations (PDEs). We derive estimates for the parametric regularity of multivariate double integration problems over high-dimensional parameter and data domains arising in Bayesian optimal design problems. We provide a detailed analysis for these double integration problems using two approaches: a full tensor product and a sparse tensor product combination of quasi-Monte Carlo (QMC) cubature rules over the parameter and data domains. Specifically, we show that the latter approach significantly improves the convergence rate, exhibiting performance comparable to that of QMC integration of a single high-dimensional integral. Furthermore, we numerically verify the predicted convergence rates for an elliptic PDE problem with an unknown diffusion coefficient in two spatial dimensions, offering empirical evidence supporting the theoretical results and highlighting practical applicability.","sentences":["This paper contributes to the study of optimal experimental design for Bayesian inverse problems governed by partial differential equations (PDEs).","We derive estimates for the parametric regularity of multivariate double integration problems over high-dimensional parameter and data domains arising in Bayesian optimal design problems.","We provide a detailed analysis for these double integration problems using two approaches: a full tensor product and a sparse tensor product combination of quasi-Monte Carlo (QMC) cubature rules over the parameter and data domains.","Specifically, we show that the latter approach significantly improves the convergence rate, exhibiting performance comparable to that of QMC integration of a single high-dimensional integral.","Furthermore, we numerically verify the predicted convergence rates for an elliptic PDE problem with an unknown diffusion coefficient in two spatial dimensions, offering empirical evidence supporting the theoretical results and highlighting practical applicability."],"url":"http://arxiv.org/abs/2405.03529v1","category":"math.NA"}
{"created":"2024-05-06 14:19:40","title":"Accurate simulation of Efimov physics in ultracold atomic gases with realistic three-body multichannel interactions","abstract":"We give a detailed and self-contained description of a recently developed theoretical and numerical method for the simulation of three identical bosonic alkali-metal atoms near a Feshbach resonance, where the Efimov effect is induced. The method is based on a direct construction of the off-shell two-body transition matrix from exact eigenfunctions of the embedded two-body Hamiltonians, obtained using realistic parameterizations of the interaction potentials which accurately reproduce the molecular energy levels. The transition matrix is then inserted into the appropriate three-body integral equations, which may be efficiently solved on a computer. We focus especially on the power of our method in including rigorously the effects of multichannel physics on the three-body problem, which are usually accounted for only by various approximations. We demonstrate the method for ${}^7$Li, where we recently showed that a correct inclusion of this multichannel physics resolves the long-standing disagreement between theory and experiment regarding the Efimovian three-body parameter. We analyze the Efimovian enhancement of the three-body recombination rate on both sides of the Feshbach resonance, revealing strong sensitivity to the spin structure of the model thus indicating the prevalence of three-body spin-exchange physics. Finally, we discuss an extension of our methodology to the calculation of three-body bound-state energies.","sentences":["We give a detailed and self-contained description of a recently developed theoretical and numerical method for the simulation of three identical bosonic alkali-metal atoms near a Feshbach resonance, where the Efimov effect is induced.","The method is based on a direct construction of the off-shell two-body transition matrix from exact eigenfunctions of the embedded two-body Hamiltonians, obtained using realistic parameterizations of the interaction potentials which accurately reproduce the molecular energy levels.","The transition matrix is then inserted into the appropriate three-body integral equations, which may be efficiently solved on a computer.","We focus especially on the power of our method in including rigorously the effects of multichannel physics on the three-body problem, which are usually accounted for only by various approximations.","We demonstrate the method for ${}^7$Li, where we recently showed that a correct inclusion of this multichannel physics resolves the long-standing disagreement between theory and experiment regarding the Efimovian three-body parameter.","We analyze the Efimovian enhancement of the three-body recombination rate on both sides of the Feshbach resonance, revealing strong sensitivity to the spin structure of the model thus indicating the prevalence of three-body spin-exchange physics.","Finally, we discuss an extension of our methodology to the calculation of three-body bound-state energies."],"url":"http://arxiv.org/abs/2405.03504v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-06 13:45:32","title":"The spectral determinant for second order elliptic operators on the real line","abstract":"We derive an expression for the spectral determinant of a second-order elliptic differential operator $\\mathcal{T}$ defined on the whole real line, in terms of the Wronskians of two particular solutions of the equation $\\mathcal{T} u=0$. Examples of application of the resulting formula include the explicit calculation of the determinant of harmonic and anharmonic oscillators with an added bounded potential with compact support.","sentences":["We derive an expression for the spectral determinant of a second-order elliptic differential operator $\\mathcal{T}$ defined on the whole real line, in terms of the Wronskians of two particular solutions of the equation $\\mathcal{T} u=0$. Examples of application of the resulting formula include the explicit calculation of the determinant of harmonic and anharmonic oscillators with an added bounded potential with compact support."],"url":"http://arxiv.org/abs/2405.03469v1","category":"math.SP"}
{"created":"2024-05-06 12:59:05","title":"DBDH: A Dual-Branch Dual-Head Neural Network for Invisible Embedded Regions Localization","abstract":"Embedding invisible hyperlinks or hidden codes in images to replace QR codes has become a hot topic recently. This technology requires first localizing the embedded region in the captured photos before decoding. Existing methods that train models to find the invisible embedded region struggle to obtain accurate localization results, leading to degraded decoding accuracy. This limitation is primarily because the CNN network is sensitive to low-frequency signals, while the embedded signal is typically in the high-frequency form. Based on this, this paper proposes a Dual-Branch Dual-Head (DBDH) neural network tailored for the precise localization of invisible embedded regions. Specifically, DBDH uses a low-level texture branch containing 62 high-pass filters to capture the high-frequency signals induced by embedding. A high-level context branch is used to extract discriminative features between the embedded and normal regions. DBDH employs a detection head to directly detect the four vertices of the embedding region. In addition, we introduce an extra segmentation head to segment the mask of the embedding region during training. The segmentation head provides pixel-level supervision for model learning, facilitating better learning of the embedded signals. Based on two state-of-the-art invisible offline-to-online messaging methods, we construct two datasets and augmentation strategies for training and testing localization models. Extensive experiments demonstrate the superior performance of the proposed DBDH over existing methods.","sentences":["Embedding invisible hyperlinks or hidden codes in images to replace QR codes has become a hot topic recently.","This technology requires first localizing the embedded region in the captured photos before decoding.","Existing methods that train models to find the invisible embedded region struggle to obtain accurate localization results, leading to degraded decoding accuracy.","This limitation is primarily because the CNN network is sensitive to low-frequency signals, while the embedded signal is typically in the high-frequency form.","Based on this, this paper proposes a Dual-Branch Dual-Head (DBDH) neural network tailored for the precise localization of invisible embedded regions.","Specifically, DBDH uses a low-level texture branch containing 62 high-pass filters to capture the high-frequency signals induced by embedding.","A high-level context branch is used to extract discriminative features between the embedded and normal regions.","DBDH employs a detection head to directly detect the four vertices of the embedding region.","In addition, we introduce an extra segmentation head to segment the mask of the embedding region during training.","The segmentation head provides pixel-level supervision for model learning, facilitating better learning of the embedded signals.","Based on two state-of-the-art invisible offline-to-online messaging methods, we construct two datasets and augmentation strategies for training and testing localization models.","Extensive experiments demonstrate the superior performance of the proposed DBDH over existing methods."],"url":"http://arxiv.org/abs/2405.03436v1","category":"cs.CV"}
{"created":"2024-05-06 12:30:08","title":"Unique solvability and error analysis of the Lagrange multiplier approach for gradient flows","abstract":"The unique solvability and error analysis of the original Lagrange multiplier approach proposed in [8] for gradient flows is studied in this paper. We identify a necessary and sufficient condition that must be satisfied for the nonlinear algebraic equation arising from the original Lagrange multiplier approach to admit a unique solution in the neighborhood of its exact solution, and propose a modified Lagrange multiplier approach so that the computation can continue even if the aforementioned condition is not satisfied. Using Cahn-Hilliard equation as an example, we prove rigorously the unique solvability and establish optimal error estimates of a second-order Lagrange multiplier scheme assuming this condition and that the time step is sufficient small. We also present numerical results to demonstrate that the modified Lagrange multiplier approach is much more robust and can use much larger time step than the original Lagrange multiplier approach.","sentences":["The unique solvability and error analysis of the original Lagrange multiplier approach proposed in [8] for gradient flows is studied in this paper.","We identify a necessary and sufficient condition that must be satisfied for the nonlinear algebraic equation arising from the original Lagrange multiplier approach to admit a unique solution in the neighborhood of its exact solution, and propose a modified Lagrange multiplier approach so that the computation can continue even if the aforementioned condition is not satisfied.","Using Cahn-Hilliard equation as an example, we prove rigorously the unique solvability and establish optimal error estimates of a second-order Lagrange multiplier scheme assuming this condition and that the time step is sufficient small.","We also present numerical results to demonstrate that the modified Lagrange multiplier approach is much more robust and can use much larger time step than the original Lagrange multiplier approach."],"url":"http://arxiv.org/abs/2405.03415v1","category":"math.NA"}
{"created":"2024-05-06 12:04:47","title":"Cosmology using Strong Gravitational Lensing","abstract":"The light we observe from distant astrophysical objects including supernovae and quasars allows us to determine large distances in terms of a cosmological model. Despite the success of the standard cosmological model in fitting the data, there remains no underlying explanation for the accelerated expansion and dark matter. Furthermore, there is a current tension between early- and late-universe determinations of the Hubble constant. New techniques may offer the possibility of measuring out to larger distances, provide complementary information, or be able to side-step current limitations. After reviewing in detail the fundamentals of standard cosmology and gravitational lensing, including a derivation of the cosmological lens equation, this thesis investigates a novel method of cosmography based on combining the techniques of strong gravitational lensing time delay measurements and quasar reverberation mapping. The motivation for this method was the possibility of avoiding lens modelling challenges, such as the mass-sheet degeneracy, typically associated with time delay cosmography. It suggested that differential time delays originating from spatially separated signals in the Broad Line Region of a quasar could be distinguished and measured from the spectroscopy of the images, and utilised to provide a ratio of cosmological distances independent of the lensing potential. An analytic description of the effect of the differential lensing on the emission line spectral flux for axisymmetric Broad Line Region geometries is given, with the inclined ring or disk, spherical shell, and double cone as examples. This critical examination shows that the proposed method is unable to recover cosmological information, as the observed time delay and inferred line-of-sight velocity do not uniquely map to the three-dimensional position within the quasar.","sentences":["The light we observe from distant astrophysical objects including supernovae and quasars allows us to determine large distances in terms of a cosmological model.","Despite the success of the standard cosmological model in fitting the data, there remains no underlying explanation for the accelerated expansion and dark matter.","Furthermore, there is a current tension between early- and late-universe determinations of the Hubble constant.","New techniques may offer the possibility of measuring out to larger distances, provide complementary information, or be able to side-step current limitations.","After reviewing in detail the fundamentals of standard cosmology and gravitational lensing, including a derivation of the cosmological lens equation, this thesis investigates a novel method of cosmography based on combining the techniques of strong gravitational lensing time delay measurements and quasar reverberation mapping.","The motivation for this method was the possibility of avoiding lens modelling challenges, such as the mass-sheet degeneracy, typically associated with time delay cosmography.","It suggested that differential time delays originating from spatially separated signals in the Broad Line Region of a quasar could be distinguished and measured from the spectroscopy of the images, and utilised to provide a ratio of cosmological distances independent of the lensing potential.","An analytic description of the effect of the differential lensing on the emission line spectral flux for axisymmetric Broad Line Region geometries is given, with the inclined ring or disk, spherical shell, and double cone as examples.","This critical examination shows that the proposed method is unable to recover cosmological information, as the observed time delay and inferred line-of-sight velocity do not uniquely map to the three-dimensional position within the quasar."],"url":"http://arxiv.org/abs/2405.03397v1","category":"astro-ph.CO"}
{"created":"2024-05-06 11:46:04","title":"3D LiDAR Mapping in Dynamic Environments Using a 4D Implicit Neural Representation","abstract":"Building accurate maps is a key building block to enable reliable localization, planning, and navigation of autonomous vehicles. We propose a novel approach for building accurate maps of dynamic environments utilizing a sequence of LiDAR scans. To this end, we propose encoding the 4D scene into a novel spatio-temporal implicit neural map representation by fitting a time-dependent truncated signed distance function to each point. Using our representation, we extract the static map by filtering the dynamic parts. Our neural representation is based on sparse feature grids, a globally shared decoder, and time-dependent basis functions, which we jointly optimize in an unsupervised fashion. To learn this representation from a sequence of LiDAR scans, we design a simple yet efficient loss function to supervise the map optimization in a piecewise way. We evaluate our approach on various scenes containing moving objects in terms of the reconstruction quality of static maps and the segmentation of dynamic point clouds. The experimental results demonstrate that our method is capable of removing the dynamic part of the input point clouds while reconstructing accurate and complete 3D maps, outperforming several state-of-the-art methods. Codes are available at: https://github.com/PRBonn/4dNDF","sentences":["Building accurate maps is a key building block to enable reliable localization, planning, and navigation of autonomous vehicles.","We propose a novel approach for building accurate maps of dynamic environments utilizing a sequence of LiDAR scans.","To this end, we propose encoding the 4D scene into a novel spatio-temporal implicit neural map representation by fitting a time-dependent truncated signed distance function to each point.","Using our representation, we extract the static map by filtering the dynamic parts.","Our neural representation is based on sparse feature grids, a globally shared decoder, and time-dependent basis functions, which we jointly optimize in an unsupervised fashion.","To learn this representation from a sequence of LiDAR scans, we design a simple yet efficient loss function to supervise the map optimization in a piecewise way.","We evaluate our approach on various scenes containing moving objects in terms of the reconstruction quality of static maps and the segmentation of dynamic point clouds.","The experimental results demonstrate that our method is capable of removing the dynamic part of the input point clouds while reconstructing accurate and complete 3D maps, outperforming several state-of-the-art methods.","Codes are available at: https://github.com/PRBonn/4dNDF"],"url":"http://arxiv.org/abs/2405.03388v1","category":"cs.CV"}
{"created":"2024-05-06 11:40:57","title":"Statistical Edge Detection And UDF Learning For Shape Representation","abstract":"In the field of computer vision, the numerical encoding of 3D surfaces is crucial. It is classical to represent surfaces with their Signed Distance Functions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like representation learning, surface classification, or surface reconstruction, this function can be learned by a neural network, called Neural Distance Function. This network, and in particular its weights, may serve as a parametric and implicit representation for the surface. The network must represent the surface as accurately as possible. In this paper, we propose a method for learning UDFs that improves the fidelity of the obtained Neural UDF to the original 3D surface. The key idea of our method is to concentrate the learning effort of the Neural UDF on surface edges. More precisely, we show that sampling more training points around surface edges allows better local accuracy of the trained Neural UDF, and thus improves the global expressiveness of the Neural UDF in terms of Hausdorff distance. To detect surface edges, we propose a new statistical method based on the calculation of a $p$-value at each point on the surface. Our method is shown to detect surface edges more accurately than a commonly used local geometric descriptor.","sentences":["In the field of computer vision, the numerical encoding of 3D surfaces is crucial.","It is classical to represent surfaces with their Signed Distance Functions (SDFs) or Unsigned Distance Functions (UDFs).","For tasks like representation learning, surface classification, or surface reconstruction, this function can be learned by a neural network, called Neural Distance Function.","This network, and in particular its weights, may serve as a parametric and implicit representation for the surface.","The network must represent the surface as accurately as possible.","In this paper, we propose a method for learning UDFs that improves the fidelity of the obtained Neural UDF to the original 3D surface.","The key idea of our method is to concentrate the learning effort of the Neural UDF on surface edges.","More precisely, we show that sampling more training points around surface edges allows better local accuracy of the trained Neural UDF, and thus improves the global expressiveness of the Neural UDF in terms of Hausdorff distance.","To detect surface edges, we propose a new statistical method based on the calculation of a $p$-value at each point on the surface.","Our method is shown to detect surface edges more accurately than a commonly used local geometric descriptor."],"url":"http://arxiv.org/abs/2405.03381v1","category":"cs.CV"}
{"created":"2024-05-06 10:55:42","title":"Perfect codes over non-prime power alphabets: an approach based on Diophantine equations","abstract":"The classification of perfect codes over non-prime power alphabets has been an open problem for which there have been no new results in almost $50$ years. In this paper, we show non-existence of perfect $2-$error correcting codes over $q-$ary alphabets for more than 170 new values of $q$. Our methods rely on techniques from the resolution of generalised Ramanujan--Nagell equations and from computational number theory.","sentences":["The classification of perfect codes over non-prime power alphabets has been an open problem for which there have been no new results in almost $50$ years.","In this paper, we show non-existence of perfect $2-$error correcting codes over $q-$ary alphabets for more than 170 new values of $q$. Our methods rely on techniques from the resolution of generalised Ramanujan--Nagell equations and from computational number theory."],"url":"http://arxiv.org/abs/2405.03347v1","category":"math.NT"}
{"created":"2024-05-06 09:40:41","title":"$Z$-critical equations for holomorphic vector bundles on K\u00e4hler surfaces","abstract":"We prove that the existence of a $Z$-positive and $Z$-critical Hermitian metric on a rank 2 holomorphic vector bundle over a compact K\\\"ahler surface implies that the bundle is $Z$-stable. As particular cases, we obtain stability results for the deformed Hermitian Yang-Mills equation and the almost Hermite-Einstein equation for rank 2 bundles over surfaces. We show examples of $Z$-unstable bundles and $Z$-critical metrics away from the large volume limit.","sentences":["We prove that the existence of a $Z$-positive and $Z$-critical Hermitian metric on a rank 2 holomorphic vector bundle over a compact K\\\"ahler surface implies that the bundle is $Z$-stable.","As particular cases, we obtain stability results for the deformed Hermitian Yang-Mills equation and the almost Hermite-Einstein equation for rank 2 bundles over surfaces.","We show examples of $Z$-unstable bundles and $Z$-critical metrics away from the large volume limit."],"url":"http://arxiv.org/abs/2405.03312v1","category":"math.DG"}
{"created":"2024-05-06 09:36:37","title":"Statistical Equivalence of Metrics for Meteor Dynamical Association","abstract":"We statistically evaluate and compare four orbital similarity criteria within five-dimensional parameter space ($D_{SH}$, $D_D$, $D_H$, and $\\varrho_2$) to study dynamical associations using the already classified meteors (manually by a human) in CAMS database as a benchmark. In addition, we assess various distance metrics typically used in Machine Learning with two different vectors: ORBIT, grounded in heliocentric orbital elements, and GEO, predicated on geocentric observational parameters. Additionally, we compute the optimal cut-offs for all methods for distinguishing sporadic background events. Our findings demonstrate the superior performance of the sEuclidean metric in conjunction with the GEO vector. Within the scope of D-criteria, $D_{SH}$ emerged as the preeminent metric, closely followed by $\\varrho_2$. $\\varrho_2$ stands out as the most equivalence to the distance metrics when utilizing the GEO vector and the most compatible with GEO and ORBIT simultaneously, whereas $D_D$ aligns more closely when using the ORBIT vector. The stark contrast in $D_D$'s behavior compared to other D-criteria highlights potential inequivalence. Geocentric features provide a more robust basis than orbital elements for meteor dynamical association. Most distance metrics associated with the GEO vector surpass the D-criteria when differentiating the meteoroid background. Accuracy displayed a dependence on solar longitude with a pronounced decrease around 180$^\\circ$ matching an apparent increase in the meteoroid background activity, tentatively associated with the transition from the Perseids to the Orionids. Considering lately identified meteor showers, $\\sim$27\\% of meteors in CAMS would have different associations. This work unveils that Machine Learning distance metrics can rival or even exceed the performance of tailored orbital similarity criteria for meteor dynamical association.","sentences":["We statistically evaluate and compare four orbital similarity criteria within five-dimensional parameter space ($D_{SH}$, $D_D$, $D_H$, and $\\varrho_2$) to study dynamical associations using the already classified meteors (manually by a human) in CAMS database as a benchmark.","In addition, we assess various distance metrics typically used in Machine Learning with two different vectors: ORBIT, grounded in heliocentric orbital elements, and GEO, predicated on geocentric observational parameters.","Additionally, we compute the optimal cut-offs for all methods for distinguishing sporadic background events.","Our findings demonstrate the superior performance of the sEuclidean metric in conjunction with the GEO vector.","Within the scope of D-criteria, $D_{SH}$ emerged as the preeminent metric, closely followed by $\\varrho_2$. $\\varrho_2$ stands out as the most equivalence to the distance metrics when utilizing the GEO vector and the most compatible with GEO and ORBIT simultaneously, whereas $D_D$ aligns more closely when using the ORBIT vector.","The stark contrast in $D_D$'s behavior compared to other D-criteria highlights potential inequivalence.","Geocentric features provide a more robust basis than orbital elements for meteor dynamical association.","Most distance metrics associated with the GEO vector surpass the D-criteria when differentiating the meteoroid background.","Accuracy displayed a dependence on solar longitude with a pronounced decrease around 180$^\\circ$ matching an apparent increase in the meteoroid background activity, tentatively associated with the transition from the Perseids to the Orionids.","Considering lately identified meteor showers, $\\sim$27\\% of meteors in CAMS would have different associations.","This work unveils that Machine Learning distance metrics can rival or even exceed the performance of tailored orbital similarity criteria for meteor dynamical association."],"url":"http://arxiv.org/abs/2405.03308v1","category":"astro-ph.EP"}
{"created":"2024-05-06 09:19:50","title":"Radial fields on the manifolds of symmetric positive definite matrices","abstract":"On Hadamard manifolds, the radial fields, which are the negative gradients of the Busemann functions, can be used to designate a canonical sense of direction. This has many potential interesting applications to Hadamard manifold-valued data, for example in defining notions of quantiles or treatment effects. Some of the most commonly encountered Hadamard manifolds in statistics are the spaces of symmetric positive definite matrices, which are used in, for example, covariance matrix analysis and diffusion tensor imaging. In this paper, we derive an expression for the radial fields on these manifolds and demonstrate their smoothness.","sentences":["On Hadamard manifolds, the radial fields, which are the negative gradients of the Busemann functions, can be used to designate a canonical sense of direction.","This has many potential interesting applications to Hadamard manifold-valued data, for example in defining notions of quantiles or treatment effects.","Some of the most commonly encountered Hadamard manifolds in statistics are the spaces of symmetric positive definite matrices, which are used in, for example, covariance matrix analysis and diffusion tensor imaging.","In this paper, we derive an expression for the radial fields on these manifolds and demonstrate their smoothness."],"url":"http://arxiv.org/abs/2405.03297v1","category":"math.DG"}
{"created":"2024-05-06 08:26:01","title":"Null controllability for stochastic fourth order semi-discrete parabolic equations","abstract":"This paper is devoted to studying null controllability for a class of stochastic fourth order semi-discrete parabolic equations, where the spatial variable is discretized with finite difference scheme and the time is kept as a continuous variable. For this purpose, we establish a new global Carleman estimate for a backward stochastic fourth order semi-discrete parabolic operators, in which the large parameter is connected to the mesh size. A relaxed observability estimate is established for backward stochastic fourth order semi-discrete parabolic equations by this new Carleman estimate, with an explicit observability constant that depends on the discretization parameter and coefficients of lower order terms. Then, the $\\phi$-null controllability of the stochastic fourth order semi-discrete parabolic equations is proved using the standard duality technique.","sentences":["This paper is devoted to studying null controllability for a class of stochastic fourth order semi-discrete parabolic equations, where the spatial variable is discretized with finite difference scheme and the time is kept as a continuous variable.","For this purpose, we establish a new global Carleman estimate for a backward stochastic fourth order semi-discrete parabolic operators, in which the large parameter is connected to the mesh size.","A relaxed observability estimate is established for backward stochastic fourth order semi-discrete parabolic equations by this new Carleman estimate, with an explicit observability constant that depends on the discretization parameter and coefficients of lower order terms.","Then, the $\\phi$-null controllability of the stochastic fourth order semi-discrete parabolic equations is proved using the standard duality technique."],"url":"http://arxiv.org/abs/2405.03257v1","category":"math.OC"}
{"created":"2024-05-06 08:21:33","title":"Automatic Assessment of Dysarthria Using Audio-visual Vowel Graph Attention Network","abstract":"Automatic assessment of dysarthria remains a highly challenging task due to high variability in acoustic signals and the limited data. Currently, research on the automatic assessment of dysarthria primarily focuses on two approaches: one that utilizes expert features combined with machine learning, and the other that employs data-driven deep learning methods to extract representations. Research has demonstrated that expert features are effective in representing pathological characteristics, while deep learning methods excel at uncovering latent features. Therefore, integrating the advantages of expert features and deep learning to construct a neural network architecture based on expert knowledge may be beneficial for interpretability and assessment performance. In this context, the present paper proposes a vowel graph attention network based on audio-visual information, which effectively integrates the strengths of expert knowledges and deep learning. Firstly, various features were combined as inputs, including knowledge based acoustical features and deep learning based pre-trained representations. Secondly, the graph network structure based on vowel space theory was designed, allowing for a deep exploration of spatial correlations among vowels. Finally, visual information was incorporated into the model to further enhance its robustness and generalizability. The method exhibited superior performance in regression experiments targeting Frenchay scores compared to existing approaches.","sentences":["Automatic assessment of dysarthria remains a highly challenging task due to high variability in acoustic signals and the limited data.","Currently, research on the automatic assessment of dysarthria primarily focuses on two approaches: one that utilizes expert features combined with machine learning, and the other that employs data-driven deep learning methods to extract representations.","Research has demonstrated that expert features are effective in representing pathological characteristics, while deep learning methods excel at uncovering latent features.","Therefore, integrating the advantages of expert features and deep learning to construct a neural network architecture based on expert knowledge may be beneficial for interpretability and assessment performance.","In this context, the present paper proposes a vowel graph attention network based on audio-visual information, which effectively integrates the strengths of expert knowledges and deep learning.","Firstly, various features were combined as inputs, including knowledge based acoustical features and deep learning based pre-trained representations.","Secondly, the graph network structure based on vowel space theory was designed, allowing for a deep exploration of spatial correlations among vowels.","Finally, visual information was incorporated into the model to further enhance its robustness and generalizability.","The method exhibited superior performance in regression experiments targeting Frenchay scores compared to existing approaches."],"url":"http://arxiv.org/abs/2405.03254v1","category":"eess.AS"}
{"created":"2024-05-06 07:55:41","title":"Minimal Lagrangian surfaces in $\\mathbb{C}P^2$ via the loop group method Part II: The general case","abstract":"We extend the techniques introduced in \\cite{DoMaB1} for contractible Riemann surfaces to construct minimal Lagrangian immersions from arbitrary Riemann surfaces into $\\mathbb{C}P^2$ via the loop group method. Based on the potentials of translationally equivariant minimal Lagrangian surfaces, we introduce perturbed equivariant minimal Lagrangian surfaces in $\\mathbb{C}P^2$ and construct a class of minimal Lagrangian cylinders. Furthermore, we show that these minimal Lagrangian cylinders approximate Delaunay cylinders with respect to some weighted Wiener norm of the twisted loop group $\\Lambda SU(3)_{\\sigma}$.","sentences":["We extend the techniques introduced in \\cite{DoMaB1} for contractible Riemann surfaces to construct minimal Lagrangian immersions from arbitrary Riemann surfaces into $\\mathbb{C}P^2$ via the loop group method.","Based on the potentials of translationally equivariant minimal Lagrangian surfaces, we introduce perturbed equivariant minimal Lagrangian surfaces in $\\mathbb{C}P^2$ and construct a class of minimal Lagrangian cylinders.","Furthermore, we show that these minimal Lagrangian cylinders approximate Delaunay cylinders with respect to some weighted Wiener norm of the twisted loop group $\\Lambda SU(3)_{\\sigma}$."],"url":"http://arxiv.org/abs/2405.03246v1","category":"math.DG"}
{"created":"2024-05-06 07:44:50","title":"Federated Reinforcement Learning with Constraint Heterogeneity","abstract":"We study a Federated Reinforcement Learning (FedRL) problem with constraint heterogeneity. In our setting, we aim to solve a reinforcement learning problem with multiple constraints while $N$ training agents are located in $N$ different environments with limited access to the constraint signals and they are expected to collaboratively learn a policy satisfying all constraint signals. Such learning problems are prevalent in scenarios of Large Language Model (LLM) fine-tuning and healthcare applications. To solve the problem, we propose federated primal-dual policy optimization methods based on traditional policy gradient methods. Specifically, we introduce $N$ local Lagrange functions for agents to perform local policy updates, and these agents are then scheduled to periodically communicate on their local policies. Taking natural policy gradient (NPG) and proximal policy optimization (PPO) as policy optimization methods, we mainly focus on two instances of our algorithms, ie, {FedNPG} and {FedPPO}. We show that FedNPG achieves global convergence with an $\\tilde{O}(1/\\sqrt{T})$ rate, and FedPPO efficiently solves complicated learning tasks with the use of deep neural networks.","sentences":["We study a Federated Reinforcement Learning (FedRL) problem with constraint heterogeneity.","In our setting, we aim to solve a reinforcement learning problem with multiple constraints while $N$ training agents are located in $N$ different environments with limited access to the constraint signals and they are expected to collaboratively learn a policy satisfying all constraint signals.","Such learning problems are prevalent in scenarios of Large Language Model (LLM) fine-tuning and healthcare applications.","To solve the problem, we propose federated primal-dual policy optimization methods based on traditional policy gradient methods.","Specifically, we introduce $N$ local Lagrange functions for agents to perform local policy updates, and these agents are then scheduled to periodically communicate on their local policies.","Taking natural policy gradient (NPG) and proximal policy optimization (PPO) as policy optimization methods, we mainly focus on two instances of our algorithms, ie, {FedNPG} and {FedPPO}.","We show that FedNPG achieves global convergence with an $\\tilde{O}(1/\\sqrt{T})$ rate, and FedPPO efficiently solves complicated learning tasks with the use of deep neural networks."],"url":"http://arxiv.org/abs/2405.03236v1","category":"cs.LG"}
{"created":"2024-05-06 07:05:07","title":"Sharp estimates, uniqueness and spikes condensation for superlinear free boundary problems arising in plasma physics","abstract":"We are concerned with Grad-Shafranov type equations, describing in dimension $N=2$ the equilibrium configurations of a plasma in a Tokamak. We obtain a sharp superlinear generalization of the result of Temam (1977) about the linear case, implying the first general uniqueness result ever for superlinear free boundary problems arising in plasma physics. Previous general uniqueness results of Beresticky-Brezis (1980) were concerned with globally Lipschitz nonlinearities. In dimension $N\\geq 3$ the uniqueness result is new but not sharp, motivating the local analysis of a spikes condensation-quantization phenomenon for superlinear and subcritical singularly perturbed Grad-Shafranov type free boundary problems, implying among other things a converse of the results about spikes condensation in Flucher-Wei (1998) and Wei (2001). Interestingly enough, in terms of the \"physical\" global variables, we come up with a concentration-quantization-compactness result sharing the typical features of critical problems (Yamabe $N\\geq 3$, Liouville $N=2$) but in a subcritical setting, the singular behavior being induced by a sort of infinite mass limit, in the same spirit of Brezis-Merle (1991).","sentences":["We are concerned with Grad-Shafranov type equations, describing in dimension $N=2$ the equilibrium configurations of a plasma in a Tokamak.","We obtain a sharp superlinear generalization of the result of Temam (1977) about the linear case, implying the first general uniqueness result ever for superlinear free boundary problems arising in plasma physics.","Previous general uniqueness results of Beresticky-Brezis (1980) were concerned with globally Lipschitz nonlinearities.","In dimension $N\\geq 3$ the uniqueness result is new but not sharp, motivating the local analysis of a spikes condensation-quantization phenomenon for superlinear and subcritical singularly perturbed Grad-Shafranov type free boundary problems, implying among other things a converse of the results about spikes condensation in Flucher-Wei (1998) and Wei (2001).","Interestingly enough, in terms of the \"physical\" global variables, we come up with a concentration-quantization-compactness result sharing the typical features of critical problems (Yamabe $N\\geq 3$, Liouville $N=2$) but in a subcritical setting, the singular behavior being induced by a sort of infinite mass limit, in the same spirit of Brezis-Merle (1991)."],"url":"http://arxiv.org/abs/2405.03203v1","category":"math.AP"}
{"created":"2024-05-06 06:32:58","title":"Exploring Frequencies via Feature Mixing and Meta-Learning for Improving Adversarial Transferability","abstract":"Recent studies have shown that Deep Neural Networks (DNNs) are susceptible to adversarial attacks, with frequency-domain analysis underscoring the significance of high-frequency components in influencing model predictions. Conversely, targeting low-frequency components has been effective in enhancing attack transferability on black-box models. In this study, we introduce a frequency decomposition-based feature mixing method to exploit these frequency characteristics in both clean and adversarial samples. Our findings suggest that incorporating features of clean samples into adversarial features extracted from adversarial examples is more effective in attacking normally-trained models, while combining clean features with the adversarial features extracted from low-frequency parts decomposed from the adversarial samples yields better results in attacking defense models. However, a conflict issue arises when these two mixing approaches are employed simultaneously. To tackle the issue, we propose a cross-frequency meta-optimization approach comprising the meta-train step, meta-test step, and final update. In the meta-train step, we leverage the low-frequency components of adversarial samples to boost the transferability of attacks against defense models. Meanwhile, in the meta-test step, we utilize adversarial samples to stabilize gradients, thereby enhancing the attack's transferability against normally trained models. For the final update, we update the adversarial sample based on the gradients obtained from both meta-train and meta-test steps. Our proposed method is evaluated through extensive experiments on the ImageNet-Compatible dataset, affirming its effectiveness in improving the transferability of attacks on both normally-trained CNNs and defense models.   The source code is available at https://github.com/WJJLL/MetaSSA.","sentences":["Recent studies have shown that Deep Neural Networks (DNNs) are susceptible to adversarial attacks, with frequency-domain analysis underscoring the significance of high-frequency components in influencing model predictions.","Conversely, targeting low-frequency components has been effective in enhancing attack transferability on black-box models.","In this study, we introduce a frequency decomposition-based feature mixing method to exploit these frequency characteristics in both clean and adversarial samples.","Our findings suggest that incorporating features of clean samples into adversarial features extracted from adversarial examples is more effective in attacking normally-trained models, while combining clean features with the adversarial features extracted from low-frequency parts decomposed from the adversarial samples yields better results in attacking defense models.","However, a conflict issue arises when these two mixing approaches are employed simultaneously.","To tackle the issue, we propose a cross-frequency meta-optimization approach comprising the meta-train step, meta-test step, and final update.","In the meta-train step, we leverage the low-frequency components of adversarial samples to boost the transferability of attacks against defense models.","Meanwhile, in the meta-test step, we utilize adversarial samples to stabilize gradients, thereby enhancing the attack's transferability against normally trained models.","For the final update, we update the adversarial sample based on the gradients obtained from both meta-train and meta-test steps.","Our proposed method is evaluated through extensive experiments on the ImageNet-Compatible dataset, affirming its effectiveness in improving the transferability of attacks on both normally-trained CNNs and defense models.   ","The source code is available at https://github.com/WJJLL/MetaSSA."],"url":"http://arxiv.org/abs/2405.03193v1","category":"cs.CV"}
{"created":"2024-05-06 05:28:44","title":"SOC-MartNet: A Martingale Neural Network for the Hamilton-Jacobi-Bellman Equation without Explicit inf H in Stochastic Optimal Controls","abstract":"In this work, we propose a martingale based neural network, SOC-MartNet, for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) equations where no explicit expression is needed for the Hamiltonian $\\inf_{u \\in U} H(t,x,u, z,p)$, and stochastic optimal control problems with controls on both drift and volatility. We reformulate the HJB equations into a stochastic neural network learning process, i.e., training a control network and a value network such that the associated Hamiltonian process is minimized and the cost process becomes a martingale.To enforce the martingale property for the cost process, we employ an adversarial network and construct a loss function based on the projection property of conditional expectations. Then, the control/value networks and the adversarial network are trained adversarially, such that the cost process is driven towards a martingale and the minimum principle is satisfied for the control.Numerical results show that the proposed SOC-MartNet is effective and efficient for solving HJB-type equations and SOCP with a dimension up to $500$ in a small number of training epochs.","sentences":["In this work, we propose a martingale based neural network, SOC-MartNet, for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) equations where no explicit expression is needed for the Hamiltonian $\\inf_{u \\in U} H(t,x,u, z,p)$, and stochastic optimal control problems with controls on both drift and volatility.","We reformulate the HJB equations into a stochastic neural network learning process, i.e., training a control network and a value network such that the associated Hamiltonian process is minimized and the cost process becomes a martingale.","To enforce the martingale property for the cost process, we employ an adversarial network and construct a loss function based on the projection property of conditional expectations.","Then, the control/value networks and the adversarial network are trained adversarially, such that the cost process is driven towards a martingale and the minimum principle is satisfied for the control.","Numerical results show that the proposed SOC-MartNet is effective and efficient for solving HJB-type equations and SOCP with a dimension up to $500$ in a small number of training epochs."],"url":"http://arxiv.org/abs/2405.03169v1","category":"math.NA"}
{"created":"2024-05-06 03:27:23","title":"TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning","abstract":"Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC.","sentences":["Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC).","However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG).","To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series.","Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token.","The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC."],"url":"http://arxiv.org/abs/2405.03140v1","category":"cs.LG"}
{"created":"2024-05-06 03:06:33","title":"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training","abstract":"Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.","sentences":["Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective.","Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks.","In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training.","Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances.","We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters.","Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%).","Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing.","We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision.","Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area."],"url":"http://arxiv.org/abs/2405.03133v1","category":"cs.CL"}
{"created":"2024-05-06 02:54:53","title":"Deep Learning for Causal Inference: A Comparison of Architectures for Heterogeneous Treatment Effect Estimation","abstract":"Causal inference has gained much popularity in recent years, with interests ranging from academic, to industrial, to educational, and all in between. Concurrently, the study and usage of neural networks has also grown profoundly (albeit at a far faster rate). What we aim to do in this blog write-up is demonstrate a Neural Network causal inference architecture. We develop a fully connected neural network implementation of the popular Bayesian Causal Forest algorithm, a state of the art tree based method for estimating heterogeneous treatment effects. We compare our implementation to existing neural network causal inference methodologies, showing improvements in performance in simulation settings. We apply our method to a dataset examining the effect of stress on sleep.","sentences":["Causal inference has gained much popularity in recent years, with interests ranging from academic, to industrial, to educational, and all in between.","Concurrently, the study and usage of neural networks has also grown profoundly (albeit at a far faster rate).","What we aim to do in this blog write-up is demonstrate a Neural Network causal inference architecture.","We develop a fully connected neural network implementation of the popular Bayesian Causal Forest algorithm, a state of the art tree based method for estimating heterogeneous treatment effects.","We compare our implementation to existing neural network causal inference methodologies, showing improvements in performance in simulation settings.","We apply our method to a dataset examining the effect of stress on sleep."],"url":"http://arxiv.org/abs/2405.03130v1","category":"stat.ML"}
{"created":"2024-05-06 01:42:40","title":"Compression-based Privacy Preservation for Distributed Nash Equilibrium Seeking in Aggregative Games","abstract":"This paper explores distributed aggregative games in multi-agent systems. Current methods for finding distributed Nash equilibrium require players to send original messages to their neighbors, leading to communication burden and privacy issues. To jointly address these issues, we propose an algorithm that uses stochastic compression to save communication resources and conceal information through random errors induced by compression. Our theoretical analysis shows that the algorithm guarantees convergence accuracy, even with aggressive compression errors used to protect privacy. We prove that the algorithm achieves differential privacy through a stochastic quantization scheme. Simulation results for energy consumption games support the effectiveness of our approach.","sentences":["This paper explores distributed aggregative games in multi-agent systems.","Current methods for finding distributed Nash equilibrium require players to send original messages to their neighbors, leading to communication burden and privacy issues.","To jointly address these issues, we propose an algorithm that uses stochastic compression to save communication resources and conceal information through random errors induced by compression.","Our theoretical analysis shows that the algorithm guarantees convergence accuracy, even with aggressive compression errors used to protect privacy.","We prove that the algorithm achieves differential privacy through a stochastic quantization scheme.","Simulation results for energy consumption games support the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.03106v1","category":"eess.SY"}
{"created":"2024-05-06 01:39:59","title":"Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs","abstract":"Large language models (LLMs) have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict latency and power demands. Deep neural network (DNN) quantization has traditionally addressed these limitations by converting models to low-precision integer formats. Yet recently alternative formats, such as Normal Float (NF4), have been shown to consistently increase model accuracy, albeit at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks to conclude most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), with respect to this distribution, that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and performance frontier across 11 datatypes, including non-traditional formats like Additive-Powers-of-Two (APoT), by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits.","sentences":["Large language models (LLMs) have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict latency and power demands.","Deep neural network (DNN) quantization has traditionally addressed these limitations by converting models to low-precision integer formats.","Yet recently alternative formats, such as Normal Float (NF4), have been shown to consistently increase model accuracy, albeit at the cost of increased chip area.","In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks to conclude most distributions follow a Student's t-distribution.","We then derive a new theoretically optimal format, Student Float (SF4), with respect to this distribution, that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks.","Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy.","Finally, we explore the quality and performance frontier across 11 datatypes, including non-traditional formats like Additive-Powers-of-Two (APoT), by evaluating their model accuracy and hardware complexity.","We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area.","For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits."],"url":"http://arxiv.org/abs/2405.03103v1","category":"cs.LG"}
{"created":"2024-05-06 00:21:27","title":"On the solvability of systems of equations revisited","abstract":"In this paper, we introduce a new and direct approach to study the solvability of systems of equations generated by bilinear forms. More precisely, let $B (\\cdot,   \\cdot)$ be a non-degenerate bilinear form and $E$ be a set in $\\mathbb{F}_q^2$. We prove that if $|E|\\gg q^{5/3}$ then the number of triples $(B(x, y), B(y, z), B(z, x))$ with $x, y, z\\in E$ is at least $cq^3$ for some positive constant $c$. This significantly improves a result due to the fifth listed author (2009).","sentences":["In this paper, we introduce a new and direct approach to study the solvability of systems of equations generated by bilinear forms.","More precisely, let $B (\\cdot,   \\cdot)$ be a non-degenerate bilinear form and $E$ be a set in $\\mathbb{F}_q^2$.","We prove that if $|E|\\gg q^{5/3}$ then the number of triples $(B(x, y), B(y, z), B(z, x))$ with $x, y, z\\in E$ is at least $cq^3$ for some positive constant $c$. This significantly improves a result due to the fifth listed author (2009)."],"url":"http://arxiv.org/abs/2405.03086v1","category":"math.NT"}
{"created":"2024-05-05 23:48:59","title":"Design optimization in unilateral contact using pressure constraints and Bayesian optimization","abstract":"Design optimization problems, e.g., shape optimization, that involve deformable bodies in unilateral contact are challenging as they require robust contact solvers, complex optimization methods that are typically gradient-based, and sensitivity derivations. Notably, the problems are nonsmooth, adding significant difficulty to the optimization process. We study design optimization problems in frictionless unilateral contact subject to pressure constraints, using both gradient-based and gradient-free optimization methods, namely Bayesian optimization. The contact simulation problem is solved via the mortar contact and finite element methods. For the gradient-based method, we use the direct differentiation method to compute the sensitivities of the cost and constraint function with respect to the design variables. Then, we use Ipopt to solve the optimization problems. For the gradient-free approach, we use a constrained Bayesian optimization algorithm based on the standard Gaussian Process surrogate model. We present numerical examples that control the contact pressure, inspired by real-life engineering applications, to demonstrate the effectiveness, strengths and shortcomings of both methods. Our results suggest that both optimization methods perform reasonably well for these nonsmooth problems.","sentences":["Design optimization problems, e.g., shape optimization, that involve deformable bodies in unilateral contact are challenging as they require robust contact solvers, complex optimization methods that are typically gradient-based, and sensitivity derivations.","Notably, the problems are nonsmooth, adding significant difficulty to the optimization process.","We study design optimization problems in frictionless unilateral contact subject to pressure constraints, using both gradient-based and gradient-free optimization methods, namely Bayesian optimization.","The contact simulation problem is solved via the mortar contact and finite element methods.","For the gradient-based method, we use the direct differentiation method to compute the sensitivities of the cost and constraint function with respect to the design variables.","Then, we use Ipopt to solve the optimization problems.","For the gradient-free approach, we use a constrained Bayesian optimization algorithm based on the standard Gaussian Process surrogate model.","We present numerical examples that control the contact pressure, inspired by real-life engineering applications, to demonstrate the effectiveness, strengths and shortcomings of both methods.","Our results suggest that both optimization methods perform reasonably well for these nonsmooth problems."],"url":"http://arxiv.org/abs/2405.03081v1","category":"math.NA"}
{"created":"2024-05-05 22:53:49","title":"Sup-slopes and sub-solutions for fully nonlinear elliptic equations","abstract":"We establish a necessary and sufficient condition for solving a general class of fully nonlinear elliptic equations on closed Riemannian or hermitian manifolds, including both hessian and hessian quotient equations. It settles an open problem of Li and Urbas. Such a condition is based on an analytic slope invariant analogous to the slope stability and the Nakai-Moishezon criterion in complex geometry. As an application, we solve the non-constant $J$-equation on both hermitian manifolds and singular K\\\"ahler spaces.","sentences":["We establish a necessary and sufficient condition for solving a general class of fully nonlinear elliptic equations on closed Riemannian or hermitian manifolds, including both hessian and hessian quotient equations.","It settles an open problem of Li and Urbas.","Such a condition is based on an analytic slope invariant analogous to the slope stability and the Nakai-Moishezon criterion in complex geometry.","As an application, we solve the non-constant $J$-equation on both hermitian manifolds and singular K\\\"ahler spaces."],"url":"http://arxiv.org/abs/2405.03074v1","category":"math.AP"}
{"created":"2024-05-05 21:30:18","title":"Convolutional Learning on Directed Acyclic Graphs","abstract":"We develop a novel convolutional architecture tailored for learning from data defined over directed acyclic graphs (DAGs). DAGs can be used to model causal relationships among variables, but their nilpotent adjacency matrices pose unique challenges towards developing DAG signal processing and machine learning tools. To address this limitation, we harness recent advances offering alternative definitions of causal shifts and convolutions for signals on DAGs. We develop a novel convolutional graph neural network that integrates learnable DAG filters to account for the partial ordering induced by the graph topology, thus providing valuable inductive bias to learn effective representations of DAG-supported data. We discuss the salient advantages and potential limitations of the proposed DAG convolutional network (DCN) and evaluate its performance on two learning tasks using synthetic data: network diffusion estimation and source identification. DCN compares favorably relative to several baselines, showcasing its promising potential.","sentences":["We develop a novel convolutional architecture tailored for learning from data defined over directed acyclic graphs (DAGs).","DAGs can be used to model causal relationships among variables, but their nilpotent adjacency matrices pose unique challenges towards developing DAG signal processing and machine learning tools.","To address this limitation, we harness recent advances offering alternative definitions of causal shifts and convolutions for signals on DAGs.","We develop a novel convolutional graph neural network that integrates learnable DAG filters to account for the partial ordering induced by the graph topology, thus providing valuable inductive bias to learn effective representations of DAG-supported data.","We discuss the salient advantages and potential limitations of the proposed DAG convolutional network (DCN) and evaluate its performance on two learning tasks using synthetic data: network diffusion estimation and source identification.","DCN compares favorably relative to several baselines, showcasing its promising potential."],"url":"http://arxiv.org/abs/2405.03056v1","category":"cs.LG"}
{"created":"2024-05-05 20:34:16","title":"Quantitative analysis of the prediction performance of a Convolutional Neural Network evaluating the surface elastic energy of a strained film","abstract":"A Deep Learning approach is devised to estimate the elastic energy density $\\rho$ at the free surface of an undulated stressed film. About 190000 arbitrary surface profiles h(x) are randomly generated by Perlin noise and paired with the corresponding elastic energy density profiles $\\rho(x)$, computed by a semi-analytical Green's function approximation, suitable for small-slope morphologies. The resulting dataset and smaller subsets of it are used for the training of a Fully Convolutional Neural Network. The trained models are shown to return quantitative predictions of $\\rho$, not only in terms of convergence of the loss function during training, but also in validation and testing, with better results in the case of the larger dataset. Extensive tests are performed to assess the generalization capability of the Neural Network model when applied to profiles with localized features or assigned geometries not included in the original dataset. Moreover, its possible exploitation on domain sizes beyond the one used in the training is also analyzed in-depth. The conditions providing a one-to-one reproduction of the ground-truth $\\rho(x)$ profiles computed by the Green's approximation are highlighted along with critical cases. The accuracy and robustness of the deep-learned $\\rho(x)$ are further demonstrated in the time-integration of surface evolution problems described by simple partial differential equations of evaporation/condensation and surface diffusion.","sentences":["A Deep Learning approach is devised to estimate the elastic energy density $\\rho$ at the free surface of an undulated stressed film.","About 190000 arbitrary surface profiles h(x) are randomly generated by Perlin noise and paired with the corresponding elastic energy density profiles $\\rho(x)$, computed by a semi-analytical Green's function approximation, suitable for small-slope morphologies.","The resulting dataset and smaller subsets of it are used for the training of a Fully Convolutional Neural Network.","The trained models are shown to return quantitative predictions of $\\rho$, not only in terms of convergence of the loss function during training, but also in validation and testing, with better results in the case of the larger dataset.","Extensive tests are performed to assess the generalization capability of the Neural Network model when applied to profiles with localized features or assigned geometries not included in the original dataset.","Moreover, its possible exploitation on domain sizes beyond the one used in the training is also analyzed in-depth.","The conditions providing a one-to-one reproduction of the ground-truth $\\rho(x)$ profiles computed by the Green's approximation are highlighted along with critical cases.","The accuracy and robustness of the deep-learned $\\rho(x)$ are further demonstrated in the time-integration of surface evolution problems described by simple partial differential equations of evaporation/condensation and surface diffusion."],"url":"http://arxiv.org/abs/2405.03049v1","category":"physics.comp-ph"}
{"created":"2024-05-05 19:38:55","title":"Hydrodynamics of vortex memory system driven by edge-current and boundary magnetization","abstract":"In this study, a magnetohydrodynamic model is developed to study the dynamics of vortices driven by edge-current. Two modeled equations for fluid and magnetic field variables are each transformed into diffusion equation for vorticity and poisson equation for stream function. A numerical solution method is designed using a simplified Lattice Boltzmann method (LBM). The LBM-D2Q5 scheme is utilized to obtain the numerical solutions for the fluid and magnetic field variables. Understanding the hydrodynamic behavior of systems employed in vortex-based memory systems is crucial for reliability and performance optimization. Based on this motivation, the effect of applied edge-current on the hydrodynamic and magnetic vortex configurations are analyzed through numerical simulations. The impact of the boundary magnetization is also conducted, by varying the strength of the magnetic field at the bottom boundary. The obtained graphical results provide some insights into the design and operation of vortex-based memory systems for next-generation data storage applications.","sentences":["In this study, a magnetohydrodynamic model is developed to study the dynamics of vortices driven by edge-current.","Two modeled equations for fluid and magnetic field variables are each transformed into diffusion equation for vorticity and poisson equation for stream function.","A numerical solution method is designed using a simplified Lattice Boltzmann method (LBM).","The LBM-D2Q5 scheme is utilized to obtain the numerical solutions for the fluid and magnetic field variables.","Understanding the hydrodynamic behavior of systems employed in vortex-based memory systems is crucial for reliability and performance optimization.","Based on this motivation, the effect of applied edge-current on the hydrodynamic and magnetic vortex configurations are analyzed through numerical simulations.","The impact of the boundary magnetization is also conducted, by varying the strength of the magnetic field at the bottom boundary.","The obtained graphical results provide some insights into the design and operation of vortex-based memory systems for next-generation data storage applications."],"url":"http://arxiv.org/abs/2405.03036v1","category":"physics.flu-dyn"}
{"created":"2024-05-05 18:47:53","title":"Optimal Box Contraction for Solving Linear Systems via Simulated and Quantum Annealing","abstract":"Solving linear systems of equations is an important problem in science and engineering. Many quantum algorithms, such as the Harrow-Hassidim-Lloyd (HHL) algorithm (for quantum-gate computers) and the box algorithm (for quantum-annealing machines), have been proposed for solving such systems.   The focus of this paper is on improving the efficiency of the box algorithm. The basic principle behind this algorithm is to transform the linear system into a series of quadratic unconstrained binary optimization (QUBO) problems, which are then solved on annealing machines.   The computational efficiency of the box algorithm is entirely determined by the number of iterations, which, in turn, depends on the box contraction ratio, typically set to 0.5. Here, we show through theory that a contraction ratio of 0.5 is sub-optimal and that we can achieve a speed-up with a contraction ratio of 0.2. This is confirmed through numerical experiments where a speed-up between $20 \\%$ to $60 \\%$ is observed when the optimal contraction ratio is used.","sentences":["Solving linear systems of equations is an important problem in science and engineering.","Many quantum algorithms, such as the Harrow-Hassidim-Lloyd (HHL) algorithm (for quantum-gate computers) and the box algorithm (for quantum-annealing machines), have been proposed for solving such systems.   ","The focus of this paper is on improving the efficiency of the box algorithm.","The basic principle behind this algorithm is to transform the linear system into a series of quadratic unconstrained binary optimization (QUBO) problems, which are then solved on annealing machines.   ","The computational efficiency of the box algorithm is entirely determined by the number of iterations, which, in turn, depends on the box contraction ratio, typically set to 0.5.","Here, we show through theory that a contraction ratio of 0.5 is sub-optimal and that we can achieve a speed-up with a contraction ratio of 0.2.","This is confirmed through numerical experiments where a speed-up between $20 \\%$ to $60 \\%$ is observed when the optimal contraction ratio is used."],"url":"http://arxiv.org/abs/2405.03029v1","category":"cs.CE"}
{"created":"2024-05-05 18:44:08","title":"Understanding the effects of data encoding on quantum-classical convolutional neural networks","abstract":"Quantum machine learning was recently applied to various applications and leads to results that are comparable or, in certain instances, superior to classical methods, in particular when few training data is available. These results warrant a more in-depth examination of when and why improvements can be observed. A key component of quantum-enhanced methods is the data encoding strategy used to embed the classical data into quantum states. However, a clear consensus on the selection of a fitting encoding strategy given a specific use-case has not yet been reached. This work investigates how the data encoding impacts the performance of a quantum-classical convolutional neural network (QCCNN) on two medical imaging datasets. In the pursuit of understanding why one encoding method outperforms another, two directions are explored. Potential correlations between the performance of the quantum-classical architecture and various quantum metrics are first examined. Next, the Fourier series decomposition of the quantum circuits is analyzed, as variational quantum circuits generate Fourier-type sums. We find that while quantum metrics offer limited insights into this problem, the Fourier coefficients analysis appears to provide better clues to understand the effects of data encoding on QCCNNs.","sentences":["Quantum machine learning was recently applied to various applications and leads to results that are comparable or, in certain instances, superior to classical methods, in particular when few training data is available.","These results warrant a more in-depth examination of when and why improvements can be observed.","A key component of quantum-enhanced methods is the data encoding strategy used to embed the classical data into quantum states.","However, a clear consensus on the selection of a fitting encoding strategy given a specific use-case has not yet been reached.","This work investigates how the data encoding impacts the performance of a quantum-classical convolutional neural network (QCCNN) on two medical imaging datasets.","In the pursuit of understanding why one encoding method outperforms another, two directions are explored.","Potential correlations between the performance of the quantum-classical architecture and various quantum metrics are first examined.","Next, the Fourier series decomposition of the quantum circuits is analyzed, as variational quantum circuits generate Fourier-type sums.","We find that while quantum metrics offer limited insights into this problem, the Fourier coefficients analysis appears to provide better clues to understand the effects of data encoding on QCCNNs."],"url":"http://arxiv.org/abs/2405.03027v1","category":"quant-ph"}
{"created":"2024-05-05 18:16:57","title":"Effective volume growth of three-manifolds with positive scalar curvature","abstract":"In this note, we prove an effective linear volume growth for complete three-manifolds with non-negative Ricci curvature and uniformly positive scalar curvature. This recovers the results obtained by Munteanu-Wang. Our method builds upon recent work by Chodosh-Li-Stryker, which utilizes the technique of $\\mu$-bubbles and the almost-splitting theorem by Cheeger-Colding.","sentences":["In this note, we prove an effective linear volume growth for complete three-manifolds with non-negative Ricci curvature and uniformly positive scalar curvature.","This recovers the results obtained by Munteanu-Wang.","Our method builds upon recent work by Chodosh-Li-Stryker, which utilizes the technique of $\\mu$-bubbles and the almost-splitting theorem by Cheeger-Colding."],"url":"http://arxiv.org/abs/2405.03023v1","category":"math.DG"}
{"created":"2024-05-05 17:51:17","title":"Pathwise uniform convergence of a full discretization for a three-dimensional stochastic Allen-Cahn equation with multiplicative noise","abstract":"This paper analyzes a full discretization of a three-dimensional stochastic Allen-Cahn equation with multiplicative noise. The discretization uses the Euler scheme for temporal discretization and the finite element method for spatial discretization. By deriving a stability estimate of a discrete stochastic convolution and utilizing this stability estimate along with the discrete stochastic maximal $L^p$-regularity estimate, a pathwise uniform convergence rate with the general spatial $ L^q $-norms is derived.","sentences":["This paper analyzes a full discretization of a three-dimensional stochastic Allen-Cahn equation with multiplicative noise.","The discretization uses the Euler scheme for temporal discretization and the finite element method for spatial discretization.","By deriving a stability estimate of a discrete stochastic convolution and utilizing this stability estimate along with the discrete stochastic maximal $L^p$-regularity estimate, a pathwise uniform convergence rate with the general spatial $ L^q $-norms is derived."],"url":"http://arxiv.org/abs/2405.03016v1","category":"math.NA"}
{"created":"2024-05-05 17:34:38","title":"DVMSR: Distillated Vision Mamba for Efficient Super-Resolution","abstract":"Efficient Image Super-Resolution (SR) aims to accelerate SR network inference by minimizing computational complexity and network parameters while preserving performance. Existing state-of-the-art Efficient Image Super-Resolution methods are based on convolutional neural networks. Few attempts have been made with Mamba to harness its long-range modeling capability and efficient computational complexity, which have shown impressive performance on high-level vision tasks. In this paper, we propose DVMSR, a novel lightweight Image SR network that incorporates Vision Mamba and a distillation strategy. The network of DVMSR consists of three modules: feature extraction convolution, multiple stacked Residual State Space Blocks (RSSBs), and a reconstruction module. Specifically, the deep feature extraction module is composed of several residual state space blocks (RSSB), each of which has several Vision Mamba Moudles(ViMM) together with a residual connection. To achieve efficiency improvement while maintaining comparable performance, we employ a distillation strategy to the vision Mamba network for superior performance. Specifically, we leverage the rich representation knowledge of teacher network as additional supervision for the output of lightweight student networks. Extensive experiments have demonstrated that our proposed DVMSR can outperform state-of-the-art efficient SR methods in terms of model parameters while maintaining the performance of both PSNR and SSIM. The source code is available at https://github.com/nathan66666/DVMSR.git","sentences":["Efficient Image Super-Resolution (SR) aims to accelerate SR network inference by minimizing computational complexity and network parameters while preserving performance.","Existing state-of-the-art Efficient Image Super-Resolution methods are based on convolutional neural networks.","Few attempts have been made with Mamba to harness its long-range modeling capability and efficient computational complexity, which have shown impressive performance on high-level vision tasks.","In this paper, we propose DVMSR, a novel lightweight Image SR network that incorporates Vision Mamba and a distillation strategy.","The network of DVMSR consists of three modules: feature extraction convolution, multiple stacked Residual State Space Blocks (RSSBs), and a reconstruction module.","Specifically, the deep feature extraction module is composed of several residual state space blocks (RSSB), each of which has several Vision Mamba Moudles(ViMM) together with a residual connection.","To achieve efficiency improvement while maintaining comparable performance, we employ a distillation strategy to the vision Mamba network for superior performance.","Specifically, we leverage the rich representation knowledge of teacher network as additional supervision for the output of lightweight student networks.","Extensive experiments have demonstrated that our proposed DVMSR can outperform state-of-the-art efficient SR methods in terms of model parameters while maintaining the performance of both PSNR and SSIM.","The source code is available at https://github.com/nathan66666/DVMSR.git"],"url":"http://arxiv.org/abs/2405.03008v1","category":"eess.IV"}
{"created":"2024-05-05 16:02:19","title":"Over-the-Air Majority Vote Computation with Modulation on Conjugate-Reciprocal Zeros","abstract":"In this study, we propose a new approach to compute the majority vote (MV) function based on modulation on conjugate-reciprocal zeros (MOCZ) and introduce three different methods. The proposed methods rely on the fact that when a linear combination of polynomials is evaluated at one of the roots of a polynomial in the combination, that polynomial does contribute to the evaluation. To utilize this property, each transmitter maps the votes to the zeros of a Huffman polynomial, and the corresponding polynomial coefficients are transmitted. The receiver evaluates the polynomial constructed by the elements of the superposed sequence at conjugate-reciprocal zero pairs and detects the MV with a direct zero-testing (DiZeT) decoder. With differential and index-based encoders, we eliminate the need for power-delay information at the receiver while improving the computation error rate (CER) performance. The proposed methods do not use instantaneous channel state information at the transmitters and receiver. Thus, they provide robustness against phase and time synchronization errors. We theoretically analyze the CERs of the proposed methods. Finally, we demonstrate their efficacy in a distributed median computation scenario in a fading channel.","sentences":["In this study, we propose a new approach to compute the majority vote (MV) function based on modulation on conjugate-reciprocal zeros (MOCZ) and introduce three different methods.","The proposed methods rely on the fact that when a linear combination of polynomials is evaluated at one of the roots of a polynomial in the combination, that polynomial does contribute to the evaluation.","To utilize this property, each transmitter maps the votes to the zeros of a Huffman polynomial, and the corresponding polynomial coefficients are transmitted.","The receiver evaluates the polynomial constructed by the elements of the superposed sequence at conjugate-reciprocal zero pairs and detects the MV with a direct zero-testing (DiZeT) decoder.","With differential and index-based encoders, we eliminate the need for power-delay information at the receiver while improving the computation error rate (CER) performance.","The proposed methods do not use instantaneous channel state information at the transmitters and receiver.","Thus, they provide robustness against phase and time synchronization errors.","We theoretically analyze the CERs of the proposed methods.","Finally, we demonstrate their efficacy in a distributed median computation scenario in a fading channel."],"url":"http://arxiv.org/abs/2405.02981v1","category":"cs.IT"}
{"created":"2024-05-05 15:50:02","title":"SkelCap: Automated Generation of Descriptive Text from Skeleton Keypoint Sequences","abstract":"Numerous sign language datasets exist, yet they typically cover only a limited selection of the thousands of signs used globally. Moreover, creating diverse sign language datasets is an expensive and challenging task due to the costs associated with gathering a varied group of signers. Motivated by these challenges, we aimed to develop a solution that addresses these limitations. In this context, we focused on textually describing body movements from skeleton keypoint sequences, leading to the creation of a new dataset. We structured this dataset around AUTSL, a comprehensive isolated Turkish sign language dataset. We also developed a baseline model, SkelCap, which can generate textual descriptions of body movements. This model processes the skeleton keypoints data as a vector, applies a fully connected layer for embedding, and utilizes a transformer neural network for sequence-to-sequence modeling. We conducted extensive evaluations of our model, including signer-agnostic and sign-agnostic assessments. The model achieved promising results, with a ROUGE-L score of 0.98 and a BLEU-4 score of 0.94 in the signer-agnostic evaluation. The dataset we have prepared, namely the AUTSL-SkelCap, will be made publicly available soon.","sentences":["Numerous sign language datasets exist, yet they typically cover only a limited selection of the thousands of signs used globally.","Moreover, creating diverse sign language datasets is an expensive and challenging task due to the costs associated with gathering a varied group of signers.","Motivated by these challenges, we aimed to develop a solution that addresses these limitations.","In this context, we focused on textually describing body movements from skeleton keypoint sequences, leading to the creation of a new dataset.","We structured this dataset around AUTSL, a comprehensive isolated Turkish sign language dataset.","We also developed a baseline model, SkelCap, which can generate textual descriptions of body movements.","This model processes the skeleton keypoints data as a vector, applies a fully connected layer for embedding, and utilizes a transformer neural network for sequence-to-sequence modeling.","We conducted extensive evaluations of our model, including signer-agnostic and sign-agnostic assessments.","The model achieved promising results, with a ROUGE-L score of 0.98 and a BLEU-4 score of 0.94 in the signer-agnostic evaluation.","The dataset we have prepared, namely the AUTSL-SkelCap, will be made publicly available soon."],"url":"http://arxiv.org/abs/2405.02977v1","category":"cs.CV"}
{"created":"2024-05-05 15:30:47","title":"Potential automorphy of certain non self-dual 3-dimensional Galois representations","abstract":"In a series of papers, van Geemen and Top have defined a family of surfaces $S_z$ indexed by a nonzero integer parameter $z$, and a compatible family of 3-dimensional Galois representations over $\\Q(i)$ attached to each surface. In this note we use recent advancements in potential automorphy and automorphy lifting to show that these compatible families are potentially automophic for all values of $z$, and hence that their L-functions have analytic continuation and a functional equation.","sentences":["In a series of papers, van Geemen and Top have defined a family of surfaces $S_z$ indexed by a nonzero integer parameter $z$, and a compatible family of 3-dimensional Galois representations over $\\Q(i)$ attached to each surface.","In this note we use recent advancements in potential automorphy and automorphy lifting to show that these compatible families are potentially automophic for all values of $z$, and hence that their L-functions have analytic continuation and a functional equation."],"url":"http://arxiv.org/abs/2405.02970v1","category":"math.NT"}
{"created":"2024-05-05 14:39:43","title":"Accelerating Legacy Numerical Solvers by Non-intrusive Gradient-based Meta-solving","abstract":"Scientific computing is an essential tool for scientific discovery and engineering design, and its computational cost is always a main concern in practice. To accelerate scientific computing, it is a promising approach to use machine learning (especially meta-learning) techniques for selecting hyperparameters of traditional numerical methods. There have been numerous proposals to this direction, but many of them require automatic-differentiable numerical methods. However, in reality, many practical applications still depend on well-established but non-automatic-differentiable legacy codes, which prevents practitioners from applying the state-of-the-art research to their own problems. To resolve this problem, we propose a non-intrusive methodology with a novel gradient estimation technique to combine machine learning and legacy numerical codes without any modification. We theoretically and numerically show the advantage of the proposed method over other baselines and present applications of accelerating established non-automatic-differentiable numerical solvers implemented in PETSc, a widely used open-source numerical software library.","sentences":["Scientific computing is an essential tool for scientific discovery and engineering design, and its computational cost is always a main concern in practice.","To accelerate scientific computing, it is a promising approach to use machine learning (especially meta-learning) techniques for selecting hyperparameters of traditional numerical methods.","There have been numerous proposals to this direction, but many of them require automatic-differentiable numerical methods.","However, in reality, many practical applications still depend on well-established but non-automatic-differentiable legacy codes, which prevents practitioners from applying the state-of-the-art research to their own problems.","To resolve this problem, we propose a non-intrusive methodology with a novel gradient estimation technique to combine machine learning and legacy numerical codes without any modification.","We theoretically and numerically show the advantage of the proposed method over other baselines and present applications of accelerating established non-automatic-differentiable numerical solvers implemented in PETSc, a widely used open-source numerical software library."],"url":"http://arxiv.org/abs/2405.02952v1","category":"cs.LG"}
{"created":"2024-05-05 11:42:37","title":"A solution in terms of mock modular forms for the $q$-Painlev\u00e9 equation of the type $(A_2+A_1)^{(1)}$","abstract":"We present a solution of the $(A_2+A_1)^{(1)}$ $q$-Painlev\\'{e} equation in terms of the $\\mu$-function. The $\\mu$-function introduced by Zwegers is the most fundamental object in the study of mock theta functions. The results of this paper give us an expectation that the theory of mock modular forms and the $\\tau$-functions of discrete integrable systems are closely related.","sentences":["We present a solution of the $(A_2+A_1)^{(1)}$ $q$-Painlev\\'{e} equation in terms of the $\\mu$-function.","The $\\mu$-function introduced by Zwegers is the most fundamental object in the study of mock theta functions.","The results of this paper give us an expectation that the theory of mock modular forms and the $\\tau$-functions of discrete integrable systems are closely related."],"url":"http://arxiv.org/abs/2405.02902v1","category":"math.CA"}
{"created":"2024-05-05 11:27:52","title":"Dark Energy predictions from GREA: Background and linear perturbation theory","abstract":"General Relativistic Entropic Acceleration (GREA) theory provides a covariant formalism for out-of-equilibrium phenomena in GR, extending the Einstein equations with an entropic force that behaves like bulk viscosity with a negative effective pressure. In particular, the growth of entropy associated with the homogeneous causal horizon can explain the present acceleration of the Universe, without introducing a cosmological constant. The dynamics of the accelerated Universe is characterized by a single parameter $\\alpha$, the ratio of the causal horizon to the curvature scale, which provides a unique history of the Universe distinguishable from that of LCDM. In particular, we explain the coincidence problem and the Hubble tension by shifting the coasting point to higher redshifts. All background observables are correlated among themselves due to their common dependence on $\\alpha$. This scenario gives a specific evolution for the effective equation of state parameter, $w(a)$. Furthermore, we study the linear growth of matter perturbations in the context of a homogeneous expanding background driven by the entropy of the causal horizon. We find that the rate of growth of matter fluctuations in GREA slows down due to the accelerated expansion and alleviates the $\\sigma_8$ tension of LCDM. We compute the growth function of matter fluctuations, the redshift space distortions in the galaxy correlation function, as well as the redshift evolution of the BAO scale, and find that the ISW effect is significantly larger than in LCDM. It is interesting to note that many of the tensions and anomalies of the standard model of cosmology are alleviated by the inclusion of this transient period of acceleration of the Universe based on known fundamental physics. In the near future we will be able to constrain this theory with present data from deep galaxy surveys.","sentences":["General Relativistic Entropic Acceleration (GREA) theory provides a covariant formalism for out-of-equilibrium phenomena in GR, extending the Einstein equations with an entropic force that behaves like bulk viscosity with a negative effective pressure.","In particular, the growth of entropy associated with the homogeneous causal horizon can explain the present acceleration of the Universe, without introducing a cosmological constant.","The dynamics of the accelerated Universe is characterized by a single parameter $\\alpha$, the ratio of the causal horizon to the curvature scale, which provides a unique history of the Universe distinguishable from that of LCDM.","In particular, we explain the coincidence problem and the Hubble tension by shifting the coasting point to higher redshifts.","All background observables are correlated among themselves due to their common dependence on $\\alpha$. This scenario gives a specific evolution for the effective equation of state parameter, $w(a)$. Furthermore, we study the linear growth of matter perturbations in the context of a homogeneous expanding background driven by the entropy of the causal horizon.","We find that the rate of growth of matter fluctuations in GREA slows down due to the accelerated expansion and alleviates the $\\sigma_8$ tension of LCDM.","We compute the growth function of matter fluctuations, the redshift space distortions in the galaxy correlation function, as well as the redshift evolution of the BAO scale, and find that the ISW effect is significantly larger than in LCDM.","It is interesting to note that many of the tensions and anomalies of the standard model of cosmology are alleviated by the inclusion of this transient period of acceleration of the Universe based on known fundamental physics.","In the near future we will be able to constrain this theory with present data from deep galaxy surveys."],"url":"http://arxiv.org/abs/2405.02895v1","category":"astro-ph.CO"}
{"created":"2024-05-05 10:55:44","title":"The Griffiths phase and beyond: A large deviations study of the magnetic susceptibility of the two-dimensional bond-diluted Ising model","abstract":"The Griffiths phase in systems with quenched disorder occurs below the ordering transition of the pure system down to the ordering transition of the actual disordered system. While it does not exhibit long-range order, large fluctuations in the disorder degrees of freedom result in exponentially rare, long-range ordered states and hence the occurrence of broad distributions in response functions. Inside the Griffiths phase of the two-dimensional bond-diluted Ising model the distribution of the magnetic susceptibility is expected to have such a broad, exponential tail. A large-deviations Monte Carlo algorithm is used to sample this distribution and the exponential tail is extracted over a wide range of the support down to very small probabilities of the order of $10^{-300}$. We study the behavior of the susceptibility distribution across the full phase diagram, from the paramagnetic state through the Griffiths phase to the ferromagnetically ordered system and down to the zero-temperature point. We extract the rate function of large-deviation theory as well as its finite-size scaling behavior and we reveal interesting differences and similarities between the cases. A connection between the fraction of ferromagnetic bonds in a given disorder sample and the size of the magnetic susceptibility is demonstrated numerically.","sentences":["The Griffiths phase in systems with quenched disorder occurs below the ordering transition of the pure system down to the ordering transition of the actual disordered system.","While it does not exhibit long-range order, large fluctuations in the disorder degrees of freedom result in exponentially rare, long-range ordered states and hence the occurrence of broad distributions in response functions.","Inside the Griffiths phase of the two-dimensional bond-diluted Ising model the distribution of the magnetic susceptibility is expected to have such a broad, exponential tail.","A large-deviations Monte Carlo algorithm is used to sample this distribution and the exponential tail is extracted over a wide range of the support down to very small probabilities of the order of $10^{-300}$. We study the behavior of the susceptibility distribution across the full phase diagram, from the paramagnetic state through the Griffiths phase to the ferromagnetically ordered system and down to the zero-temperature point.","We extract the rate function of large-deviation theory as well as its finite-size scaling behavior and we reveal interesting differences and similarities between the cases.","A connection between the fraction of ferromagnetic bonds in a given disorder sample and the size of the magnetic susceptibility is demonstrated numerically."],"url":"http://arxiv.org/abs/2405.02889v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-05 10:53:09","title":"HEP ML Lab: An end-to-end framework for applying machine learning into phenomenology studies","abstract":"Recent years have seen the development and growth of machine learning in high energy physics. There will be more effort to continue exploring its full potential. To make it easier for researchers to apply existing algorithms and neural networks and to advance the reproducibility of the analysis, we develop the \\texttt{HEP ML Lab} (\\texttt{hml}), a Python-based, end-to-end framework for phenomenology studies. It covers the complete workflow from event generation to performance evaluation, and provides a consistent style of use for different approaches. We propose an observable naming convention to streamline the data extraction and conversion processes. In the \\texttt{Keras} style, we provide the traditional cut-and-count and boosted decision trees together with neural networks. We take the $W^+$ tagging as an example and evaluate all built-in approaches with the metrics of significance and background rejection. With its modular design, \\texttt{HEP ML Lab} is easy to extend and customize, and can be used as a tool for both beginners and experienced researchers.","sentences":["Recent years have seen the development and growth of machine learning in high energy physics.","There will be more effort to continue exploring its full potential.","To make it easier for researchers to apply existing algorithms and neural networks and to advance the reproducibility of the analysis, we develop the \\texttt{HEP ML Lab} (\\texttt{hml}), a Python-based, end-to-end framework for phenomenology studies.","It covers the complete workflow from event generation to performance evaluation, and provides a consistent style of use for different approaches.","We propose an observable naming convention to streamline the data extraction and conversion processes.","In the \\texttt{Keras} style, we provide the traditional cut-and-count and boosted decision trees together with neural networks.","We take the $W^+$ tagging as an example and evaluate all built-in approaches with the metrics of significance and background rejection.","With its modular design, \\texttt{HEP ML Lab} is easy to extend and customize, and can be used as a tool for both beginners and experienced researchers."],"url":"http://arxiv.org/abs/2405.02888v1","category":"hep-ph"}
{"created":"2024-05-05 10:25:18","title":"Cosmology with Higher-Derivative Gravities","abstract":"We introduce an ingenious approach to explore cosmological implications of higher-derivative gravity theories. The key novelty lies in the characterization of the additional massive spin-0 modes constructed from Hubble derivatives as an effective density, with the corresponding pressure uniquely determined by energy conservation, while terms with no Hubble derivatives directly alter Friedmann equations. This classification of the various high-derivative contributions to Friedmann equations develops insight about their cosmological impacts and is essential for understanding the universe's evolution across energy scales. Various examples of higher-derivative gravity theories illustrate the power of this method in efficiently solving Friedmann equations and exploring new phenomena. Using CMB and BAO data, we apply this method to assess the observational feasibility of wall-bouncing universes, as predicted by scenarios with, e.g., certain third order modifications to general relativity. These models also provide an inflationary phase without the need to introduce extra scalar fields.","sentences":["We introduce an ingenious approach to explore cosmological implications of higher-derivative gravity theories.","The key novelty lies in the characterization of the additional massive spin-0 modes constructed from Hubble derivatives as an effective density, with the corresponding pressure uniquely determined by energy conservation, while terms with no Hubble derivatives directly alter Friedmann equations.","This classification of the various high-derivative contributions to Friedmann equations develops insight about their cosmological impacts and is essential for understanding the universe's evolution across energy scales.","Various examples of higher-derivative gravity theories illustrate the power of this method in efficiently solving Friedmann equations and exploring new phenomena.","Using CMB and BAO data, we apply this method to assess the observational feasibility of wall-bouncing universes, as predicted by scenarios with, e.g., certain third order modifications to general relativity.","These models also provide an inflationary phase without the need to introduce extra scalar fields."],"url":"http://arxiv.org/abs/2405.02879v1","category":"gr-qc"}
{"created":"2024-05-05 10:20:15","title":"Asymptotic profiles of ground state solutions for Choquard equations with a general local perturbation","abstract":"In this paper, we study the asymptotic behavior of ground state solutions for the nonlinear Choquard equation with a general local perturbation $$ -\\Delta u+\\varepsilon u=(I_\\alpha \\ast |u|^{p})|u|^{p-2}u+ g(u), \\quad {\\rm in} \\ \\mathbb R^N,   \\eqno(P_\\varepsilon)   $$ where $N\\ge 3$ is an integer, $p=\\frac{N+\\alpha}{N}$, or $\\frac{N+\\alpha}{N-2}$, $I_\\alpha$ is the Riesz potential and $\\varepsilon>0$ is a parameter. Under some mild conditions on $g(u)$, we show that as $\\varepsilon\\to \\infty$, after {\\em a suitable rescaling} the ground state solutions of $(P_\\varepsilon)$ converge to a particular solution of some limit equations, and establish a sharp asymptotic characterisation of such a rescaling, which depend in a non-trivial way on the asymptotic behavior of the function $g(s)$ at infinity and the space dimension $N$. Based on this study, we also present some results on the existence and asymptotic behaviors of positive normalized solutions of $(P_\\varepsilon)$ with the normalization constraint $\\int_{\\mathbb R^N}|u|^2=a^2$. Particularly, we obtain the asymptotic behavior of positive normalized solutions of such a problem as $a\\to 0$ and $a\\to \\infty$.","sentences":["In this paper, we study the asymptotic behavior of ground state solutions for the nonlinear Choquard equation with a general local perturbation $$ -\\Delta u+\\varepsilon u=(I_\\alpha \\ast |u|^{p})|u|^{p-2}u+ g(u), \\quad {\\rm in} \\ \\mathbb R^N,   \\eqno(P_\\varepsilon)   $$ where $N\\ge 3$ is an integer, $p=\\frac{N+\\alpha}{N}$, or $\\frac{N+\\alpha}{N-2}$, $I_\\alpha$ is the Riesz potential and $\\varepsilon>0$ is a parameter.","Under some mild conditions on $g(u)$, we show that as $\\varepsilon\\to \\infty$, after {\\em a suitable rescaling} the ground state solutions of $(P_\\varepsilon)$ converge to a particular solution of some limit equations, and establish a sharp asymptotic characterisation of such a rescaling, which depend in a non-trivial way on the asymptotic behavior of the function $g(s)$ at infinity and the space dimension $N$. Based on this study, we also present some results on the existence and asymptotic behaviors of positive normalized solutions of $(P_\\varepsilon)$ with the normalization constraint $\\int_{\\mathbb R^N}|u|^2=a^2$. Particularly, we obtain the asymptotic behavior of positive normalized solutions of such a problem as $a\\to 0$ and $a\\to \\infty$."],"url":"http://arxiv.org/abs/2405.02877v1","category":"math.AP"}
{"created":"2024-05-05 10:03:59","title":"The weighted and shifted seven-step BDF method for parabolic equations","abstract":"Stability of the BDF methods of order up to five for parabolic equations can be established by the energy technique via Nevanlinna--Odeh multipliers. The nonexistence of Nevanlinna--Odeh multipliers makes the six-step BDF method special; however, the energy technique was recently extended by the authors in [Akrivis et al., SIAM J. Numer. Anal. \\textbf{59} (2021) 2449--2472] and covers all six stable BDF methods. The seven-step BDF method is unstable for parabolic equations, since it is not even zero-stable. In this work, we construct and analyze a stable linear combination of two non zero-stable schemes, the seven-step BDF method and its shifted counterpart, referred to as WSBDF7 method. The stability regions of the WSBDF$q, q\\leqslant 7$, with a weight $\\vartheta\\geqslant1$, increase as $\\vartheta$ increases, are larger than the stability regions of the classical BDF$q,$ corresponding to $\\vartheta=1$. We determine novel and suitable multipliers for the WSBDF7 method and establish stability for parabolic equations by the energy technique. The proposed approach is applicable for mean curvature flow, gradient flows, fractional equations and nonlinear equations.","sentences":["Stability of the BDF methods of order up to five for parabolic equations can be established by the energy technique via Nevanlinna--Odeh multipliers.","The nonexistence of Nevanlinna--Odeh multipliers makes the six-step BDF method special; however, the energy technique was recently extended by the authors in [Akrivis et al., SIAM J. Numer.","Anal.","\\textbf{59} (2021) 2449--2472]","and covers all six stable BDF methods.","The seven-step BDF method is unstable for parabolic equations, since it is not even zero-stable.","In this work, we construct and analyze a stable linear combination of two non zero-stable schemes, the seven-step BDF method and its shifted counterpart, referred to as WSBDF7 method.","The stability regions of the WSBDF$q, q\\leqslant 7$, with a weight $\\vartheta\\geqslant1$, increase as $\\vartheta$ increases, are larger than the stability regions of the classical BDF$q,$ corresponding to $\\vartheta=1$. We determine novel and suitable multipliers for the WSBDF7 method and establish stability for parabolic equations by the energy technique.","The proposed approach is applicable for mean curvature flow, gradient flows, fractional equations and nonlinear equations."],"url":"http://arxiv.org/abs/2405.02872v1","category":"math.NA"}
{"created":"2024-05-05 09:54:44","title":"Tetrahedron duality","abstract":"A certain two-dimensional supersymmetric gauge theory is argued to satisfy a relation that promotes the Zamolodchikov tetrahedron equation to an infrared duality between two quantum field theories. Solutions of the tetrahedron equation with continuous spin variables are obtained from partition functions of this theory and its variants.","sentences":["A certain two-dimensional supersymmetric gauge theory is argued to satisfy a relation that promotes the Zamolodchikov tetrahedron equation to an infrared duality between two quantum field theories.","Solutions of the tetrahedron equation with continuous spin variables are obtained from partition functions of this theory and its variants."],"url":"http://arxiv.org/abs/2405.02870v1","category":"hep-th"}
{"created":"2024-05-05 09:25:14","title":"Stellar X-ray activity and habitability revealed by ROSAT sky survey","abstract":"Using the homogeneous X-ray catalog from ROSAT observations, we conducted a comprehensive investigation into stellar X-ray activity-rotation relations for both single and binary stars. Generally, the relation for single stars consists of two distinct regions: a weak decay region, indicating a continued dependence of the magnetic dynamo on stellar rotation rather than a saturation regime with constant activity, and a rapid decay region, where X-ray activity is strongly correlated with the Rossby number. Detailed analysis reveals more fine structures within the relation: in the extremely fast rotating regime, a decrease in X-ray activity was observed with increasing rotation rate, referred to as super-saturation, while in the extremely slow rotating region, the relation flattens, mainly due to the scattering of F stars. This scattering may result from intrinsic variability in stellar activities over one stellar cycle or the presence of different dynamo mechanisms. Binaries exhibit a similar relation to that of single stars while the limited sample size prevented the identification of fine structures in the relation for binaries. We calculated the mass loss rates of planetary atmosphere triggered by X-ray emissions from host stars. Our findings indicate that for an Earth-like planet within the stellar habitable zone, it would easily lose its entire primordial H/He envelope (equating to about 1% of the planetary mass).","sentences":["Using the homogeneous X-ray catalog from ROSAT observations, we conducted a comprehensive investigation into stellar X-ray activity-rotation relations for both single and binary stars.","Generally, the relation for single stars consists of two distinct regions: a weak decay region, indicating a continued dependence of the magnetic dynamo on stellar rotation rather than a saturation regime with constant activity, and a rapid decay region, where X-ray activity is strongly correlated with the Rossby number.","Detailed analysis reveals more fine structures within the relation: in the extremely fast rotating regime, a decrease in X-ray activity was observed with increasing rotation rate, referred to as super-saturation, while in the extremely slow rotating region, the relation flattens, mainly due to the scattering of F stars.","This scattering may result from intrinsic variability in stellar activities over one stellar cycle or the presence of different dynamo mechanisms.","Binaries exhibit a similar relation to that of single stars while the limited sample size prevented the identification of fine structures in the relation for binaries.","We calculated the mass loss rates of planetary atmosphere triggered by X-ray emissions from host stars.","Our findings indicate that for an Earth-like planet within the stellar habitable zone, it would easily lose its entire primordial H/","He envelope (equating to about 1% of the planetary mass)."],"url":"http://arxiv.org/abs/2405.02863v1","category":"astro-ph.SR"}
{"created":"2024-05-05 08:58:41","title":"On gamma functions with respect to the alternating Hurwitz zeta functions","abstract":"In 1730, Euler defined the Gamma function $\\Gamma(x)$ by the integral representation. It possesses many interesting properties and has wide applications in various branches of mathematics and sciences. According to Lerch, the Gamma function $\\Gamma(x)$ can also be defined by the derivative of the Hurwitz zeta function $$\\zeta(z,x)=\\sum_{n=0}^{\\infty}\\frac{1}{(n+x)^{z}}$$ at $z=0$. Recently, Hu and Kim defined the corresponding Stieltjes constants $\\widetilde{\\gamma}_{k}(x)$ and Euler constant $\\widetilde{\\gamma}_{0}$ from the Taylor series of the alternating Hurwitz zeta function $\\zeta_{E}(z,x)$ $$\\zeta_{E}(z,x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{(n+x)^z}.$$ And they also introduced the corresponding Gamma function $\\widetilde{\\Gamma}(x)$ which has the following Weierstrass--Hadamard type product $$\\widetilde{\\Gamma}(x)=\\frac{1}{x}e^{\\widetilde{\\gamma}_{0}x}\\prod_{k=1}^{\\infty}\\left(e^{-\\frac{x}{k}}\\left(1+\\frac{x}{k}\\right)\\right)^{(-1)^{k+1}}.$$   In this paper, we shall further investigate the function $\\widetilde{\\Gamma}(x)$, that is, we obtain several properties in analogy to the classical Gamma function $\\Gamma(x)$, including the integral representation, the limit representation, the recursive formula, the special values, the log-convexity, the duplication formula and the reflection equation. Furthermore, we also prove a Lerch-type formula, which shows that the derivative of $\\zeta_{E}(z,x)$ can be representative by $\\widetilde\\Gamma(x)$. As an application to Stark's conjecture in algebraic number theory, we will explicit calculate the derivatives of the partial zeta functions for the maximal real subfield of cyclotomic fields at $z=0$.","sentences":["In 1730, Euler defined the Gamma function $\\Gamma(x)$ by the integral representation.","It possesses many interesting properties and has wide applications in various branches of mathematics and sciences.","According to Lerch, the Gamma function $\\Gamma(x)$ can also be defined by the derivative of the Hurwitz zeta function $$\\zeta(z,x)=\\sum_{n=0}^{\\infty}\\frac{1}{(n+x)^{z}}$$ at $z=0$. Recently, Hu and Kim defined the corresponding Stieltjes constants $\\widetilde{\\gamma}_{k}(x)$ and Euler constant $\\widetilde{\\gamma}_{0}$ from the Taylor series of the alternating Hurwitz zeta function $\\zeta_{E}(z,x)$ $$\\zeta_{E}(z,x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{(n+x)^z}.$$","And they also introduced the corresponding Gamma function $\\widetilde{\\Gamma}(x)$ which has the following Weierstrass--Hadamard type product $$\\widetilde{\\Gamma}(x)=\\frac{1}{x}e^{\\widetilde{\\gamma}_{0}x}\\prod_{k=1}^{\\infty}\\left(e^{-\\frac{x}{k}}\\left(1+\\frac{x}{k}\\right)\\right)^{(-1)^{k+1}}.$$   In this paper, we shall further investigate the function $\\widetilde{\\Gamma}(x)$, that is, we obtain several properties in analogy to the classical Gamma function $\\Gamma(x)$, including the integral representation, the limit representation, the recursive formula, the special values, the log-convexity, the duplication formula and the reflection equation.","Furthermore, we also prove a Lerch-type formula, which shows that the derivative of $\\zeta_{E}(z,x)$ can be representative by $\\widetilde\\Gamma(x)$. As an application to Stark's conjecture in algebraic number theory, we will explicit calculate the derivatives of the partial zeta functions for the maximal real subfield of cyclotomic fields at $z=0$."],"url":"http://arxiv.org/abs/2405.02854v1","category":"math.NT"}
{"created":"2024-05-05 08:56:54","title":"Development and validation of a short form of the medication literacy scale for Chinese College Students","abstract":"Medication literacy is integral to health literacy, pivotal for medication safety and adherence. It denotes an individual's capacity to discern, comprehend, and convey medication-related information. Existing scales, however, are time-consuming and predominantly cater to patients and community dwellers, necessitating a more succinct instrument. This study presents the development of a brief Medication Literacy Scale (MLS-14) utilizing classical test theory (CTT) and item response theory (IRT), targeting a college student demographic. The MLS-14's abbreviated version, a 6-item scale (MLS-SF), was distilled through CTT and IRT methodologies, engaging 2431 Chinese college students to scrutinize its psychometric properties. The MLS-SF demonstrated a Cronbach's {\\alpha} of 0.765, with three extracted factors via exploratory factor analysis, accounting for 66% of the cumulative variance. All items exhibited factor loadings above 0.5. The scale's three-factor structure was substantiated through confirmatory factor analysis with satisfactory fit indices (chi2/df=5.11, RMSEA=0.063, GFI=0.990, AGFI=0.966, NFI=0.984, IFI=0.987, CFI=0.987). IRT modeling confirmed reasonable discrimination and location parameters for all items, free of differential item functioning (DIF) by gender. Except for items 4 and 10, the remaining items were informative at medium theta levels, indicating their utility in assessing medication literacy efficiently. The developed 6-item Medication Literacy Short Form (MLS-SF) proves to be a reliable and valid instrument for the expedited evaluation of college students' medication literacy, offering a valuable addition to the arsenal of health literacy assessment tools.","sentences":["Medication literacy is integral to health literacy, pivotal for medication safety and adherence.","It denotes an individual's capacity to discern, comprehend, and convey medication-related information.","Existing scales, however, are time-consuming and predominantly cater to patients and community dwellers, necessitating a more succinct instrument.","This study presents the development of a brief Medication Literacy Scale (MLS-14) utilizing classical test theory (CTT) and item response theory (IRT), targeting a college student demographic.","The MLS-14's abbreviated version, a 6-item scale (MLS-SF), was distilled through CTT and IRT methodologies, engaging 2431 Chinese college students to scrutinize its psychometric properties.","The MLS-SF demonstrated a Cronbach's {\\alpha} of 0.765, with three extracted factors via exploratory factor analysis, accounting for 66% of the cumulative variance.","All items exhibited factor loadings above 0.5.","The scale's three-factor structure was substantiated through confirmatory factor analysis with satisfactory fit indices (chi2/df=5.11, RMSEA=0.063, GFI=0.990, AGFI=0.966, NFI=0.984, IFI=0.987, CFI=0.987).","IRT modeling confirmed reasonable discrimination and location parameters for all items, free of differential item functioning (DIF) by gender.","Except for items 4 and 10, the remaining items were informative at medium theta levels, indicating their utility in assessing medication literacy efficiently.","The developed 6-item Medication Literacy Short Form (MLS-SF) proves to be a reliable and valid instrument for the expedited evaluation of college students' medication literacy, offering a valuable addition to the arsenal of health literacy assessment tools."],"url":"http://arxiv.org/abs/2405.02853v1","category":"q-bio.OT"}
{"created":"2024-05-05 08:55:00","title":"On Enhancing Brain Tumor Segmentation Across Diverse Populations with Convolutional Neural Networks","abstract":"Brain tumor segmentation is a fundamental step in assessing a patient's cancer progression. However, manual segmentation demands significant expert time to identify tumors in 3D multimodal brain MRI scans accurately. This reliance on manual segmentation makes the process prone to intra- and inter-observer variability. This work proposes a brain tumor segmentation method as part of the BraTS-GoAT challenge. The task is to segment tumors in brain MRI scans automatically from various populations, such as adults, pediatrics, and underserved sub-Saharan Africa. We employ a recent CNN architecture for medical image segmentation, namely MedNeXt, as our baseline, and we implement extensive model ensembling and postprocessing for inference. Our experiments show that our method performs well on the unseen validation set with an average DSC of 85.54% and HD95 of 27.88. The code is available on https://github.com/BioMedIA-MBZUAI/BraTS2024_BioMedIAMBZ.","sentences":["Brain tumor segmentation is a fundamental step in assessing a patient's cancer progression.","However, manual segmentation demands significant expert time to identify tumors in 3D multimodal brain MRI scans accurately.","This reliance on manual segmentation makes the process prone to intra- and inter-observer variability.","This work proposes a brain tumor segmentation method as part of the BraTS-GoAT challenge.","The task is to segment tumors in brain MRI scans automatically from various populations, such as adults, pediatrics, and underserved sub-Saharan Africa.","We employ a recent CNN architecture for medical image segmentation, namely MedNeXt, as our baseline, and we implement extensive model ensembling and postprocessing for inference.","Our experiments show that our method performs well on the unseen validation set with an average DSC of 85.54% and HD95 of 27.88.","The code is available on https://github.com/BioMedIA-MBZUAI/BraTS2024_BioMedIAMBZ."],"url":"http://arxiv.org/abs/2405.02852v1","category":"eess.IV"}
{"created":"2024-05-05 08:19:04","title":"Residual-Conditioned Optimal Transport: Towards Structure-preserving Unpaired and Paired Image Restoration","abstract":"Deep learning-based image restoration methods have achieved promising performance. However, how to faithfully preserve the structure of the original image remains challenging. To address this challenge, we propose a novel Residual-Conditioned Optimal Transport (RCOT) approach, which models the image restoration as an optimal transport (OT) problem for both unpaired and paired settings, integrating the transport residual as a unique degradation-specific cue for both the transport cost and the transport map. Specifically, we first formalize a Fourier residual-guided OT objective by incorporating the degradation-specific information of the residual into the transport cost. Based on the dual form of the OT formulation, we design the transport map as a two-pass RCOT map that comprises a base model and a refinement process, in which the transport residual is computed by the base model in the first pass and then encoded as a degradation-specific embedding to condition the second-pass restoration. By duality, the RCOT problem is transformed into a minimax optimization problem, which can be solved by adversarially training neural networks. Extensive experiments on multiple restoration tasks show the effectiveness of our approach in terms of both distortion measures and perceptual quality. Particularly, RCOT restores images with more faithful structural details compared to state-of-the-art methods.","sentences":["Deep learning-based image restoration methods have achieved promising performance.","However, how to faithfully preserve the structure of the original image remains challenging.","To address this challenge, we propose a novel Residual-Conditioned Optimal Transport (RCOT) approach, which models the image restoration as an optimal transport (OT) problem for both unpaired and paired settings, integrating the transport residual as a unique degradation-specific cue for both the transport cost and the transport map.","Specifically, we first formalize a Fourier residual-guided OT objective by incorporating the degradation-specific information of the residual into the transport cost.","Based on the dual form of the OT formulation, we design the transport map as a two-pass RCOT map that comprises a base model and a refinement process, in which the transport residual is computed by the base model in the first pass and then encoded as a degradation-specific embedding to condition the second-pass restoration.","By duality, the RCOT problem is transformed into a minimax optimization problem, which can be solved by adversarially training neural networks.","Extensive experiments on multiple restoration tasks show the effectiveness of our approach in terms of both distortion measures and perceptual quality.","Particularly, RCOT restores images with more faithful structural details compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.02843v1","category":"cs.CV"}
{"created":"2024-05-05 07:39:15","title":"RKHS, Odzijewicz, Berezin and Fedosov-type quantizations on smooth compact manifolds","abstract":"In this article we define Odzijewicz, Berezin and Fedosov-type quantization on compact smooth manifolds. The method is as follows. We embed the smooth manifold of real dimension $n$ into ${\\mathbb C}P^n$ (and in the Fedosov quantization case embed into any real $2n$ dimensional symplectic manifold). The pullback coherent states are defined in the usual way. In the Odzijewicz-type, Berezin-type quantization the Hilbert space of geometric quantization is the pullback by the embedding of the Hilbert space of geometric quantization of ${\\mathbb C}P^n$. In the Berezin case, the operators that are quantized are those induced from the ambient space. The Berezin-type quantization exhibited here is a generalization of an earlier work of the author and Kohinoor Ghosh (where we had needed totally real embedding). The Fedosov-type quantization is carried out by restriction to the submanifold given by the embedding.","sentences":["In this article we define Odzijewicz, Berezin and Fedosov-type quantization on compact smooth manifolds.","The method is as follows.","We embed the smooth manifold of real dimension $n$ into ${\\mathbb C}P^n$ (and in the Fedosov quantization case embed into any real $2n$ dimensional symplectic manifold).","The pullback coherent states are defined in the usual way.","In the Odzijewicz-type, Berezin-type quantization the Hilbert space of geometric quantization is the pullback by the embedding of the Hilbert space of geometric quantization of ${\\mathbb C}P^n$. In the Berezin case, the operators that are quantized are those induced from the ambient space.","The Berezin-type quantization exhibited here is a generalization of an earlier work of the author and Kohinoor Ghosh (where we had needed totally real embedding).","The Fedosov-type quantization is carried out by restriction to the submanifold given by the embedding."],"url":"http://arxiv.org/abs/2405.02838v1","category":"math-ph"}
{"created":"2024-05-05 06:57:40","title":"You Only Need Half: Boosting Data Augmentation by Using Partial Content","abstract":"We propose a novel data augmentation method termed You Only Need hAlf (YONA), which simplifies the augmentation process. YONA bisects an image, substitutes one half with noise, and applies data augmentation techniques to the remaining half. This method reduces the redundant information in the original image, encourages neural networks to recognize objects from incomplete views, and significantly enhances neural networks' robustness. YONA is distinguished by its properties of parameter-free, straightforward application, enhancing various existing data augmentation strategies, and thereby bolstering neural networks' robustness without additional computational cost. To demonstrate YONA's efficacy, extensive experiments were carried out. These experiments confirm YONA's compatibility with diverse data augmentation methods and neural network architectures, yielding substantial improvements in CIFAR classification tasks, sometimes outperforming conventional image-level data augmentation methods. Furthermore, YONA markedly increases the resilience of neural networks to adversarial attacks. Additional experiments exploring YONA's variants conclusively show that masking half of an image optimizes performance. The code is available at https://github.com/HansMoe/YONA.","sentences":["We propose a novel data augmentation method termed You Only Need hAlf (YONA), which simplifies the augmentation process.","YONA bisects an image, substitutes one half with noise, and applies data augmentation techniques to the remaining half.","This method reduces the redundant information in the original image, encourages neural networks to recognize objects from incomplete views, and significantly enhances neural networks' robustness.","YONA is distinguished by its properties of parameter-free, straightforward application, enhancing various existing data augmentation strategies, and thereby bolstering neural networks' robustness without additional computational cost.","To demonstrate YONA's efficacy, extensive experiments were carried out.","These experiments confirm YONA's compatibility with diverse data augmentation methods and neural network architectures, yielding substantial improvements in CIFAR classification tasks, sometimes outperforming conventional image-level data augmentation methods.","Furthermore, YONA markedly increases the resilience of neural networks to adversarial attacks.","Additional experiments exploring YONA's variants conclusively show that masking half of an image optimizes performance.","The code is available at https://github.com/HansMoe/YONA."],"url":"http://arxiv.org/abs/2405.02830v1","category":"cs.CV"}
{"created":"2024-05-05 05:42:33","title":"Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization","abstract":"This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets.","sentences":["This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work.","Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process.","Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG.","We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems.","By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."],"url":"http://arxiv.org/abs/2405.02816v1","category":"cs.CL"}
{"created":"2024-05-05 04:58:18","title":"Neural Network Enhanced Single-Photon Fock State Tomography","abstract":"Even though heralded single-photon sources have been generated routinely through the spontaneous parametric down conversion, vacuum and multiple photon states are unavoidably involved. With machine-learning, we report the experimental implementation of single-photon quantum state tomography by directly estimating target parameters. Compared to the Hanbury Brown and Twiss (HBT) measurements only with clicked events recorded, our neural network enhanced quantum state tomography characterizes the photon number distribution for all possible photon number states from the balanced homodyne detectors. By using the histogram-based architecture, a direct parameter estimation on the negativity in Wigner's quasi-probability phase space is demonstrated. Such a fast, robust, and precise quantum state tomography provides us a crucial diagnostic toolbox for the applications with single-photon Fock states and other non-Gaussisan quantum states.","sentences":["Even though heralded single-photon sources have been generated routinely through the spontaneous parametric down conversion, vacuum and multiple photon states are unavoidably involved.","With machine-learning, we report the experimental implementation of single-photon quantum state tomography by directly estimating target parameters.","Compared to the Hanbury Brown and Twiss (HBT) measurements only with clicked events recorded, our neural network enhanced quantum state tomography characterizes the photon number distribution for all possible photon number states from the balanced homodyne detectors.","By using the histogram-based architecture, a direct parameter estimation on the negativity in Wigner's quasi-probability phase space is demonstrated.","Such a fast, robust, and precise quantum state tomography provides us a crucial diagnostic toolbox for the applications with single-photon Fock states and other non-Gaussisan quantum states."],"url":"http://arxiv.org/abs/2405.02812v1","category":"quant-ph"}
{"created":"2024-05-05 02:29:41","title":"Graph as Point Set","abstract":"Graph is a fundamental data structure to model interconnections between entities. Set, on the contrary, stores independent elements. To learn graph representations, current Graph Neural Networks (GNNs) primarily use message passing to encode the interconnections. In contrast, this paper introduces a novel graph-to-set conversion method that bijectively transforms interconnected nodes into a set of independent points and then uses a set encoder to learn the graph representation. This conversion method holds dual significance. Firstly, it enables using set encoders to learn from graphs, thereby significantly expanding the design space of GNNs. Secondly, for Transformer, a specific set encoder, we provide a novel and principled approach to inject graph information losslessly, different from all the heuristic structural/positional encoding methods adopted in previous graph transformers. To demonstrate the effectiveness of our approach, we introduce Point Set Transformer (PST), a transformer architecture that accepts a point set converted from a graph as input. Theoretically, PST exhibits superior expressivity for both short-range substructure counting and long-range shortest path distance tasks compared to existing GNNs. Extensive experiments further validate PST's outstanding real-world performance. Besides Transformer, we also devise a Deepset-based set encoder, which achieves performance comparable to representative GNNs, affirming the versatility of our graph-to-set method.","sentences":["Graph is a fundamental data structure to model interconnections between entities.","Set, on the contrary, stores independent elements.","To learn graph representations, current Graph Neural Networks (GNNs) primarily use message passing to encode the interconnections.","In contrast, this paper introduces a novel graph-to-set conversion method that bijectively transforms interconnected nodes into a set of independent points and then uses a set encoder to learn the graph representation.","This conversion method holds dual significance.","Firstly, it enables using set encoders to learn from graphs, thereby significantly expanding the design space of GNNs.","Secondly, for Transformer, a specific set encoder, we provide a novel and principled approach to inject graph information losslessly, different from all the heuristic structural/positional encoding methods adopted in previous graph transformers.","To demonstrate the effectiveness of our approach, we introduce Point Set Transformer (PST), a transformer architecture that accepts a point set converted from a graph as input.","Theoretically, PST exhibits superior expressivity for both short-range substructure counting and long-range shortest path distance tasks compared to existing GNNs.","Extensive experiments further validate PST's outstanding real-world performance.","Besides Transformer, we also devise a Deepset-based set encoder, which achieves performance comparable to representative GNNs, affirming the versatility of our graph-to-set method."],"url":"http://arxiv.org/abs/2405.02795v1","category":"cs.LG"}
{"created":"2024-05-05 02:06:13","title":"A causal inference approach of monosynapses from spike trains","abstract":"Neuroscientists have worked on the problem of estimating synaptic properties, such as connectivity and strength, from simultaneously recorded spike trains since the 1960s. Recent years have seen renewed interest in the problem, coinciding with rapid advances in the technology of high-density neural recordings and optogenetics, which can be used to calibrate causal hypotheses about functional connectivity. Here, a rigorous causal inference framework for pairwise excitatory and inhibitory monosynaptic effects between spike trains is developed. Causal interactions are identified by separating spike interactions in pairwise spike trains by their timescales. Fast algorithms for computing accurate estimates of associated quantities are also developed. Through the lens of this framework, the link between biophysical parameters and statistical definitions of causality between spike trains is examined across a spectrum of dynamical systems simulations. In an idealized setting, we demonstrate a correspondence between the synaptic causal metric developed here and the probabilities of causation developed by Tian and Pearl. Since the probabilities of causation are derived under distinct assumptions and include data from experimental randomization, this opens up the possibility of testing the synaptic inference framework's assumptions with juxtacellular or optogenetic stimulation. We simulate such an experiment with a biophysically detailed channelrhodopsin model and show that randomization is not achieved; strong confounding persists even with strong stimulations. A principal goal is to ask how carefully articulated causal assumptions might better inform the design of neural stimulation experiments and, in turn, support experimental tests of those assumptions.","sentences":["Neuroscientists have worked on the problem of estimating synaptic properties, such as connectivity and strength, from simultaneously recorded spike trains since the 1960s.","Recent years have seen renewed interest in the problem, coinciding with rapid advances in the technology of high-density neural recordings and optogenetics, which can be used to calibrate causal hypotheses about functional connectivity.","Here, a rigorous causal inference framework for pairwise excitatory and inhibitory monosynaptic effects between spike trains is developed.","Causal interactions are identified by separating spike interactions in pairwise spike trains by their timescales.","Fast algorithms for computing accurate estimates of associated quantities are also developed.","Through the lens of this framework, the link between biophysical parameters and statistical definitions of causality between spike trains is examined across a spectrum of dynamical systems simulations.","In an idealized setting, we demonstrate a correspondence between the synaptic causal metric developed here and the probabilities of causation developed by Tian and Pearl.","Since the probabilities of causation are derived under distinct assumptions and include data from experimental randomization, this opens up the possibility of testing the synaptic inference framework's assumptions with juxtacellular or optogenetic stimulation.","We simulate such an experiment with a biophysically detailed channelrhodopsin model and show that randomization is not achieved; strong confounding persists even with strong stimulations.","A principal goal is to ask how carefully articulated causal assumptions might better inform the design of neural stimulation experiments and, in turn, support experimental tests of those assumptions."],"url":"http://arxiv.org/abs/2405.02786v1","category":"q-bio.NC"}
{"created":"2024-05-05 01:54:21","title":"Linear Noise Approximation Assisted Bayesian Inference on Mechanistic Model of Partially Observed Stochastic Reaction Network","abstract":"To support mechanism online learning and facilitate digital twin development for biomanufacturing processes, this paper develops an efficient Bayesian inference approach for partially observed enzymatic stochastic reaction network (SRN), a fundamental building block of multi-scale bioprocess mechanistic model. To tackle the critical challenges brought by the nonlinear stochastic differential equations (SDEs)-based mechanistic model with partially observed state and having measurement error, an interpretable Bayesian updating linear noise approximation (LNA) metamodel, incorporating the structure information of the mechanistic model, is proposed to approximate the likelihood of observations. Then, an efficient posterior sampling approach is developed by utilizing the gradients of the derived likelihood to speed up the convergence of MCMC. The empirical study demonstrates that the proposed approach has a promising performance.","sentences":["To support mechanism online learning and facilitate digital twin development for biomanufacturing processes, this paper develops an efficient Bayesian inference approach for partially observed enzymatic stochastic reaction network (SRN), a fundamental building block of multi-scale bioprocess mechanistic model.","To tackle the critical challenges brought by the nonlinear stochastic differential equations (SDEs)-based mechanistic model with partially observed state and having measurement error, an interpretable Bayesian updating linear noise approximation (LNA) metamodel, incorporating the structure information of the mechanistic model, is proposed to approximate the likelihood of observations.","Then, an efficient posterior sampling approach is developed by utilizing the gradients of the derived likelihood to speed up the convergence of MCMC.","The empirical study demonstrates that the proposed approach has a promising performance."],"url":"http://arxiv.org/abs/2405.02783v1","category":"stat.ML"}
{"created":"2024-05-05 01:51:58","title":"A self-supervised text-vision framework for automated brain abnormality detection","abstract":"Artificial neural networks trained on large, expert-labelled datasets are considered state-of-the-art for a range of medical image recognition tasks. However, categorically labelled datasets are time-consuming to generate and constrain classification to a pre-defined, fixed set of classes. For neuroradiological applications in particular, this represents a barrier to clinical adoption. To address these challenges, we present a self-supervised text-vision framework that learns to detect clinically relevant abnormalities in brain MRI scans by directly leveraging the rich information contained in accompanying free-text neuroradiology reports. Our training approach consisted of two-steps. First, a dedicated neuroradiological language model - NeuroBERT - was trained to generate fixed-dimensional vector representations of neuroradiology reports (N = 50,523) via domain-specific self-supervised learning tasks. Next, convolutional neural networks (one per MRI sequence) learnt to map individual brain scans to their corresponding text vector representations by optimising a mean square error loss. Once trained, our text-vision framework can be used to detect abnormalities in unreported brain MRI examinations by scoring scans against suitable query sentences (e.g., 'there is an acute stroke', 'there is hydrocephalus' etc.), enabling a range of classification-based applications including automated triage. Potentially, our framework could also serve as a clinical decision support tool, not only by suggesting findings to radiologists and detecting errors in provisional reports, but also by retrieving and displaying examples of pathologies from historical examinations that could be relevant to the current case based on textual descriptors.","sentences":["Artificial neural networks trained on large, expert-labelled datasets are considered state-of-the-art for a range of medical image recognition tasks.","However, categorically labelled datasets are time-consuming to generate and constrain classification to a pre-defined, fixed set of classes.","For neuroradiological applications in particular, this represents a barrier to clinical adoption.","To address these challenges, we present a self-supervised text-vision framework that learns to detect clinically relevant abnormalities in brain MRI scans by directly leveraging the rich information contained in accompanying free-text neuroradiology reports.","Our training approach consisted of two-steps.","First, a dedicated neuroradiological language model - NeuroBERT - was trained to generate fixed-dimensional vector representations of neuroradiology reports (N = 50,523) via domain-specific self-supervised learning tasks.","Next, convolutional neural networks (one per MRI sequence) learnt to map individual brain scans to their corresponding text vector representations by optimising a mean square error loss.","Once trained, our text-vision framework can be used to detect abnormalities in unreported brain MRI examinations by scoring scans against suitable query sentences (e.g., 'there is an acute stroke', 'there is hydrocephalus' etc.), enabling a range of classification-based applications including automated triage.","Potentially, our framework could also serve as a clinical decision support tool, not only by suggesting findings to radiologists and detecting errors in provisional reports, but also by retrieving and displaying examples of pathologies from historical examinations that could be relevant to the current case based on textual descriptors."],"url":"http://arxiv.org/abs/2405.02782v1","category":"cs.CV"}
{"created":"2024-05-05 00:43:21","title":"A hydrodynamic model of capillary flow in an axially symmetric tube with a non-slowly-varying cross section and a boundary slip","abstract":"The capillary flow of a Newtonian and incompressible fluid in an axially symmetric horizontal tube with a non-slowly-varying cross section and a boundary slip is considered theoretically under the assumption that the Reynolds number is small enough for the Stokes approximation to be valid. Combining the Stokes equation with the hydrodynamic model assuming the Hagen-Poiseulle flow, a general formula for the capillary flow in a non-slowly-varying tube is derived. Using the newly derived formula, the capillary imbibition and the time evolution of meniscus in tubes with non-uniform cross sections such as a conical tube, a power-law-shaped diverging tube, and a power-law-shaped converging tube are reconsidered. The perturbation parameters and the corrections due to the non-slowly-varying effects are elucidated and the new scaling formulas for the time evolution of the meniscus of these specific examples are derived. Our study could be useful for understanding various natural fluidic systems and for designing functional fluidic devices such as a diode and a switch.","sentences":["The capillary flow of a Newtonian and incompressible fluid in an axially symmetric horizontal tube with a non-slowly-varying cross section and a boundary slip is considered theoretically under the assumption that the Reynolds number is small enough for the Stokes approximation to be valid.","Combining the Stokes equation with the hydrodynamic model assuming the Hagen-Poiseulle flow, a general formula for the capillary flow in a non-slowly-varying tube is derived.","Using the newly derived formula, the capillary imbibition and the time evolution of meniscus in tubes with non-uniform cross sections such as a conical tube, a power-law-shaped diverging tube, and a power-law-shaped converging tube are reconsidered.","The perturbation parameters and the corrections due to the non-slowly-varying effects are elucidated and the new scaling formulas for the time evolution of the meniscus of these specific examples are derived.","Our study could be useful for understanding various natural fluidic systems and for designing functional fluidic devices such as a diode and a switch."],"url":"http://arxiv.org/abs/2405.02780v1","category":"physics.flu-dyn"}
{"created":"2024-05-04 21:55:33","title":"TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes","abstract":"In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms.","sentences":["In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception.","Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions.","We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors.","The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images.","Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos.","We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms."],"url":"http://arxiv.org/abs/2405.02762v1","category":"cs.CV"}
{"created":"2024-05-04 21:52:52","title":"Mid-latitude interactions expand the Hadley circulation","abstract":"The Hadley circulation describes a planetary-scale tropical atmospheric flow, which has a major influence on climate. Contemporary theoretical understanding is based upon angular momentum conservation, the basic dynamical constraint governing the state of the flow pattern. However, despite the degree of success in representing the Hadley circulation, the canonical theoretical model does not treat interactions with other regions, particularly the mid-latitudes. Here, we extend the original model of Held and Hou (1980) to include the influence of mid-latitude large-scale atmospheric dynamics, which we treat using the planetary-scale heat equation with a parameterized poleward heat flux driven by synoptic eddies. The energy flux balance within the Hadley cell includes the poleward heat flux at the poleward edge of the cell, which is controlled by the baroclinic instability of the sub-tropical jet. We find that an increase (decrease) in the poleward heat flux leads to a strengthening (weakening) of tropical convection, driving an equatorward (poleward) shift of the edge of the Hadley cell. Thus, our theoretical solutions suggest that global warming, which can reduce the baroclinicity of the subtropical jet, can lead to the poleward expansion of the Hadley cell due to the change in energy flux balance within it.","sentences":["The Hadley circulation describes a planetary-scale tropical atmospheric flow, which has a major influence on climate.","Contemporary theoretical understanding is based upon angular momentum conservation, the basic dynamical constraint governing the state of the flow pattern.","However, despite the degree of success in representing the Hadley circulation, the canonical theoretical model does not treat interactions with other regions, particularly the mid-latitudes.","Here, we extend the original model of Held and Hou (1980) to include the influence of mid-latitude large-scale atmospheric dynamics, which we treat using the planetary-scale heat equation with a parameterized poleward heat flux driven by synoptic eddies.","The energy flux balance within the Hadley cell includes the poleward heat flux at the poleward edge of the cell, which is controlled by the baroclinic instability of the sub-tropical jet.","We find that an increase (decrease) in the poleward heat flux leads to a strengthening (weakening) of tropical convection, driving an equatorward (poleward) shift of the edge of the Hadley cell.","Thus, our theoretical solutions suggest that global warming, which can reduce the baroclinicity of the subtropical jet, can lead to the poleward expansion of the Hadley cell due to the change in energy flux balance within it."],"url":"http://arxiv.org/abs/2405.02761v1","category":"physics.ao-ph"}
{"created":"2024-05-04 20:32:02","title":"A Bayesian mixture model approach to quantifying the empirical nuclear saturation point","abstract":"The equation of state (EOS) in the limit of infinite symmetric nuclear matter exhibits an equilibrium density, $n_0 \\approx 0.16 \\, \\mathrm{fm}^{-3}$, at which the pressure vanishes and the energy per particle attains its minimum, $E_0 \\approx -16 \\, \\mathrm{MeV}$. Although not directly measurable, the nuclear saturation point $(n_0,E_0)$ can be extrapolated by density functional theory (DFT), providing tight constraints for microscopic interactions derived from chiral effective field theory (EFT). However, when considering several DFT predictions for $(n_0,E_0)$ together, a discrepancy between model classes emerges at high confidence levels that each model prediction's uncertainty cannot explain. How can we leverage these DFT constraints to rigorously benchmark nuclear saturation properties of chiral interactions? To address this question, we present a Bayesian mixture model that combines multiple DFT predictions for $(n_0,E_0)$ using an efficient conjugate prior approach. The inferred posterior distribution for the saturation point's mean and covariance matrix follows a Normal-inverse-Wishart class, resulting in posterior predictives in the form of correlated, bivariate $t$-distributions. The DFT uncertainty reports are then used to mix these posteriors using an ordinary Monte Carlo approach. At the 95% credibility level, we estimate $n_0 \\approx 0.157 \\pm 0.010 \\, \\mathrm{fm}^{-3}$ and $E_0 \\approx -15.97 \\pm 0.40 \\, \\mathrm{MeV}$ for the marginal (univariate) $t$-distributions. Combined with chiral EFT calculations of the pure neutron matter EOS, we obtain bivariate normal distributions for the nuclear symmetry energy and its slope parameter evaluated at $n_0$: $S_v \\approx 32.0 \\pm 1.1 \\, \\mathrm{MeV}$ and $L\\approx 52.6\\pm 8.1 \\, \\mathrm{MeV}$ (95%), respectively. Our Bayesian framework is publicly available, so practitioners can readily use and extend our results.","sentences":["The equation of state (EOS) in the limit of infinite symmetric nuclear matter exhibits an equilibrium density, $n_0 \\approx 0.16 \\, \\mathrm{fm}^{-3}$, at which the pressure vanishes and the energy per particle attains its minimum, $E_0 \\approx -16 \\, \\mathrm{MeV}$. Although not directly measurable, the nuclear saturation point $(n_0,E_0)$ can be extrapolated by density functional theory (DFT), providing tight constraints for microscopic interactions derived from chiral effective field theory (EFT).","However, when considering several DFT predictions for $(n_0,E_0)$ together, a discrepancy between model classes emerges at high confidence levels that each model prediction's uncertainty cannot explain.","How can we leverage these DFT constraints to rigorously benchmark nuclear saturation properties of chiral interactions?","To address this question, we present a Bayesian mixture model that combines multiple DFT predictions for $(n_0,E_0)$ using an efficient conjugate prior approach.","The inferred posterior distribution for the saturation point's mean and covariance matrix follows a Normal-inverse-Wishart class, resulting in posterior predictives in the form of correlated, bivariate $t$-distributions.","The DFT uncertainty reports are then used to mix these posteriors using an ordinary Monte Carlo approach.","At the 95% credibility level, we estimate $n_0 \\approx 0.157 \\pm 0.010 \\, \\mathrm{fm}^{-3}$ and $E_0 \\approx -15.97","\\pm 0.40 \\, \\mathrm{MeV}$ for the marginal (univariate) $t$-distributions.","Combined with chiral EFT calculations of the pure neutron matter EOS, we obtain bivariate normal distributions for the nuclear symmetry energy and its slope parameter evaluated at $n_0$: $S_v \\approx 32.0 \\pm 1.1 \\, \\mathrm{MeV}$ and $L\\approx 52.6\\pm 8.1 \\, \\mathrm{MeV}$ (95%), respectively.","Our Bayesian framework is publicly available, so practitioners can readily use and extend our results."],"url":"http://arxiv.org/abs/2405.02748v1","category":"nucl-th"}
{"created":"2024-05-04 17:40:27","title":"Hypersurfaces with capillary boundary evolving by volume preserving power mean curvature flow","abstract":"In this paper, we introduce a volume- or area-preserving curvature flow for hypersurfaces with capillary boundary in the half-space, with speed given by a positive power of the mean curvature with a non-local averaging term. We demonstrate that for any convex initial hypersurface with a capillary boundary, the flow exists for all time and smoothly converges to a spherical cap as $t \\to +\\infty$","sentences":["In this paper, we introduce a volume- or area-preserving curvature flow for hypersurfaces with capillary boundary in the half-space, with speed given by a positive power of the mean curvature with a non-local averaging term.","We demonstrate that for any convex initial hypersurface with a capillary boundary, the flow exists for all time and smoothly converges to a spherical cap as $t \\to +\\infty$"],"url":"http://arxiv.org/abs/2405.02722v1","category":"math.DG"}
{"created":"2024-05-04 16:21:24","title":"Higher-order topology protected by latent crystalline symmetries","abstract":"We demonstrate that rotation symmetry is not a necessary requirement for the existence of fractional corner charges in Cn-symmetric higher-order topological crystalline insulators. Instead, it is sufficient to have a latent rotation symmetry, which may be revealed upon performing an isospectral reduction on the system. We introduce the concept of a filling anomaly for latent crystalline symmetric systems, and propose modified topological invariants. The notion of higher- order topology in two dimensions protected by Cn symmetry is thus generalized to a protection by latent symmetry. Our claims are corroborated by concrete examples of models that show non-trivial corner charge in the absence of Cn-symmetry. This work extends the classification of topological crystalline insulators to include latent symmetries.","sentences":["We demonstrate that rotation symmetry is not a necessary requirement for the existence of fractional corner charges in Cn-symmetric higher-order topological crystalline insulators.","Instead, it is sufficient to have a latent rotation symmetry, which may be revealed upon performing an isospectral reduction on the system.","We introduce the concept of a filling anomaly for latent crystalline symmetric systems, and propose modified topological invariants.","The notion of higher- order topology in two dimensions protected by Cn symmetry is thus generalized to a protection by latent symmetry.","Our claims are corroborated by concrete examples of models that show non-trivial corner charge in the absence of Cn-symmetry.","This work extends the classification of topological crystalline insulators to include latent symmetries."],"url":"http://arxiv.org/abs/2405.02704v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-04 15:04:06","title":"Diffeomorphic Transformer-based Abdomen MRI-CT Deformable Image Registration","abstract":"This paper aims to create a deep learning framework that can estimate the deformation vector field (DVF) for directly registering abdominal MRI-CT images. The proposed method assumed a diffeomorphic deformation. By using topology-preserved deformation features extracted from the probabilistic diffeomorphic registration model, abdominal motion can be accurately obtained and utilized for DVF estimation. The model integrated Swin transformers, which have demonstrated superior performance in motion tracking, into the convolutional neural network (CNN) for deformation feature extraction. The model was optimized using a cross-modality image similarity loss and a surface matching loss. To compute the image loss, a modality-independent neighborhood descriptor (MIND) was used between the deformed MRI and CT images. The surface matching loss was determined by measuring the distance between the warped coordinates of the surfaces of contoured structures on the MRI and CT images. The deformed MRI image was assessed against the CT image using the target registration error (TRE), Dice similarity coefficient (DSC), and mean surface distance (MSD) between the deformed contours of the MRI image and manual contours of the CT image. When compared to only rigid registration, DIR with the proposed method resulted in an increase of the mean DSC values of the liver and portal vein from 0.850 and 0.628 to 0.903 and 0.763, a decrease of the mean MSD of the liver from 7.216 mm to 3.232 mm, and a decrease of the TRE from 26.238 mm to 8.492 mm. The proposed deformable image registration method based on a diffeomorphic transformer provides an effective and efficient way to generate an accurate DVF from an MRI-CT image pair of the abdomen. It could be utilized in the current treatment planning workflow for liver radiotherapy.","sentences":["This paper aims to create a deep learning framework that can estimate the deformation vector field (DVF) for directly registering abdominal MRI-CT images.","The proposed method assumed a diffeomorphic deformation.","By using topology-preserved deformation features extracted from the probabilistic diffeomorphic registration model, abdominal motion can be accurately obtained and utilized for DVF estimation.","The model integrated Swin transformers, which have demonstrated superior performance in motion tracking, into the convolutional neural network (CNN) for deformation feature extraction.","The model was optimized using a cross-modality image similarity loss and a surface matching loss.","To compute the image loss, a modality-independent neighborhood descriptor (MIND) was used between the deformed MRI and CT images.","The surface matching loss was determined by measuring the distance between the warped coordinates of the surfaces of contoured structures on the MRI and CT images.","The deformed MRI image was assessed against the CT image using the target registration error (TRE), Dice similarity coefficient (DSC), and mean surface distance (MSD) between the deformed contours of the MRI image and manual contours of the CT image.","When compared to only rigid registration, DIR with the proposed method resulted in an increase of the mean DSC values of the liver and portal vein from 0.850 and 0.628 to 0.903 and 0.763, a decrease of the mean MSD of the liver from 7.216 mm to 3.232 mm, and a decrease of the TRE from 26.238 mm to 8.492 mm.","The proposed deformable image registration method based on a diffeomorphic transformer provides an effective and efficient way to generate an accurate DVF from an MRI-CT image pair of the abdomen.","It could be utilized in the current treatment planning workflow for liver radiotherapy."],"url":"http://arxiv.org/abs/2405.02692v1","category":"cs.CV"}
{"created":"2024-05-04 14:55:25","title":"On finding bifurcations for non-variational elliptic systems by the extended quotients method","abstract":"We develop a novel method for finding bifurcations for nonlinear systems of equations based on directly finding bifurcations through saddle points of extended quotients. The method is applied to find the saddle-node bifurcation point for elliptic equations with the nonlinearity of the general convex-concave type. The main result justifies the variational formula for the detection of the maximum saddle-node type bifurcation point of stable positive solutions. As a consequence, a precise threshold value separating the interval of the existence of stable positive solutions is established.","sentences":["We develop a novel method for finding bifurcations for nonlinear systems of equations based on directly finding bifurcations through saddle points of extended quotients.","The method is applied to find the saddle-node bifurcation point for elliptic equations with the nonlinearity of the general convex-concave type.","The main result justifies the variational formula for the detection of the maximum saddle-node type bifurcation point of stable positive solutions.","As a consequence, a precise threshold value separating the interval of the existence of stable positive solutions is established."],"url":"http://arxiv.org/abs/2405.02684v1","category":"math.AP"}
{"created":"2024-05-04 14:01:28","title":"Global models of collapsing scalar field: endstate","abstract":"The study of dynamic singularity formation in spacetime, focusing on scalar field collapse models, is analysed. We revisit key findings regarding open spatial topologies, concentrating on minimal conditions necessary for singularity and apparent horizon formation. Moreover, we examine the stability of initial data in the dynamical system governed by Einstein's equations, considering variations in parameters that influence naked singularity formation. We illustrate how these results apply to a family of scalar field models, concluding with a discussion on the concept of genericity in singularity studies.","sentences":["The study of dynamic singularity formation in spacetime, focusing on scalar field collapse models, is analysed.","We revisit key findings regarding open spatial topologies, concentrating on minimal conditions necessary for singularity and apparent horizon formation.","Moreover, we examine the stability of initial data in the dynamical system governed by Einstein's equations, considering variations in parameters that influence naked singularity formation.","We illustrate how these results apply to a family of scalar field models, concluding with a discussion on the concept of genericity in singularity studies."],"url":"http://arxiv.org/abs/2405.02671v1","category":"gr-qc"}
{"created":"2024-05-04 13:52:54","title":"The lower bound of first Dirichlet eigenvalue of p-Laplacian in Riemannian manifolds","abstract":"This paper investigates the first Dirichlet eigenvalue of bounded domains for the $p$-Laplacian in complete Riemannian manifolds. Firstly, we establish a lower bound for this eigenvalue under the condition that the domain includes a specific function which fulfills certain criteria related to divergence and gradient conditions. As an application, we show that $p(\\lambda_{1,p})^{1/p}$ is an increasing function about $p.$ We further explore Barta's inequality and other relevant applications stemming from this foundational result. In the subsequent section, we introduce an enhanced lower bound for the eigenvalue, which is linked to the distance function defined in the domain. As a practical application, we provide an estimation for the first Dirichlet eigenvalue of geodesic balls with large radius in asymptotically hyperbolic Einstein manifolds.","sentences":["This paper investigates the first Dirichlet eigenvalue of bounded domains for the $p$-Laplacian in complete Riemannian manifolds.","Firstly, we establish a lower bound for this eigenvalue under the condition that the domain includes a specific function which fulfills certain criteria related to divergence and gradient conditions.","As an application, we show that $p(\\lambda_{1,p})^{1/p}$ is an increasing function about $p.$ We further explore Barta's inequality and other relevant applications stemming from this foundational result.","In the subsequent section, we introduce an enhanced lower bound for the eigenvalue, which is linked to the distance function defined in the domain.","As a practical application, we provide an estimation for the first Dirichlet eigenvalue of geodesic balls with large radius in asymptotically hyperbolic Einstein manifolds."],"url":"http://arxiv.org/abs/2405.02669v1","category":"math.DG"}
{"created":"2024-05-04 13:29:11","title":"Metric Differential Privacy at the User-Level","abstract":"Metric differential privacy (DP) provides heterogeneous privacy guarantees based on a distance between the pair of inputs. It is a widely popular notion of privacy since it captures the natural privacy semantics for many applications (such as, for location data) and results in better utility than standard DP. However, prior work in metric DP has primarily focused on the \\textit{item-level} setting where every user only reports a single data item. A more realistic setting is that of user-level DP where each user contributes multiple items and privacy is then desired at the granularity of the user's \\textit{entire} contribution. In this paper, we initiate the study of metric DP at the user-level. Specifically, we use the earth-mover's distance ($d_\\textsf{EM}$) as our metric to obtain a notion of privacy as it captures both the magnitude and spatial aspects of changes in a user's data.   We make three main technical contributions. First, we design two novel mechanisms under $d_\\textsf{EM}$-DP to answer linear queries and item-wise queries. Specifically, our analysis for the latter involves a generalization of the privacy amplification by shuffling result which may be of independent interest. Second, we provide a black-box reduction from the general unbounded to bounded $d_\\textsf{EM}$-DP (size of the dataset is fixed and public) with a novel sampling based mechanism. Third, we show that our proposed mechanisms can provably provide improved utility over user-level DP, for certain types of linear queries and frequency estimation.","sentences":["Metric differential privacy (DP) provides heterogeneous privacy guarantees based on a distance between the pair of inputs.","It is a widely popular notion of privacy since it captures the natural privacy semantics for many applications (such as, for location data) and results in better utility than standard DP.","However, prior work in metric DP has primarily focused on the \\textit{item-level} setting where every user only reports a single data item.","A more realistic setting is that of user-level DP where each user contributes multiple items and privacy is then desired at the granularity of the user's \\textit{entire} contribution.","In this paper, we initiate the study of metric DP at the user-level.","Specifically, we use the earth-mover's distance ($d_\\textsf{EM}$) as our metric to obtain a notion of privacy as it captures both the magnitude and spatial aspects of changes in a user's data.   ","We make three main technical contributions.","First, we design two novel mechanisms under $d_\\textsf{EM}$-DP to answer linear queries and item-wise queries.","Specifically, our analysis for the latter involves a generalization of the privacy amplification by shuffling result which may be of independent interest.","Second, we provide a black-box reduction from the general unbounded to bounded $d_\\textsf{EM}$-DP (size of the dataset is fixed and public) with a novel sampling based mechanism.","Third, we show that our proposed mechanisms can provably provide improved utility over user-level DP, for certain types of linear queries and frequency estimation."],"url":"http://arxiv.org/abs/2405.02665v1","category":"cs.CR"}
{"created":"2024-05-04 13:14:29","title":"DDE-Find: Learning Delay Differential Equations from Data","abstract":"Delay Differential Equations (DDEs) are a class of differential equations that can model diverse scientific phenomena. However, identifying the parameters, especially the time delay, that make a DDE's predictions match experimental results can be challenging. We introduce DDE-Find, a data-driven framework for learning a DDE's parameters, time delay, and initial condition function. DDE-Find uses an adjoint-based approach to efficiently compute the gradient of a loss function with respect to the model parameters. We motivate and rigorously prove an expression for the gradients of the loss using the adjoint. DDE-Find builds upon recent developments in learning DDEs from data and delivers the first complete framework for learning DDEs from data. Through a series of numerical experiments, we demonstrate that DDE-Find can learn DDEs from noisy, limited data.","sentences":["Delay Differential Equations (DDEs) are a class of differential equations that can model diverse scientific phenomena.","However, identifying the parameters, especially the time delay, that make a DDE's predictions match experimental results can be challenging.","We introduce DDE-Find, a data-driven framework for learning a DDE's parameters, time delay, and initial condition function.","DDE-Find uses an adjoint-based approach to efficiently compute the gradient of a loss function with respect to the model parameters.","We motivate and rigorously prove an expression for the gradients of the loss using the adjoint.","DDE-Find builds upon recent developments in learning DDEs from data and delivers the first complete framework for learning DDEs from data.","Through a series of numerical experiments, we demonstrate that DDE-Find can learn DDEs from noisy, limited data."],"url":"http://arxiv.org/abs/2405.02661v1","category":"cs.LG"}
{"created":"2024-05-04 10:54:07","title":"Unsupervised machine learning for data-driven classification of rock mass using drilling data: How can a data-driven system handle limitations in existing rock mass classification systems?","abstract":"Rock mass classification systems are crucial for assessing stability and risk in underground construction globally and guiding support and excavation design. However, systems developed primarily in the 1970s lack access to modern high-resolution data and advanced statistical techniques, limiting their effectiveness as decision-support systems. Initially, we outline the limitations observed in this context and later describe how a data-driven system, based on drilling data as detailed in this study, can overcome these limitations. Using extracted statistical information from thousands of MWD-data values in one-meter sections of a full tunnel profile, thus working as a signature of the rock mass, we have demonstrated that it is possible to form well-defined clusters that can act as a foundational basis for various rock mass classification systems. We reduced the dimensionality of 48-value vectors using nonlinear manifold learning techniques (UMAP) and linear principal component analysis (PCA) to enhance clustering. Unsupervised machine learning methods (HDBSCAN, Agglomerative Clustering, K-means) were employed to cluster the data, with hyperparameters optimised through multi-objective Bayesian optimisation for effective clustering. Using domain knowledge, we experienced improved clustering and system tuning opportunities in adding extra features to core clusters of MWD-data. We structured and correlated these clusters with physical rock mass properties, including labels of rock type and rock quality, and analysed cumulative distributions of key MWD-parameters for rock mass assessment to determine if clusters meaningfully differentiate rock masses. The ability of MWD data to form distinct rock mass clusters suggests substantial potential for future classification systems grounded in this objective, data-driven methodology, free from human bias.","sentences":["Rock mass classification systems are crucial for assessing stability and risk in underground construction globally and guiding support and excavation design.","However, systems developed primarily in the 1970s lack access to modern high-resolution data and advanced statistical techniques, limiting their effectiveness as decision-support systems.","Initially, we outline the limitations observed in this context and later describe how a data-driven system, based on drilling data as detailed in this study, can overcome these limitations.","Using extracted statistical information from thousands of MWD-data values in one-meter sections of a full tunnel profile, thus working as a signature of the rock mass, we have demonstrated that it is possible to form well-defined clusters that can act as a foundational basis for various rock mass classification systems.","We reduced the dimensionality of 48-value vectors using nonlinear manifold learning techniques (UMAP) and linear principal component analysis (PCA) to enhance clustering.","Unsupervised machine learning methods (HDBSCAN, Agglomerative Clustering, K-means) were employed to cluster the data, with hyperparameters optimised through multi-objective Bayesian optimisation for effective clustering.","Using domain knowledge, we experienced improved clustering and system tuning opportunities in adding extra features to core clusters of MWD-data.","We structured and correlated these clusters with physical rock mass properties, including labels of rock type and rock quality, and analysed cumulative distributions of key MWD-parameters for rock mass assessment to determine if clusters meaningfully differentiate rock masses.","The ability of MWD data to form distinct rock mass clusters suggests substantial potential for future classification systems grounded in this objective, data-driven methodology, free from human bias."],"url":"http://arxiv.org/abs/2405.02631v1","category":"cs.LG"}
{"created":"2024-05-06 17:59:07","title":"Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames","abstract":"Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.","sentences":["Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams.","Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior.","In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation.","We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade.","We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions.","We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023.","For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election).","While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods."],"url":"http://arxiv.org/abs/2405.03688v1","category":"cs.CL"}
{"created":"2024-05-06 17:48:24","title":"Cutting through buggy adversarial example defenses: fixing 1 line of code breaks Sabre","abstract":"Sabre is a defense to adversarial examples that was accepted at IEEE S&P 2024. We first reveal significant flaws in the evaluation that point to clear signs of gradient masking. We then show the cause of this gradient masking: a bug in the original evaluation code. By fixing a single line of code in the original repository, we reduce Sabre's robust accuracy to 0%. In response to this, the authors modify the defense and introduce a new defense component not described in the original paper. But this fix contains a second bug; modifying one more line of code reduces robust accuracy to below baseline levels.","sentences":["Sabre is a defense to adversarial examples that was accepted at IEEE S&P 2024.","We first reveal significant flaws in the evaluation that point to clear signs of gradient masking.","We then show the cause of this gradient masking: a bug in the original evaluation code.","By fixing a single line of code in the original repository, we reduce Sabre's robust accuracy to 0%.","In response to this, the authors modify the defense and introduce a new defense component not described in the original paper.","But this fix contains a second bug; modifying one more line of code reduces robust accuracy to below baseline levels."],"url":"http://arxiv.org/abs/2405.03672v1","category":"cs.CR"}
{"created":"2024-05-06 16:41:52","title":"$\u03b5$-Policy Gradient for Online Pricing","abstract":"Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task. The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference. We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors. The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials.","sentences":["Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task.","The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference.","We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors.","The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials."],"url":"http://arxiv.org/abs/2405.03624v1","category":"cs.LG"}
{"created":"2024-05-06 16:32:01","title":"Nonnegative Matrix Factorization in Dimensionality Reduction: A Survey","abstract":"Dimensionality Reduction plays a pivotal role in improving feature learning accuracy and reducing training time by eliminating redundant features, noise, and irrelevant data. Nonnegative Matrix Factorization (NMF) has emerged as a popular and powerful method for dimensionality reduction. Despite its extensive use, there remains a need for a comprehensive analysis of NMF in the context of dimensionality reduction. To address this gap, this paper presents a comprehensive survey of NMF, focusing on its applications in both feature extraction and feature selection. We introduce a classification of dimensionality reduction, enhancing understanding of the underlying concepts. Subsequently, we delve into a thorough summary of diverse NMF approaches used for feature extraction and selection. Furthermore, we discuss the latest research trends and potential future directions of NMF in dimensionality reduction, aiming to highlight areas that need further exploration and development.","sentences":["Dimensionality Reduction plays a pivotal role in improving feature learning accuracy and reducing training time by eliminating redundant features, noise, and irrelevant data.","Nonnegative Matrix Factorization (NMF) has emerged as a popular and powerful method for dimensionality reduction.","Despite its extensive use, there remains a need for a comprehensive analysis of NMF in the context of dimensionality reduction.","To address this gap, this paper presents a comprehensive survey of NMF, focusing on its applications in both feature extraction and feature selection.","We introduce a classification of dimensionality reduction, enhancing understanding of the underlying concepts.","Subsequently, we delve into a thorough summary of diverse NMF approaches used for feature extraction and selection.","Furthermore, we discuss the latest research trends and potential future directions of NMF in dimensionality reduction, aiming to highlight areas that need further exploration and development."],"url":"http://arxiv.org/abs/2405.03615v1","category":"cs.LG"}
{"created":"2024-05-06 13:14:28","title":"Behavioral analysis in immersive learning environments: A systematic literature review and research agenda","abstract":"The rapid growth of immersive technologies in educational areas has increased research interest in analyzing the specific behavioral patterns of learners in immersive learning environments. Considering the fact that research on the technical affordances of immersive technologies and the pedagogical affordances of behavioral analysis remains fragmented, this study first contributes by developing a conceptual framework that amalgamates learning requirements, specification, evaluation, and iteration into an integrated model to identify learning benefits and potential hurdles of behavioral analysis in immersive learning environments. Then, a systematic review was conducted underpinning the proposed conceptual framework to retrieve valuable empirical evidence from the 40 eligible articles during the last decade. The review findings suggest that (1) there is an essential need to sufficiently prepare the salient pedagogical requirements to define the specific learning stage, envisage intended cognitive objectives, and specify an appropriate set of learning activities, when developing comprehensive plans on behavioral analysis in immersive learning environments. (2) Researchers could customize the unique immersive experimental implementation by considering factors from four dimensions: learner, pedagogy, context, and representation. (3) The behavioral patterns constructed in immersive learning environments vary by considering the influence of behavioral analysis techniques, research themes, and immersive technical features. (4) The use of behavioral analysis in immersive learning environments faces several challenges from technical, implementation, and data processing perspectives. This study also articulates critical research agenda that could drive future investigation on behavioral analysis in immersive learning environments.","sentences":["The rapid growth of immersive technologies in educational areas has increased research interest in analyzing the specific behavioral patterns of learners in immersive learning environments.","Considering the fact that research on the technical affordances of immersive technologies and the pedagogical affordances of behavioral analysis remains fragmented, this study first contributes by developing a conceptual framework that amalgamates learning requirements, specification, evaluation, and iteration into an integrated model to identify learning benefits and potential hurdles of behavioral analysis in immersive learning environments.","Then, a systematic review was conducted underpinning the proposed conceptual framework to retrieve valuable empirical evidence from the 40 eligible articles during the last decade.","The review findings suggest that (1) there is an essential need to sufficiently prepare the salient pedagogical requirements to define the specific learning stage, envisage intended cognitive objectives, and specify an appropriate set of learning activities, when developing comprehensive plans on behavioral analysis in immersive learning environments.","(2) Researchers could customize the unique immersive experimental implementation by considering factors from four dimensions: learner, pedagogy, context, and representation.","(3) The behavioral patterns constructed in immersive learning environments vary by considering the influence of behavioral analysis techniques, research themes, and immersive technical features.","(4) The use of behavioral analysis in immersive learning environments faces several challenges from technical, implementation, and data processing perspectives.","This study also articulates critical research agenda that could drive future investigation on behavioral analysis in immersive learning environments."],"url":"http://arxiv.org/abs/2405.03442v1","category":"cs.HC"}
{"created":"2024-05-06 10:51:09","title":"Molecular dynamics simulations of the defect evolution in tungsten on successive collision cascades","abstract":"Molecular dynamics (MD) simulations of successive collision cascades within the same simulation domain were performed using two different inter-atomic potentials (IAP) in tungsten, one EAM based and the other a `quantum accurate' machine learning potential, SNAP. The micro-structural changes are analyzed as a function of displacements per atom (dpa) up-to irradiation dose of 3 dpa. Five simulations are carried out at primary knock-on atom (PKA) energies of 20 keV and 50 keV for observing stochastic differences in the evolution of damage. Results for the variation of the number of defects, number of defect clusters, their size and morphology distribution are compared for the two energies and two IAPs, up-to irradiation doses of 1-3 dpa.   It is seen that for a given IAP, the number density of defect clusters as a function of dpa does not depend on the energy of the PKA used in the simulations implying that, like dpa, it could be a good measure of material property changes in irradiated materials. In contrast, the defect sizes depend strongly on the energy of the PKA. It is also seen that the micro-structure resulting from the EAM potential was mostly composed of <1 1 1> and <1 0 0> dislocations, while that from the SNAP potential was mostly composed of small, sessile, C15 ring like structures with a smattering of smaller <1 1 1> dislocations and very few <1 0 0> dislocations. The significant disagreement between the two IAPs with regard to defect morphology can be used for validation by comparing against the experimental results of dislocation types, density and size distributions.","sentences":["Molecular dynamics (MD) simulations of successive collision cascades within the same simulation domain were performed using two different inter-atomic potentials (IAP) in tungsten, one EAM based and the other a `quantum accurate' machine learning potential, SNAP.","The micro-structural changes are analyzed as a function of displacements per atom (dpa) up-to irradiation dose of 3 dpa.","Five simulations are carried out at primary knock-on atom (PKA) energies of 20 keV and 50 keV for observing stochastic differences in the evolution of damage.","Results for the variation of the number of defects, number of defect clusters, their size and morphology distribution are compared for the two energies and two IAPs, up-to irradiation doses of 1-3 dpa.   ","It is seen that for a given IAP, the number density of defect clusters as a function of dpa does not depend on the energy of the PKA used in the simulations implying that, like dpa, it could be a good measure of material property changes in irradiated materials.","In contrast, the defect sizes depend strongly on the energy of the PKA.","It is also seen that the micro-structure resulting from the EAM potential was mostly composed of <1 1 1> and <1 0 0> dislocations, while that from the SNAP potential was mostly composed of small, sessile, C15 ring like structures with a smattering of smaller <1 1 1> dislocations and very few <1 0 0> dislocations.","The significant disagreement between the two IAPs with regard to defect morphology can be used for validation by comparing against the experimental results of dislocation types, density and size distributions."],"url":"http://arxiv.org/abs/2405.03344v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 09:41:31","title":"Deep Learning-based Point Cloud Registration for Augmented Reality-guided Surgery","abstract":"Point cloud registration aligns 3D point clouds using spatial transformations. It is an important task in computer vision, with applications in areas such as augmented reality (AR) and medical imaging. This work explores the intersection of two research trends: the integration of AR into image-guided surgery and the use of deep learning for point cloud registration. The main objective is to evaluate the feasibility of applying deep learning-based point cloud registration methods for image-to-patient registration in augmented reality-guided surgery. We created a dataset of point clouds from medical imaging and corresponding point clouds captured with a popular AR device, the HoloLens 2. We evaluate three well-established deep learning models in registering these data pairs. While we find that some deep learning methods show promise, we show that a conventional registration pipeline still outperforms them on our challenging dataset.","sentences":["Point cloud registration aligns 3D point clouds using spatial transformations.","It is an important task in computer vision, with applications in areas such as augmented reality (AR) and medical imaging.","This work explores the intersection of two research trends: the integration of AR into image-guided surgery and the use of deep learning for point cloud registration.","The main objective is to evaluate the feasibility of applying deep learning-based point cloud registration methods for image-to-patient registration in augmented reality-guided surgery.","We created a dataset of point clouds from medical imaging and corresponding point clouds captured with a popular AR device, the HoloLens 2.","We evaluate three well-established deep learning models in registering these data pairs.","While we find that some deep learning methods show promise, we show that a conventional registration pipeline still outperforms them on our challenging dataset."],"url":"http://arxiv.org/abs/2405.03314v1","category":"cs.CV"}
{"created":"2024-05-06 09:21:15","title":"DarkFed: A Data-Free Backdoor Attack in Federated Learning","abstract":"Federated learning (FL) has been demonstrated to be susceptible to backdoor attacks. However, existing academic studies on FL backdoor attacks rely on a high proportion of real clients with main task-related data, which is impractical. In the context of real-world industrial scenarios, even the simplest defense suffices to defend against the state-of-the-art attack, 3DFed. A practical FL backdoor attack remains in a nascent stage of development.   To bridge this gap, we present DarkFed. Initially, we emulate a series of fake clients, thereby achieving the attacker proportion typical of academic research scenarios. Given that these emulated fake clients lack genuine training data, we further propose a data-free approach to backdoor FL. Specifically, we delve into the feasibility of injecting a backdoor using a shadow dataset. Our exploration reveals that impressive attack performance can be achieved, even when there is a substantial gap between the shadow dataset and the main task dataset. This holds true even when employing synthetic data devoid of any semantic information as the shadow dataset. Subsequently, we strategically construct a series of covert backdoor updates in an optimized manner, mimicking the properties of benign updates, to evade detection by defenses. A substantial body of empirical evidence validates the tangible effectiveness of DarkFed.","sentences":["Federated learning (FL) has been demonstrated to be susceptible to backdoor attacks.","However, existing academic studies on FL backdoor attacks rely on a high proportion of real clients with main task-related data, which is impractical.","In the context of real-world industrial scenarios, even the simplest defense suffices to defend against the state-of-the-art attack, 3DFed.","A practical FL backdoor attack remains in a nascent stage of development.   ","To bridge this gap, we present DarkFed.","Initially, we emulate a series of fake clients, thereby achieving the attacker proportion typical of academic research scenarios.","Given that these emulated fake clients lack genuine training data, we further propose a data-free approach to backdoor FL.","Specifically, we delve into the feasibility of injecting a backdoor using a shadow dataset.","Our exploration reveals that impressive attack performance can be achieved, even when there is a substantial gap between the shadow dataset and the main task dataset.","This holds true even when employing synthetic data devoid of any semantic information as the shadow dataset.","Subsequently, we strategically construct a series of covert backdoor updates in an optimized manner, mimicking the properties of benign updates, to evade detection by defenses.","A substantial body of empirical evidence validates the tangible effectiveness of DarkFed."],"url":"http://arxiv.org/abs/2405.03299v1","category":"cs.CR"}
{"created":"2024-05-06 08:24:55","title":"MARE: Multi-Agents Collaboration Framework for Requirements Engineering","abstract":"Requirements Engineering (RE) is a critical phase in the software development process that generates requirements specifications from stakeholders' needs. Recently, deep learning techniques have been successful in several RE tasks. However, obtaining high-quality requirements specifications requires collaboration across multiple tasks and roles. In this paper, we propose an innovative framework called MARE, which leverages collaboration among large language models (LLMs) throughout the entire RE process. MARE divides the RE process into four tasks: elicitation, modeling, verification, and specification. Each task is conducted by engaging one or two specific agents and each agent can conduct several actions. MARE has five agents and nine actions. To facilitate collaboration between agents, MARE has designed a workspace for agents to upload their generated intermediate requirements artifacts and obtain the information they need. We conduct experiments on five public cases, one dataset, and four new cases created by this work. We compared MARE with three baselines using three widely used metrics for the generated requirements models. Experimental results show that MARE can generate more correct requirements models and outperform the state-of-the-art approaches by 15.4%. For the generated requirements specifications, we conduct a human evaluation in three aspects and provide insights about the quality","sentences":["Requirements Engineering (RE) is a critical phase in the software development process that generates requirements specifications from stakeholders' needs.","Recently, deep learning techniques have been successful in several RE tasks.","However, obtaining high-quality requirements specifications requires collaboration across multiple tasks and roles.","In this paper, we propose an innovative framework called MARE, which leverages collaboration among large language models (LLMs) throughout the entire RE process.","MARE divides the RE process into four tasks: elicitation, modeling, verification, and specification.","Each task is conducted by engaging one or two specific agents and each agent can conduct several actions.","MARE has five agents and nine actions.","To facilitate collaboration between agents, MARE has designed a workspace for agents to upload their generated intermediate requirements artifacts and obtain the information they need.","We conduct experiments on five public cases, one dataset, and four new cases created by this work.","We compared MARE with three baselines using three widely used metrics for the generated requirements models.","Experimental results show that MARE can generate more correct requirements models and outperform the state-of-the-art approaches by 15.4%.","For the generated requirements specifications, we conduct a human evaluation in three aspects and provide insights about the quality"],"url":"http://arxiv.org/abs/2405.03256v1","category":"cs.SE"}
{"created":"2024-05-06 07:52:44","title":"Examining Changes in Internal Representations of Continual Learning Models Through Tensor Decomposition","abstract":"Continual learning (CL) has spurred the development of several methods aimed at consolidating previous knowledge across sequential learning. Yet, the evaluations of these methods have primarily focused on the final output, such as changes in the accuracy of predicted classes, overlooking the issue of representational forgetting within the model. In this paper, we propose a novel representation-based evaluation framework for CL models. This approach involves gathering internal representations from throughout the continual learning process and formulating three-dimensional tensors. The tensors are formed by stacking representations, such as layer activations, generated from several inputs and model `snapshots', throughout the learning process. By conducting tensor component analysis (TCA), we aim to uncover meaningful patterns about how the internal representations evolve, expecting to highlight the merits or shortcomings of examined CL strategies. We conduct our analyses across different model architectures and importance-based continual learning strategies, with a curated task selection. While the results of our approach mirror the difference in performance of various CL strategies, we found that our methodology did not directly highlight specialized clusters of neurons, nor provide an immediate understanding the evolution of filters. We believe a scaled down version of our approach will provide insight into the benefits and pitfalls of using TCA to study continual learning dynamics.","sentences":["Continual learning (CL) has spurred the development of several methods aimed at consolidating previous knowledge across sequential learning.","Yet, the evaluations of these methods have primarily focused on the final output, such as changes in the accuracy of predicted classes, overlooking the issue of representational forgetting within the model.","In this paper, we propose a novel representation-based evaluation framework for CL models.","This approach involves gathering internal representations from throughout the continual learning process and formulating three-dimensional tensors.","The tensors are formed by stacking representations, such as layer activations, generated from several inputs and model `snapshots', throughout the learning process.","By conducting tensor component analysis (TCA), we aim to uncover meaningful patterns about how the internal representations evolve, expecting to highlight the merits or shortcomings of examined CL strategies.","We conduct our analyses across different model architectures and importance-based continual learning strategies, with a curated task selection.","While the results of our approach mirror the difference in performance of various CL strategies, we found that our methodology did not directly highlight specialized clusters of neurons, nor provide an immediate understanding the evolution of filters.","We believe a scaled down version of our approach will provide insight into the benefits and pitfalls of using TCA to study continual learning dynamics."],"url":"http://arxiv.org/abs/2405.03244v1","category":"cs.LG"}
{"created":"2024-05-06 07:26:32","title":"OMP-Engineer: Bridging Syntax Analysis and In-Context Learning for Efficient Automated OpenMP Parallelization","abstract":"In advancing parallel programming, particularly with OpenMP, the shift towards NLP-based methods marks a significant innovation beyond traditional S2S tools like Autopar and Cetus. These NLP approaches train on extensive datasets of examples to efficiently generate optimized parallel code, streamlining the development process. This method's strength lies in its ability to swiftly produce parallelized code that runs efficiently. However, this reliance on NLP models, without direct code analysis, can introduce inaccuracies, as these models might not fully grasp the nuanced semantics of the code they parallelize. We build OMP-Engineer, which balances the efficiency and scalability of NLP models with the accuracy and reliability of traditional methods, aiming to enhance the performance of automating parallelization while navigating its inherent challenges.","sentences":["In advancing parallel programming, particularly with OpenMP, the shift towards NLP-based methods marks a significant innovation beyond traditional S2S tools like Autopar and Cetus.","These NLP approaches train on extensive datasets of examples to efficiently generate optimized parallel code, streamlining the development process.","This method's strength lies in its ability to swiftly produce parallelized code that runs efficiently.","However, this reliance on NLP models, without direct code analysis, can introduce inaccuracies, as these models might not fully grasp the nuanced semantics of the code they parallelize.","We build OMP-Engineer, which balances the efficiency and scalability of NLP models with the accuracy and reliability of traditional methods, aiming to enhance the performance of automating parallelization while navigating its inherent challenges."],"url":"http://arxiv.org/abs/2405.03215v1","category":"cs.DC"}
{"created":"2024-05-06 06:47:14","title":"Stability Evaluation via Distributional Perturbation Analysis","abstract":"The performance of learning models often deteriorates when deployed in out-of-sample environments. To ensure reliable deployment, we propose a stability evaluation criterion based on distributional perturbations. Conceptually, our stability evaluation criterion is defined as the minimal perturbation required on our observed dataset to induce a prescribed deterioration in risk evaluation. In this paper, we utilize the optimal transport (OT) discrepancy with moment constraints on the \\textit{(sample, density)} space to quantify this perturbation. Therefore, our stability evaluation criterion can address both \\emph{data corruptions} and \\emph{sub-population shifts} -- the two most common types of distribution shifts in real-world scenarios. To further realize practical benefits, we present a series of tractable convex formulations and computational methods tailored to different classes of loss functions. The key technical tool to achieve this is the strong duality theorem provided in this paper. Empirically, we validate the practical utility of our stability evaluation criterion across a host of real-world applications. These empirical studies showcase the criterion's ability not only to compare the stability of different learning models and features but also to provide valuable guidelines and strategies to further improve models.","sentences":["The performance of learning models often deteriorates when deployed in out-of-sample environments.","To ensure reliable deployment, we propose a stability evaluation criterion based on distributional perturbations.","Conceptually, our stability evaluation criterion is defined as the minimal perturbation required on our observed dataset to induce a prescribed deterioration in risk evaluation.","In this paper, we utilize the optimal transport (OT) discrepancy with moment constraints on the \\textit{(sample, density)} space to quantify this perturbation.","Therefore, our stability evaluation criterion can address both \\emph{data corruptions} and \\emph{sub-population shifts} -- the two most common types of distribution shifts in real-world scenarios.","To further realize practical benefits, we present a series of tractable convex formulations and computational methods tailored to different classes of loss functions.","The key technical tool to achieve this is the strong duality theorem provided in this paper.","Empirically, we validate the practical utility of our stability evaluation criterion across a host of real-world applications.","These empirical studies showcase the criterion's ability not only to compare the stability of different learning models and features but also to provide valuable guidelines and strategies to further improve models."],"url":"http://arxiv.org/abs/2405.03198v1","category":"stat.ML"}
{"created":"2024-05-06 06:05:41","title":"Braced Fourier Continuation and Regression for Anomaly Detection","abstract":"In this work, the concept of Braced Fourier Continuation and Regression (BFCR) is introduced. BFCR is a novel and computationally efficient means of finding nonlinear regressions or trend lines in arbitrary one-dimensional data sets. The Braced Fourier Continuation (BFC) and BFCR algorithms are first outlined, followed by a discussion of the properties of BFCR as well as demonstrations of how BFCR trend lines may be used effectively for anomaly detection both within and at the edges of arbitrary one-dimensional data sets. Finally, potential issues which may arise while using BFCR for anomaly detection as well as possible mitigation techniques are outlined and discussed. All source code and example data sets are either referenced or available via GitHub, and all associated code is written entirely in Python.","sentences":["In this work, the concept of Braced Fourier Continuation and Regression (BFCR) is introduced.","BFCR is a novel and computationally efficient means of finding nonlinear regressions or trend lines in arbitrary one-dimensional data sets.","The Braced Fourier Continuation (BFC) and BFCR algorithms are first outlined, followed by a discussion of the properties of BFCR as well as demonstrations of how BFCR trend lines may be used effectively for anomaly detection both within and at the edges of arbitrary one-dimensional data sets.","Finally, potential issues which may arise while using BFCR for anomaly detection as well as possible mitigation techniques are outlined and discussed.","All source code and example data sets are either referenced or available via GitHub, and all associated code is written entirely in Python."],"url":"http://arxiv.org/abs/2405.03180v1","category":"stat.ML"}
{"created":"2024-05-06 04:01:42","title":"Video Diffusion Models: A Survey","abstract":"Diffusion generative models have recently become a robust technique for producing and modifying coherent, high-quality video. This survey offers a systematic overview of critical elements of diffusion models for video generation, covering applications, architectural choices, and the modeling of temporal dynamics. Recent advancements in the field are summarized and grouped into development trends. The survey concludes with an overview of remaining challenges and an outlook on the future of the field. Website: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models","sentences":["Diffusion generative models have recently become a robust technique for producing and modifying coherent, high-quality video.","This survey offers a systematic overview of critical elements of diffusion models for video generation, covering applications, architectural choices, and the modeling of temporal dynamics.","Recent advancements in the field are summarized and grouped into development trends.","The survey concludes with an overview of remaining challenges and an outlook on the future of the field.","Website: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models"],"url":"http://arxiv.org/abs/2405.03150v1","category":"cs.CV"}
{"created":"2024-05-06 02:06:26","title":"Vector Quantization for Recommender Systems: A Review and Outlook","abstract":"Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.","sentences":["Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today.","With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution.","This paper starts with a comprehensive review of vector quantization techniques.","It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives.","Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches.","Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems.","We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field."],"url":"http://arxiv.org/abs/2405.03110v1","category":"cs.IR"}
{"created":"2024-05-06 01:40:20","title":"GeoContrastNet: Contrastive Key-Value Edge Learning for Language-Agnostic Document Understanding","abstract":"This paper presents GeoContrastNet, a language-agnostic framework to structured document understanding (DU) by integrating a contrastive learning objective with graph attention networks (GATs), emphasizing the significant role of geometric features. We propose a novel methodology that combines geometric edge features with visual features within an overall two-staged GAT-based framework, demonstrating promising results in both link prediction and semantic entity recognition performance. Our findings reveal that combining both geometric and visual features could match the capabilities of large DU models that rely heavily on Optical Character Recognition (OCR) features in terms of performance accuracy and efficiency. This approach underscores the critical importance of relational layout information between the named text entities in a semi-structured layout of a page. Specifically, our results highlight the model's proficiency in identifying key-value relationships within the FUNSD dataset for forms and also discovering the spatial relationships in table-structured layouts for RVLCDIP business invoices. Our code and pretrained models will be accessible on our official GitHub.","sentences":["This paper presents GeoContrastNet, a language-agnostic framework to structured document understanding (DU) by integrating a contrastive learning objective with graph attention networks (GATs), emphasizing the significant role of geometric features.","We propose a novel methodology that combines geometric edge features with visual features within an overall two-staged GAT-based framework, demonstrating promising results in both link prediction and semantic entity recognition performance.","Our findings reveal that combining both geometric and visual features could match the capabilities of large DU models that rely heavily on Optical Character Recognition (OCR) features in terms of performance accuracy and efficiency.","This approach underscores the critical importance of relational layout information between the named text entities in a semi-structured layout of a page.","Specifically, our results highlight the model's proficiency in identifying key-value relationships within the FUNSD dataset for forms and also discovering the spatial relationships in table-structured layouts for RVLCDIP business invoices.","Our code and pretrained models will be accessible on our official GitHub."],"url":"http://arxiv.org/abs/2405.03104v1","category":"cs.CV"}
{"created":"2024-05-06 01:08:22","title":"Bayesian optimization for stable properties amid processing fluctuations in sputter deposition","abstract":"We introduce a Bayesian optimization approach to guide the sputter deposition of molybdenum thin films, aiming to achieve desired residual stress and sheet resistance while minimizing susceptibility to stochastic fluctuations during deposition. Thin films are pivotal in numerous technologies, including semiconductors and optical devices, where their properties are critical. Sputter deposition parameters, such as deposition power, vacuum chamber pressure, and working distance, influence physical properties like residual stress and resistance. Excessive stress and high resistance can impair device performance, necessitating the selection of optimal process parameters. Furthermore, these parameters should ensure the consistency and reliability of thin film properties, assisting in the reproducibility of the devices. However, exploring the multidimensional design space for process optimization is expensive. Bayesian optimization is ideal for optimizing inputs/parameters of general black-box functions without reliance on gradient information. We utilize Bayesian optimization to optimize deposition power and pressure using a custom-built objective function incorporating observed stress and resistance data. Additionally, we integrate prior knowledge of stress variation with pressure into the objective function to prioritize films least affected by stochastic variations. Our findings demonstrate that Bayesian optimization effectively explores the design space and identifies optimal parameter combinations meeting desired stress and resistance specifications.","sentences":["We introduce a Bayesian optimization approach to guide the sputter deposition of molybdenum thin films, aiming to achieve desired residual stress and sheet resistance while minimizing susceptibility to stochastic fluctuations during deposition.","Thin films are pivotal in numerous technologies, including semiconductors and optical devices, where their properties are critical.","Sputter deposition parameters, such as deposition power, vacuum chamber pressure, and working distance, influence physical properties like residual stress and resistance.","Excessive stress and high resistance can impair device performance, necessitating the selection of optimal process parameters.","Furthermore, these parameters should ensure the consistency and reliability of thin film properties, assisting in the reproducibility of the devices.","However, exploring the multidimensional design space for process optimization is expensive.","Bayesian optimization is ideal for optimizing inputs/parameters of general black-box functions without reliance on gradient information.","We utilize Bayesian optimization to optimize deposition power and pressure using a custom-built objective function incorporating observed stress and resistance data.","Additionally, we integrate prior knowledge of stress variation with pressure into the objective function to prioritize films least affected by stochastic variations.","Our findings demonstrate that Bayesian optimization effectively explores the design space and identifies optimal parameter combinations meeting desired stress and resistance specifications."],"url":"http://arxiv.org/abs/2405.03092v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 00:18:35","title":"Analyzing Emotional Trends from X platform using SenticNet: A Comparative Analysis with Cryptocurrency Price","abstract":"This study delves into the relationship between emotional trends from X platform data and the market dynamics of well-known cryptocurrencies Cardano, Binance, Fantom, Matic, and Ripple over the period from October 2022 to March 2023. Leveraging SenticNet, we identified emotions like Fear and Anxiety, Rage and Anger, Grief and Sadness, Delight and Pleasantness, Enthusiasm and Eagerness, and Delight and Joy. Following data extraction, we segmented each month into bi-weekly intervals, replicating this process for price data obtained from Finance-Yahoo. Consequently, a comparative analysis was conducted, establishing connections between emotional trends observed across bi-weekly intervals and cryptocurrency prices, uncovering significant correlations between emotional sentiments and coin valuations.","sentences":["This study delves into the relationship between emotional trends from X platform data and the market dynamics of well-known cryptocurrencies Cardano, Binance, Fantom, Matic, and Ripple over the period from October 2022 to March 2023.","Leveraging SenticNet, we identified emotions like Fear and Anxiety, Rage and Anger, Grief and Sadness, Delight and Pleasantness, Enthusiasm and Eagerness, and Delight and Joy.","Following data extraction, we segmented each month into bi-weekly intervals, replicating this process for price data obtained from Finance-Yahoo.","Consequently, a comparative analysis was conducted, establishing connections between emotional trends observed across bi-weekly intervals and cryptocurrency prices, uncovering significant correlations between emotional sentiments and coin valuations."],"url":"http://arxiv.org/abs/2405.03084v1","category":"cs.CL"}
{"created":"2024-05-05 23:59:51","title":"Causal K-Means Clustering","abstract":"Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.","sentences":["Causal effects are often characterized with population summaries.","These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups.","Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects.","We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure.","Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions.","We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence.","We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models.","Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels.","Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions.","Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse."],"url":"http://arxiv.org/abs/2405.03083v1","category":"stat.ME"}
{"created":"2024-05-05 22:23:14","title":"Automated Deep Learning Optimization via DSL-Based Source Code Transformation","abstract":"As deep learning models become increasingly bigger and more complex, it is critical to improve model training and inference efficiency. Though a variety of highly optimized libraries and packages (known as DL kernels) have been developed, it is tedious and time-consuming to figure out which kernel to use, where to use, and how to use them correctly. To address this challenge, we propose an Automated Deep learning OPTimization approach called Adopter. We design a Domain-Specific Language (DSL) to represent DL model architectures and leverage this DSL to specify model transformation rules required to integrate a DL kernel into a model. Given the source code of a DL model and the transformation rules for a set of kernels, Adopter first performs inter-procedural analysis to identify and express the model architecture in our DSL. Then, Adopter performs scope analysis and sub-sequence matching to identify locations in the model architecture where the transformation rules can be applied. Finally, Adopter proposes a synthesis-based code transformation method to apply the transformation rule. We curated a benchmark with 199 models from Hugging Face and a diverse set of DL kernels. We found that, compared to a state-of-the-art automated code transformation technique, Adopter helps improve the precision and recall by 3% and 56%, respectively. An in-depth analysis of 9 models revealed that on average, Adopter improved the training speed by 22.7% while decreasing the GPU memory usage by 10.5%.","sentences":["As deep learning models become increasingly bigger and more complex, it is critical to improve model training and inference efficiency.","Though a variety of highly optimized libraries and packages (known as DL kernels) have been developed, it is tedious and time-consuming to figure out which kernel to use, where to use, and how to use them correctly.","To address this challenge, we propose an Automated Deep learning OPTimization approach called Adopter.","We design a Domain-Specific Language (DSL) to represent DL model architectures and leverage this DSL to specify model transformation rules required to integrate a DL kernel into a model.","Given the source code of a DL model and the transformation rules for a set of kernels, Adopter first performs inter-procedural analysis to identify and express the model architecture in our DSL.","Then, Adopter performs scope analysis and sub-sequence matching to identify locations in the model architecture where the transformation rules can be applied.","Finally, Adopter proposes a synthesis-based code transformation method to apply the transformation rule.","We curated a benchmark with 199 models from Hugging Face and a diverse set of DL kernels.","We found that, compared to a state-of-the-art automated code transformation technique, Adopter helps improve the precision and recall by 3% and 56%, respectively.","An in-depth analysis of 9 models revealed that on average, Adopter improved the training speed by 22.7% while decreasing the GPU memory usage by 10.5%."],"url":"http://arxiv.org/abs/2405.03067v1","category":"cs.SE"}
{"created":"2024-05-05 21:49:51","title":"Tree-based Ensemble Learning for Out-of-distribution Detection","abstract":"Being able to successfully determine whether the testing samples has similar distribution as the training samples is a fundamental question to address before we can safely deploy most of the machine learning models into practice. In this paper, we propose TOOD detection, a simple yet effective tree-based out-of-distribution (TOOD) detection mechanism to determine if a set of unseen samples will have similar distribution as of the training samples. The TOOD detection mechanism is based on computing pairwise hamming distance of testing samples' tree embeddings, which are obtained by fitting a tree-based ensemble model through in-distribution training samples. Our approach is interpretable and robust for its tree-based nature. Furthermore, our approach is efficient, flexible to various machine learning tasks, and can be easily generalized to unsupervised setting. Extensive experiments are conducted to show the proposed method outperforms other state-of-the-art out-of-distribution detection methods in distinguishing the in-distribution from out-of-distribution on various tabular, image, and text data.","sentences":["Being able to successfully determine whether the testing samples has similar distribution as the training samples is a fundamental question to address before we can safely deploy most of the machine learning models into practice.","In this paper, we propose TOOD detection, a simple yet effective tree-based out-of-distribution (TOOD) detection mechanism to determine if a set of unseen samples will have similar distribution as of the training samples.","The TOOD detection mechanism is based on computing pairwise hamming distance of testing samples' tree embeddings, which are obtained by fitting a tree-based ensemble model through in-distribution training samples.","Our approach is interpretable and robust for its tree-based nature.","Furthermore, our approach is efficient, flexible to various machine learning tasks, and can be easily generalized to unsupervised setting.","Extensive experiments are conducted to show the proposed method outperforms other state-of-the-art out-of-distribution detection methods in distinguishing the in-distribution from out-of-distribution on various tabular, image, and text data."],"url":"http://arxiv.org/abs/2405.03060v1","category":"cs.LG"}
{"created":"2024-05-05 20:06:30","title":"Bayesian Functional Graphical Models with Change-Point Detection","abstract":"Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data. Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series. For this task, we propose a dynamic and Bayesian functional graphical model. Our modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns. We introduce a novel block-structured sparsity prior paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm. Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics. Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods. We apply the proposed approach to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and discovers meaningful edges.","sentences":["Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data.","Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series.","For this task, we propose a dynamic and Bayesian functional graphical model.","Our modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns.","We introduce a novel block-structured sparsity prior paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm.","Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics.","Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods.","We apply the proposed approach to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and discovers meaningful edges."],"url":"http://arxiv.org/abs/2405.03041v1","category":"stat.ME"}
{"created":"2024-05-05 17:19:35","title":"Exploring prompts to elicit memorization in masked language model-based named entity recognition","abstract":"Training data memorization in language models impacts model capability (generalization) and safety (privacy risk). This paper focuses on analyzing prompts' impact on detecting the memorization of 6 masked language model-based named entity recognition models. Specifically, we employ a diverse set of 400 automatically generated prompts, and a pairwise dataset where each pair consists of one person's name from the training set and another name out of the set. A prompt completed with a person's name serves as input for getting the model's confidence in predicting this name. Finally, the prompt performance of detecting model memorization is quantified by the percentage of name pairs for which the model has higher confidence for the name from the training set. We show that the performance of different prompts varies by as much as 16 percentage points on the same model, and prompt engineering further increases the gap. Moreover, our experiments demonstrate that prompt performance is model-dependent but does generalize across different name sets. A comprehensive analysis indicates how prompt performance is influenced by prompt properties, contained tokens, and the model's self-attention weights on the prompt.","sentences":["Training data memorization in language models impacts model capability (generalization) and safety (privacy risk).","This paper focuses on analyzing prompts' impact on detecting the memorization of 6 masked language model-based named entity recognition models.","Specifically, we employ a diverse set of 400 automatically generated prompts, and a pairwise dataset where each pair consists of one person's name from the training set and another name out of the set.","A prompt completed with a person's name serves as input for getting the model's confidence in predicting this name.","Finally, the prompt performance of detecting model memorization is quantified by the percentage of name pairs for which the model has higher confidence for the name from the training set.","We show that the performance of different prompts varies by as much as 16 percentage points on the same model, and prompt engineering further increases the gap.","Moreover, our experiments demonstrate that prompt performance is model-dependent but does generalize across different name sets.","A comprehensive analysis indicates how prompt performance is influenced by prompt properties, contained tokens, and the model's self-attention weights on the prompt."],"url":"http://arxiv.org/abs/2405.03004v1","category":"cs.CL"}
{"created":"2024-05-05 14:56:34","title":"Score-based Generative Priors Guided Model-driven Network for MRI Reconstruction","abstract":"Score matching with Langevin dynamics (SMLD) method has been successfully applied to accelerated MRI. However, the hyperparameters in the sampling process require subtle tuning, otherwise the results can be severely corrupted by hallucination artifacts, particularly with out-of-distribution test data. In this study, we propose a novel workflow in which SMLD results are regarded as additional priors to guide model-driven network training. First, we adopted a pretrained score network to obtain samples as preliminary guidance images (PGI) without the need for network retraining, parameter tuning and in-distribution test data. Although PGIs are corrupted by hallucination artifacts, we believe that they can provide extra information through effective denoising steps to facilitate reconstruction. Therefore, we designed a denoising module (DM) in the second step to improve the quality of PGIs. The features are extracted from the components of Langevin dynamics and the same score network with fine-tuning; hence, we can directly learn the artifact patterns. Third, we designed a model-driven network whose training is guided by denoised PGIs (DGIs). DGIs are densely connected with intermediate reconstructions in each cascade to enrich the features and are periodically updated to provide more accurate guidance. Our experiments on different sequences revealed that despite the low average quality of PGIs, the proposed workflow can effectively extract valuable information to guide the network training, even with severely reduced training data and sampling steps. Our method outperforms other cutting-edge techniques by effectively mitigating hallucination artifacts, yielding robust and high-quality reconstruction results.","sentences":["Score matching with Langevin dynamics (SMLD) method has been successfully applied to accelerated MRI.","However, the hyperparameters in the sampling process require subtle tuning, otherwise the results can be severely corrupted by hallucination artifacts, particularly with out-of-distribution test data.","In this study, we propose a novel workflow in which SMLD results are regarded as additional priors to guide model-driven network training.","First, we adopted a pretrained score network to obtain samples as preliminary guidance images (PGI) without the need for network retraining, parameter tuning and in-distribution test data.","Although PGIs are corrupted by hallucination artifacts, we believe that they can provide extra information through effective denoising steps to facilitate reconstruction.","Therefore, we designed a denoising module (DM) in the second step to improve the quality of PGIs.","The features are extracted from the components of Langevin dynamics and the same score network with fine-tuning; hence, we can directly learn the artifact patterns.","Third, we designed a model-driven network whose training is guided by denoised PGIs (DGIs).","DGIs are densely connected with intermediate reconstructions in each cascade to enrich the features and are periodically updated to provide more accurate guidance.","Our experiments on different sequences revealed that despite the low average quality of PGIs, the proposed workflow can effectively extract valuable information to guide the network training, even with severely reduced training data and sampling steps.","Our method outperforms other cutting-edge techniques by effectively mitigating hallucination artifacts, yielding robust and high-quality reconstruction results."],"url":"http://arxiv.org/abs/2405.02958v1","category":"cs.CV"}
{"created":"2024-05-05 14:47:24","title":"Analysis of the Identifying Regulation with Adversarial Surrogates Algorithm","abstract":"Given a time-series of noisy measured outputs of a dynamical system z[k], k=1...N, the Identifying Regulation with Adversarial Surrogates (IRAS) algorithm aims to find a non-trivial first integral of the system, namely, a scalar function g() such that g(z[i]) = g(z[j]), for all i,j. IRAS has been suggested recently and was used successfully in several learning tasks in models from biology and physics. Here, we give the first rigorous analysis of this algorithm in a specific setting. We assume that the observations admit a linear first integral and that they are contaminated by Gaussian noise. We show that in this case the IRAS iterations are closely related to the self-consistent-field (SCF) iterations for solving a generalized Rayleigh quotient minimization problem. Using this approach, we derive several sufficient conditions guaranteeing local convergence of IRAS to the correct first integral.","sentences":["Given a time-series of noisy measured outputs of a dynamical system z[k], k=1...N, the Identifying Regulation with Adversarial Surrogates (IRAS) algorithm aims to find a non-trivial first integral of the system, namely, a scalar function g() such that g(z[i]) = g(z[j]), for all i,j. IRAS has been suggested recently and was used successfully in several learning tasks in models from biology and physics.","Here, we give the first rigorous analysis of this algorithm in a specific setting.","We assume that the observations admit a linear first integral and that they are contaminated by Gaussian noise.","We show that in this case the IRAS iterations are closely related to the self-consistent-field (SCF) iterations for solving a generalized Rayleigh quotient minimization problem.","Using this approach, we derive several sufficient conditions guaranteeing local convergence of IRAS to the correct first integral."],"url":"http://arxiv.org/abs/2405.02953v1","category":"eess.SY"}
{"created":"2024-05-06 17:29:31","title":"Exponential optimization of quantum state preparation via adiabatic thermalization","abstract":"The preparation of a given quantum state on a quantum computing register is a typically demanding operation, requiring a number of elementary gates that scales exponentially with the size of the problem. Using the adiabatic theorem for state preparation, whose error decreases exponentially as a function of the thermalization time, we derive an explicit analytic expression for the dependence of the characteristic time on the Hamiltonian used in the adiabatic evolution. Exploiting this knowledge, we then design a preconditioning term that modifies the adiabatic preparation, thus reducing its characteristic time and hence giving an exponential advantage in state preparation. We prove the efficiency of our method with extensive numerical experiments on prototypical spin-models, which gives a promising strategy to perform quantum simulations of manybody models via Trotter evolution on near-term quantum processors.","sentences":["The preparation of a given quantum state on a quantum computing register is a typically demanding operation, requiring a number of elementary gates that scales exponentially with the size of the problem.","Using the adiabatic theorem for state preparation, whose error decreases exponentially as a function of the thermalization time, we derive an explicit analytic expression for the dependence of the characteristic time on the Hamiltonian used in the adiabatic evolution.","Exploiting this knowledge, we then design a preconditioning term that modifies the adiabatic preparation, thus reducing its characteristic time and hence giving an exponential advantage in state preparation.","We prove the efficiency of our method with extensive numerical experiments on prototypical spin-models, which gives a promising strategy to perform quantum simulations of manybody models via Trotter evolution on near-term quantum processors."],"url":"http://arxiv.org/abs/2405.03656v1","category":"quant-ph"}
{"created":"2024-05-06 16:56:16","title":"Cosine Annealing Optimized Denoising Diffusion Error Correction Codes","abstract":"To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing. In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance. By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency. This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods. This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices.","sentences":["To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing.","In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance.","By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency.","This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods.","This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices."],"url":"http://arxiv.org/abs/2405.03638v1","category":"cs.IT"}
{"created":"2024-05-06 16:21:47","title":"Energy-Based Optimization of Physical-Layer Challenge-Response Authentication with Drones","abstract":"Drones are expected to be used for many tasks in the future and require secure communication protocols. In this work, we propose a novel physical layer authentication (PLA)-based challenge-response (CR) protocol in which a drone Bob authenticates the sender (either on the ground or air) by exploiting his prior knowledge of the wireless channel statistic (fading, path loss, and shadowing). In particular, Bob will move to a set of positions in the space, and by estimating the attenuations of the received signals he will authenticate the sender. We take into account the energy consumption in the design and provide three solutions: a purely greedy solution (PG), an optimal Bellman iterative solution (BI), and a heuristic solution based on the evaluation of the standard deviation of the attenuations in the space. Finally, we demonstrate the effectiveness of our approach through numerical simulations.","sentences":["Drones are expected to be used for many tasks in the future and require secure communication protocols.","In this work, we propose a novel physical layer authentication (PLA)-based challenge-response (CR) protocol in which a drone Bob authenticates the sender (either on the ground or air) by exploiting his prior knowledge of the wireless channel statistic (fading, path loss, and shadowing).","In particular, Bob will move to a set of positions in the space, and by estimating the attenuations of the received signals he will authenticate the sender.","We take into account the energy consumption in the design and provide three solutions: a purely greedy solution (PG), an optimal Bellman iterative solution (BI), and a heuristic solution based on the evaluation of the standard deviation of the attenuations in the space.","Finally, we demonstrate the effectiveness of our approach through numerical simulations."],"url":"http://arxiv.org/abs/2405.03608v1","category":"eess.SP"}
{"created":"2024-05-06 15:10:16","title":"Optimizing Hand Region Detection in MediaPipe Holistic Full-Body Pose Estimation to Improve Accuracy and Avoid Downstream Errors","abstract":"This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy. We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension. Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method. Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix.","sentences":["This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy.","We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension.","Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method.","Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix."],"url":"http://arxiv.org/abs/2405.03545v1","category":"cs.CV"}
{"created":"2024-05-06 13:48:51","title":"A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures","abstract":"Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace. However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position. This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm. The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved. We then propose a solution that is based on a Linear Quadratic Tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios.","sentences":["Implementing virtual fixtures in guiding tasks constrains the movement of the robot's end effector to specific curves within its workspace.","However, incorporating guiding frameworks may encounter discontinuities when optimizing the reference target position to the nearest point relative to the current robot position.","This article aims to give a geometric interpretation of such discontinuities, with specific reference to the commonly adopted Gauss-Newton algorithm.","The effect of such discontinuities, defined as Euclidean Distance Singularities, is experimentally proved.","We then propose a solution that is based on a Linear Quadratic Tracking problem with minimum jerk command, then compare and validate the performances of the proposed framework in two different human-robot interaction scenarios."],"url":"http://arxiv.org/abs/2405.03473v1","category":"cs.RO"}
{"created":"2024-05-06 13:29:14","title":"Performance of H-Matrix-Vector Multiplication with Floating Point Compression","abstract":"Matrix-vector multiplication forms the basis of many iterative solution algorithms and as such is an important algorithm also for hierarchical matrices. However, due to its low computational intensity, its performance is typically limited by the available memory bandwidth. By optimizing the storage representation of the data within such matrices, this limitation can be lifted and the performance increased. This applies not only to hierarchical matrices but for also for other low-rank approximation schemes, e.g. block low-rank matrices.","sentences":["Matrix-vector multiplication forms the basis of many iterative solution algorithms and as such is an important algorithm also for hierarchical matrices.","However, due to its low computational intensity, its performance is typically limited by the available memory bandwidth.","By optimizing the storage representation of the data within such matrices, this limitation can be lifted and the performance increased.","This applies not only to hierarchical matrices but for also for other low-rank approximation schemes, e.g. block low-rank matrices."],"url":"http://arxiv.org/abs/2405.03456v1","category":"cs.DC"}
{"created":"2024-05-06 12:13:47","title":"Improved scalar auxiliary variable schemes for original energy stability of gradient flows","abstract":"Scalar auxiliary variable (SAV) methods are a class of linear schemes for solving gradient flows that are known for the stability of a `modified' energy. In this paper, we propose an improved SAV (iSAV) scheme that not only retains the complete linearity but also ensures rigorously the stability of the original energy. The convergence and optimal error bound are rigorously established for the iSAV scheme and discussions are made for its high-order extension. Extensive numerical experiments are done to validate the convergence, robustness and energy stability of iSAV, and some comparisons are made.","sentences":["Scalar auxiliary variable (SAV) methods are a class of linear schemes for solving gradient flows that are known for the stability of a `modified' energy.","In this paper, we propose an improved SAV (iSAV) scheme that not only retains the complete linearity but also ensures rigorously the stability of the original energy.","The convergence and optimal error bound are rigorously established for the iSAV scheme and discussions are made for its high-order extension.","Extensive numerical experiments are done to validate the convergence, robustness and energy stability of iSAV, and some comparisons are made."],"url":"http://arxiv.org/abs/2405.03403v1","category":"math.NA"}
{"created":"2024-05-06 12:12:03","title":"Distributional Reference Class Forecasting of Corporate Sales Growth With Multiple Reference Variables","abstract":"This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors. The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes. These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis. Forecasts are optimal if they match the underlying distribution as closely as possible. Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019. In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes. Comparisions of actual analysts' estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method.","sentences":["This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors.","The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes.","These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis.","Forecasts are optimal if they match the underlying distribution as closely as possible.","Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019.","In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes.","Comparisions of actual analysts' estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method."],"url":"http://arxiv.org/abs/2405.03402v1","category":"q-fin.ST"}
{"created":"2024-05-06 09:59:20","title":"Enhancing Aeroacoustic Wind Tunnel Studies through Massive Channel Upscaling with MEMS Microphones","abstract":"This paper presents a large 6~m x 3~m aperture 7200 MEMS microphone array. The array is designed so that sub-arrays with optimized point spread functions can be used for beamforming and thus, enable the research of source directivity in wind tunnel facilities. The total array consists of modular 800 microphone panels, each consisting of four unique PCB board designs. This modular architecture allows for the time-synchronized measurement of an arbitrary number of panels and thus, aperture size and total number of sensors. The panels can be installed without a gap so that the array's microphone pattern avoids high sidelobes in the point spread function. The array's capabilities are evaluated on a 1:9.5 airframe half model in an open wind tunnel at DNW-NWB. The total source emission is quantified and the directivity is evaluated with beamforming. Additional far-field microphones are employed to validate the results.","sentences":["This paper presents a large 6~m x 3~m aperture 7200 MEMS microphone array.","The array is designed so that sub-arrays with optimized point spread functions can be used for beamforming and thus, enable the research of source directivity in wind tunnel facilities.","The total array consists of modular 800 microphone panels, each consisting of four unique PCB board designs.","This modular architecture allows for the time-synchronized measurement of an arbitrary number of panels and thus, aperture size and total number of sensors.","The panels can be installed without a gap so that the array's microphone pattern avoids high sidelobes in the point spread function.","The array's capabilities are evaluated on a 1:9.5 airframe half model in an open wind tunnel at DNW-NWB.","The total source emission is quantified and the directivity is evaluated with beamforming.","Additional far-field microphones are employed to validate the results."],"url":"http://arxiv.org/abs/2405.03322v1","category":"cs.SD"}
{"created":"2024-05-06 09:57:44","title":"Distributed Model Checking on Graphs of Bounded Treedepth","abstract":"We establish that every monadic second-order logic (MSO) formula on graphs with bounded treedepth is decidable in a constant number of rounds within the CONGEST model. To our knowledge, this marks the first meta-theorem regarding distributed model-checking. Various optimization problems on graphs are expressible in MSO. Examples include determining whether a graph $G$ has a clique of size $k$, whether it admits a coloring with $k$ colors, whether it contains a graph $H$ as a subgraph or minor, or whether terminal vertices in $G$ could be connected via vertex-disjoint paths. Our meta-theorem significantly enhances the work of Bousquet et al. [PODC 2022], which was focused on distributed certification of MSO on graphs with bounded treedepth. Moreover, our results can be extended to solving optimization and counting problems expressible in MSO, in graphs of bounded treedepth.","sentences":["We establish that every monadic second-order logic (MSO) formula on graphs with bounded treedepth is decidable in a constant number of rounds within the CONGEST model.","To our knowledge, this marks the first meta-theorem regarding distributed model-checking.","Various optimization problems on graphs are expressible in MSO.","Examples include determining whether a graph $G$ has a clique of size $k$, whether it admits a coloring with $k$ colors, whether it contains a graph $H$ as a subgraph or minor, or whether terminal vertices in $G$ could be connected via vertex-disjoint paths.","Our meta-theorem significantly enhances the work of Bousquet et al.","[PODC 2022], which was focused on distributed certification of MSO on graphs with bounded treedepth.","Moreover, our results can be extended to solving optimization and counting problems expressible in MSO, in graphs of bounded treedepth."],"url":"http://arxiv.org/abs/2405.03321v1","category":"cs.DS"}
{"created":"2024-05-06 09:49:06","title":"Numerical optimization of quantum vacuum signals","abstract":"The identification of prospective scenarios for observing quantum vacuum signals in high-intensity laser experiments requires both accurate theoretical predictions and the exploration of high-dimensional parameter spaces. Numerical simulations address the first requirement, while optimization provides an efficient solution for the second one. In the present work, we demonstrate the potential of Bayesian optimization in maximizing photonic quantum vacuum signals on the example of two-beam collisions. This allows us to find the optimal waist sizes for beams with elliptic cross sections, and to identify the specific physical process leading to a discernible signal in a coherent harmonic focusing configuration scenario.","sentences":["The identification of prospective scenarios for observing quantum vacuum signals in high-intensity laser experiments requires both accurate theoretical predictions and the exploration of high-dimensional parameter spaces.","Numerical simulations address the first requirement, while optimization provides an efficient solution for the second one.","In the present work, we demonstrate the potential of Bayesian optimization in maximizing photonic quantum vacuum signals on the example of two-beam collisions.","This allows us to find the optimal waist sizes for beams with elliptic cross sections, and to identify the specific physical process leading to a discernible signal in a coherent harmonic focusing configuration scenario."],"url":"http://arxiv.org/abs/2405.03317v1","category":"hep-ph"}
{"created":"2024-05-06 08:57:58","title":"Hermite expansions for spaces of functions with nearly optimal time-frequency decay","abstract":"We establish Hermite expansion characterizations for several subspaces of the Fr\\'{e}chet space of functions on the real line satisfying the time-frequency decay bounds \\[ |f(x)| \\lesssim e^{-(\\frac{1}{2} - \\lambda) x^{2}} , \\qquad |\\widehat{f}(\\xi)| \\lesssim e^{-(\\frac{1}{2} - \\lambda) \\xi^{2}} , \\qquad \\forall \\lambda > 0 . \\] In particular, our results improve and extend upon recent Fourier characterizations of the so-called proper Pilpovi\\'{c} spaces obtained in [J. Funct. Anal. 284 (2023), 109724]. The main ingredients in our proofs are the Bargmann transform and some optimal forms of the Phragm\\'{e}n-Lindel\\\"{o}f principle on sectors, also provided in this article.","sentences":["We establish Hermite expansion characterizations for several subspaces of the Fr\\'{e}chet space of functions on the real line satisfying the time-frequency decay bounds \\","[ |f(x)| \\lesssim e^{-(\\frac{1}{2} - \\lambda) x^{2}} , \\qquad |\\widehat{f}(\\xi)| \\lesssim e^{-(\\frac{1}{2} - \\lambda) \\xi^{2}} , \\qquad \\forall \\lambda > 0 .","\\]","In particular, our results improve and extend upon recent Fourier characterizations of the so-called proper Pilpovi\\'{c} spaces obtained in [J. Funct.","Anal. 284 (2023), 109724].","The main ingredients in our proofs are the Bargmann transform and some optimal forms of the Phragm\\'{e}n-Lindel\\\"{o}f principle on sectors, also provided in this article."],"url":"http://arxiv.org/abs/2405.03282v1","category":"math.FA"}
{"created":"2024-05-06 06:38:49","title":"CityLLaVA: Efficient Fine-Tuning for VLMs in City Scenario","abstract":"In the vast and dynamic landscape of urban settings, Traffic Safety Description and Analysis plays a pivotal role in applications ranging from insurance inspection to accident prevention. This paper introduces CityLLaVA, a novel fine-tuning framework for Visual Language Models (VLMs) designed for urban scenarios. CityLLaVA enhances model comprehension and prediction accuracy through (1) employing bounding boxes for optimal visual data preprocessing, including video best-view selection and visual prompt engineering during both training and testing phases; (2) constructing concise Question-Answer sequences and designing textual prompts to refine instruction comprehension; (3) implementing block expansion to fine-tune large VLMs efficiently; and (4) advancing prediction accuracy via a unique sequential questioning-based prediction augmentation. Demonstrating top-tier performance, our method achieved a benchmark score of 33.4308, securing the leading position on the leaderboard. The code can be found: https://github.com/alibaba/AICITY2024_Track2_AliOpenTrek_CityLLaVA","sentences":["In the vast and dynamic landscape of urban settings, Traffic Safety Description and Analysis plays a pivotal role in applications ranging from insurance inspection to accident prevention.","This paper introduces CityLLaVA, a novel fine-tuning framework for Visual Language Models (VLMs) designed for urban scenarios.","CityLLaVA enhances model comprehension and prediction accuracy through (1) employing bounding boxes for optimal visual data preprocessing, including video best-view selection and visual prompt engineering during both training and testing phases; (2) constructing concise Question-Answer sequences and designing textual prompts to refine instruction comprehension; (3) implementing block expansion to fine-tune large VLMs efficiently; and (4) advancing prediction accuracy via a unique sequential questioning-based prediction augmentation.","Demonstrating top-tier performance, our method achieved a benchmark score of 33.4308, securing the leading position on the leaderboard.","The code can be found: https://github.com/alibaba/AICITY2024_Track2_AliOpenTrek_CityLLaVA"],"url":"http://arxiv.org/abs/2405.03194v1","category":"cs.CV"}
{"created":"2024-05-05 20:44:44","title":"Quantum Chemistry Model of Surface Reactions and Kinetic Model of Diamond Growth: Effects of CH3 Radicals and C2H2 Molecules at Low-Temperatures CVD","abstract":"The objective of this study is to explore conditions that facilitate a significant reduction in substrate temperature during diamond growth. The typical temperature for this process is around 1200K; we aim to reduce it to a much lower level. To achieve this, we need to understand processes that limit the diamond growth at low temperatures. Therefore, we developed a detailed chemical kinetic model to analyze diamond growth on the (100) surface. This model accounts for variations in substrate temperature and gas composition. Using an ab initio quantum chemistry, we calculated the reaction rates of all major gas phase reactants with the diamond surface, totaling 91 elemental surface reactions. Consistent with previous studies, the model identifies that CH3 is a major precursor of diamond growth, and the contribution from C2H2 to the growth is significantly smaller. However, C2H2 can also contribute to forming a sp2-phase instead of a sp3-phase, and this process becomes dominant below a critical temperature. As a result, C2H2 flux inhibits diamond growth at low temperatures. To quantify this deleterious process, we developed a new mechanism for sp2-phase nucleation on the (100) surface. Similar to the so-called HACA mechanism for soot formation it involves hydrogen abstraction and C2H2 addition. Consequently, optimal low-temperature CVD growth could be realized in a reactor designed to maximize the CH3 radical production, while minimizing the generation of C2H2 and other sp and sp2 hydrocarbons.","sentences":["The objective of this study is to explore conditions that facilitate a significant reduction in substrate temperature during diamond growth.","The typical temperature for this process is around 1200K; we aim to reduce it to a much lower level.","To achieve this, we need to understand processes that limit the diamond growth at low temperatures.","Therefore, we developed a detailed chemical kinetic model to analyze diamond growth on the (100) surface.","This model accounts for variations in substrate temperature and gas composition.","Using an ab initio quantum chemistry, we calculated the reaction rates of all major gas phase reactants with the diamond surface, totaling 91 elemental surface reactions.","Consistent with previous studies, the model identifies that CH3 is a major precursor of diamond growth, and the contribution from C2H2 to the growth is significantly smaller.","However, C2H2 can also contribute to forming a sp2-phase instead of a sp3-phase, and this process becomes dominant below a critical temperature.","As a result, C2H2 flux inhibits diamond growth at low temperatures.","To quantify this deleterious process, we developed a new mechanism for sp2-phase nucleation on the (100) surface.","Similar to the so-called HACA mechanism for soot formation it involves hydrogen abstraction and C2H2 addition.","Consequently, optimal low-temperature CVD growth could be realized in a reactor designed to maximize the CH3 radical production, while minimizing the generation of C2H2 and other sp and sp2 hydrocarbons."],"url":"http://arxiv.org/abs/2405.03050v1","category":"physics.chem-ph"}
{"created":"2024-05-05 18:39:43","title":"Enhanced Detection Classification via Clustering SVM for Various Robot Collaboration Task","abstract":"We introduce an advanced, swift pattern recognition strategy for various multiple robotics during curve negotiation. This method, leveraging a sophisticated k-means clustering-enhanced Support Vector Machine algorithm, distinctly categorizes robotics into flying or mobile robots. Initially, the paradigm considers robot locations and features as quintessential parameters indicative of divergent robot patterns. Subsequently, employing the k-means clustering technique facilitates the efficient segregation and consolidation of robotic data, significantly optimizing the support vector delineation process and expediting the recognition phase. Following this preparatory phase, the SVM methodology is adeptly applied to construct a discriminative hyperplane, enabling precise classification and prognostication of the robot category. To substantiate the efficacy and superiority of the k-means framework over traditional SVM approaches, a rigorous cross-validation experiment was orchestrated, evidencing the former's enhanced performance in robot group classification.","sentences":["We introduce an advanced, swift pattern recognition strategy for various multiple robotics during curve negotiation.","This method, leveraging a sophisticated k-means clustering-enhanced Support Vector Machine algorithm, distinctly categorizes robotics into flying or mobile robots.","Initially, the paradigm considers robot locations and features as quintessential parameters indicative of divergent robot patterns.","Subsequently, employing the k-means clustering technique facilitates the efficient segregation and consolidation of robotic data, significantly optimizing the support vector delineation process and expediting the recognition phase.","Following this preparatory phase, the SVM methodology is adeptly applied to construct a discriminative hyperplane, enabling precise classification and prognostication of the robot category.","To substantiate the efficacy and superiority of the k-means framework over traditional SVM approaches, a rigorous cross-validation experiment was orchestrated, evidencing the former's enhanced performance in robot group classification."],"url":"http://arxiv.org/abs/2405.03026v1","category":"cs.RO"}
{"created":"2024-05-05 16:24:30","title":"Defense against Joint Poison and Evasion Attacks: A Case Study of DERMS","abstract":"There is an upward trend of deploying distributed energy resource management systems (DERMS) to control modern power grids. However, DERMS controller communication lines are vulnerable to cyberattacks that could potentially impact operational reliability. While a data-driven intrusion detection system (IDS) can potentially thwart attacks during deployment, also known as the evasion attack, the training of the detection algorithm may be corrupted by adversarial data injected into the database, also known as the poisoning attack. In this paper, we propose the first framework of IDS that is robust against joint poisoning and evasion attacks. We formulate the defense mechanism as a bilevel optimization, where the inner and outer levels deal with attacks that occur during training time and testing time, respectively. We verify the robustness of our method on the IEEE-13 bus feeder model against a diverse set of poisoning and evasion attack scenarios. The results indicate that our proposed method outperforms the baseline technique in terms of accuracy, precision, and recall for intrusion detection.","sentences":["There is an upward trend of deploying distributed energy resource management systems (DERMS) to control modern power grids.","However, DERMS controller communication lines are vulnerable to cyberattacks that could potentially impact operational reliability.","While a data-driven intrusion detection system (IDS) can potentially thwart attacks during deployment, also known as the evasion attack, the training of the detection algorithm may be corrupted by adversarial data injected into the database, also known as the poisoning attack.","In this paper, we propose the first framework of IDS that is robust against joint poisoning and evasion attacks.","We formulate the defense mechanism as a bilevel optimization, where the inner and outer levels deal with attacks that occur during training time and testing time, respectively.","We verify the robustness of our method on the IEEE-13 bus feeder model against a diverse set of poisoning and evasion attack scenarios.","The results indicate that our proposed method outperforms the baseline technique in terms of accuracy, precision, and recall for intrusion detection."],"url":"http://arxiv.org/abs/2405.02989v1","category":"cs.CR"}
{"created":"2024-05-05 16:06:46","title":"CVXSADes: a stochastic algorithm for constructing optimal exact regression designs with single or multiple objectives","abstract":"We propose an algorithm to construct optimal exact designs (EDs). Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results. To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them. Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps. The first step involves constructing an AD by utilizing the convex nature of many design criteria. The second step employs a simulated annealing algorithm to search for the ED stochastically. Through several applications, we demonstrate the utility of our method for various design problems. Additionally, we show that the design efficiency approaches unity as the number of design points increases.","sentences":["We propose an algorithm to construct optimal exact designs (EDs).","Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974).","ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results.","To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them.","Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps.","The first step involves constructing an AD by utilizing the convex nature of many design criteria.","The second step employs a simulated annealing algorithm to search for the ED stochastically.","Through several applications, we demonstrate the utility of our method for various design problems.","Additionally, we show that the design efficiency approaches unity as the number of design points increases."],"url":"http://arxiv.org/abs/2405.02983v1","category":"stat.ME"}
{"created":"2024-05-05 15:52:22","title":"A Long-Short-Term Mixed-Integer Formulation for Highway Lane Change Planning","abstract":"This work considers the problem of optimal lane changing in a structured multi-agent road environment. A novel motion planning algorithm that can capture long-horizon dependencies as well as short-horizon dynamics is presented. Pivotal to our approach is a geometric approximation of the long-horizon combinatorial transition problem which we formulate in the continuous time-space domain. Moreover, a discrete-time formulation of a short-horizon optimal motion planning problem is formulated and combined with the long-horizon planner. Both individual problems, as well as their combination, are formulated as MIQP and solved in real-time by using state-of-the-art solvers. We show how the presented algorithm outperforms two other state-of-the-art motion planning algorithms in closed-loop performance and computation time in lane changing problems. Evaluations are performed using the traffic simulator SUMO, a custom low-level tracking model predictive controller, and high-fidelity vehicle models and scenarios, provided by the CommonRoad environment.","sentences":["This work considers the problem of optimal lane changing in a structured multi-agent road environment.","A novel motion planning algorithm that can capture long-horizon dependencies as well as short-horizon dynamics is presented.","Pivotal to our approach is a geometric approximation of the long-horizon combinatorial transition problem which we formulate in the continuous time-space domain.","Moreover, a discrete-time formulation of a short-horizon optimal motion planning problem is formulated and combined with the long-horizon planner.","Both individual problems, as well as their combination, are formulated as MIQP and solved in real-time by using state-of-the-art solvers.","We show how the presented algorithm outperforms two other state-of-the-art motion planning algorithms in closed-loop performance and computation time in lane changing problems.","Evaluations are performed using the traffic simulator SUMO, a custom low-level tracking model predictive controller, and high-fidelity vehicle models and scenarios, provided by the CommonRoad environment."],"url":"http://arxiv.org/abs/2405.02979v1","category":"cs.RO"}
{"created":"2024-05-05 15:33:02","title":"FairRelay: Fair and Cost-Efficient Peer-to-Peer Content Delivery through Payment Channel Networks","abstract":"Peer-to-Peer (P2P) content delivery, known for scalability and resilience, offers a decentralized alternative to traditional centralized Content Delivery Networks (CDNs). A significant challenge in P2P content delivery remains: the fair compensation of relayers for their bandwidth contributions. Existing solutions employ blockchains for payment settlements, however, they are not practical due to high on-chain costs and over-simplified network assumptions. In this paper, we introduce FairRelay, a fair and cost-efficient protocol that ensures all participants get fair payoff in complex content delivery network settings. We introduce a novel primitive, Enforceable Accumulative Hashed TimeLock Contract (Enforceable A-HTLC), designed to guarantee payment atomicity - ensuring all participants receive their payments upon successful content delivery.   The fairness of FairRelay is proved using the Universal Composability (UC) framework. Our evaluation demonstrates that, in optimistic scenarios, FairRelay employs zero on-chain costs. In pessimistic scenarios, the on-chain dispute costs for relayers and customers are constant, irrespective of the network complexity. Specifically, empirical results indicate that the on-chain dispute costs for relayers and customers are 24,902 gas (equivalent to 0.01 USD on Optimism L2) and 290,797 gas (0.07 USD), respectively. In a 10-hop relay path, FairRelay introduces less than 1.5% additional overhead compared to pure data transmission, showcasing the efficiency of FairRelay.","sentences":["Peer-to-Peer (P2P) content delivery, known for scalability and resilience, offers a decentralized alternative to traditional centralized Content Delivery Networks (CDNs).","A significant challenge in P2P content delivery remains: the fair compensation of relayers for their bandwidth contributions.","Existing solutions employ blockchains for payment settlements, however, they are not practical due to high on-chain costs and over-simplified network assumptions.","In this paper, we introduce FairRelay, a fair and cost-efficient protocol that ensures all participants get fair payoff in complex content delivery network settings.","We introduce a novel primitive, Enforceable Accumulative Hashed TimeLock Contract (Enforceable A-HTLC), designed to guarantee payment atomicity - ensuring all participants receive their payments upon successful content delivery.   ","The fairness of FairRelay is proved using the Universal Composability (UC) framework.","Our evaluation demonstrates that, in optimistic scenarios, FairRelay employs zero on-chain costs.","In pessimistic scenarios, the on-chain dispute costs for relayers and customers are constant, irrespective of the network complexity.","Specifically, empirical results indicate that the on-chain dispute costs for relayers and customers are 24,902 gas (equivalent to 0.01 USD on Optimism L2) and 290,797 gas (0.07 USD), respectively.","In a 10-hop relay path, FairRelay introduces less than 1.5% additional overhead compared to pure data transmission, showcasing the efficiency of FairRelay."],"url":"http://arxiv.org/abs/2405.02973v1","category":"cs.CR"}
{"created":"2024-05-05 15:01:29","title":"VectorPainter: A Novel Approach to Stylized Vector Graphics Synthesis with Vectorized Strokes","abstract":"We propose a novel method, VectorPainter, for the task of stylized vector graphics synthesis. Given a text prompt and a reference style image, VectorPainter generates a vector graphic that aligns in content with the text prompt and remains faithful in style to the reference image. We recognize that the key to this task lies in fully leveraging the intrinsic properties of vector graphics. Innovatively, we conceptualize the stylization process as the rearrangement of vectorized strokes extracted from the reference image. VectorPainter employs an optimization-based pipeline. It begins by extracting vectorized strokes from the reference image, which are then used to initialize the synthesis process. To ensure fidelity to the reference style, a novel style preservation loss is introduced. Extensive experiments have been conducted to demonstrate that our method is capable of aligning with the text description while remaining faithful to the reference image.","sentences":["We propose a novel method, VectorPainter, for the task of stylized vector graphics synthesis.","Given a text prompt and a reference style image, VectorPainter generates a vector graphic that aligns in content with the text prompt and remains faithful in style to the reference image.","We recognize that the key to this task lies in fully leveraging the intrinsic properties of vector graphics.","Innovatively, we conceptualize the stylization process as the rearrangement of vectorized strokes extracted from the reference image.","VectorPainter employs an optimization-based pipeline.","It begins by extracting vectorized strokes from the reference image, which are then used to initialize the synthesis process.","To ensure fidelity to the reference style, a novel style preservation loss is introduced.","Extensive experiments have been conducted to demonstrate that our method is capable of aligning with the text description while remaining faithful to the reference image."],"url":"http://arxiv.org/abs/2405.02962v1","category":"cs.CV"}
{"created":"2024-05-05 13:01:04","title":"Constructing $(h,d)$ cooperative MSR codes with sub-packetization $(d-k+h)(d-k+1)^{\\lceil n/2 \\rceil}$","abstract":"We address the multi-node failure repair challenges for MDS array codes. Presently, two primary models are employed for multi-node repairs: the centralized model where all failed nodes are restored in a singular data center, and the cooperative model where failed nodes acquire data from auxiliary nodes and collaborate amongst themselves for the repair process.This paper focuses on the cooperative model, and we provide explicit constructions of optimal MDS array codes with $d$ helper nodes under this model. The sub-packetization level of our new codes is $(d-k+h)(d-k+1)^{\\lceil n/2 \\rceil}$ where $h$ is the number of failed nodes, $k$ the number of information nodes and $n$ the code length. This improves upon recent constructions given by Liu \\emph{et al.} (IEEE Transactions on Information Theory, Vol. 69, 2023).","sentences":["We address the multi-node failure repair challenges for MDS array codes.","Presently, two primary models are employed for multi-node repairs: the centralized model where all failed nodes are restored in a singular data center, and the cooperative model where failed nodes acquire data from auxiliary nodes and collaborate amongst themselves for the repair process.","This paper focuses on the cooperative model, and we provide explicit constructions of optimal MDS array codes with $d$ helper nodes under this model.","The sub-packetization level of our new codes is $(d-k+h)(d-k+1)^{\\lceil n/2 \\rceil}$ where $h$ is the number of failed nodes, $k$ the number of information nodes and $n$ the code length.","This improves upon recent constructions given by Liu \\emph{et al.}","(IEEE Transactions on Information Theory, Vol. 69, 2023)."],"url":"http://arxiv.org/abs/2405.02923v1","category":"cs.IT"}
{"created":"2024-05-05 12:58:38","title":"Polarimeter optical spectrum analyzer","abstract":"A coherent optical spectrum analyzer is integrated with a rotating quarter wave plate polarimeter. The combined polarimeter optical spectrum analyzer (POSA) allows the extraction of the state of polarization with high spectral resolution. POSA is used in this work to study two optical systems. The first is an optical modulator based on a ferrimagnetic sphere resonator. POSA is employed to explore the underlying magneto-optical mechanism responsible for modulation sideband asymmetry. The second system under study is a cryogenic fiber loop laser, which produces an unequally spaced optical comb. Polarization measurements provide insights on the nonlinear processes responsible for comb creation. Characterizations extracted from POSA data provide guidelines for performance optimization of applications based on these systems under study.","sentences":["A coherent optical spectrum analyzer is integrated with a rotating quarter wave plate polarimeter.","The combined polarimeter optical spectrum analyzer (POSA) allows the extraction of the state of polarization with high spectral resolution.","POSA is used in this work to study two optical systems.","The first is an optical modulator based on a ferrimagnetic sphere resonator.","POSA is employed to explore the underlying magneto-optical mechanism responsible for modulation sideband asymmetry.","The second system under study is a cryogenic fiber loop laser, which produces an unequally spaced optical comb.","Polarization measurements provide insights on the nonlinear processes responsible for comb creation.","Characterizations extracted from POSA data provide guidelines for performance optimization of applications based on these systems under study."],"url":"http://arxiv.org/abs/2405.02920v1","category":"physics.optics"}
{"created":"2024-05-05 12:55:02","title":"Hedge Error Analysis In Black Scholes Option Pricing Model: An Asymptotic Approach Towards Finite Difference","abstract":"The Black-Scholes option pricing model remains a cornerstone in financial mathematics, yet its application is often challenged by the need for accurate hedging strategies, especially in dynamic market environments. This paper presents a rigorous analysis of hedge errors within the Black-Scholes framework, focusing on the efficacy of finite difference techniques in calculating option sensitivities. Employing an asymptotic approach, we investigate the behavior of hedge errors under various market conditions, emphasizing the implications for risk management and portfolio optimization. Through theoretical analysis and numerical simulations, we demonstrate the effectiveness of our proposed method in reducing hedge errors and enhancing the robustness of option pricing models. Our findings provide valuable insights into improving the accuracy of hedging strategies and advancing the understanding of option pricing in financial markets.","sentences":["The Black-Scholes option pricing model remains a cornerstone in financial mathematics, yet its application is often challenged by the need for accurate hedging strategies, especially in dynamic market environments.","This paper presents a rigorous analysis of hedge errors within the Black-Scholes framework, focusing on the efficacy of finite difference techniques in calculating option sensitivities.","Employing an asymptotic approach, we investigate the behavior of hedge errors under various market conditions, emphasizing the implications for risk management and portfolio optimization.","Through theoretical analysis and numerical simulations, we demonstrate the effectiveness of our proposed method in reducing hedge errors and enhancing the robustness of option pricing models.","Our findings provide valuable insights into improving the accuracy of hedging strategies and advancing the understanding of option pricing in financial markets."],"url":"http://arxiv.org/abs/2405.02919v1","category":"q-fin.MF"}
{"created":"2024-05-05 12:41:33","title":"Close Encounters of Wide Binaries Induced by the Galactic Tide: Implications for Stellar Mergers and Gravitational-Wave Sources","abstract":"A substantial fraction of stars can be found in wide binaries with projected separations between $\\sim10^2$ and $10^5\\,\\rm AU$. In the standard lore of binary physics, these would evolve as effectively single stars that remotely orbit one another on stationary Keplerian ellipses. However, embedded in their Galactic environment their low binding energy makes them exceptionally prone to perturbations from the gravitational potential of the Milky Way and encounters with passing stars. Employing a fully relativistic $N$-body integration scheme, we study the impact of these perturbations on the orbital evolution of wide binaries along their trajectory through the Milky Way. Our analysis reveals that the torques exerted by the Galaxy can cause large-amplitude oscillations of the binary eccentricity to $1-e\\lesssim10^{-8}$. As a consequence, the wide binary members pass close to each other at periapsis, which, depending on the type of binary, potentially leads to a mass transfer or collision of stars or to an inspiral and subsequent merger of compact remnants due to gravitational-wave radiation. Based on a simulation of $10^5$ wide binaries across the Galactic field, we find that this mechanism could significantly contribute to the rate of stellar collisions and binary black hole mergers as inferred from observations of Luminous Red Novae and gravitational-wave events by LIGO/Virgo/Kagra. We conclude that the dynamics of wide binaries, despite their large mean separation, can give rise to extreme interactions between stars and compact remnants.","sentences":["A substantial fraction of stars can be found in wide binaries with projected separations between $\\sim10^2$ and $10^5\\,\\rm","AU$.","In the standard lore of binary physics, these would evolve as effectively single stars that remotely orbit one another on stationary Keplerian ellipses.","However, embedded in their Galactic environment their low binding energy makes them exceptionally prone to perturbations from the gravitational potential of the Milky Way and encounters with passing stars.","Employing a fully relativistic $N$-body integration scheme, we study the impact of these perturbations on the orbital evolution of wide binaries along their trajectory through the Milky Way.","Our analysis reveals that the torques exerted by the Galaxy can cause large-amplitude oscillations of the binary eccentricity to $1-e\\lesssim10^{-8}$. As a consequence, the wide binary members pass close to each other at periapsis, which, depending on the type of binary, potentially leads to a mass transfer or collision of stars or to an inspiral and subsequent merger of compact remnants due to gravitational-wave radiation.","Based on a simulation of $10^5$ wide binaries across the Galactic field, we find that this mechanism could significantly contribute to the rate of stellar collisions and binary black hole mergers as inferred from observations of Luminous Red Novae and gravitational-wave events by LIGO/Virgo/Kagra.","We conclude that the dynamics of wide binaries, despite their large mean separation, can give rise to extreme interactions between stars and compact remnants."],"url":"http://arxiv.org/abs/2405.02912v1","category":"astro-ph.GA"}
{"created":"2024-05-05 12:31:51","title":"Optical spectra of silver clusters and nanoparticles of all sizes from the TDDFT+U method","abstract":"The localized surface-plasmon resonances (LSPRs) of coinage-metal clusters and nanoparticles provide the basis for a great number of applications, the conception and necessary optimization of which require precise theoretical description and understanding. However, for the size range from clusters of a few atoms through nanoparticles of a few nanometers, where quantum effects and atomistic structure play a significant role, none of the methods employed to date has been able to provide high-quality spectra for all sizes. The main problem is the description of the filled shells of d electrons which influence the optical response decisively. In the present work we show that the DFT+U method, employed with real-time time-dependent density-functional theory calculations (RT-TDDFT), provides spectra in good agreement with experiment for silver clusters ranging from 4 to 923 atoms, the latter representing a nanoparticle with a diameter of 3 nm. Both the electron-hole-type discrete spectra of the smallest clusters and the broad plasmon resonances of the larger sizes are obtained. All calculations have been carried out using the same value of the effective U parameter that has been found to provide good results in bulk silver. The agreement with experiment for all sizes shows that the U parameter is surprisingly transferable. Our results open the pathway for TDDFT calculations of many practically relevant systems including clusters coupled to bio-molecules or to other nano-objects.","sentences":["The localized surface-plasmon resonances (LSPRs) of coinage-metal clusters and nanoparticles provide the basis for a great number of applications, the conception and necessary optimization of which require precise theoretical description and understanding.","However, for the size range from clusters of a few atoms through nanoparticles of a few nanometers, where quantum effects and atomistic structure play a significant role, none of the methods employed to date has been able to provide high-quality spectra for all sizes.","The main problem is the description of the filled shells of d electrons which influence the optical response decisively.","In the present work we show that the DFT+U method, employed with real-time time-dependent density-functional theory calculations (RT-TDDFT), provides spectra in good agreement with experiment for silver clusters ranging from 4 to 923 atoms, the latter representing a nanoparticle with a diameter of 3 nm.","Both the electron-hole-type discrete spectra of the smallest clusters and the broad plasmon resonances of the larger sizes are obtained.","All calculations have been carried out using the same value of the effective U parameter that has been found to provide good results in bulk silver.","The agreement with experiment for all sizes shows that the U parameter is surprisingly transferable.","Our results open the pathway for TDDFT calculations of many practically relevant systems including clusters coupled to bio-molecules or to other nano-objects."],"url":"http://arxiv.org/abs/2405.02910v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-05 11:48:50","title":"Predicting Open-Hole Laminates Failure Using Support Vector Machines With Classical and Quantum Kernels","abstract":"Modeling open hole failure of composites is a complex task, consisting in a highly nonlinear response with interacting failure modes. Numerical modeling of this phenomenon has traditionally been based on the finite element method, but requires to tradeoff between high fidelity and computational cost. To mitigate this shortcoming, recent work has leveraged machine learning to predict the strength of open hole composite specimens. Here, we also propose using data-based models but to tackle open hole composite failure from a classification point of view. More specifically, we show how to train surrogate models to learn the ultimate failure envelope of an open hole composite plate under in-plane loading. To achieve this, we solve the classification problem via support vector machine (SVM) and test different classifiers by changing the SVM kernel function. The flexibility of kernel-based SVM also allows us to integrate the recently developed quantum kernels in our algorithm and compare them with the standard radial basis function (RBF) kernel. Finally, thanks to kernel-target alignment optimization, we tune the free parameters of all kernels to best separate safe and failure-inducing loading states. The results show classification accuracies higher than 90% for RBF, especially after alignment, followed closely by the quantum kernel classifiers.","sentences":["Modeling open hole failure of composites is a complex task, consisting in a highly nonlinear response with interacting failure modes.","Numerical modeling of this phenomenon has traditionally been based on the finite element method, but requires to tradeoff between high fidelity and computational cost.","To mitigate this shortcoming, recent work has leveraged machine learning to predict the strength of open hole composite specimens.","Here, we also propose using data-based models but to tackle open hole composite failure from a classification point of view.","More specifically, we show how to train surrogate models to learn the ultimate failure envelope of an open hole composite plate under in-plane loading.","To achieve this, we solve the classification problem via support vector machine (SVM) and test different classifiers by changing the SVM kernel function.","The flexibility of kernel-based SVM also allows us to integrate the recently developed quantum kernels in our algorithm and compare them with the standard radial basis function (RBF) kernel.","Finally, thanks to kernel-target alignment optimization, we tune the free parameters of all kernels to best separate safe and failure-inducing loading states.","The results show classification accuracies higher than 90% for RBF, especially after alignment, followed closely by the quantum kernel classifiers."],"url":"http://arxiv.org/abs/2405.02903v1","category":"cs.CE"}
{"created":"2024-05-05 11:28:12","title":"Nonclassical effects of photon-phonon antibunching in a multifield driven optomechanical cavity","abstract":"The nonclassical signature of a photon-phonon pair can be tested effectively by violating Cauchy-Schwarz and Bell's inequality, which can arise due to antibunching phenomena in coupled bosonic systems. In this paper, we analyze the measurement criteria imposed on the second-order coherence functions and investigate the quantum correlations leading to the suppression of multi-photon-phonon excitation in a single optomechanical cavity upon driving it with two pumping fields. It is also shown that the Cauchy-Schwarz violation can serve as an ideal precursor to demonstrate stronger tests of locality related to Bell's inequality. We consider weak driving and optomechanical coupling coefficient parameters in the system that enables the unconventional nature of photon (phonon) blockades while operating in the resonance of cavity detuning and mechanical frequency. These findings are valuable for generating sub-Poissonian signals in optimal conditions and have potential applications in hybrid systems for on-demand single photon (phonon) detection.","sentences":["The nonclassical signature of a photon-phonon pair can be tested effectively by violating Cauchy-Schwarz and Bell's inequality, which can arise due to antibunching phenomena in coupled bosonic systems.","In this paper, we analyze the measurement criteria imposed on the second-order coherence functions and investigate the quantum correlations leading to the suppression of multi-photon-phonon excitation in a single optomechanical cavity upon driving it with two pumping fields.","It is also shown that the Cauchy-Schwarz violation can serve as an ideal precursor to demonstrate stronger tests of locality related to Bell's inequality.","We consider weak driving and optomechanical coupling coefficient parameters in the system that enables the unconventional nature of photon (phonon) blockades while operating in the resonance of cavity detuning and mechanical frequency.","These findings are valuable for generating sub-Poissonian signals in optimal conditions and have potential applications in hybrid systems for on-demand single photon (phonon) detection."],"url":"http://arxiv.org/abs/2405.02896v1","category":"quant-ph"}
{"created":"2024-05-05 09:43:04","title":"Continuous Monitoring for Road Flooding With Satellite Onboard Computing For Navigation for OrbitalAI \u03a6sat-2 challenge","abstract":"Continuous monitoring for road flooding could be achieved through onboard computing of satellite imagery to generate near real-time insights made available to generate dynamic information for maps used for navigation. Given the existing computing hardware like the one considered for the PhiSat-2 mission, the paper describes the feasibility of running the road flooding detection. The simulated onboard imagery dataset development and its annotation process for the OrbitalAI {\\Phi}sat-2 challenge is described. The flooding events in the city of Bengaluru, India were considered for this challenge. This is followed by the model architecture selection, training, optimization and accuracy results for the model. The results indicate that it is possible to build low size, high accuracy models for the road flooding use case.","sentences":["Continuous monitoring for road flooding could be achieved through onboard computing of satellite imagery to generate near real-time insights made available to generate dynamic information for maps used for navigation.","Given the existing computing hardware like the one considered for the PhiSat-2 mission, the paper describes the feasibility of running the road flooding detection.","The simulated onboard imagery dataset development and its annotation process for the OrbitalAI {\\Phi}sat-2 challenge is described.","The flooding events in the city of Bengaluru, India were considered for this challenge.","This is followed by the model architecture selection, training, optimization and accuracy results for the model.","The results indicate that it is possible to build low size, high accuracy models for the road flooding use case."],"url":"http://arxiv.org/abs/2405.02868v1","category":"cs.RO"}
{"created":"2024-05-05 09:38:49","title":"Universal exponential pointwise convergence for weighted multiple ergodic averages over $ \\mathbb{T}^\\infty $","abstract":"By employing an accelerated weighting method, we establish arbitrary polynomial and exponential pointwise convergence for multiple ergodic averages under general conditions in both discrete and continuous settings, involving quasi-periodic and almost periodic cases, which breaks the well known slow convergence rate observed in classical ergodic theory. We also present joint Diophantine rotations as explicit applications. Especially, in the sense that excluding nearly rational rotations with zero measure, we demonstrate that the pointwise exponential convergence is universal via analytic observables, even when multiplicatively averaging over the infinite-dimensional torus $ \\mathbb{T}^\\infty $, utilizing a novel truncated approach. Moreover, by constructing counterexamples concerning with multiple ergodicity, we highlight the irremovability of the joint nonresonance and establish the optimality of our weighting method in preserving rapid convergence. We also provide numerical simulations with analysis to further illustrate our results.","sentences":["By employing an accelerated weighting method, we establish arbitrary polynomial and exponential pointwise convergence for multiple ergodic averages under general conditions in both discrete and continuous settings, involving quasi-periodic and almost periodic cases, which breaks the well known slow convergence rate observed in classical ergodic theory.","We also present joint Diophantine rotations as explicit applications.","Especially, in the sense that excluding nearly rational rotations with zero measure, we demonstrate that the pointwise exponential convergence is universal via analytic observables, even when multiplicatively averaging over the infinite-dimensional torus $ \\mathbb{T}^\\infty $, utilizing a novel truncated approach.","Moreover, by constructing counterexamples concerning with multiple ergodicity, we highlight the irremovability of the joint nonresonance and establish the optimality of our weighting method in preserving rapid convergence.","We also provide numerical simulations with analysis to further illustrate our results."],"url":"http://arxiv.org/abs/2405.02866v1","category":"math.DS"}
{"created":"2024-05-05 09:04:42","title":"MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior","abstract":"Despite the emergence of successful NeRF inpainting methods built upon explicit RGB and depth 2D inpainting supervisions, these methods are inherently constrained by the capabilities of their underlying 2D inpainters. This is due to two key reasons: (i) independently inpainting constituent images results in view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure high-quality geometry completion and alignment with inpainted RGB images.   To overcome these limitations, we propose a novel approach called MVIP-NeRF that harnesses the potential of diffusion priors for NeRF inpainting, addressing both appearance and geometry aspects. MVIP-NeRF performs joint inpainting across multiple views to reach a consistent solution, which is achieved via an iterative optimization process based on Score Distillation Sampling (SDS). Apart from recovering the rendered RGB images, we also extract normal maps as a geometric representation and define a normal SDS loss that motivates accurate geometry inpainting and alignment with the appearance. Additionally, we formulate a multi-view SDS score function to distill generative priors simultaneously from different view images, ensuring consistent visual completion when dealing with large view variations. Our experimental results show better appearance and geometry recovery than previous NeRF inpainting methods.","sentences":["Despite the emergence of successful NeRF inpainting methods built upon explicit RGB and depth 2D inpainting supervisions, these methods are inherently constrained by the capabilities of their underlying 2D inpainters.","This is due to two key reasons: (i) independently inpainting constituent images results in view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure high-quality geometry completion and alignment with inpainted RGB images.   ","To overcome these limitations, we propose a novel approach called MVIP-NeRF that harnesses the potential of diffusion priors for NeRF inpainting, addressing both appearance and geometry aspects.","MVIP-NeRF performs joint inpainting across multiple views to reach a consistent solution, which is achieved via an iterative optimization process based on Score Distillation Sampling (SDS).","Apart from recovering the rendered RGB images, we also extract normal maps as a geometric representation and define a normal SDS loss that motivates accurate geometry inpainting and alignment with the appearance.","Additionally, we formulate a multi-view SDS score function to distill generative priors simultaneously from different view images, ensuring consistent visual completion when dealing with large view variations.","Our experimental results show better appearance and geometry recovery than previous NeRF inpainting methods."],"url":"http://arxiv.org/abs/2405.02859v1","category":"cs.CV"}
{"created":"2024-05-05 09:02:54","title":"Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation","abstract":"Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial role in global communication but often encounter strict regulations in geopolitically sensitive regions. This situation has prompted users to ingeniously modify their way of communicating, frequently resorting to coded language in these regulated social media environments. This shift in communication is not merely a strategy to counteract regulation, but a vivid manifestation of language evolution, demonstrating how language naturally evolves under societal and technological pressures. Studying the evolution of language in regulated social media contexts is of significant importance for ensuring freedom of speech, optimizing content moderation, and advancing linguistic research. This paper proposes a multi-agent simulation framework using Large Language Models (LLMs) to explore the evolution of user language in regulated social media environments. The framework employs LLM-driven agents: supervisory agent who enforce dialogue supervision and participant agents who evolve their language strategies while engaging in conversation, simulating the evolution of communication styles under strict regulations aimed at evading social media regulation. The study evaluates the framework's effectiveness through a range of scenarios from abstract scenarios to real-world situations. Key findings indicate that LLMs are capable of simulating nuanced language dynamics and interactions in constrained settings, showing improvement in both evading supervision and information accuracy as evolution progresses. Furthermore, it was found that LLM agents adopt different strategies for different scenarios.","sentences":["Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial role in global communication but often encounter strict regulations in geopolitically sensitive regions.","This situation has prompted users to ingeniously modify their way of communicating, frequently resorting to coded language in these regulated social media environments.","This shift in communication is not merely a strategy to counteract regulation, but a vivid manifestation of language evolution, demonstrating how language naturally evolves under societal and technological pressures.","Studying the evolution of language in regulated social media contexts is of significant importance for ensuring freedom of speech, optimizing content moderation, and advancing linguistic research.","This paper proposes a multi-agent simulation framework using Large Language Models (LLMs) to explore the evolution of user language in regulated social media environments.","The framework employs LLM-driven agents: supervisory agent who enforce dialogue supervision and participant agents who evolve their language strategies while engaging in conversation, simulating the evolution of communication styles under strict regulations aimed at evading social media regulation.","The study evaluates the framework's effectiveness through a range of scenarios from abstract scenarios to real-world situations.","Key findings indicate that LLMs are capable of simulating nuanced language dynamics and interactions in constrained settings, showing improvement in both evading supervision and information accuracy as evolution progresses.","Furthermore, it was found that LLM agents adopt different strategies for different scenarios."],"url":"http://arxiv.org/abs/2405.02858v1","category":"cs.SI"}
{"created":"2024-05-05 07:44:57","title":"Bayesian uncertainty quantification for synthesizing superheavy elements","abstract":"To improve the theoretical prediction power for synthesizing superheavy elements beyond Og, a Bayesian uncertainty quantification method is employed to evaluate the uncertainty of the calculated evaporation residue cross sections (ERCS) for the first time. The key parameters of the dinuclear system (DNS) model, such as the diffusion parameter $\\textit{a}$, the damping factor $E_\\mathrm{d}$, and the level-density parameter ratio $a_\\mathrm{f}/a_\\mathrm{n}$ are systematically constrained by the Bayesian analysis of recent ERCS data. One intriguing behavior is shown that the optimal incident energies (OIE) corresponding to the largest ERCS weakly depend on the fission process. We also find that these parameters are strongly correlated and the uncertainty propagation considering the parameters independently is not reasonable. The 2$\\sigma$ confidence level of posterior distributions for $a = 0.58_{-0.01}^{+0.01}$ fm, $E_\\mathrm{d} = 21.9_{-8.6}^{+8.8}$ MeV, and $a_\\mathrm{f}/a_\\mathrm{n} = 1.044_{-0.059}^{+0.061}$ are obtained. Furthermore, the confidence levels of the ERCS and OIE for synthesizing Z = 119 via the reactions($^{54}\\mathrm{Cr}+^{243}\\mathrm{Am}$), (${}^{50}\\mathrm{Ti}+{}^{249}\\mathrm{Bk}$), and (${}^{51}\\mathrm{V}+{}^{248}\\mathrm{Cm}$) are predicted. This work sets the stage for future analyses to explore the OIE and reaction systems for the synthesis of superheavy elements.","sentences":["To improve the theoretical prediction power for synthesizing superheavy elements beyond Og, a Bayesian uncertainty quantification method is employed to evaluate the uncertainty of the calculated evaporation residue cross sections (ERCS) for the first time.","The key parameters of the dinuclear system (DNS) model, such as the diffusion parameter $\\textit{a}$, the damping factor $E_\\mathrm{d}$, and the level-density parameter ratio $a_\\mathrm{f}/a_\\mathrm{n}$ are systematically constrained by the Bayesian analysis of recent ERCS data.","One intriguing behavior is shown that the optimal incident energies (OIE) corresponding to the largest ERCS weakly depend on the fission process.","We also find that these parameters are strongly correlated and the uncertainty propagation considering the parameters independently is not reasonable.","The 2$\\sigma$ confidence level of posterior distributions for $a = 0.58_{-0.01}^{+0.01}$ fm, $E_\\mathrm{d} = 21.9_{-8.6}^{+8.8}$ MeV, and $a_\\mathrm{f}/a_\\mathrm{n} = 1.044_{-0.059}^{+0.061}$ are obtained.","Furthermore, the confidence levels of the ERCS and OIE for synthesizing Z = 119 via the reactions($^{54}\\mathrm{Cr}+^{243}\\mathrm{Am}$), (${}^{50}\\mathrm{Ti}+{}^{249}\\mathrm{Bk}$), and (${}^{51}\\mathrm{V}+{}^{248}\\mathrm{Cm}$) are predicted.","This work sets the stage for future analyses to explore the OIE and reaction systems for the synthesis of superheavy elements."],"url":"http://arxiv.org/abs/2405.02839v1","category":"nucl-th"}
{"created":"2024-05-05 07:23:26","title":"Algorithmic collusion in a two-sided market: A rideshare example","abstract":"With dynamic pricing on the rise, firms are using sophisticated algorithms for price determination. These algorithms are often non-interpretable and there has been a recent interest in their seemingly emergent ability to tacitly collude with each other without any prior communication whatsoever. Most of the previous works investigate algorithmic collusion on simple reinforcement learning (RL) based algorithms operating on a basic market model. Instead, we explore the collusive tendencies of Proximal Policy Optimization (PPO), a state-of-the-art continuous state/action space RL algorithm, on a complex double-sided hierarchical market model of rideshare. For this purpose, we extend a mathematical program network (MPN) based rideshare model to a temporal multi origin-destination setting and use PPO to solve for a repeated duopoly game. Our results indicate that PPO can either converge to a competitive or a collusive equilibrium depending upon the underlying market characteristics, even when the hyper-parameters are held constant.","sentences":["With dynamic pricing on the rise, firms are using sophisticated algorithms for price determination.","These algorithms are often non-interpretable and there has been a recent interest in their seemingly emergent ability to tacitly collude with each other without any prior communication whatsoever.","Most of the previous works investigate algorithmic collusion on simple reinforcement learning (RL) based algorithms operating on a basic market model.","Instead, we explore the collusive tendencies of Proximal Policy Optimization (PPO), a state-of-the-art continuous state/action space RL algorithm, on a complex double-sided hierarchical market model of rideshare.","For this purpose, we extend a mathematical program network (MPN) based rideshare model to a temporal multi origin-destination setting and use PPO to solve for a repeated duopoly game.","Our results indicate that PPO can either converge to a competitive or a collusive equilibrium depending upon the underlying market characteristics, even when the hyper-parameters are held constant."],"url":"http://arxiv.org/abs/2405.02835v1","category":"cs.GT"}
{"created":"2024-05-05 06:26:21","title":"Probabilistic tube-based control synthesis of stochastic multi-agent systems under signal temporal logic","abstract":"We consider the control design of stochastic discrete-time linear multi-agent systems (MASs) under a global signal temporal logic (STL) specification to be satisfied at a predefined probability. By decomposing the dynamics into deterministic and error components, we construct a probabilistic reachable tube (PRT) as the Cartesian product of reachable sets of the individual error systems driven by disturbances lying in confidence regions (CRs) with a fixed probability. By bounding the PRT probability with the specification probability, we tighten all state constraints induced by the STL specification by solving tractable optimization problems over segments of the PRT, and convert the underlying stochastic problem into a deterministic one. This approach reduces conservatism compared to tightening guided by the STL structure. Additionally, we propose a recursively feasible algorithm to attack the resulting problem by decomposing it into agent-level subproblems, which are solved iteratively according to a scheduling policy. We demonstrate our method on a ten-agent system, where existing approaches are impractical.","sentences":["We consider the control design of stochastic discrete-time linear multi-agent systems (MASs) under a global signal temporal logic (STL) specification to be satisfied at a predefined probability.","By decomposing the dynamics into deterministic and error components, we construct a probabilistic reachable tube (PRT) as the Cartesian product of reachable sets of the individual error systems driven by disturbances lying in confidence regions (CRs) with a fixed probability.","By bounding the PRT probability with the specification probability, we tighten all state constraints induced by the STL specification by solving tractable optimization problems over segments of the PRT, and convert the underlying stochastic problem into a deterministic one.","This approach reduces conservatism compared to tightening guided by the STL structure.","Additionally, we propose a recursively feasible algorithm to attack the resulting problem by decomposing it into agent-level subproblems, which are solved iteratively according to a scheduling policy.","We demonstrate our method on a ten-agent system, where existing approaches are impractical."],"url":"http://arxiv.org/abs/2405.02827v1","category":"eess.SY"}
{"created":"2024-05-05 06:09:58","title":"Reconfigurable Massive MIMO: Precoding Design and Channel Estimation in the Electromagnetic Domain","abstract":"Reconfigurable massive multiple-input multiple-output (RmMIMO) technology offers increased flexibility for future communication systems by exploiting previously untapped degrees of freedom in the electromagnetic (EM) domain. The representation of the traditional spatial domain channel state information (sCSI) limits the insights into the potential of EM domain channel properties, constraining the base station's (BS) utmost capability for precoding design. This paper leverages the EM domain channel state information (eCSI) for radiation pattern design at the BS. We develop an orthogonal decomposition method based on spherical harmonic functions to decompose the radiation pattern into a linear combination of orthogonal bases. By formulating the radiation pattern design as an optimization problem for the projection coefficients over these bases, we develop a manifold optimization-based method for iterative radiation pattern and digital precoder design. To address the eCSI estimation problem, we capitalize on the inherent structure of the channel. Specifically, we propose a subspace-based scheme to reduce the pilot overhead for wideband sCSI estimation. Given the estimated full-band sCSI, we further employ parameterized methods for angle of arrival estimation. Subsequently, the complete eCSI can be reconstructed after estimating the equivalent channel gain via the least squares method. Simulation results demonstrate that, in comparison to traditional mMIMO systems with fixed antenna radiation patterns, the proposed RmMIMO architecture offers significant throughput gains for multi-user transmission at a low channel estimation overhead.","sentences":["Reconfigurable massive multiple-input multiple-output (RmMIMO) technology offers increased flexibility for future communication systems by exploiting previously untapped degrees of freedom in the electromagnetic (EM) domain.","The representation of the traditional spatial domain channel state information (sCSI) limits the insights into the potential of EM domain channel properties, constraining the base station's (BS) utmost capability for precoding design.","This paper leverages the EM domain channel state information (eCSI) for radiation pattern design at the BS.","We develop an orthogonal decomposition method based on spherical harmonic functions to decompose the radiation pattern into a linear combination of orthogonal bases.","By formulating the radiation pattern design as an optimization problem for the projection coefficients over these bases, we develop a manifold optimization-based method for iterative radiation pattern and digital precoder design.","To address the eCSI estimation problem, we capitalize on the inherent structure of the channel.","Specifically, we propose a subspace-based scheme to reduce the pilot overhead for wideband sCSI estimation.","Given the estimated full-band sCSI, we further employ parameterized methods for angle of arrival estimation.","Subsequently, the complete eCSI can be reconstructed after estimating the equivalent channel gain via the least squares method.","Simulation results demonstrate that, in comparison to traditional mMIMO systems with fixed antenna radiation patterns, the proposed RmMIMO architecture offers significant throughput gains for multi-user transmission at a low channel estimation overhead."],"url":"http://arxiv.org/abs/2405.02823v1","category":"cs.IT"}
{"created":"2024-05-05 05:05:15","title":"Model Predictive Control for Joint Ramping and Regulation-Type Service from Distributed Energy Resource Aggregations","abstract":"Distributed energy resources (DERs) such as grid-responsive loads and batteries can be harnessed to provide ramping and regulation services across the grid. This paper concerns the problem of optimal allocation of different classes of DERs, where each class is an aggregation of similar DERs, to balance net-demand forecasts. The resulting resource allocation problem is solved using model-predictive control (MPC) that utilizes a rolling sequence of finite time-horizon constrained optimizations. This is based on the concept that we have more accurate estimates of the load forecast in the short term, so each optimization in the rolling sequence of optimization problems uses more accurate short term load forecasts while ensuring satisfaction of capacity and dynamical constraints. Simulations demonstrate that the MPC solution can indeed reduce the ramping required from bulk generation, while mitigating near-real time grid disturbances.","sentences":["Distributed energy resources (DERs) such as grid-responsive loads and batteries can be harnessed to provide ramping and regulation services across the grid.","This paper concerns the problem of optimal allocation of different classes of DERs, where each class is an aggregation of similar DERs, to balance net-demand forecasts.","The resulting resource allocation problem is solved using model-predictive control (MPC) that utilizes a rolling sequence of finite time-horizon constrained optimizations.","This is based on the concept that we have more accurate estimates of the load forecast in the short term, so each optimization in the rolling sequence of optimization problems uses more accurate short term load forecasts while ensuring satisfaction of capacity and dynamical constraints.","Simulations demonstrate that the MPC solution can indeed reduce the ramping required from bulk generation, while mitigating near-real time grid disturbances."],"url":"http://arxiv.org/abs/2405.02813v1","category":"eess.SY"}
{"created":"2024-05-05 04:28:45","title":"Does Optimal Control Always Benefit from Better Prediction? An Analysis Framework for Predictive Optimal Control","abstract":"The ``prediction + optimal control'' scheme has shown good performance in many applications of automotive, traffic, robot, and building control. In practice, the prediction results are simply considered correct in the optimal control design process. However, in reality, these predictions may never be perfect. Under a conventional stochastic optimal control formulation, it is difficult to answer questions like ``what if the predictions are wrong''. This paper presents an analysis framework for predictive optimal control where the subjective belief about the future is no longer considered perfect. A novel concept called the hidden prediction state is proposed to establish connections among the predictors, the subjective beliefs, the control policies and the objective control performance. Based on this framework, the predictor evaluation problem is analyzed. Three commonly-used predictor evaluation measures, including the mean squared error, the regret and the log-likelihood, are considered. It is shown that neither using the mean square error nor using the likelihood can guarantee a monotonic relationship between the predictor error and the optimal control cost. To guarantee control cost improvement, it is suggested the predictor should be evaluated with the control performance, e.g., using the optimal control cost or the regret to evaluate predictors. Numerical examples and examples from automotive applications with real-world driving data are provided to illustrate the ideas and the results.","sentences":["The ``prediction + optimal control'' scheme has shown good performance in many applications of automotive, traffic, robot, and building control.","In practice, the prediction results are simply considered correct in the optimal control design process.","However, in reality, these predictions may never be perfect.","Under a conventional stochastic optimal control formulation, it is difficult to answer questions like ``what if the predictions are wrong''.","This paper presents an analysis framework for predictive optimal control where the subjective belief about the future is no longer considered perfect.","A novel concept called the hidden prediction state is proposed to establish connections among the predictors, the subjective beliefs, the control policies and the objective control performance.","Based on this framework, the predictor evaluation problem is analyzed.","Three commonly-used predictor evaluation measures, including the mean squared error, the regret and the log-likelihood, are considered.","It is shown that neither using the mean square error nor using the likelihood can guarantee a monotonic relationship between the predictor error and the optimal control cost.","To guarantee control cost improvement, it is suggested the predictor should be evaluated with the control performance, e.g., using the optimal control cost or the regret to evaluate predictors.","Numerical examples and examples from automotive applications with real-world driving data are provided to illustrate the ideas and the results."],"url":"http://arxiv.org/abs/2405.02809v1","category":"eess.SY"}
{"created":"2024-05-05 03:25:25","title":"Is Flash Attention Stable?","abstract":"Training large-scale machine learning models poses distinct system challenges, given both the size and complexity of today's workloads. Recently, many organizations training state-of-the-art Generative AI models have reported cases of instability during training, often taking the form of loss spikes. Numeric deviation has emerged as a potential cause of this training instability, although quantifying this is especially challenging given the costly nature of training runs. In this work, we develop a principled approach to understanding the effects of numeric deviation, and construct proxies to put observations into context when downstream effects are difficult to quantify. As a case study, we apply this framework to analyze the widely-adopted Flash Attention optimization. We find that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. We then use a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training.","sentences":["Training large-scale machine learning models poses distinct system challenges, given both the size and complexity of today's workloads.","Recently, many organizations training state-of-the-art Generative AI models have reported cases of instability during training, often taking the form of loss spikes.","Numeric deviation has emerged as a potential cause of this training instability, although quantifying this is especially challenging given the costly nature of training runs.","In this work, we develop a principled approach to understanding the effects of numeric deviation, and construct proxies to put observations into context when downstream effects are difficult to quantify.","As a case study, we apply this framework to analyze the widely-adopted Flash Attention optimization.","We find that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass.","We then use a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training."],"url":"http://arxiv.org/abs/2405.02803v1","category":"cs.LG"}
{"created":"2024-05-05 02:29:51","title":"Canonical data-reconstructions via kernels, Hilbert space-valued Gaussian processes, and quantum states","abstract":"We offer new results and new directions in the study of operator-valued kernels and their factorizations. Our approach provides both more explicit realizations and new results, as well as new applications. These include: (i) an explicit covariance analysis for Hilbert space-valued Gaussian processes, (ii) optimization results for quantum gates (from quantum information), (iii) new results for positive operator-valued measures (POVMs), and (iv) a new approach/result in inverse problems for quantum measurements.","sentences":["We offer new results and new directions in the study of operator-valued kernels and their factorizations.","Our approach provides both more explicit realizations and new results, as well as new applications.","These include: (i) an explicit covariance analysis for Hilbert space-valued Gaussian processes, (ii) optimization results for quantum gates (from quantum information), (iii) new results for positive operator-valued measures (POVMs), and (iv) a new approach/result in inverse problems for quantum measurements."],"url":"http://arxiv.org/abs/2405.02796v1","category":"quant-ph"}
{"created":"2024-05-04 22:48:53","title":"Linear Convergence of Independent Natural Policy Gradient in Games with Entropy Regularization","abstract":"This work focuses on the entropy-regularized independent natural policy gradient (NPG) algorithm in multi-agent reinforcement learning. In this work, agents are assumed to have access to an oracle with exact policy evaluation and seek to maximize their respective independent rewards. Each individual's reward is assumed to depend on the actions of all the agents in the multi-agent system, leading to a game between agents. We assume all agents make decisions under a policy with bounded rationality, which is enforced by the introduction of entropy regularization. In practice, a smaller regularization implies the agents are more rational and behave closer to Nash policies. On the other hand, agents with larger regularization acts more randomly, which ensures more exploration. We show that, under sufficient entropy regularization, the dynamics of this system converge at a linear rate to the quantal response equilibrium (QRE). Although regularization assumptions prevent the QRE from approximating a Nash equilibrium, our findings apply to a wide range of games, including cooperative, potential, and two-player matrix games. We also provide extensive empirical results on multiple games (including Markov games) as a verification of our theoretical analysis.","sentences":["This work focuses on the entropy-regularized independent natural policy gradient (NPG) algorithm in multi-agent reinforcement learning.","In this work, agents are assumed to have access to an oracle with exact policy evaluation and seek to maximize their respective independent rewards.","Each individual's reward is assumed to depend on the actions of all the agents in the multi-agent system, leading to a game between agents.","We assume all agents make decisions under a policy with bounded rationality, which is enforced by the introduction of entropy regularization.","In practice, a smaller regularization implies the agents are more rational and behave closer to Nash policies.","On the other hand, agents with larger regularization acts more randomly, which ensures more exploration.","We show that, under sufficient entropy regularization, the dynamics of this system converge at a linear rate to the quantal response equilibrium (QRE).","Although regularization assumptions prevent the QRE from approximating a Nash equilibrium, our findings apply to a wide range of games, including cooperative, potential, and two-player matrix games.","We also provide extensive empirical results on multiple games (including Markov games) as a verification of our theoretical analysis."],"url":"http://arxiv.org/abs/2405.02769v1","category":"cs.LG"}
{"created":"2024-05-04 21:04:44","title":"On the Impact of Dark Matter Scattering on the Trajectory of High-Energy Cosmic Rays","abstract":"We study the impact on the trajectory of high-energy cosmic-ray protons of scattering off the cosmic dark matter. We compute the scattering angle as a function of the cosmic-ray energy, of the dark matter mass, and of the interaction strength for a few representative choices for the relevant interaction cross section. We find that the typical deflection angle over the cosmic ray path is largely independent of the dark matter mass. Given existing limits on the interaction strength, we compute the average deflection angle. We find that for large interaction cross sections and low cosmic ray energies, the predicted deflection angle is much larger than the angular resolution of very high-energy cosmic-ray observatories such as Pierre Auger.","sentences":["We study the impact on the trajectory of high-energy cosmic-ray protons of scattering off the cosmic dark matter.","We compute the scattering angle as a function of the cosmic-ray energy, of the dark matter mass, and of the interaction strength for a few representative choices for the relevant interaction cross section.","We find that the typical deflection angle over the cosmic ray path is largely independent of the dark matter mass.","Given existing limits on the interaction strength, we compute the average deflection angle.","We find that for large interaction cross sections and low cosmic ray energies, the predicted deflection angle is much larger than the angular resolution of very high-energy cosmic-ray observatories such as Pierre Auger."],"url":"http://arxiv.org/abs/2405.02755v1","category":"astro-ph.HE"}
{"created":"2024-05-04 20:58:57","title":"Unscented Trajectory Optimization","abstract":"In a nutshell, unscented trajectory optimization is the generation of optimal trajectories through the use of an unscented transform. Although unscented trajectory optimization was introduced by the authors about a decade ago, it is reintroduced in this paper as a special instantiation of tychastic optimal control theory. Tychastic optimal control theory (from \\textit{Tyche}, the Greek goddess of chance) avoids the use of a Brownian motion and the resulting It\\^{o} calculus even though it uses random variables across the entire spectrum of a problem formulation. This approach circumvents the enormous technical and numerical challenges associated with stochastic trajectory optimization. Furthermore, it is shown how a tychastic optimal control problem that involves nonlinear transformations of the expectation operator can be quickly instantiated using an unscented transform. These nonlinear transformations are particularly useful in managing trajectory dispersions be it associated with path constraints or targeted values of final-time conditions. This paper also presents a systematic and rapid process for formulating and computing the most desirable tychastic trajectory using an unscented transform. Numerical examples are used to illustrate how unscented trajectory optimization may be used for risk reduction and mission recovery caused by uncertainties and failures.","sentences":["In a nutshell, unscented trajectory optimization is the generation of optimal trajectories through the use of an unscented transform.","Although unscented trajectory optimization was introduced by the authors about a decade ago, it is reintroduced in this paper as a special instantiation of tychastic optimal control theory.","Tychastic optimal control theory (from \\textit{Tyche}, the Greek goddess of chance) avoids the use of a Brownian motion and the resulting It\\^{o} calculus even though it uses random variables across the entire spectrum of a problem formulation.","This approach circumvents the enormous technical and numerical challenges associated with stochastic trajectory optimization.","Furthermore, it is shown how a tychastic optimal control problem that involves nonlinear transformations of the expectation operator can be quickly instantiated using an unscented transform.","These nonlinear transformations are particularly useful in managing trajectory dispersions be it associated with path constraints or targeted values of final-time conditions.","This paper also presents a systematic and rapid process for formulating and computing the most desirable tychastic trajectory using an unscented transform.","Numerical examples are used to illustrate how unscented trajectory optimization may be used for risk reduction and mission recovery caused by uncertainties and failures."],"url":"http://arxiv.org/abs/2405.02753v1","category":"math.OC"}
{"created":"2024-05-04 17:46:11","title":"On the role of chirping in pulsed single photon spectroscopy","abstract":"We investigate the precision of estimating the interaction strength between a two-level system (TLS) and a single-photon pulse when the latter is subject to chirping. We consider linear, quadratic, and sinusoidal temporal phases applied to Gaussian and exponential temporal profiles. At the asymptotic time, when the TLS has fully decayed to its ground state, the fundamental precision depends solely on the magnitude of its spectral amplitude. For quadratically phase-modulated Gaussian pulses, this is entirely determined by the spectral bandwidth. We provide expressions for evaluating the fundamental precision for general temporal profiles and phase modulations. Finally, we show that experimentally feasible mode-resolved measurements are optimal, or close to it, for chirped, pulsed single photon spectroscopy.","sentences":["We investigate the precision of estimating the interaction strength between a two-level system (TLS) and a single-photon pulse when the latter is subject to chirping.","We consider linear, quadratic, and sinusoidal temporal phases applied to Gaussian and exponential temporal profiles.","At the asymptotic time, when the TLS has fully decayed to its ground state, the fundamental precision depends solely on the magnitude of its spectral amplitude.","For quadratically phase-modulated Gaussian pulses, this is entirely determined by the spectral bandwidth.","We provide expressions for evaluating the fundamental precision for general temporal profiles and phase modulations.","Finally, we show that experimentally feasible mode-resolved measurements are optimal, or close to it, for chirped, pulsed single photon spectroscopy."],"url":"http://arxiv.org/abs/2405.02723v1","category":"quant-ph"}
{"created":"2024-05-04 16:56:14","title":"CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions","abstract":"Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models.","sentences":["Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks.","We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' reasoning capacity when generating SQL queries.","In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency.","We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain.","We also conduct extensive ablation studies to determine the optimal configuration of our approach.","Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models."],"url":"http://arxiv.org/abs/2405.02712v1","category":"cs.CL"}
{"created":"2024-05-04 16:48:05","title":"Enhancing News Summarization with ELearnFit through Efficient In-Context Learning and Efficient Fine-Tuning","abstract":"With the deluge of information delivered by the daily news cycle, there is a growing need to effectively and efficiently summarize news feeds for quick consumption. We leverage large language models (LLMs), with their advanced learning and generative abilities as compared to conventional language models, to generate concise and coherent summaries for news articles from the XSum dataset. Our paper focuses on two key aspects of LLMs: Efficient in-context Learning (ELearn) and Parameter Efficient Fine-tuning (EFit). Under ELearn, we find that increasing the number of shots in prompts and utilizing simple templates generally improve the quality of summaries. We also find that utilizing relevant examples in few-shot learning for ELearn does not improve model performance. In addition, we studied EFit using different methods and demonstrate that fine-tuning the first layer of LLMs produces better outcomes as compared to fine-tuning other layers or utilizing LoRA. We also find that leveraging more relevant training samples using selective layers does not result in better performance. By combining ELearn and EFit, we create a new model (ELearnFit) that leverages the benefits of both few-shot learning and fine-tuning and produces superior performance to either model alone. We also use ELearnFit to highlight the trade-offs between prompting and fine-tuning, especially for situations where only a limited number of annotated samples are available. Ultimately, our research provides practical techniques to optimize news summarization during the prompting and fine-tuning stages and enhances the synthesis of news articles.","sentences":["With the deluge of information delivered by the daily news cycle, there is a growing need to effectively and efficiently summarize news feeds for quick consumption.","We leverage large language models (LLMs), with their advanced learning and generative abilities as compared to conventional language models, to generate concise and coherent summaries for news articles from the XSum dataset.","Our paper focuses on two key aspects of LLMs: Efficient in-context Learning (ELearn) and Parameter Efficient Fine-tuning (EFit).","Under ELearn, we find that increasing the number of shots in prompts and utilizing simple templates generally improve the quality of summaries.","We also find that utilizing relevant examples in few-shot learning for ELearn does not improve model performance.","In addition, we studied EFit using different methods and demonstrate that fine-tuning the first layer of LLMs produces better outcomes as compared to fine-tuning other layers or utilizing LoRA.","We also find that leveraging more relevant training samples using selective layers does not result in better performance.","By combining ELearn and EFit, we create a new model (ELearnFit) that leverages the benefits of both few-shot learning and fine-tuning and produces superior performance to either model alone.","We also use ELearnFit to highlight the trade-offs between prompting and fine-tuning, especially for situations where only a limited number of annotated samples are available.","Ultimately, our research provides practical techniques to optimize news summarization during the prompting and fine-tuning stages and enhances the synthesis of news articles."],"url":"http://arxiv.org/abs/2405.02710v1","category":"cs.CL"}
{"created":"2024-05-04 15:41:53","title":"Platform Competition in the Autobidding World","abstract":"We study the problem of auction design for advertising platforms that face strategic advertisers who are bidding across platforms. Each advertiser's goal is to maximize their total value or conversions while satisfying some constraint(s) across all the platforms they participates in. In this paper, we focus on advertisers with return-over-investment (henceforth, ROI) constraints, i.e. each advertiser is trying to maximize value while making sure that their ROI across all platforms is no less than some target value. An advertiser interacts with the platforms through autobidders -- for each platform, the advertiser strategically chooses a target ROI to report to the platform's autobidder, which in turn uses a uniform bid multiplier to bid on the advertiser's behalf on the queries owned by the given platform.   Our main result is that for a platform trying to maximize revenue, competition with other platforms is a key factor to consider when designing their auction. While first-price auctions are optimal (for both revenue and welfare) in the absence of competition, this no longer holds true in multi-platform settings. We show that there exists a large class of advertiser valuations over queries such that, from the platform's perspective, running a second price auction dominates running a first price auction.   Furthermore, our analysis reveals the key factors influencing platform choice of auction format: (i) intensity of competition among advertisers, (ii) sensitivity of bid landscapes to an auction change (driven by advertiser sensitivity to price changes), and (iii) relative inefficiency of second-price auctions compared to first-price auctions.","sentences":["We study the problem of auction design for advertising platforms that face strategic advertisers who are bidding across platforms.","Each advertiser's goal is to maximize their total value or conversions while satisfying some constraint(s) across all the platforms they participates in.","In this paper, we focus on advertisers with return-over-investment (henceforth, ROI) constraints, i.e. each advertiser is trying to maximize value while making sure that their ROI across all platforms is no less than some target value.","An advertiser interacts with the platforms through autobidders -- for each platform, the advertiser strategically chooses a target ROI to report to the platform's autobidder, which in turn uses a uniform bid multiplier to bid on the advertiser's behalf on the queries owned by the given platform.   ","Our main result is that for a platform trying to maximize revenue, competition with other platforms is a key factor to consider when designing their auction.","While first-price auctions are optimal (for both revenue and welfare) in the absence of competition, this no longer holds true in multi-platform settings.","We show that there exists a large class of advertiser valuations over queries such that, from the platform's perspective, running a second price auction dominates running a first price auction.   ","Furthermore, our analysis reveals the key factors influencing platform choice of auction format: (i) intensity of competition among advertisers, (ii) sensitivity of bid landscapes to an auction change (driven by advertiser sensitivity to price changes), and (iii) relative inefficiency of second-price auctions compared to first-price auctions."],"url":"http://arxiv.org/abs/2405.02699v1","category":"cs.GT"}
{"created":"2024-05-06 17:57:06","title":"Species scale, worldsheet CFTs and emergent geometry","abstract":"We study infinite-distance limits in the moduli space of perturbative string vacua. The remarkable interplay of string dualities seems to determine a highly non-trivial dichotomy, summarized by the emergent string conjecture, by which in some duality frame either internal dimensions decompactify or a unique critical string becomes tensionless. Assuming the existence of light states, we investigate whether this pattern persists in potentially non-geometric settings, showing that (a proxy for) the cutoff of the gravitational effective field theory in perturbative type II vacua scales with the spectral gap of the internal conformal field theory in the same manner as in decompactification or emergent string limits, regardless of supersymmetry or whether the internal sector is geometric. As a byproduct, we elucidate the role of the species scale in (de)compactifications and show compatibility between effective field theory and worldsheet approaches in the presence of curvature deformations in geometric settings.","sentences":["We study infinite-distance limits in the moduli space of perturbative string vacua.","The remarkable interplay of string dualities seems to determine a highly non-trivial dichotomy, summarized by the emergent string conjecture, by which in some duality frame either internal dimensions decompactify or a unique critical string becomes tensionless.","Assuming the existence of light states, we investigate whether this pattern persists in potentially non-geometric settings, showing that (a proxy for) the cutoff of the gravitational effective field theory in perturbative type II vacua scales with the spectral gap of the internal conformal field theory in the same manner as in decompactification or emergent string limits, regardless of supersymmetry or whether the internal sector is geometric.","As a byproduct, we elucidate the role of the species scale in (de)compactifications and show compatibility between effective field theory and worldsheet approaches in the presence of curvature deformations in geometric settings."],"url":"http://arxiv.org/abs/2405.03683v1","category":"hep-th"}
{"created":"2024-05-06 17:45:56","title":"IMELL Cut Elimination with Linear Overhead","abstract":"Recently, Accattoli introduced the Exponential Substitution Calculus (ESC) given by untyped proof terms for Intuitionistic Multiplicative Exponential Linear Logic (IMELL), endowed with rewriting rules at-a-distance for cut elimination. He also introduced a new cut elimination strategy, dubbed the good strategy, and showed that its number of steps is a time cost model with polynomial overhead for the ESC/IMELL, and the first such one.   Here, we refine Accattoli's result by introducing an abstract machine for ESC and proving that it implements the good strategy and computes cut-free terms/proofs within a linear overhead.","sentences":["Recently, Accattoli introduced the Exponential Substitution Calculus (ESC) given by untyped proof terms for Intuitionistic Multiplicative Exponential Linear Logic (IMELL), endowed with rewriting rules at-a-distance for cut elimination.","He also introduced a new cut elimination strategy, dubbed the good strategy, and showed that its number of steps is a time cost model with polynomial overhead for the ESC/IMELL, and the first such one.   ","Here, we refine Accattoli's result by introducing an abstract machine for ESC and proving that it implements the good strategy and computes cut-free terms/proofs within a linear overhead."],"url":"http://arxiv.org/abs/2405.03669v1","category":"cs.LO"}
{"created":"2024-05-06 15:52:12","title":"Opening a meV mass window for Axion-like particles with a microwave-laser-mixed stimulated resonant photon collider","abstract":"We propose a microwave-laser-mixed three-beam stimulated resonant photon collider, which enables access to axion-like particles in the meV mass range. Collisions between a focused pulse laser beam and a focused microwave pulse beam directly produce axion-like particles (ALPs) and another focused pulse laser beam stimulates their decay. The expected sensitivity in the meV mass range has been evaluated. The result shows that the sensitivity can reach the ALP-photon coupling down to $\\mathcal{O}(10^{-13})$~GeV${}^{-1}$ with $10^6$ shots if 10-100 TW class high-intensity lasers are properly combined with a conventional 100 MW class S-band klystron. This proposal paves the way for identifying the nature of ALPs as candidates for dark matter, independent of any cosmological and astrophysical models.","sentences":["We propose a microwave-laser-mixed three-beam stimulated resonant photon collider, which enables access to axion-like particles in the meV mass range.","Collisions between a focused pulse laser beam and a focused microwave pulse beam directly produce axion-like particles (ALPs) and another focused pulse laser beam stimulates their decay.","The expected sensitivity in the meV mass range has been evaluated.","The result shows that the sensitivity can reach the ALP-photon coupling down to $\\mathcal{O}(10^{-13})$~GeV${}^{-1}$ with $10^6$ shots if 10-100 TW class high-intensity lasers are properly combined with a conventional 100 MW class S-band klystron.","This proposal paves the way for identifying the nature of ALPs as candidates for dark matter, independent of any cosmological and astrophysical models."],"url":"http://arxiv.org/abs/2405.03577v1","category":"hep-ph"}
{"created":"2024-05-06 15:46:52","title":"Impact of Planetary Parameters on Water Clouds Microphysics","abstract":"Potentially habitable exoplanets are targets of great interest for the James Webb Space Telescope and upcoming mission concepts such as the Habitable Worlds Observatory. Clouds strongly affect climate and habitability, but predicting their properties is difficult. In Global Climate Models (GCMs), especially those aiming at simulating Earth, cloud microphysics is often crudely approximated by assuming that all cloud particles have a single, constant size or a prescribed size distribution and that all clouds in a grid cell are identical. For exoplanets that range over a large phase space of planetary properties, this method could result in large errors. In this work, our goal is to determine how cloud microphysics on terrestrial exoplanets, whose condensable is mainly water vapor, depend on aerosol properties and planetary parameters such as surface pressure, surface gravity, and incident stellar radiation. We use the Community Aerosol and Radiation Model for Atmospheres as a 1D microphysical model to simulate the formation and evolution of clouds including the processes of nucleation, condensation, evaporation, coagulation, and vertical transfer. In these 1D idealized experiments, we find that the parameters that determine the macrophysical thermal structure of the atmospheres, including surface pressure and stellar flux, impact cloud radiative effect (CRE) most significantly. Parameters such as gravity and number density of aerosols working as cloud condensation nuclei affect the microphysical processes of cloud formation, including activation and vertical transfer. They also have a significant, though weaker effect on CRE. This work motivates the development of more accurate GCM cloud schemes and should aid in the interpretation of future observations.","sentences":["Potentially habitable exoplanets are targets of great interest for the James Webb Space Telescope and upcoming mission concepts such as the Habitable Worlds Observatory.","Clouds strongly affect climate and habitability, but predicting their properties is difficult.","In Global Climate Models (GCMs), especially those aiming at simulating Earth, cloud microphysics is often crudely approximated by assuming that all cloud particles have a single, constant size or a prescribed size distribution and that all clouds in a grid cell are identical.","For exoplanets that range over a large phase space of planetary properties, this method could result in large errors.","In this work, our goal is to determine how cloud microphysics on terrestrial exoplanets, whose condensable is mainly water vapor, depend on aerosol properties and planetary parameters such as surface pressure, surface gravity, and incident stellar radiation.","We use the Community Aerosol and Radiation Model for Atmospheres as a 1D microphysical model to simulate the formation and evolution of clouds including the processes of nucleation, condensation, evaporation, coagulation, and vertical transfer.","In these 1D idealized experiments, we find that the parameters that determine the macrophysical thermal structure of the atmospheres, including surface pressure and stellar flux, impact cloud radiative effect (CRE) most significantly.","Parameters such as gravity and number density of aerosols working as cloud condensation nuclei affect the microphysical processes of cloud formation, including activation and vertical transfer.","They also have a significant, though weaker effect on CRE.","This work motivates the development of more accurate GCM cloud schemes and should aid in the interpretation of future observations."],"url":"http://arxiv.org/abs/2405.03570v1","category":"astro-ph.EP"}
{"created":"2024-05-06 14:58:12","title":"Adolescent sports participation and health in early adulthood: An observational study","abstract":"We study the impact of teenage sports participation on early-adulthood health using longitudinal data from the National Study of Youth and Religion. We focus on two primary outcomes measured at ages 23--28 -- self-rated health and total score on the PHQ9 Patient Depression Questionnaire -- and control for several potential confounders related to demographics and family socioeconomic status. To probe the possibility that certain types of sports participation may have larger effects on health than others, we conduct a matched observational study at each level within a hierarchy of exposures. Our hierarchy ranges from broadly defined exposures (e.g., participation in any organized after-school activity) to narrow (e.g., participation in collision sports). We deployed an ordered testing approach that exploits the hierarchical relationships between our exposure definitions to perform our analyses while maintaining a fixed family-wise error rate. Compared to teenagers who did not participate in any after-school activities, those who participated in sports had statistically significantly better self-rated and mental health outcomes in early adulthood.","sentences":["We study the impact of teenage sports participation on early-adulthood health using longitudinal data from the National Study of Youth and Religion.","We focus on two primary outcomes measured at ages 23--28 -- self-rated health and total score on the PHQ9 Patient Depression Questionnaire -- and control for several potential confounders related to demographics and family socioeconomic status.","To probe the possibility that certain types of sports participation may have larger effects on health than others, we conduct a matched observational study at each level within a hierarchy of exposures.","Our hierarchy ranges from broadly defined exposures (e.g., participation in any organized after-school activity) to narrow (e.g., participation in collision sports).","We deployed an ordered testing approach that exploits the hierarchical relationships between our exposure definitions to perform our analyses while maintaining a fixed family-wise error rate.","Compared to teenagers who did not participate in any after-school activities, those who participated in sports had statistically significantly better self-rated and mental health outcomes in early adulthood."],"url":"http://arxiv.org/abs/2405.03538v1","category":"stat.AP"}
{"created":"2024-05-06 14:49:34","title":"Five-dimensional non-Abelian supersymmetric Chern-Simons action in Projective Superspace","abstract":"In this paper, we derive the five-dimensional non-Abelian $\\mathcal{N}=1$ Chern-Simons action from its established definition of variation with respect to an infinitesimal deformation of the vector prepotential in a projective superspace setting. It has been a long-standing open problem how to integrate this variation. Now, thanks to the simplicity of a projective superspace technique, we have been finally able to obtain this term. As a bonus, we also give the supersymmetric Yang-Mills action with a half measure as well.","sentences":["In this paper, we derive the five-dimensional non-Abelian $\\mathcal{N}=1$ Chern-Simons action from its established definition of variation with respect to an infinitesimal deformation of the vector prepotential in a projective superspace setting.","It has been a long-standing open problem how to integrate this variation.","Now, thanks to the simplicity of a projective superspace technique, we have been finally able to obtain this term.","As a bonus, we also give the supersymmetric Yang-Mills action with a half measure as well."],"url":"http://arxiv.org/abs/2405.03532v1","category":"hep-th"}
{"created":"2024-05-06 14:27:22","title":"Linear correlations of Gibbs free energy for rare earth element oxide, hydroxide, chloride, fluoride, carbonate, and ferrite minerals and crystalline solids","abstract":"Rare Earth Elements (REE) are critical minerals (metals) for the transition from fossil fuels to renewable and clean energy. Accurate thermodynamic properties of REE minerals and other crystalline solids are crucial for geochemical modeling of the solubility, speciation, and transport of REE in ore formation, extraction, chemical processing, and recycling processes. However, the Gibbs free energies of formation (DGof, REEX) for these solids from different sources vary by 10s kJ/mol. We applied the Sverjensky linear free energy relationship (LFER) to evaluate their internal consistency and predict the unavailable DGof of the REE solids. By considering both the effects of ionic radius size and corresponding aqueous ion properties, the Sverjensky LFER allows estimates with much accuracy and precision. Here, rREEZ+ represents the Shannon-Prewitt ionic radii of REEZ+, and DGon, REEZ+ denotes the non-solvation contribution to the DGof of the aqueous REEZ+ ion. X represents the remainder of the compounds. In this study, the parameters aREEX, bREEX, and beta REEX were regressed from DGof compilations in the literature for 13 isostructural families. Based on these linear relationships, we recommend a set of internally consistent DGof, REEX for 119 end-members of REE oxides, hydroxides, chlorides, fluorides, carbonates, hydrous carbonates, and ferrites. These DGof, REEX are combined with experimental or predicted values of So, Vo, and Cpo from the literature and incorporated into a new SUPCRT database, which allows the calculations of thermodynamic properties to high P-T conditions (e.g., up to 1000 oC and 5 kb).","sentences":["Rare Earth Elements (REE) are critical minerals (metals) for the transition from fossil fuels to renewable and clean energy.","Accurate thermodynamic properties of REE minerals and other crystalline solids are crucial for geochemical modeling of the solubility, speciation, and transport of REE in ore formation, extraction, chemical processing, and recycling processes.","However, the Gibbs free energies of formation (DGof, REEX) for these solids from different sources vary by 10s kJ/mol.","We applied the Sverjensky linear free energy relationship (LFER) to evaluate their internal consistency and predict the unavailable DGof of the REE solids.","By considering both the effects of ionic radius size and corresponding aqueous ion properties, the Sverjensky LFER allows estimates with much accuracy and precision.","Here, rREEZ+ represents the Shannon-Prewitt ionic radii of REEZ+, and DGon, REEZ+ denotes the non-solvation contribution to the DGof of the aqueous REEZ+ ion.","X represents the remainder of the compounds.","In this study, the parameters aREEX, bREEX, and beta REEX were regressed from DGof compilations in the literature for 13 isostructural families.","Based on these linear relationships, we recommend a set of internally consistent DGof, REEX for 119 end-members of REE oxides, hydroxides, chlorides, fluorides, carbonates, hydrous carbonates, and ferrites.","These DGof, REEX are combined with experimental or predicted values of So, Vo, and Cpo from the literature and incorporated into a new SUPCRT database, which allows the calculations of thermodynamic properties to high P-T conditions (e.g., up to 1000 oC and 5 kb)."],"url":"http://arxiv.org/abs/2405.03515v1","category":"physics.geo-ph"}
{"created":"2024-05-06 14:04:03","title":"A Full Model of the Response of Surface Detectors to Extensive Air Showers Based on Shower Universality","abstract":"We present a full model of surface-detector responses to extensive air showers. The model is motivated by the principles of air-shower universality and can be applied to different types of surface detectors. Here we describe a parametrization for both water-Cerenkov detectors and scintillator surface detectors, as for instance employed by the upgraded detector array of the Pierre Auger Observatory. Using surface detector data, the model can be used to reconstruct with reasonable precision shower observables such as the depth of the shower maximum $X_\\text{max}$ and the number of muons $R_\\mu$.","sentences":["We present a full model of surface-detector responses to extensive air showers.","The model is motivated by the principles of air-shower universality and can be applied to different types of surface detectors.","Here we describe a parametrization for both water-Cerenkov detectors and scintillator surface detectors, as for instance employed by the upgraded detector array of the Pierre Auger Observatory.","Using surface detector data, the model can be used to reconstruct with reasonable precision shower observables such as the depth of the shower maximum $X_\\text{max}$ and the number of muons $R_\\mu$."],"url":"http://arxiv.org/abs/2405.03494v1","category":"hep-ph"}
{"created":"2024-05-06 13:36:21","title":"Next-to-leading-order electroweak correction to $H\\to Z^0\u03b3$","abstract":"Inspired by the recent observation of the Higgs boson radiative decay into $Z^0$ by {\\tt ATLAS} and {\\tt CMS} Collaborations, we investigate the next-to-leading-order (NLO) electroweak correction to this rare decay process in Standard Model (SM). Implementing the on-shell renormalization scheme, we find that the magnitude of the NLO electroweak correction may reach $7\\%$ of the leading order (LO) prediction, much more significant than that of the NLO QCD correction, which is merely about $0.3\\%$. After incorporating the ${\\cal O}(\\alpha)$ correction, the predicted partial width from various $\\alpha$ schemes tend to converge to each other. Including both NLO electroweak and QCD corrections, we present the most accurate SM prediction for the branching fraction of this rare decay channel to be ${\\cal B}_{\\rm th}[H\\to Z^0\\gamma]=(1.55\\pm 0.06)\\times 10^{-3}$, considerably lower than the measured value ${\\cal B}_{\\rm exp}[H\\to Z^0\\gamma]=(3.4\\pm 1.1)\\times 10^{-3}$. Resolving this alarming discrepancy clearly calls for further theoretical investigations, and, more importantly, experimental efforts from {\\tt HL-LHC} and the prospective Higgs factories such as {\\tt CEPC} and {\\tt FCC}.","sentences":["Inspired by the recent observation of the Higgs boson radiative decay into $Z^0$ by {\\tt ATLAS} and {\\tt CMS} Collaborations, we investigate the next-to-leading-order (NLO) electroweak correction to this rare decay process in Standard Model (SM).","Implementing the on-shell renormalization scheme, we find that the magnitude of the NLO electroweak correction may reach $7\\%$ of the leading order (LO) prediction, much more significant than that of the NLO QCD correction, which is merely about $0.3\\%$. After incorporating the ${\\cal O}(\\alpha)$ correction, the predicted partial width from various $\\alpha$ schemes tend to converge to each other.","Including both NLO electroweak and QCD corrections, we present the most accurate SM prediction for the branching fraction of this rare decay channel to be ${\\cal B}_{\\rm th}[H\\to Z^0\\gamma]=(1.55\\pm 0.06)\\times 10^{-3}$, considerably lower than the measured value ${\\cal B}_{\\rm exp}[H\\to Z^0\\gamma]=(3.4\\pm 1.1)\\times 10^{-3}$.","Resolving this alarming discrepancy clearly calls for further theoretical investigations, and, more importantly, experimental efforts from {\\tt HL-LHC} and the prospective Higgs factories such as {\\tt CEPC} and {\\tt FCC}."],"url":"http://arxiv.org/abs/2405.03464v1","category":"hep-ph"}
{"created":"2024-05-06 13:31:33","title":"Revisiting the spatially inhomogeneous condensates in the $(1 + 1)$-dimensional chiral Gross-Neveu model via the bosonic two-point function in the infinite-$N$ limit","abstract":"This work shows that the known phase boundary between the phase with chiral symmetry and the phase of spatially inhomogeneous chiral symmetry breaking in the phase diagram of the $(1 + 1)$-dimensional chiral Gross-Neveu model can be detected from the bosonic two-point function alone and thereby confirms and extends previous results arXiv:hep-th/0008175, arXiv:0807.2571, arXiv:0909.3714, arXiv:1810.03921, arXiv:2203.08503. The analysis is refered to as the stability analysis of the symmetric phase and does not require knowledge about spatial modulations of condensates. We perform this analysis in the infinite-$N$ limit at nonzero temperature and nonzero quark and chiral chemical potentials also inside the inhomogeneous phase. Thereby we observe an interesting relation between the bosonic $1$-particle irreducible two-point vertex function of the chiral Gross-Neveu model and the spinodal line of the Gross-Neveu model.","sentences":["This work shows that the known phase boundary between the phase with chiral symmetry and the phase of spatially inhomogeneous chiral symmetry breaking in the phase diagram of the $(1 + 1)$-dimensional chiral Gross-Neveu model can be detected from the bosonic two-point function alone and thereby confirms and extends previous results arXiv:hep-th/0008175, arXiv:0807.2571, arXiv:0909.3714, arXiv:1810.03921, arXiv:2203.08503.","The analysis is refered to as the stability analysis of the symmetric phase and does not require knowledge about spatial modulations of condensates.","We perform this analysis in the infinite-$N$ limit at nonzero temperature and nonzero quark and chiral chemical potentials also inside the inhomogeneous phase.","Thereby we observe an interesting relation between the bosonic $1$-particle irreducible two-point vertex function of the chiral Gross-Neveu model and the spinodal line of the Gross-Neveu model."],"url":"http://arxiv.org/abs/2405.03459v1","category":"hep-th"}
{"created":"2024-05-06 13:19:56","title":"One-dimensional power spectrum from first DESI Lyman-\u03b1 forest","abstract":"The Lyman-alpha forest is a unique probe of large-scale matter density fluctuations at high redshift z > 2. We measure the one-dimensional Lyman-alpha forest power spectrum using the first data provided by the Dark Energy Spectroscopic Instrument (DESI), with a fast Fourier transform estimator. The data sample contains quasar spectra included in the DESI Early Data Release and the first two months of the main survey. This first set of data already provides an improvement in terms of spectroscopic resolution with respect to the previous measurements. We investigated methodological and instrumental contaminants associated with DESI and used synthetic data to validate and correct our measurement. Coupling our measurement with theoretical predictions from hydrodynamical simulations will yield strong constraints on the primordial matter power spectrum, neutrino masses, and dark matter properties. A quadratic maximum likelihood estimator was applied to the same data set on a companion paper and agrees with our measurement.","sentences":["The Lyman-alpha forest is a unique probe of large-scale matter density fluctuations at high redshift z > 2.","We measure the one-dimensional Lyman-alpha forest power spectrum using the first data provided by the Dark Energy Spectroscopic Instrument (DESI), with a fast Fourier transform estimator.","The data sample contains quasar spectra included in the DESI Early Data Release and the first two months of the main survey.","This first set of data already provides an improvement in terms of spectroscopic resolution with respect to the previous measurements.","We investigated methodological and instrumental contaminants associated with DESI and used synthetic data to validate and correct our measurement.","Coupling our measurement with theoretical predictions from hydrodynamical simulations will yield strong constraints on the primordial matter power spectrum, neutrino masses, and dark matter properties.","A quadratic maximum likelihood estimator was applied to the same data set on a companion paper and agrees with our measurement."],"url":"http://arxiv.org/abs/2405.03447v1","category":"astro-ph.CO"}
{"created":"2024-05-06 13:14:57","title":"Higher-order NLO initial state QED radiative corrections to e+e- annihilation revisited","abstract":"Radiative corrections due to initial state radiation in electron-positron annihilation are calculated within the QED structure function approach. Results are shown in the next-to-leading logarithmic approximation up to $O(\\alpha^4 L^3)$ order, where $L=\\ln(s/m_e^2)$ is the large logarithm. Several mistakes in previous calculations are corrected. The results are relevant for future high-precision experiments at $e^+e^-$ colliders.","sentences":["Radiative corrections due to initial state radiation in electron-positron annihilation are calculated within the QED structure function approach.","Results are shown in the next-to-leading logarithmic approximation up to $O(\\alpha^4 L^3)$ order, where $L=\\ln(s/m_e^2)$ is the large logarithm.","Several mistakes in previous calculations are corrected.","The results are relevant for future high-precision experiments at $e^+e^-$ colliders."],"url":"http://arxiv.org/abs/2405.03443v1","category":"hep-ph"}
{"created":"2024-05-06 13:12:30","title":"Prediction of the amplitude of solar cycle 25 from the ratio of sunspot number to sunspot-group area, low latitude activity, and 130-year solar cycle","abstract":"We analysed the combined data of sunspot groups from Greenwich Photoheliographic Results (GPR) during the period 1874-1976 and Debrecen Photoheliographic Data (DPD) during 1977-2017 and determined the monthly mean, annual mean, and 13-month smoothed monthly mean whole sphere sunspot-group area (WSGA). We have also analysed the monthly mean, annual mean, and 13-month smoothed monthly mean version 2 of international sunspot number (SN) during the period 1874-2017. We fitted the annual mean WSGA and SN data during each of Solar Cycles 12-24 separately to the linear and nonlinear (parabola) forms. In the cases of Solar Cycles 14, 17, and 24 the nonlinear fits are found better than the linear fits. We find that there exists a secular decreasing trend in the slope of the WSGA-SN linear relation during Solar Cycles 12-24. A secular decreasing trend is also seen in the coefficient of the first order term of the nonlinear relation. The existence of ~77-year variation is seen in the ratio of the amplitude to WSGA at the maximum epoch of solar cycle. From the pattern of this long-term variation of the ratio we inferred that Solar Cycle 25 will be larger than both Solar Cycles 24 and 26. Using an our earlier method (now slightly revised) we predicted 127 (plus or minus 26) and 141 (plus or minus 19) for the amplitude of Solar Cycle~25. Based on ~130-year periodicity found in the cycle-to-cycle variation of the amplitudes of Solar Cycles 12-24 we find the shape of Solar Cycle 25 would be similar to that of Solar Cycle 13 and predicted for Solar Cycle 25 the amplitude 135 (plus or minus 8), maximum epoch 2024.21 (March 2024) plus or minus 6-month, and end epoch 2032.21 (March 2032) plus or minus 6-month with SN ~4.","sentences":["We analysed the combined data of sunspot groups from Greenwich Photoheliographic Results (GPR) during the period 1874-1976 and Debrecen Photoheliographic Data (DPD) during 1977-2017 and determined the monthly mean, annual mean, and 13-month smoothed monthly mean whole sphere sunspot-group area (WSGA).","We have also analysed the monthly mean, annual mean, and 13-month smoothed monthly mean version 2 of international sunspot number (SN) during the period 1874-2017.","We fitted the annual mean WSGA and SN data during each of Solar Cycles 12-24 separately to the linear and nonlinear (parabola) forms.","In the cases of Solar Cycles 14, 17, and 24 the nonlinear fits are found better than the linear fits.","We find that there exists a secular decreasing trend in the slope of the WSGA-SN linear relation during Solar Cycles 12-24.","A secular decreasing trend is also seen in the coefficient of the first order term of the nonlinear relation.","The existence of ~77-year variation is seen in the ratio of the amplitude to WSGA at the maximum epoch of solar cycle.","From the pattern of this long-term variation of the ratio we inferred that Solar Cycle 25 will be larger than both Solar Cycles 24 and 26.","Using an our earlier method (now slightly revised) we predicted 127 (plus or minus 26) and 141 (plus or minus 19) for the amplitude of Solar Cycle~25.","Based on ~130-year periodicity found in the cycle-to-cycle variation of the amplitudes of Solar Cycles 12-24 we find the shape of Solar Cycle 25 would be similar to that of Solar Cycle 13 and predicted for Solar Cycle 25 the amplitude 135 (plus or minus 8), maximum epoch 2024.21 (March 2024) plus or minus 6-month, and end epoch 2032.21 (March 2032) plus or minus 6-month with SN ~4."],"url":"http://arxiv.org/abs/2405.03441v1","category":"astro-ph.SR"}
{"created":"2024-05-06 12:58:44","title":"Point-Spread Function errors for weak lensing - density cross-correlations. Application to UNIONS","abstract":"Aims:Calibrating the point spread function (PSF) is a fundamental part of weak gravitational lensing analyses. Even with corrected galaxy images, imperfect calibrations can introduce biases. We propose an analytical framework for quantifying PSF-induced systematics as diagnostics for cross-correlation measurements of weak lensing with density tracers, e.g., galaxy-galaxy lensing. We show how those systematics propagate to physical parameters of the density tracers. Those diagnostics only require a shape catalogue of PSF stars and foreground galaxy positions. Methods:We consider the PSF-induced multiplicative bias, and introduce three second-order statistics as additive biases. We compute both biases for the weak-lensing derived halo mass of spectroscopic foreground galaxy samples, in particular, their effect on the tangential shear and fitted halo mass as a function of stellar mass. In addition, we assess their impact on the recently published black-hole - halo-mass relation for type I Active Galactic Nuclei (AGNs). Results:Using weak-lensing catalogues from the Ultraviolet Near Infrared Optical Northern Survey (UNIONS) and Dark Energy Survey (DES), we find the multiplicative biases in the tangential shear to be less than $0.5\\%$. No correlations between additive bias and galaxy properties of the foreground sample are detected. The combined PSF systematics affect low-mass galaxies and small angular scales; halo mass estimates can be biased by up to 18$\\%$ for a sample of central galaxies in the stellar mass range 9.0 $\\leq$ log $M_*/\\rm M_{\\odot}$ < 9.5. Conclusions:The PSF-induced multiplicative bias is a subdominant contribution to current studies of weak-lensing - density cross-correlations, but might become significant for upcoming Stage-VI surveys. For samples with a low tangential shear, additive PSF systematics can induce a significant bias on derived properties such as halo mass.","sentences":["Aims:","Calibrating the point spread function (PSF) is a fundamental part of weak gravitational lensing analyses.","Even with corrected galaxy images, imperfect calibrations can introduce biases.","We propose an analytical framework for quantifying PSF-induced systematics as diagnostics for cross-correlation measurements of weak lensing with density tracers, e.g., galaxy-galaxy lensing.","We show how those systematics propagate to physical parameters of the density tracers.","Those diagnostics only require a shape catalogue of PSF stars and foreground galaxy positions.","Methods:We consider the PSF-induced multiplicative bias, and introduce three second-order statistics as additive biases.","We compute both biases for the weak-lensing derived halo mass of spectroscopic foreground galaxy samples, in particular, their effect on the tangential shear and fitted halo mass as a function of stellar mass.","In addition, we assess their impact on the recently published black-hole - halo-mass relation for type I Active Galactic Nuclei (AGNs).","Results:Using weak-lensing catalogues from the Ultraviolet Near Infrared Optical Northern Survey (UNIONS) and Dark Energy Survey (DES), we find the multiplicative biases in the tangential shear to be less than $0.5\\%$. No correlations between additive bias and galaxy properties of the foreground sample are detected.","The combined PSF systematics affect low-mass galaxies and small angular scales; halo mass estimates can be biased by up to 18$\\%$ for a sample of central galaxies in the stellar mass range 9.0 $\\leq$ log $M_*/\\rm M_{\\odot}$ < 9.5.","Conclusions:The PSF-induced multiplicative bias is a subdominant contribution to current studies of weak-lensing - density cross-correlations, but might become significant for upcoming Stage-VI surveys.","For samples with a low tangential shear, additive PSF systematics can induce a significant bias on derived properties such as halo mass."],"url":"http://arxiv.org/abs/2405.03434v1","category":"astro-ph.CO"}
{"created":"2024-05-06 12:16:24","title":"Hall effect on the joint cascades of magnetic energy and helicity in helical magnetohydrodynamic turbulence","abstract":"Helical magnetohydrodynamic turbulence with Hall effects is ubiquitous in heliophysics and plasma physics, such as star formation and solar activities, and its intrinsic mechanisms are still not clearly explained. Direct numerical simulations reveal that when the forcing scale is comparable to the ion inertial scale, Hall effects induce remarkable cross helicity. It then suppresses the inverse cascade efficiency, leading to the accumulation of large-scale magnetic energy and helicity. The process is accompanied by the breaking of current sheets via filaments along magnetic fields. Using the Ulysses data, the numerical findings are separately confirmed. These results suggest a novel mechanism wherein small-scale Hall effects could strongly affect large-scale magnetic fields through cross helicity.","sentences":["Helical magnetohydrodynamic turbulence with Hall effects is ubiquitous in heliophysics and plasma physics, such as star formation and solar activities, and its intrinsic mechanisms are still not clearly explained.","Direct numerical simulations reveal that when the forcing scale is comparable to the ion inertial scale, Hall effects induce remarkable cross helicity.","It then suppresses the inverse cascade efficiency, leading to the accumulation of large-scale magnetic energy and helicity.","The process is accompanied by the breaking of current sheets via filaments along magnetic fields.","Using the Ulysses data, the numerical findings are separately confirmed.","These results suggest a novel mechanism wherein small-scale Hall effects could strongly affect large-scale magnetic fields through cross helicity."],"url":"http://arxiv.org/abs/2405.03405v1","category":"physics.plasm-ph"}
{"created":"2024-05-06 12:04:47","title":"Non-Perturbative Corrections to 3d BPS Indices and Topological Strings","abstract":"For a 3d gauged linear sigma model parametrized by a Kahler manifold X, the 3d BPS index defines a q-series that can be analytically continued in the Kahler modulus by standard methods. It is argued that an SL(2,Z)-transform of the Birkhoff connection matrix captures non-perturbative corrections to the 3d GLSM. As an application, a 3d lift of the standard 2d GLSM for the resolved conifold is shown to provide a world-volume dual for the non-perturbative topological string on the resolved conifold. The perturbative 3d BPS index computes the Gopakumar-Vafa partition function, while the analytic continuation matches existing proposals for a non-perturbative completion of the topological string.","sentences":["For a 3d gauged linear sigma model parametrized by a Kahler manifold X, the 3d BPS index defines a q-series that can be analytically continued in the Kahler modulus by standard methods.","It is argued that an SL(2,Z)-transform of the Birkhoff connection matrix captures non-perturbative corrections to the 3d GLSM.","As an application, a 3d lift of the standard 2d GLSM for the resolved conifold is shown to provide a world-volume dual for the non-perturbative topological string on the resolved conifold.","The perturbative 3d BPS index computes the Gopakumar-Vafa partition function, while the analytic continuation matches existing proposals for a non-perturbative completion of the topological string."],"url":"http://arxiv.org/abs/2405.03398v1","category":"hep-th"}
{"created":"2024-05-06 11:54:35","title":"Blueberry galaxies up to 200 Mpc and their optical/infrared properties","abstract":"Dwarf highly star-forming galaxies (SFGs) dominated the early Universe and are considered the main driver of its reionization. Direct observations of these very distant galaxies are limited mainly to rest-frame ultraviolet and visible wavelengths, and as a result, some of their properties are out of reach. Therefore, the study of their local analogs, the Green Pea (GP) and Blueberry (BB) galaxies is still of paramount importance. This work aims to expand the number of known BBs to smaller distances and the south equatorial sky. In addition to the already known, this new sample of BBs allows for a statistically significant study of their properties probed by visible and infrared (IR) light. We utilize HECATE, a catalog of galaxies up to 200 Mpc, which provides optical/IR photometry and characterization of the sources. We adopt the Pan-STARSS and SDSS optical photometries, and MPA-JHU analysis for sources having spectroscopic observations. Benefited by the multiwavelength coverage, we perform spectral energy distribution fitting to provide homogeneous star-formation rates and stellar masses. This work has identified 48 BBs out of which 40 are new. The nearest BB lies at 19 Mpc while 14 BBs are in the south equatorial sky. The BBs tend to be extremely IR red in both WISE W1-W2 and W2-W3 colors, located in different positions in this diagram compared to typical SFGs. Overall, dwarf SFGs with higher specific star-formation rates tend to have redder IR colors. Although GPs and BBs share many similarities, the latter are the most intensively star-forming sources of the local Universe among dwarf galaxies. Overall BBs are intrinsically bluer in visible light, redder in the IR, less massive, have higher specific star-formation rates and equivalent widths, lower metallicities, and have the most strongly ionized interstellar medium compared to typical SFGs and GPs.","sentences":["Dwarf highly star-forming galaxies (SFGs) dominated the early Universe and are considered the main driver of its reionization.","Direct observations of these very distant galaxies are limited mainly to rest-frame ultraviolet and visible wavelengths, and as a result, some of their properties are out of reach.","Therefore, the study of their local analogs, the Green Pea (GP) and Blueberry (BB) galaxies is still of paramount importance.","This work aims to expand the number of known BBs to smaller distances and the south equatorial sky.","In addition to the already known, this new sample of BBs allows for a statistically significant study of their properties probed by visible and infrared (IR) light.","We utilize HECATE, a catalog of galaxies up to 200 Mpc, which provides optical/IR photometry and characterization of the sources.","We adopt the Pan-STARSS and SDSS optical photometries, and MPA-JHU analysis for sources having spectroscopic observations.","Benefited by the multiwavelength coverage, we perform spectral energy distribution fitting to provide homogeneous star-formation rates and stellar masses.","This work has identified 48 BBs out of which 40 are new.","The nearest BB lies at 19 Mpc while 14 BBs are in the south equatorial sky.","The BBs tend to be extremely IR red in both WISE W1-W2 and W2-W3 colors, located in different positions in this diagram compared to typical SFGs.","Overall, dwarf SFGs with higher specific star-formation rates tend to have redder IR colors.","Although GPs and BBs share many similarities, the latter are the most intensively star-forming sources of the local Universe among dwarf galaxies.","Overall BBs are intrinsically bluer in visible light, redder in the IR, less massive, have higher specific star-formation rates and equivalent widths, lower metallicities, and have the most strongly ionized interstellar medium compared to typical SFGs and GPs."],"url":"http://arxiv.org/abs/2405.03391v1","category":"astro-ph.GA"}
{"created":"2024-05-06 11:19:55","title":"Jet Collimation and Acceleration in the Flat Spectrum Radio Quasar 1928+738","abstract":"Using time-resolved multifrequency Very Long Baseline Array data and new KaVA (KVN and VERA Array) observations, we study the structure and kinematics of the jet of the flat spectrum radio quasar (FSRQ) 1928+738. We find two distinct jet geometries as function of distance from the central black hole, with the inner jet having a parabolic shape, indicating collimation, and the outer jet having a conical shape, indicating free expansion of the jet plasma. Jet component speeds display a gradual outward acceleration up to a bulk Lorentz factor $\\Gamma_{\\rm max} \\approx10$, followed by a deceleration further downstream. The location of the acceleration zone matches the region where the jet collimation occurs; this is the first direct observation of an acceleration and collimation zone (ACZ) in an FSRQ. The ACZ terminates approximately at a distance of 5.6$\\times 10^6$ gravitational radii, which is in good agreement with the sphere of gravitational influence of the supermassive black hole, implying that the physical extent of the ACZ is controlled by the black hole gravity. Our results suggest that confinement by an external medium is responsible for the jet collimation and that the jet is accelerated by converting Poynting flux energy to kinetic energy.","sentences":["Using time-resolved multifrequency Very Long Baseline Array data and new KaVA (KVN and VERA Array) observations, we study the structure and kinematics of the jet of the flat spectrum radio quasar (FSRQ) 1928+738.","We find two distinct jet geometries as function of distance from the central black hole, with the inner jet having a parabolic shape, indicating collimation, and the outer jet having a conical shape, indicating free expansion of the jet plasma.","Jet component speeds display a gradual outward acceleration up to a bulk Lorentz factor $\\Gamma_{\\rm max} \\approx10$, followed by a deceleration further downstream.","The location of the acceleration zone matches the region where the jet collimation occurs; this is the first direct observation of an acceleration and collimation zone (ACZ) in an FSRQ.","The ACZ terminates approximately at a distance of 5.6$\\times 10^6$ gravitational radii, which is in good agreement with the sphere of gravitational influence of the supermassive black hole, implying that the physical extent of the ACZ is controlled by the black hole gravity.","Our results suggest that confinement by an external medium is responsible for the jet collimation and that the jet is accelerated by converting Poynting flux energy to kinetic energy."],"url":"http://arxiv.org/abs/2405.03365v1","category":"astro-ph.HE"}
{"created":"2024-05-06 10:36:05","title":"Collision geometry in UPC dijet production","abstract":"We present a comprehensive NLO pQCD study on inclusive dijet photoproduction in ultraperipheral nucleus-nucleus collisions (UPCs). Our analysis takes into account the finite size of both the photon-emitting and the target nucleus, estimated using the Wood-Saxon nuclear density profile. We show that a significant part of the measured dijets at large $z_\\gamma$ in UPC Pb+Pb collisions at 5.02 TeV come from events with relatively small impact parameters of the order of a few nuclear radii, and the cross section predictions thus become sensitive to the modelling of the collision geometry and photon flux near the source nucleus. In addition, we include the modelling of electromagnetic breakup survival factor needed for a direct comparison with the experimental data and study the resolution power of this process in terms of the impact-parameter dependent nuclear parton distribution functions.","sentences":["We present a comprehensive NLO pQCD study on inclusive dijet photoproduction in ultraperipheral nucleus-nucleus collisions (UPCs).","Our analysis takes into account the finite size of both the photon-emitting and the target nucleus, estimated using the Wood-Saxon nuclear density profile.","We show that a significant part of the measured dijets at large $z_\\gamma$ in UPC Pb+Pb collisions at 5.02 TeV come from events with relatively small impact parameters of the order of a few nuclear radii, and the cross section predictions thus become sensitive to the modelling of the collision geometry and photon flux near the source nucleus.","In addition, we include the modelling of electromagnetic breakup survival factor needed for a direct comparison with the experimental data and study the resolution power of this process in terms of the impact-parameter dependent nuclear parton distribution functions."],"url":"http://arxiv.org/abs/2405.03337v1","category":"hep-ph"}
{"created":"2024-05-06 10:03:01","title":"Investigation of Galactic supernova remnants and their environment in 26.6\u00b0 < l < 30.6\u00b0, $\\vert b \\vert \\leq$ 1.25\u00b0 using radio surveys","abstract":"The problem of missing Galactic supernova remnants (SNRs) refers to the issue that the currently known Galactic SNRs are significantly incomplete compared to the theoretical prediction. To expand the sample of Galactic SNRs, we use GLEAM and THOR+VGPS data across four wavebands ranging from 118 to 1420 MHz to drive a spectral index map covering the region within 26.6{\\deg} < l < 30.6{\\deg}, $\\vert b \\vert \\leq$ 1.25{\\deg}, where numerous SNR candidates were recently found. By using the spectral index map of the sky region and detailed analysis of the spectral indices of individual sources, we confirmed four SNR candidates, namely G26.75+0.73, G27.06+0.04, G28.36+0.21, and G28.78$-$0.44, as SNRs. Additionally, we discovered an expanding molecular superbubble located in this region, discussed pulsars associated with SNR candidates, and discovered a long H$\\alpha$ filament that spatially overlaps with the candidate G29.38+0.10. We suggest that the problem of missing Galactic SNRs not only arises from observation limitations, but also could be due to the low-density environments of some SNRs, and the different SN explosion properties.","sentences":["The problem of missing Galactic supernova remnants (SNRs) refers to the issue that the currently known Galactic SNRs are significantly incomplete compared to the theoretical prediction.","To expand the sample of Galactic SNRs, we use GLEAM and THOR+VGPS data across four wavebands ranging from 118 to 1420 MHz to drive a spectral index map covering the region within 26.6{\\deg} < l <","30.6{\\deg}, $\\vert b \\vert \\leq$ 1.25{\\deg}, where numerous SNR candidates were recently found.","By using the spectral index map of the sky region and detailed analysis of the spectral indices of individual sources, we confirmed four SNR candidates, namely G26.75+0.73, G27.06+0.04, G28.36+0.21, and G28.78$-$0.44, as SNRs.","Additionally, we discovered an expanding molecular superbubble located in this region, discussed pulsars associated with SNR candidates, and discovered a long H$\\alpha$ filament that spatially overlaps with the candidate G29.38+0.10.","We suggest that the problem of missing Galactic SNRs not only arises from observation limitations, but also could be due to the low-density environments of some SNRs, and the different SN explosion properties."],"url":"http://arxiv.org/abs/2405.03324v1","category":"astro-ph.HE"}
{"created":"2024-05-06 08:17:25","title":"Calibration of a soft secondary vertex tagger using proton-proton collisions at $\\sqrt{s}=$13 TeV with the ATLAS detector","abstract":"Several processes studied by the ATLAS experiment at the Large Hadron Collider produce low-momentum $b$-flavored hadrons in the final state. This paper describes the calibration of a dedicated tagging algorithm that identifies $b$-flavored hadrons outside of hadronic jets by reconstructing the soft secondary vertices originating from their decays. The calibration is based on a proton-proton collision dataset at a center-of-mass energy of 13 TeV corresponding to an integrated luminosity of 140 fb$^{-1}$. Scale factors used to correct the algorithm's performance in simulated events are extracted for the $b$-tagging efficiency and the mistag rate of the algorithm using a data sample enriched in $t\\bar{t}$ events. Several orthogonal measurement regions are defined, binned as a function of the multiplicities of soft secondary vertices and jets containing a $b$-flavored hadron in the event. The mistag rate scale factors are estimated separately for events with low and high average number of interactions per bunch crossing. The results, which are derived from events with low missing transverse momentum, are successfully validated in a phase space characterized by high missing transverse momentum and therefore are applicable to new physics searches carried out in either phase space regimes.","sentences":["Several processes studied by the ATLAS experiment at the Large Hadron Collider produce low-momentum $b$-flavored hadrons in the final state.","This paper describes the calibration of a dedicated tagging algorithm that identifies $b$-flavored hadrons outside of hadronic jets by reconstructing the soft secondary vertices originating from their decays.","The calibration is based on a proton-proton collision dataset at a center-of-mass energy of 13 TeV corresponding to an integrated luminosity of 140 fb$^{-1}$. Scale factors used to correct the algorithm's performance in simulated events are extracted for the $b$-tagging efficiency and the mistag rate of the algorithm using a data sample enriched in $t\\bar{t}$ events.","Several orthogonal measurement regions are defined, binned as a function of the multiplicities of soft secondary vertices and jets containing a $b$-flavored hadron in the event.","The mistag rate scale factors are estimated separately for events with low and high average number of interactions per bunch crossing.","The results, which are derived from events with low missing transverse momentum, are successfully validated in a phase space characterized by high missing transverse momentum and therefore are applicable to new physics searches carried out in either phase space regimes."],"url":"http://arxiv.org/abs/2405.03253v1","category":"hep-ex"}
{"created":"2024-05-06 07:50:29","title":"Probing flavor violation and baryogenesis via primordial gravitational waves","abstract":"We show that observations of primordial gravitational waves of inflationary origin can shed light into the scale of flavor violation in a flavon model which also explains the mass hierarchy of fermions. The energy density stored in oscillations of the flavon field around the minimum of its potential redshifts as matter and is expected to dominate over radiation in the early universe. At the same time, the evolution of primordial gravitational waves acts as bookkeeping to understand the expansion history of the universe. Importantly, the gravitational wave spectrum is different if there is an early flavon dominated era compared to radiation domination expected from a standard cosmological model and this spectrum gets damped by the entropy released in flavon decays, determined by the mass of the flavon field $m_S$ and new scale of flavor violation $\\Lambda_{\\rm FV}$. We derive analytical expressions of the frequency above which the spectrum is damped, as-well-as the amount of damping, in terms of $m_S$ and $\\Lambda_{\\rm FV}$. We show that the damping of the gravitational wave spectrum would be detectable at BBO, DECIGO, U-DECIGO, $\\mu-$ARES, LISA, CE and ET detectors for $\\Lambda_{\\rm FV}=10^{5-10}$ GeV and $m_S=\\mathcal{O({\\rm TeV})}$. Furthermore, the flavon decays can source the baryon asymmetry of the universe. We identify the $m_S-\\Lambda_{\\rm FV}$ parameter space where the observed baryon asymmetry $\\eta \\sim 10^{-10}$ is produced and can be tested by gravitational wave detectors like LISA and ET. We also discuss our results in the context of the recently measured stochastic gravitational background signals by NANOGrav.","sentences":["We show that observations of primordial gravitational waves of inflationary origin can shed light into the scale of flavor violation in a flavon model which also explains the mass hierarchy of fermions.","The energy density stored in oscillations of the flavon field around the minimum of its potential redshifts as matter and is expected to dominate over radiation in the early universe.","At the same time, the evolution of primordial gravitational waves acts as bookkeeping to understand the expansion history of the universe.","Importantly, the gravitational wave spectrum is different if there is an early flavon dominated era compared to radiation domination expected from a standard cosmological model and this spectrum gets damped by the entropy released in flavon decays, determined by the mass of the flavon field $m_S$ and new scale of flavor violation $\\Lambda_{\\rm FV}$.","We derive analytical expressions of the frequency above which the spectrum is damped, as-well-as the amount of damping, in terms of $m_S$ and $\\Lambda_{\\rm FV}$.","We show that the damping of the gravitational wave spectrum would be detectable at BBO, DECIGO, U-DECIGO, $\\mu-$ARES, LISA, CE and ET detectors for $\\Lambda_{\\rm FV}=10^{5-10}$ GeV and $m_S=\\mathcal{O({\\rm TeV})}$.","Furthermore, the flavon decays can source the baryon asymmetry of the universe.","We identify the $m_S-\\Lambda_{\\rm FV}$ parameter space where the observed baryon asymmetry $\\eta \\sim 10^{-10}$ is produced and can be tested by gravitational wave detectors like LISA and ET.","We also discuss our results in the context of the recently measured stochastic gravitational background signals by NANOGrav."],"url":"http://arxiv.org/abs/2405.03241v1","category":"hep-ph"}
{"created":"2024-05-06 07:29:21","title":"Geometric Constraints on Page Curves: Insights from Island Rule and Quantum Focusing Conjecture","abstract":"Exploring the inverse problem tied to the Page curve phenomenon and island paradigm, we investigate the geometric conditions underpinning black hole evaporation where information is preserved and islands manifest, giving rise to the characteristic Page curve. Focusing on a broad class of static black hole metrics in asymptotically Minkowski or (anti-)de Sitter spacetimes, we derive a pivotal constraint on the blacken factor $f(r)$ for which the island exists and reproduce the Page curve. Specifically, we reveal that a sufficient yet not universally necessary criterion -- manifested in the negativity of the second derivative of $f(r)$, i.e. $f^{\\prime \\prime} (r)<0$, in proximity to the event horizon where $r \\sim r_h+ {\\cal O} (G_N)$, ensures the emergence of Page curves in a manner transcending specific theoretical models. This pivotal finding, supported by the tenets of the quantum focusing conjecture.","sentences":["Exploring the inverse problem tied to the Page curve phenomenon and island paradigm, we investigate the geometric conditions underpinning black hole evaporation where information is preserved and islands manifest, giving rise to the characteristic Page curve.","Focusing on a broad class of static black hole metrics in asymptotically Minkowski or (anti-)de Sitter spacetimes, we derive a pivotal constraint on the blacken factor $f(r)$ for which the island exists and reproduce the Page curve.","Specifically, we reveal that a sufficient yet not universally necessary criterion -- manifested in the negativity of the second derivative of $f(r)$, i.e. $f^{\\prime \\prime} (r)<0$, in proximity to the event horizon where $r \\sim r_h+ {\\cal O} (G_N)$, ensures the emergence of Page curves in a manner transcending specific theoretical models.","This pivotal finding, supported by the tenets of the quantum focusing conjecture."],"url":"http://arxiv.org/abs/2405.03220v1","category":"hep-th"}
{"created":"2024-05-06 07:18:26","title":"Using magnetic dynamics to measure the spin gap in a candidate Kitaev material","abstract":"Materials potentially hosting Kitaev spin-liquid states are considered crucial for realizing topological quantum computing. However, the intricate nature of spin interactions within these materials complicates the precise measurement of low-energy spin excitations indicative of fractionalized excitations. Using Na$_{2}$Co$_2$TeO$_{6}$ as an example, we study these low-energy spin excitations using the time-resolved resonant elastic x-ray scattering (tr-REXS). Our observations unveil remarkably slow spin dynamics at the magnetic peak, whose recovery timescale is several nanoseconds. This timescale aligns with the extrapolated spin gap of $\\sim$ 1 $\\mu$eV, obtained by density matrix renormalization group (DMRG) simulations in the thermodynamic limit. The consistency demonstrates the efficacy of tr-REXS in discerning low-energy spin gaps inaccessible to conventional spectroscopic techniques.","sentences":["Materials potentially hosting Kitaev spin-liquid states are considered crucial for realizing topological quantum computing.","However, the intricate nature of spin interactions within these materials complicates the precise measurement of low-energy spin excitations indicative of fractionalized excitations.","Using Na$_{2}$Co$_2$TeO$_{6}$ as an example, we study these low-energy spin excitations using the time-resolved resonant elastic x-ray scattering (tr-REXS).","Our observations unveil remarkably slow spin dynamics at the magnetic peak, whose recovery timescale is several nanoseconds.","This timescale aligns with the extrapolated spin gap of $\\sim$ 1 $\\mu$eV, obtained by density matrix renormalization group (DMRG) simulations in the thermodynamic limit.","The consistency demonstrates the efficacy of tr-REXS in discerning low-energy spin gaps inaccessible to conventional spectroscopic techniques."],"url":"http://arxiv.org/abs/2405.03212v1","category":"cond-mat.str-el"}
{"created":"2024-05-06 07:17:10","title":"Magnetic dipole transition in proton-deuteron radiative capture at BBN energies within potential model","abstract":"The $pd$ radiative capture reaction plays a vital role in Big Bang nucleosynthesis and stellar proton-proton chain. The study of the low-energy reaction is challenging in both experiments and theories. Using the framework of potential model, we analyze $pd$ radiative capture below 1 MeV for both electric dipole ($E1$) and magnetic dipole ($M1$) transitions. The obtained astrophysical $S$ factors agree well with recent results, especially at energies relevant to sensitive deuterium abundance. The calculated reaction rate shows good agreement, with less than a 5\\% difference compared to recent works. The extrapolated value for $S(0)$ including both transitions is determined to be $0.211 \\pm 0.016$ eV b. A comparison with experimental data using the $\\chi^2$ test reveals the sensitivity of the $M1$ cross section at low energies to the scattering potential depth.","sentences":["The $pd$ radiative capture reaction plays a vital role in Big Bang nucleosynthesis and stellar proton-proton chain.","The study of the low-energy reaction is challenging in both experiments and theories.","Using the framework of potential model, we analyze $pd$ radiative capture below 1 MeV for both electric dipole ($E1$) and magnetic dipole ($M1$) transitions.","The obtained astrophysical $S$ factors agree well with recent results, especially at energies relevant to sensitive deuterium abundance.","The calculated reaction rate shows good agreement, with less than a 5\\% difference compared to recent works.","The extrapolated value for $S(0)$ including both transitions is determined to be $0.211 \\pm 0.016$ eV b.","A comparison with experimental data using the $\\chi^2$ test reveals the sensitivity of the $M1$ cross section at low energies to the scattering potential depth."],"url":"http://arxiv.org/abs/2405.03210v1","category":"nucl-th"}
{"created":"2024-05-06 06:41:44","title":"Probing dark energy using anisotropies in the clustering of post-EoR HI distribution","abstract":"We propose an anisotropy quantifier of the HI 21-cm signal traditionally used to clock the astrophysics of the reionization era as a post-reionization dark energy diagnostic. We find that the anisotropy probe can be measured at SNR $\\sim 10$ in both auto-correlation and in cross-correlation with the Ly-$\\alpha$ forest over a wide $z$ and $k-$range. We propose to use the BAO signature on the anisotropy signal to measure $( H(z), D_A(z))$. Subsequently, we put constraints on a dark energy model involving a negative cosmological constant on top of a quintessence scalar field and find that such a model is consistent with futuristic observations.","sentences":["We propose an anisotropy quantifier of the HI 21-cm signal traditionally used to clock the astrophysics of the reionization era as a post-reionization dark energy diagnostic.","We find that the anisotropy probe can be measured at SNR $\\sim 10$ in both auto-correlation and in cross-correlation with the Ly-$\\alpha$ forest over a wide $z$ and $k-$range.","We propose to use the BAO signature on the anisotropy signal to measure $( H(z), D_A(z))$. Subsequently, we put constraints on a dark energy model involving a negative cosmological constant on top of a quintessence scalar field and find that such a model is consistent with futuristic observations."],"url":"http://arxiv.org/abs/2405.03195v1","category":"astro-ph.CO"}
{"created":"2024-05-06 06:28:43","title":"LURAD: Design Study of a Comprehensive Radiation Monitor Package for the Gateway and the Lunar Surface","abstract":"Moon is an auspicious environment for the study of Galactic cosmic rays (GCR) and Solar particle events (SEP) due to the absence of magnetic field and atmosphere. The same characteristics raise the radiation risk for human presence in orbit around it or at the lunar surface. The secondary (albedo) radiation resulting from the interaction of the primary radiation with the lunar soil adds an extra risk factor, because neutrons are produced, but also it can be exploited to study the soil composition. In this paper, the design of a comprehensive radiation monitor package tailored to the lunar environment is presented. The detector, named LURAD, will perform spectroscopic measurements of protons, electrons, heavy ions, as well as gamma-rays, and neutrons. A microdosimetry monitor subsystem is foreseen which can provide measurements of LET(Si) spectra in a wide dynamic range of LET(Si) and flux for SPE and GCR, detection of neutrons and biological dose for radiation protection of astronauts. The LURAD design leverages on the following key enabling technologies: (a) Fully depleted Si monolithic active pixel sensors; (b) Scintillators read by silicon photomultipliers (SiPM); (c) Silicon on Insulator (SOI) microdosimetry sensors; These technologies promise miniaturization and mass reduction with state-of-the-art performance. The instrument's design is presented, and the Monte Carlo study of the feasibility of particle identification and kinetic energy determination is discussed","sentences":["Moon is an auspicious environment for the study of Galactic cosmic rays (GCR) and Solar particle events (SEP) due to the absence of magnetic field and atmosphere.","The same characteristics raise the radiation risk for human presence in orbit around it or at the lunar surface.","The secondary (albedo) radiation resulting from the interaction of the primary radiation with the lunar soil adds an extra risk factor, because neutrons are produced, but also it can be exploited to study the soil composition.","In this paper, the design of a comprehensive radiation monitor package tailored to the lunar environment is presented.","The detector, named LURAD, will perform spectroscopic measurements of protons, electrons, heavy ions, as well as gamma-rays, and neutrons.","A microdosimetry monitor subsystem is foreseen which can provide measurements of LET(Si) spectra in a wide dynamic range of LET(Si) and flux for SPE and GCR, detection of neutrons and biological dose for radiation protection of astronauts.","The LURAD design leverages on the following key enabling technologies: (a) Fully depleted Si monolithic active pixel sensors; (b) Scintillators read by silicon photomultipliers (SiPM); (c) Silicon on Insulator (SOI) microdosimetry sensors; These technologies promise miniaturization and mass reduction with state-of-the-art performance.","The instrument's design is presented, and the Monte Carlo study of the feasibility of particle identification and kinetic energy determination is discussed"],"url":"http://arxiv.org/abs/2405.03187v1","category":"astro-ph.IM"}
{"created":"2024-05-06 05:52:25","title":"Limits via relations","abstract":"In this paper, we study operations on functors in the category of abelian groups simplar to the derivation in the sense of Dold-Puppe. They are defined as derived limits of a functor applied to the relation subgroup over a category of free presentations of the group. The integral homology of the Eilenberg-Maclane space $K(\\mathbb Z,3)$ appears as a part of description of these operations applied to symmetric powers.","sentences":["In this paper, we study operations on functors in the category of abelian groups simplar to the derivation in the sense of Dold-Puppe.","They are defined as derived limits of a functor applied to the relation subgroup over a category of free presentations of the group.","The integral homology of the Eilenberg-Maclane space $K(\\mathbb Z,3)$ appears as a part of description of these operations applied to symmetric powers."],"url":"http://arxiv.org/abs/2405.03175v1","category":"math.KT"}
{"created":"2024-05-06 05:51:06","title":"Beam energy dependence of transverse momentum distribution and elliptic flow in Au-Au collisions using HYDJET++ model","abstract":"In this work, we present the transverse momentum spectra and elliptic flow ($v_2$) of $\\pi^\\pm,k^\\pm,$ $p$ and $\\bar{p}$ in Au-Au collisions at $\\sqrt{s_{NN}}=$ 62.4, 39.0, 27.0, 19.6 and 11.5 GeV using HYDJET++ model. For each beam energy the transverse momentum spectra are shown in four centrality classes at mid rapidity $|y|\\le$ 0.1 and for elliptic flow ($v_2$) we have shown results for minimum bias events. We have performed a qualitative comparison between the hard and soft processes at different beam energies. The HYDJET++ model calculations for particle ratios, $p_T$ spectra, and $v_2$ agree well with the available experimental data. The HYDJET++ parameters like baryonic and strangeness chemical potential, and chemical and thermal freeze-out temperature vary with collision energy. Azimuthal spatial anisotropy ($\\epsilon$) and azimuthal momentum anisotropy ($\\delta$) decrease with collision energy which leads to a smaller value of $v_2$ at lower collision energies. Furthermore, the hadrons containing strange quarks tend to have smaller values of spatial and momentum anisotropy than the non-strange hadrons.","sentences":["In this work, we present the transverse momentum spectra and elliptic flow ($v_2$) of $\\pi^\\pm,k^\\pm,$ $p$ and $\\bar{p}$ in Au-Au collisions at $\\sqrt{s_{NN}}=$ 62.4, 39.0, 27.0, 19.6 and 11.5 GeV using HYDJET++ model.","For each beam energy the transverse momentum spectra are shown in four centrality classes at mid rapidity $|y|\\le$ 0.1 and for elliptic flow ($v_2$) we have shown results for minimum bias events.","We have performed a qualitative comparison between the hard and soft processes at different beam energies.","The HYDJET++ model calculations for particle ratios, $p_T$ spectra, and $v_2$ agree well with the available experimental data.","The HYDJET++ parameters like baryonic and strangeness chemical potential, and chemical and thermal freeze-out temperature vary with collision energy.","Azimuthal spatial anisotropy ($\\epsilon$) and azimuthal momentum anisotropy ($\\delta$) decrease with collision energy which leads to a smaller value of $v_2$ at lower collision energies.","Furthermore, the hadrons containing strange quarks tend to have smaller values of spatial and momentum anisotropy than the non-strange hadrons."],"url":"http://arxiv.org/abs/2405.03174v1","category":"hep-ph"}
{"created":"2024-05-06 05:14:03","title":"Crystal orbital overlap population based on all-electron ab initio simulation with numeric atom-centered orbitals and its application to chemical-bonding analysis in Li-intercalated layered materials","abstract":"Crystal Orbital Overlap Population (COOP) is one of the effective tools for chemical-bonding analysis, and thus it has been utilized in the materials development and characterization. In this study, we developed a code to perform the COOP-based chemical-bonding analysis based on the wavefunction obtained from a first principles all-electron calculation with numeric atom-centered orbitals. The chemical-bonding analysis using the developed code was demonstrated for F2 and Si. Furthermore, we applied the method to analyze the chemical-bonding changes associated with a Li intercalation in three representative layered materials: graphite, MoS2, and ZrNCl, because of their great industrial importance, particularly for the applications in battery and superconducting materials. The COOP analysis provided some insights for understanding the intercalation mechanism and the stability of the intercalated materials from a chemical-bonding viewpoint.","sentences":["Crystal Orbital Overlap Population (COOP) is one of the effective tools for chemical-bonding analysis, and thus it has been utilized in the materials development and characterization.","In this study, we developed a code to perform the COOP-based chemical-bonding analysis based on the wavefunction obtained from a first principles all-electron calculation with numeric atom-centered orbitals.","The chemical-bonding analysis using the developed code was demonstrated for F2 and Si.","Furthermore, we applied the method to analyze the chemical-bonding changes associated with a Li intercalation in three representative layered materials: graphite, MoS2, and ZrNCl, because of their great industrial importance, particularly for the applications in battery and superconducting materials.","The COOP analysis provided some insights for understanding the intercalation mechanism and the stability of the intercalated materials from a chemical-bonding viewpoint."],"url":"http://arxiv.org/abs/2405.03165v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-06 04:25:55","title":"Primordial power spectrum at N3LO in effective theories of inflation","abstract":"We develop a systematic framework to compute the primordial power spectrum up to next-to-next-to-next to leading order (N3LO) in the Hubble-flow parameters for a large class of effective theories of inflation. We assume that the quadratic action for perturbations is characterized by two functions of time, the kinetic amplitude and the speed of sound, that are independent of the Fourier mode $k$. Using the Green's function method introduced by Stewart $\\&$ Gong and developed by Auclair $\\&$ Ringeval, we determine the primordial power spectrum, including its amplitude, spectral indices, their running and running of their running, starting from a given generic action for perturbations. As a check, we reproduce the state-of-the-art results for scalar and the tensor power spectrum of the simplest \"vanilla\" models of single-field inflation. The framework applies to Weinberg's effective field theory of inflation (with the condition of no parity violation) and to effective theory of spontaneous de Sitter-symmetry breaking. As a concrete application, we provide the expression for the N3LO power spectrum of $R+R^2$ Starobinsky inflation, without a field redefinition. All expressions are provided in terms of an expansion in one single parameter, the number of inflationary e-foldings $N_*$. Surprisingly we find that, compared to previous leading-order calculations, for $N_* = 55$ the N3LO correction results in a $7\\%$ decrease of the predicted tensor-to-scalar ratio, in addition to a deviation from the consistency relation. These results provide precise theoretical predictions for the next generation of CMB observations.","sentences":["We develop a systematic framework to compute the primordial power spectrum up to next-to-next-to-next to leading order (N3LO) in the Hubble-flow parameters for a large class of effective theories of inflation.","We assume that the quadratic action for perturbations is characterized by two functions of time, the kinetic amplitude and the speed of sound, that are independent of the Fourier mode $k$. Using the Green's function method introduced by Stewart $\\&$ Gong and developed by Auclair $\\&$ Ringeval, we determine the primordial power spectrum, including its amplitude, spectral indices, their running and running of their running, starting from a given generic action for perturbations.","As a check, we reproduce the state-of-the-art results for scalar and the tensor power spectrum of the simplest \"vanilla\" models of single-field inflation.","The framework applies to Weinberg's effective field theory of inflation (with the condition of no parity violation) and to effective theory of spontaneous de Sitter-symmetry breaking.","As a concrete application, we provide the expression for the N3LO power spectrum of $R+R^2$ Starobinsky inflation, without a field redefinition.","All expressions are provided in terms of an expansion in one single parameter, the number of inflationary e-foldings $N_*$. Surprisingly we find that, compared to previous leading-order calculations, for $N_*","= 55$ the N3LO correction results in a $7\\%$ decrease of the predicted tensor-to-scalar ratio, in addition to a deviation from the consistency relation.","These results provide precise theoretical predictions for the next generation of CMB observations."],"url":"http://arxiv.org/abs/2405.03157v1","category":"gr-qc"}
{"created":"2024-05-06 03:39:54","title":"Projection-Free Method for the Full Frank-Oseen Model of Liquid Crystals","abstract":"Liquid crystals are materials that experience an intermediate phase where the material can flow like a liquid, but the molecules maintain an orientation order. The Frank-Oseen model is a continuum model of a liquid crystal. The model represents the liquid crystal orientation as a vector field and posits that the vector field minimizes some elastic energy subject to a pointwise unit length constraint, which is a nonconvex constraint. Previous numerical methods in the literature assumed restrictions on the physical constants or had regularity assumptions that ruled out point defects, which are important physical phenomena to model. We present a finite element discretization of the full Frank-Oseen model and a projection free gradient flow algorithm for the discrete problem in the spirit of Bartels (2016). We prove Gamma-convergence of the discrete to the continuous problem: weak convergence of subsequences of discrete minimizers and convergence of energies. We also prove that the gradient flow algorithm has a desirable energy decrease property. Our analysis only requires that the physical constants are positive, which presents challenges due to the additional nonlinearities from the elastic energy.","sentences":["Liquid crystals are materials that experience an intermediate phase where the material can flow like a liquid, but the molecules maintain an orientation order.","The Frank-Oseen model is a continuum model of a liquid crystal.","The model represents the liquid crystal orientation as a vector field and posits that the vector field minimizes some elastic energy subject to a pointwise unit length constraint, which is a nonconvex constraint.","Previous numerical methods in the literature assumed restrictions on the physical constants or had regularity assumptions that ruled out point defects, which are important physical phenomena to model.","We present a finite element discretization of the full Frank-Oseen model and a projection free gradient flow algorithm for the discrete problem in the spirit of Bartels (2016).","We prove Gamma-convergence of the discrete to the continuous problem: weak convergence of subsequences of discrete minimizers and convergence of energies.","We also prove that the gradient flow algorithm has a desirable energy decrease property.","Our analysis only requires that the physical constants are positive, which presents challenges due to the additional nonlinearities from the elastic energy."],"url":"http://arxiv.org/abs/2405.03145v1","category":"math.NA"}
{"created":"2024-05-06 03:26:44","title":"Choked precessing jets in tidal disruption events and high-energy neutrinos","abstract":"It has been suggested that relativistic jets might have been commonly formed in tidal disruption events (TDEs), but those with relatively weak power could be choked by the surrounding envelope. The discovery of high-energy neutrinos possibly associated with some normal TDEs may support this picture in the hypothesis that the neutrinos are produced by choked jets. Recently, it was noted that disrupted stars generally have misaligned orbits with respect to the supermassive black hole spin axis and highly misaligned precessing jets are more likely to be choked. Here we revisit the jet break-out condition for misaligned precessing jets by considering the jet could be collimated by the cocoon pressure while propagating in the disk wind envelope. The jet head opening angle decreases as the jet propagates in the envelope, but the minimum power of a successful jet remains unchanged in terms of the physical jet power. We further calculate the neutrino flux from choked precessing jets, assuming that the cocoon energy does not exceed the kinetic energy of the disk wind. We find that neutrino flux from highly misaligned choked jets is sufficient to explain the neutrinos from AT2019aalc, while it is marginal to explain the neutrinos from AT2019dsg and AT2019fdr. The latter could be produced by weakly misaligned choked jets, since the duty cycle that the jet sweeps across increases as the misaligned angle decreases. We also show that the population of choked TDE jets could contribute to ~10% of the observed diffuse neutrino flux measured by IceCube.","sentences":["It has been suggested that relativistic jets might have been commonly formed in tidal disruption events (TDEs), but those with relatively weak power could be choked by the surrounding envelope.","The discovery of high-energy neutrinos possibly associated with some normal TDEs may support this picture in the hypothesis that the neutrinos are produced by choked jets.","Recently, it was noted that disrupted stars generally have misaligned orbits with respect to the supermassive black hole spin axis and highly misaligned precessing jets are more likely to be choked.","Here we revisit the jet break-out condition for misaligned precessing jets by considering the jet could be collimated by the cocoon pressure while propagating in the disk wind envelope.","The jet head opening angle decreases as the jet propagates in the envelope, but the minimum power of a successful jet remains unchanged in terms of the physical jet power.","We further calculate the neutrino flux from choked precessing jets, assuming that the cocoon energy does not exceed the kinetic energy of the disk wind.","We find that neutrino flux from highly misaligned choked jets is sufficient to explain the neutrinos from AT2019aalc, while it is marginal to explain the neutrinos from AT2019dsg and AT2019fdr.","The latter could be produced by weakly misaligned choked jets, since the duty cycle that the jet sweeps across increases as the misaligned angle decreases.","We also show that the population of choked TDE jets could contribute to ~10% of the observed diffuse neutrino flux measured by IceCube."],"url":"http://arxiv.org/abs/2405.03139v1","category":"astro-ph.HE"}
{"created":"2024-05-06 02:46:52","title":"KdV conformal symmetry breaking in nearly AdS$_{2}$","abstract":"We study the gauge theory formulation of Jackiw-Teitelboim gravity and propose Korteweg-de Vries asymptotic conditions that generalize the asymptotic dynamics of the theory. They permit to construct an enlarged set of boundary actions formed by higher order generalizations of the Schwarzian derivative that contain the Schwarzian as lower term in a tower of SL$(2,\\mathbb{R})$ invariants. They are extracted from the KdV Hamiltonians and can be obtained recursively. As a result, the conformal symmetry breaking observed in nearly AdS$_{2}$ is characterized by a much larger set of dynamical modes associated to the stationary KdV hierarchy. We study quantum perturbation theory for the generalized Schwarzian action including the symplectic measure and compute the one-loop correction to the partition function. We find that that despite the non-linear nature of the higher-Schwarzian contribution, it acquires a manageable expression that renders a curious leading temperature dependence on the entropy $S=\\#T^{a}$ for $a$ an odd integer.","sentences":["We study the gauge theory formulation of Jackiw-Teitelboim gravity and propose Korteweg-de Vries asymptotic conditions that generalize the asymptotic dynamics of the theory.","They permit to construct an enlarged set of boundary actions formed by higher order generalizations of the Schwarzian derivative that contain the Schwarzian as lower term in a tower of SL$(2,\\mathbb{R})$ invariants.","They are extracted from the KdV Hamiltonians and can be obtained recursively.","As a result, the conformal symmetry breaking observed in nearly AdS$_{2}$ is characterized by a much larger set of dynamical modes associated to the stationary KdV hierarchy.","We study quantum perturbation theory for the generalized Schwarzian action including the symplectic measure and compute the one-loop correction to the partition function.","We find that that despite the non-linear nature of the higher-Schwarzian contribution, it acquires a manageable expression that renders a curious leading temperature dependence on the entropy $S=\\#T^{a}$ for $a$ an odd integer."],"url":"http://arxiv.org/abs/2405.03128v1","category":"hep-th"}
{"created":"2024-05-06 02:46:12","title":"When Standard Model Higgs Meets Its Lighter 95 GeV Higgs","abstract":"Two excesses reported recently at the LHC in the lighter Higgs mass region around 95 GeV and in the rare $Z \\gamma$ final state of the Standard Model (SM) 125 GeV Higgs decay are simultaneously scrutinized within the framework of minimal gauged two-Higgs-doublet model (G2HDM). Viable parameter space in G2HDM is obtained to account for both excesses. We find a strong correlation between the signal strengths of SM 125 GeV Higgs decays into $\\gamma \\gamma$ and $Z \\gamma$ modes, whereas this correlation does not extend to its lighter 95 GeV cousin.","sentences":["Two excesses reported recently at the LHC in the lighter Higgs mass region around 95 GeV and in the rare $Z \\gamma$ final state of the Standard Model (SM) 125 GeV Higgs decay are simultaneously scrutinized within the framework of minimal gauged two-Higgs-doublet model (G2HDM).","Viable parameter space in G2HDM is obtained to account for both excesses.","We find a strong correlation between the signal strengths of SM 125 GeV Higgs decays into $\\gamma \\gamma$ and $Z \\gamma$ modes, whereas this correlation does not extend to its lighter 95 GeV cousin."],"url":"http://arxiv.org/abs/2405.03127v1","category":"hep-ph"}
{"created":"2024-05-06 02:23:23","title":"Galaxies with Biconical Ionized Structure in MaNGA - I. Sample Selection and Driven Mechanisms","abstract":"Based on the integral field unit (IFU) data from Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey, we develop a new method to select galaxies with biconical ionized structures, building a sample of 142 edge-on biconical ionized galaxies. We classify these 142 galaxies into 81 star-forming galaxies, 31 composite galaxies, and 30 AGNs (consisting of 23 Seyferts and 7 LI(N)ERs) according to the {\\nii}-BPT diagram. The star-forming bicones have bar-like structures while AGN bicones display hourglass structures, and composite bicones exhibit transitional morphologies between them due to both black hole and star-formation activities. Star-forming bicones have intense star-formation activities in their central regions, and the primary driver of biconical structures is the central star formation rate surface density. The lack of difference in the strength of central black hole activities (traced by dust attenuation corrected {\\oiii}$\\lambda$5007 luminosity and Eddington ratio) between Seyfert bicones and their control samples can be naturally explained as that the accretion disk and the galactic disk are not necessarily coplanar. Additionally, the biconical galaxies with central LI(N)ER-like line ratios are edge-on disk galaxies that show strong central dust attenuation. The radial gradients of {\\ha} surface brightness follow the $r^{-2.35}$ relation, roughly consistent with $r^{-2}$ profile, which is expected in the case of photoionization by a central point-like source. These observations indicate obscured AGNs or AGN echoes as the primary drivers of biconical structures in LI(N)ERs.","sentences":["Based on the integral field unit (IFU) data from Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey, we develop a new method to select galaxies with biconical ionized structures, building a sample of 142 edge-on biconical ionized galaxies.","We classify these 142 galaxies into 81 star-forming galaxies, 31 composite galaxies, and 30 AGNs (consisting of 23 Seyferts and 7 LI(N)ERs) according to the {\\nii}-BPT diagram.","The star-forming bicones have bar-like structures while AGN bicones display hourglass structures, and composite bicones exhibit transitional morphologies between them due to both black hole and star-formation activities.","Star-forming bicones have intense star-formation activities in their central regions, and the primary driver of biconical structures is the central star formation rate surface density.","The lack of difference in the strength of central black hole activities (traced by dust attenuation corrected {\\oiii}$\\lambda$5007 luminosity and Eddington ratio) between Seyfert bicones and their control samples can be naturally explained as that the accretion disk and the galactic disk are not necessarily coplanar.","Additionally, the biconical galaxies with central LI(N)ER-like line ratios are edge-on disk galaxies that show strong central dust attenuation.","The radial gradients of {\\ha} surface brightness follow the $r^{-2.35}$ relation, roughly consistent with $r^{-2}$ profile, which is expected in the case of photoionization by a central point-like source.","These observations indicate obscured AGNs or AGN echoes as the primary drivers of biconical structures in LI(N)ERs."],"url":"http://arxiv.org/abs/2405.03117v1","category":"astro-ph.GA"}
{"created":"2024-05-06 02:07:13","title":"An Active Inference Agent for Simulating Human Translation Processes in a Hierarchical Architecture: Integrating the Task Segment Framework and the HOF taxonomy","abstract":"In this paper, we propose modelling human translation production as a hierarchy of three embedded translation processes. The proposed architecture replicates the temporal dynamics of keystroke production across sensorimotor, cognitive, and phenomenal layers. Utilizing data from the CRITT TPR-DB, the Task Segment Framework, and the HOF taxonomy, we demonstrate the temporal breakdown of the typing flow on distinct timelines within these three layers.","sentences":["In this paper, we propose modelling human translation production as a hierarchy of three embedded translation processes.","The proposed architecture replicates the temporal dynamics of keystroke production across sensorimotor, cognitive, and phenomenal layers.","Utilizing data from the CRITT TPR-DB, the Task Segment Framework, and the HOF taxonomy, we demonstrate the temporal breakdown of the typing flow on distinct timelines within these three layers."],"url":"http://arxiv.org/abs/2405.03111v1","category":"cs.CL"}
{"created":"2024-05-06 01:47:11","title":"Gate-defined quantum point contacts in a germanium quantum well","abstract":"We report an experimental study of quantum point contacts defined in a high-quality strained germanium quantum well with layered electric gates. At zero magnetic field, we observe quantized conductance plateaus in units of 2$e^2/h$. Bias-spectroscopy measurements reveal that the energy spacing between successive one-dimensional subbands ranges from 1.5 to 5\\,meV as a consequence of the small effective mass of the holes and the narrow gate constrictions. At finite magnetic fields perpendicular to the device plane, the edges of the conductance plateaus get splitted due to the Zeeman effect and Land\\'{e} $g$ factors are estimated to be $\\sim6.6$ for the holes in the germanium quantum well. We demonstrate that all quantum point contacts in the same device have comparable performances, indicating a reliable and reproducible device fabrication process. Thus, our work lays a foundation for investigating multiple forefronts of physics in germanium-based quantum devices that require quantum point contacts as a building block.","sentences":["We report an experimental study of quantum point contacts defined in a high-quality strained germanium quantum well with layered electric gates.","At zero magnetic field, we observe quantized conductance plateaus in units of 2$e^2/h$. Bias-spectroscopy measurements reveal that the energy spacing between successive one-dimensional subbands ranges from 1.5 to 5\\,meV as a consequence of the small effective mass of the holes and the narrow gate constrictions.","At finite magnetic fields perpendicular to the device plane, the edges of the conductance plateaus get splitted due to the Zeeman effect and Land\\'{e} $g$ factors are estimated to be $\\sim6.6$ for the holes in the germanium","quantum well.","We demonstrate that all quantum point contacts in the same device have comparable performances, indicating a reliable and reproducible device fabrication process.","Thus, our work lays a foundation for investigating multiple forefronts of physics in germanium-based quantum devices that require quantum point contacts as a building block."],"url":"http://arxiv.org/abs/2405.03107v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-06 01:41:19","title":"Thermodynamic stability in relativistic viscous and spin hydrodynamics","abstract":"We have applied thermodynamic stability analysis to derive the stability and causality conditions for conventional relativistic viscous hydrodynamics and spin hydrodynamics. We obtain the thermodynamic stability conditions for second-order relativistic hydrodynamics with shear and bulk viscous tensors, finding them identical to those derived from linear mode analysis. We then derive the thermodynamic stability conditions for minimal causal extended second-order spin hydrodynamics in canonical form, both with and without viscous tensors. Without viscous tensors, the constraints from thermodynamic stability exactly match those from linear mode analysis. In the presence of viscous tensors, the thermodynamic stability imposes more stringent constraints than those obtained from linear mode analysis. Our results suggest that conditions derived from thermodynamic stability analysis can guarantee both causality and stability in linear mode analysis.","sentences":["We have applied thermodynamic stability analysis to derive the stability and causality conditions for conventional relativistic viscous hydrodynamics and spin hydrodynamics.","We obtain the thermodynamic stability conditions for second-order relativistic hydrodynamics with shear and bulk viscous tensors, finding them identical to those derived from linear mode analysis.","We then derive the thermodynamic stability conditions for minimal causal extended second-order spin hydrodynamics in canonical form, both with and without viscous tensors.","Without viscous tensors, the constraints from thermodynamic stability exactly match those from linear mode analysis.","In the presence of viscous tensors, the thermodynamic stability imposes more stringent constraints than those obtained from linear mode analysis.","Our results suggest that conditions derived from thermodynamic stability analysis can guarantee both causality and stability in linear mode analysis."],"url":"http://arxiv.org/abs/2405.03105v1","category":"nucl-th"}
{"created":"2024-05-06 01:21:13","title":"Exact Sampling of Spanning Trees via Fast-forwarded Random Walks","abstract":"Tree graphs are routinely used in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous--Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain sub-graphs. We formalize this phenomenon using the bottlenecks in the random walk's transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset.","sentences":["Tree graphs are routinely used in statistics.","When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty.","Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing.","A promising approach is to instead directly sample spanning trees on an auxiliary graph.","Current spanning tree samplers, such as the celebrated Aldous--Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph.","Such algorithms are prone to getting stuck in certain sub-graphs.","We formalize this phenomenon using the bottlenecks in the random walk's transition probability matrix.","We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks.","The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node.","Unlike many existing approximation algorithms, our algorithm yields exact samples.","We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset."],"url":"http://arxiv.org/abs/2405.03096v1","category":"stat.ME"}
{"created":"2024-05-06 01:13:48","title":"The $H\\rightarrow Z\u03b3$ decay and $CP$ violation","abstract":"The effects of $CP$-violation on the reported result by the LHC $\\mu^{Z\\gamma}=2.2\\pm 0.7$ for the $H\\rightarrow Z\\gamma$ signal strength are studied in new physics models, where bounds on the real and absorptive parts of the $CP$-violating form factor $h_3^{Z\\gamma}$ are obtained, which are found to be less than 1.02 GeV. The leading order SM contributions to the $H\\rightarrow Z\\gamma$ decay are also revisited, whereas contributions to the $CP$-violating form factor $h_3^{Z\\gamma}$ from FCNC complex couplings mediated by the $Z$ and $H$ bosons are reported. Using the current bounds on such a couplings, where for these related to the Higgs boson are calculated to be or order $10^{-1}$. We find that the FCNC contribution to $h_3^{Z\\gamma}$ with top and charm quarks in the loop is of order $10^{-6}$. A generic model with new quarks is also studied. It is found a parameter space, which agrees with the $H\\rightarrow\\gamma\\gamma$ decay and the new $\\mu^{Z\\gamma}$ LHC result.","sentences":["The effects of $CP$-violation on the reported result by the LHC $\\mu^{Z\\gamma}=2.2\\pm 0.7$ for the $H\\rightarrow Z\\gamma$ signal strength are studied in new physics models, where bounds on the real and absorptive parts of the $CP$-violating form factor $h_3^{Z\\gamma}$ are obtained, which are found to be less than 1.02 GeV.","The leading order SM contributions to the $H\\rightarrow Z\\gamma$ decay are also revisited, whereas contributions to the $CP$-violating form factor $h_3^{Z\\gamma}$ from FCNC complex couplings mediated by the $Z$ and $H$ bosons are reported.","Using the current bounds on such a couplings, where for these related to the Higgs boson are calculated to be or order $10^{-1}$. We find that the FCNC contribution to $h_3^{Z\\gamma}$ with top and charm quarks in the loop is of order $10^{-6}$. A generic model with new quarks is also studied.","It is found a parameter space, which agrees with the $H\\rightarrow\\gamma\\gamma$ decay and the new $\\mu^{Z\\gamma}$ LHC result."],"url":"http://arxiv.org/abs/2405.03094v1","category":"hep-ph"}
{"created":"2024-05-06 00:18:43","title":"Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation","abstract":"Large Language Models (LLMs) have made significant strides in information acquisition. However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries. Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge. Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention. Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm. The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features. The concepts explicitly constrain LLMs to focus solely on vital information in the inference process. We conduct extensive experiments on open-domain question-answering datasets to empirically evaluate the proposed method's effectiveness. The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs. This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information. To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression.","sentences":["Large Language Models (LLMs) have made significant strides in information acquisition.","However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries.","Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge.","Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention.","Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm.","The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features.","The concepts explicitly constrain LLMs to focus solely on vital information in the inference process.","We conduct extensive experiments on open-domain question-answering datasets to empirically evaluate the proposed method's effectiveness.","The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs.","This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information.","To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression."],"url":"http://arxiv.org/abs/2405.03085v1","category":"cs.CL"}
